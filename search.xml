<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[计算广告面面观（5）：DSSN模型]]></title>
    <url>%2F2020%2F03%2F28%2FAD-RS-5%2F</url>
    <content type="text"><![CDATA[在搜索广告的场景中，query 和 document 使用不同的单词、同一个单词的不同形态（如：缩写、时态、单复数）来表达同一个概念。如果简单的通过文本的单词匹配来计算 query 和 document 的相似性，则效果不好。 一种解决方式是：利用潜在语义模型latent semantic model （如：LSA），将 query 和 document 都降维到低维语义空间，然后根据二者在低维空间的距离来计算二者相似度。 论文 “Learning Deep Structured Semantic Models for Web Search using Clickthrough Data” 提出 Deep Structured Semantic Model:DSSM 模型，该模型也是将 query 和 document 降维到公共的低维空间。在该低维空间中，query 和 document 的相似性也是通过二者的距离来衡量。 其中LSA 的低维空间是通过无监督学习，利用单词的共现规律来训练；而DSSM的低维空间是通过有监督学习，利用 (query,document) pair 对的点击规律来训练。最终实验表明：DSSM 模型要优于 LSA 模型。 为解决搜索广告中词汇量大的问题（即：词汇表过于庞大），DSSN模型采用了 word hash 技术。 一、模型DSSM 模型将原始的文本特征映射到低维的语义空间。 首先将 query 和 document 表示为词频向量，该向量由每个单词出现的词频组成。如：query = 苹果手机 / 价格， document = Iphone / Xs / 最低 / 售价 / 11399 / 元 / Iphone / X / 价格 / 6999 元 。 构建词汇表： 1苹果手机 Iphone x Xs 价格 最低 售价 6999 11399 元 则得到 query 向量和 document 向量为： 然后将$\overrightarrow { q } $和 $\overrightarrow { d } $ 映射到低维语义空间，得到 query 语义向量 和 document 语义向量 。 计算$\overrightarrow { { y }_{ q } } $和 $\overrightarrow { { y }_{ d } } $ 的相似度： 给定 query i ，计算所有document 与它的相似度，并截取 top K 个 document 即可得到排序结果： 其中$L_i$是 query i 的排序结果（根据相似度降序排列），$D_i$是所有与 query i 有关的文档。 隐藏层的处理： 假设输入向量为$\overrightarrow { x }$，输出向量为 $\overrightarrow { y }$ ，网络一共有$L$层。对于 query，输入就是$\overrightarrow { q }$，输出就是$\overrightarrow { y_q}$ ；对于 document，输入就是$\overrightarrow { d }$ ，输出就是 $\overrightarrow { y_d }$ 第$l$层的隐向量为： 其中：$\sigma(·)$为激活函数。论文（2013年）采用 tanh 激活函数，但是现在推荐 relu 激活函数。$W_i$,$\overrightarrow { b_i }$为待学习的网络参数。 训练过程 给定 query $\overrightarrow { q }$ 和 document $\overrightarrow { d }$ ，用户点击该文档的概率为： 其中$\gamma$为平滑因子，它是一个超参数，需要根据验证集来执行超参数搜索；$D$是候选的文档集合。 实际应用中，给定一对点击样本$(\overrightarrow { q },\overrightarrow { d }^+)$，我们从曝光但是未点击的文档中随机选择K篇文档作为负样本$(\overrightarrow { q },\overrightarrow { d_k }^-),k=1,2,···,K$ ，则$D =\{\overrightarrow { d^+ },\overrightarrow { d_1 }^-,\overrightarrow { d_2 }^-···\overrightarrow { d_K }^-\}$ 论文中选择K=4，并且论文表示：K不同的负采样策略对结果没有显著影响。 模型训练的目标是：最大化点击样本的对数似然： 然后基于随机梯度下降优化算法来求解该最优化问题。 注意：这里并没有计算负样本的概率$p(\overrightarrow { d_k }^-|\overrightarrow { q })$，负样本的信息在计算概率$p(\overrightarrow { d_k }^+|\overrightarrow { q })$时被使用。 二、word hash在将 query/document 的文本转化为输入向量的过程中，输入向量的维度等于词表的大小。由于实际 web search 任务中的词汇表非常庞大，这导致 DSSM 网络的输入层的参数太多，模型难以训练。 假设词汇表有50万，经过 embedding 之后的维度为300维，则输入层权重为$W\in R^{50w·300}$万，一共1.5亿参数。为解决该问题，DSSM 模型在第一层引入 word hash 技术。该层是一个线性映射，虽然参数非常多，但是这些参数不需要更新和学习。 word hash 技术用于降低输入向量的维度。给定一个单词，如：good，word hash 的步骤为： 首先添加开始标记、结束标记：#good# 然后将其分解为字符级的 n-gram 格式：#go,goo,ood,od# （n=3 时） 最后将文本中的单词 good 用一组 char-level n-gram 替代。 虽然英语词汇的数量可以是无限的（可以出现大量的、新的合成词），但是英语（或其它类似语言）的字符n-gram 数量通常是有限的。因此word hash 能够大幅降低词汇表的大小。 50万规模的词汇表经过 word hash 之后降低到3万规模，这使得输入层的参数降低到 900万（假设 embedding 维度为 300 维）。相比较于原始的1.5亿，参数降低到原始数量的 1/16 。 除此之外，word-hash 技术还有以下优点： 它能够将同一个单词的不同形态变化映射到 char-level n-gram 空间中彼此接近的点。 它能够有效缓解 out-of-vocabulary:OOV 问题。在推断期间，虽然有些词汇未出现在训练集中（未登陆词），但是当拆解未 char-level n-gram 之后，每个 n-gram 都在训练集中出现过。 从单词到 char-level n-gram 的映射关系是固定的线性映射，不需要学习。 char-level n-gram 可以视作 word 的一个简单的 representation，而 word-hash 技术就是得到这个 representation 。 word-hash 一个潜在的问题是冲突 collision：两个不同的单词可能具有相同的 char-level n-gram 表示。下表中统计了两个词汇表中的冲突统计信息。可以看到，当采用 3-gram 表示时，冲突的占比小于千分之一。 三、实验论文实现的 DSSM 模型，包含四层： 第一层为 word hash 层，它将 word 映射为 char-level 3-gram 。其映射规则是固定的，不需要学习参数。 第二层、第三层为中间层，每层输出为 300维。 最后一层为输出层，输出 128维向量。 权重初始化：权重通过在$\left[ -\sqrt { \frac { 6 }{ { fan }_{ in }+fan_{ out } } } ,\sqrt { \frac { 6 }{ { fan }_{ in }+fan_{ out } } } , \right] $之间均匀分布的随机变量来初始化。其中$ { fan }_{ in }$,$ { fan }_{ out }$ 表示输入单元数量和输出单元数量。 模型通过 mini-batch 随机梯度下降法优化，每个 batch 包含 1024个样本，一共训练 20 个 epoch 。 模型原始词汇表为 50万（即：保留常见的50万词汇），经过 word hash 之后降低到 3万。 实验数据集：数据集是从商业搜索引擎的 1年 query 日志文件中采样的 16510 个 query，平均每个 query 有 15 个相关的 document。 每对 (query,document) 都有人工标注的标签。标签一共5个等级 0-4，0 表示无关，4 表示最相关。 DSSM 模型和其它模型的比较结果如图所示，其中模型的评估指标为 NDCG 。 9~12 行给出了不同的 DSSM 变化： DNN：没有采用 word-hash 的 DSSM 。它和第六行的DAE 结构相同，但是DAE 采用无监督学习训练，而DNN 采用有监督学习训练。为了能够训练DNN 模型，我们采用4万规模的词汇表（即：保留常见的4万词汇）。 L-WH linear：线性的 word hash 模型。在经过 word hash 之后，直接连接到输出层，且输出层不采用任何非线性函数。因此整个模型都是线性的。 L-WH non-linear：非线性的 word hash 模型。在经过 word hash 之后，直接连接到输出层，但是输出层采用非线性函数。 L-WH DNN：标准的 DSSM 模型。 结论： 从 DNN 和 DAE 的比较结果发现：监督学习普遍比无监督学习效果好 word hash 允许我们使用更大规模的词汇表。如 L-WH-DNN 采用 50万规模的词汇表，而 DNN 采用 4万规模的词汇表，但是 L-WH-DNN 的模型参数反而更少。词汇表越小，则未登陆词越多，这导致文本被丢弃的信息越多。模型的效果越差。因此 word hash 技术既可以减少模型参数，又能提升模型效果。 深层网络强于浅层网络。 无监督学习： LSA 可以看作浅层网络。深层网络的 DAE 效果强于浅层网络 LSA 。 监督学习：L-WH non-linear 可以视为 L-WH DNN 的浅层版本，实验结果表明后者效果更好。]]></content>
      <categories>
        <category>计算广告</category>
      </categories>
      <tags>
        <tag>搜索广告</tag>
        <tag>DSSN</tag>
        <tag>Word hash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark笔记（10）：宽依赖与窄依赖]]></title>
    <url>%2F2020%2F01%2F02%2Fspark-10%2F</url>
    <content type="text"><![CDATA[宽依赖与窄依赖RDD之间的依赖关系可以分为两类，即宽依赖和窄依赖，宽依赖与窄依赖的区分主要是父partition与子partition的对应关系，区分宽窄依赖主要就是看父RDD的一个partition的流向，要是流向一个的话就是窄依赖，流向多个的话就是宽依赖。。 如下图所示，其中空心方框表示一个RDD，实心蓝底的框表示partition： （1）窄依赖（narrow dependencies）：父partition对子partition是一对一或多对一（只有一个儿子，或者说父partition的出度为1）（2）宽依赖（wide dependencies）：父partition对子partition是一对多(有多个儿子，或者说父partition的出度大于1) 窄依赖一般是对RDD进行map，filter，union等Transformations。 union: 在两个RDD上执行union操作，返回两个父RDD分区的并集。通过相应父RDD上的窄依赖关系计算每个子RDD分区（注意union操作不会过滤重复值，相当于SQL中的UNION ALL）。 map: 任何RDD上都可以执行map操作，返回一个MappedRDD对象。该操作传递一个函数参数给map，对父RDD上的记录按照iterator的方式执行这个函数，并返回一组符合条件的父RDD分区及其位置。 宽依赖一般是对RDD进行groupByKey，reduceByKey等操作，就是对RDD中的partition中的数据进行shuffle。 groupByKey: 子RDD的所有Partition(s)会依赖于parent RDD的所有Partition(s)，子RDD的Partition是parent RDD的所有Partition Shuffle的结果，因此这两个RDD是不能通过一个计算任务来完成的。 对两个RDD执行join操作可能产生窄依赖（如果这两个RDD拥有相同的哈希分区或范围分区），可能是宽依赖，也可能两种依赖都有（比如一个父RDD有分区，而另一父RDD没有）。 stage的划分 Spark任务会根据RDD之间的依赖关系，形成一个DAG有向无环图，DAG会提交给DAGScheduler，DAGScheduler会把DAG划分成互相依赖的多个stage，划分stage的依据就是RDD之间的宽窄依赖。 如上图所示，A/B/C/D/E/F代表RDD，当执行算子存在shuffle操作的时候，就划分一个stage，即用宽依赖来划分stage。窄依赖会被划分到同一个stage中，这样他们就可以以管道的方式执行，宽依赖由于依赖的上游不止一个，所以往往需要需要跨节点传输数据， 宽依赖与窄依赖的对比相比于宽依赖，窄依赖对优化很有利，主要有两点： 宽依赖往往对应着shuffle操作，需要在运行过程中将同一个父RDD的分区传入到不同的子RDD分区中，中间可能涉及多个节点间的数据传输，而窄依赖的每个父RDD分区只会传入到一个子RDD分区中，通常可以在一个节点内完成转换。 当RDD分区丢失时（某个节点故障），spark会对数据进行重算： 对于窄依赖，由于父RDD的一个分区只对应一个子RDD分区，这样只需要重算和子RDD分区对应的父RDD分区即可，所以这个重算对数据的利用率是100%的 对于宽依赖，重算的父RDD分区对应多个字RDD分区，这样实际上父RDD中只有一部分的数据是被用于恢复这个丢失的子RDD分区的，另一部分对应子RDD的其他未丢失分区，这就造成了多余的计算，宽依赖中子RDD分区通常来自于多个父RDD分区，极端情况下，所有的父RDD分区都要重新计算 如下图所示，b1分区丢失，则需要重新计算a1，a2和a3，这样就产生了冗余计算（a1,a2,a3中对应着b2的数据）]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>宽依赖</tag>
        <tag>窄依赖</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[海德格尔中文出版书籍汇总]]></title>
    <url>%2F2020%2F01%2F01%2F%E6%9E%97%E4%B8%AD%E8%B7%AF%EF%BC%8811%EF%BC%89%EF%BC%9A%E6%B5%B7%E5%BE%B7%E6%A0%BC%E5%B0%94%E4%B8%AD%E6%96%87%E5%87%BA%E7%89%88%E4%B9%A6%E7%B1%8D%2F</url>
    <content type="text"><![CDATA[海德格尔 书名 作者 出版社 豆瓣评分 存在与时间 [德] 马丁·海德格尔 生活·读书·新知三联书店 8.7 / 3305人评价 林中路 [德] 马丁·海德格尔 上海译文出版社 8.8 / 913人评价 形而上学导论 [德] 马丁·海德格尔 商务印书馆 8.6 / 565人评价 路标 [德] 马丁·海德格尔 商务印书馆 8.9 (352人评价) 海德格尔存在哲学 [德] 马丁·海德格尔 九州出版社 8.2 (136人评价) 人，诗意地安居 [德] 马丁·海德格尔 广西师范大学出版社 8.1 (539人评价) 尼采（上下） [德] 马丁·海德格尔 商务印书馆 8.8 / 423人评价 荷尔德林诗的阐释 [德] 马丁·海德格尔 商务印书馆 8.7 / 392人评价 面向思的事情 [德] 马丁·海德格尔 商务印书馆 8.7 / 350人评价 演讲与论文集 [德] 马丁·海德格尔 生活·读书·新知三联书店 9.1 / 310人评价 论真理的本质 [德] 马丁·海德格尔 华夏出版社 9.3 / 129人评价 现象学之基本问题 [德] 马丁·海德格尔 上海译文出版社 9.0 / 128人评价 尼采十讲 [德] 马丁·海德格尔 中国言实出版社 7.6 / 108人评价 同一与差异 [德] 马丁·海德格尔 商务印书馆 9.1 / 96人评价 哲学论稿 [德]马丁·海德格尔 商务印书馆 9.3 / 90人评价 物的追问 [德]马丁·海德格尔 上海译文出版社 9.1 / 85人评价 思的经验 [德]马丁·海德格尔 人民出版社 8.0 / 75人评价 康德与形而上学疑难 [德]马丁·海德格尔 上海译文出版社 9.4 / 75人评价 时间概念史导论 [德]马丁·海德格尔 商务印书馆 9.1 / 71人评价 系于孤独之途 [德]马丁·海德格尔 天津人民出版社 8.2 / 62人评价 荷尔德林的新神话 [德] 马丁·海德格尔 华夏出版社 8.2 / 37人评价 存在论 [德]马丁·海德格尔 人民出版社 8.3 / 37人评价 根据律 [德] 马丁·海德格尔 商务印书馆 9.6 / 16人评价 在通向语言的途中 [德]马丁·海德格尔 商务印书馆 9.3 / 10人评价 柏拉图的《智者》 [德]马丁·海德格尔 商务印书馆 9.4 (19人评价) 亚里士多德哲学的基本概念 [德] 马丁·海德格尔 华夏出版社 9.1(22人评价) 海德格尔文集 [德] 马丁·海德格尔 华夏出版社 9.1 / 22人评价 形式显示的现象学 马丁·海德格尔 同济大学出版社 8.7 (89人评价) 书名 作者 出版社 豆瓣评分 来自德国的大师 吕迪格尔·萨弗兰斯基 商务印书馆 9.1 (231人评价) 海德格尔导论 彼得·特拉夫尼 同济大学出版社 8.6 (119人评价) 海德格尔 比梅尔 商务印书馆 8.7 (41人评价) 海德格尔 乔治·斯坦纳 浙江大学出版社 8.4(66人评价) 海德格尔传 吕迪格尔·萨弗兰斯基 商务印书馆 8.5(93人评价) 海德格尔 [英国] 迈克尔·英伍德 译林出版社 7.7(50人评价) 海德格尔 帕特里夏·奥坦伯德·约翰逊 中华书局 8.2(33人评价) 《存在与时间》释义 张汝伦 上海人民出版社 阿伦特与海德格尔 : 爱和思的故事 安东尼娅·格鲁嫩贝格 商务印书馆 7.6(175人评价) 《存在与时间》读本 陈嘉映 三联书店 8.5(114人评价) 海德格尔思想与中国天道 张祥龙 生活·读书·新知三联书店 8.6(153人评价) 还原与给予 : 胡塞尔、海德格尔与现象学研究 [法] 让-吕克·马里翁 上海译文出版社 9.5(52人评价) 分道而行 : 卡尔纳普、卡西尔和海德格尔 [美] 迈克尔·弗里德曼 / 张卜天 北京大学出版社 8.9(112人评价) 海德格尔哲学概论 陈嘉映 北京三联书店 8.5(144人评价) 策兰与海德格尔 : 一场悬而未决的对话：1951－1970 [美] 詹姆斯·K. 林恩 北京大学出版社 7.8(92人评价) 海德格尔与其思想的开端 : 海德格尔年鉴 第一卷 [法]阿尔弗雷德·登克尔 商务印书馆 9.0(37人评价) 海德格尔的根 : 尼采，国家社会主义和希腊人 [美]查尔斯·巴姆巴赫 上海书店出版社 8.0(56人评价) 海德格尔与伦理学问题 韩潮 同济大学出版社 8.6(65人评价) 存在的一代 : 海德格尔哲学在法国1927-1961 伊森•克莱因伯格 新星出版社 8.5(41人评价) 海德格尔与哲学的开端 王庆节 生活·读书·新知三联书店 7.9(77人评价) 说不可说之神秘 : 海德格尔后期思想研究 孙周兴 生活·读书·新知上海三联出版社 8.7(31人评价) 论精神 : 海德格尔与问题 [法]雅克·德里达 上海译文出版社 9.1(21人评价) 时间性：自身与他者 : 从胡塞尔、海德格尔到列维纳斯 王恒 江苏人民出版社 7.7(22人评价) 时间与永恒 : 论海德格尔哲学中的时间问题 黄裕生 江苏人民出版社 9.3(24人评价) 时间与存在 : 胡塞尔与海德格尔现象学的基本问题 方向红 商务印书馆 8.4(33人评价) 自由.平等.必死性-海德格尔以后的哲学 汉斯·艾伯林 华东师范大学出版社 7.4 (54人评价) 无之无化 彭富春 上海三联 7.8 (50人评价) 论海德格尔 彭富春 人民出版社 8.4 (25人评价) 存在与在 马丁・海德格尔 民族出版社 7.3 (75人评价) 海德格尔的政治时刻 刘小枫 / 陈少明 主编 华夏出版社 8.2 (18人评价) 海德格尔早期思想研究 靳希平 上海人民出版社 8.1 (41人评价) 海德格尔和马尔库塞 安德鲁·芬博格 上海社会科学院出版社 7.8 (15人评价) 海德格尔的政治存在论 皮埃尔·布迪厄 学林出版社 7.9 (29人评价) 海德格尔的弟子 理查德·沃林 江苏教育出版社 7.4 (55人评价) 分道而行 迈克尔·弗里德曼 北京大学出版社 9.0 (180人评价) 还原与给予 让-吕克·马里翁 上海译文出版社 9.5 (84人评价) 后哲学的哲学问题 孙周兴 商务印书馆 7.8 (48人评价) 存在的政治 理查德·沃林 商务印书馆 7.6 (17人评价) 回到源初的生存现象 朱清华 首都师范大学出版社 7.5 (15人评价) 海德格尔论东西方对话 马琳 中国人民大学出版社 8.2 (14人评价) 我们时代的思想姿态 孙周兴 东方出版社 7.6 (52人评价) 纯粹现代性批判 大卫・库尔珀 商务印书馆 8.0 (31人评价) 论精神 雅克·德里达 上海译文出版社 8.4 (106人评价) 海德格尔与现代哲学 张汝伦 复旦大学出版社 7.7 (16人评价) 真理之光 李文堂 江苏人民出版社 8.2 (13人评价) 海德格尔与传统 维尔纳·马克思 上海人民出版社 8.5 (35人评价) 海德格尔与雅斯贝尔斯往复书简 瓦尔特·比默尔 上海人民出版社 8.2 (42人评价) 艺术与归家 余虹 中国人民大学出版社 9.0 (244人评价) 语言存在论 孙周兴 商务印书馆 8.2 (77人评价) 回到思的事情 陈春文 武汉大学出版社 8.2 (34人评价) 解读《存在与时间》 王路 北京大学出版社 7.6 (12人评价) 技术与时间 贝尔纳·斯蒂格勒 译林出版社 8.1 (110人评价) 诗与哲学之争 罗森 (Stanley Rosen) 华夏出版社 7.8 (95人评价)]]></content>
      <categories>
        <category>海德格尔</category>
      </categories>
      <tags>
        <tag>海德格尔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark笔记（9）：Spark性能优化之shuffle调优]]></title>
    <url>%2F2019%2F11%2F16%2Fspark-9%2F</url>
    <content type="text"><![CDATA[大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。 ShuffleManager发展概述在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。 在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。 因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。 下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。 HashShuffleManager运行原理未经优化的HashShuffleManager]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark性能</tag>
        <tag>shuffle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark笔记（8）：Spark性能优化之数据倾斜处理]]></title>
    <url>%2F2019%2F11%2F10%2Fspark-8%2F</url>
    <content type="text"><![CDATA[前言继《Spark性能优化：开发调优篇》和《Spark性能优化：资源调优篇》讲解了每个Spark开发人员都必须熟知的开发调优与资源调优之后，本文作为《Spark性能优化指南》的高级篇，将深入分析数据倾斜调优与shuffle调优，以解决更加棘手的性能问题。 调优概述有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。 数据倾斜发生时的现象 绝大多数task执行得都非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。 原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。 数据倾斜发生的原理数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。 因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。 下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。 如何定位导致数据倾斜的代码数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。 某个task执行特别慢的情况首先要看的，就是数据倾斜发生在第几个stage中。 如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。 比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。 知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。 这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。 stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。 stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。 123456789val conf = new SparkConf()val sc = new SparkContext(conf) val lines = sc.textFile(&quot;hdfs://...&quot;)val words = lines.flatMap(_.split(&quot; &quot;))val pairs = words.map((_, 1))val wordCounts = pairs.reduceByKey(_ + _) wordCounts.collect().foreach(println(_)) 通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由educeByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。 某个task莫名其妙内存溢出的情况这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。 但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。 查看导致数据倾斜的key的数据分布情况知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。 此时根据你执行操作的情况不同，可以有很多种查看key分布的方式： 如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。 如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。 举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。 123val sampledPairs = pairs.sample(false, 0.1)val sampledWordCounts = sampledPairs.countByKey()sampledWordCounts.foreach(println(_)) 数据倾斜的解决方案解决方案一：使用Hive ETL预处理数据方案适用场景：导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。 方案实现思路：此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。 方案实现原理：这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。 方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。 方案缺点：治标不治本，Hive ETL中还是会发生数据倾斜。 方案实践经验：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。 解决方案二：过滤少数导致倾斜的key方案适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。 方案实现思路：如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。 方案实现原理：将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。 方案优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。 方案缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。 方案实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。 解决方案三：提高shuffle操作的并行度方案适用场景：如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。 方案实现思路：在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。 方案实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。 方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。 方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。 方案实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。 解决方案四：两阶段聚合（局部聚合+全局聚合）方案适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。 方案实现思路：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。 方案实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。 方案优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。 方案缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 第一步，给RDD中的每个key都打上一个随机前缀。JavaPairRDD&lt;String, Long&gt; randomPrefixRdd = rdd.mapToPair( new PairFunction&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(10); return new Tuple2&lt;String, Long&gt;(prefix + &quot;_&quot; + tuple._1, tuple._2); &#125; &#125;); // 第二步，对打上随机前缀的key进行局部聚合。JavaPairRDD&lt;String, Long&gt; localAggrRdd = randomPrefixRdd.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;); // 第三步，去除RDD中每个key的随机前缀。JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair( new PairFunction&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;String, Long&gt; tuple) throws Exception &#123; long originalKey = Long.valueOf(tuple._1.split(&quot;_&quot;)[1]); return new Tuple2&lt;Long, Long&gt;(originalKey, tuple._2); &#125; &#125;); // 第四步，对去除了随机前缀的RDD进行全局聚合。JavaPairRDD&lt;Long, Long&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;); 解决方案五：将reduce join转为map join方案适用场景：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。 方案实现思路：不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。 方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。 方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。 方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。 123456789101112131415161718192021222324252627282930313233// 首先将数据量比较小的RDD的数据，collect到Driver中来。List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。// 可以尽可能节省内存空间，并且减少网络传输性能开销。final Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data); // 对另外一个RDD执行map类操作，而不再是join类操作。JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; // 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。 List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value(); // 可以将rdd1的数据转换为一个Map，便于后面进行join操作。 Map&lt;Long, Row&gt; rdd1DataMap = new HashMap&lt;Long, Row&gt;(); for(Tuple2&lt;Long, Row&gt; data : rdd1Data) &#123; rdd1DataMap.put(data._1, data._2); &#125; // 获取当前RDD数据的key以及value。 String key = tuple._1; String value = tuple._2; // 从rdd1数据Map中，根据key获取到可以join到的数据。 Row rdd1Value = rdd1DataMap.get(key); return new Tuple2&lt;String, String&gt;(key, new Tuple2&lt;String, Row&gt;(value, rdd1Value)); &#125; &#125;); // 这里得提示一下。// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。// rdd2中每条数据都可能会返回多条join后的数据。 解决方案六：采样倾斜key并分拆join操作方案适用场景：两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。 方案实现思路： 对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。 然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。 接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。 再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。 而另外两个普通的RDD就照常join即可。 最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。 方案实现原理：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。 方案优点：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。 方案缺点：如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。JavaPairRDD&lt;Long, String&gt; sampledRDD = rdd1.sample(false, 0.1); // 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。JavaPairRDD&lt;Long, Long&gt; mappedSampledRDD = sampledRDD.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return new Tuple2&lt;Long, Long&gt;(tuple._1, 1L); &#125; &#125;);JavaPairRDD&lt;Long, Long&gt; countedSampledRDD = mappedSampledRDD.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;);JavaPairRDD&lt;Long, Long&gt; reversedSampledRDD = countedSampledRDD.mapToPair( new PairFunction&lt;Tuple2&lt;Long,Long&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple) throws Exception &#123; return new Tuple2&lt;Long, Long&gt;(tuple._2, tuple._1); &#125; &#125;);final Long skewedUserid = reversedSampledRDD.sortByKey(false).take(1).get(0)._2; // 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。JavaPairRDD&lt;Long, String&gt; skewedRDD = rdd1.filter( new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return tuple._1.equals(skewedUserid); &#125; &#125;);// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。JavaPairRDD&lt;Long, String&gt; commonRDD = rdd1.filter( new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return !tuple._1.equals(skewedUserid); &#125; &#125;); // rdd2，就是那个所有key的分布相对较为均匀的rdd。// 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。// 对扩容的每条数据，都打上0～100的前缀。JavaPairRDD&lt;String, Row&gt; skewedRdd2 = rdd2.filter( new Function&lt;Tuple2&lt;Long,Row&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; return tuple._1.equals(skewedUserid); &#125; &#125;).flatMapToPair(new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call( Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; Random random = new Random(); List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;(); for(int i = 0; i &lt; 100; i++) &#123; list.add(new Tuple2&lt;String, Row&gt;(i + &quot;_&quot; + tuple._1, tuple._2)); &#125; return list; &#125; &#125;); // 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD1 = skewedRDD.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(100); return new Tuple2&lt;String, String&gt;(prefix + &quot;_&quot; + tuple._1, tuple._2); &#125; &#125;) .join(skewedUserid2infoRDD) .mapToPair(new PairFunction&lt;Tuple2&lt;String,Tuple2&lt;String,Row&gt;&gt;, Long, Tuple2&lt;String, Row&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt; call( Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; tuple) throws Exception &#123; long key = Long.valueOf(tuple._1.split(&quot;_&quot;)[1]); return new Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt;(key, tuple._2); &#125; &#125;); // 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD2 = commonRDD.join(rdd2); // 将倾斜key join后的结果与普通key join后的结果，uinon起来。// 就是最终的join结果。JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2); 解决方案七：使用随机前缀和扩容RDD进行join方案适用场景：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。 方案实现思路： 1、该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。2、然后将该RDD的每条数据都打上一个n以内的随机前缀。3、同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。4、最后将两个处理后的RDD进行join即可。 方案实现原理：将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。 方案优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。 方案缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。 方案实践经验：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。 123456789101112131415161718192021222324252627282930// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。JavaPairRDD&lt;String, Row&gt; expandedRDD = rdd1.flatMapToPair( new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;(); for(int i = 0; i &lt; 100; i++) &#123; list.add(new Tuple2&lt;String, Row&gt;(0 + &quot;_&quot; + tuple._1, tuple._2)); &#125; return list; &#125; &#125;); // 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。JavaPairRDD&lt;String, String&gt; mappedRDD = rdd2.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(100); return new Tuple2&lt;String, String&gt;(prefix + &quot;_&quot; + tuple._1, tuple._2); &#125; &#125;); // 将两个处理后的RDD进行join即可。JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD); 解决方案八：多种方案组合使用在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，预处理一部分数据，并过滤一部分数据来缓解；其次可以对某些shuffle操作提升并行度，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。 转载自：【美团技术博客】]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark性能</tag>
        <tag>数据倾斜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[林中路（10）：略萨 | 绦虫寓言]]></title>
    <url>%2F2019%2F09%2F10%2F%E6%9E%97%E4%B8%AD%E8%B7%AF%EF%BC%8810%EF%BC%89%EF%BC%9A%E7%BB%A6%E8%99%AB%E5%AF%93%E8%A8%80%2F</url>
    <content type="text"><![CDATA[亲爱的朋友： 您的信让我激动，因为借助这封信，我又看到了自己十四五岁时的身影，那是在奥德亚将军独裁统治下的灰色的利马，我时而因为怀抱着总有一天要当上作家的梦想而兴奋，时而因为不知道如何迈步、如何开始把我感到的抱负付诸实施而苦闷；我感到我的抱负仿佛一道紧急命令：写出让读者眼花缭乱的故事来，如同那几位让我感到眼花缭乱的作家的作品一样，那几位我刚刚供奉在自己设置的私人神龛里的作家：福克纳、海明威、马尔罗、多斯·帕索斯、加缪、萨特。 我脑海里曾经多次闪过给他们中间某一位写信的念头（那时他们还都健在），想请他们指点我如何当上作家。可是我从来没有敢动笔，可能处于胆怯，或者可能出于压抑的悲观情绪——既然我知道他们谁也不肯屈尊回信，那为什么还要去信呢？类似我这样的情绪常常会白白浪费许多青年的抱负，因为他们生活在这样的国家里，对于大多数人来说，文学算不上什么大事，文学在社会生活的边缘处苟延残喘，仿佛地下活动似的。 既然给我写了信，那您就没有体验过这样的压抑情绪，这对于您愿意踏上的冒险之路以及您为此而期盼的许多奇迹，是个良好的开端——尽管您在信中没有提及，但我可以肯定您是寄希望于奇迹的。请允许我斗胆提醒您：对此，不要有过高期望，也不要对成就抱有过多幻想。当然，没有任何理由说您不会取得成就。但是，假若您坚持不懈地写作和发表作品，您将很快发现，作家能获奖、得到公众认可、作品畅销、拥有极高知名度，都有极其独特的走向，因为有事这些名和利会顽固地躲避在那些最应该受之无愧的人，而偏偏纠缠和降临到受之有愧的人身上。这样一来，只要把名利看作对自己抱负的根本性激励，那就有可能看到梦想的破灭，因为他可能混淆了文学抱负和极少数作家获得的华而不实的荣誉与利益。献身文学的抱负和求取名利是不相同的。 文学抱负的基本属性是，有抱负的人如果能够实现自己的抱负，那就是对这一抱负的最高奖励，这样的奖励要超过、远远地超过它作为创作成果所获得的一切名利。关于文学抱负，我有许多不敢肯定的看法，但我敢肯定的观点之一是：作家从内心深处感到写作是他经历和可能经历的最美好事情，因为对作家来说，写作意味着最好的生活方式，作家并不十分在意其作品可能产生的社会、政治和经济后果。 谈及怎样成为作家这个振奋又苦恼的话题，我觉得文学抱负是必要的起点。当然，这是个神秘的题目，它被裹在不确定性和主观性之中。但是，这并不构成用一种理性的方式加以说明的障碍。只要避免虚荣心，只要不带迷信和狂妄的深化色彩就可以进行。浪漫派一度怀抱这样的神话：把作家变成众神的选民，即被一种超自然的先验力量指定的人，以便写出神的话语，而只有借助神气，人类将神才可能得到升华，再经过大写的”美“的感染，人类才有可能得到永生。 今天，再也不会有人这样谈论文学或者艺术抱负了。但是，尽管现在的说法不那么神圣或者辉煌，抱负依然是个相当难以确定的话题，依然是个起因不详的因素；抱负推动一些男女把毕生的精力投入一种活动：一天，突然感到自己被召唤，身不由己地去从事这种活动——比如写故事，根据自身条件，使出浑身解数，终于觉得实现了自我的价值，而丝毫不认为是在浪费生命。 我不相信早在妊娠期上帝就为人的诞生预定了一种命运，我不相信什么偶然性或者乖戾的神意给在母腹中的胎儿身上分配了抱负或者无能、欲望或者无欲。但是，今天我也不相信青年时有一个阶段在法国存在主义唯意志论的影响下——尤其是萨特的影响——曾经相信的东西：抱负是一种选择，是用什么来决定人未来的个人意志的自由运动。虽然我认为文学抱负不是镌刻在未来作家身上基因的预示性东西，虽然我坚信教育和持之以恒的努力可能在某些情况下造就天才，但我最终确信的还是，文学抱负不能仅仅解释为自由选择。我认为，这样的选择是必要的，但那是只有到第二个阶段才发生的事情，而从第一个阶段开始，即从少儿时期起，首先需要主观的安排和培养；后来的理性选择是来加强少儿期的教育，而不是从头到脚制造出一个作家。 如果我的怀疑没错的话（当然，很有可能不对），一个男孩或者女孩过早地在童年或者少年时期展示了一种倾向：能够想象出与生活不同的天地里的人物、情节、故事和世界，这种倾向就是后来可能称之为文学抱负的起点。当然，从这样一个喜欢展开想象的翅膀远离现实世界、远离真实世界的倾向，到开始文学生涯，这中间还有个大多数人不能跨越的深渊。能够跨越这个深渊、通过语言文字来创造世界的人们，即成为作家的人，总是少数，他们把萨特说的一种选择的意志运动补充到那种倾向里去了。时机一旦可能，他们就决定当作家。于是，就这样做了自我选择。他们为了把自己的抱负转移到书面话语上而安排了自己的生活，而从前这种抱负仅限于在无法触摸的内心深处虚构别样的生活和世界。这就是您现在体验到的时刻：困难而又激动的处境，因为您必须决定除去凭借想象虚构现实之外，是否还要把这样的虚构化作具体的文字。如果您已经决定这样做，那等于还要把这样的虚构化作具体的文字。如果您已经决定这样做，那等于您已经卖出了极其重要的一步，当然，这丝毫不能保证您将来一定能当上作家。但是，只要您坚持下去，只要您按照这个计划安排自己的生活，那就是一种（唯一的）开始成为作家的方式。 这个会编造人物和故事的早熟才能，即作家抱负的起点，它的起源是什么呢？我想答案是：反抗精神。我坚信：凡是刻苦创作与现实生活不同的生活的人们，就用这种间接的方式表示对这一现实生活的拒绝和批评，表示用这样的拒绝和批评以及自己的想象和希望制造出来的世界替代现实世界的愿望。那些对现状和目前生活心满意足的人们，干嘛要把自己的时间和精力投入创作虚拟的现实这样虚无缥缈、不切实际的事情中去呢？然后，使用简单工具写作创作别样生活和别样人群的人们，有可能是在种种理由的推动下进行的。这些理由或者是利他主义的，或者是不高尚的，或者是卑劣吝啬的，或者是复杂的，或者是简单的。无论对生活现实提出何种质问，都是无关紧要的，依我之见，这样的质问是跳动在每个写匠心中的。重要的是对现实生活的拒绝和批评应该坚决、彻底和深入，永远保持这样的行动热情——如同堂吉诃德那样挺起长矛冲向风车，即用敏锐和短暂的虚构天地通过幻想的方式来代替这个经过生活体验的具体和客观的世界。 但是，尽管这样的行动是幻想性质的，是通过主观、想象、非历史的方式进行的，可是最终会在现实世界，即有血有肉的人们的生活里，产生长期的精神效果。 关于现实生活的这种怀疑态度，即文学存在的秘密理由——也是文学抱负存在的理由，决定了文学能够给我们提供关于特定时代的唯一的证据。虚构小说描写的生活——尤其是成功之作——绝对不是编造、写作、阅读和欣赏这些作品的人们实实在在的生活，而是虚构的生活，是不得不人为创造的生活，因为在现实中他们不可能过这种虚构的生活，因此就心甘情愿地仅仅以这种间接和主观的方式来体验它，来体验那另类生活：梦想和虚构的生活。虚构是掩盖深刻真理的谎言，虚构是不曾有过的生活，是一个特定时代的人们渴望享有、但不曾享有，因此不得不编造的生活。虚构不是历史的画像，确切的说，是历史的反面，或者说历史的背面；虚构是实际上没有发生的事情，因此，这样的事情才必须由想象和话语来创造，以便安抚实际生活难以满足的雄心，以便填补人们发现自己周围并用幻想充斥其间的空白。 当然，反抗精神是相对的。许多写匠根本就没有意识到这一精神的存在，或许还有可能他们弄明白了自己想象才能的颠覆性质之后，会吃惊和害怕，因为他们在公公开场合绝对不认为自己是用炸弹破坏这个世界的秘密恐怖分子。另一方面说到底，这是一种相当和平的反抗，因为用虚构小说中那触摸不到的生活来反抗实在的生活，又能造成什么伤害呢？对于实在的生活，这些竞争又能意味着什么危险呢？粗略得看是没有的，这是一种游戏。不是吗？各种游戏只要补齐图越过自己的空间、不牵连到实在的生活，通常是没有危险的，好了，如果现在有人——比如，堂吉诃德或者包法利夫人——坚持要把虚构小说与生活混淆起来，非要生活得像想说你那个模样不可，其结果常常是悲惨的，凡是要这么行动的人，那往往要以可怕的失望作代价。 但是文学这个游戏也并非无害，由于虚构小说是内心对生活不满的结果，因此也就成为抱怨和宣泄不满的根源。因为，凡通过阅读体验到伟大小说中的生活，比如上面刚刚提到的塞万提斯和福楼拜的作品的人，回到现实生活时，面对生活的局限和种种毛病，其感觉会格外敏感，因为他通过作品中的美妙想象已经明白：现实世界——这实在的生活——比起小说家编造的生活不知要庸俗多少。优秀文学鼓励的这种对现实世界的焦虑，在特定的环境里也可能转化为面向政权、制度或者既定信仰的反抗精神。 因此在历史上，西班牙宗教裁判所是不信任虚构小说的，并实行严格的书刊审查，甚至在长达三百年的时间里禁止整个美洲殖民地出售小说。其借口是那些胡说八道的故事会分散印第安人对上帝的信仰，对于一个以神权统治的社会来说，这是惟一重要的心事。与宗教裁判所一样，任何企图控制公民生活的政府和政权，都对小说表示了同样的不信任，都对小说采取监视的态度，都使用了限制手段：书刊审查。前者和后者都没有搞错：透过那无害的表面，编造小说是一种享受自由和对那些企图取消小说的人一一无论教会还是政府的反抗方式。这正是一切独裁政权、法西斯、伊斯兰传统派政权、非洲和拉丁美洲军事专制政权企图以书刊审查方式强制文学穿上拘束服(限定在某种范围内)以控制文学。 可是，这样泛泛的思考让我们有些脱离了您的具体情况，我们还是回到具体问题上来吧。您在内心深处已经感觉到了这一文学倾向的存在，并且已经把献身文学置于高于一切的坚定不移的行动之中了。那现在呢？您把文学爱好当作前途的决定，有可能会变成奴役，不折不扣的奴隶制。为了用一种形象的方式说明这一点，我要告诉您，您的这一决定显然与十九世纪某些贵夫人的做法如出一辙：她们因为害怕腰身变粗，为了恢复美女一样的身材就吞吃一条绦虫。您曾经看到过什么人肠胃里养着这种寄生虫吗？我是看到过的。我敢肯定地对您说：这些夫人都是了不起的女杰，是为美丽而牺牲的烈士。六十年代初，在巴黎，我有一位好朋友，他名叫何塞·马利亚，一个西班牙青年，画家和电影工作者，他就患上了这种病。绦虫一旦钻进他身体的某个器官，就安家落户了：吸收他的营养，同他一道成长，用他的血肉壮大自己，很难、很难把这条绦虫驱逐出境，因为它已经牢牢地建立了殖民地。何塞·马利亚日渐消瘦，尽管他为了这个扎根于他肠胃的小虫子不得不整天吃喝不停(尤其要喝牛奶)，因为不这样的话，它就烦得你无法忍受。可何塞吃喝下去的都不是为了满足他自己的快感和食欲，而是让那条绦虫高兴。有一天，我们正在蒙巴拿斯的一家小酒吧里聊天，他说出一席坦率的话让我吃了一惊：“咱们一道做了许多事情。看电影，看展览，逛书店，几个小时、几个小时地谈论政治、图书、影片和共同朋友的情况。你以为我做这些事情的时候是和你一样的吗？因为做这些事情会让你快活，那你可就错了。我做这些事情是为了它，为这条绦虫。我现在的感觉就是：现在我生活中的一切，都不是为我自己，而是为着我肠胃里的这个生物，我只不过是它的糊、甜蜜和忘却的梦想。这蠕虫在这之前就钻进我的心中，它蜷曲在那里，用我的大脑、精神和记忆做食粮。我知道，自己已经被心中的火焰抓住，已经被自己点燃的火吞食，已经被多年来耗费我生命的愤怒与无法满足的欲望铁爪撕得粉碎。一句话，我知道，脑海里或者心中或是记忆中，一个发光的细胞将永远闪耀，日日夜夜地闪耀，闪耀在我生命的每时每刻，无论是清醒还是在梦中。我知道那蠕虫会得到营养，永远光芒四射，我知道无论什么消遣，什么吃喝玩乐，都不能熄灭这个发光的细胞。我知道即使死亡用它那无限的黑暗夺去了我的生命，我也不能摆脱这条蠕虫。”我知道终于我还是变成了作家，我也终于知道了一个人如果要过作家的生活，他会发生什么事情。我想，只有那种献身文学如同献身宗教一样的人，当他准备把时间、精力、勤奋全部投入文学抱负中去，那时他才有条件真正地成为作家，才有可能写出领悟文学为何物的作品。而另外那个神秘的东西，我们称之为才能、天才的东西，不是以早熟和突发的方式诞生的，至少在小说家中不是，虽然有时在诗人或者音乐家中有这种情况，经典性的例子可以举出兰波和莫扎特，而是要通过漫长的程序、多年的训练和坚持不懈的努力才有可能使之出现。没有早熟的小说家。任何大作家、任何令人钦佩的小说家，一开始都是练笔的学徒，他们的才能是在恒心加信心的基础上逐渐孕育出来的。那些逐渐培养自己才能的作家的榜样力量，是非常鼓舞人的，对吗？他们的情况当然与兰波不同，后者在少年时期就已经是个天才诗人了。假如对这个孕育文学天才的话题感兴趣，那么我建议您读读福楼拜的书信集，尤其是一八五0至一八五四年间他在创作第一部杰作《包法利夫人》时写给情人路易莎·科勒的那些信。我在写自己最初的那几部作品时，阅读这些书信让我受益匪浅。尽管福楼拜是悲观主义者，他的书信中充满了对人性的辱骂，但他对文学却有着无限的热爱。因为他把自己的抱负表现为参加远征，怀着狂热的信念日日夜夜投身其中，对自己苛求到难以形容的程度。结果，他终于冲破自身的局限性(在他早期的文字中，由于受流行的浪漫主义模式的影响而咬文嚼字、亦步亦趋，这十分明显)并且写出了像《包法利夫人》和《情感教育》这样的长篇小说，可以说这是最早的两部现代小说。另一部与这封信的话题有关的作品，我冒昧地推荐给您，就是美国一位非常特别的作家威廉·巴勒斯写的《吸毒者》。巴勒斯作为小说家，我丝毫不感兴趣。他那些实验性、心理迷恋性的故事，总是让我特别厌烦，甚至让我觉得不能卒读。但是，他写的第一部作品《吸毒者》是有事实根据的，有自传性质，那里面讲述了他如何变成吸毒者、如何在吸毒成瘾后自由选择的结果，毫无疑问是某种爱好所致，变成了一个幸福的奴隶、快乐的瘾君子。我认为描写得准确无误，是他文学抱负发挥的结果，也写出了这一抱负在作家和作家任务之间的从属关系以及作家在写作中吸收营养的方式。 但是，我的朋友，对于书信体文字来说，我这封信已经超过了合适的长度，而书信体文字的主要优点恰恰应该是短小，因此我说声：再见吧。 拥抱您。]]></content>
      <categories>
        <category>林中路</category>
      </categories>
      <tags>
        <tag>略萨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算广告面面观（4）：计算广告基础知识]]></title>
    <url>%2F2019%2F08%2F23%2FAD-RS-4%2F</url>
    <content type="text"><![CDATA[在互联网商业模式中，如果把商业化产品比作是船体，其核心作用是作为流量与变现的载体，那么数据就是引擎，没有数据的指引，商业模式就会很低效，机器学习作为数据提高效率的机器，则充当了这艘船的船长的地位。 无论是用户产品还是商业产品，不深入了解在线广告，就不太可能全面地了解互联网业务。以广告为载体的商业化体系支撑了整个互联网行业的大半壁江山。从数据的角度看，在线广告开启了大规模、自动化利用数据改善产品和提高收入的先河，而大数据（big data）这一方法论在实践中唯一形成规模化营收的落地行业就是在线广告。广告不完全等同于搜索或推荐，广告首先是一项商业活动，然后才是一项在互联网环境下需要技术优化的商业活动；其次，在这一商业活动中，广告主、媒体和用户利益都需要被认真考虑很满足，这样才能达到整个市场的平衡和不断发展。 计算广告作为唯一得到充分商业化和规模化的大数据应用，有以下几点显著特征： 1）计算广告位规模化地讲用户行为数据转化为可衡量的商业价值提供了完整产品线和解决方案，并且实际上创造了互联网行业大部分的营收。下表为2015年互联网巨头广告收入占比。2）在线广告孕育和孵化了较为成熟的数据加工和交易产业链，并对其中的用户隐私边界有深入探讨，这值得所有涉及用户数据的互联网应用学习和借鉴。 3）由于有了商业上的限制条件，计算广告的技术和产品逻辑比单纯的个性化系统更加复杂周密。因此，理解在线广告的产品和市场对于设计正确有效的商业产品大有益处。 广告的定义与目的广告是三方利益博弈的活动，这三方分别是【广告主】、【媒体】和【用户】，这是广告活动永恒的主线。此外，广告是有偿的、非人员的信息传播活动。 广告的根本目的是【广告主通过媒体达到低成本的用户接触】，也就是按某种市场意图接触相应的人群，进而影响其中的潜在用户，使他们选择广告主产品的几率增加，或者对产品性价比的苛求程度降低。根据目标的差异，可分为品牌广告和效果广告。 【品牌广告】：希望借助媒体的力量来快速接触大量用户，以达到宣传品牌形象、提升中长期购买率与利润空间的目的。 【效果广告】：希望能马上带来大量的购买或其他转化行为的广告。 对于互联网广告，广告的本质没有发生变化，仍然是【付费的信息推广】，只是其表现形式越来越丰富和灵活了，与线下类似的条幅、搜索竞价排名、软文甚至表面上与广告并不相干的游戏联运。 【互联网广告中，一切付费的信息、产品或服务的传播渠道，都是广告。】 总结： 传统的视角: 广告的根本目的是广告主通过媒体达到低成本的用户接触 广告的投入产出比(Returm on Investment, ROD)相比于销售人里乘员的劝服活动应该较高 互联网新视角: 一切付费的信息、产品或服务的传播渠道，都是广告 直接效果广告的ROI应该可衡量、可优化 互联网广告的形态【横幅广告】：嵌入在页面中相对固定的位置，占据固定版面，有静态和动态的形式。【文字链广告】：搜索广告的主流，其形式为一段链接到广告主落地页的文字。【富媒体广告】：利用视觉冲击较强的表现形式，在不占用固定版面位置的情况下，向用户侵入式地投送广告素材。如弹窗、对联、全屏等。【视频广告】：在视频流播放的间隙插入广告，分为前插片、后插片、暂停等类型，前插片广告一般采用短视频形式，创意冲击力和表现力远远强于普通的展示广告。【社交广告】：在社交网络环境下嵌入的广告，最典型的形式是插入在社交网络信息流中，力求在用户自然关注的交互过程中尽可能地插入广告，也被归于原生广告的范畴。社交广告希望达到的效果是通过用户的扩散式传播获得更大的影响力和口碑。【移动广告】：移动互联网大有取代桌面互联网之势，移动广告与桌面广告没有大的区别，典型的形式有横幅、开屏、插屏、积分墙或推荐墙等。【邮件定向营销广告】:E-mail Direct Marketing，EDM，通过电子邮件向目标用户传递推广信息，是一种主动的广告形式，无需等到用户接触的机会出现时才被动地提供广告，容易被处理成垃圾邮件。对运营者而言，精准把握用户兴趣，有节制地提供对用户有用的价值的相关信息很关键。 泛广告商业产品 团购：团购本质上是一种按照效果付费的泛广告产品，广告主除了付推广费用外，还向用户让利以获得转化。 游戏联运：根据用户的最终游戏内消费在推广渠道和游戏开发商之间分成的商业产品，是一种按效果付费的泛广告产品 固定位导航：网站导航站的位置入口、应用分发平台的推荐位置等付费推广位置，按时间付费，广告主除了引流以外，往往更关注入口位置的橱窗效应。 返利购买：电商行业常见的推广模式，采用积分或折扣激励用户购买 注意：当一个公司同时运营普通广告和上述泛广告商品时，它们之间甚至是和用户产品之间常会出现争夺广告位或其他入口资源的问题，这时候最合理的分配方式是通过他们之间的竞价来决策。 在线广告进化路径【固定位置合约交易阶段】 在互联网上展示广告创意的产品形式，这一阶段的展示广告售卖模式称为合约广告，即采用合同约定的方式确定某一广告位在某一时间段为特定广告主所独占，并且根据双方的要求，确定广告创意和投放策略。没有计算的需求，唯一需要的是把广告主的创意作为一个HTML片段插入到媒体页面中。 【受众定向、按展示量结算的合约交易阶段】 在线广告不同于传统媒体广告的本质特点是可以对不同的受众呈现不同的广告创意。这样的广告系统对计算技术产生了两个具体需求： 受众定向：通过技术手段标定某个用户的性别、年龄或其他标签； 广告投放：即将广告投放由直接嵌入页面变为实时响应前端请求，并根据用户标签自动决策和返回合适的广告创意。 此时的定向广告仍以合约的方式进行。媒体向广告主保证某个投放量，并在此基础上确定合同的总金额以及投放量未完成情况下的赔偿方案。这叫做担保式投放（Guaranteed Delivery）,GD逐渐成为互联网合约式广告的主要商业模式，主要面向品牌广告主，遵循按千次展示付费（Cost per Mille, CPM）的计费方式。 合约广告系统要求在满足各合约受众量要求的同时尽可能为所有广告商分配到更好的流量，也即【在线分配 online allocation】问题，这其中有两个难点：一是如何有效地将流量分配到各个合约互相交叉的人群覆盖上；二是要在在线的环境下实时完成每一次展示决策。这可利用带约束（各合约的量）优化的数学框架来探索。 【封闭竞价交易模式】竞价广告产生的最初场景是在搜索广告，是天然的定向广告系统，即根据用户的即时兴趣定向投送的广告，而即时兴趣的标签就是关键词。这种定向从一开始就直接达到了非常精准的程度，很自然地采用了竞价的方式售卖。 如果将用户的即时兴趣标签由搜索词换成正在浏览页面中的关键词，可将这套竞价广告系统从搜索结果页照搬到媒体页面上，于是产生了【上下文广告（contextual advertising）】。 基于竞价机制和精准人群定向这两个核心功能，在线广告分化出【广告网络 ad NetWork ADN】它批量运营媒体的广告位资源，按人群或上下文标签售卖给需求方，并用竞价的方式决定流量分配。ADN使得大量中小互联网媒体有了切实可行的变现手段。 【开放实时竞价交易模式】将拍卖的过程由广告主预先出价，变成每次展示时实时出价。只要把广告展示的上下文页面URL以及访客的用户标识等信息传给需求方，让需求方按自己的人群定义来挑选流量，这就是实时竞价，于是市场上出现了大量聚合各媒体的剩余流量并采用实时竞价方式为他们变现的产品形态 【广告交易平台（ad Exchange,ADX）】 通过实时竞价的方式，按照定制化的人群标签购买广告，这样的产品叫做【需求方平台（Demand Side Platform,DSP）】，这样的广告采买方式也被叫做【程序化交易】 广告有效性原理一则广告的信息接收过程分为【选择】=&gt;【解释】=&gt;【态度】，进一步分解为【曝光】=&gt;【关注】=&gt;【理解】=&gt;【接受】=&gt;【保持】=&gt;【决策】六个子过程。 1.【曝光】：广告物理上展现出来的过程，没有太多可用技术优化的空间，在实践中曝光的有效性对最终结果的影响往往高于其他技术性因素，即”位置为王“。互联网广告中，位置的影响更显著，因此，从算法上消除由此带来的点击率预估偏差就很重要。 【关注】：受众从物理上接触到广告到意识上注意到它的过程。曝光并不意味着用户实际有效的关注。如何提高关注阶段效率： 尽量不要打断用户的任务，这是上下文广告投送的原理基础，也是当今讨论原生广告产品的出发点之一。当用户明确辨识出某个固定不变的广告位，且不再认为它与自己当前浏览网页的任务有关联时，他会下意识屏蔽其中的内容。 明确传达向用户推送此广告的原因，这是受众定向创意广告优化的重要方向。 内容符合用户的兴趣或需求，是行为定向的原理基础。 【理解】：用户关注到了广告的内容并不意味着他一定能够理解广告传达的信息。理解阶段的原则： 广告内容要在用户能理解的具体兴趣范围内，这就要求精准的受众定向 注意设定与关注程度相匹配的理解门槛，对于互联网广告，由于用户的关注程度很低，应该强调一个主要诉求以吸引用户注意力。 【接受】：理解并不代表认可，让合适的广告出现在合适的媒体上，此即广告安全问题 【保持】：对于不仅仅追求短期转化的广告商希望广告传达的信息给用户留下长久的记忆。 【决策】：成功广告的最终作用是带来用户的转化行为，虽然已经离开了广告的业务范围，但好的广告能够为转化率的提高做好铺垫。 越靠前的阶段，其效果的改善对点击率的贡献越大；越靠后的阶段，其效果的改善对转化率的贡献越大。 计算广告的核心问题eCPM在一次广告展示产生后，后续会有【点击】【打开落地页】【转化】这几步操作。分别对应几个重要指标： 【点击率】：广告点击与广告展现的比率【到达率】：落地页成功打开次数与点击次数的比例【转化率】：转化次数与到达次数的比例 按照媒体网站和广告主网站上的行为段对回报r进行分解： $eCPM = r(a,u,c) = \mu(a,u,c)· v(a,u,c )$ 其中 $\mu$为点击率，描述的是发生在媒体上的行为 $v$为点击价值（click value），是发生在广告主网站上的行为 两部分乘积定量表示某次或若干次展示的期望CPM值，也即eCPM，是最关键的定量评估收益指标。 结算方式根据eCPM的分解决定哪部分是由谁来估计是广告市场各种击飞模式产生的根本原因，也是广告市场中商业逻辑与产品架构衔接的关键一环。 CPM模式：按千次展示结算，一般由需求方根据其市场策略与预算控制流量的单价并按CPM方式结算是比较合理的交易模式。在大部分互联网品牌广告中，特别是视频广告中，CPM是主流的结算方式。 CPC结算：按点击结算，点击率由供给方完成，供给方通过收集大量用户行为数据可相对准确估计点击率；点击价值由需求方估计，因为转化效果是广告站内的行为，他们自己的数据分析体系也就能更准确地对其作出评估。CPC结算方式在效果广告具有垄断地位。 CPS（sale）/CPA（action）/ROI结算：按照销售订单数、转化行为数或投入产出比来结算，都是按转化付费的变种，需求方只按照最后的转化收益来结算，供给方除了估计点击率还要对点击价值作出估计，这样才能合理地决定流量分配。但这样有两个问题，一是转化行为发生在广告商站内，并非供给方能直接监测和控制，因此无法进行准确的估计和优化；二是实际执行中，存在广告主故意扣单以降低转化率，以低成本赚取大量品牌曝光。对于和广告主利益直接挂钩的需求方广告产品来讲，CPS在一定条件下是可行的；但对于普通的中间市场广告产品来说，CPS并不是一种趋势性的结算方式。在移动应用下载的场景下，由于转化流程统一在Apple Store或Google Play中，且存在较完善的第三方转化监测，因而市场较成熟。 CPT（time）结算：针对大品牌广告主特定的广告活动，将某个广告位以独占方式交给某广告主，并按独占的时间段收取费用的方式。不利于受众定向和程序交易，长期来看会有下降的趋势。 结算方式 点击率预估 点击价值预估 优缺点 适用场景 CPT 需求方 需求方 可以充分发挥橱窗效应，无法利用受众定向技术 高曝光的品牌广告 CPM 需求方 需求方 可以利用受众定向选择选择目标人群，合约售卖下，受众定向不能过细 有受众选择需求的品牌广告、实时竞价广告交易 CPC 供给方 需求方 可以非常精细地划分受众人群，比较合理的供给方和需求方分工 竞价广告网络 CPS/CPA/ROI 供给方 供给方 需求方无任何风险，供给方运营难度较大 效果类广告联盟、效果类DSP 参考资料：《计算广告》]]></content>
      <categories>
        <category>计算广告</category>
      </categories>
      <tags>
        <tag>商业化</tag>
        <tag>广告形态</tag>
        <tag>泛广告商业</tag>
        <tag>广告交易模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算广告面面观（3）：竞价广告定价策略与实现]]></title>
    <url>%2F2019%2F07%2F12%2FAD-RS-3%2F</url>
    <content type="text"><![CDATA[在竞价广告中，广告主能否获得某个广告位是由其变现能力也就是eCPM的大小来决定的。在以CPC结算即点击结算的单子中，eCPM即千次展示期望收入的计算公式为:eCPM = \mu *v其中： $\mu$ 表示点击率（click rate），是由供给方在媒体侧通过收集广告数据（a）、用户数据（u）、上下文数据（c），使用模型预估用户在当前环境下，对当前广告的点击概率 $v$ 表示点击价值（click value），是由需求方根据用户在广告主自身站内的行为数据评估出来的一个出价 但需求方往往不会完全按照其估计的真实点击价值来定价，而往往会以较低的价格寻求套利空间，为了避免这种套利行为，就有了竞价广告的定价策略，通过机制设计来抑制广告主积极调价。这也是本文所关注的。 假如我们有一组广告，记为集合A，另有一组广告位，记为集合S，那么一次广告竞价过程可以描述为：集合A中的广告通过出价来拍卖集合B中的广告位置，拍卖的依据是广告的eCPM。其中出价是很重要的一环，所以其机制设计就尤为重要，要做到竞价市场收益良好，稳定且公平。 从拍卖理论说起广告位置竞价源自于拍卖理论，看具体的拍卖方式之前先说一个概念，帕累托有效，即拍卖品最终归于支付意愿最高的竞买人之手，支付意愿并不由出价呈现，而是一种买家的心理状态。拍卖方式主要有四种，英国式拍卖、荷兰式拍卖、第一价格密封拍卖、第二价格密封拍卖，如下： 英国式拍卖： 也叫增价拍卖，指拍卖竞价由低向高依次递增，直到以最高价击槌成交的一种拍卖，拍卖人设定一个保留价格，如果最终的出价低于保留价，则流拍。为了保证竞价收敛，一般会为竞价设定一个终止时间。 英式拍卖最古老，也是大家接触较多的拍卖方式。在这种拍卖方式中，竞买人要冒一定的风险，他可能会被令人兴奋的竞价过程吸引，出价超出了预估价，这种心理现象称为“赢者诅咒”。 荷兰式拍卖： 与英国式拍卖相反，也称减价拍卖，它是指拍卖竞价由高到低依次递减，直到第一个竞买人应价时击槌成交的一种拍卖。它起源于荷兰的鲜花交易市场，拍卖周期很短，所以对于短期易腐食品，如果蔬、鱼类、鲜花较为适合。 它克服了英式拍卖中“知道其他买家出价”的缺点，是一种密封式的拍卖方法。竞买人往往坐等观望，企盼价格不断减低，因而现场竞争气氛不够热烈。所以有“无声拍卖”的名声。 第一价格密封拍卖： 每一个买方写下自己的期望价格，买方之间互不知道对方的出价（故称密封拍卖），最后在同一个时间揭晓每个买家的出价，出价最高者获得标的物。 这种拍卖方式每个买家只出一次价格，一般来讲，买方会出一个低于心理预期的价格，只有在买方足够多的情况下，最终的价格才会更接近物品的真实价值；且由于存在一些暗箱操作，无法真的保证密封性。 第二价格密封拍卖： 也叫维克里拍卖（Vickrey Auction），拍卖过程和第一价格密封拍卖一致，由出价最高的买家获得物品，但他只需要支付所有投标者中的第二高价。在维克里拍卖中，每个买家最优出价策略就是“自己对这个物品价格的估值”，即“诚实才是最优的竞价策略”，因为这种策略的最后的成交价格和买家的出价是相对独立的，中标者对价格没有影响，出标的高低只决定最后是否获得标的，但不决定最终的成交价格。比如竞标一幅画，最终中标人出价是2万元，未中标人中最高价格是1.8万元，所以中标人只需要出1.8万元。 在这种策略下，买家可以将所有精力放到对物品价值的评估上，而不需要研究其他竞争对手，不需要研究投标策略，不需要进行市场评估。理论上讲，第二价格密封拍卖是一种有效的拍卖机制。但实际中，买家之间合谋，买家与卖家之间合谋的情况经常存在；同时，作为非理性人，其他买家的出价在一定程度上也会影响自己的出价。 维克里拍卖是诺贝尔经济学奖获得者维克里的名字命名的。他在《反投机、拍卖与竞争性密封投标》文中对拍卖理论做了非常开创性的工作，首次运用博弈论来分析拍卖问题, 极富预见性地提出了拍卖理论中的多数关键问题, 从而引导了该理论的基本研究方向。这也是他在1996年获得诺贝尔经济学家奖的重要因素。他在论文中主要证明了以下几个结论： 英式拍卖中的每个竞买人的占优战略都是保持竞价, 直到价格达到自己的估价为止, 估价最高的竞买人将以大致等于次高估价的价格夺走拍卖品, 这种配置结果显然是帕累托有效的。 在竞买人对称的荷式拍卖中, 每个竞买人的报价应该严格低于自己的估价, 估价最高的竞买人也必定成为赢家, 因而也是帕累托有效的。但是, 如果竞买人非对称, 荷式拍卖的配置结果很可能是无效率的。 荷式拍卖与第一价格密封拍卖在战略上是完全等价的, 因为竞买人在两种情形中所面临的局势完全相同。在此基础上,维克里独创性地提出了英式拍卖的密封等价形式第二价格密封拍卖，即维克里拍卖。这种拍卖最显著的特征是每个竞买人的的占优战略都是按其真实支付意愿出价（说真话）, 这种拍卖机制显然是激励相容的。由于拍卖品最终归于支付意愿最高的竞买人之手, 它也是一种具有帕累托效率的配置机制。 维克里最重要的贡献在于, 他针对竞买人对称的情形证明, 荷式拍卖与英式拍卖所产生的期望价格相同。结合战略等价关系, 这实际上意味着四种标准拍卖机制给卖主带来的平均收入相等。这就是著名的收入等价定理，该定理是整个拍卖理论研究的起点。 具体的可以研读阅读论文《反投机、拍卖与竞争性密封投标》或者Vijay Krishna的《Auction Theory》了解更多的拍卖理论的细节。互联网广告中常用的的二价定价策略就是从维克多拍卖推广开来的，具体看一下。 广义第二高价广义第二高价（GSP）是互联网广告中运用最广泛的定价方式，像谷歌、百度运用的都是该策略。其原理如下： 单位置第二高价拍卖 和拍卖理论中的维克里拍卖一致，指的是在只有一个位置的拍卖中，向赢得该位置的广告主收取其下一位广告主的出价，比如说耐克出10块，阿迪出8块，那么耐克赢得单广告位展示，但最终的定价是8元。 多位置广义第二高价拍卖 由单位置二价拍卖推广开来放到搜索广告的多广告位拍卖中，对赢得每一个位置的广告主，都按照他下一位的广告位置出价来收取费用。 如果最终的单子按照CPM即曝光收费，广义第二高价拍卖可直接应用； 如果是按照CPC即点击结算，广告主出价是根据点击来的，而竞价是根据eCPM排序的，最终的定价需要进行如下的换算： q_s = \frac{\mu_{s+1}b_{s+1}}{\mu_s+\Delta}也就是根据以下一位的广告位置的eCPM作为分子，以该位置的点击率加上一个很小的$\Delta$作为分母，两者相除得到该位置的定价，具体的计算可以看最后的定价示例。 VCG定价VCG 定价是 Vickrey、Clarke 和 Groves 在研究竞价系统均衡状态时得到的一种理论上较为优越的定价策略。 其基本思想是：对于赢得了某个位置的广告主，其所付出的成本应该等于他占据这个位置给其他市场参与者带来的价值损害。有点类似于机会成本的概念。 在这一原则下，VCG的定价策略可以表示为公式： q_s = \sum_{t>s}(\mu_{t-1}-\mu_t)v_t其优点是： 在VCG定价策略的稳定状态下，每个广告主都找到了自己的最优状态 相较于其他定价策略，VCG向广告主收取的费用最少 但VCG并不主流，只有少数的广告厂商如facebook使用，其缺陷如下： 定价逻辑过于复杂，很难向广告主解释清 “给其他市场参与者带来的价值损害”的计算很难验证 市场保留价和拍卖过程一直，互联网竞价市场也会设置一个拍卖底价，即市场保留价（Market Reserve Price,MRP），也称底价或起价。这个底价体现在两个方面： 出价只有在高于底价时才有竞价机会 若赢得了广告位，根据定价策略计算出的费用低于市场保留价，也需要调整到底价的水平 如何确定底价： 竞争较为充分时、广告主深度足够时，底价可以设的高一些 反之则适当降低 底价的设置方式： 整个竞价市场采用同样的保留价格 根据不同的标的物设置不同的保留价格，一般对竞争激烈的关键词设置较高的MRP，这个也叫动态市场保留价 底价的基本原理 根据竞价广告主的eCPM分布，找到一个使得填充率没有明显下降的CPM底价，然年后再根据质量度倒算其CPC底价。 动态市场保留价 雅虎搜索关键词动态调价，即对不同的关键词调整不同的起价，对收入的影响达到13%，谷歌也更早推出其动态调价策略 动态调价与a/u/c都有关，可以做到完全的动态，具体可参考文献Reserve Prices in Internet Advertising Auctions:A Field Experiment∗ 价格挤压在CPC广告竞价机制设计中，会对eCPM的计算公式 $eCPM = \mu * bid_{CPC}$做一些微调，如下： eCPM = \mu^{k} * bid_{CPC}其中k大于0，k越大，对出价的挤压就越大，故称价格挤压因子。看两种极端情况： $k\rightarrow \infty$：只根据点击率排序，不考虑出价的作用 $k\rightarrow 0$：只根据出价排序 作用：根据市场情况更主动影响竞价体系朝着需要的方向发展 若市场上存在大量的出价较高但品质不佳的广告主，则可调高$k$来强调质量和用户反馈的影响，即更多的体现点击率；同时想鼓励广告主提高广告质量和相关性，也可以提高$k$ 若市场竞价激烈程度不足，可降低$k$鼓励竞争，并短期增加营收 定价示例与算法实现目前主流的定价体系综合运用了以上所述的GSP、市场保留价和价格挤压三种策略，下面是一个具体示例： A/B/C/D四个广告竞价，其中底价MRP为0.25元，第一列为广告主出价，第二列为预估的点击率，第三、四、五列分别是$k=1.0/2.5/0.5$时的eCPM排序和最终定价，以第三列为例说一下计算的步骤： 1、对每一个广告按照给定的挤压因子计算调整后的eCPM eCPM = \mu^{k} * bid_{CPC}所以其eCPM排序为 $A:1,B:2,C:3,D:4$ 2、依据GSP策略计算定价，此处$\Delta$设为0 q_s = \frac{eCPM_{s+1}}{\mu_s+\Delta}A的下一排序是B，所以$q_s = \frac{eCPM_B}{\mu_s}= 0.008/0.016 = 0.5$B的下一排序是C：所以$q_s = \frac{eCPM_C}{\mu_s}= 0.003/0.004 = 0.75$C的下一排序是D：所以$q_s = \frac{eCPM_D}{\mu_s}= 0.002/0.01 = 0.2$，因为底价MRP=0.25,故最终的定价为0.25D为最后一个广告，没有下一排序广告，所以按照底价定价为0.25 整个计算过程比较简单，下面用python实现一下GSP定价的过程。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class cal_gsp(object): def __init__(self): ## 初始化价格挤压因子和市场保留价 self.squash_k = 1 self.MRP = 0.25 ## 计算加入挤压因子的ecpm def cal_ecpm(self,ad_gsp): for ad_id,ad_mes in ad_gsp.items(): bid = ad_mes.get(&apos;bid&apos;) if bid&lt;MRP: continue ctr = ad_mes.get(&apos;ctr&apos;) eCPM = (ctr**squash_k) * bid ad_gsp[ad_id][&apos;eCPM&apos;] = eCPM return ad_gsp ## 按照ecpm排序 def get_sorted_ecpm(self,ad_gsp): return sorted(ad_gsp.items(),key = lambda x: x[1].get(&apos;eCPM&apos;),reverse=True) ## 按照gsp策略计算最终的定价 def get_gsp(self, ad_gsp,sorted_ad_gsp): for i in range(len(sorted_ad_gsp)): cur_ad = sorted_ad_gsp[i] ad_id = cur_ad[0] if i &lt; len(sorted_ad_gsp)-1: next_ad = sorted_ad_gsp[i+1] price = next_ad[1].get(&apos;eCPM&apos;)/cur_ad[1].get(&apos;ctr&apos;) if price &lt; MRP: price = MRP ad_gsp[cur_ad[0]][&apos;gsp&apos;] = price else: ad_gsp[cur_ad[0]][&apos;gsp&apos;] = MRP return ad_gspif __name__ == &apos;__main__&apos;: ## 示例 ad_gsp = &#123;&apos;A&apos;:&#123;&apos;bid&apos;:0.8,&apos;ctr&apos;:1.6&#125;,&apos;B&apos;:&#123;&apos;bid&apos;:2.0,&apos;ctr&apos;:0.4&#125;,&apos;C&apos;:&#123;&apos;bid&apos;:0.3,&apos;ctr&apos;:1&#125;,&apos;D&apos;:&#123;&apos;bid&apos;:0.4,&apos;ctr&apos;:0.5&#125;&#125; cal_gsp = cal_gsp() ad_gsp = cal_gsp.cal_ecpm(ad_gsp) sorted_ad_gsp = cal_gsp.get_sorted_ecpm(ad_gsp) result = cal_gsp.get_gsp(ad_gsp,sorted_ad_gsp) print(result) 参考资料： 计算广告 知乎问答 定价问题延伸阅读： Research Frontier of Real-Time Bidding based Display Advertising Budget Constrained Bidding by Model-free Reinforcement Learning in Display Advertising Real-Time Bidding with Multi-Agent Reinforcement Learning in Display Advertising Real-Time Bidding by Reinforcement Learning in Display Advertising Combining Powers of Two Predictors in Optimizing Real-Time Bidding Strategy under Constrained Budget Bid-aware Gradient Descent for Unbiased Learning with Censored Data in Display Advertising Optimized Cost per Click in Taobao Display Advertising Real-Time Bidding Algorithms for Performance-Based Display Ad Allocation Deep Reinforcement Learning for Sponsored Search Real-time Bidding]]></content>
      <categories>
        <category>计算广告</category>
      </categories>
      <tags>
        <tag>定价</tag>
        <tag>维克里拍卖</tag>
        <tag>价格挤压</tag>
        <tag>底价</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark笔记（7）：Spark性能优化之资源调优篇]]></title>
    <url>%2F2019%2F07%2F10%2Fspark-7%2F</url>
    <content type="text"><![CDATA[在开发完Spark作业之后，就该为作业配置合适的资源了。spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会及其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论哪种情况，都会导致spark作业的运行效率低下，甚至根本无法运行。因此我们必须对spark作业的资源使用原理有一个清晰的认识，并知道spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。 一、spark作业基本运行原理 详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。 在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。 Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。 当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。 因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。 task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。 以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。 资源参数调优了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。 num-executors 参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。 参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。 executor-memory 参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。 参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，就代表了你的Spark作业申请到的总内存量（也就是所有Executor进程的内存总和），这个量是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的总内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。 executor-cores 参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。 参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。 driver-memory 参数说明：该参数用于设置Driver进程的内存。 参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。 spark.default.parallelism 参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。 参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。 spark.storage.memoryFraction 参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。 参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 spark.shuffle.memoryFraction 参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。 参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。 资源参数参考示例以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节： 123456789./bin/spark-submit \ --master yarn-cluster \ --num-executors 100 \ --executor-memory 6G \ --executor-cores 4 \ --driver-memory 1G \ --conf spark.default.parallelism=1000 \ --conf spark.storage.memoryFraction=0.5 \ --conf spark.shuffle.memoryFraction=0.3 \ 转载自：【美团技术博客】]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark性能</tag>
        <tag>资源调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark笔记（6）：Spark性能优化之开发调优篇]]></title>
    <url>%2F2019%2F07%2F09%2Fspark-6%2F</url>
    <content type="text"><![CDATA[在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。 然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。 Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。 笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。 本文作为Spark性能优化指南的基础篇，主要讲解开发调优。 Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。 原则一：避免创建重复的RDD通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。 我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。 一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。 1234567891011121314151617181920212223242526//需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。//也就是说，需要对一份数据执行两次算子操作。//错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。//这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，//然后分别对每个RDD都执行了一个算子操作。//这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；//第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。 val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)rdd1.map(...)val rdd2 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)rdd2.reduce(...) //正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。//这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，//然后对这一个RDD执行了多次算子操作。//但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，//还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。//要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，//才能保证一个RDD被多次使用时只被计算一次。 val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)rdd1.map(...)rdd1.reduce(...) 原则二：尽可能复用同一个RDD除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。 1234567891011121314151617181920212223242526272829303132333435// 错误的做法。 // 有一个&lt;long , String&gt;格式的RDD，即rdd1。// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，//而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。 JavaPairRDD&lt;/long&gt;&lt;long , String&gt; rdd1 = ...JavaRDD&lt;string&gt; rdd2 = rdd1.map(...) // 分别对rdd1和rdd2执行了不同的算子操作。 rdd1.reduceByKey(...)rdd2.map(...) // 正确的做法。 // 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，//rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。 // 其实在这种情况下完全可以复用同一个RDD。// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。 JavaPairRDD&lt;long , String&gt; rdd1 = ...rdd1.reduceByKey(...)rdd1.map(tuple._2...) // 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，//才能保证一个RDD被多次使用时只被计算一次。 原则三：对多次使用的RDD进行持久化当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。 Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。 因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。 12345678910111213141516171819202122232425// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。 // 正确的做法。// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。 val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;).cache()rdd1.map(...)rdd1.reduce(...) // persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，//内存不充足时持久化到磁盘文件中。// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition//都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，//从而发生频繁GC。 val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;).persist(StorageLevel.MEMORY_AND_DISK_SER)rdd1.map(...)rdd1.reduce(...) 对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。 Spark的持久化级别 持久化级别 含义解释 MEMORY_ONLY 使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。 MEMORY_AND_DISK 使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。 MEMORY_ONLY_SER 基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 MEMORY_AND_DISK_SER 基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 DISK_ONLY 使用未序列化的Java对象格式，将数据全部写入磁盘文件中。 MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等. 对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。 如何选择一种最合适的持久化策略 1、默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。 2、如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。 3、如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。 4、通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。 原则四：尽量避免使用shuffle类算子如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。 shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。 因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。 Broadcast与map进行join代码示例 12345678910111213141516171819// 传统的join操作会导致shuffle操作。// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。 val rdd3 = rdd1.join(rdd2) // Broadcast+map的join操作，不会导致shuffle操作。// 使用Broadcast将一个数据量较小的RDD作为广播变量。val rdd2Data = rdd2.collect()val rdd2DataBroadcast = sc.broadcast(rdd2Data) // 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，//那么就判定可以进行join。// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，//拼接在一起（String或Tuple）。val rdd3 = rdd1.map(rdd2DataBroadcast...) // 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。 原则五：使用map-side预聚合的shuffle操作如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。 所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。 比如下图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。 原则六：使用高性能的算子除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。 使用reduceByKey/aggregateByKey替代groupByKey 详情见“原则五：使用map-side预聚合的shuffle操作”。 使用mapPartitions替代普通map mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！ 使用foreachPartitions替代foreach 原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。 使用filter之后进行coalesce操作 通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。 使用repartitionAndSortWithinPartitions替代repartition与sort类操作 repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。 原则七：广播大变量有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。 在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。 因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。 广播大变量的代码示例 123456789101112// 以下代码在算子函数中，使用了外部的变量。// 此时没有做任何特殊操作，每个task都会有一份list1的副本。val list1 = ...rdd1.map(list1...) // 以下代码将list1封装成了Broadcast类型的广播变量。// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。// 每个Executor内存中，就只会驻留一份广播变量副本。val list1 = ...val list1Broadcast = sc.broadcast(list1)rdd1.map(list1Broadcast...) 原则八：使用Kryo优化序列化性能在Spark中，主要有三个地方涉及到了序列化： 1、在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。2、将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。3、使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。 对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。 以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）： 123456// 创建SparkConf对象。val conf = new SparkConf().setMaster(...).setAppName(...)// 设置序列化器为KryoSerializer。conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)// 注册要序列化的自定义类型。conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2])) 原则九：优化数据结构Java中，有三种类型比较耗费内存： 1、对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。 2、字符串，每个字符串内部都有一个字符数组以及长度等额外信息。 3、集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。 因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。 但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。 转载自：【美团技术博客】]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>开发调优</tag>
        <tag>spark性能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark笔记（5）：Spark SQL]]></title>
    <url>%2F2019%2F06%2F28%2Fspark-5%2F</url>
    <content type="text"><![CDATA[Spark SQL所使用的数据抽象并非RDD，而是DataFrame。DataFrame的推出，让Spark具备了处理大规模结构化数据的能力，它不仅比原有的RDD转化方式更加简单易用，而且获得了更高的计算性能。Spark能够轻松实现从Mysql到DataFrame的转化，并且支持SQL查询。 Spark2.0以上版本开始，Spark使用全新的SparkSession接口替代Spark1.6中的SQLCOntext及HiveContext接口，来实现对其数据加载、转换、处理等功能。SparkSession实现了SQLContext及HiveContext所有功能。 Sparksession支持从不同的数据源加载数据，以及把数据转换成DataFrame，并且支持把DataFrame转换成SQLContext自身的表，然后使用SQL语句来操作数据。 一、RDD 转换成 DataFrameSpark支持两种方法实现从RDD转换成DataFrame。第一种方法是，利用反射来推断包含特定类型对象的RDD的schema，适用对已知数据结构的RDD转换；第二种方法是，使用编程接口，构造一个schema并将其应用在已知的RDD上。 反射机制123456789101112131415%pysparkfrom pyspark.sql import SparkSessionspark = SparkSession(sc)data = sc.parallelize([(&quot;hadoop&quot;,2),(&quot;spark&quot;,3),(&quot;hive&quot;,4),(&quot;spark&quot;,4)])dataframe = data.toDF([&quot;project&quot;,&quot;number&quot;])dataframe.createOrReplaceTempView(&apos;TempTable&apos;)spark.sql(&quot;select project, sum(number) as sum from TempTable group by project&quot;).show()+-------+---+|project|sum|+-------+---+| spark| 7|| hadoop| 2|| hive| 4|+-------+---+ 运用编程接口12345678910111213141516171819202122232425262728293031323334353637%pysparkfrom pyspark.sql.types import Rowfrom pyspark.sql import SparkSessionfrom pyspark.sql.types import StructType,StructField,StringType,IntegerTypespark = SparkSession(sc)data = sc.parallelize([(&quot;hadoop&quot;,2),(&quot;spark&quot;,3),(&quot;hive&quot;,4),(&quot;spark&quot;,4)])def return_row(item): return Row(**&#123;&quot;project&quot;:item[0],&quot;number&quot;:item[1]&#125;)schema = StructType([ StructField(&apos;project&apos;,StringType(),True), StructField(&apos;number&apos;,IntegerType(),True ) ]) data = data.map(return_row)data.take(10)data = spark.createDataFrame(data,schema)data.show()data.createOrReplaceTempView(&quot;TempTable&quot;)spark.sql(&quot;select project,sum(number) as sum from TempTable group by project&quot;).show()+-------+------+|project|number|+-------+------+| hadoop| 2|| spark| 3|| hive| 4|| spark| 4|+-------+------++-------+---+|project|sum|+-------+---+| spark| 7|| hadoop| 2|| hive| 4|+-------+---+ 二、DataFrame常用操作123456789101112131415161718192021222324252627282930313233343536373839404142dataframe.printSchema()root |-- project: string (nullable = true) |-- number: long (nullable = true) dataframe.select(dataframe.project,dataframe.number+1).show()+-------+------------+|project|(number + 1)|+-------+------------+| hadoop| 3|| spark| 4|| hive| 5|| spark| 5|+-------+------------+dataframe.filter(dataframe.number&gt;3).show()+-------+------+|project|number|+-------+------+| hive| 4|| spark| 4|+-------+------+dataframe.groupby(&quot;project&quot;).sum().show()+-------+-----------+|project|sum(number)|+-------+-----------+| spark| 7|| hadoop| 2|| hive| 4|+-------+-----------+dataframe.sort(dataframe.number).show()+-------+------+|project|number|+-------+------+| hadoop| 2|| spark| 3|| spark| 4|| hive| 4|+-------+------+ 三、读取与保存读取12345678910111213141516171819202122## jsonspark.read.json(file)## csvspark.read.csv(file, header=True, inferSchema=True)## mysqlsql=&quot;(select * from mysql.db where db=&apos;wp230&apos;) t&quot;df = spark.read.format(&apos;jdbc&apos;).options( url=&apos;jdbc:mysql://127.0.0.1&apos;, dbtable=sql, user=&apos;root&apos;, password=&apos;123456&apos; ).load()## parquetspark.read.parquet(file)## hdfsspark.read.csv(file) 保存1234567891011121314151617## csvdf.write.csv(path=file_path, header=True, sep=&quot;,&quot;, mode=&apos;overwrite&apos;)## parquetdf.write.parquet(path=file_path,mode=&apos;overwrite&apos;)## hdfs df.write.mode(&quot;overwrite&quot;).options(header=&quot;true&quot;).csv(file_path)## mysql df.write.mode(&quot;overwrite&quot;).format(&quot;jdbc&quot;).options( url=&apos;jdbc:mysql://127.0.0.1&apos;, user=&apos;root&apos;, password=&apos;123456&apos;, dbtable=&quot;test.test&quot;, batchsize=&quot;1000&quot;,).save() 四、用户自定义函数虽然spark.sql.function中已包含大量常用函数，但总有一些特定场景无法满足，这就需要使用udf，一个基本udf创建的流程如下： 1234567891011121314# 1.创建普通的python函数def toDate(s): return str(s)+&apos;-&apos;# 2.注册自定义函数from pyspark.sql.functions import udffrom pyspark.sql.types import StringType# 根据python的返回值类型定义好spark对应的数据类型# python函数中返回的是string，对应的pyspark是StringTypetoDateUDF=udf(toDate, StringType()) # 使用自定义函数df1.withColumn(&apos;color&apos;,toDateUDF(&apos;color&apos;)).show() 最简单的就是通过lambda函数，不需要定义返回值类型，可以直接使用 1234567# 创建udf自定义函数from pyspark.sql import functionsconcat_func = functions.udf(lambda name,age:name+&apos;_&apos;+str(age)) # 简单的连接两个字符串# 应用自定义函数concat_df = spark_df.withColumn(&quot;name_age&quot;,concat_func(final_data.name, final_data.age))concat_df.show() 以上两例是在dataframe中使用，也可以在spark.sql中使用： 1234567891011# 定义自定义函数def is_nulludf(fieldValue, defaultValue): if fieldValue == None: return defaultValue return fieldValue# 注册自定义函数spark.udf.register(&quot;is_nulludf&quot;, is_nulludf)# 使用自定义函数spark.sql(&quot;select col_name, is_nulludf(col_name) as col_name2 from table &quot;)]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark笔记（4）：RDD编程 Scala版本]]></title>
    <url>%2F2019%2F06%2F27%2Fspark-4%2F</url>
    <content type="text"><![CDATA[创建 从本地文件系统/分布式文件系统HDFS加载： 12scala &gt; val path = &apos;&apos;scala &gt; val lines = sc.textFile(path) 通过并行集合（数组）创建 12345678910val array = Array(1,2,3,4,5)val rdd = sc.parallelize(array)array: Array[Int] = Array(1, 2, 3, 4, 5)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:29val list = List(1,2,3,4,5)val rdd = sc.parallelize(array)list: List[Int] = List(1, 2, 3, 4, 5)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at &lt;console&gt;:29 RDD操作惰性机制：整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到行动操作时，才会触发“从头到尾”真正的计算。 转换操作惰性求值，遇到行动操作才会触发“从头到尾”的真正的计算 filter map flatmap groupByKey reduceByKey 行动操作真正触发计算的地方。 count():返回元素个数 collect():以数组形式返回所有元素 first():返回第一个元素 take(n):以数组形式返回前n个元素 reduce(func):通过函数func（输入两个参数并返回一个值）聚合元素 foreach(func):将每个元素传递到函数func中运行 123456789101112131415161718192021222324val rdd = sc.parallelize(Array(1,2,3,4,5,6))rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at parallelize at &lt;console&gt;:27rdd.count()res28: Long = 6rdd.first()res29: Int = 1rdd.take(3)res30: Array[Int] = Array(1, 2, 3)rdd.collect()res31: Array[Int] = Array(1, 2, 3, 4, 5, 6)rdd.reduce((a,b)=&gt;a+b)res32: Int = 21rdd.take(5).foreach(println)12345 注意：Local模式单机执行时，rdd.foreach(elem=&gt;println(elem))会打印出一个RDD中的所有元素。但在集群模式下，在Worker节点上执行打印语句是输出到Worker节点的stdout中，而不是输出到任务控制节点Driver中，因此，任务控制节点Driver中的stdout是不会显示打印语句的这些输出内容的。为了能够把所有Worker节点上的打印输出信息也显示到Driver中，就需要使用collect()方法。但collect()方法会把各个Worker节点上的所有RDD元素都抓到Driver中，因此，可能会导致Driver所在节点发生内存溢出。 实例 12345val text = sc.parallelize(List(&quot;hadoop is good&quot;,&quot;spark is fast&quot;,&quot;spark is better&quot;))text: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[45] at parallelize at &lt;console&gt;:43text.flatMap(line =&gt; line.split(&quot; &quot;)).map(word=&gt; (word,1)).reduceByKey((a,b)=&gt;a+b).take(10)res155: Array[(String, Int)] = Array((is,3), (fast,1), (better,1), (spark,2), (hadoop,1), (good,1)) 持久化为避免重复计算产生的开销，可使用persist()方法对RDD标记为持久化。 persist(MEMORY_ONLY):将RDD作为反序列化的对象存储于JVM中，若内存不足，按照LRU原则（least recently used最近最少使用原则）替换缓存中的内容。 persist(MEMORY_AND_DISK):将RDD作为反序列化的对象存储于JVM中,若内存不足，超出的分区将会被存放在磁盘上。 cache()方法，会调用persist(MEMORY_ONLY) 12345678910111213val rdd = sc.parallelize(List(&quot;hadoop&quot;,&quot;spark&quot;,&quot;Hive&quot;))rdd.cache() // 会调用persist(MEMORY_ONLY)，但是语句执行到这里，不会缓存rdd，因为这时的rdd还没有计算生成println(rdd.count())// 第一次行动操作，触发一次真正从头到尾的计算，这时上面的rdd.cache()才会被执行，把这个rdd放到缓存中println(rdd.collect().mkString(&quot;,&quot;))//第二次行动操作，不需要触发从头到尾的计算，只需要重复使用上面缓存的rddrdd.unpersist()rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[27] at parallelize at &lt;console&gt;:27res109: rdd.type = ParallelCollectionRDD[27] at parallelize at &lt;console&gt;:273hadoop,spark,Hiveres114: rdd.type = ParallelCollectionRDD[27] at parallelize at &lt;console&gt;:27 分区分区的作用： 增加并行度：分区可以在不同的工作节点上启动不同的线程进行并行处理 减少通信开销：分布式系统中，通信代价巨大，控制数据分布以获得最少的网络传输可以极大地提升整体性能 如下图，当userData表和Events表进行连接时，默认情况下会将两个数据集中的所有key的哈希值都求出来，将哈希值相同的记录传送到同一台机器上，之后在该机器上对所有key相同的记录进行连接操作。这样每次进行连接操作都会有数据混洗的问题，造成很大的网络传输开销。 实际上，由于userData这个RDD要比events大很多，可以先对userData进行哈希分区，这样在连接时，只有events表发生了数据混洗产生网络通信，userData是在本地引用的，不会产生网络开销。可以看出，Spark通过数据分区，对于一些特定类型的操作，比如join()/leftOuterJoin()/groupByKey()/reduceByKey()等，可以大大降低网络传输开销。 分区原则：分区个数尽量等于集群中CPU核心（core）数目。通过spark.default.parallelism配置默认分区数目 默认分区： Local模式：默认是本地CPU数目 Standalone或YARN模式：在“集群中所有CPU核心数目总和”和“2”取大值 Mesos模式：默认分区为8 设置 创建RDD时：sc.textFile(path,partitionNum)，若从HDFS读取文件，分区数为文件分片数。 repartition 12345678val rdd = sc.parallelize(List(&quot;hadoop&quot;,&quot;spark&quot;,&quot;Hive&quot;))rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[30] at parallelize at &lt;console&gt;:27rdd.partitions.sizeres123: Int = 2rdd.repartition(1).partitions.sizeres124: Int = 1 自定义分区 12345678910import org.apache.spark.HashPartitionerclass MyPartitioner(numParts:Int) extends HashPartitioner(numParts:Int)&#123; override def numPartitions: Int = numParts override def getPartition(key: Any): Int = &#123;key.toString.toInt%10&#125; &#125; data = sc.textFile(List(1,2,3,4,5,6))data.map((_,1)).partitionBy(new MyPartitioner(10)).map((_._1)).take(10) 键值对 reduceByKey() groupByKey() keys() values() sortByKey() sortBy() mapValues() join() combineBykey() 123456789101112131415161718192021222324252627282930313233343536373839404142val text = sc.parallelize(Array((&quot;hadoop&quot;,1),(&quot;spark&quot;,1),(&quot;spark&quot;,2),(&quot;hive&quot;,1)))text: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[294] at parallelize at &lt;console&gt;:43text.reduceByKey(_+_).take(10)res521: Array[(String, Int)] = Array((hive,1), (spark,3), (hadoop,1))text.groupByKey().map(x=&gt;(x._1,x._2.sum)).take(10)res522: Array[(String, Int)] = Array((hive,1), (spark,3), (hadoop,1))text.keys.take(10)res523: Array[String] = Array(hadoop, spark, spark, hive)text.values.take(10)res524: Array[Int] = Array(1, 1, 2, 1)text.sortByKey().take(10)res525: Array[(String, Int)] = Array((hadoop,1), (hive,1), (spark,1), (spark,2))text.reduceByKey((x,y)=&gt;x+y).sortBy(x=&gt;x._2,false).take(10)res526: Array[(String, Int)] = Array((spark,3), (hive,1), (hadoop,1))text.reduceByKey((x,y)=&gt;x+y).mapValues(x=&gt;x+1).take(10)res527: Array[(String, Int)] = Array((hive,2), (spark,4), (hadoop,2))val text2 = sc.parallelize(List((&quot;spark&quot;,&quot;best&quot;)))text2: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[311] at parallelize at &lt;console&gt;:44text.join(text2).take(10)res529: Array[(String, (Int, String))] = Array((spark,(1,best)), (spark,(2,best)))val data = sc.parallelize(List((&quot;c1&quot;,88),(&quot;c2&quot;,99),(&quot;c3&quot;,28),(&quot;c2&quot;,56),(&quot;c1&quot;,58)))data: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[315] at parallelize at &lt;console&gt;:44data.combineByKey( (x)=&gt;(x,1), (x:(Int,Int),z) =&gt; (x._1+z,x._2+1), (x:(Int,Int),y:(Int,Int)) =&gt; (x._1+y._1,x._2+y._2) ).map(&#123;case (x,y)=&gt;(x,y._1/y._2.toFloat)&#125;).take(10)res531: Array[(String, Float)] = Array((c3,28.0), (c1,73.0), (c2,77.5)) data.mapValues(x=&gt;(x,1)).reduceByKey((a,b)=&gt;(a._1+b._1,a._2+b._2)).mapValues(x=&gt;x._1/x._2.toFloat).take(10)res532: Array[(String, Float)] = Array((c3,28.0), (c1,73.0), (c2,77.5))]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>RDD</tag>
        <tag>转换</tag>
        <tag>行动</tag>
        <tag>持久化</tag>
        <tag>键值对</tag>
        <tag>读写</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark笔记（3）：集群运行Spark程序实例讲解]]></title>
    <url>%2F2019%2F06%2F26%2FSpark-3%2F</url>
    <content type="text"><![CDATA[之前都是在spark shell上执行，当数据量达到一定程度，我们可以利用Spark的集群模式来运行，增加算力，而且本地小数据量验证成功的代码可以直接放到集群上跑。 这一小节从提交一个集群环境下的Spark Job出发，讨论了在集群运行Spark Job时的配置项，再讲到Spark基础的架构，最后讲解了一下Spark Job性能调试的经验。 一、spark-submit应用部署Spark使用spark-submit脚本来启动集群部署模式，进入/usr/bin/目录，可以看到一个spark-submit脚本，输入spark-submit —help可以看到一些配置选项介绍，截取一部分如下图： 以下是一个spark-submit提交scala应用的shell脚本的示例，我们可以看到在主程序job_submit中，我们从控制台传入了四个所需参数，对spark-submit进行了配置，比如class/master/queue等等，以下会逐一讲解，最后我们传入了JAR_FILE及其参数，这是我们的需要执行的打包的scala程序： 所以spark-submit的一般格式是这样的： 12usr/bin/spark-submit [options] &lt;app jar | python file&gt; [app options]usr/bin/spark-submit [选项配置] &lt;程序jar包或者python脚本文件&gt; [程序参数] 这里主要讲几个比较常用的配置项： — master: 集群的master URL，上面的示例中我们传入了yarn集群 有以下可以接收的值： 值 描述 spark://host:port 连接到指定端口的Spark独立集群，默认Saprk独立主节点端口为7077,例如spark://23.195.26.187:7077 mesos://host:port 连接到指定端口的Mesos集群，默认为5050端口 yarn yarn集群，需设置环境变量HADOOP_CONF_DIR为HADOOP配置目录 local 本地模式，使用单核 local[N] 本地模式，N个核 local[*] 本地模式 尽可能多的核 — class: 运行Java或scala程序时应用的主类，示例中为com.stat.UserAdProfileGenerator — delopy-mode: 选择在客户端client还是在集群cluster启动驱动器 — name: 该应用显示在Spark网页用户界面上的名称，示例中未设置该参数 — files: 需放到该应用工作目录中的文件列表，我们程序中往往会使用一些数据文件，可以放在这里 — py-files: 需添加到PYTHONPATH中的文件列表，可以包含.py/.egg/.zip文件 — driver-memory: 驱动器进程使用的内存量，示例为16G — executor-memory: 执行器进程使用的内存量，示例为16G — num-executors: 指定执行器数量，只在yarn集群模式下使用，示例为50个 — exectors-cores: 执行器节点的核心数，只在yarn集群模式下使用，示例为1个 以上这些选项主要包括两部分，第一部分为调度的信息，比如master、driver-memory、exectors-cores，这部分和我们的性能调试息息相关，后面会讲到怎么去做调整；第二部分是依赖，比如files、py-files。 上面出现了一些陌生的概念，比如驱动器、执行器、集群等等，这些都是Spark架构中的概念，分别看一下。 二、Spark基础架构下图为Spark架构组成图，有三个主要组成部分，Driver Program、Cluster Manager和Worker Node。 Driver Program：驱动器程序，负责中央协调，调度各个分布式工作节点； Cluster Manager：集群管理器，控制整个集群，监控worker。自带的集群管理器称为独立集群管理器，也可以运行在Hadoop YARN或者Apache Mesos上； Worker Node：工作节点，负责控制计算节点，启动Executor或者Driver。 Executor：执行器节点，运行在worker node上的一个进程； 分别具体介绍一下驱动器节点、执行器节点和集群管理器，如下： 驱动器节点 作用：执行main()方法的进程，比如创建SparkContext、创建RDD、RDD转化和行动操作的代码，当我们启动Spark shell时，就启动了Spark驱动器程序，而当驱动器程序一旦终止，Spark应用也就结束了。 职责： 所有的Spark程序不外乎创建RDD，RDD转化操作生成新的RDD，最后行动操作存储RDD的数据，这样就构成了一个有向无环图（DAG）的操作逻辑图，当驱动器节点启动时，Spark把逻辑图转为物理执行计划，即转变为一系列步骤（Stage），每个步骤包含多个任务（Task），这些任务打包后被送到集群进行分布式计算。Task是Spark中最小的工作单元。 驱动器节点启动，生成物理执行计划，驱动器程序负责协调各执行器进程之间的各个任务，执行器进程启动后会在驱动器上注册自己，以在驱动器上有每个执行器进程的完整记录，每个执行器节点代表一个能够处理任务和存储RDD数据的进程。Spark会把分配任务给合适的执行器进程，执行器进程会缓存数据，驱动器跟踪数据缓存任务，从而调度之后的任务，尽可能减少数据网络传输。 执行器节点 作用：负责在Spark作业中运行任务，任务间相互独立；与Spark任务同时启动，伴随整个生命周期，即使执行器节点发生了异常奔溃，Saprk也可继续执行 职责： 负责运行组成Spark应用的任务，并将结果返回给驱动器进程； 通过自身的块管理器(Block Manager)为用户程序中要求缓存的RDD提供内存式存储。RDD是直接缓存在执行器进程中的，所以可以在运行时充分利用缓存数据提高运算速度。 集群管理器 作用：Spark依赖于集群管理器启动执行器节点，在某些特殊情况下，也会依赖集群管理器来启动驱动器节点。Spark有自带的独立集群管理器，也可以运行在其他外部集群管理器上，如YARN和Mesos等。 这里主要说一下YARN集群管理器，Yarn（Yet Another Resource Negotiator）是Hadoop的资源管理层，它旨在提供一个通用且灵活的框架来管理Hadoop集群中的计算资源。它也是一种主/从结构，由三个部件组成，分别是： Resource Manager (RM) Node Manager (NM) Application Master (AM) 想了解这三个组成部分的细节，可以细读这篇文章Hadoop Yarn的架构 在使用层面，我们看一下如何设置YARN作为集群管理器： 设置指向HADOOP配置目录的环境变量：找到HADOOP的配置目录，设为环境变量HADOOP_CONF_DIR =&gt; export HADOOP_CONF_DIR = “…” 使用spark-submit向主节点URL提交作业 =&gt; spark-submit —master yarn ··· 再看一下如何配置资源用量，这个在之前已经提到过了： —num -executors :设置执行器节点，默认值为2，一般要提高这个值 —executor -memory: 设置每个执行器的内存用量 —executor -cores: 设置每个执行器进程从YARN中占用的核心数目 —queue：设置队列名称，YARN可以将应用调度到多个队列中 这里几个参数的调优会在第四小节讲到。 讲完了spark-summit和架构部分，那如果想在代码层面进行debug，有哪些比较好的方法，下一章节分别介绍一下。 三、Spark UI与日志Spark可以通过单机和集群模式来部署代码，单机部署的时候我们可以通过断点调试来修正代码，但集群部署却很难用该方法去调试，但我们可以通过日志分析或者Spark UI界面来进行调试和优化我们的代码。 Spark UISpark UI是呈现Spark应用性能以及其他信息的前端展示页面， 看一下导航栏，按照数字标注分别是： job页面：列出了当前应用的所有任务，以及所有的excutors中action的执行时间、耗时、完成情况； Stages：查看应用的所有stage，粒度上要比job更细一些； storage：查看应用目前使用了多少缓存，一般由cache persist等操作触发 environment：展示当前spark所依赖的环境，比如jdk,lib等等 executors：查看申请使用的内存大小以及shuffle中input和output等数据 AppName：显示代码中使用setAppName设定应用名字 下图为Spark的UI主页面，这里展示了已完成的Job信息，点击Job进入详细页面，分为两部分，一部分是event timeline，另一部分是进行中和完成的job任务。 更多页面的详细介绍可以参考这个链接 基于SparkUI性能优化与调试 yarn logs如果我们没法从UI界面中排查出错原因，这时候只有把driver和executor进程所生成的日志来得到更多的信息，因为日志会详细记录各种异常事件，比如内部警告以及用户代码输出的详细异常信息。 在YARN模式下，我们可以在控制台输入： 1yarn logs -applicationId &lt;app ID&gt; 示例如下： app ID一般会在程序执行的时候打印到控制台，或者一般把日志重定向到log文件中，在执行过程中用tail -f 查看执行日志，实时监控执行进程。 四、Spark 性能调优经验打印信息可以在程序中打印出一些输出信息，数据量很大的时候，做一些take(10)/show(10)/count()这样的action操作，就可以看到一些变量的中间状态了，比如将yarn作为资源管理器时，直接可以在yarn logs中就可以看到这些信息。这对我们调试也是很有帮助的，可以知道我们的程序是否得到了我们想得到的结果。 并行度调优在物理执行期间，RDD会分为多个分区，每个分区存储了全体数据的一个子集，然后Spark会给每个分区的数据创建一个Task，Spark一般会根据其底层的存储系统自动推断出合适的并行度，比如从HDFS读数据会为每个文件区块创建一个分区，从混洗后的RDD派生下来的RDD的并行度保持和父一致。 并行度会从两方面影响性能： 并行度过低时，会出现资源限制的情况，可以提高并行度来充分利用更多的计算核心 并行度过高时，每个分区产生的间接开销累计起来会更大。评价并行度是否过高可以看你的任务是不是在瞬间(毫秒级)完成的，或者任务是不是没有读写任何数据。 如何调优： 在数据混洗操作时，对混洗后的RDD指定并行度 对任何已有的RDD进行重新分区来获取更多或更少的分区数，重新分区可用repartition()，减少分区可用coalesce()，coalesce不会打乱数据，所以比repartition效率高 以下是一个实例： 12345678910input = sc.textFile("s3o://···")input.getNumPartitions()10000lines = input.filter(lambda x: x.startwith('20190625'))lines.getNumPartitions()10000lines = lines.coalesce(5).cache()4 内存管理 1.改变内存比例 RDD存储（60%）：当调用RDD的persist()或cache()方法时，这个RDD分区会被存储到缓存中，Spark会根据spark.storage.memoryFraction限制用来缓存的内存占整个JVM堆空间的比例大小。若超出限制，旧的分区数据会被移出内存。 数据混洗与聚合缓存区（20%）：当数据进行数据混洗时，Spark会创造一些中间缓存区来存储数据混洗的输出数据。根据spark.shuffle.memoryFraction限定这种缓存区占总内存的比例。 用户的代码(20%)：spark可以执行任意代码，所以用户的代码可以申请大量内存，它可以访问JVM堆空间中除了分配给RDD存储和数据混洗存储以外的全部空间。 2.改进缓存等级 cache()操作：它以内存优先即MEMORY_ONLY的存储等级持久化数据，分区不够用的话，旧分区会直接删除。需要用到时会再重新算，这样效率会低。一般不太建议直接使用cache，一旦cache的量很大，就会导致内存溢出。 persist()操作可以持久化级别，比如使用MEMORY_AND_DISK级别调用persist()，内存如果放不下的旧分区会被写入磁盘，当再次需要用到的时候再从磁盘上读取，代价比重算要低，性能也会稳定一些，比如从数据库中读取数据时，重算（即读取）的代价很大，这个时候就很有效 硬件资源硬件资源会很大程度上影响Job的效率，主要有以下几个： — executor-memory: executor的内存，各种部署模式都可以，当任务失败，报错信息为sparkContext shutdown时，基本是内存不足导致的。可以尝试调大—excutor-memory参数，但若系统条件受限，无法加大内存，可以局部进行调试，把程序按执行步骤检查问题点。 — executor-cores: executor的核数，仅在yarn模式下 — num—executors: executor节点的总数，仅在yarn模式下 性能调优可以在实践中不断积累经验，以上只是一些参考。如果想了解更多的调优方法，可以访问Tuning Spark 参考资料： 《Spark大数据分析》]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>驱动器</tag>
        <tag>执行器</tag>
        <tag>spark-submit</tag>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark笔记（2）：Pair RDD与数据分区]]></title>
    <url>%2F2019%2F06%2F22%2FSpark-2%2F</url>
    <content type="text"><![CDATA[这一章节我们来看一下在Spark常用来进行聚合操作的Pair RDD，其实类似于字典，由key-value对构成，同样的，Pair RDD也有很多的操作接口，比如reduceByKey()、join()，下面会逐一对介绍，很多语言的语法都是相同的，这里的聚合操作就类似于SQL中的group by或者python中的groupby，所以领会起来也不难。 然后再看一下数据分区相关的内容，数据分区对分布式集群上跑数据来说及其重要，一个小的优化就会极大的降低时间成本和内存开销，这一块也是写Spark Job过程中需要重点关注的。 一、Pair RDD创建123456在python中使用第一个单词作为键创建出一个pair RDD%pysparklines = sc.parallelize([&apos;I have a dream&apos;,&apos;hello world&apos;])lines.map(lambda x: (x.split(&apos; &apos;)[0],x)).take(10)[(&apos;I&apos;, &apos;I have a dream&apos;), (&apos;hello&apos;, &apos;hello world&apos;)] 二、Pair RDD转化操作 聚合操作 1234567891011121314151617181920212223242526272829# reduceByKey() 以key聚合对value进行操作rdd = sc.parallelize([(1,2),(3,4),(3,6)])rdd.reduceByKey(lambda x,y:x+y) [(1,2),(3,10)]# mapValues() 对pair的values进行操作rdd = sc.parallelize([(panda,0),(pink,3),(pirate,3),(panda,1),(pink,4)])rdd.mapvalues(lambda x:(x,1)).reduceByKey(lambda x,y:())[(panda,(1,2)),(pink,(7,2)),(pirate,(3,1))]# 单词计数rdd = sc.parallelize(['stay hungry','stay foolish']) words = rdd.flatMap(lambda x:x.split(" ")) # 分隔单词，铺展开来result = words.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y) # mapreduce 实现计数[("stay",2),("hungry",1),("foolish",1)]# combineByKey() 对每个键求对应的均值nums = sc.parallelize([(panda,0),(pink,3),(pirate,3),(panda,1),(pink,4)])sumcount = nums.combineByKey((lambda x:(x,1))(lambda x,y:(x[0]+y,x[1]+1))(lambda x,y:(x[0]+x[1],y[0],y[1]))).map(lambda x,y:(x,y[0]/y[1]))[("panda",0),("pink",3.5),("pirate",3)] 数据分组 12345# groupByKey() 以key聚合进行分组 rdd = sc.parallelize([(1,2),(3,4),(3,6)])rdd.groupByKey()[(1,[2]),(3,[4,6])] 连接 内连接 =&gt; join()左连接 =&gt; leftOuterJoin()右连接 =&gt; rightOuterJoin() 排序 12# 在python中以字符串顺序对整数进行升序排序rdd.sortedByKey(ascending=True, numPartitions=None, keyfunc = lambda x:str(x)) 三、Pair RDD行动操作上一节说到的行动操作都适用于Pair RDD，此外Pair RDD还有以下行动操作： 12345678# 对每个键对应的元素分别计数rdd = sc.parallelize([(1,1),(3,4),(3,6)])rdd.countByKey()[(1,1),(3,2)]# 返回给定健所对应的所有值rdd.lookup(3)[4,6] 四、数据分区在执行以上的聚合或分组操作时，可以给定Spark的分区数，每一个RDD都有固定给定数目的分区数，Spark会根据集群大小推断有意义的默认值，当然我们也可以对并行度进行调优来获取更好的性能表现。 1234# 在python中自定义reduceByKey()的并行度data = sc.parallelize([(&quot;a&quot;,3),(&quot;b&quot;,4),(&quot;a&quot;，1)])data.reduceByKey(lambda x,y:x+y) ## 默认并行度data.reduceByKey(lambda x,y:x+y,10) ## 自定义并行度为10 coalesce和repartition此外Spark还提供了repartition()函数，以用于在分组和聚合操作之外改变分区，repartition()函数会先把数据通过网络进行混洗，创建新的分区集合。但这样网络开销会很大，coalesce()函数正是对此做了优化。我们可以通过rdd.getNumPartitions查看RDD分区数。 coalesce()函数中有两个传入参数，coalesce(numPartitions,shuffle),其中numPartitions为指定分区数，shuffle为是否进行shuffle，默认为false，若numPartitions大于原有的分区数，必须指定shuffle=True；但避免进行shuffle可以节省网络开销。 repartition()函数只有一个传入参数，repartition(numPartitions)，因为它指定了shuffle为True。 combineByKey()分区数据处理过程combineByKey是Spark中一个比较核心的高级函数，groupByKey、reduceByKey的底层都是使用combineByKey实现的，我们来看一下combineByKey()是如何处理分区数据的。 这个数据流图中出现了三个函数createCombiner、mergeValue、mergeCombiners，分别看一下概念： createCombiner: combineByKey()会遍历分区中的所有元素，因此每个元素的键要么新出现，要么之前遇到过。若是一个新元素， combineByKey()会使用一个createCombiner() 函数创建那个键对应的累加器的初始值。 mergeValue: 如果键在之前遇到过，可以使用mergeValue()方法将该键的累加器对应的当前值与这个新的值进行合并。 mergeCombiners: 每个分区独立操作，所以对于同一个键可有多个累加器。若多个分区都有同一个键的累加器，就需要用mergeCombiners() 将各个分区的结果合并。 这样就很好理解了，整个combineByKey的过程就是在不同的分区上执行类似的操作，遇到新键，执行createCombiner，遇到已存在的键，执行mergeValue，最终对所有分区执行mergeCombiners。 数据分区优化Spark程序可以通过控制RDD分区方式来减少通信开销。举个具体的例子，下面这段scala代码计算了查阅自己订阅主题页面的用户数量。 12345678910111213141516val sc.new SparkContext(...)val userData = sc.sequenceFile[UserID,UserInfo](&quot;hdfs://...&quot;).persist() //UserID用户ID,UserInfo用户订阅的主题，类似于(&quot;Mike&quot;,List(&quot;sports&quot;,&quot;math&quot;)这样的元素def processNewLogs(logFileName:String)&#123; val events = sc.sequenceFile[UserID,LinkInfo](logFileName) # 用户访问情况，元素类似于(&quot;Mike&quot;,&quot;sports&quot;) userData.persist() val joined = userData.join(events) val results = joined.filter(&#123; case (id, (info, link)) =&gt; info.contains(link) &#125; ).count() println(&apos;Number of visits to subscribed topics:&apos;+results)&#125; 但这里有个问题就是，每次调用processNewLogs时，会有一个join操作，会将两个数据集中的所有键的哈希值都求出来，将该哈希值相同的记录通过网络传到同一台机器上，然后在那台机器上对所有键相同的记录进行连接操作。假如userdata表很大很大，而且几乎是不怎么变化的，那么每次都对userdata表进行哈希值计算和跨节点的数据混洗，就会产生很多的额外开销。这个过程的join操作如下： 如何解决网络开销的问题呢？可以再程序开始时，对userdata表使用partitionBy()转化操作，将这张表转为哈希分区。具体实现如下： 12345678910111213141516val sc.new SparkContext(...)val userData = sc.sequenceFile[UserID,UserInfo](&quot;hdfs://...&quot;).partitionBy(new HashPartitioner).persist() // UserID用户ID,UserInfo用户订阅的主题，类似于(&quot;Mike&quot;,List(&quot;sports&quot;,&quot;math&quot;)这样的元素def processNewLogs(logFileName:String)&#123; val events = sc.sequenceFile[UserID,LinkInfo](logFileName) # 用户访问情况，元素类似于(&quot;Mike&quot;,&quot;sports&quot;) userData.persist() val joined = userData.join(events) val results = joined.filter(&#123; case (id, (info, link)) =&gt; info.contains(link) &#125; ).count() println(&apos;Number of visits to subscribed topics:&apos;+results)&#125; 构建userData时调用了partitionBy()，在调用join()时，Spark只会对events进行数据混洗操作，将events中特定UserID的记录发送到userData的对应分区所在的那台机器上。这样，通过网络传输的数据就大大减少，程序运行速度也可以显著提升。partitionBy()是一个转化操作，因此它的返回值是一个新的RDD。还有一点要注意，这里必须要持久化才可以在后面用到RDD时不重复分区操作。 scala可以使用RDD的partitioner属性来获取RDD的分区方式，它会返回一个scala.Option对象。 可以从数据分区中获益的操作有cogroup() , groupWith() , join() , leftOuterJoin() , rightOuterJoin() , groupByKey() , reduceByKey() , combineByKey()以及lookup()。 参考资料：《Spark快速大数据分析》]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Pair RDD</tag>
        <tag>数据分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算广告面面观（2）：大规模分片线性模型MLR]]></title>
    <url>%2F2019%2F06%2F21%2FAD-RS-2%2F</url>
    <content type="text"><![CDATA[点击率预估需要解决的超高维度离散特征空间模式识别的问题，它需要算法在做到可以有效发现当前数据规律的同时，还要具有足够的泛化能力去应对线上多变的user-context-content模式，所以到目前为止有许多的CTR模型被应用于实际场景中，诸如LR、DNN、Tree Model、FM/FFM，这些模型都有各自的优势，但也存在缺陷，整理如下： 一、动机阿里妈妈在2011年提出了MLR模型，全称Large Scale Piecewise Linear Model (LS-PLM)，大规模分片线性模型，背后的思想就是根据领域知识先验将整个空间划分为多个区域，每个区域由一个特定的线性模型来拟合，区域之间平滑连接，当区域达到一定的数量，即可拟合任意复杂的非线性模式。MLR可以看做是LR模型的推广，从线性过渡到非线性，同时省去了大量的人工特征工程，可以端到端训练。 二、模型上面讲到MLR通过先验领域知识划分空间为局部区域，然后每个区域拟合线性模型，可使用如下判别公式来刻画： 其中$\sigma$代表领域先验，决定空间划分（dividing function），$\eta$代表每个局部区域的分类预测模型（fitting function），函数$g$保证模型输出符合概率定义。 在实际中，使用softmax函数作为dividing function，使用sigmoid函数作为fitting function，以及规定$g(x)=x$，则判别函数可改写为如下： 由（2）可知，该模型包含$2m$个参数，论文中使用了$L_{2,1}$正则和$L_1$正则，其中$L_{2,1}$对参数分组范数约束结构化稀疏，压缩$2m$个参数逼近零值，$L_1$则进一步产生稀疏性，使得模型兼具可解释性和泛化能力。 目标函数如下： 但是，引入$L_{2,1}$和$L_1$正则项之后，得到了非凸非平滑的目标函数，因为不是处处可导，所以没法用传统的梯度下降算法或者EM算法来优化，但又因为运算的效率与存储对在线预测来说很关键，模型稀疏性就必须要纳入，所以文章论文针对这个问题提出一个大规模非凸问题的优化求解方法，即利用LBFGS算法框架来根据方向导数找到最快下降方向进行参数更新，此外团队实现了一个大规模分布式系统来支持模型的高效并行训练，以下会一一介绍。 三、优化3.1 方向导数上面说到，目标函数非凸非光滑，无法使用导数，作者退而使用方向导数，顾名思义，方向导数就是某个方向上的导数，而且在每一个方向上都是有导数的，梯度与方向导数的关系就是在梯度方向上的方向导数最大。如下图所示，紫色箭头表示某一方向，在这个方向上的函数的方向导数如黑色实线所示 对于一个存在梯度的函数$f(x,y)$来说，其具有一阶连续偏导数，即可微，这就意味着函数$f(x,y)$在一个点$(x_0,y_0)$处的所有方向导数在一个平面上。如下图所示，红色平面即为所有方向导数所构成的平面 作者在附录A证明了在方向$d$上的方向导数$f^{‘}(\Theta;d)$是一定存在的，附录B求出了目标函数(4)的方向导数，我们依次看一下： 证明与求解首先证明（4）式方向导数一定存在： 首先$f^{‘}(\Theta;d)$可以展开为三部分，分别是损失函数、$L_{2,1} $罚函数、$L_{1}$罚函数在$\Theta$处的偏导。对于第一部分，可以直接求导： 第二部分$L_{2,1} $在$||\Theta_i·||_{2,1}≠0$时，其偏导存在而当$||\Theta_i·||_{2,1}= 0$时，则意味着参数矩阵的行参数$\Theta_{ij}=0,1≤j≤2m$，其方向导数为： 将以上两种情况整合到一起：类似地，对于$L_{1}$罚函数，其导数可以表示为：至此，我们证明了三个部分对于任意的$\Theta$和方向$d$，$f^{‘}(\Theta;d)$一定存在。 然后求解方向导数： 将问题转化为不等式约束优化问题:采用拉格朗日乘数法求解，其中$\mu$为拉格朗日乘子：求函数$L(d,\mu)$对$d$的偏导,令为零，可分为三种情况,过程略，最终求解得到如下： 其中： 至此我们得到了方向导数，接下里的问题就是如何利用方向导数来更新参数，文中使用LBFGS框架，并采取了类似于OWLQN算法的做法，接下来介绍一下论文是如何优化的。 3.2 LBFGS大多数的数值优化算法都是迭代式的，优化的目的就是希望参数序列收敛于$x^*$，从而使$f(x)$最优化。 在牛顿法中使用二阶泰勒展开来近似目标函数： 令$g_n$表示$f$在$x_n$处的梯度，$H_n$表示目标函数$f$在$x_n$处的Hessian矩阵，简化$f(x+\Delta x)$=$h_n(\Delta x)$求导令其为零，得到的$\Delta x$都是他的局部极值点，若$f$是凸函数，则$H$为正定矩阵，则此时局部最优即为全局最优。 得到如下更新式： 其中步长$a$可采用line search的方法，比如backtracking line search方法，即选用逐渐递减的步长。 其迭代算法伪代码如下： 但实践中，模型参数都是超高维度的，根本无法计算Hessian矩阵及其逆，BFGS就是为了解决这个问题，他使用了一种QuasiUpdate的策略生成$H_n^{-1}$的近似，伪代码如下： 其中$\{s_k\}$与$\{y_k\}$保存的是输入和梯度的变化量，即$s_k = x_k-x_{k-1}$，$y_k = g_k-g_{k-1}$,初值$H_0^{-1}$选取任意对称的正定矩阵。： 具体的QuasiUpdate那一步的推导略复杂，略过，最终BFGS的迭代伪代码如下： 其中 $\rho_n = (y_n^Ts_n)^{-1}$ 只要给定方向d，该算法可以直接计算出$H_n^{-1}$，却不需要求出$H_n^{-1}$。 LBFGS是对BFGS的改进，因为BFGS拟牛顿虽然无需计算海森矩阵，但仍然需要保存每次迭代的$s_n$与$y_n$的历史值。内存负担依然很重，L-BFGS是limited BFGS的缩写，简单地只使用最近的m个$s_n$与$y_n$记录值，以此来近似计算$H_n^{-1}g_n$ 在BFGS的迭代过程中，需要使用梯度信息，但因为我们的目标函数有$L_1$正则和$L_{2,1}$正则项，不是处处可导的，这时候OWLQN就上场了 3.3 OWLQNOWLQN算法在LBFGS框架下针对非凸非平滑函数优化主要做出了两点改进： 1、在不可导处用次梯度取代梯度 当模型参数不为零时，此时可导，按一般方法计算即可；当模型参数为零时，此时不可导，正则项的左偏导数为${ \partial }_{ i }^{ - }f(x)$，右偏导数${ \partial }_{ i }^{ + }f(x)$。 如何确定在零的位置的时候用左导数还是右导数呢？做法是防止象限穿越，通俗的讲就是在负象限的时候减去正值，在正象限的时候减去负值（加上正值） 2、在line search的时候做象限约束 当参数不在零点时，line search保持在参数所在象限内搜索；当参数在零点时，参数在次梯度约束的象限内进行line search 以下公式可以描述以上两点改进 3.4 MLR的优化算法MLR中使用的优化算法是从OWLQN改进过来的，主要有三个地方的变化： MLR使用方向导数来优化目标函数，而不是OWLQN的次梯度 MLR对更新方向p进行了象限约束：非正定时直接用方向导数作为搜索方向，否则要进行象限约束在方向导数所在象限内。 线性搜索的象限约束不同，当MLR参数不在零点时，line search保持在参数所在象限内搜索，在零点时，参数在方向导数约束的象限内进行line search 给定更新方向，MLR使用了 backtracking line search方法找到合适的步长$\alpha$，其迭代更新公式如下： MLR最终的优化算法伪代码如下： 四、分布式4.1 参数服务器 4.2 Common Feature Trick用户在一次浏览的过程中遇到多个广告，每个广告都会支持组成一条样本，这样会导致很多特征重复使用，比如用户的人口统计特征，用户基于历史行为的兴趣偏好等，为了解决重复计算的问题，文中提出在向量内积的过程中分成两部分： 从这个事实出发，训练就可以下列三个Trick： 训练过程中把有common Feature的样本放在一组，且仅仅保存一次以节省内存 优化参数时，Common Feature的只更新一次损失函数 最终效果如下，内存开销减少65%，时间开销减少91%，效果很明显： 五、高级特性在阿里妈妈团队写的MLR分享文章里面，提及了以下几点MLR的高级特性： 领域知识先验，这个分别体现在上文说到的dividing function和fitting function，他是基于领域的先验知识来设定dividing function和fitting function，比方说以用户的特征设计分片函数，以广告的特征设计拟合函数，这是基于不同人群具有聚类特性，同一类人群对广告有类似的偏好这样一个前提，此外这样的结构先验也有助于缩小解空间，更容易收敛! 加入线性偏置，因为特征的差异导致点击率天然存在一些差异，比如说位置和资源位，所以在损失函数中加入如公式所示的线性偏置，实践中对位置bias信息的建模，获得了4%的RPM提升效果。 模型级联，与LR模型级联式联合训练，实践中发现一些强feature配置成级联模式有助于提高模型的收敛性，比如以统计反馈类特征构建第一层模型，它的输出(如下图中的FBCtr)级联到第二级大规模稀疏ID特征体系中去，这样能够有助于获得更好的提升效果。 增量训练。实践证明，MLR通过结构先验进行pretrain，然后再增量进行全空间参数寻优训练，会获得进一步的效果提升。同时增量训练模式下模型达到收敛的步数更小，收敛更为稳定。在我们的实际应用中，增量训练带来的RPM增益达到了3%。 六、业务现状阿里妈妈团队主要将MLR主要应用于定向广告的CTR预估和定向广告的Learning to Match： 对于定向广告CTR，他们将用户画像特征、用户历史行为特征、广告画像特征组成2亿为的embedding向量，直接放到MLR模型中训练，并且采用了线性偏置、领域知识先验、增量训练的高级特征，提升了CTR预估的精度。 Learning to Match，即基于用户的人口属性、历史行为等信息来猜测用户可能感兴趣的广告集合，一般会使用规则匹配和协同过滤来做召回模块，阿里妈妈研发了基于MLR的learning to match算法框架，首先基于用户的行为历史来学习用户个性化兴趣，召回候选集，MLR框架很容易融合将不同的特征源、标签体系，省去交叉组合的成本，灵活性很高。 参考资料： Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction 数值优化：理解L-BFGS算法]]></content>
      <categories>
        <category>计算广告</category>
      </categories>
      <tags>
        <tag>MLR</tag>
        <tag>方向导数</tag>
        <tag>OWLQN</tag>
        <tag>广告CTR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算广告面面观（1）：经久不衰的逻辑回归]]></title>
    <url>%2F2019%2F06%2F19%2FAD-RS-1%2F</url>
    <content type="text"><![CDATA[推荐和广告已成为互联网公司的标配，这两者之间有相通的地方，推荐系统被广泛的应用于计算广告中，是其不可或缺的模块，当然推荐系统可以有更多其他的场景，比如淘宝京东的商品推荐、APP Store的应用推荐，今日头条的内容推荐等等，它需要兼顾媒体和用户这样个参与者；而广告作为互联网公司的核心业务模块，需要有推荐系统的支持，它将来自广告主的广告更有效的推荐给在媒体端观看的用户，串联的是广告主、媒体、用户三个参与方，必须要这三方玩的开心。广告的永恒目标是直接或者间接地帮助公司盈利，偏重商业，而推荐更多的是改善用户体验，提高留存，偏重产品。 这个系列，我们会从经典的点击率预估算法开始，包括LR、MLR、GBDT+LR、FM/FFM、DeepFM、Wide&amp;Deep、DIN、DIEN、PNN、NFM、AFM、DCN等等，层出不穷的算法模型正是为了因具体的业务场景而生，即使是很微小的提升，也会对公司的收益有很大的贡献。再之后深入到计算广告和推荐系统身后更为广义的业务相关的知识、系统的架构等等。这一系列的文章会邀请业界的前辈来撰写或者转载优质的总结，争取有一个完整的展示。 第一篇，我们先从逻辑回归模型开始。 虽然目前已经有很多深度学习模型可以在不同场景下获得不错的预测广告点击率，但仍然有很多公司的广告算法部门在使用逻辑回归模型，那为何在深度时代背景下传统的逻辑回归模型在点击率预估中仍然有其用武之地呢？可以归纳为以下几点： 逻辑回归的解释性很强。当业务人员问你ecpm是如何预估出来的，你就可以对产品说，我们的模型是通过对这里的N多个有业务意义的特征进行加权求和，然后再经过一个Sigmoid函数，映射到一个0到1的区间，就预估得到了我们所需的点击率，产品一定会对你的工作表示赞同与认可；此外，当bad case出现的时候，就可以轻松解释是什么特征在作祟影响结果，而那些深度模型，八个字归纳就是：玄学调参+玄学解释。 逻辑回归的训练并行方案比较成熟。广告点击率预测需要基于大规模的样本和特征来做的，一般是上亿级别，这个时候就需要用多台机器来分布式计算参数更新；此外在线更新的时候，如果是其他模型，需要动用所有的特征，而逻辑回归得益于其线性模型的优势，可以只更新那些有变动的特征，其余不变的特征缓存起来即可，这样就大大减少了存储压力和计算代价。 逻辑回归的简单易用使得很多公司并不会花太多的精力去升级需要耗费大量精力和计算代价的深度模型，而且在数据量极大的时候，逻辑回归与其他深度模型的预测差异并不见得那么大，所以，何苦要把自己往费时费精力费钱的处境推呢；要知道，高性能机器并不是每个公司都可以承受的。算法工程师就可以把更多的时间用于特征工程，当然这两者也是一个trade-off； 接下来细致的看一看逻辑斯蒂回归具体的解释。 一、逻辑斯谛分布介绍逻辑斯谛回归模型之前，首先看一个并不常见的概率分布，即逻辑斯谛分布。设$X$是连续随机变量，$X$服从逻辑斯谛分布是指$X$具有下列的分布函数和密度函数： F\left(x\right)=P\left(X\le x\right)=\frac{1}{1+e^{-\left(x-\mu\right)/\gamma}} f\left(x\right)=F^,\left(x\right)=\frac{e^{-\left(x-\mu\right)/\gamma}}{\gamma\left(1+e^{-\left(x-\mu\right)/\gamma}\right)^2}式中，$\mu$为位置参数，$\gamma&gt;0 $为形状参数。逻辑斯谛的分布的密度函数$f(x)$和分布函数$F(x)$的图形如下图所示。其中分布函数属于逻辑斯谛函数，其图形为一条$S$形曲线。该曲线以点$(\mu,\frac{1}{2})$为中心对称，即满足 F\left(-x+\mu\right)-\frac{1}{2}=-F\left(x+\mu\right)+\frac{1}{2}曲线在中心附近增长较快，在两端增长速度较慢。形状参数$\gamma$的值越小，曲线在中心附近增长得越快。 二、逻辑斯谛回归模型线性回归的应用场合大多是回归分析，一般不用在分类问题上。原因可以概括为以下两个： 1）回归模型是连续型模型，即预测出的值都是连续值（实数值），非离散值； 2）预测结果受样本噪声的影响比较大。 2.1 LR模型表达式LR模型表达式为参数化的逻辑斯谛函数（默认参数$\mu=0,\gamma=1$）,即 h_{\theta}\left(x\right)=\frac{1}{1+e^{-\theta^Tx}}其中$h_\theta{(x)}$作为事件结果$y=1$的概率取值。这里,$x\in R^{n+1},y\in \{1,0\},\theta\in R^{n+1}$是权值向量。其中权值向量$w$中包含偏置项，即$w=(w_0,w_1,···,w_n)，x=(1,x_1,x_2,···,x_n)$ 2.2 理解LR模型2.2.1 对数几率一个事件发生的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是$p$，那么该事件的几率为$\frac{p}{1-p}$，该事件的对数几率（log odds）或logit函数是： logit\left(p\right)=\log\frac{p}{1-p}对LR而言，根据模型表达式可以得到： \log\frac{h_{\theta}\left(x\right)}{1-h_{\theta}\left(x\right)}=\theta^Tx即在LR模型中，输出$y=1$的对数几率是输入$x$的线性函数。或者说输出$y=1$的对数几率是由输入$x$的线性函数表示的模型，即LR模型 2.2.2 函数映射除了从对数几率的角度理解LR外，从函数映射也可以理解LR模型。 考虑对输入实例$x$进行分类的线性表达式$\theta^T$，其值域为实数域。通过LR模型表达式可以将线性函数$\theta^Tx$的结果映射到(0,1)区间，取值表示为结果为1的概率（在二分类场景中）。 线性函数的值越接近于正无穷大，概率值就越接近1；反之，其值越接近于负无穷，概率值就越接近0。这样的模型就是LR模型。 LR本质上还是线性回归，知识特征到结果的映射过程中加了一层函数映射（即sigmoid函数），即先把特征线性求和，然后使用sigmoid函数将线性和约束至（0，1）之间，结果值用于二分或回归预测。 2.2.3 概率解释LR模型多用于解决二分类问题，如广告是否被点击（是/否）、商品是否被购买（是/否）等互联网领域中常见的应用场景。但是实际场景中，我们又不把它处理成“绝对的”分类问题，而是用其预测值作为事件发生的概率。 这里从事件、变量以及结果的角度给予解释。 我们所能拿到的训练数据统称为观测样本。问题：样本是如何生成的？ 一个样本可以理解为发生的一次事件，样本生成的过程即事件发生的过程。对于0/1分类问题来讲，产生的结果有两种可能，符合伯努利试验的概率假设。因此，我们可以说样本的生成过程即为伯努利试验过程，产生的结果（0/1）服从伯努利分布。这里我们假设结果为1的概率为$h_\theta{(x)}$，结果为0的概率为$1-h_\theta{(x)}$。 那么对于第$i$个样本，概率公式表示如下： P(y^{(i)}=1|x^{(i)};\theta )=h_\theta{(x^{(i)})}$$$$P(y^{(i)}=0 |x^{(i)};\theta )=1- h_\theta{(x^{(i)})}将上面两个公式合并在一起，可得到第$i$个样本正确预测的概率： P(y^{(i)}|x^{(i)};\theta)=(h_\theta(x^{(i)})^{y(i)})·（1-h_\theta(x^{(i)}))^{1-y(i)}上式是对一个样本进行建模的数据表达。对于所有的样本，假设每条样本生成过程独立，在整个样本空间中（N个样本）的概率分布（即似然函数）为： P\left(Y|X;\theta\right)=\prod_{i=1}^N{\left(h_{\theta}\left(x^{\left(i\right)}\right)^{y^{\left(i\right)}}\left(1-h_{\theta}\left(x^{\left(i\right)}\right)^{1-y^{\left(i\right)}}\right)\right)}通过极大似然估计（Maximum Likelihood Evaluation，简称MLE）方法求概率参数。具体地，第三节给出了通过随机梯度下降法（SGD）求参数。 三、模型参数估计3.1 Sigmoid函数 上图所示即为sigmoid函数，它的输入范围为$-\infty\rightarrow +\infty$，而值域刚好为$(0,1)$，正好满足概率分布为$(0,1)$的要求。用概率去描述分类器，自然要比阈值要来的方便。而且它是一个单调上升的函数，具有良好的连续性，不存在不连续点。 此外非常重要的，sigmoid函数求导后为：以下的推导中会用到，带来了很大的便利。 3.2 参数估计推导上一节的公式不仅可以理解为在已观测的样本空间中的概率分布表达式。如果从统计学的角度可以理解为参数$\theta$似然性的函数表达式（即似然函数表达式）。就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大。参数在整个样本空间的似然函数可表示为： L\left(\theta\right)=P\left(\overrightarrow{Y}|X;\theta\right) =\prod_{i=1}^N{P\left(y^{\left(i\right)}\parallel x^{\left(i\right)};\theta\right)} =\prod_{i=1}^N{\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)^{y\left(i\right)}\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)^{1-y^{\left(i\right)}}}为了方便参数求解，对这个公式取对数，可得对数似然函数： l\left(\theta\right)=\sum_{i=1}^N{\log l\left(\theta\right)} =\sum_{i=1}^N{y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)}最大化对数似然函数其实就是最小化交叉熵误差（Cross Entropy Error）。先不考虑累加和，我们针对每一个参数$w_j$求偏导： \frac{\partial}{\partial\theta_j}l\left(\theta\right)=\left(y\frac{1}{h_{\theta}\left(x\right)}-\left(1-y\right)\frac{1}{1-h_{\theta}\left(x\right)}\right)\frac{\partial}{\partial\theta_j}h_{\theta}\left(x\right) =\left(\frac{y\left(1-h_{\theta}\left(x\right)\right)-\left(1-y\right)h_{\theta}\left(x\right)}{h_{\theta}\left(x\right)\left(1-h_{\theta}\left(x\right)\right)}\right)h_{\theta}\left(x\right)\left(1-h_{\theta}\left(x\right)\right)\frac{\partial}{\partial\theta_j}\theta^Tx =\left(y-h_{\theta}\left(x\right)\right)x_j最后，通过扫描样本，迭代下述公式可求得参数： \theta_j:=\theta_j+a\left(y^{\left(i\right)}-h_{\theta}\left(x^{\left(i\right)}\right)\right)x_{j}^{\left(i\right)}其中$a$表示学习率，又称学习步长。此外还有Batch GD，共轭梯度，拟牛顿法（LBFGS），ADMM分布学习算法等都可以用来求解参数。另作优化算法一章进行补充。 以上的推导是LR模型的核心部分，在机器学习相关面试中，LR模型公式推导可能是考察频次最高的一个点。要将其熟练推导。 3.3 分类边界知道如何求解参数后，我们看一下模型得到的最后结果是什么样的。假设我们的决策函数为： y^∗=1, \ \ if \ \ P(y=1|x)>0.5选择0.5作为阈值是一个一般的做法，实际应用时特定的情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。很容易看出，当$\theta ^Tx&gt;0$时，$y=1$，否则$y=0$。$\theta ^Tx=0$是模型隐含的分类平面（在高维空间中，我们说是超平面）。所以说逻辑回归本质上是一个线性模型，但是这不意味着只有线性可分的数据能通过LR求解，实际上，我们可以通过特征变换的方式把低维空间转换到高维空间（kernel trick），而在低维空间不可分的数据，到高维空间中线性可分的几率会高一些，这就涉及到了特征的交叉组合，这一块需要耗费许多人力去做人工特征组合。 值得阅读的相关文章 前深度学习时代CTR预估模型的演化之路 CTR预估[二]: Algorithm-Naive Logistic Regression 科普｜大家都看得懂的CTR预估解析 计算广告系统算法与架构综述]]></content>
      <categories>
        <category>计算广告</category>
      </categories>
      <tags>
        <tag>LR</tag>
        <tag>计算广告</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis笔记（2）]]></title>
    <url>%2F2019%2F06%2F19%2FRedis-2%2F</url>
    <content type="text"><![CDATA[Redis 命令Redis 命令用于在 Redis 服务上执行操作，我们使用 Redis 服务自带的 redis-cli 客户端来发送命令，最新版的 redis-cli 会有命令提示功能，比较方便。 启动 redis-cli 客户端 1$ redis-cli 范例 下面的范例演示了如何启动 redis 客户端，并发送 ping 命令 1234$ redis-cli127.0.0.1:6379&gt;127.0.0.1:6379&gt; PINGPONG PING 命令用于检测 Redis 服务是否启动 使用 redis-cli 在远程 Redis 服务上执行命令启动远程 redis-cli 语法 1$ redis-cli -h host -p port -a password 下面的范例演示了如何连接到主机为 192.168.1.100，端口为 6379 ，密码为 123456 的 Redis 服务上 123192.168.1.100&gt;192.168.1.100&gt; PINGPONG Redis 键(key) 命令Redis 键相关的命令用于管理 redis 的键，Redis 键命令的基本语法如下： 1127.0.0.1:6379&gt; COMMAND KEY_NAME 范例 1234127.0.0.1:6379&gt; set site plushunter.github.ioOK127.0.0.1:6379&gt; del site(integer) 1 DEL 是一个命令，用来删除一个键 site如果键被删除成功，命令执行后输出 (integer) 1 ，否则将输出 (integer) 0 Redis keys 命令]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis笔记（1）]]></title>
    <url>%2F2019%2F06%2F19%2FRedis-1%2F</url>
    <content type="text"><![CDATA[Redis简介Redis ( Remote Dictionary Server ) 是由 Salvatore Sanfilippo 开发的 key-value 缓存数据库。 Redis 是完全开源免费的，遵守 BSD 协议，是一个高性能的 key-value 数据库，与其它 key/value 缓存产品有以下三个特点： Redis 支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用，在对不同数据集进行高速读写时需要权衡内存，因为数据量不能大于硬件内存。 Redis 不仅支持 key-value 类型的数据，还提供list，set，zset，hash等数据结构的存储，对程序员透明，无需进行额外的抽象 Redis 支持数据的备份，即 master-slave 模式的数据备份 Redis的优势 高性能 ： Redis 能读的速度是 110000次/s ,写的速度是 81000次/s 丰富的数据类型：支持 Strings, Lists, Hashes, Sets 及 Ordered Sets数据类型操作 原子型操作 : Redis的所有操作都是原子性的，还支持对几个操作合并后的原子性执行 丰富的特性 : Redis 支持 publish/subscribe, 通知, key过期等等特性 Redis安装Mac OS 下安装 1$ brew install redis 查看安装的 redis-server 版本 12$ redis-server --versionRedis server v=4.0.2 sha=00000000:0 malloc=libc bits=64 build=993aa70a2300c21e 启动 Redis 1$ redis-server 检查 redis 是否启动？ 1$ redis-cli 运行以上命令将打开以下终端 1127.0.0.1:6379&gt; 127.0.0.1 是本机 IP ，6379 是 redis 服务端口 现在输入 PING 命令 12127.0.0.1:6379&gt; pingPONG 以上说明我们已经成功安装了 Redis Redis Cofig配置Redis 提供了很多配置选项来优化 Redis 服务，配置文件位于 Redis 安装目录下 /usr/local/etc/，文件名为 redis.conf；可以通过 Redis CONFIG 命令查看或设置配置项 Redis CONFIG GET 命令语法格式 1CONFIG GET CONFIG_SETTING_NAME 范例 1234127.0.0.1:6379&gt; CONFIG GET loglevel1) &quot;loglevel&quot;2) &quot;notice&quot; 可以使用 * 号获取所有的 Redis 配置 123456789127.0.0.1:6379&gt; CONFIG GET * 1) &quot;dbfilename&quot; 2) &quot;dump.rdb&quot; 3) &quot;requirepass&quot; 4) &quot;&quot; 5) &quot;masterauth&quot; 6) &quot;&quot; ... ... 编辑配置 Redis CONFIG SET 命令用来设置配置选项，命令语法格式如下: 1CONFIG SET CONFIG_SETTING_NAME NEW_CONFIG_VALUE 范例 123456127.0.0.1:6379&gt; CONFIG SET loglevel &quot;notice&quot;OK127.0.0.1:6379&gt; CONFIG GET loglevel1) &quot;loglevel&quot;2) &quot;notice&quot; redis.conf 配置选项Redis配置文件redis.conf常见配置项说明如下，位于/usr/local/etc/redis.conf 1、daemonizeRedis 默认不是以守护进程的方式运行，可以通过该配置项修改，使用 yes 启用守护进程 12# daemonize yes|nodaemonize yes 2、pidfileRedis 以守护进程方式运行时，Redis 默认会把 pid 写入 /var/run/redis.pid 文件 可以通过 pidfile 项指定 1pidfile /var/run/redis.pid 3、port 指定 Redis 监听端口，默认端口为 6379 6379 的典故: Redis 作者曾经解释了为什么选用 6379 作为默认端口：因为 6379 在手机按键上 MERZ 对应的号码，而 MERZ 取自意大利歌女 Alessia Merz 的名字 1port 6379 4、loglevel 指定日志记录级别Redis 支持四个级别：debug、verbose、notice、warning，默认为 verbose 1loglevel verbose 5、logfile 日志记录方式，默认为标准输出如果配置 Redis 为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给 /dev/null 1logfile stdout Redis 数据类型Redis 比 Memcached 更优秀的地方之一就是支持更丰富的数据类型 Redis 支持七种数据类型 string ( 字符串 ) hash ( 哈希 ) list ( 列表 ) set ( 集合 ) zset ( sorted set：有序集合 ) Bitmaps ( 位图 ) HyperLogLogs ( 基数统计 ) String（字符串）string 是 Redis 最基本的数据类型，一个 key 对应一个 value，string 类型是二进制安全的，Redis 的 string 可以包含任何数据，比如 jpg 图片或者序列化的对象，其中string 类型的一个键最大能存储 512 MB 数据。 1234127.0.0.1:6379&gt; set site &quot;plushunter.github.io&quot;OK127.0.0.1:6379&gt; get site&quot;plushunter.github.io&quot; 上面的范例中我们使用了 Redis 的 SET 和 GET 命令 Hash（哈希）Redis Hash 是一个键名对集合，是一个 string 类型的 field 和 value 的映射表，它特别适合用于存储对象，其中每个 hash 可以存储 $2^{32}-1$ 键值对（40多亿） 123456127.0.0.1:6379&gt; HMSET user:1 name huaz age 27OK127.0.0.1:6379&gt; HGET user:1 name&quot;huaz&quot;127.0.0.1:6379&gt; HGET user:1 age&quot;27&quot; 上面的范例中 hash 数据类型存储了包含用户脚本信息的用户对象 范例中我们使用了 Redis HMSET, HGETALL 命令， user:1 为键 List（列表）Redis List ( 列表 ) 是简单的字符串列表，按照插入顺序排序，可以添加一个元素到列表的头部 ( 左边 ) 或者尾部 ( 右边 )，最多可存储$2^{32}-1$ 个元素 (4294967295, 每个列表可存储40多亿) 1234567891011121314151617181920127.0.0.1:6379&gt; lpush database redis(integer) 1127.0.0.1:6379&gt; lpush database mysql(integer) 2127.0.0.1:6379&gt; lpush database rabitmq(integer) 3127.0.0.1:6379&gt; lpush database mongodb(integer) 4127.0.0.1:6379&gt; lpush database rabitmq(integer) 5127.0.0.1:6379&gt; lrange database 0 21) &quot;rabitmq&quot;2) &quot;mongodb&quot;3) &quot;rabitmq&quot;127.0.0.1:6379&gt; lrange database 0 101) &quot;rabitmq&quot;2) &quot;mongodb&quot;3) &quot;rabitmq&quot;4) &quot;mysql&quot;5) &quot;redis&quot; Set（集合）Redis Set 是 string 类型的无序集合，通过哈希表实现的，所以添加，删除，查找的时间复杂度都是 O(1)，最大的成员数为$2^{32}-1$ (4294967295, 每个集合可存储40多亿个成员)。Redis Set 内元素具有唯一性，不管插入多少次都只会有一份。 123456789101112131415127.0.0.1:6379&gt; sadd databases redis(integer) 1127.0.0.1:6379&gt; sadd databases mysql(integer) 1127.0.0.1:6379&gt; sadd databases mongodb(integer) 1127.0.0.1:6379&gt; sadd databases rabitmq(integer) 1127.0.0.1:6379&gt; sadd databases rabitmq(integer) 0127.0.0.1:6379&gt; smembers databases1) &quot;mongodb&quot;2) &quot;rabitmq&quot;3) &quot;mysql&quot;4) &quot;redis&quot; 上面的范例，rabitmq 添加了两次，但最后只存储了一份 zset ( sorted set：有序集合 )Redis zset和set一样也是string类型元素的集合,不同的是每个元素都会关联一个 double 类型的分数,并通过分数来为集合中的成员进行从小到大的排序，成员是唯一的，但分数( score ) 却可以重复。 Redis zset 添加元素到集合，如果元素在集合中存在则更新对应 score Redis zadd 命令语法格式 1zadd key score member Redis zset 范例 12345678910127.0.0.1:6379&gt; zadd language 1 mysql(integer) 1127.0.0.1:6379&gt; zadd language 3 redis(integer) 1127.0.0.1:6379&gt; zadd language 2 python(integer) 1127.0.0.1:6379&gt; ZRANGEBYSCORE language 0 101) &quot;mysql&quot;2) &quot;python&quot;3) &quot;redis&quot; Redis Bitmap ( 位图 )Redis Bitmap 通过类似 map 结构存放 0 或 1 ( bit 位 ) 作为值，可以用来统计状态，如 日活，是否浏览过某个东西 Redis setbit 命令：Redis setbit 命令用于设置或者清除一个 bit 位 Redis setbit 命令语法格式 1SETBIT key offset value 范例 123456127.0.0.1:6379&gt; setbit user:001 10000 1(integer) 0127.0.0.1:6379&gt; setbit user:001 10 0(integer) 0127.0.0.1:6379&gt; setbit user:001 10 2 # 如果值不是0或1就报错(error) ERR bit is not an integer or out of range HyperLogLogs ( 基数统计 )Redis HyperLogLog 可以接受多个元素作为输入，并给出输入元素的基数估算值。 基数：集合中不同元素的数量，比如 {‘apple’, ‘banana’, ‘cherry’, ‘banana’, ‘apple’} 的基数就是 3 估算值：算法给出的基数并不是精确的，可能会比实际稍微多一些或者稍微少一些，但会控制在合理的范围之内，HyperLogLog 的优点是即使输入元素的数量或者体积非常非常大，计算基数所需的空间总是固定的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近$2^64$个不同元素的基数，这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素 Redis PFADD 命令将元素添加至 HyperLogLogRedis PFADD 命令语法格式 1PFADD key element [element ...] 范例 12345678127.0.0.1:6379&gt; PFADD unique::ip::counter &quot;1.2.3.4&quot;(integer) 1127.0.0.1:6379&gt; PFADD unique::ip::counter &quot;127.0.0.1&quot;(integer) 1127.0.0.1:6379&gt; PFADD unique::ip::counter &quot;255.255.255.255&quot;(integer) 1127.0.0.1:6379&gt; PFCOUNT unique::ip::counter(integer) 3]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark笔记（1）：RDD编程]]></title>
    <url>%2F2019%2F06%2F19%2FSpark-1%2F</url>
    <content type="text"><![CDATA[Spark笔记系列我们准备以《Spark大数据分析》这本书的总体框架为主线，从RDD编程的核心概念说起，到基本的RDD操作、数据IO、Spark Job，以及Spark SQL、Spark Streaming、Spark MLlib这些Spark组件，结合实例系统的进行讲解，之后会将其延伸开来，争取照顾到Spark的方方面面。 “Apache Spark is a unified analytics engine for large-scale data processing.”这是来自官网的介绍，Spark是一个用于大数据处理的统一分析引擎，上亿的大数据集在单机上跑一个分析几乎不可能，而在Spark上可以以分钟级别的速度就可以完成，这要归功于其先进的调度程序DAG、查询优化器和物理执行引擎，这几个概念在后边会一一介绍，总而言之就是Spark出乎意料的快。除了性能好，Spark还异常亲民，你可以用Java写、用Scala写、用Python写，同时也支持R、SQL，上手非常简单，用惯了python DataFrame的可以在Spark找到对应的DataFrames库，用惯了SQL的分析员也可以在里头找到SQL，机器学习工程师也照样可以使用MLlib进行建模。 在Spark中，有一个核心概念叫RDD（Resilient Distributed Dataset），基本所有的操作都是围绕其展开的，所以第一节我们先讲解RDD编程的核心概念和基本操作，当然这之前要先按照官网提供的安装教程进行安装好Spark。如果没有集群，可以先在单机版上练习。 RDD 核心概念Spark中，所有的数据操作归纳起来就三种： RDD的创建（create） RDD的转化（transformation） RDD的行动（action） 那什么是RDD呢？ RDD是Resilient Distributed Dataset的简称，首先Dataset意味着RDD是一个数据集，但它与我们常见的数据集格式不同，他是弹性的（Resilient），如何理解弹性，就是在集群上的某一节点失效时可以高效地重建数据集，RDD就像海绵一样有弹性似的，在被挤压之后仍可以恢复完整，即是容错的。 在容错这一点上，Spark采用了记录数据更新而不是数据检查点的方式，因为数据检查点方式会消耗大量的存储资源，但若更新达到一定的数量，记录数据更新的成本也很高。因此，RDD只支持粗粒度的转化，我们后面会看到RDD都是在大量记录上执行的单个操作。 而分布式（Distributed）指的是每个RDD都被分为多个分区，这些分区运行在集群中的不同节点上，这样就保证了其负载均衡（多台机器负载）、扩展性强（多台机器扩展）的优点。 以上这些概念比较抽象，我们来看一个具体的RDD从创建到转化再到行动的实例，从实例出发了解RDD编程: 假如我们有一个存储在HDFS上的文件，其路径为hdfs:///adalgo/profile/20190615/*，每一行的格式如下所示，\t左边为用户id，\t右边为用户的兴趣分类（单个人没有重复的key），以json格式存储。假设总共有一亿多行这样的数据（id没有重复），我们想要得到的最终结果是每一个类别的人数。 1[&apos;00000f9d\t&#123;&quot;cat&quot;:&#123;&quot;football&quot;:2,&quot;basketball&quot;:3&#125;] 以下是本节作为示例的代码： 12345678910111213141516171819%pysparkimport json## 创建profile = sc.textFile("hdfs:///adalgo/profile/20190615/*")## 转化profile_2 = profile.filter(lambda x:len(x.split('\t'))==2).map(lambda x: x.split('\t')[1]).map(lambda x: json.loads(x)).map(lambda x:' '.join(x['ad_scat'].keys()))## 行动profile_2.take(100)## 转化scat = profile_2.flatMap(lambda x:x.split(' '))scat_count = ad_scat.map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y)ad_scat_count.toDF().orderBy("_2")## 行动scat_count.show(100) 接下来我们一边讲解概念，一边解读这段代码。 RDD 创建一般我们可以通过两种方式创建RDD。 一种是在驱动器程序中并行的对象集合，简单说就是一个我们希望传入的列表或者元组： num_rdd = sc.parallelize([1,2,3]) 忽然冒出了一个sc，这是一个什么玩意儿？ 每一个spark程序都是由driver program发起集群并行操作的，当我们启动了spark shell时，就自动创建了一个SparkContext对象，即sc变量，可以用它来创建RDD。 但这种方式一般就是在平时测试的时候用，在真实生产场景中很少用到，因为这种方式需要将整个数据集先放到driver程序所在的机器的内存中。 另一种是从外部存储中读取数据，比如HDFS/Amazon S3，具体的会在后面的章节介绍，我们的程序实例中就是从HDFS文件系统读取的数据集： 1user_ad_profile = sc.textFile(&quot;hdfs:///adalgo/profile/20190615/*&quot;) 这样我们就可以接着对这个RDD进行操作了。 RDD 操作与惰性求值RDD有两种操作：转化操作和行动操作。那如何定义转化和行动操作呢？转化操作返回的是一个新的RDD，而行动操作则是返回结果或把结果写入外部系统，触发实际的计算。 这样说还是有点抽象，看实例中就是一连串的转化操作，首先.filter(lambda x:len(x.split(&#39;\t&#39;))==2)是从user_ad_profile_1中筛选出用\t分隔后长度为2的行，把一些不合法的行也去掉，否则后边就会报错；之后的.map(lambda x: x.split(&#39;\t&#39;)[1])是取出第2个元素，用.map(lambda x: json.loads(x))来加载json格式元素，在最后用.map(lambda x:&#39; &#39;.join(x[&#39;ad_scat&#39;].keys()))取出自字典所有的key返回一个空格分隔的列表，具体的每个函数的作用我们会详细介绍。以上这些操作返回的其实都是经过转化操作作用在各个元素上然后生成一个新的RDD，但并没有执行真正的计算操作。这就是之后会提到的惰性求值。 12## 转化profile_2 = profile.filter(lambda x:len(x.split(&apos;\t&apos;))==2).map(lambda x: x.split(&apos;\t&apos;)[1]).map(lambda x: json.loads(x)).map(lambda x:&apos; &apos;.join(x[&apos;ad_scat&apos;].keys())) 而行动操作就是会有真正的计算，比如下面.take(100)是从profile_2中返回200个元素。 12## 行动profile_2.take(100) 这里面有一个很重要的概念：惰性求值。比如在函数式编程语言中，表达式往往不在它被绑定到变量之后就立即求值，而是在该值被取用的时候求值，对应到Spark中就是在调用行动操作之前Spark不会开始计算，无论是读取数据还是转化操作，都是如此。 在执行操作时，Spark会记录下当前执行操作的指令列表：在读取数据时，Spark即使执行了sc.textFile()的操作，也不会真正的读取数据出来，等到行动操作时才会读取。在执行转化操作时，操作也不会立即执行，它只是记录了一个计算操作的指令列表。 那为何要惰性求值呢？因为如果每经过一次转化操作都触发真正的计算，将会有系统负担，而惰性求值会将多个转化操作合并到一起，抵消不必要的步骤后，在最后必要的时才进行运算，获得性能的提升同时又减轻系统运算负担。 接下来让我们看一下常见的一些转化操作和行动操作。 RDD 转化操作 基本转化操作：以num_rdd = sc.parallelize([‘Hello New World’,’Hello China’])为例 函数名 目的 示例 结果 map() 将函数应用于每一个元素中，返回值构成新的RDD num_rdd.map(lambda x: x.lower()) [‘hello new world’,’hello china’] flatMap() 把元素内容铺展开来，将函数作用于所有的元素内容 num_rdd.flatMap(lambda x: x.split(‘ ‘)) [‘Hello’,’New’,’World’,’Hello’,’China’] filter() 元素过滤 num_rdd.map(lambda x:len(x.split(‘ ‘))==2) [‘Hello China’] distinct() 去重 num_rdd.distinct() [‘Hello New World’,’Hello China’] 集合转换操作，以rdd_1=[1,2,3],rdd_2=[3,4,5]为例 函数名 目的 示例 结果 union() 合并两个RDD所有元素（不去重） rdd1.union(rdd2) [1,2,3,3,4,5] intersection() 求两个RDD的交集 rdd_1.intersection(rdd2) [3] substract() 移除在RDD2中存在的RDD1元素 rdd_1.substract(rdd2) [1,2] cartesian() 求两个RDD的笛卡尔积 rdd_1.cartesian(rdd2) [(1,3),(1,4),(1,5)…(3,5)] RDD 行动操作基本行动操作，以rdd = [1,2,3,3]为例 函数名 目的 示例 结果 collect() 收集并返回RDD中所有元素，往往在单元测试时使用，要求数据可放入单台机器内存 rdd.collect() [1,2,3,3] count() RDD中元素的个数 rdd.count() 4 countByValue() 各元素出现的个数 rdd.countByValue() [(1,1),(2,1),(3,2)] take(num) 从RDD中返回前num个元素，用于单元测试和快速调试 rdd.take(2) [1,2] top(num) 返回降序排序最前面的num个元素， rdd.take(2) [3,3] reduce(f) 并行整合RDD中所有元素，返回一个同一类型元素 rdd.reduce(lambda x: x+y ) 9 fold(zeroValue)(f) 与reduce一样，不过需要提供初始值 rdd.fold(0)(lambda x,y: x+y ) 9 aggregate(zeroValue)(seqOp , combOp) 与reduce相似，不过返回不同类型的元素 rdd.aggregate((0,0)) (lambda x,y: (x[0] + y, x[1] + 1), lambda x,y: (x[0] + y[0], x[1] + y[1] )) [9,4] foreach(f) 给每个元素使用给定的函数，结果不需发回本地 rdd.foreach(f) 无 其中aggregate和fold函数的理解会稍难一些，可以查阅资料深入了解。 持久化上面说到，RDD是惰性求值的，而我们会重复使用同一个RDD，而如果简单的对其调用行动操作，Spark每次都会重算RDD，资源消耗很大。比如加入我们上述说的实例中，有以下的执行： 123input = sc.parallelize([1,2,3])print result.count()print result.collect() 这就多次计算同一个RDD了，为了避免这种情况，我们可以使用persist()对其持久化存储，而且可以根据需求选择不同的持久化级别，一般内存成分多的速度会快一些，磁盘部分多的速度稍慢一些，如下： 级别 使用空间 CPU时间 是否在内存中 是否在磁盘上 MEMORY_ONLY 高 低 是 否 MEMORY_ONLY_SER 低 高 是 否 MEMORY_AND_DISK 高 中 部分 部分 MEMORY_AND_DISK_SER 低 高 部分 部分 DISK_ONLY 低 高 否 是 比如下面的这个例子： 1234input = sc.parallelize([1,2,3])result = input.persist(StorageLevel.DISK_ONLY)print result.count()print result.collect() 若想把持久化的RDD从缓存中移除，可以使用unpersist()方法。 实例讲解最后把一开始的实例讲解一下： 12345678910111213141516171819%pysparkimport json## 创建profile = sc.textFile("hdfs:///adalgo/profile/20190615/*")## 转化profile_2 = profile.filter(lambda x:len(x.split('\t'))==2).map(lambda x: x.split('\t')[1]).map(lambda x: json.loads(x)).map(lambda x:' '.join(x['ad_scat'].keys()))## 行动profile_2.take(100)## 转化scat = profile_2.flatMap(lambda x:x.split(' '))scat_count = ad_scat.map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y)ad_scat_count.toDF().orderBy("_2")## 行动scat_count.show(100) 首先.filter(lambda x:len(x.split(&#39;\t&#39;))==2)是从user_ad_profile_1中筛选出用\t分隔后长度为2的行，把一些不合法的行也去掉，否则后边就会报错；之后的.map(lambda x: x.split(&#39;\t&#39;)[1])是取出第2个元素，用.map(lambda x: json.loads(x))来加载json格式元素，在最后用.map(lambda x:&#39; &#39;.join(x[&#39;ad_scat&#39;].keys()))取出自字典所有的key返回一个空格分隔的列表。 profile_2.flatMap(lambda x:x.split(&#39; &#39;))是把所有单词分割开，铺展开为一个列表；ad_scat.map(lambda x:(x,1))则对单词进行单个计数，即都记为1；reduceByKey(lambda x,y: x+y)是根据单词作为key，对其总计数；.toDF().orderBy(&quot;_2&quot;)则是将其DataFrame化，并按第二列排序，这样就完成了一个简单的单词计数，同时也是各个类别人数计数的功能。 以上就是Spark RDD中最重要的一些概念讲解，包括RDD的基本概念、创建、转化操作、行动操作与持久化，下一章节我们会继续跟着快速大数据分析一起看一下Spark并行聚合、分组操作，即键值对的操作。 参考资料：《Spark快速大数据分析》]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>RDD编程</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[林中路（9）：阿玛蒂亚·森 | 身份与暴力：命运的幻象]]></title>
    <url>%2F2019%2F04%2F20%2F%E6%9E%97%E4%B8%AD%E8%B7%AF%EF%BC%889%EF%BC%89%EF%BC%9A%E9%98%BF%E7%8E%9B%E8%92%82%E4%BA%9A%C2%B7%E6%A3%AE%20%7C%20%E8%BA%AB%E4%BB%BD%E4%B8%8E%E6%9A%B4%E5%8A%9B%EF%BC%9A%E5%91%BD%E8%BF%90%E7%9A%84%E5%B9%BB%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[几年前，我经过一次短暂的国外旅行回到英国（当时我任剑桥大学三一学院的院长），伦敦希思罗机场的移民局官员在极其细致地查看了我的印度护照后，提出了一个从哲学角度来看颇为棘手的问题。他注视着我在入境单上所填的家庭住址（剑桥三一学院院长公寓），问该院长——我肯定与院长有着很好的关系——是不是我的一个亲密朋友。他的问题让我犹豫了片刻，因为我不知道我自己能否称得上是我自己的朋友。思索片刻后，我得出结论，回答应该是肯定的，因为我对待自己一向不赖。并且，即使有时我说错了什么话，像我自己这样的朋友，对自己也没有任何恶意。但由于我迟疑了片刻才给出答案，移民局官员希望知道我犹豫的原因，尤其是他想了解清楚我是否不合法地居住在英国。 这一具体的事情最终得到了解决，但那次与移民局官员的谈话却不时提醒我，身份（identity）是一个极其复杂的问题。当然，我们不难相信，一个事物总是等同于其自身。维特根斯坦（Wittgenstein），这位伟大的哲学家曾指出，再也没有比“某物等同于其自身”这样的说法“更为绝妙但毫无用处的命题”了。但他又接着说，这种命题，尽管完全无益，但仍然“与某种想象的发挥相关”。 当我们从“自身认同”转到“与某个特殊群体中的他人认同”（这是社会认同所最常采取的形式）这个问题上时，问题的复杂性将大大增加。确实，许多当代的政治和社会争端都与有着不同身份认同的不同群体所提出的相互对立的要求有关。因为关于自己身份的观念以各种方式影响着我们的思想和行动。 过去几年来，世界各地的暴行和暴力事件不但引发了难解的冲突，而且也导致了极度的思想混乱。全球政治对立往往被视为世界宗教和文化对立的必然结果。确实，即使不是那么明确，这个世界已日益被视作各种宗教或文化的联盟，而人们的其他身份则被完全忽视。隐含在这种思维路径之中的是这样一个古怪的假设，即可以根据某种“单一而又涵括一切的标准”来将世界上所有的人加以分类。对世界人口的这种宗教或文化分类导致了一种人类身份的“单一主义”（solitarist）认识，这种认识将人们视为仅仅属于某一单个群体（或是像现在这样根据宗教或文化区分不同人群，或是像以往那样按照民族和阶级来加以区分）。 单一主义的认识往往容易导致对世界上几乎每一个人的误解。在我们的日常生活中，我们把自己视为各种各样的群体的成员——我们属于其中的每一个。同一个人可以毫不矛盾地既是美国公民，又是来自加勒比地区，还可以拥有非洲血统；此外，还可以是一名基督徒、自由主义者、女性、小说家、女权主义者、异性恋者、一个主张同性恋者有权利自行其是的人、戏剧爱好者、环保积极分子、网球迷、爵士乐弹奏家；而且坚信外层空间也有智慧生物存在，并迫切渴望与他们交流（最好是用英语）。上述的每一个群体——她同时属于这些群体——都给予她一种特殊的身份。没有一种能够算得上是该人唯一的或单一的成员资格或身份。既然我们不可避免地拥有多重身份，在每一情况下，我们必须确定，各种不同的身份对于我们的相对重要性。 因此，对于人类生活而言，最关键的莫过于选择与推理的责任。与此相反，暴力往往孕育于这样的一种认知，即我们不可避免地属于某种所谓唯一的——并且往往是好斗的——身份，该身份可不容置疑地向我们提出极其广泛的要求（虽然有时候这些要求是那么不易接受）。通常，将某一唯一身份强加于一个人是挑拨派别对立的一个关键的“竞技”技巧。 不幸的是，许多主观上试图制止这类暴力的良好意愿也往往因为以为关于我们的身份没有多少选择可做而遭到挫折，这严重损害了我们克服暴力的能力。如果在人类不同群体之间实现友好关系的前景主要被视为诸如“不同文明之间的友善共存”、“宗教之间的对话”，或者“不同社群的友好往来”（我们正日益朝这个方向前进），而忽视人们之间千丝万缕的联系，那么我们在为和平设计进程之前，就把人类渺小化了。 一旦世界上的种种区别被整合简化成某一单维度的、具有支配性的分类体系——诸如按照宗教、社群、文化、民族或者文明划分并在处理战争与和平问题时按照这种方法把其相关维度看做是唯一起作用的，那么我们所共享的人性便受到了粗暴的挑战。这样一个单一划分的世界比我们所实际生活其中的多重而有差异的世界更具分裂性。它不仅与那种过时的，认为“我们人类大体上一样”的信念（这种观念常被不无理由地讥讽为过于幼稚）相悖，而且也与另一种较少受到关注但更为合情合理的观念相悖，即人们之间的差异是多种多样的（diverselydifferent）。在当代，实现世界和谐的希望很大程度上取决于我们对人类身份多重性的更为清晰的把握，以及充分认识到，人们的这种多重身份是纷繁复杂的，并且坚决反对将人们按某一单一的、鲜明的界限来进行划分。 确实，概念混乱，而不仅仅是恶意，很大程度上应该对我们周围所发生的骚乱与残暴承担责任。对命运的幻象（illusion of destiny），尤其是那些宣扬这种或那种单一身份（及其相应的含义）的观念，有意无意地孕育了这个世界的暴力。我们必须清楚地看到，我们拥有各种不同的所属关系，可以以各种不同的方式互动（不管那些煽动者或他们的困惑不安的对手是如何向我们宣传的）。我们可以对自己的事情自由决定先后轻重。 对于我们所属关系的多样性以及对需要做出推理和选择的必要性的忽视，使我们对生活于其中的世界缺乏清晰的认识，并因此把我们自己推向马修·阿诺德（Matthew Arnold）在《多佛海滩》（Dover Beach）一诗中所描绘的可怕情景： 我们犹如置身于黑暗的旷野，陷入混乱的进军和撤退之中，在那里，无知的军队在黑夜中混战。 我们能够比这做得更好。 本文来源于阿马蒂亚•森《身份与暴力》，阿玛蒂亚•森（Amartya Sen），1998年诺贝尔经济学奖得主。森1933年生于印度，现在仍然保留印度国籍。1953年森在印度完成大学学业后赴剑桥大学就读，1959年取得博士学位。森曾执教于伦敦经济学院、牛津大学、哈佛大学等著名学府，现任剑桥大学三一学院院长。 森的突出贡献表现在五个领域内，分别是：社会选择理论、个人自由与帕累托最优的关系、福利和贫困指数衡量、饥荒问题与权利分配不均的关系以及道德哲学问题。森的学术思想继承了从亚里士多德到亚当•斯密等古典思想家的遗产。他深切关注全世界各地遭受苦难的人们，被誉为“经济学良心的肩负者”、“穷人的经济学家”。]]></content>
      <categories>
        <category>林中路</category>
      </categories>
      <tags>
        <tag>阿玛蒂亚·森</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[林中路（8）：德鲁克 | 管理自己]]></title>
    <url>%2F2019%2F04%2F14%2F%E6%9E%97%E4%B8%AD%E8%B7%AF%EF%BC%888%EF%BC%89%EF%BC%9A%E5%BE%B7%E9%B2%81%E5%85%8B%20%20%E7%AE%A1%E7%90%86%E8%87%AA%E5%B7%B1%2F</url>
    <content type="text"><![CDATA[本文是《哈佛商业评论》创刊以来重印次数最多的文章之一。作者彼得·德鲁克，自1971年后长期在美国加利福尼亚州克莱尔蒙特研究生大学任教。该文首次发表于1999年，节选自其著作《21世纪的管理挑战》（Management Challenges for the 21st Century，HarperCollins出版社，1999）。本文有删节。 我们生活的这个时代充满着前所未有的机会：如果你有雄心，又不乏智慧，那么不管你从何处起步，你都可以沿着自己所选择的道路登上事业的顶峰。 不过，有了机会，也就有了责任。今天的公司并不怎么管员工的职业发展；实际上，知识工作者必须成为自己的首席执行官。你应该在公司中开辟自己的天地，知道何时改变发展道路，并在可能长达50年的职业生涯中不断努力、干出实绩。要做好这些事情，你首先要对自己有深刻的认识——不仅清楚自己的优点和缺点，也知道自己是怎样学习新知识和与别人共事的，并且还明白自己的价值观是什么、自己又能在哪些方面做出最大贡献。因为只有当所有工作都从自己的长处着眼，你才能真正做到卓尔不群。 历史上的伟人——拿破仑、达芬奇、莫扎特——都很善于自我管理。这在很大程度上也是他们成为伟人的原因。不过，他们属于不可多得的奇才，不但有着不同于常人的天资，而且天生就会管理自己，因而才取得了不同于常人的成就。而我们当中的大多数人，甚至包括那些还算有点天赋的人，都不得不通过学习来掌握自我管理的技巧。我们必须学会自我发展，必须知道把自己放在什么样的位置上，才能做出最大的贡献，而且还必须在长达50年的职业生涯中保持着高度的警觉和投入——也就是说，我们得知道自己应该何时换工作，以及该怎么换。 我的长处是什么多数人都以为他们知道自己擅长什么。其实不然，更多的情况是，人们只知道自己不擅长什么——即便是在这一点上，人们也往往认识不清。然而，一个人要有所作为，只能靠发挥自己的长处，而如果从事自己不太擅长的工作是无法取得成就的，更不用说那些自己根本干不了的事情了。 以前的人没有什么必要去了解自己的长处，因为一个人的出身就决定了他一生的地位和职业：农民的儿子也会当农民，工匠的女儿会嫁给另一个工匠等。但是，现在人们有了选择。我们需要知己所长，才能知己所属。 要发现自己的长处，唯一途径就是回馈分析法（feedback analysis）。每当做出重要决定或采取重要行动时，你都可以事先记录下自己对结果的预期。9到12个月后，再将实际结果与自己的预期比较。我本人采用这种方法已有15到20年了，而每次使用都有意外的收获。比如，回馈分析法使我看到，我对专业技术人员，不管是工程师、会计师还是市场研究人员，都容易从直觉上去理解他们。这令我大感意外。它还使我看到，我其实与那些涉猎广泛的通才没有什么共鸣。 回馈分析法并不是什么新鲜的东西。早在14世纪，这种方法由一个原本会永远默默无闻的德国神学家发明，大约150年后被法国神学家约翰·加尔文和西班牙神学家圣依纳爵分别采用。他们都把这种方法用于其信徒的修行。事实上，回馈分析法使他们的信徒养成了一种始终注重实际表现和结果的习惯，这也是他们创立的教派——加尔文教会和耶稣会——能够主宰欧洲长达30年的原因。 我们只要持之以恒地运用这个简单的方法，就能在较短的时间内（可能两三年），发现自己的长处——这是你需要知道的最重要的事情。在采用这种方法之后，你就能知道，自己正在做（或没有做）的哪些事情会让你的长处无法发挥出来。同时，你也将看到自己在哪些方面能力不是特别强。最后，你还将了解到自己在哪些方面完全不擅长，做不出成绩来。 根据回馈分析的启示，你需要在几方面采取行动。首先最重要的是，专注于你的长处，把自己放到那些能发挥长处的地方。 其次，加强你的长处。回馈分析会迅速地显示，你在哪些方面需要改善自己的技能或学习新技能。它还将显示你在知识上的差距——这些差距通常都可以弥补。数学家是天生的，但是人人都能学习三角学。 第三，发现任何由于恃才傲物而造成的偏见和无知，并且加以克服。有太多的人，尤其是那些术业有专攻的人，往往对其他领域的知识不屑一顾，或者认为聪明的头脑就可取代知识。比如，很多一流的工程师遇上与人相关的事就束手无策，他们还以此为荣——因为他们觉得，对条理清晰的工程师头脑来说，人太混乱无序了。与此形成鲜明对照的是，人力资源方面的专业人员常常以他们连基本的会计知识或数量分析都一无所知而自傲。不过，人们要是对这样的无知还沾沾自喜的话，那无异于自取灭亡。其实，要让自己的长处得到充分发挥，你就应该努力学习新技能、汲取新知识。 另外一点也同样重要——纠正你的不良习惯。所谓不良习惯，是指那些会影响你的工作成效和工作表现的事情。这样的习惯能很快地在回馈中反映出来。例如，一位企划人员可能发现自己美妙的计划最终落空，原因是他没有把计划贯彻到底。同那些才华横溢的人一样，他也相信好的创意能够移动大山。但是，真正移山的是推土机，创意只不过是为推土机指引方向，让它知道该到何处掘土。这位企划人员必须意识到不是计划做好就大功告成，接下来还得找人执行计划，并向他们解释计划，在付诸行动前须做出及时的调整和修改，最后要决定何时中止计划。 与此同时，回馈还会反映出哪些问题是由缺乏礼貌造成的。礼貌是一个组织的润滑剂。两个移动物相互接触时发生摩擦是一个自然规律，不仅无生命的物体是这样，人类也是如此。礼貌，其实也很简单，无非是说声“请”和“谢谢”，记住别人的名字，或问候对方家人这样的小事，但就是这种不起眼的细节，使得两个人能够融洽相处，不管他们彼此之间是否有好感。许多聪明人，尤其是聪明的年轻人，没有意识到这一点。如果回馈分析表明某个人只要一遇到需要别人合作的事就屡屡失败，那么很可能就意味着这个人的举止不大得体——也就是缺乏礼貌。 把预期和实际结果进行比较，也会发现自己不能做什么。我们每个人都有许多一窍不通、毫无天分的领域，在这些领域我们甚至连平庸的水平都达不到。人们，尤其是知识工作者，就不应该试图去完成这些领域的工作和任务。他们应该尽量少把精力浪费在那些不能胜任的领域上，因为从无能到平庸要比从一流到卓越需要人们付出多得多的努力。然而，大多数人，尤其是教师，还有组织，都一门心思要把能力低下的人变成合格者。其实，他们还不如把精力、资源和时间花在将称职者培养成佼佼者上。 我的工作方式是怎样的令人惊讶的是，很少有人知道自己平时是怎样把事情给做成的。实际上，我们当中的大多数人甚至不知道不同人有着不同的工作方式和表现。许多人不是以他们习惯的方式工作，这当然就容易造成无所作为。对于知识工作者来说，“我的工作方式是怎样的？”可能比“我的长处是什么？”这个问题更加重要。 同一个人的长处一样，一个人的工作方式也是独一无二的。这由人的个性决定。不管个性是先天决定的，还是后天培养的，它肯定是早在一个人进入职场前就形成了。正如一个人擅长什么、不擅长什么是既定的一样，一个人的工作方式也基本固定，它可以略微有所调整，但是不可能完全改变——当然也不会轻易改变。而且就像人们从事自己最拿手的工作容易做出成绩一样，他们要是采取了自己最擅长的工作方式也容易取得成就。通常，几个常见的个性特征就决定了一个人的工作方式。 我属于读者型，还是听者型？首先，你要搞清楚的是，你是读者型（习惯阅读信息）还是听者型（习惯听取信息）的人。绝大多数人甚至都不知道还有读者型和听者型之说，而且很少有人既是读者型又是听者型。知道自己属于哪种类型的人更少。但是，有一些例子说明了这样的无知可能造成多大的危害。 德怀特·艾森豪威尔担任欧洲盟军最高统帅时，一直是新闻媒体的宠儿。他的记者招待会以其独特的风格出名——不管记者提出什么问题，艾森豪威尔将军都从容地对答如流。无论是介绍情况，还是解释政策，他都能够用两三句言简意赅的话就说清楚。十年后，艾森豪威尔当上了总统，当年曾对他十分崇拜的同一批记者，这时却公开瞧不起他。他们抱怨说，他从不正面回答问题，而是喋喋不休地胡侃着其他事情。他们总是嘲笑他回答问题时语无伦次，不合乎语法，糟蹋标准英语。 艾森豪威尔显然不知道自己属于读者型，而不是听者型。当他担任欧洲盟军最高统帅时，他的助手设法确保媒体提出的每一个问题至少在记者招待会开始前半小时以书面形式提交。这样，艾森豪威尔就完全掌握了记者提出的问题。而当他就任总统时，他的两个前任都是听者型——富兰克林·罗斯福和哈里·杜鲁门。这两位总统知道自己是听者型的，并且都喜欢举行畅所欲言的记者招待会。艾森豪威尔可能认为他必须去做两位前任所做的事。可是，他甚至连记者们在问些什么都从来没听清楚过。而且，艾森豪威尔并不是个极端的例子。 几年后，林登·约翰逊把自己的总统职位给搞砸了，这在很大程度上是因为他不知道自己是听者型的人。他的前任约翰·肯尼迪是个读者型的人，他搜罗了一些出色的笔杆子当他的助手，要求他们每次进行当面讨论之前务必先给他写通报。约翰逊留下了这些人，他们则继续写通报。可是他显然根本看不懂他们写的东西。不过，约翰逊以前当参议员时曾经表现非凡，因为议员首先必须是听者型。 没有几个听者型的人可以通过努力变成合格的读者型——不管是主动还是被动的努力，反之亦然。因此，试图从听者型转为读者型的人会遭受林登·约翰逊的命运，而试图从读者型转为听者型的人会遭受德怀特·艾森豪威尔的命运。他们都不可能发挥才干或取得成就。 我如何学习要了解一个人的工作方式，需要弄清的第二点是，他是如何学习的。许多一流的笔杆子都不是好学生——温斯顿·邱吉尔就是一例。在他们的记忆中，上学往往是十足的折磨。然而，他们的同学有这种记忆的却很少。他们可能在学校里得不到什么乐趣，对他们来说上学的最大痛苦是无聊。有关这个问题的解释是，笔头好的人一般不靠听和读来学习，而靠写来学习，这已成了一种规律。学校不让他们以这种方式学习，所以他们的成绩总是很糟糕。 所有的学校都遵循这样的办学思路：只有一种正确的学习方式，而且人人都得遵从。但是，对学习方式跟别人不大一样的学生来说，被迫按学校教的方式来学习就是地狱。实际上，学习大概有六七种不同的方式。 像邱吉尔这样的人靠写来学习。还有些人以详尽的笔记来学习。例如，贝多芬留下了许多随笔小抄，然而他说，实际上他作曲时从来不看这些随笔小抄。当被问及他为什么还要用笔记下来时，据说他回答道：“如果我不马上写下来的话，我很快就会忘得一干二净。如果我把它们写到小本子上，我就永远不会忘记了，也用不着再看一眼。”有些人在实干中学习。另一些人通过听自己讲话学习。 我认识一位公司总经理，他把一个平庸的小家族企业发展成行业领军企业。他是一个通过讲话学习的人。他习惯于每周一次把全体高层管理人员召集到他的办公室，随后对他们讲上两三个小时。他总是提出政策性问题，在每一个问题上提出三种不同观点。但他很少请这帮同事发表意见或提出问题，他只需要听众听他讲话。这就是他的学习方式。虽然他是一个比较极端的例子，但是通过讲话学习绝不是一种少见的方法。成功的出庭律师也以同样的方式学习，许多诊断医师也是如此（我自己也是这样）。 在所有最重要的自我认识当中，最容易做到的就是知道自己是怎样学习的。当我问人们：“你怎么学习？”大多数人都知道答案。但是，当我问：“你根据这个认识来调整自己的行为吗？”没有几个人回答“是”。然而，知行合一是取得成就的关键；如果知行不合一，人们就会无所作为。 我属于读者型还是听者型？我如何学习？这是你首先要问自己的问题。但是，光这些问题显然不够。要想做好自我管理，你还需要问这样的问题：我能与别人合作得好吗？还是喜欢单枪匹马？如果你确实有与别人进行合作的能力，你还得问问这个问题：我在怎样的关系下与他人共事？ 有些人最适合当部属。二战时期美国的大英雄乔治·巴顿将军是一个很好的例子。巴顿是美军的一名高级将领。然而，当有人提议他担任独立指挥官时，美国陆军参谋长、可能也是美国历史上最成功的伯乐，乔治·马歇尔将军说：“巴顿是美国陆军造就的最优秀的部下，但是，他会成为最差劲的司令官。” 一些人作为团队成员工作最出色。另一些人单独工作最出色。一些人当教练和导师特别有天赋，另一些人却没能力做导师。 另一个关键的问题是，我如何才能取得成果——是作为决策者还是作为顾问？许多人做顾问时的表现会很出色，但是不能够承担决策的负担和压力。与此相反，也有许多人需要顾问来迫使他们思考，随后他们才能做出决定，接着迅速、自信和大胆地执行决定。 顺便说一下，一个组织的二号人物在提升到一号职位时常常失败，也正是因为这个原因。最高职位需要一个决策者，而一个强势的决策者常常把其信赖的人放在二号位置，当他的顾问。顾问在二号位置上往往是很出色的，但是换到一号位置，他就不行了。他虽然知道应该做出什么样的决定，但是不能接受真正做决定的责任。 其他有助于认识自我的重要问题包括：我是在压力下表现出色，还是适应一种按部就班、可预测的工作环境？我是在一个大公司还是在一个小公司中工作表现最佳？在各种环境下都工作出色的人寥寥无几。我不止一次地看到有些人在大公司中十分成功，换到小公司中则很不顺利。反过来也是如此。 下面这个结论值得我们反复强调：不要试图改变自我，因为这样你不大可能成功。但是，你应该努力改进你的工作方式。另外，不要从事你干不了或干不好的工作。 我的价值观是什么要能够自我管理，你最后不得不问的问题是：我的价值观是什么？这不是一个有关伦理道德的问题。道德准则对每一个人都一样。要对一个人的道德进行测试，方法很简单。我把它称为“镜子测试”。 20世纪初，德国驻英国大使是当时在伦敦所有大国中最受尊重的一位外交官。显然，他命中注定会承担重任，即使不当本国的总理，至少也要当外交部长。然而，在1906年，他突然辞职，不愿主持外交使团为英国国王爱德华七世举行的晚宴。这位国王是一个臭名昭著的色鬼，并且明确表示他想出席什么样的晚宴。据有关报道，这位德国大使曾说：“我不想早晨刮脸时在镜子里看到一个皮条客。” 这就是镜子测试。我们所尊从的伦理道德要求你问自己：我每天早晨在镜子里想看到一个什么样的人？在一个组织或一种情形下合乎道德的行为，在另一个组织或另一种情形下也是合乎道德的。但是，道德只是价值体系的一部分——尤其对于一个组织的价值体系来说。 如果一个组织的价值体系不为自己所接受或者与自己的价值观不相容，人们就会备感沮丧，工作效力低下。 让我们来看看一位十分成功的人力资源主管的经历。这位主管所在的公司被一家大企业收购。收购之后，她得到了提升，从事的是她以前做得最出色的工作，包括为重要职位挑选人才。这位主管深信，在选人时，公司只有在排除内部的所有可能人选后才能从外部招聘人才。但是她的新公司认为应该首先从外部招聘，以吸收新鲜血液。对于这两种方式，需要说明的一点是，根据我的经验，适当的方式是两者兼顾。然而，这两种方式在根本上是互不相容的——表面上是政策不同，实质是价值观的不同。这说明在该公司人们对以下三个问题有着不同看法：组织与员工之间是怎样的关系；组织应该为员工以及员工的发展承担何种责任；一个人对企业最重要的贡献是什么。经过几年挫折，这位主管最终辞职——尽管她的经济损失很大。她的价值观和这个组织的价值观就是无法融合。 同样，一家制药公司无论是通过不断的小幅改进，还是通过几次费用高昂、风险巨大的“突破”来取得出色业绩，都主要不是一个经济问题。这两种战略的结果可能都差不多。实质上，这是两种价值体系之间的冲突。一种价值体系认为公司的贡献是帮助医生把他们已经在做的工作锦上添花，另一种价值体系的取向是进行更多的科学发现。 至于一个企业的经营是着眼于短期结果，还是注重长远发展，这同样是价值观问题。财务分析师认为，企业可两者同时兼顾。成功的企业家知道得更清楚。诚然，每一家公司都必须取得短期成果。但是在短期成果与长期增长之间的冲突中，每一家公司都将决定自己所选择的重点。从根本上说，这是一种关于企业职能与管理层责任的价值观冲突。 价值观冲突并不限于商业组织。美国发展最快的一个牧师教会，衡量工作成败的尺度是新教徒的人数。它的领导层认为，重要的是有多少新教徒入会。随后，上帝将满足他们的精神需求，或者至少会满足足够比例的新教徒的需求。另一个福音派牧师教会认为，重要的是人们的精神成长。这个教会慢慢地让那些形式上入会但精神上并没有融入教会生活的新教徒选择了离开。 这同样不是一个数量问题。乍一看，第二个教会好像发展较慢。但是，它留住新教徒的比例要远高于第一个。换言之，它的发展比较稳固。这也不是一个神学问题，至少首先并不是神学问题，而是有关价值观的问题。在一次公开辩论中，一位牧师这样说：“除非你先加入教会，否则你永远找不到天国之门。” 而另一位牧师反驳说：“不，除非你先有心寻找天国之门，否则你就不属于教会。” 组织和人一样，也有价值观。为了在组织中取得成效，个人的价值观必须与这个组织的价值观相容。两者的价值观不一定要相同，但是必须相近到足以共存。不然，这个人在组织中不仅会感到沮丧，而且做不出成绩。 一个人的工作方式和他的长处很少发生冲突，相反，两者能产生互补。但是，一个人的价值观有时会与他的长处发生冲突。一个人做得好甚至可以说是相当好、相当成功的事情——可能与其价值体系不吻合。在这种情况下，这个人所做的工作似乎并不值得贡献毕生的精力（甚至没必要贡献太多的精力）。 如果可以，请允许我插入一段个人的故事。多年前，我也曾不得不在自己的价值观和做得很成功的工作之间做出选择。20世纪30年代中期，我还是一个年轻人，在伦敦做投资银行业务，工作非常出色。这项工作显然能发挥我的长处。然而，我并不认为自己担任资产管理人是在做贡献。我认识到，我所重视的是对人的研究。我认为，一生忙于赚钱、死了成为墓地中的最大富翁没有任何意义。当时我没有钱，也没有任何就业前景。尽管当时大萧条仍在持续，我还是辞去了工作。这是一个正确的选择。换言之，价值观是并且应该是最终的试金石。 我属于何处少数人很早就知道他们属于何处。比如，数学家、音乐家和厨师，通常在四五岁的时候就知道自己会成为数学家、音乐家和厨师了。物理学家通常在十几岁甚至更早的时候就决定了自己的工作生涯。但是，大多数人，尤其是很有天赋的人，至少要过了二十五六岁才知道他们将身属何处。然而，到这个时候，他们应该知道上面所谈的三个问题的答案：我的长处是什么？我的工作方式是怎样的？我的价值观是什么？随后，他们就能够并且应该决定自己该向何处投入精力。 或者，他们应该能够决定自己不属于何处。已经知道自己在大公司里干不好的人，应该学会拒绝在一个大公司中任职。已经知道自己不适合担任决策者的人，应该学会拒绝做决策工作。巴顿将军（他自己大概永远不知道这一点）本来应该学会拒绝担任独立总指挥的。 同样重要的是，知道上述三个问题的答案，也使得一个人能够坦然接受一个机会、一个邀请或一项任务。“是的，我将做这件事。但是，我将按照我自己的特点，采取这样的方式来做这件事，进行这样的组织安排，这样来处理当中所牵涉的关系。这是我在这个时间范围内应该会取得的成果，因为这就是我。” 成功的事业不是预先规划的，而是在人们知道了自己的长处、工作方式和价值观后，准备把握机遇时水到渠成的。知道自己属于何处，可使一个勤奋、有能力但原本表现平平的普通人，变成出类拔萃的工作者。 我该做出什么贡献综观人类的发展史，绝大多数人永远都不需要提出这样一个问题：我该做出什么贡献？因为他们该做出什么贡献是由别人告知的，他们的任务或是由工作本身决定的（例如农民或工匠的任务），或是由主人决定的（例如佣人的任务）。以前的人大多都处于从属地位，别人吩咐他们做什么，就做什么，这被认为是理所当然的。甚至到了20世纪50年代和60年代，那时涌现出的知识工作者（即所谓的“组织人”， organization man）还指望公司的人事部为他们做职业规划。 随后，到20世纪60年代末，就再没有人想让别人来安排自己的职业生涯了。年轻的男男女女开始提出这个问题：我想做什么？而他们所听到的答案就是“你们自行其是吧”。但是，这种回答同“组织人”听命公司的做法一样错误。那些相信自行其是就能做出贡献、实现抱负、取得成功的人，一般连三点中的任何一点都做不到。 尽管如此，我们还是不能走回头路，让别人来吩咐、安排自己要干什么。对于知识工作者来说，他们还不得不提出一个以前从来没有提出过的问题：我的贡献应该是什么？要回答这个问题，他们必须考虑三个不同的因素：当前形势的要求是什么？鉴于我的长处、我的工作方式以及我的价值观，我怎样才能对需要完成的任务做出最大贡献？最后，必须取得什么结果才能产生重要影响？ 请看一位新任命的医院院长的经历。这是一所享有盛名的大医院，30年来一直就靠名气顺利经营着。新院长上任后决定了自己应做的贡献：两年内在医院的某个重要领域建立起卓越服务的标准。他决定以急诊室为重点，因为该院的急诊室地方比较大，受人注意，而又秩序混乱。他决定，到急诊室就诊的每一个患者必须在60秒钟之内由一名合格的护士接待。一年之内，该医院的急诊室变成了美国所有医院的样板，又过了两年，整个医院的面貌焕然一新。 正如这个事例所表明的，把眼光放得太远是不大可能的——甚至不是特别有效。一般来说，一项计划的时间跨度如果超过了 18个月，就很难做到明确和具体。因此，在多数情况下我们应该提出的问题是：我在哪些方面能取得将在今后一年半内见效的结果？如何取得这样的结果？回答这个问题时必须对几个方面进行权衡。首先，这些结果应该是比较难实现的——用当前的一个时髦词说，就是要有“张力” （stretching）。但是，这些结果也应该是能力所及的。设定一个不能实现的目标或者只能在可能性微乎其微的情况下实现的目标，根本不能叫雄心勃勃，简直就是愚蠢。其次，这些结果应该富有意义，要能够产生一定影响。最后，结果应该明显可见，如果可能的话，还应当能够衡量。确定了要实现的结果之后，接着就可以制订行动方针：做什么，从何处着手，如何开始，目标是什么，在多长时间内完成。 对人际关系负责除了少数伟大的艺术家、科学家和运动员，很少有人是靠自己单枪匹马而取得成果的。不管是组织成员还是个体职业者，大多数人都要与别人进行合作，并且是有效的合作。要实现自我管理，你需要对自己的人际关系负起责任。这包括两部分内容。 首先是要接受别人是和你一样的个体这个事实。他们会执意展现自己作为人的个性。这就是说，他们也有自己的长处，自己的做事方式和自己的价值观。因此，要想卓有成效，你就必须知道共事者的长处、工作方式和价值观。这个道理听起来让人很容易明白，但是没有几个人真正会去注意。一个习惯于写报告的人就是个典型的例子——他在第一份工作时就培养起写报告的习惯，因为他的老板是一个读者型的人，而即使下一个老板是个听者型，此人也会继续写着那肯定没有任何结果的报告。这位老板因此肯定会认为这个员工愚蠢、无能、懒惰，肯定干不好工作。但是，如果这个员工事先研究过新老板的情况，并分析过这位老板的工作方式，这种情况本来可以避免。 老板既不是组织结构图上的一个头衔，也不是一个“职能”。他们是有个性的人，他们有权以自己最得心应手的方式来工作。与他们共事的人有责任观察他们，了解他们的工作方式，并做出相应的自我调整，去适应老板最有效的工作方式。事实上，这就是“管理”上司的秘诀。 这种方法适用于所有与你共事的人。每个人都有他自己的做事方法，也有权按照自己的方式来工作，而不是按你的方法来工作。重要的是，他们能否有所作为以及他们持有什么样的价值观。至于工作方式，人各有别。提高效力的第一个秘诀是了解跟你合作和你要依赖的人，以利用他们的长处、工作方式和价值观。工作关系应当既以工作为基础，也以人为基础。 人际关系责任的第二部分内容是沟通责任。在我或是其他人开始给一个组织做咨询时，我们听到的第一件事都与个性冲突有关。其中大部分冲突都是因为：人们不知道别人在做什么，他们又是采取怎样的工作方式，专注于做出什么样的贡献以及期望得到怎样的结果。而这些人不了解情况的原因是，他们没有去问，结果也就不得而知。 这种不去问明情况的做法，与其说是反映了人类的愚蠢，倒不如说是历史使然。在以前，人们没必要把这些情况告诉任何人。比如在中世纪的城市，一个区的每一个人从事的行业都一样。在乡村，土地刚一解冻，山谷里的每一个人就开始播种同一种农作物。即使有少数人做的事情和大家不一样，他们也是单独工作，因此不需要告诉任何人他们在做什么。 而现在，大多数人都与承担着不同任务和责任的人一道工作。市场营销副总裁可能是销售出身，知道有关销售的一切，但是，对于自己从未做过的事情，比如定价、广告、包装等等，就一无所知了。所以，那些正在做这些工作的人必须确保营销副总裁懂得他们设法做的是什么、他们为什么要做这件事、他们将如何去做以及期望取得什么结果。 如果营销副总裁不懂得这些高层次的、知识型的专业人士在做什么，错主要在后者身上，而不在自己。反过来说，营销副总裁的责任则是确保他的所有同事都知道自己是怎样看待营销这项工作的：他的目标是什么、他如何工作，以及他对他本人和他的每一个同事有什么期望。 即使一些人懂得负起人际关系责任的重要性，他们和同事的交流也往往不够。他们总是有所顾虑，怕别人把自己看成是一个冒昧、愚蠢、爱打听的人。他们错了。因为我们看到，每当有人找到他的同事说“这是我所擅长的工作。这是我的做事方式。这是我的价值观。这是我计划做出的贡献和应当取得的成果”，这个人总会得到如此回答：“这太有帮助了，可你为什么不早点告诉我？” 如果一个人继续问道：“那么，关于你的长处、你的工作方式、你的价值观以及你计划做出的贡献，我需要知道什么？”他也会得到类似的答复——据我的经验，无一例外。事实上，知识工作者应该向与他们共事的每一个人，不管是下属、上司、同事还是团队成员，都发出这样的疑问。而且，每次提出此类问题，都会得到这样的回答：“谢谢你来问我。但是，你为什么不早点问我？” 组织已不再建立在强权的基础上，而是建立在信任的基础上。人与人之间相互信任，不一定意味着他们彼此喜欢对方，而是意味着彼此了解。因此，人们绝对有必要对自己的人际关系负责。这是一种义务。不管一个人是公司的一名成员，还是公司的顾问、供应商或经销商，他都需要对他的所有共事者负起这种责任。所谓共事者，是指在工作上他所依赖的同事以及依赖他的同事。 管理后半生当多数人的工作是体力劳动时，你不必为自己的后半生担心。你只要继续从事你一直在做的工作就行了。如果你够幸运，能在工厂或铁路辛勤工作40年后撑下来，你就可以快乐地度过余生，什么也用不着干。然而，现在的多数工作都是知识工作，而知识工作者在干了40年后，仍能发挥余热，他们只是有些厌倦。 我们听到了许多有关经理人中年危机的谈论，“厌倦”这个词在其中频频出现。45岁时，多数经理人的职业生涯达到了顶峰，他们也知道这一点。在做了 20年完全相同的工作之后，他们已经得心应手。但是他们学不到新东西，也没有什么新贡献，从工作中得不到挑战，因而也谈不上满足感。然而，在他们面前，还有20到25年的职业道路要走。这就是为什么经理人在进行自我管理后，越来越多地开始发展第二职业的原因。 发展第二职业有三种方式。第一种是完全投身于新工作。这常常只需要从一种组织转到另一种组织。例如，一家大公司某事业部的会计师成为一家中型医院的财务总监。但是也有越来越多的人转入完全不同的职业。例如，公司经理在45岁时进入政府内阁；或者中层管理人员在公司工作20年后离职，到法学院进修，成为一个小镇的律师。 还有许多人在第一份职业中取得的成功有限，于是改行从事第二职业。这样的人有很多技能，他们也知道该如何工作。而且，他们需要一个社群——因为孩子已长大单飞，剩下一座空屋。他们也需要收入。但最重要的是，他们需要挑战。 为后半生做准备的第二种方式是，发展一个平行的职业。许多人的第一职业十分成功，他们还会继续从事原有工作，或全职或兼职，甚至只是当顾问。但是，除此之外，他们会开创一项平行的工作，通常是在非营利机构，每周占用10个小时。例如，他们可能接手教会的管理，或者担任当地女童子军顾问委员会主席。他们也可能管理受虐妇女庇护所，担任当地公共图书馆的儿童图书管理员，或在学校董事会任职等。 最后一种方法是社会创业。社会创业者通常是在第一职业中非常成功的人士。他们都热爱自己的工作，但是这种工作对他们已经不再有挑战性。在许多情况下，他们虽然继续做着原来的工作，但在这份工作上花的时间越来越少。他们同时开创了另一项事业，通常是非营利性活动。例如，我的朋友鲍勃·布福德创办了一个非常成功的电视公司，现在他仍然经营着。但与此同时，他还创建了一个与新教教会合作的非营利组织，也做得非常成功。现在他又创建了一个组织，专门指导社会创业者在经营原有业务的同时，如何管理自己另外创办的非营利机构。 管理好自己后半生的人可能总是少数。多数人可能“一干到底”，数着年头一年一年过去，直至退休。但是，正是这些少数人，这些把漫长的工作寿命看做是自己和社会之机会的男男女女，才会成为领袖和模范。 管理好后半生有一个先决条件：你必须早在你进入后半生之前就开始行动。当30年前人们首次认识到工作寿命正在迅速延长时，许多观察家（包括我自己）认为，退休人员会越来越多地成为非营利机构的志愿者。可是，这种情况并没有发生。一个人如果不在40岁之前就开始做志愿者，那他60岁之后也不会去做志愿者。 同样，我认识的所有社会创业者，都是早在他们原有的事业达到顶峰之前就开始从事他们的第二事业。请看一名成功律师的例子。这位律师是一家大公司的法律顾问，他同时在自己所在的州开办了模特培训学校。早在他 35岁左右的时候，他就开始志愿为学校提供法律咨询。40岁时被推选为一家学校的董事会成员。50岁时，他积累起了一笔财富，办起了自己的企业——建立并经营模特培训学校。然而此时，他依旧在那家他年轻时参与创建的公司里担任首席法律顾问，而且几乎是全职工作。 发展第二兴趣（而且是趁早发展）还有一个原因：任何人都不能指望在生活或工作中很长时间都不遭遇严重挫折。有一位很能干的工程师在45岁时错过了晋升的机会。另一位也很能干的普通学院的教授在42岁时认识到，即使她完全具备担任教授的资格，她永远也不会在一所有名的大学里获得教授职位。还有一位则是在家庭生活里出现了悲剧：婚姻破裂或者痛失子女。在这样的时刻，第二兴趣——不仅仅是业余爱好——可能发挥重要作用。例如，这位工程师现在知道他在工作上并不十分成功。但是，在公司以外的活动中，例如负责教会资金的管理，他是成功的。一个人可能家庭破碎，但是他能在第二兴趣的活动中发现还有社区这个大“家庭”。 在一个崇尚成功的社会里，拥有各种选择变得越来越重要。从历史上来看，却没有“成功”一说。绝大多数人只期望坚守“适当的位置”。唯一的流动性是向下的流动性。然而，在知识社会里，我们期望每一个人都能取得成功。这显然是不可能的。对许多人来说，能避免失败就行。可是有成功的地方，就会有失败。因此，有一个能够让人们做出贡献、发挥影响力或成为“大人物”的领域，这不仅对个人十分重要，对个人的家庭也同样重要。这意味着人们需要找到一个能够有机会成为领袖、受到尊重、取得成功的第二领域——可能是第二份职业，也可能是平行的职业或社会创业。 自我管理中面临的挑战看上去比较明显，甚至非常基本，其答案可能不言自明，甚至近乎幼稚。但是，自我管理需要个人，尤其是知识工作者，做出以前从未做过的事情。实际上，自我管理需要每一个知识工作者在思想和行动上都要成为自己的首席执行官。更进一步来看，这样的转变——从一切听从别人吩咐的体力劳动者到不得不自我管理的知识工作者——也使得社会结构发生了深刻变化。历史上每一个社会，甚至是个人主义倾向最强的社会，都认为两件事情理所当然（即使只是下意识的）：第一，组织比员工更长寿；第二，大多数人从不挪地方。 如今，情况恰恰相反。知识工作者的寿命超过了组织寿命，而且他们来去自如。于是，人们对自我管理的需要在人类事务中掀起了一场革命]]></content>
      <categories>
        <category>林中路</category>
      </categories>
      <tags>
        <tag>德鲁克</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[林中路（7）：朱苏力 | 《自由秩序原理》读书笔记]]></title>
    <url>%2F2019%2F02%2F17%2F%E6%9E%97%E4%B8%AD%E8%B7%AF%EF%BC%887%EF%BC%89%EF%BC%9A%E6%9C%B1%E8%8B%8F%E5%8A%9B%20%E3%80%8A%E8%87%AA%E7%94%B1%E7%A7%A9%E5%BA%8F%E5%8E%9F%E7%90%86%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[意志自由理论是统治阶级的一个发明。——尼采[1] 哈耶克的进路 许多人曾经以不同的进路（理论的和诗意的）强调自由的重要性。如果仅仅就对促进人们充分理解自由的重要性并为之行动而言，哈耶克的所有著作也许并不比裴多菲的名章“生命诚可贵，爱情价更高，若为自由故，两者皆可抛”对于一个民族或一个普通人具有更大的作用。因为，从根本上看，自由于人之可贵并不在于它有一个重要的很能自圆其说的论证；而在于自由是人的生命的一种需求。 自由对于人的意义不在于它是如何阐述出来的，而在于是否为人们以这种或那种方式感受到并得以坚持。无论中国近代以来追求民族独立的志士仁人，还是为了婚姻自由而私奔甚至不怕被沉塘的村姑，我想都是出于一种生命的本能而追求自由，并以自己的行动甚至生命实践着自由。在这个意义上，我认为，哈耶克关于自由主义的论证只是强调自由的进路之一，而不是唯一，它并不具有更本真的特点。世界可能会因为哈耶克关于自由主义的论证而多了几个哈耶克的理论信徒，但是世界不大可能因为哈耶克的论述而增加了对于自由的本能渴求。 我这样理解哈耶克，并不是贬低哈耶克，而是要使哈耶克获得他应得的地位。我并不否认哈耶克对于自由之阐述是有贡献的。但是这种贡献不过是如哈耶克在《自由秩序原理》的开篇中所言：“旧有的真理若要保有对人之心智的支配，就必须根据当下的语言和概念予以重述。人们在过去对旧真理所做的最为有效的表述，已日渐失用，因而也就不再含有明确的意义。尽管这些旧真理赖以为基础的理念之确当性一如往昔，但其语词（甚至当它们指涉的依旧是我们当下所面临的问题时）却已不再传递其往昔的信念；其论辩的情境也已不为我们所知悉；而且他们对我们所面临的问题亦几乎无力作出直接的回答。”[2]哈耶克无疑是20世纪的一位重要思想家，他为市场经济的辩护无疑是深刻的，同时是影响深远的。然而哈耶克的贡献就在于针对当代的一些问题，从自由主义的传统，以一套新的学术语言和概念对于自由作出了重述。因此，哈耶克为自由的辩护也许只有在自由主义的论述传统之中才是独步20世纪的。 哈耶克自由论述的独到之处，在我看来，在于他的论证进路是个体主义的知识论，而不是或不限于传统的政治哲学。他的基本命题是，自由使得每个个体都能更充分地运用自己的知识，进行空前的实验和创造，积累对于自己有用的知识，但由于人类具有模仿能力，个体也就是在积累对于人类有用或潜在有用的知识，他人和人类社会可能以此来应付来自各方面的挑战和危险。[3]例如，在一种群居形成习惯的社会中，一个人独居寡处可能会被这个群体的其他人视为怪僻、异端，但是，按照哈耶克的论证逻辑，个体选择的这种生活方式不仅创造了一种替代的生活方式，更重要的是，一旦发生一场人们无法事先预知的迅速传播的瘟疫，仅仅由于这样一种生活方式的存在，不仅这个个体可能得以存活下来，而且他的生活方式有可能为他人效仿（而不是“知道”或“理解”――在哈耶克看来，人的更大能力是效仿，而不是知道），因此可能使这个群体免于灭顶之灾。同样的道理，个体的知识和技术创新有可能为人类改善自己的生存状态而利用。因此，这种逻辑不仅对于一个群体是如此，对于一个社会，甚至对于整个人类均如此。因此，自由在哈耶克那里，不是作为一种抽象的绝对价值出现的，而是从其对于人类的生存价值上分析的；在这一分析框架中，自由永远是人类生存的backup,是一种战略储备。在这个意义上看，哈耶克的自由是功利主义的，而不是形而上学的。这种为自由的论证或辩护的确具有很强的说服力。据我有限的阅读还没有哪一位学者曾经在这一高度上论述过自由对于人类生存和发展的意义，一般只是从自由能带来社会繁荣兴旺这一点上论述（例如密尔），或者强调自由本身对于个体的价值（例如康德）。 哈耶克的这一基本命题至少有两方面的假定，一方面是关于知识主体的，即不可能有一个全知全能的人，个体的理性总是有限的，即有限理性的假定。这一点已经受到人们的高度重视。但是，在我看来，哈耶克的论述中还隐含了一个关于人类生存状态和环境的假定，即时间和空间都并非一成不变的，我称之为时空非均质化假定。这个假定是非常重要的，在一定意义上，这一假定甚至是前一假定的前设，如果接受了这一假定，甚至必定推出有限理性或理性不及的假定。对于这后一假定，我将在后面论述。 迪尔凯姆的进路 哈耶克的论证是强有力的。但是，如前面所强调的，强调自由的重要性，这并不是唯一的论证进路。实际上，从其他理论进路，其他学者曾经或者也可以得出同样的结论。据我有限的阅读，我想举两个例子，一个是迪尔凯姆以及其后法国学者，另一个就是我们的常识进路——“情况是不断变化的”。 迪尔凯姆在《自杀论》和《社会学方法的准则》两书中都提出过与哈耶克相当类似的观点，尽管是从一条与哈耶克相对立的进路――总体主义的（holistic）进路，并且没有直接讨论“自由”的问题。[4]迪尔凯姆认为，所谓违法犯罪不过是对集体形成的一种惯常的行为习惯和道德（集体良知）的违反，这种违反，在每个社会中每天都在发生，并往往受到制裁。但是他认为，其实犯罪“为必要的改革直接作了准备。哪里有犯罪，哪里的集体感情就处于为形成新的形式所必要的可塑状态”；他特别举了苏格拉底为例，认为“尽管按照雅典的法律，苏格拉底就是一个罪犯，对他的判决也完全正确。然而他的罪行，即他的独立的思想，不仅对全人类有益，而且对他的祖国也是有益的……，他的罪刑为雅典人所必须的新的道德和新的信仰的形成作了准备。”因此迪尔凯姆认为，这种犯罪是“社会生活的正常成分。”[5]他还曾指出“如果考虑到社会在同一时期里不仅必须面对各种不同的情况，而不可能保持一成不变，那么这种必要性就更加明显了。……如果现在没有未来的萌芽，那就不可能有未来。”[6]特别要指出的是，哈耶克在强调自由的时候，并不仅仅强调思的自由，而是更强调做的自由，[7]实际也就是强调竞争的自由。因此，在这个关于违背常规对于人类生存和繁荣的意义问题上，迪尔凯姆和哈耶克的论证的实质性要点是一致的。 当然，迪尔凯姆讨论的是社会生活的规范性与个体创新之间的关系，而不是“自由”。尽管如此，这个问题实际上可以转化成哈耶克笔下的自由问题的讨论。并且，在一定意义上，至少在我看来，迪尔凯姆甚至更为深刻。哈耶克强调自由无疑是对的，他看到个体自由在社会中可能受到限制，个人的创造力可能受到压制，但他不承认这是一个难以避免的两难，而更多倾向于将“自由”在近代社会之丧失视为一种虚假个人主义欺骗的结果，一种思想和观念的结果。他的解决办法是在政治哲学层面提出一个法律下的自由制度。但是这种解决问题的办法实际只是在形而上的层面，无法落实到实践层面；他只能提出一些原则，而无法落实在操作层面。他解决问题的办法是追求自由概念上的包容性。在这个意义上，他的确是、而且也仅仅是18世纪自由主义政治哲学的后人――他的进路限制了他对现代社会科学的更深刻全面的理解和把握。尽管他是经济学家的重镇，但是一旦进入社会理论和法律理论问题时，他基本停留在18世纪。迪尔凯姆的深刻之处在于他发现这种社会有序与个人自由之间的矛盾是一种难以在理智、智识甚或制度层面完全解决的问题，是一种人类的近乎“荒诞”的悖论的或两难的生存状态。这并不是说迪尔凯姆认为社会规范性与个人自由就一定是对立的，迪尔凯姆和哈耶克一样都意识到社会规则会促进自由；但是，哈耶克从传统的自由主义政治哲学进路没有看到的，而迪尔凯姆特别是迪尔凯姆的后代法国学者例如福柯和布迪厄从社会科学研究中看到了的是，社会如果要成为一个社会，事实上不可能在一切方向上发展，无论在社会资源、知识路径、社会结构上都不可能，因此社会即使在促进最大可能的自由时，也必定会有自由的“机会成本”的问题，而且我们无法事先计算这种自由的机会成本。 迪尔凯姆的这一洞察到了福柯这一代就更为明显了。福柯的大量著作都充满了对那些被压抑的声音的同情，展现出他的知识考古所发现的被埋藏的那些知识，他更展现了现代资本主义社会的自由是如何在征服上建立起来的。这就是福柯的名言所概括的，“启蒙运动发现了自由也发明了训诫”。[8]布迪厄关于社会结构与人的能动性之间的辩证法，所针对的问题也是类似的问题，尽管布迪厄讨论的不再是迪尔凯姆的具体的自杀或犯罪问题，但布迪厄不过是在法国的迪尔凯姆传统中以一种更为理论化的框架重构了迪尔凯姆的问题。[9] 常识进路我们再转到我所说的“常识进路”，即强调“情况是不断变化的”，当然，这里的“情况变化”不仅是时间上的，也是空间上的。如果从这一进路看，甚至无需假定个体的有限理性，我们也将同样得出自由重要的结论。相反，所谓关于人的理性有限或理性不及，都不过是情况不断变化的结论。在空间上，由于各地的情况不同，因此，对于一地一事知识之了解不能压制他人对于此地此事之了解，在时间上，由于个人生命之有限，前人对于某地某事之做法不能排除后人对于某地某事之做法。如果按照中国人的这种常识，那么所谓理性有限不有限、及与不及的问题几乎可以完全无关。 事实上，如果在一个更大的文化传统中看，所谓理性有限或理性不及这个概念也许只有在西方传统中才是有意义的，甚至才可能发生。因为，在西方的犹太－基督教文化传统中，一直预设了一个无所不在、无所不能、无所不知的上帝。此后，由于所谓的“人”和“理性”的发现，一个大写的理性或人又先后取代了先前上帝的位置；甚或可以说，人又“按照上帝的面目创造了自己”。因此，只有在这样的文化传统中，进入现代之后，才有必要、同时也才有可能提出理性是否有限或有所不及的问题。而在中国传统中，从来没有构建出来这样全知全能、理性无际的神人，中国传统只有圣人，但这只是道德上的，而不是智识上的，即使是圣人也强调的是“知之为知之，不知为不知，是知也”。正因为中国人这种关于世界“情况不断变化”的前提性判断，这种强调面对“三十年河东，四十年河西”的实践理性，因此，中国人似乎历来不大相信原则（道德原则似乎除外），人们总是强调自己去面对现实，解决问题，获得真知，获得创新。这种思路就是具体问题具体分析、具体解决的思路，就是所谓“普遍真理与具体实践相结合”的思路，这就是“实事求是”的原则。事实上，这也是中国近二十年来改革之所以取得相当成果的一条基本经验，用中国人的话来说，“尊重人民群众的首创精神”（这近乎哈耶克的“自发秩序”）。尽管其论证方式完全不是哈耶克的自由主义进路，完全不需要有限理性的假定。 因此，哈耶克对于自由的论述很难说是一种“最正确的”“最真实的“进路，因为并不存在这样的进路。“太阳底下无新事”，“条条大路通罗马”。 自由的两难但是，在我看来，哈耶克并没有充分回答为什么自由会被“遗忘”，为什么“传统的语词和表述会无力对当下的问题作出直接回答”。对于这个问题，哈耶克的基本观点是，由于一种虚假的个人主义，一种欧陆理性主义的个人主义偷梁换柱篡改了真正的自由主义。这种分析有真确的成分，但仅仅从思想传统上辨析，不能令人信服，而且也违背了哈耶克自己关于制度是行动的产物的命题。我们可以沿着哈耶克的逻辑来分析哈耶克，我们可以发现其在理论上难以自恰或有不少虚构。哈耶克认为，在英美的自由主义传统中，人们有更多的自由，可以产生更具生命力和活力的自发秩序，保持社会的高度自由状态。但是哈耶克所批判的福利国家、“社会主义”和国家干预的因素也同样在英美发展起来了，这很难说是为一种虚假个人主义所蒙骗的结果，如果哈耶克坚持自己的理论，他就应当承认，这种所谓的“奴役之路”也是一种“自发秩序”。在这里，不分析社会历史条件的演变，而仅仅分析观念或学说的演变，哈耶克对历史难题选择的是一个最为小儿科但未必能令人信服的答案，他的进路仍然是18世纪政治哲学家的进路，尽管他是一位重要的经济学家，但是他对19和20世纪的社会科学研究成果显然是缺乏了解或缺乏重视的。 如果仅仅从逻辑上看，自由之所以被“遗忘”（其实未必会被遗忘），就是因为自由有一个内在的两难。简单地说，自由对于一个个体来说就是可以不断创新，不断竞争，按照自己的意愿进行社会交往。但是一旦要进行社会交往，就必须要有一些起码的规则，也就是要形成哈耶克也不得不认可的那种法律下的自由（freedomunderlaw）。规则并不等同于剥夺自由，相反，规则往往是自由得以构成的条件之一；即“有所不为才能有所为”，但是，它也毕竟要求“有所不为”。因此，不可否认，规则对于自由创新是有限制作用的。如果要写诗，就不能像写小说那样写；用中文交流就必须不能夹带太多英文单词同时使用俄文语法。并且这种规则必须具有一定的稳定性和持久性。而稳定性和持久性实际上就是要限制创新，排除创新。而且，由于事先人们并不知道什么样的创新是对未来有意义的，这种限制和排除也未必就是不正当的、恶意的或毫无道理的。因此，即使是一种起初自由的制度，如果没有外来的刺激，没有异端，长期下去，也会逐步僵化起来，缺乏新的包容性，对某些创新变得不自由。因此，就现实生活来说，不可能存在一种各种创新创意层出不穷永远竞争的自由状态，这种状态如果有，也是人们无法忍受的。因此在人的生命对于未来之预期中，蕴含着一种深刻的保守即希望相对简单、稳定和稳步发展的倾向。而这种倾向本身是对自由和创新的一种潜在的威胁。 哈耶克似乎没有看到现实生活中这一两难问题；或者看到了，也没有真正理解这个问题。他只是提出了法律下的自由这一概念，因此在思辩层面“回答了”这一问题。但是，这种思辨式的答案无法回答现实中的具体难题。因此在这一点上，我认为迪尔凯姆的思想要比哈耶克更为深刻。在迪尔凯姆看来，个体的创造力往往会被一个社会视为某种异端，甚至是犯罪，而且社会也的确有理由这样认为；但恰恰是这种创造力是未来的种子，是社会变迁的开始，是对社会的重塑。而哈耶克在《自由秩序原理》的第二部分中，并没有展示人类自由的这种无法避免的两难，而是试图用法律下的自由、自由秩序、自由的原则这样一些大的原则来描述一个自由的乌托邦同时也是一个法律的乌托邦。事实上，这也是哈耶克为什么在这一部分总是言不及义，无法讨论具体的法律问题，而只能限于重述一些永远正确的“大词”（波普尔语）或原则，一些法理学的常识或理想的根本所在。[10]在我看来，这第二部分是缺少份量、最缺少干货的一部分。[11] 当然，有人可能会反驳我说，哈耶克并不要对法律下的自由作出详尽分析，他只是提出一个自由的理想，一个自由的乌托邦，以鼓舞人们为此努力。这种反驳是无力的，是偏袒的，是无原则的。如果仅仅是想提一个乌托邦，我不知道柏拉图的理想国或孟子的“老吾老及他人之老，幼吾幼及他人之幼”或基督教的天国，甚至哈耶克批评的社会主义计划经济有什么地方弱于哈耶克。固然，作为理论应当具有独立于现实的一面，但是，如果一个理论完全不考虑实际操作问题或者实际上无法操作和实现，那将无非是一种煽情，和“若为自由故，两者皆可抛”，与“人生来自由，却无往不在枷锁之中”之类的命题毫无两样。 法治哈耶克在第二部分中集中讨论了自由与法律。其中有不少重要的、富有启发性的然而是自我重复的思想，例如法律与立法的区别，自发秩序等等。然而，令人不满意的是，恰恰是在法律和法治问题上，他并没有什么新的可以称之为他的贡献的。他基本是重述了到20世纪末已经成为常识然而未必真确的一些原则。特别是在第10章中，他似乎要界定法律的特性或者是法治的特性，然而，他的分析就总体说来，没有任何独到之处。 他列举了“真正的法律”的一些特性：抽象的一般性规则，具有公开性、可预测性，法律面前人人平等，不溯及即往。然而这几点不仅为受哈耶克批评的戴雪等人早就系统指出，[12]而且这些特点若要仔细分析起来，实际就是一点，强调规则的统治，其目的和功能都是为了和使得人们可以根据规则作出合理预期和安排，并在规则的制约下，同时也是在规则的促成下充分运用个体的知识。[13] 例如，法律的抽象性和一般性或普遍性原则，就是说一个法律不能是对个具体问题的决定，而必须是针对所有人的某一类问题。事实上，只有在这个意义上的法律，才可能称之为规则。如果总是针对一个具体事件或某个具体个人，这样的“法律”就是一个行为（act），就是哈耶克所说的命令，而无法成为规则。又如法律公开的原则，即制定法必须颁布，法律必须为人们所知晓。[14]但是法律为什么必须公开，必须为人们所知晓，这里的核心问题也许并不在于法律是否“颁布”本身这个行为，重要的在于颁布在理论上可以说是使人们了解法律的最便捷的途径，尤其是在一个以文字作为基本的交流手段的社会中。因此，我们可以看到无论强调的是普遍性还是公开性或可预知性，实际上真正重要的都是法律便于人们掌握、预测和利用。而只有法律作为规则时，才是便利的。 法律面前人人平等原则实际是法律普遍性的另一种表述形式，因为只有对人们同等适用的法律，才有可能并便利人们了解和预测，人们才可以从他人如此行为的得失中预测自己的类似行为的后果，从而了解规则。否则的话，人们就会因不了解法律对自己的要求是什么、自己应如何行为，而陷于无所适从的境地。又如法律不溯及即往的原则，也正是因为人们无法依据尚未形成的法律规则来决定自己的行为。而司法独立则被视为规则性得以真正贯彻、落实的保障。从这一方面看，所有这些特点都可以用信息经济学中的信息费用来解释：规则性是节约信息费用的，规则是有效率的。如果从这一角度看，我们也还可以进一步理解，号称不成文法的普通法国家，为什么可以没有法律颁布的问题，以及在小型社会之中，也没有成文法，为什么仍然可能有法律下的自由。这就是因为，在这里，尽管没有成文法，人们也仍然可以依据他们的习惯行为和一般智识、根据他人的行为后果预测法律将如何要求。事实上，现代各国的法律实践也总是认为，任何个人不能以不了解法律而作为违法的借口。因此，法律的所有这些特点以及法治的这些标准，都是为保障人们预期、运用个体知识的不同说法而已，而并非什么可以分解的特点。这种分析本来应当成为哈耶克的知识论进路研究自由与法律的自然延伸，然而，哈耶克不知是放弃了还是并不理解从他自己的知识论进路研究法治的巨大可能性。 由于放弃了这一可能性，哈耶克实际上就只能回到传统的政治法律哲学，在法律概念或特性上兜圈子了，并试图依据这些所谓的原则来规定法律或评价。由于不熟悉法律问题，他甚至得出一些荒唐的结论。例如他批评霍姆斯关于一般性法律命题并不决定具体案件的观点，[15]认为这是进步党人（在某种意义上，这是社会主义思潮在美国的一种表现形态）的观点。这种将法官从长期司法实践中获得的真知灼见意识形态化的做法充分反映了一个脱离法律实践的人可能会有什么样的偏见，并且会堕落到何等程度。在哈耶克看来，一般性规则似乎是永远是明确的，所有的案件也总是齐整地落入这个或那个众口称是的规则之内，因此必定可以用一般性法律命题来裁定具体案件。但是事实并不如此，每个案件都可能涉及到诸多规则，或者因为当事人各方在规则指导下利用他们个体的知识而不得不涉及到诸多规则，甚至有许多案件是两可的。例如商家完全可以以保护私有产权的法律而反对知假买假的做法，王海这样的消费者也完全可以诉诸同一规则和消费者权益保护法而声称自己的权利。在这种境况下，一般性的抽象法律命题就很难解决具体问题。解决具体纠纷时究竟运用哪个规则，因此，就无法与审判者的判断相分离。空谈法律的基本特点和一般原则至少在很多时候并不能解决任何实际问题，就如同仅仅懂得保持平衡之原理的人未必能骑好自行车一样。世界不会因为有更多的人在理论层面懂得了某个原则而变得更好。在这一点上，哈耶克似乎完全忘记了自己几十年前研究得出的并在此书中多次重复的结论：市场、法律、语言和国家都是人们行动的产物，而不是对原则思考的产物。[16] 由于对于自由原则的高度重视，哈耶克得出的另一个重大的然而又是有重大问题的结论，这就是他对程序的轻视，以及逻辑上的一系列自我矛盾。哈耶克在许多地方主张要区分形式合法和实质合法，强调法律必须坚持自由的原则，并引述了亚里士多德的法治是良法之治的观点。如果仅仅在思辨层面，这并不成问题，但是哈耶克将这种观点延伸到司法实践中，这就造成了依据以自由为核心的“高级法”对立法进行实质性审查可以保证法律不出差错的观点。当然，这也许还不是一个问题，如果有一个真正了解高级法的全知全能且没有偏私的法官或法院的话。问题在于哈耶克将美国最高法院当成了这样一个具有神奇才智的范例。这如果不是无知，那么就是欺骗。因为美国最高法院的大法官们也是一些“人”组成的，他们大多是由历届总统从自己党派中挑选的，其中一些人往往会为了党派的利益而决一死战。[17]在许多重大问题上，最高法院都是分裂的，甚至是严重分裂的。例如受哈耶克批评的罗斯福新政的“社会主义”措施，为最高法院之否决仅仅是以4：5票之差，而此后不久近乎同一的法案在罗斯福“重新包装最高法院”的威胁之下又以5：4获得通过。这就表明哈耶克所主张的以自由的基本原则、以高级法来审查法律并不像哈耶克所描绘的那样可以信赖。另一个典型的例子则是DredScott案件，此案中联邦最高法院，在审查美国联邦政府立法之后，竟然肯定了一个因来到北方而成为自由人的黑人返回南方后仍然是奴隶，并因此认定联邦的有关废除奴隶制的立法（密苏里合约）违宪。此外，尽管哈耶克大力批评法律仅仅重视程序性合法或形式合法，而事实上，他所强调的“真正的法律”的那些特点本身都是一些形式化的特点，并且他所赞美的司法审查制度本身也是一种程序化的制度。 在第二部分中，诸如此类的事实、分析上的问题以及自我矛盾之处相当多。 哈耶克的失足之处这个小标题本身也许就有问题：也许哈耶克根本就没有失足，因此谈不上失足之处。尽管有这种可能，我还是认为哈耶克在这本著作中违背了他的知识论的要点。尽管哈耶克认为人的理性是有限的，有些问题是无法言说的，但是哈耶克在许多问题上似乎表现出一种“致命的自负”（借用他自己的一本书名），即认为自己发现了社会发展的真理，关于自由的真理，这最典型地（或之一）表现在他第一章中对作为单数的自由与复数自由的辨析上。这种辨析是需要的，也是有一定道理的，但是字里行间，他充分展示了那种霸气，即只有自己所理解的自由才是真正的自由，是自由的精髓，而其他都是假的或误解。他相信天才，过分地不相信所谓的即时民主，都是这种智识上的精英观念的体现。我当然承认人的智识上有差别，也不大相信民主可以解决一切问题；但是，如果有一点休谟传统的怀疑主义（这是哈耶克尊敬的传统），那么在这些问题上，我们同样应当有一点自省。 哈耶克的另一个失足之处可能来自于他对计划经济的正确批判。由于在这一方面判断的正确，很可能导致他对自己判断所依赖的某些思想根据统统确信无疑。事实上，一个正确的判断完全可能是一个系列错误的结果。一个人相信气功有疗效并且确实在气功锻炼中病愈的人也许“证明了”他关于自己的病会好的判断是正确的，但并不因此证明了他关于气功治病的确信和推理是正确的。人们常常因为而且完全可能因为自己某个判断得到“证实”而误认为自己的理论资源都是正确的。哈耶克关于计划经济的判断可能是来自他的理论，但未必如此；在我看来，更可能是洞察在先，而论证在后。在这个意义上，任何证实都是可疑的，都不具有终结性；也正是看到了一点，波普尔反对证实论，而主张证伪论，尽管在另一个意义上证伪也仍然是证实。 上述分析，并不是要贬低哈耶克。而是要将哈耶克放在一个知识传统和脉络中来理解哈耶克，从而看到他的贡献和他缺陷。同时这也是坚持哈耶克以及上面所提到的其他进路的基本命题，即关于自由重要性的论证并非只有一条进路，而是可以并且也应当有竞争的。人们对于自由之理论论证之接受，如前所说，很大程度上不取决于该理论的论证是否周密、详尽，而在于他们当下的欲求；并且取决于理论范式的简洁和畅销（这在科学上常常如此），并因此也取决于理论的路径依赖。如果我们在获知哈耶克对自由的出色分析之后，误以为这就是对自由的唯一论证进路，我们就很可能会忘记哈耶克关于自由之真谛，而成为一个哈耶克教条主义者。 为了强化这一点，我们还必须注意，哈耶克关于自由的理论和论证之所以获得了世界的重视在很大程度上是因为他深刻地看到了、预见了计划经济的失败。这当然是了不起的。但是，正是由于这一点，我才认为，并不一定是由于他的理论深刻，最终使得人们最终理解了他的理论的重要，而是因为世界这个大文本改变了他的小文本的意蕴，是一系列非话语的实践改变了其话语的意义，改变了我们对他的理解和解释。 写到了这里，获得这样的感想，是我自己也未曾料想到的。但是我并不想因此否定哈耶克，哈耶克的许多洞见仍然是深刻的，但是，就到目前为止我的一些阅读来看，他最有独创意义的并可以称之为贡献的是他的知识论，而不是他的政治哲学和法律哲学。 1998年2月25－26日于北大蔚秀园 *原载于《中国社会科学季刊》（香港），1998年春季号。 [1]FriedrichNietzsche,TheWandererandHisShadow,inHuman,AllTooHuman,trans.byR.J.Hollingdale,CambridgeUniversityPress,1986,no.9,p.305. [2]《自由秩序原理》，邓正来译，三联书店，1997年，页1－2。 [3]《自由秩序原理》，第2－4章。 [4]迪尔凯姆，《自杀论》，冯韵文译，商务印书馆，1996年；《社会学方法的准则》，狄玉明译，商务印书馆，1995年。 [5]《社会学方法的准则》，页88，89。 [6]《自杀论》，页346。 [7]《自由秩序原理》，页34。 [8]MichelFoucault,DisciplineandPunish,theBirthofthePrison,transl.ByAlanSheridan,VintageBooks,1978,pp.222. [9]当然这一传统甚至可以追溯到更早期，追溯到卢梭的名言“人生而自由，却无往不在枷锁之中”。这里的枷锁，据萨拜因，实际是指社会的风俗习惯、传统文化，但并不一定具有贬义。见，《政治学说史》下册，刘山等译，商务印书馆，1986年，第29章。 [10]“我们由此进入了法理学领域的探讨，但我们将从历史的角度去关照其间的问题。然而需要强调指出的是，我们对保障个人自由的各种制度的进化过程所持的认识，主要依凭的既非法律家的观点，亦非历史家的观点。我们所关注的乃是一种理想的发展，然而必须指出的是，在过去的历史长河中（除个别时期以外），人们只是模糊地认识到了这一理想或者说不尽完善地实现了这一理想”。《自由秩序原理》，页7。 [11]不尽如此，哈耶克的这一部分甚至使我怀疑他对法学研究是否有足够的能力和真诚。这两点指责是严厉的，但并不是苛刻的。哈耶克根本没有理解法理学的一些基本问题，还是停留在前法学阶段的阶段；例如关于法律的基本特点的分析，所谓普遍性、一般性、公开性、可预知性，法律面前人人平等，法律不溯及即往等。他对霍姆斯、戴雪的批评都是“站着说话不腰疼”，不着边际的批评。他所引用的学者或语言显然太随意和太selective。他对约翰·马歇尔大法官的赞美和引用完全无视马歇尔的所作所为。他甚至隐含地默认了以司法审查方式肯定美国南方奴隶制的DredScott案件，但又不提出自己的理论论证。他完全无视美国最高法院是一个由不同意识形态和政治倾向的人组成的机构，而将它视为一个有集体意志的人格，这也完全违反了他的个体主义研究进路。这种例子太多了。在这里，我并不是仅仅因某些问题质疑哈耶克，而是使我对这一部分所体现的哈耶克的学术品格产生根本怀疑――我更倾向认为他是自由主义的辩护士，而不是自由主义的思想家。 [12]A.V.Dicey,IntroductiontotheStudyoftheLawoftheConstitution,Macmillan,1968,pp.188-196，提出的法治三原则，大致是，法律必须事先确立，法律面前人人平等，司法决定法律适用;LonFuller,TheMoralityofLaw,rev.ed.,YaleUniversityPress,1969，提出了8项原则：法律的一般性、公布、不溯及即往、确定、不自相矛盾、可行、稳定和实际落实；JohnFinnis,NaturalLawandNaturalRights,ClarendonPress,1980,基本重复了富勒的原则，增加了规则限制的裁量和官员守法两点 [13]因此，美国最高法院大法官斯葛利亚经过多年司法之后得出的结论是：法治就是规则之法（theruleoflawasalawofrules）。见，AntoninScalia,“TheRuleofLawasaLawofRules”,UniversityofChicagoLawReview,vol.56,no.4,1989,pp.1175。 [14]这一点，如今――由于法律太多――已经为法律必须可能为人们所知晓所替代。大量的立法，实际上包括律师都不可能完全在不查阅法律的情况下知晓（因此现代社会需要律师，甚至律师也需要律师），更不用说常人了。 [15]《自由秩序原理》，页194。 [16]哈耶克，《个人主义与经济秩序》，贾湛、文跃然等译，北京经济学院出版社，1991年，页7以下。 [17]最典型的例子就是哈耶克赞扬过的Marburyv.Madison案件；关于此案的背景，请参见，苏力，“制度是如何形成的”，《比较法研究》，1998年1期。]]></content>
      <categories>
        <category>林中路</category>
      </categories>
      <tags>
        <tag>自由</tag>
        <tag>法治</tag>
        <tag>哈耶克</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[林中路（6）：哈耶克著作集]]></title>
    <url>%2F2019%2F02%2F16%2F%E6%9E%97%E4%B8%AD%E8%B7%AF%EF%BC%886%EF%BC%89%EF%BC%9A%E5%93%88%E8%80%B6%E5%85%8B%E8%91%97%E4%BD%9C%E9%9B%86%2F</url>
    <content type="text"><![CDATA[哈耶克著作集 书名 作者 译者 出版社 豆瓣评分 《致命的自负 : 社会主义的谬误》 弗里德利希·哈耶克 冯克利等 中国社会科学出版社 8.8(2456人评价) 《通往奴役之路》 弗里德利希·奥古斯特·哈耶克 王明毅/冯兴元 中国社会科学出版社 8.8(10206人评价) 《自由秩序原理》 弗里德利希·哈耶克 邓正来 三联书店 9.1(953人评价) 《自由宪章》 弗里德利希·奥古斯特·哈耶克 杨玉生 中国社会科学出版社 9.0(550人评价) 《法律、立法与自由(第一卷) : 规则与秩序》 弗里德利希·哈耶克 邓正来 张守东 李静冰 中国大百科全书出版社 9.1(318人评价) 《凯恩斯大战哈耶克》 [美]尼古拉斯•韦普肖特 闾佳 机械工业出版社 8.1(601人评价) 《哈耶克文选 : 哈耶克论文演讲集》 弗里德利希·哈耶克 冯克利 江苏人民出版社 9.1(548人评价) 《个人主义与经济秩序》 弗里德利希·哈耶克 邓正来 生活·读书·新知三联书店 8.8(474人评价) 《个人主义与经济秩序》 弗里德利希·哈耶克 邓正来 复旦大学出版社 8.9(37人评价) 《法律、立法与自由(第二、三卷) : 社会正义的幻象和自由社会的政治秩序》 弗里德利希·哈耶克 邓正来 张守东 李静冰 中国大百科全书出版社 9.3(202人评价) 《科学的反革命 : 理性滥用之研究》 弗里德利希·哈耶克 冯克利 译林出版社 8.4(268人评价) 《货币的非国家化 : 对多元货币的理论与实践的分析》 弗里德利希·哈耶克 姚中秋 新星出版社 8.9(543人评价) 《物价与生产》 弗里德利希·哈耶克 滕维藻 / 朱宗风 上海人民出版社 8.6(23人评价) 《资本主义与历史学家》 弗里德利希·哈耶克 秋风 吉林人民出版社 7.6(91人评价) 书名 作者 译者 出版社 豆瓣评分 《邓正来选译哈耶克论文集（全三册）》 弗里德利希·哈耶克 邓正来 首都经济贸易大学出版社 8.8(17人评价) 《哈耶克传》 阿兰・艾伯斯坦 秋风 中国社会科学出版社 8.3(441人评价) 《哈耶克评传》 [美]布鲁斯·考德威尔 冯克利 商务印书馆 8.8(207人评价) 《哈耶克文选》 弗里德利希·哈耶克 冯克利 河南大学出版社 9.7(41人评价) 《重读哈耶克》 韦森 中信出版股份有限公司 7.5(127人评价) 《知识分子为什么反对市场》 罗伯特・诺齐克 秋风 吉林人民出版社 7.6(154人评价) 《漫说哈耶克》 秋风 中信出版社 7.5(183人评价) 《哈耶克与古典自由主义》 萨丽 秋风 贵州人民出版社 8.8(56人评价) 《自由的铁笼 : 哈耶克传》 [英]甘布尔 王晓冬/朱之江 江苏人民出版社 7.3(113人评价) 《心智.知识与道德 : 哈耶克的道德哲学及其基础研究》 马永翔 生活．读书．新知三联 8.9(23人评价) 《哈耶克自由理论研究》 何信全 北京大学出版社 7.3(34人评价) 《自由主义政治哲学 : 哈耶克的政治思想》 霍伊 刘锋 北京三联书店 8.2(21人评价) 《法律秩序与自由正义 : 哈耶克的法律与宪政思想》 高全喜 北京大学出版社 8.0(25人评价) 《周德伟论哈耶克》 周德伟 北京大学出版社 6.6(29人评价) 《自由与法律》 布鲁诺·莱奥尼 秋风 吉林人民出版社 8.1(51人评价) 《知识自由与秩序》 [德]帕普克 中国社会科学出版社 7.9(98人评价) 《新自由主义不死之谜》 [英] 科林·克劳奇 蒲艳 中国人民大学出版社 7.7(18人评价) 《哈耶克法律哲学的研究》 邓正来 法律出版社 8.2(27人评价) 《哈耶克社会理论》 邓正来 复旦大学出版社 8.4(36人评价) 《哈耶克法律哲学》 邓正来 复旦大学出版社 8.1(15人评价) 《规则·秩序·无知》 邓正来 生活·读书·新知三联书店 8.6(64人评价) 《哈耶克读本》 邓正来 编 北京大学出版社 8.5(69人评价) 《哈耶克论文集》 邓正来 编译 首都经济贸易大学出版社 9.0(59人评价) 《自由主义社会理论 : 解读哈耶克《自由秩序原理》 邓正来 山东人民出版社 8.2(35人评价)]]></content>
      <categories>
        <category>林中路</category>
      </categories>
      <tags>
        <tag>自由</tag>
        <tag>法治</tag>
        <tag>哈耶克</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[林中路（5）：陆建德 | 回忆中的“新乐音”]]></title>
    <url>%2F2019%2F02%2F15%2F%E6%9E%97%E4%B8%AD%E8%B7%AF%EF%BC%885%EF%BC%89%EF%BC%9A%E9%99%86%E5%BB%BA%E5%BE%B7%20%E5%9B%9E%E5%BF%86%E4%B8%AD%E7%9A%84%E2%80%9C%E6%96%B0%E4%B9%90%E9%9F%B3%E2%80%9D%2F</url>
    <content type="text"><![CDATA[（本文是陆建德为朱利安·巴恩斯《终结的感觉》所撰导言，该书2012年8月由译林出版社出版。） 不变的自我往往是美好的虚构。我们不少成语（如白璧无瑕、怀瑾握瑜、怀真抱素、泥而不滓）都强调人的恒定性，《离骚》里的诗人还自称“纷吾既有此内美兮，又重之以修能”。（楚辞中有很多类似的表述，比如《怀沙》：“内厚质正”；“文质疏内兮，众不知余之异彩。” ）“内美”指先天的品质，是一种先于社会与历史的完足，就像橘树的基因一般。其实自我受制于时间，通过回忆不断生成变化。如果一个人在不同的时候对发生在自己身上的事理解有所不同，那么回忆在发挥关键作用。朱利安•巴恩斯（1946—）的《终结的感觉》是一部回忆之书，它既是在回忆中写成，也是对回忆的心理特点的探究。小说叙述者对过去的理解变了，对自己的认识也变了。“我是谁？”这一问题不仅在无形中引导主人公与自己的搏斗，也在每一位读者的耳边回响。 回忆是一口深井。爱尔兰诗人西默斯•希尼在《个人的诗泉》里写道：有的浅井闪过访问者的脸庞，而“有些井发出回声，用纯洁的新乐音/应对你的呼声”。两种不同的井象征了不同的回忆目的，一种是自恋式的，另一种是为了发掘： 去拨弄污泥，去窥测根子，去凝视泉水中的那喀索斯，他有双大眼睛，都有伤成年人的自尊。我写诗是为了认识自己，使黑暗发出回音。 但是“拨弄污泥，去窥测根子”很难与“认识自己”完全对立起来。有时候人们需要有“拨弄污泥，去窥测根子”的勇气，因为那个沉睡的地带也许正是自己记忆的禁区。《终结的感觉》的叙述者就是不断在记忆深处中挖掘，冲破自己不知不觉间设置的障碍，最终听到了“纯洁的新乐音”。他跳脱出狭隘的自我，开始从他人的角度来观察、分析自己的历史，并且变得具有同情心。 早在80年代，巴恩斯就凭他为数不多的几部小说跻身英国一流作家的行列，我国英语文学研究界早就对他寄予厚望。1997年出版的《英国小说研究》第四卷《现代主义之后》（中国社会科学出版社）就收有阮炜先生论巴恩斯和他的成名之作《福楼拜的鹦鹉》（1984）的文章。 巴恩斯在牛津就学时读的是“现代语言”专业，主修法语。《福楼拜的鹦鹉》展示出巴恩斯在法国文学上的精湛知识，深得法国人好感，出版那年获法国梅迪奇奖，数年后作者还被法国文化部授予文学艺术军官勋章。可以说他在法国是最受欢迎的当代英语作家之一。巴恩斯曾任《纽约客》的伦敦通讯员，在美国也拥有大量读者。以笔者之见，巴恩斯是当今世界上最优秀的作家之一。 《终结的感觉》叙述者托尼•韦布斯特是一个极其普通的人，年龄应该与巴恩斯差不多，在上世纪60年代上的大学。小说分为两部分。第一部分是托尼对学生时代的回顾（也夹杂了四十年之后的评点），主要讲他中学与大学的生活。 托尼在中学里有两个好朋友，形成三人帮，他们的手表表面从来戴在手腕内侧，后来这“铁三角”增加了艾德里安•芬。艾德里安智力超群，他喜爱的作家是加缪和尼采，可见高出同学一截。他在课堂上的表现如此出色，看来是读书的好料子。艾德里安的母亲早就离家出走，父亲一人抚养他和妹妹。小团伙里的同伴用粗鲁的语言追问艾德里安，他母亲究竟为什么离开，艾德里安不知道，他的回答十分平和。他说自己“爱母亲，敬重父亲”。奇怪的是艾德里安没有因为成长于单亲家庭而愤世嫉俗，怨天尤人。反之，他待人接物显得比“铁三角”中的成员都要成熟，对少年人故意冲撞的用语，他小心回避。他获得奖学金进剑桥，大家并不意外。 中学毕业后，托尼到布里斯托尔读大学，很快就交上了维罗妮卡，一位学西班牙文学的女生。他曾经把自己的女朋友介绍给中学里那几位朋友，一起在伦敦留了影。有一次托尼应邀到维罗妮卡家度周末，那是一次让他难以忘怀的经历。托尼带的皮箱偏大了，维罗妮卡的父亲借此开玩笑，很不得体，他的一系列语言都有故意伤害托尼自尊心之嫌。托尼意识到自己家庭背景不及女友，感到有点羞愧。维罗妮卡家是独栋房子，位于比较殷实的肯特郡，她父亲还是公务员，那在英国是受人尊敬的职业，而托尼压根儿没有提及他自己的父亲，想来是他的自卑在作怪。托尼做客时因自卑更加拘谨，然而只有维罗妮卡的母亲使他感到一丝温暖。托尼究竟爱维罗妮卡吗？从托尼早期的回忆来看，他对这位身材娇小的姑娘的兴趣主要停留在生理学层面上，性冲动说来就来，情感之流却是受到压制的。他认定维罗妮卡是处女，仅仅因为她不愿意和他上床。并不合理的推断会被下意识地误解为“事实”，一旦“事实”崩塌，心理上就无法承受。 他们最终还是上床了，但是在托尼的回忆里是这样表述的：“我们分手以后，她和我上了床。” 对托尼而言，那次经历太重要了，然而他的叙述十分简略，可见他是不大愉快的。托尼没有告诉读者他的重大发现以及随之而起的不快，这一点逃不过读者的眼睛。维罗妮卡有一点如何安全做爱经验，这让托尼无法忍受。但是他没有说。巴恩斯在关节处留白，这是他的高明。实际上两人上床并不是在分手之后，而是在分手之前。托尼颠倒时间次序，图的是自己的方便：他不必因做爱而负有责任，以更明确的语言来界定两人的关系。维罗妮卡意识到了他的畏缩与自私，说：“你可以相信任何你愿意相信的事情。” 两人居然就这样在上床后立即分手了，维罗妮卡感觉极坏，不过她并没有记恨。维罗妮卡的慷慨可以从这件事上看出来：大学最后一年，艾德里安写信给托尼，表示想跟维罗妮卡交往，希望征得托尼的理解与同意。这封信本来是善意的见证，如果托尼回信，做出一些友好的表示，他们可以继续做朋友，结束托尼制造出来的他与维罗妮卡之间的敌对。但是托尼对这封本来不应引起误会的信件做了很多猜想，断定是维罗妮卡“想让我知道，她，维罗妮卡，是如何以旧换新的：换成了我最聪明的朋友，……，同时，也为了警告我：如果我打算去见艾德里安，她也会悠然到场——很明显，这是想让我不要和艾德里安见面。” 他习惯于从最坏的方面来认识别人的动机，一而再，再而三。 这是他的自白：“我觉得我有一种生存的本能，一种自我保护的本能。也许这就是维罗妮卡所说的胆小吧，但我称之为温和。”他回了信，并且交代了大致内容：他的顾忌和告诫，然后是他的祝福。他决心再也不受这两人的干扰，要把他们从自己的生活中清除出去。托尼自己受到了非正常情绪的伤害，但是他却断定维罗妮卡“很早以前一定受过伤害”。他并不是完全无意识的。他知道，很多人做出一个本能的决定，会找出一些大道理来解释自己的决定，并将结果称为常识。然而，托尼关于自己那封回信的记忆是否有误？ 就在他大学毕业不久，传来艾德里安在剑桥自杀的消息。在这样的时候，他应该向维罗妮卡表达哀思，但是他是这样想的：“她肯定会觉得我很虚伪。如果我联系她，她要么会对我不理不睬，要么就会扭曲事实，那样我更没办法理清头绪了。” 小说第二部分是主人公退休之后的叙述。托尼志愿管理住处附近一家医院的图书室，穿梭于病房送书、收书。但是有一天，他的生活变了：他收到了法律事务所来信，原来维罗妮卡的母亲去世了，她五年前立下了遗嘱，要遗赠托尼五百英镑，并要把艾德里安的日记转给托尼保管。为什么有这笔遗赠？为什么维罗妮卡的母亲有权支配艾德里安的日记？正是这些问题启动了托尼对自己过往历史的修正，寻回了被记忆扭曲、删改甚至完全抽毁的片段。 无非出于好奇，他想知道维罗妮卡和艾德里安后来的生活，更想从维罗妮卡那里索取法律上说现在应该属于他的艾德里安日记。托尼又与维罗妮卡联系，即使碰了钉子，还是不依不饶。维罗妮卡不堪其扰，当面交给他一封信的复印件，那就是托尼当年写给艾德里安的所谓告诫与祝福。信上处处是平庸的恶毒，如不是亲眼所见，托尼难以相信。此时他带着羞愧回忆往事。多年封压住自己记忆的那块青石板渐渐被移开，托尼“拨弄污泥，去窥测根子”，开始真正认识他自己。 他的记忆之城出现了裂痕，原来坚实的地基动摇了。记忆作弄人，只说明时间是宰制一切的神力：“时间先安顿我们，继而又迷惑我们。我们以为自己是在慢慢成熟，而其实我们只是安然无恙而已。我们以为自己很有担当，其实我们十分懦弱。我们所谓的务实，充其量不过是逃避现实，绝非直面以对。”托尼突然明白，他以往的人生故事是讲给人听的，更是讲给自己听的，免不了有很多下意识的调整、修饰和剔除。维罗妮卡和艾德里安决定写信给托尼，告知他们的恋爱关系，绝非故意为之的残酷之举。托尼自问： 我为什么要[在回信中]表现出很愤怒呢？因为受伤的自尊、考试前的压力、孤立感？这些全都是借口而已。不，我此刻没有感到耻辱，或者愧疚，而是我生命中很少有过的、比前两者更强烈的感觉：悔恨。 托尼在自己这封粗鄙的信面前低下头来，他向维罗妮卡真诚致歉，并通过搜寻记忆深处角落里的点点滴滴来建构一个新的自我。原来他关注的焦点总是自己，现在他打听四十年来维罗妮卡和她家庭的境况。她父亲过世后，母亲把房子卖了，在伦敦买了公寓，后来收了房客。这些虽然是简约得不能再简约的陈述，而且没有任何抱怨或感伤，读者却不会不意识到一家人经济与社会地位的变化。维罗妮卡自己的居住区也是各种肤色的人杂居的，那区域在一般伦敦人心目中的地位不必明言。 新的细节的涌现还使托尼想到别人。中学同学罗布森因女友怀孕自杀了，只留下一张纸条，上面写着：“妈，对不起。”当时“铁三角”听到这消息非但毫无触动，甚至还妒忌乃至怨恨罗布森：为什么你这个小子有福气把女生肚子搞大？他们猜测死者的女友究竟是一本正经的处女还是脏婊子。现在托尼由自己的经历想到当年那位从未谋面的姑娘，想到她所受的压力，她的痛苦和社会对她的歧视，想到她腹中的胎儿如生下来已经年近五十。这时他想请求她原谅自己和朋友们的冷漠和恶毒，尽管她完全不知道。 最重要的是他渐渐想起一些朦胧的情愫在他和维罗妮卡之间萌发。有一天晚上，从来不喜欢跳舞的维罗妮卡心血来潮，竟然在他房间里踩着留声机上播放的歌曲拍子轻盈起舞。她跳得优美，让人怀疑她学过芭蕾；她跳得投入，“转啊转地就撞到我身上来了”。可是当时的托尼却是拘谨的，大概还在担心维罗妮卡嫌他音乐鉴赏的档次偏低。 同一件事出现于两个部分，内容却不能吻合。小说开头有六个场景，第四个是：“——一条河莫名地逆流而上，奔涌跃腾，在六七缕追逐的手电筒光线照耀下破光粼粼”。同样的场面又出现于小说收尾处，可见它特别的分量。在年轻的托尼的叙述中，某晚他和同学们到布里斯托尔旁的塞汶河河边观潮。他的记忆是这样的：“我们一干人在河岸上一直等到午夜以后，终于，等待获得了回报。”然后是小说中轰然作响的潮汐描写。维罗妮卡一直不在场。但是在四十年之后的回忆中，缺席的维罗妮卡出场了：托尼和维罗妮卡两人一起坐在河边一块湿漉漉的毯子上，手握着手，溶溶月色下潮汐汹涌卷来。同去的朋友们打着手电筒追随潮汐消失在夜色之中，他们两人依然在老地方坐着，谈论着世上不可思议之事。那天出行时，维罗妮卡还带上一只保温瓶，装满热巧克力饮料，想必是为她和托尼准备的，两人可以从同一个杯子里饮用。为什么前后记忆出现如此巨大的偏差？对此，小说的开端有所交代。那是第五个场景： ——另一条河，宽阔而灰暗，一阵狂风搅乱了水面，掩盖了河的流向。 读完小说回到第一页，我们才明白这句话可能喻指托尼自己的生命之河，他的黑暗的自我，他意识层面之下的潜流。这“另一条河”在隐约规定着四十年前托尼的选择，使他永远失去了生活中的一种可能性，并且间接为相关人物（包括维罗妮卡和艾德里安）的命运负责。觉醒之后的“新乐音”让人感到温暖，它带来新生，也带来别样的痛苦。小说的“终结”表达了叙述者托尼的不安，也令读者不安：“有累积。有责任。除此之外，还有动荡不安。浩大的动荡不安。” 这条“宽阔而灰暗”的河流也可以象征艾德里安的情感世界。艾德里安究竟是怎样的人，依然不很确定。表面上看起来他见识不凡，极其冷静而富有理性，最后走上自杀之路，仿佛全部是出于自己的选择和意志。这只是托尼的印象而已，绝非艾德里安人生故事的全部。为了保留悬念，这篇序言不涉及小说最后披露的情节。 对一位上世纪60年代的中学生而言，艾德里安的历史观是非常新潮的。历史既不是胜利者的谎言，也不是失败者的自欺欺人，也许，“不可靠的记忆与不充分的材料相遇所产生的确定性就是历史。”我们可以说，巴恩斯几十年来都在试图回答英国史学家E. H. 卡尔提出来的问题：什么是历史？在讨论谁该为第一次世界大战爆发负责的时候，艾德里安说：“这一整个追究责任的行为难道不就是一种逃避吗？我们责备某个个人，其目的就是想为其余人开脱罪责。或者呢，我们归咎于历史进程，是想为一个个个体免责。抑或将一切归咎于一片混沌，那结果也一样。在我看来，似乎有——或者曾经有—— 一条个体责任链，所有责任不可或缺，但此链并非无限之长，不然谁都可以轻率归咎于他人。当然，我想要究责，这或许只是反映了我本人的心境，并非对事件的合理分析。” 因此，各种历史故事难免经过写作者的中介和建构：“我们必须了解历史学家的历史才能理解此刻放在我们面前的历史版本。”就《福楼拜的鹦鹉》来说，读者要了解故事的真相，还得进入叙述者、退休医生布雷斯科特的情感生活。在该书卷首，巴恩斯引用了福楼拜在致剧作家费多信中一句带有反讽意味的话：“当你为朋友立传时，一定要做得像你在为他报仇雪恨那样。”很多传记确实像是出自传主好友的手笔。司马迁为屈原、贾谊作传，何尝不是在为他们报仇雪恨？不是为他自己报仇雪恨？下意识地在传中为朋友报仇，是把意愿等同于事实。福楼拜这句话说明他不仅怀疑单一视角，而且怀疑历史是否可靠。然而常人读书往往被书上的文字牵着鼻子走。《十又二分之一章世界史》（1989）是巴恩斯的另一部杰作，小说一开头就颠覆了最权威的故事：《旧约•创世记》中的挪亚方舟。讲故事的细微的声音来自方舟上藏身于船体的木蠹。挪亚虚构出上帝以及上帝与他的契约，为的是自己可以随心所欲，为的是可以[永远]借着上帝的名义压迫或消灭他自己不喜欢的周边其他部落和民族。他才是真正的暴君。在这部书的“二分之一章”中，讨论着“爱”的主题的叙述者说：“我们编造出故事来掩盖我们不知道或者不能接受的事实；我们保留一些事情真相，围绕这些事实编织新的故事。我们的恐慌和痛苦只有靠安慰性的编造功夫缓解；我们称之为历史。”在《终结的感觉》里，托尼所面临的任务是透过记忆的迷雾认识他自己的“编织”，他那几乎是出于自我保护的潜意识中“安慰性的编造功夫”。 末了再提供一些与此书相关的花絮。巴恩斯这部新作是献给“帕特”的，即他夫人帕特•凯伐纳（巴恩斯在《福楼拜的鹦鹉》，之前出版过几本犯罪小说，都用了“丹•凯伐纳”的笔名）一位著名的文学经纪人，2008年10月死于脑瘤。小说家马丁•艾米斯（《幸运的吉姆》作者金斯利•艾米斯之子）一度也是凯伐纳的主顾。90年代中期，马丁•艾米斯中断与凯伐纳的合同，他与巴恩斯之间不免产生一点嫌隙，一时成为媒体上的热门话题。关注一下艾米斯关于穆斯林、伊拉克的言论，不难知道艾米斯与巴恩斯道不相谋。艾米斯不会从木蠹的角度来挑战自以为蒙上帝之恩的挪亚的权威话语。《终结的感觉》是帕特•凯伐纳逝世后巴恩斯的第一部小说，获2011年曼—布克奖，也可以让作者告慰亡妻。还要说一下，这一届曼-布克小说奖评委会主席是身份非常特殊的斯泰拉•雷明顿。她多年服务于英国军情五处，1992年任该处主管，1996年退休，出版了回忆录《公开的秘密》和侦探小说数种。一个享有世界声誉的国家级情报机构主管有如此难得的文学才能，在各国是少见的。“9.11”后，雷明顿曾批评美国，称布什政府反应过度。 《终结的感觉》也可以理解为巴恩斯对著名学者弗兰克•克莫德（1919—2010）的追忆。克莫德的批评著作《终结的意义：小说理论研究》（1967年，2000年再版）讨论的是末世论思维与阅读、写作小说的关系。克莫德认为，对终局的预测反过来会影响到对初始和中间阶段的理解，或者说，故事的结尾使得前面发生的一切具有意义。在巴恩斯这部同名小说第二部第一页，托尼设定自己站在未来的某一点回望过去，体会岁月或时间带来的新的情感。可以断定，巴恩斯得益于克莫德四十几年前的小说理论。但是对人写的历史的真实性，巴恩斯也许更加悲观：如果每个人都有私密的、未曾记录的历史，而“我”以及“我”的记忆又受制于时间，史学家怎么才能讲出可信的故事来？好在他竟从怀疑主义出发，让过去的深井发出确确实实的“新乐音”。]]></content>
      <categories>
        <category>林中路</category>
      </categories>
      <tags>
        <tag>回忆</tag>
        <tag>个人历史</tag>
        <tag>自我</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[林中路（4）：宗白华 | 歌德之人生启示]]></title>
    <url>%2F2019%2F02%2F10%2F%E6%9E%97%E4%B8%AD%E8%B7%AF%EF%BC%884%EF%BC%89%EF%BC%9A%E5%AE%97%E7%99%BD%E5%8D%8E%20%E6%AD%8C%E5%BE%B7%E4%B9%8B%E4%BA%BA%E7%94%9F%E5%90%AF%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[人生是什么？人生的真相如何？人生的意义何在？人生的目的是何？这些人生最重大最中心的问题，不只是古来一切大宗教家哲学家所殚精竭虑以求解答的。 世界上第一流的大诗人凝神冥想，深入灵魂的幽邃，或纵身大化中，于一朵花中窥见天国，一滴露水参悟生命，然后用他们的生花之笔，幻现层层世界，幕幕人生，归根也不外乎启示这生命的真相与意义。宗教家对这些问题的方法与态度是预言的和说教的，哲学家是解释的和说明的，诗人文豪是表现的启示的。 荷马的长歌启示了希腊艺术文明幻美的人生与理想，但丁的神曲启示了中古基督教文化心灵的生活与信仰，莎士比亚的剧本表现了文艺复兴时人们的生活矛盾与权力意志。 至于近代的，建筑于这三种文明精神之上而同时开展一个新时代，所谓近代人生，则由伟大的歌德以他的人格、生活、作品表现出它的特殊意义与内在的问题。 歌德对人生的启示有几层意义，几种方面。就人类全体讲，他的人格与生活可谓极尽了人类的可能性。他同时是诗人、科学家、政治家、思想家，他也是近代泛神论信仰的一个伟大的代表。他表现了西方文明自强不息的精神，又同时具有东方乐天知命宁静致远的智慧。 德国哲学家齐美尔（Simmel）说：“歌德的人生所以给我们以无穷兴奋与深沉的安慰的，他只是一个人，他只是极尽了人性，但却如此伟大，使我们对人类感到有希望，鼓动我们努力向前做一个人。”我们可以说歌德是世界一扇明窗，我们由他窥见了人生生命永恒幽邃奇丽广大的天空！ 再缩小范围，就欧洲文化的观点说，歌德确是代表文艺复兴以后近代人的心灵生活及其内在的问题。近代人失去了希腊文化中人与宇宙的谐和，又失去了基督教对一超越上帝虔诚的信仰。人类精神上获得了解放，得着了自由；但也就同时失所依傍，彷徨、摸索、苦闷、追求，欲在生活本身的努力中寻得人生的意义与价值。歌德是这时代精神伟大的代表，他的主著《浮士德》是这人生全部的反映与其问题的解决（现代哲学家斯宾格勒 Spengler 在他名著《西方文化之衰落》中名近代文化为浮士德文化）。歌德与其替身浮士德一生生活的内容就是尽量体验这近代人生特殊的精神意义，了解其悲剧而努力以解决其问题，指出解救之道。所以有人称他的浮士德是近代人的《圣经》。 但歌德与但丁、莎士比亚不同的地方，就是他不单是由作品里启示我们人生真相，尤其在他自己的人格与生活中表现了人生广大精微的义谛。所以我们也就从两方面去接受歌德对于人类的贡献： 一、从他的人格与生活，了解人生之意义；二、从他的文艺作品，欣赏人生真相之表现。 一、歌德人格与生活之意义比学斯基（Bielschowsky)在《歌德传记•导论》中分析歌德人格的特性，描述他生活的丰富与矛盾，最为详尽（见拙译《歌德论》）。但这个矛盾丰富的人格终是一个谜。所谓谜，就是这些矛盾中似乎潜伏着一个道理，由这个道理我们可以解释这个迷，而这个道理也就是构成这个谜的原因。我们获得这个道理解释了这个谜，也就可说是懂了那谜的意义。 歌德生活中之矛盾复杂最使人有无穷的兴趣去探索他人格与生活的意义，所以人们关于歌德生活的研究与播述异常丰富，超过世界任何文豪。近代德国哲学家努力于歌德人生意义的探索者尤多，如齐美尔（Simmel)［Georg Simmel, 1858-1918. 通译齐美尔。德国哲学家，社会学家，生命哲学代表人物之一。主要著作有《论社会的区别》、《伦理学导论》、《历史哲学问题》、《叔本华和尼采》、《社会学的根本问题》、《人生观》等］、李凯尔特（Rickert)［Heinrich Rickert, 1863-1936. 德国哲学家，新康德主义西南德学派主要代表人物。主要著作有《自然科学概念形式的界限》、《文化科学和自然科学》、《历史哲学问题》、《哲学体系》、《现代文化人康德》等］、龚多夫（Gundolf)、寇乃曼（Küehnemann)、可尔夫（Korff)等等，尤以可尔夫的研究颇多新解。我们现在根据他们的发挥，略参个人的意见，叙述于后。 我们先再认清这歌德之谜的真面目：第一个印象就是歌德生活全体的无穷丰冨；第二个印象是他一生生活中一种奇异的谐和；第三个印象是许多不可思议的矛盾。这三种相反的印象却是互相依赖，但也使我们表面看来，没有一个整个的歌德而呈现无数歌德的图画。 首先有少年歌德与老年歌德之分。细看起来，可以说有一个莱布齐希大学学生的歌德，有一个少年维特的歌德，有一个魏玛朝廷的歌德，有一个意大利旅行中的歌德，与席勒交友时的歌德，艾克曼谈话中的哲人歌德。这就是说歌德的人生是永恒变迁的，他当时朋友都有此感，他与朋友爱人间的种种误会与负心皆由于此。 人类的生活本都是变迁的，但歌德每一次生活上的变迁就启示一次人生生活上重大的意义，而留下了伟大的成绩，为人生永久的象征。这是什么原故？因歌德在他每一种生活的新倾向中，无论是文艺政治科学或恋爱，他都是以全副精神整个人格浸沉其中；每一种生活的过程里都是一个整个的歌德在内。维特时代的歌德完全是一个多情善感热爱自然的青年，著《伊菲格尼》（Iphigenie〕的歌德完全是个清明懦雅，徘徊于罗马古墟中希腊的人。他从人性之南极走到北极，从极端主观主义的少年维特走到极端客观主义的伊菲格尼，似乎完全两个人。然而每个人都是新鲜活泼原版的人。所以他的生平给与我们一种永久青春永远矛盾的感觉。 歌德的一生并非真是从迷途错误走到真理，乃是继续地经历全人生各式的形态。他在《浮士德》中说： “我要在内在的自我中深深领略，领略全人类所赋有的一切。最崇高的最深远的我都要了解。我要把全人类的苦乐堆积在我的胸心，我的小我、便扩大成为全人类的大我。我愿和全人类一样，最后归于消灭。” 这样伟大勇敢的生命肯定，使他穿历人生的各阶段，而每阶段都成为人生深远的象征。他不只是经过少年诗人时期，中年政治家时期，老年思想家、科学家时期，就在文学上他也是从最初罗可可（Rococo）式的纤巧到少年维特的自然流露，再从意大利游后古典风格的写实到老年时浮士德第二部象征的描写。 他少年时反抗一切传统道德势力的缚束，他的口号“情感是一切！”老年时尊重社会的秩序与礼法，重视克制的道德。他的口号“事业是一切！”在对人接物方面，少年歌德是开诚坦率热情倾倒的待人。在老年时则严肃令人难以亲近。在政治方面，少年的大作中“瞿支” (Goetz)临死时口中喊着“自由”，而老年歌德对法国大革命中的残暴深为厌恶，赞美拿破仑重给欧洲以秩序。 在恋爱方面，因各时期之心灵需要，舍弃最知心、最有文化的十年女友石坦因夫人而娶一个无知识、无教育纯朴自然的扎花女子。歌德生活是努力不息，但又似乎毫无预计，听机缘与命运之驱使。所以有些人悼惜歌德荒废太多时间做许多不相干的事，像绘画，政治事务，研究科学，尤其是数十年不断的颜色学研究。但他知道这些“迷途”、“错道”是他完成他伟大人性所必经的。人在“迷途中努力，终会寻着他的正道”。 歌德在生活中所经历的“迷途”与“正道”表现于一个最可令人注意的现象。这现象就是他生活中历次的“逃走”。他的逃走是他浸沉于一种生活方向将要失去了自己时，猛然的回头，突然的退却，再返于自己的中心。他从莱布齐希大学身心破产后逃回故乡，他历次逃开他的情人弗利德利克，绿蒂，丽莉等，他逃到魏玛，又逃脱魏玛政务的压迫走入意大利艺术之宫。他又从意大利逃回德国。他从文学逃入政治，从政治逃入科学。老年时且由西方文明逃往东方，借中国印度波斯的幻美热情以重振他的少年心。每一次逃走，他新生一次，他开辟了生活的新领域，他对人生有了新创造新启示。他重新发现了自己，而他在“迷途”中的经历已丰富了深化了自己。他说：“各种生活皆可以过，只要不失去了自己。” 歌德之所以敢于全心倾注于任何一种人生方面，尽量发挥，以致有伟大的成就，就是因为他自知不会完全失去了自己，他能在紧要关头逃走退回他自己的中心。这是歌德一生生活的最大的秘密。但在这个秘密背后伏有更深的意义。我们再进一步研究之。 歌德在近代文化史上的意义可以说，他带给近代人生一个新的生命情绪。他在少年时他已自觉是个新的人生宗教的预言者。他早期文艺的题目大都是人类的大教主如普罗美修斯(Prometheus)［希腊神话中创造人类和造福于人类的受人尊崇的神，提坦神伊阿佩托斯的儿子］，苏格拉底，基督与摩哈默德。 这新的人生情绪是什么呢？就是“生命本身价值的肯定”。基督教以为人类的灵魂必须赖救主的恩惠始能得救，获得意义与价值。近代启蒙运动的理性主义则以为人生须服从理性的规范，理智的指导，始能达到高明的合理的生活。歌德少年时即反抗十八世纪一切人为的规范与法律。他的《瞿支》是反抗一切传统政治的缚束；他的维特是反抗一切社会人为的礼法，而热烈崇拜生命的自然流露。 一言蔽之，一切真实的，新鲜的，如火如荼的生命，未受理性文明矫揉造作的原版生活，对于他是世界上最可宝贵的东西。而这种天真活泼的生命他发现于许多绚漫而朴质如花的女性。他作品中所描写的绿蒂，玛甘泪，玛丽亚等，他自身所迷恋的弗利德丽克，丽莉，绿蒂等，都灿烂如鲜花而天真活泼，朴素温柔，如枝头的翠鸟。而他少年作品中这种新鲜活跃的描写，将妩媚生命的本体熠烁在读者眼前，真是在他以前的徳国文学所未尝梦见的，而为世界文学中的粒粒晶珠。 这种崇拜真实生命的态度也表现于他对自然的顶礼。他1782年的《自然赞歌》可为代表。译其大意如下： 自然，我们被他包围，被他环抱；无法从他走出，也无法向他深入。他未得请求，又未加警告，就携带我们加人他跳舞的圈子，带着我们动，直待我们疲倦极了，从他臂中落下。他永远创造新的形体，去者不复返，来者永远新，一切都是新创，但一切也仍旧是老的。他的中间是永恒的生命演进，活动。但他自己并未曾移走。他变化无穷，没有一刻的停止。他没有留恋的意思，停留是他的诅咒，生命是他最美的发明，死亡是他的手段，以多得生命。 歌德这时的生命情绪完全是浸沉于理性精神之下层的永恒活跃的生命本体。 但说到这里，在我们的心影上会涌现出另一个歌德来。而这歌德的特征是谐和的形式，是创造形式的意志。歌德生活中一切矛盾之最后的矛盾，就是他对流动不居的生命与圆满谐和的形式有同样强烈的情感。他在哲学上固然受斯宾诺莎泛神论的影响；但斯宾诺莎所给予他的仍是偏于生活上道德上的受用，使他紊乱烦恼的心灵得以入于清明。 以大宇宙中永恒谐和的秩序整理内心的秩序，化冲动的私欲为清明合理的意志。但歌德从自己的活跃生命所体验的能动的创造的宇宙人生，则与斯宾诺莎倾向机械论与几何学的宇宙观迥然不同。所以歌德自己的生活与人格却是实现了德国大哲学家莱布尼茨［德国著名哲学家、科学家。著有《形而上学谈话》、《人类理智论》、《单子论》、《神正论》等］的宇宙论。宇宙是无数活跃的精神原子，每一个原子顺着内在的定律，向着前定的形式永恒不息的活动发展，以完成实现他内潜的可能性，而每一个精神原子是一个独立的小宇宙，在他里面像一面镜子反映着大宇宙生命的全体。歌德的生活与人格不是这样一个精神原子么？ 生命与形式，流动与定律，向外的扩张与向内的收缩，这是人生的两极，这是一切生活的原理。歌德曾名之宇宙生命的一呼一吸。而歌德自己的生活实在象征了这个原则。他的一生，他的矛盾，他的种种逃走，都可以用这个原理来了解。当他纵身于宇宙生命的大海时，他的小我扩张而为大我，他自己就是自然，就是世界，与万物为一体。他或者是柔软地像少年维特，一花一草一树一石都与他的心灵合而为一，森林里的飞禽走兽都是他的同胞兄弟。他或者刚强地察觉着自己就是大自然创造生命之一体，他可以和地神唱道： 生潮中，业浪里， 淘上或淘下， 浮来又浮去！ 生而死，死而葬， 一个永恒的大洋， 一个连续的波浪， 一个有光辉的生长， 我架起时辰的机杼， 替神性制造生动的衣裳。 ——郭沫若译《浮士德》 但这生活片面的扩张奔放是不能维持的，一个个体的小生命更是会紧张极度而趋于毁灭的。所以浮士德见地神现形那样的庞大，觉得自己好像侏儒一般，他的狂妄完全消失： 我，自以力超过了火焰天使， 已把自由的力量使自然甦生， 满以为创造的生活可以俨然如神！ 啊，我现在是受了个怎样的处分！ 一声霹雳把我推堕了万丈深坑。 …… 哦，我们努力自身，如同我们的烦闷， 一样地阻碍着我们生长的前程。 ——郭沫若译《浮士德》 生命片面的努力伸张反要使生命受阻碍，所以生命同时要求秩序，形式，定律，轨道。生命要谦虚，克制，收缩，遵循那支配有主持一切的定律，然后才能完成，才能使生命有形式，而形式在生命之中。 依着永恒的，正直的 伟大的定律， 完成着 我们生命的圈。 ——摘《神性》一个有限的圈子 范围着我们的人生， 世世代代 排列在无尽的生命底链上。 ——摘《人类之界限》 生命是要发扬，前逬，但也要收缩，循轨。一部生命的历史就是生活形式的创造与破坏。生命在永恒的变化之中、形式也在永恒的变化之中。所以一切无常，一切无住，我们的心，我们的情，也息息生灭，逝同流水。向之所欣，俯仰之间，已成陈迹。这是人生真正的悲剧，这悲剧的源泉就是这追求不已的自心。 人生在各方面都要求着永久；但我们的自心的变迁使没有一景一物可以得暂时的停留，人生飘堕在滚滚流转的生命海中，大力推移，欲罢不能，欲留不许。这是一个何等的重负，何等的悲哀烦恼。所以浮士德情愿拿他的灵魂底毁灭与魔鬼打赌，他只希望能有一个瞬间的真正的满足，俾他可以对那瞬间说：“请你暂停，你是何等的美呀！” 由这话看来，一切无常的主因是在我们自心的无常，心的无休止的前进追求，不肯暂停留恋。人生的悲剧正是在我们恒变的心情中，歌德是人类的代表，他感到这人生的悲剧特别深刻，他的一生真是息息不停的追求前进，变向无穷。这心的变迁使他最感到苦痛负疚的就是他恋爱心情的变迁，他一生最热烈的恋爱都不能久住，他对每一个恋人都是负心，这种负心的忏悔自诉是他许多最大作品的动机与内容。剧本《瞿支》中，魏斯林根背弃玛利亚；剧本《浮士德》中，浮士德遗弃垂死的玛甘泪于狱中，是歌德最明显最沉痛的自诉。但他的生活情绪不停留的前进使他不能不负心，使他不能安于一范围，狭于一境界而不向前开辟生活的新领域。所以歌德无往而不负心，他弃掉法律投入文学，弃掉文学投入政治，又逃脱政治走入艺术科学，他若不负心，他不能尝遍全人生的各境地，完成一个最人性的人格。他说： 你想走向无尽么？ 你要在有限里面往各方面走！ 然而这个负心现象，这个生活矛盾，终是他生活里内在的悲剧与问题，使他不能不努力求解决的。这矛盾的调解，心灵负咎的解脱，是歌德一生生活之意义与努力。 再总结一句，歌德的人生问题，就是如何从生活的无尽流动中获得谐和的形式，但又不要让僵固的形式阻碍生命前进的发展。这个一切生命现象中内在的矛盾，在歌德的生活里表现得最为深刻。他的一切大作品也就是这个经历的供状。我们现在再从歌德的文艺创作中去寻歌德的人生启示与这问题最后的解答。 二、歌德文艺作品中所表现的人生与人生问题我们说过，歌德启示给我们的人生是扩张与收缩，流动与形式，变化与定律；是情感的奔放与秩序的严整，是纵身大化中与宇宙同流，但也是反抗一切的阻碍压迫以自成一个独立的人格形式。他能忘怀自己，倾心于自然，于事业，于恋爱；但他又能主张自己，贯彻自己，逃开一切的包围。歌德心中这两个方面表现于他生平一切的作品中。 他的剧本《瞿支》、《塔索》，他的小说《少年维特之烦恼》，是表现生命的奔放与倾注，破坏一切传统的秩序与形式。他的《伊菲格尼》与叙事诗《赫尔曼与多罗蒂》等，则内容外形都表现最高的谐和节制，以圆融高朗的优美的形式调解心灵的纠纷冲突。在抒情诗中他的《卜罗米陀斯》是主张人类由他自己的力量创造他的生活的领域，不需要神的援助，否认神的支配，是近代人生思想中最伟人的一首革命诗。但他在《人类之界限》、《神性》等诗中、则又承认宇宙间含有创造一切的定律与形式，人生当在永恒的定律与前进的形式中完成他自己；但人生不息的前进追求，所获得的形式终不能满足，生活的苦闷由此而生。这个与歌德生活中心相终始的问题则表现于他毕生的大作《浮士德》中。《浮士德》是歌德全部生活意义的反映，歌德生命中最深的问题于此表现，也于此解决。我们特别提出研究之。 浮士德是歌德人生情绪最纯粹的代表。《浮士德》戏剧最初本，所谓“原始浮士德”的基本意念是什么？在他下面的两句诗： 我有敢于入世的胆量， 下界的苦乐我要一概担当。 浮士德人格的中心是无尽的生活欲与无尽的知识欲。他欲呼召生命的本体，所以先用符咒呼召宇宙与行为的神。神出现后，被神呵斥其狂妄，他认识了个体生命在宇宙大生命面前的渺小。于是乃欲投身生命的海洋中体验人生的一切。他肯定这生命的本身，不管他是苦是乐，超越一切利害的计较，是有生活的价值的，是应当在他的中间努力寻得意义的。这是歌德的悲壮的人生观，也是他《浮士德》诗中的中心思想。 浮士德因知识追求的无结果，投身于现实生活，而生活的顶点，表现于恋爱，但这恋爱生活成了悲剧。生活的前进不停，使恋爱离弃了浮士德，而浮士德离弃了玛甘泪，生活成了罪恶与苦痛。《浮士德》的剧本从原始本经过1790年的残篇以至第一部完成，他的内容是肯定人生为最髙的价值，最高的欲望，但同时也是最大的问题。初期的《浮士德》剧本之结局，窥歌德之意是倾向纯悲剧的。人生是将由他内在的矛盾，即欲望的无尽与能力的有限，自趋于毁灭，浮士德也将由生活的罪过趋于灭亡，生活并不是理想而是诅咒。但歌德自己生活的发展使问题大变，他在意大利获得了生命的新途径，而剧本中的浮士德也将得救。在 1797 年的《浮士德》中的天上序曲里，魔鬼梅菲斯特诅咒人生真如歌德自己原始的意思，但现在则上帝反对梅菲斯特的话，他指出那生活中问题最多最严重的浮士德将终于得救。这个歌德人生思想的大变化最值得注意，是我们了解浮士德与歌德自己的生活最重要的钥匙。 我们知道“原始浮士德”的生活悲剧，他的苦痛，他的罪过，就是他自己心的恒变，使他对一切不能满足，对一切都负心。人生是个不能息肩的重负，是个不能驻足的前奔。这个可诅咒的人生在歌德生活的进展中忽然得找价值的重新估定。人生最可诅咒的永恒流变一跃而为人生最高贵的意义与价值。人生之得以解救，浮士德之得以升天，正赖这永恒的努力与追求。浮士德将死前说出他生活的意义是永远的前进： 在前进中他获得苦痛与幸福， 他这没有一瞬间能满足的。 而拥着他升天的天使们也唱道： 惟有不断的努力者 我扪可以解脱之！ 原本是人生的诅咒，那不停息的追求，现在却变成了人生最高贵的印记。人生的矛盾苦痛罪过在其中，人生之得救也由于此。 我们看浮士德和魔鬼梅菲斯特订契约的时候，他是何等骄傲于他的苦闷与他的不满足。他说他愿毁灭自己，假使人生能使他有一瞬间的满足而愿意暂停留恋。梅菲斯特起初拿浅薄的人世享乐来诱惑他，徒然使他冷笑。 以前他愿意毁灭，因为人生无价值；现在他宁愿毀灭，假使人生能有价值。这是很大的一个差别，前者是消极的悲观，后者是积极的悲壮主义。前者是在心理方面认识，一切美境之必然消逝；后者是在伦理方面肯定，这不停息的追求是人生之意义与价值。将心理的必然变迁改造成意义丰富的人生进化，将每一段的变化经历包含于后一段的演进里，生活愈益丰富深厚，愈益广大高超，像歌德从科学艺术政治文学以及各种人生经历以完成他最后博大的人格。歌德的象征浮士德也是如此，他经过知识追求的幻灭走进恋爱的罪过，又从真美的憧憬走回实际的事业。每一次的经历并不是消磨于无形，乃是人格演进完成必要的阶石： 你想走向无尽么？ 你要在有限里面往各方面走！ 有限里就含着无尽，每一段生活里潜状着生命的整个与永久。每一刹那都须消逝，每一刹那即是无尽，即是永久。我们懂了这个意思，我们任何一种生活都可以过，因为我们可以由自己给与它深沉永久的意义。《浮士德》全书最后的智慧即是： 一切生灭者 皆是一象征。 在这些如梦如幻流变无常的象征背后潜伏着生命与宇宙永久深沉的意义。 现在我们更可以了解人生中的形式问题。形式是生活在流动进展中每一阶段的综合组织，他包含过去的一切，成一音乐的和谐。生活愈丰富，形式也愈重要。形式不但不阻碍生活，限制生活，乃是组织生活，集合生活的力量。老年时歌德因他生活内容过分的丰富，所以格外要求形式，定律，克制，宁静，以免生活的分崩而求谐和的保持。这谐和的人格是中年以后的歌德所兢兢努力惟恐或失的。他的诗句： 人类孩儿最高的幸福 就是他的人格！流动的生活演进而为人格，还有一层意义，就是人生的清明与自觉的进展。人在世界经历中认识了世界，也认识了自己，世界与人生渐趋于最高的和谐；世界给予人生以丰富的内容，人生给予世界以深沉的意义。这不是人生问题可能的最高的解决么？这不是文艺复兴以来，人类失了上帝，失了宇宙，从自己的生活的努力所能寻到的人生意义么？ 浮士德最初欲在书本中求智慧，终于在人生的航行中获得清明。他人生问题的解决我们可以说： 人当完成人格的形式而不失去生命的流动！生命是无尽的，形式也是无尽的，我们当从更丰富的生命去实现更高一层的生活形式。 这样的生活不是人生所能达到的最高的境地么？我们还能说人生无意义无目的么？歌德说： 人生，无论怎样，他是好的！ 歌德的人生启示固然以《浮士德》为中心，但他的其他创作都是这种生活之无限肯定的表现。尤其是他的抒情诗，完全证实了我们前面所说的歌德生活的特点： 他一切诗歌的源泉，就是他那鲜艳活泼，如火如荼的生命本体。而他诗歌的效用与目的却是他那流动追求的生命中所产生的矛盾苦痛之解脱。他的诗，一方面是他生命的表白，自然的流露，灵魂的呼喊，苦闷的象征。他像鸟儿在叫，泉水在流。 他说：“不是我做诗，是诗在我心中歌唱。”所以他诗句的节律里跳动着他自己的脉搏，活跃如波澜。他在生活憧憬中陷入苦闷纠缠，不能自拔时，他要求上帝给他一支歌，唱出他心灵的沉痛，在歌唱时他心里的冲突的情调，矛盾的意欲，都醇化而升入节奏，形式，组合成音乐的谐和。混乱浑沌的太空化为秩序井然的宇宙，迷途苦恼的人生获得清明的自觉。因为诗能将他纷扰的生活与刺激他生活的世界，描绘成一幅境界清朗，意义深沉的图画（《浮士德》就是这样一辐入生图画）。 这图画纠正了他生活的错误，解脱了他心灵的迷茫，他重新得到宁静与清明。但若没有热烈的人生，何取乎这髙明的形式。所以我们还是从动的方面去了解他诗的特色。 歌德以外的诗人的写诗，大概是这样：一个景物，一个境界，一种人事的经历，触动了诗人的心。诗人用文字，音调，节奏，形式，写出这景物在心情里所引起的澜漪。他们很能描绘出历历如画的境界，也能表现极其强烈动人的情感。但他们一面写景，一面叙情，往往情景成了对待。且依人类心理的倾向，喜欢写景如画，这就是将意境景物描摹得线清条楚，轮廓宛然，恍如目睹的对象。 人类之诉说内心，也喜欢缕缕细述，说出心情的动机原委。虽莎士比亚、但丁的抒情诗，尽管他们描绘的能力与情感的白热，有时超过歌德，但他们仍未能完全脱离这种态度。 歌德在人类抒情诗上的特点，就是根本打破心与境的对待，取消歌咏者与被歌咏者中间的隔离。他不去描绘一个景，而景物历落飘摇，浮沉隐显在他的词句中间。他不愿直说他的情意；而他的情意缠绵，婉转流露于音韵节奏的起落里面。他激昂时，文字境界节律音调无不激越兴起；他低徊留恋时，他的歌辞如泣如诉，如怨如慕，令人一往情深，不能自已，忘怀于诗人与读者之分。 王国维先生说诗有隔与不隔的差别，歌德的抒情诗真可谓最为不隔的。他的诗中的情绪与景物完全融合无间，他的情与景又同词句音节完全融合无间，所以他的诗也可以同我们读者的心情完全融合无间，极尽浑然不隔的能事。然而这个心灵与世界浑然合一的情绪是流动的，飘渺的，绚缦的，音乐的；因世界是动，人心也是动，诗是这动与动接触会合时的交响曲。所以歌德诗人的任务首先是努力改造社会传统的，用旧了的文字词句，以求能表现出这新的动的人生与世界。 原来我们人类的名词概念文字，是我们把捉这流动世界万事万象的心之构造物；但流动不居者难以捉摸，我们人类的思想语言天然的倾向于静止的形态与轮廓的描绘，历时愈久，文字愈抽象，并这描绘轮廓的能力也将失去，遑论做心与景合一的直接表现。 歌德是文艺复兴以来近代的流动追求的人生最伟大的代表（所谓浮士德精神)。他的生命，他的世界是激越的动，所以他格外感到传统文字不足以写这纯动的世界。于是他这位世界最伟大的语言创造的天才，在德国文字中创造了不可计数的新字眼，新句法，以写出他这新的动的人生情绪。（歌德他不仅是德国文学上最大诗人，而且是马丁•路德以后创新德国文字最重大的人物。现代继起努力创新与美化德国文字的大诗人是斯特凡•格奥尔格［Stefan George，1868－1933. 德国诗人，受法国象征主义诗歌的影响，主张“为艺术而艺术”。“曾创办《艺术篇页》杂志，发表他的艺术理论和创作。诗集主要有《灵魂之年》、《第七环》、《新国》等”］。他变化无数的名词为动词，又化此动词为形容词，以形容这流动不居的世界。例如“塔堆的巨人”（形容大树），“塔层的远”，“影阴着的湾”，“成熟中的果”等等，不胜枚举，且不能译。 他又熔情入景，化景为情，融合不同的感官铸成新字以写难状之景，难摹之情。因为他是以一整个的心灵体验这整个的世界，(新字如“领袖的步”“云路”“星眼”“梦的幸福”“花梦”等等也是不能有确切的中译，虽然诗意发达极高的中囯文词颇富于这类字眼），所以他的每一首小诗都荡漾在一种浩瀚流动的气氛中，像宋元画中的山水。不过西方的心灵更倾向于活动而巳。我们举他一首《湖上》诗为例。歌德的诗是不能译的，但又不能不勉强译出，力求忠于原诗，供未能读原文者参考。 湖上 ［1775年瑞士湖上作，时方逃出丽莉（Lili）姑娘的情网。（按：姑娘原名 Elise von Schlussmann，嫁TÜV Kheim 氏）］ 并且新鲜的粮食，新鲜的血我吸取自自由的世界：自然何等温柔，何等的好，将我拥在怀抱。波澜摇荡着小船在击桨声中上前，山峰，高插云霄，迎着我们的水道。 眼睛，我的眼睛，你为何沉下了？金黄色的梦，你又来了？ 去罢，你这梦，虽然是黄金，此地也有生命与爱情。 在波上辉映着千万飘浮的星，柔软的雾吸饮着四围塔层的远。晓风翼覆了影阴着的湾，渐中影映着成熟中的果。 开头一句“并且新鲜的粮食，新鲜的血，我吸取自自由的世界。……”就突然地拖着我们走进一个碧草绿烟柔波如语的瑞士湖上。开义一字用“并且”（德文Und即英文And）将我们读者一下子就放在一个整个的自然与人生的全景中间。“自然何等温柔，何等的好，将我拥在怀抱”。写大自然生命的柔静而自由，反观人在社会生活中受种种人事的缚束与苦闷，歌德自己在丽莉小姐家庭中礼仪的拘束与恋爱的包围，但“自然”是人类原来的故乡，我们离开了自然，关闭在城市文明中烦闷的人生，常常怀着“乡愁”想逃回自然慈母的怀抱，恢复心灵的自由。“波澜摇荡着小船，在击桨声中上前……”两句进一步写我们的状况。动荡的湖光中动荡的波澜，摇动着我们的小船，使我们身内身外的一切都成动象，而击桨的声音给与这流动以谐和的节奏。“上前”遥指那“山峰，高插云霄，迎着我们的水道……”自然景物的柔媚，勾引心头温馨旖旎的回忆。眼睛低低沉下，金黄色的情梦又浮在眼帘。但过去的情景，转眼成空，不堪回首，且享受新获着的自由罢！自然的丽景展布在我们的面前：“在波上辉映着千万飘浮的星……”短短的几句写尽了归舟近岸时的烟树风光。 全篇荡漾着波澜的闪耀，烟景的飘渺，心情的旖旎，自然与人生谐和的节奏。但歌德的生活仍是以动为主体，个体生命的动热烈地要求着与自然造物主的动相接触，相融合。这种向上追求的激动及与宇宙创造力相拥抱的情绪表现在《格丽曼》 (Ganymed)一诗中（慧田哲学注：希腊神话中，格丽曼为一绝美的少年王子。天父爱惜之，遣神鹰攫去天空，送至阿林比亚神人之居)。 格丽曼 你在晓光灿烂中， 怎么这样向我闪烁， 亲爱的春天！ 你永恒的温暖中， 神圣的情绪， 以一千倍的热爱 压向我的心， 你这无尽的美！ 我想用我的臂， 拥抱着你！ 啊，我睡在你的胸脯， 我焦渴欲燃， 你的花，你的草， 压在我的心前。 亲爱的晓风， 吹凉我胸中的热， 夜莺从雾谷里， 向我呼唤！ 我来了，我来了， 到那里？到那里？ 向上，向上去， 云彩飘流下来， 飘流下来， 俯向我热烈相思的爱！ 向我，向我， 我在你的怀中上升！ 拥抱着被捆抱着！ 升上你的胸脯！ 爱护一切的天父！ 这首诗充分表现了歌德热情主义唯动主义的泛神思想。但因动感的激越，放弃了谐和的形式而流露为生命表现的自由诗句，为近代自由诗句的先驱。然而这狂热活动的人生，虽然灿烂，虽然壮阔，但激动久了，则和平宁狰的要求油然而生。这个在生活中倥偬不停的“游行者”也曾急迫地渴求着休息与和平。 游行者之夜歌（二首） 一 你这从天上来的 宁息一切颂恼与苦痛的； 给与这双倍的受难者 以双倍的新鲜的， 啊，我已倦于人事之倥偬! 一切的苦乐皆何为？ 甜蜜的和平！ 来，啊，来到我的胸里！ 二 一切山峰上 是寂静， 一切树杪中 感不到 些微的风； 森林中众鸟无音。 等着罢，你不久 也将得着安宁。歌德是个诗人，他的诗是给与他自己心灵的烦扰以和平以宁静的。但他这位近代人生与宇宙动象的代表，虽在极端的静中仍潜示着何等的鸢飞鱼跃！大自然的山川在虼然峙立里周流着不舍昼夜的消息。 海上的寂静 深沉的寂静停在水上。 大海微波不兴。 船夫瞅着眼， 愁视着四面的平镜。 空气里没有微风！ 可怕的死的寂静！ 在无边寥廓里， 不摇一个波影。这是歌德所写意境最静寂的一首诗。但在这天空海阔晴波无际的境界里绝不真是死，不是真寂灭。他是大自然创造生命里“一刹那倾静的假象”。一切宁宙万象里有秩序，有轨道，所以也启示着我们静的假象。 歌德生平最好的诗，都含蕴着这大宇宙潜在的音乐。宇宙的气息，宇宙的神韵，往往包含在他一首小小的诗里。但他也有几首人生的悲歌，如《威廉传》中《弦琴师》与《迷娘》（Mignon）的歌曲，也深深启示着人生的沉痛，永久相思的哀感： 弦琴师（歌曲） 谁居寂寞中？ 嗟彼将孤独。 生人皆欢笑， 留彼独自苦。 嗟乎，请君让我独自苦! 我果能孤独， 我将非无侣。 情人偷来听， 所欢是否孤无侣？ 日夜偷来寻我者， 只是我之忧， 只是我之苦。 一旦我在坟墓中， 彼始让我真无侣！ 迷娘（歌曲） 谁人识相思？ 乃解侬心苦， 寂寞而无欢， 望彼天一方， 爱我知我人。 呜呼在远方， 我头昏欲眩， 五脏焦欲燃， 谁解相思苦， 乃识侬心煎。 歌德的诗歌真如长虹在天，表现了人生沉痛而美丽的永久生命，他们也要求着永久的生存： 你知道，诗人的词句 飘摇在天堂的门前， 轻轻的叩着 请求永久的生存。而歌德自己一生的猛勇精进，周历人生的全景，实现人生最高的形式、也自知他“生活的遗迹不致消磨于无形”。而他永恒前进的灵魂将走进天堂最高的境域，他想像他死后将对天门的守者说： 请你不必多言， 尽管让我进去！ 因为我做了一个人， 这就说曾是一个战士！]]></content>
      <categories>
        <category>林中路</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[林中路（3）：齐美尔 | 大城市与精神生活]]></title>
    <url>%2F2019%2F01%2F27%2F%E6%9E%97%E4%B8%AD%E8%B7%AF%EF%BC%883%EF%BC%89%EF%BC%9A%E9%BD%90%E7%BE%8E%E5%B0%94%20%E5%A4%A7%E5%9F%8E%E5%B8%82%E4%B8%8E%E7%B2%BE%E7%A5%9E%E7%94%9F%E6%B4%BB%2F</url>
    <content type="text"><![CDATA[大城市人的个性特点所赖以建立的心理基础，是表面和内心印象的接连不断地迅速变化而引起的精神生活的紧张。 首先要理解大城市精神生活的理性主义特点，大城市的精神生活跟小城市的不一样，确切地说，后者的精神生活是建立在情感和直觉的关系之上的。直觉的关系扎根于无意识的情感土壤之中，所以很容易在一贯习惯的稳定均衡中生长。相反，当外界环境的潮流和矛盾使大城市人感到有失去依靠的威胁时，他们——当然是许许多多个性不同的人——就会建立防卫机构来对付这种威胁、他们不是用情感来对这些外界环境的潮流和矛盾作出反应，主要的是理智，意识的加强使其获得精神特权的理智。 典型的大城市人的相互关系和各种事务往往是各种各样的，复杂的。这使得现代的聪明才智越来越变成一种计算智慧。首先，这么多人聚居在一起，利害关系千差万别，他们的各种来往和活动相互间有多方面的有机联系，如果在约好的事情上和工作中没有准确的时间观念，那就会全都乱了套， 大城市生活的复杂性和广泛性迫使生活要遵守时间，要精打细算，要准确，这不仅与它的货币经济和理性主义的特点有密切的关系，而且也使生活的内容富有色彩，有利于克服那种要由自己来决定生活方式、拒不接受被认为是普普通通千篇一律的外界生活方式的非理性的、本能的、主观独断的性格特点和冲动。 另一方面，准确地一分钟一分钟地规定生活方式而形成最无个性的同样的因素也在谋求最有个性的东西。当主体必须完全接受这种存在形式的同时，他们要面对大城市进行自卫，这就要求他们表现出社会性的消极行为。大城市人相互之间的这种心理状态一般可以叫做矜持。在小城市里人人都几乎认识他所遇到的每一个人，而且跟每一个人都有积极的关系。在大城市里，如果跟如此众多的人的不断表面接触中都要像小城市里的人那样作出内心反应．那么他除非要会分身术，否则将陷于完全不可没想的心理状态。这种心理状态，或者说我们面对在短暂的接触中瞬息即逝的大城市生活特点所拥有的怀疑权利，迫使我们矜持起来，于是，我们跟多年的老邻居往往也互不相见，互不认识，往往教小城市里的人以为我们冷漠，毫无感情。 为什么偏偏只有大城市才会引起最独特的个人存在的欲望呢(不管它是否总有道理，也不管它是否都能成功)?我认为最根本的原因是：通过那种可以称之为客观精神的东西对主观精神的优势，现代文明的发展形成了自己的特点，即在诸如语言和法律、生产技术和艺术、科学和家庭环境问题上体现出了一种总体精神，这种总体精神日渐发展，结果是主观的精神发展很不完善，距离越拉越大。如果我们纵观一下一百年来由于各种事物和知识、由于教育和舒适的条件而形成的文明，用它来跟同一时期的人的文明进步比较一下(哪怕跟最高的水平比较)，就可以发现，两者之间的发展差异是令人吃惊的。在某些方面，如教养、关心体贴人和献身精神，人的文明与过去相比反而有所倒退。这种差异的主要原因是分工的越来越细。因为分工越来越细，对人的工作要求也越来越单一化。这种情况发展到极点时，往往就使作为整体的人的个性丧失殆尽，至少也是越来越无法跟客观文明的蓬勃发展相媲美。人被贬低到徽不足道的地步，在庞大的雇佣和权力组织面前成了一粒小小的灰尘。 还需要指出的是，大城市是超越于一切个性的文明的舞台。在大城市里，雄伟舒适的公寓建筑、学校的集体生活方式和明确的校服制度，都说明大城市充满着具体的无个性特点的思想。可以说，这种情况不能使个人保持自己的特点。一方面，个人的生活变得极为简单，个人的行动、兴趣、时间的度过以及意识都要由各方面来决定，他们似乎被放到河面上托着，几乎不需要自己游泳。可是另一方面，生活却越来越由无个性特点的内容和现象组成，而这些无个性特点的内容和现象要排斥本来有个性的色彩和特点，这就刚好使得这种必定能产生最大特点的个性丧失了。]]></content>
      <categories>
        <category>林中路</category>
      </categories>
      <tags>
        <tag>精神紧张</tag>
        <tag>主体文化</tag>
        <tag>客体文化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[林中路（2）：杨祖陶 | 康德哲学体系问题]]></title>
    <url>%2F2019%2F01%2F22%2F%E6%9E%97%E4%B8%AD%E8%B7%AF%EF%BC%882%EF%BC%89%EF%BC%9A%E6%9D%A8%E7%A5%96%E9%99%B6%20%E5%BA%B7%E5%BE%B7%E5%93%B2%E5%AD%A6%E4%BD%93%E7%B3%BB%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[康德的哲学体系是什么，由哪些部分组成，其根本性质是什么，这些问题远比最初想到的要复杂得多。问题在于康德的哲学思想，不仅从前批判时期到批判时期是一个复杂的变化发展的过程，而且在批判时期里也仍然处在变化发展之中。正是这种情况决定了，即使批判时期的康德哲学体系问题，也必须予以具体的考察，不容作出简单的回答。 一康德是近代哲学、也是整个哲学史上最伟大的改革家之一。他在18世纪末，经过长达十余年的酝酿准备，发动了一场推翻传统形而上学，即所谓旧形而上学的革命。这种形而上学源自柏拉图和亚里士多德，在近代为笛卡儿、斯宾诺莎和莱布尼兹所复兴和发扬，到18世纪末虽已趋于没落，但仍以莱布尼兹—沃尔夫体系的形式统治着学院讲坛。康德创始的哲学革命并不是像怀疑主义那样要从根本上抛弃或否定形而上学，而是要挽救它，改造它，革新它，使它获得新的生命和形式，走上科学的坦途，成为一种作为科学的形而上学。由于这种形而上学当时还不存在；所以康德也把它称为未来形而上学。 在康德看来，形而上学乃是来自纯粹理性的哲学知识的体系，旧形而上学之所以陷于非科学或假科学的境地，是由于没有预先批判研究人类理性本身而盲目地、武断地、自以为是地从理性的先天概念和原理出发进行分析和推论，以回答世界的终极实在的问题、建立起所谓终极真理的形而上学体系的结果。针对这种情况，康德提出，为了使形而上学摆脱困境，纯粹理性哲学，即形而上学必须从研究程序和内容上区分为性质不同的前后两个部分。前一部分是：批判人类一般认识能力、即“纯粹理性”，或如康德所说，“从理性的一切纯粹的先天知识着眼研究理性能力”[①]，判定各种先天知识的来源和限度，以确定形而上学是可能还是不可能及其所以可能的源泉、条件和范围，从而把形而上学的前提一劳永逸、坚如磐石地确立起来。康德把这一部分称为纯粹理性的“批判”，而由于“批判”的使命仅在于“清除和平整杂草丛生的土地”，“深入勘探形而上学的地基”，为建立这样的大厦做好必要的准备，所以康德就把这一部分看作是形而上学的“导论”、“预备阶段”、“入门”或“初阶”等等。随后的一部分是：系统地阐述从已经判定的纯粹理性的概念和原理中引申或派生出来的全部纯粹理性的哲学知识，这即是在业已准备就绪的地基上建构起科学的形而上学体系。康德把这一部分看作是纯粹理性的“体系”，并把它称为“形而上学”。把哲学这样地区分为纯粹理性的“批判”和“体系”，或形而上学的“导论”和“形而上学”两部分，是康德在其整个哲学革命过程中贯彻始终、坚定不移、最重要、最基本的指导思想。 哲学的这两个部分虽然就其性质、内容、使命、地位、意义等等而言是彼此不同的，但它们又是内在联系、不可分离的。第一部分，即纯粹理性的“批判”，由于已经判明了纯粹理性的概念和原理的全部财富，因而也就含有“使形而上学成为科学的、经过充分研究和证实的整个方案”，以至可以说其中已经孕育着了形而上学的“幼芽”或“胚胎”[②]，而第二部分的形而上学无非就是这一完整方案的实施或这个幼芽之发育成长为一棵枝叶扶疏的大树。第一部分所揭示的纯粹理性的概念和原理既然是科学的形而上学的基础，即形而上学由以出发的基本概念和基本原理，因而这一部分就远比第二部分重要；但由于这些概念和原理在这里尚且是作为单纯的思想形式，其内容尚未展开，因而显得抽象而空洞。第二部分，即纯粹理性的体系或形而上学虽然就重要性而言次于第一部分，但它却包含了由纯粹理性的原理和概念派生的全部概念和原理，因而就内容而言要比第一部分无比丰富；而且它还通过提出例证(具体情况)，把“意思和意义赋予了”作为单纯思想形式的那些概念和原理，使它们成为真正现实的概念和原理，这就证实了第一部分、即纯粹理性的批判的正确性，从而“对一般的形而上学作出了卓越的不可缺少的贡献”[③]。由于这两部分的这种有机的联系和统一，因此康德认为，”形而上学’这个名称也可以指全部的纯粹哲学，包括批判在内”[④]。这就是说，在康德看来，最广义的、即一般形而上学乃是由形而上学的导论(纯粹理性的批判)和形而上学本身(纯粹理性的体系)这样两个部分构成的统一整体。 由上所述，我们可以看到，在康德那里，形而上学的对象即是纯粹理性，即人类理性中能够独立于经验、先天地认识和实践的那一部分。照康德看来，人类理性并不是消极被动地接受印象和冲动的容器，而是一种独立自主的、能动的、立法的主体，它通过理性的思辨(理论)的使用向自然立法，以确立自然的规律和关于一切实有事物的知识；通过理性的实践使用而向自由立法，以确定自由，即道德的规律和原理以及关于一切应有的事物的知识。与此相应，形而上学首先就区分为纯粹理性思辨使用的形而上学，即自然形而上学和纯粹理性实践使用的形而上学，即道德形而上学。这两者首先都自成体系。但在这两种形而上学中，道德形而上学是关于人的整个职责、关于人类理性的主要目的和最后目的科学，而理性的一切知识、使用、主要目的都必须作为手段从属于理性的最后目的，这就决定了自然形而上学应当从属于道德形而上学以构成一个单一的、完整的、纯粹理性的目的论的形而上学体系。这样，在康德那里，与纯粹理性的“批判”相并立的纯粹理性的“体系”或与形而上学的“导论”相并立的“形而上学”，也即是一种较广意义的形而上学，指的就是这样一种包括自然形而上学和道德形而上学在内的“自然—道德(或道德—自然)形而上学体系”。 康德认为，道德哲学虽然是哲学史上早已有的东西，其地位也优越于理性的包括思辨自然哲学在内的一切其他工作，但是过去的道德哲学一般说来都是以经验原理为依据的经验哲学，而不是以理性的先天概念为依据的纯粹哲学，因而与思辨自然哲学不同，就还不曾占有形而上学的名称，真正的道德形而上学是一种尚待(康德本人)建立的东西。这样，在康德看来，严格意义或狭义的形而上学就仅仅是形而上学中“从先天的概念出发考虑一切就其为实有(而非应有)的事物”的那一部分，即思辨理性的形而上学或自然形而上学。接着康德又依据理性的系统统一性原理对自然形而上学进行了三个层次的二分：首先将它区分为“只论究在与一般对象相关的一切概念及原理的系统中的知性和理性本身”的先验哲学或本体论(Ontologia，更恰当的译名是“存在论”)和论究所予对象(无论是给予感官或给予其他种类的直观)的总和的纯粹理性的“自然学”(physiologie)；进而又将后者区分为以感官对象的总和为对象的内在自然学和以经验对象的超经验联系为对象的超越自然学；复次，内在的自然学又区分为以外感对象的总和为对象的理性物理学或有形自然形而上学和以内感对象、即心灵为对象的理性心理学或思维自然形而上学，而超越的自然学则区分为论究自然全体的理性宇宙论和论究自然全体与自然之上的一个存在者，即上帝的关系的理性神学。康德最后总括起来说：“这样，形而上学的全部体系就是由四个主要部分构成的：(1)本体论；(2)理性自然学(包含理性物理学和理性心理学两类。——引者)；(3)理性宇宙论；(4)理性神学。”[⑤] 我们可以把康德在《纯粹理性批判》的《纯粹理性的建筑术》一节中所详细发挥的形而上学的理念，或他所规划的哲学的体系及其构成列表如下： 康德所规划的哲学的体系及其构成，是对于西方哲学史上的传统观念的一种继承和改造。 首先，康德显然是继承了西方自亚里士多德以来把哲学划分为理论(自然)哲学和实践(道德)哲学的传统，但对它作了重大的改造和发展。这明显地表现在： (1)以对认识能力的批判(理性批判)作为全部哲学的导论； (2)要求把道德(实践)哲学从经验哲学提升成为纯粹哲学，即形而上学，并和自然(理论)形而上学融合成为一个单一的形而上学体系； (3)全部哲学或形而上学的对象都是人类理性自身，——纯粹理性批判的对象即是理性自身，无论自然形而上学或道德形而上学所探讨的都是纯粹理性批判所揭示的理性自身的原理在自然领域或自由领域的应用，所谓哲学或形而上学无非就是这样从理性自身的先天原理和概念中引申出来的全部纯粹理性知识的体系。 其次，我们看到，康德关于自然形而上学的划分同近代早期形而上学(从笛卡儿到莱布尼兹)的系统化者沃尔夫关于形而上学区分为本体论、理性宇宙论、理性心理学和理性神学的划分之间有着明显的继承关系，以至连埃·阿迪凯斯、诺·康·斯密等康德专家都认为，康德的划分“绝大部分不是康德的独创，而是从沃尔夫体系那里抄来的”，它们“不是对科学，而是对想了解康德性格的人有意义”[⑥]。但是，我以为我们无论如何不能囿于他们的看法，而是必须还要看到康德和沃尔夫的划分之间的本质区别，或者说，必须着眼于康德在这里的改造和发展。从外表上来看，他们两人的划分之间的细微区别仅在于：康德所规划的自然形而上学的第二部分是由理性心理学(思维自然形而上学)和理性物理学(有形自然形而上学)两者构成的理性的内在自然学，而沃尔夫形而上学体系的第二部分”[⑦]仅仅是理性心理学。为了理解这一微小区别的本质意义，我们有必要把(1)康德关于自然形而上学的划分，(2)沃尔夫关于形而上学的划分，(3)《纯粹理性批判》中以批判沃尔夫形而上学体系各组成部分为目标或背景的先验分析论和先验辩证论的内容划分作一对照和比较： 《纯粹理性批判》中以知性的先天概念和先天原理为研究对象的先验分析论，是对沃尔夫形而上学本体论的批判，也是它的代替物，因而实际上也就成了康德所规划的自然形而上学的第一部分或先验部分——本体论。关于这点康德是这样说的：“一种自负以系统学说的形式提供一般事物的先天综合知识(例如因果性原理)的本体论的夸耀名称，必须让位于一种纯粹知性的单纯分析论这个谦逊的名称。”[⑧]《纯粹理性批判》中以理性的先验理念和先验原理为对象的先验辩证论的三个部分(纯粹理性的谬误推论、二律背反和理想)是依次对沃尔夫形而上学理性心理学、理性宇宙论和理性神学的批判，它们证明了旧形而上学的这些学科作为超出人类理性能力之外的科学是不能成立的。但是，康德并不因此主张从根本上否定或抛弃这些学科，而是认为，只要把它们当作对纯粹理性的“辩证推论”的批判研究，它们就会具有“相当大的消极价值”，即可以起到一种锻炼(训练)理性、使之自觉地限制自己、从而避免或防止超出理性自身能力之外的错误的作用。先验辩证论的上述三个部分就是对纯粹理性的三种不同类型的“辩证推论”的批判研究，因而它们实际上也就是康德所规划的自然形而上学中的理性心理学、理性宇宙论和理性神学，或者至少是它们具体而微的雏型。这样一来，康德关于自然形而上学的划分中，就只剩下理性物理学这一部分是在《纯粹理性批判》中没有其相应物的，而这个部分也正好是沃尔夫形而上学体系所缺少的。[⑨]在康德看来，理性物理学不仅不同于理性宇宙论和理性神学这类超越的自然学，而且也不同于和它同属内在的自然学的理性心理学。虽然理性物理学作为外感对象的学科和理性心理学作为内感对象的学科，都同样有一个有关其对象的经验概念(某种物质[广延而不可入的存在物]或能思维的存在者〔在经验的内部表象“我思”里〕的概念)为基础，但是只有理性物理学才能先天地从单纯的物质概念综合地知道外感对象的总和即有形自然的许多东西，而理性心理学却不能先天地从能思维的存在者的概念综合地知道内感对象即灵魂、我或思维自然的任何东西，因为内感的形式——时间不像外感的形式——空间那样有常住的东西可以作为转瞬即逝的种种感知印象的确定的基础，因而不可能提供确定的对象来认识，而经验的内在表象里的那个常住的“我”又只是伴随我的一切表象的意识的单纯形式，而不是一种直观，因而不能提供认识一个表象的对象所必需的质料。所以，理性物理学作为一种对于应用于自然的纯粹知性的先天知识(知性的先天概念和原理)的批判，不像理性心理学(以及理性宇宙论和理性神学)那样只具有“锻炼”理性的“消极价值”，而是具有仅在物质这唯一一个经验概念的基础上就能提供关于有形自然的许多先天综合知识的“积极价值”，是一种积极的或肯定的自然哲学。因而只有理性物理学或有形自然形而上学才是康德所规划的除本体论外的整个自然形而上学的主体和核心，而康德在这门学科上的成果实际上也构成了往后德国古典哲学的自然哲学发展的开端。正是由于这种情况，它就和道德形而上学一起，成了康德构造其“纯粹理性体系”即较广意义的形而上学体系的两个支点。 在《纯粹理性批判》于1781年发表后，康德除了为使学术界注意、理解这部著作和消除人们对它的误解而写作《未来形而上学导论)(1783年)和一些小的论文外，可以说是集中主要精力来建设他的形而上学体系。1785年发表的《道德形而上学原理》为道德形而上学体系准备了基础，而次年发表的《自然科学的形而上学基础》，则为系统的理性物理学或有形自然形而上学奠定了基本原理。在为1787年出版的《纯粹理性批判》第二版所写的序言中康德说：“……我的年事已高(本月就进入64岁)，我如果要完成我的计划，提供出自然形而上学和道德形而上学，作为思辨理性批判和实践理性批判的正确性的证实，我就必须节约地使用我的时间。”[⑩]在这里，康德力图完成他所规划的形而上学体系的急迫心情，真是溢于言表。在《判断力批判》问世的前夕，康德在1789年5月26日致马库斯·赫茨的信中说，他已经是66岁的人了，但还担负着他计划中须待完成的工作，其中除上书的出版准备以外，就是“按照批判哲学的要求，撰写一个自然形而上学和道德形而上学的体系”[11]。在1790年发表的《判断力批判》的序中康德表示：“我以此结束我的全部的批判工作。我将毫不迟疑地着手学说的(doktrinal)工作”，而“构成学说工作的将是自然形而上学和道德形而上学”[12]。高龄的康德按照道德哲学优越于理性的一切其他工作的原则依然首先致力于道德形而上学的建设。直到1797年，康德才正式出版了《道德形而上学》，这部著作所体现的完整体系是由先后发表的《法权论的形而上学原理》和《德性论的形而上学原理》两大部分组成的。在以道德形而上学体系证实实践理性批判的正确性以后，康德的注意力立即转向解决以自然形而上学体系证实思辨理性批判的正确性的任务。1798年，在发表《实用人类学》的同时，他在致伽尔韦的信(1789年9月21日)中语重心长地写道：“我现在正在解决的任务涉及‘从自然科学的形而上学原理[基础〕向物理学的过渡’。这个任务必须解决，若不然，批判哲学的体系中就会留下一个漏洞。”[13]随后又在致基塞维特尔的信(1789年10月19日)中说到“弥补留下来的漏洞”的重要性，他说：“从自然科学的形而上学原理向物理学的过渡”“是philosophia naturalis(自然哲学)的一个独特部分。在体系中，它是不可缺少的”[14]。但是，风烛残年的康德已不可能完成这部他定名为《从自然科学的形而上学原理向物理学的过渡》的重要著作，他只留下一堆零散的笔记，一些或长或短、互不联系的片断，就与世长辞了。 总起来说，康德整个批判时期所追求的“科学形而上学”或“纯粹理性体系”并没有完全建立起来。这种情况当然不是偶然的。因为在康德看来，批判理性是建立起科学形而上学体系的前提，因而也更为关键、更为重要；这是一项前无古人的开创性工作，当时只有他一人洞见到了哲学发展的这种迫切需要；因此，一旦“批判工作”有了需要，他就应当自觉地放下其他一切工作，集中全部的时间和精力来完成这项历史赋予他的重任，至于建立体系或学说的工作尚可交给他人或等待来者去做。同时，康德对于理性批判的内涵的广度和深度、所涉及的问题的复杂和困难，是他在完成第一部批判著作时始料不及的，以致理性批判工作的结束不像他最初以为的那样是1781年发表的{纯粹理性批判》，而是如他后来所宣称的那样是1790年问世的《判断力批判》。这就是说，为了及时地、全面地完成理性批判的工作，康德不得不中断他在80年代就已经开始了的建立道德—自然形而上学体系的工作几近十年之久。再者，在批判工作和体系工作两相比较时，康德的确认为前者更困难些，后者要轻松些，但就体系工作本身而言却也是困难的，而且其困难的程度也是康德始料不及的。这就是为什么康德在《纯粹理性批判，第一版序言中关于建立形而上学体系只消“以微小而集中的努力，而且在短时期内即可达一种后人……在内容上不能有所增益的完备性”这类性质的话，在后来就再也见不到了的原因所在。当然，归根到底，最根本的原因还是在于，康德用来作为其全部哲学或形而上学的出发点的“纯粹理性”，虽然具有不同方面和不同程度的能动性和主动性，但终究是形式的、静态的、脱离人的实践活动和历史发展的，本身没有内在发展的动力和源泉，没有通过否定自身以实现自身、上升到自身的更高形态和阶段的辩证发展的必然性和规律性，因而说到底还是被动的和不自主的。不借助于某种经验的东西(如物质和运动的经验概念)，要从这样的“纯粹理性”逻辑地引申出关于自然和自由的全部哲学知识的体系是绝不可能的。甚至即使在作为体系的“导论”的“纯粹理性批判”中，就连知性(理性)的范畴(先天概念)也不是从纯粹理性自身必然地引申、发展而来，而是外在地从形式逻辑关于判断的分类，即从经验中假借来的，又遑论从它那里引申、发展出一个完整的哲学体系了。康德没有建成这样的体系，这并不重要。他的伟大和功绩正在于他比同时代人都站得高、看得远、想得深，而以极大的勇气提出了这样的设想，即以理性批判为前导，从纯粹理性中引申出既是认识论和逻辑学，又是本体论的范畴(概念和原理)系统和作为这个范畴系统之应用的自然哲学以及自由(精神)哲学这样三个部分构成的哲学体系，并为此做了在当时条件下所可能做到的、必不可少的、开创性的奠基工作。这个宏伟的设想的实现，本来就不是一代人的事，而是需要几代人连续不断、呕心沥血的创造和劳动。我们知道，在康德之后，经过费希特和谢林，直到黑格尔，这个设想才成为现实，而这时的纯粹理性及其体系同康德最初所设想的相比，可真是面目全非了。然而，不管两者的差别、乃至对立多么的大，但它们之间从少儿到成人的生长轨迹依然是清晰可辨，不容置疑的。 二在康德看来，科学的形而上学或哲学一定是一个科学的系统，没有系统的统一性就没有哲学。这是康德提出和始终坚持的一个重要思想。但是，他认为真正科学的哲学又不能单讲系统性、科学性、知识性或逻辑性。如果这样，哲学所关注的就只是少数学院人士关切的东西，哲学就会沦为一种理性的技巧的学问，康德认为沃尔夫的哲学就属于这种哲学。在他看来，对于哲学更为重要、而且任何时候都成为哲学的“实在基础”的，是哲学的“有用性(Nützlichkeit)”。所谓哲学的有用性是说哲学所关注的是“人人都必然对之关切的东西”，这就是能够指明人的一切知识和理性使用同人类理性的最终目的之间的内在联系，从而能够促使人们向着人类理性的最终目的接近。哲学由于这种有用性才会具有绝对的价值和无上的尊严。这当然并不意味着哲学的系统性或科学性与哲学的有用性或目的性是对立的，因为有用的哲学也必须是科学，是一个以人类最终目的为根据的系统统一的整体，不然它就会成为一种空洞的幻影。在康德看来，他所规划的由纯粹理性的“批判’’和“体系”所构成的形而上学就应该是这样一个系统性和有用性，科学性和目的性相结合的哲学体系。 正因为这样，康德在《纯粹理性批判》里，在讨论“纯粹理性的建筑术”之前，首先讨论了“至善这个理想作为纯粹理性最终目的的规定根据”，对“人人都必然对之关切的东西”从哲学的层面上进行了概括和阐述。他说：“我的理性所关切的一切(思辨的和实践的)结合为以下三个问题： 1．我能够知道什么? 2．我应当做什么? 3．我可以希望什么?”[15] 康德认为，第一个问题纯是思辨的或理论的问题，即关于实有事物的知识问题。人能够通过经验认识由自然必然性决定其实存的一切自然事物，其中包括作为自然一分子的人在内，从而获得对于自然的科学知识，这是一个不容否认的经验事实。除此以外，凡不是实有的事物或不能成为现象或经验对象的事物，如人有没有不受自然必然性决定的自由意志，特别是有没有不死的灵魂(来生)和上帝是否存在，都是人类理性所不知的，不能进入人的知识范围的。为了彻底解决什么是人能够知道或认识的，什么是人不能够知道或认识的，哲学首先就要研究人的一切知识的源泉，一切理性使用的范围，从而确定人类理性的界限。划界问题特别重要，它的解决一方面为自然科学的可能性奠定了理论基础，二方面论证了上帝等等不可知，从而为人超出自然必然性的自由意志，以及来生和上帝的存在留下了地盘。康德坚决反对旧形而上学把认识自由意志、来生和上帝存在看作是人类理性的最高或最后目的，他认为关于它们，特别是来生和上帝存在的问题，我们是不能得到任何知识的，而且即使获得了这样的知识，对于认识自然事物、取得科学的自然知识也毫无意义。再退一万步说，就算有来生和上帝存在，也还有一个“我应当做什么”的问题需要解决。这第二个问题纯是一个实践的问题，即关于通过自由而可能的东西的表象问题。在康德那里，实有的事物是依照自然必然性而发生的，应有的事物则是不依赖自然必然性而通过意志的自由决定才可能发生的。人的意志只有不顾欲望、爱好和外界的支配，完全依照理性的先天道德原理，做到自律、即真正的自由或独立自主时，才有道德性，而人也才真正成为人。意志自由不仅是道德的必要条件或基础，而且就是道德本身。照康德看来，理性的最高目的不是知识、科学、自然必然性，而是使意志实现自由，成为一个自由的、道德的、善的意志，这也就是人类理性的整个使命或天职。但是，人类理性关切的东西并不就此完结了，因为我做了所应当做的，就还有一个“我可以希望什么”的问题。这是一个既是实践的同时又是理论的问题。在康德看来，“一切希望都指向幸福”，而“幸福是我们一切愿望的满足”。虽然对幸福的追求绝对不能像法国唯物主义者主张的那样是道德的基础，但是人类实践理性希望得到幸福也是必然的，合理的。因此，问题只能是把道德作为配享幸福的条件，从而把道德与幸福统一起来，使幸福能够按照道德以精确的比例进行分配。这种“理想”的境界才是最高的和最完全的善，是“至善”，因而也才是人类理性真正的最终目的。但是，这个“至善”理想的实现不仅要假定自由意志，而且更为重要的是要在来生和上帝存在的条件下才可能。这样，实践的问题就作为三种引线而引到理论问题的答案那里去了。就是说，“至善”理想应当实现的问题为思辨理性所想解决而不能解决的上帝存在等问题顺理成章地提供了一种来自实践理性的“信仰”的答案。因此，我们可以看到，当康德经过这样三个问题而把“至善”这个理想规定为人类理性的最终目的时，他已经把哲学看作是对于人在世界中占的位置、人的职责和最终命运的探究了。 康德提出的三个问题表明康德在纯粹哲学领域内探讨的主要问题不是来自书本，而是来自人的现实生活，是现实的人所面临的现实问题的哲学升华。对这三个问题的研究和回答构成了康德哲学的真实内容方面，而“纯粹理性的建筑术”中所规划的体系及其构成则属于表达这种内容的形式方面。在康德看来，这个形式是适合于表达它所要表达的内容的，因为在他的构想中，自然哲学应从属于道德哲学，从而溶合成为一个以人类理性的终极目的为基础的单一哲学体系。 可是，到了90年代，康德关于哲学问题的内容和观点都有了新的发展。他在致司徒林的信(1793年5月4日)中说，长久以来，他在纯粹哲学领域里的研究计划，就是要解决上面的三个问题，接着他令人注目地在这三个问题之后，补充了“第四个，也是最后一个问题：人是什么?”[16]大约与此同时，康德在其逻辑学讲义(后来于1800年由耶什整理出版)中把这四个问题作为一个整体提出来，认为它们一起构成了“世界公民意义上的哲学的领域”，并且指出：“形而上学回答第一个问题，道德学回答第二个问题，宗教学回答第三个问题，人类学回答第四个问题。但从根本上说，可以把这一切都看成是人类学，因为前三个问题都与最后一个问题有联系。”[17]在这里，比起在致司徒林的信中来，不仅更明确地说明了研究和回答这四个问题的各个学科，而且更为重要的是表达了一个前所未有的新思想：把前三个问题同人是什么这个问题联系起来，把它们全都看作人类学的问题，从而研究和回答这三个问题的各学科也都可以看作是人类学或从属于人类学。这样一来，人是什么这个问题就成了康德提出的四个哲学问题中的总问题，而人类学则成了统率和囊括其余学科的总学科。就此而言，我们的确可以而且应当认为，这是康德哲学思想中一种十分值得注意的新变化。因为在提出前三个问题和规划哲学的体系及其构成时，康德原本不是这么看的。在对自然形而上学的理性心理学中是否应当包括经验心理学在内这个问题的讨论中，他明确地表示，经验心理学虽然可以暂时留在理性心理学内，但长远地看，它却应定居在“一种与经验性的自然学配对的详尽的人类学中”[18]。这就是说，康德在这时是把人类学作为一种经验性的学说而排斥在他的纯粹理性的体系、即道德—自然形而上学体系之外的。由此也可以推断出，康德当时是说不上把他提出的三个问题归结到人是什么这个人类学的问题上来的。 当然，90年代康德哲学思想中的这个新变化或新发展并不是突如其来的。正如康德在上述致司徒林的信中提出人是什么这个问题时所注明的那样，这是一个人类学的问题，而20多年来，他每年都要讲授一遍人类学。由此可见，人是什么的问题虽然是他现在才明确提出来的，但这个问题本身从前批判时期起(康德从1772年起开始讲授人类学)就一直蕴藏在他的心里，并潜在地支配着他的思想。但是，由于康德始终认为，人类学作为一种经验性的学说必须放在经验性的自然学所在的同一个地方，所以他在提出哲学研究的三个问题和哲学体系的构架时，就理所当然地把人类学排除在形而上学体系之外。那么，到了90年代，他又为什么会提出人是什么这个人类学问题作为其他三个问题都与之相关联的总问题，而把回答其他问题的学科都看成是人类学呢?这也许是康德（1）关于人类学的长期思考，(2)关于建立纯粹理性的“体系”，即道德—自然形而上学体系的长期计划，(3)回答我能够知道什么等三大问题的研究计划，(4)纯粹理性的“批判”的扩展、深入和完成等四大要素或方面的进程在一定阶段上相互交汇和碰撞所必然产生的结果。在《纯粹理性批判》回答了我能够知道什么这个问题之后，为了彻底解决我应当做什么这个问题，康德继《道德形而上学原理》后又写了《实践理性批判》。在这以后，康德没有立即去回答我可以希望什么的问题，而是不得不着手解决由于前面两大批判著作所造成的理论理性和实践理性、自然必然性和自由、科学和道德的对立问题，因为只有解决了这个问题，一方面，才能为康德所最关心的哲学或形而上学的两大部分，即自然形而上学和道德形而上学之联结为一个统一整体提供可能性，另方面，才能为解决纯粹理性所追求的最终目的在感性世界里实现的可能性问题提供思路。这时，人类学中关于人的心灵具有知、情、意三种能力的经验性考察和先验哲学对人类高级认识能力中知性、判断力、理性三种成分的划分之间的对比，终于使康德在山穷水尽时看到了出路，去从事于判断力的批判，以发现情感能力的先天原理。在这里，特别明显地显示出来，人类学对于康德来说不仅是他始终立足的经验基地，而且还具有一种他在先验哲学中进行探索和求得正确答案的指南的意义。正如康德自己在谈到他如何走上第三批判的道路时所说的那样：“如果我有时不能正确地确定某个对象的研究方法，那么，只要我能够回顾一下认识和与此相关的心灵能力各要素的全貌，就能找到我所期待的答案。”[19]另方面，对于判断力的批判反过来又使他认识到了，人生活的世界是以人的创造文化的活动为基础的，人的一切，包括人自身，都是通过人的自觉的、有目的的、自由创造文化的活动的产物，而人类理性的最终理想或目的的实现，即人的最终命运或前途，也必须联系到人的创造文化的活动来考虑。这样，人的认识(科学和自然必然性)的问题，人的道德和自由的问题，人的希望或最终命运的问题，都不能离开人是什么这个问题而得到单独的、至少是圆满的解决，而勿宁说，它们都只是人是什么这个总问题中的应有之义或其不同的成分或方面的表现，因而这个总问题的解决也必然地蕴含着那些问题的解决或为那些问题的真正彻底解决提供了现实的可能性。 因此，根据康德把人是什么这个问题看作是一切其他问题归宗的根本问题，把人类学看作回答这一根本问题的学科，我们可以说，他在这里实际上(不管他自觉与否)已经超出了他1781年提出三个问题时所设计的纯粹理性体系，即道德—自然形而上学体系，而构想出了一种新的哲学体系，即接近于我们今天称之为哲学人类学的那样一种哲学体系。现在我们就来看看康德以回答四大问题为基本内容，以建构某种类似哲学人类学的体系为宗旨的长期研究计划完成的情况。在康德看来，第一批判，即《纯粹理性批判》，是这个计划的第一部分，在这部分里他“已经穷尽了”我能够知道什么“这个问题的一切可能的答案，而且最后找到了理性必然对之感到满意的那个答案”[20]。第二批判，即《实践理性批判》，回答了我应当做什么这个问题，它是这个计划的第二部分。值得注意的是，由于在康德心目中，回答我可以希望什么这个问题的应是宗教学，因此，构成这个计划的第三部分的就不是像通常有的人所想的那样是第三批判，即《判断力批判》，而是如康德自己指明的那样是后来于1793年出版的《单纯理性范围内的宗教》(Die Religion innerhalb der Grenzen der biopen Vernunft)。康德非常自信地认为，他在这部著作中已经认识到了，“基督教与最纯粹的实践理性的结合是可能的”[21]。在他看来，人们为了实现具有一个善良意志，不仅需要个人道德修养上的努力，而且必须生活在力求意志善良的人们组成的某种“伦理团体”或“神秘团体”里，而历史中产生的宗教就是把人们结合成为这样的团体的必不可少的手段。不过能真正实现其道德使命的不是同神做交易的、有个人打算的侍奉神灵的宗教，而是理性信仰的宗教。这种宗教所唯一要求的是：要按照理性的道德命令来生活，在善良生活方式中通过内心善与恶的斗争，克服人性的劣根性，达到道德上的最高完善，好像有来生在等待着你，有上帝在君临着尘世似的。这种情况，在实践理性看来，是完全必要的和可能的。因为在这里，人们对自己道德力量的信仰也就是对神的信仰，只是人们虽然有力量使自己道德上完善，却无力使道德与幸福统一起来，因而必须承认，也就是假设有来生和全知全能全善的上帝作为这个世界的统治者。我们看到，尽管康德的道德思想和宗教思想渗入了值得注意的社会性和历史性的因素，但人类理性的最终理想—道德与幸福的统一仍然像过去一样被推到了彼岸。也许康德本人也觉察到了，而且也不满意于这种情况，所以他并不是在回答第三个问题后，就立即去回答第四个，也是最后一个“人是什么”的问题，而是把目光首先转向探索人们除去寄希望于自己和宗教信仰外，是否还要以及如何寄希望于社会、政治、法及其制度的问题，陆续发表了《永久和平论》(1795年)，《法学的形而上学原理》(《道德形而上学》的先行发表的部分，1797年)等著作。直到1798年，康德才在长期讲授人类学的基础上，出版了《实用人类学》，这也是他的全部哲学著述活动中最后一部由他自己撰写和发表的著作。康德没有改变他关于人类学是一门经验性科学的一贯立场，先验论(先天的知识原理、先天的道德原理、先天的自然合目的性原理以及先验自我意识等等)在这里至多也只是一个背景，而没有被强调和作为解决问题的手段。康德人类学的出发点是“根据他(人)的类，把他作为具有天赋理性的地球生物来认识”，“研究的是人作为自由行动的生物由自身作出的东西，或能够和应该作出的东西”[22]。所谓自由行动，是指人因为赋有理性而能自由地选择目的和采取相应的手段使之得以实现的这样一种行动。人由于他的这种自由行动而具有了一种把他同地球上其他动物或生物区别开来的自己创造自己、发展自己、完善自己的特性。关于这点，康德是这样说的：“人具有一种自己创造自己的特性，因为他有能力根据他自己所采取的目的来使自己完善化，他因此可以作为天赋有理性能力的动物而自己把自己造成为一个理性的动物。”[23]这就是说，在康德看来，人是这样一种地球生物，他能够通过自己的创造行动使自己从潜在的理性动物变为现实的理性动物。但是，人的这种本质的规定性不能在个体那里，而只能在类里得到完整的体现，这也是由于人因其理性而被规定为与人们处在一个社会中使然的。因此，人自己创造自己的过程或从一个仅具理性潜能的动物变为一个真正现实的理性动物的过程，也就只能在类的世代延续的无穷系列的进步中实现。康德就是从这样一点出发，在这部著作的第二部分“人类学的特性”中，依据经验和历史，通过概括和推论，力图表明：人如何作为一个类，虽然由于自私自利而彼此不和、从而造成了巨大牺牲和浪费，但通过社会中的劳动、社会交往中的纪律(社会的、政治的、法的制度)、包括科学和艺术在内的整个文化的进步，而把自己创造成为一个日益接近有高度道德意识的、幸福的和永久和平相处的世界公民组织的。康德人类学的这一基本思想显然具有某些接近于现代文化哲学人类学的特征，在它里面蕴含着这样一个必然的推论：既然人的一切都是在人创造自身的进程里发生发展起来而同人本身的存在和发展统一而不可分的，那么人的一切就都不可能离开这个完整的人创造自身的过程而单独地得到理解，因此，人类学就应当成为关于人的任何一个方面，如认识、道德、艺术、宗教等等的研究的基础。然而，就康德的时代而论，由于当时经验科学提供的关于人的知识(如关于人的起源的生物进化论的知识、人种学和民族学的知识等)还是如此的稀少和残缺不全，以致还没有必要的前提，在对它们进行哲学的解释的基础上建立起某种形态的完整的哲学人类学。因此，康德本人还不可能真正把他的人类学作为基础来重新考察他所提出的有关人的重大问题(我能够知道什么，我应当做什么和我可以希望什么)，使对这些问题的解答能同其人类学的基础相协调一致，又何况耄龄的康德很快就虚弱到丧失了工作和思考的能力了呢。 总之，康德以人是什么为总问题的，包括其他三个问题在内的某种哲学人类学体系，虽然每个问题都有相应的著作来解决，因而他计划中的各个组成部分都已有了，但就作为一个内部协调的完整的体系而言，依然没有真正完成。这是不足为奇的。相反地，值得大书特书的倒是，在早期理性主义和经验主义的唯科学主义统治的情况下，能率先提出人是什么及人在世界中的地位、职责、命运等问题来作为哲学研究的主要问题，并首次把“人类学”作为哲学的科目来研究并试图建立起某种哲学人类学体系，这应当看作是康德在哲学上高瞻远瞩和另辟蹊径的丰功伟绩，其影响巨大而深远。这首先表现在它为康德自己所开创的德国古典哲学的传人从费希特到黑格尔的精神发展提供了一个崭新的视角，即要从人创造自己的现实活动及其实际的发展出发，在人同社会、历史的相互联系中考察人类理性，从而把康德提出作为“纯粹理性体系”的“未来形而上学”在新的基础上建立起来，同时也预示了费尔巴哈的以人类学代替神学，以完整的人代替黑格尔的“绝对精神”、甚至笛卡儿和康德本人的“人类理性”作为哲学出发点的“未来哲学”。康德哲学的人类学方面对现代西方哲学的影响尤为深刻，它为许多流派如存在哲学，特别是哲学人类学启示了一种全新的哲学出发点。正是海德格尔、雅斯贝尔斯和舍勒等人把康德提出的人是什么的问题，甚至包括这个问题在内的四大问题作为自己哲学思考的引线和中心，他们由此出发建立了不同形态的存在哲学和哲学人类学，从而也把康德提出的人是什么等问题从长期受忽视的状态推到前沿，使人们对康德哲学的认识进到一个全新的境界。 三这样，在康德那里，真正完成了的哲学体系就只有由三大批判著作构成的“批判哲学”体系，这也就是我们通常所说的康德哲学体系。照康德的看法，这个哲学体系实际上是作为科学形而上学之导论的“纯粹理性的‘批判’”体系。 康德多次指出，1781年的《纯粹理性批判》是他12年期间精心思索成果的总结，而从《纯粹理性批判》到1790年的《判断力批判》又经过了9年。因此，康德的批判哲学体系的建成是他20多年来不断探索的结果，是他的哲学思想持续发展的产物。如果我们把1781年前的12年称为“纯粹理性批判”的前史，那么17’81至1790年则应看作“纯粹理性批判”本身发展完成的过程。现在我们就来鸟瞰一下这个过程。 康德在1781年发表的第一部批判著作《纯粹理性批判》里，把“纯粹理性批判”规定为一门以批判考察人类先天认识能力，即基于先天原理的认识能力为对象的科学，这门科学的主要任务就是要确定人类认识能力有哪些先天要素以及这些先天要素的来源、功能、条件、范围和界限。康德认为人类认识能力首先区分为：作为低级认识能力的感性和作为高级认识能力的理性。感性通过先天的直观形式时间和空间接受由于物自体对感官的刺激而产生的感觉，从而为高级认识能力提供对象和质料。感性直观是认识的开始，同时也是人类认识的不可超越的范围，因为失去了感性直观，高级认识能力也就没有了认识的对象和质料。康德把高级认识能力区分为知性、判断力和理性三种。他确定，只有知性的先天原理(由先天范畴体现的规律性)是构成性的，就是说，是使经验(科学知识)和经验对象(科学认识的对象)——作为现象全体的合乎规律的自然界成为可能的原理，因而也就是知性向人类认识能力颁定的先天法则或规律。判断力则运用知性的先天原理去统摄或规定特殊的感性现象，以形成关于对象的经验知识，康德后来称这里的判断力为规定的判断力，以别于作为“判断力批判”之对象的反思的判断力。理性则通过它的先验的理念和原理来指导和推动经验或科学认识的最大可能的系统化、继续和扩大。知性的范畴必须同感性质料结合才能形成知识和知识对象，离开了感性质料它们只不过是空洞的思维形式。理性的迷误正在于不知道它的理念和原理只是一种指导经验如何进行的范导性原理，而不是把仅适用于感性世界的范畴扩大到一切可能经验之外，即扩大到经验所能提供的对象之外的对象去的构成性原理，从而运用范畴去规定那超经验、超感性的本体，其结果产生的不是关于本体的知识，而是形形色色旧形而上学的谬误推论和自相矛盾的假知识或伪科学。既然只有现象可知，本体不可知，这就限制了理性的理论使用，即知识的，范围，而正因为如此，也就为人的摆脱自然必然性的意志自由、道德、对来生和神的信仰，一句话，为理性的实践使用留下了余地。这就从理论上为作为科学出现的未来形而上学(内在的自然形而上学和超验的道德形而上学)的可能性进行了充分的论证。 正因为《纯粹理性批判》已经确定了人类认识能力的先天原理，弄清楚了各种认识能力在认识总体中各自所能做出的那一份贡献，为理性的实践使用准备好了地盘，所以康德认为他对一般认识能力，即纯粹理性的批判已经完成了。这时，无论在《纯粹理性批判》中，还是在其他地方，康德都从没有提到过实践理性批判，即所谓第二批判的问题，而是计划如何建立道德—自然形而上学体系。甚至在1785年发表的作为道德形而上学之“导言”的《道德形而上学原理))中，虽然康德肯定除去纯粹实践理性批判以外，道德形而上学照理说就没有任何别的原理，正如已发表的纯粹思辨理性批判就是[自然〕形而上学的原理一样，并且还认为他在这部著作的第三章(从道德形而上学过渡到纯粹实践理性批判)已经提出了这样一个批判的足够当时需要的主要线索，但是他仍然认为对实践理性进行批判并没有像对理论理性进行批判那样必要，“因为在道德方面，人类理性就是连最普通的知性也容易达到较大的正确性和完满性”[24]。因此，在本书中康德并没有显示出任何撰写一部“实践理性批判”著作的打算，而是鲜明地表示要完成一部《道德形而上学》。尽管这样，康德在这里已经明确地提出了，在道德形而上学的范围内是不能解决道德的最高原理，即道德律如何先天的可能，为什么它是普遍必然的课题的，为此就必须对实践理性进行一番批判，而在完成这种批判的同时还有必要说明实践理性“在一个共同原理上”同理论理性的统一或一致，因为归根到底只有一个理性，只是在运用方面有所不同罢了。[25]这就是说，对实践理性进行批判，乃是“纯粹理性批判”作为一门具有系统统一性的完整科学和一种为形而上学奠基的系统工程的内在需要。不过’，只是在《道德形而上学原理》问世引起许多关于康德的道德哲学及其同《纯粹理性批判》的关系的批评和责难以后，康德意识到同批评者和论敌进行零敲碎打式的争论是不行的，必须对自己的道德原理的必然性和可能性进行深入的、系统的科学论证，就是说，对实践理性进行一番类似第一批判那样的批判考察，才能彻底解决问题。他在1786年4月7日致贝林的信里谈到他正在修订《纯粹理性批判》，但不打算对它作重大的改动，因为所有属于“这个体系”的命题都是合适的，然后谈到他在这项工作之后的进一步打算：“我将继续把自己关于形而上学的研究抛开，为的是争取时间，构建实践哲学的体系。这个体系与前一个体系是姊妹篇，需要加以类似的处理，但尽管如此，却不会遇到前一个体系那样大的困难。”[26]显然，这里所说的“实践哲学的体系(das System der praktischen Weltweisheit)”即是指“实践理性批判的体系”。起初，康德曾计划把实践理性批判合并到《纯粹理性批判》第二版里去，后来他放弃了这个打算。1787年4月《批判》第二版发表，6月他在通信中告诉友人“《实践理性批判》已经大功告成”[27]，即将付印。这部著作在出版商那里一直延误到1788年才正式出版。 《实践理性批判》的任务是：完整地确定实践理性(或理性的实践使用)的先天原理的可能性、范围和界限。康德的出发点是：纯粹理性自身就是实践的，因而具有先天的实践原理，这就是由道德律体现的最后目的。这个原理是使意志的自由、道德成为可能的构成性原理，因而也就是纯粹理性向人心的高级欲求能力(意志)颁定的先天法则或规律，意志应当做的就是以道德律为根据自立规律，敬重和遵从自立的规律，从而实现由道德律所体现或交给的那个最后目的——成为一个自由的、道德的意志。不过，先天实践原理不能用于现象，而只能用于超感性的本体，就是说，它不能用来从理论上认识、解释和推断一切的实有、自然事物、经验对象、包含作为现象，即自然之一分子的人在内的现象界或自然界的存在、性质和规律，而只能用来“从实践上”认识、解释和推断一切应有的事物，首先是自由、道德(善)、目的国、至善等等，从而对它们的存在、性质和规律得到一种实践的体会和信念，它涉及的只是“应当如此”，而不是“事实如此”或“必然如此”。 康德在撰写和结束《实践理性批判》时，很可能认为他对理性批判的任务已经实现了，因为他通过两大批判已把人的心灵的两个主要能力——认识能力和欲求能力的先天原理的来源、内容和界限揭示出来了，而同时通过实践理性和理论理性在同一认识中只有在实践理性占优先地位的条件下才能必然地先天地结合起来的原理又已解决了它们两者的统一问题，自然(理论)—道德(实践)形而上学的奠基工程也就似乎完成了。正如他在《实践理性批判》的序中说的那样：“因此，在这种方式下(指先分析地研究各个部分、然后进到综览全局的方法。——引者)，心灵的两种能力，即认识能力和欲求能力的先天原理就会被发现出来，并且就它们应用的条件、范围和限度而言也都被确定了，这样就给作为科学的系统的(理论的和实践的)哲学打下了牢固的基础。”[28]不过，这只是康德思想的一个方面。另一方面，也是更为重要的一个方面，是他深深地意识到，在作为主体的人里面有两种不同的立法——知性在认识中的立法和理性在自由中的立法——虽然并不矛盾，但这样两种立法的后果却导致了作为主体的人的巨大分裂，使之分属于两个绝对不可跨越、互不影响的领域：一个是可以认识的、受自然必然性支配的、作为感性现象的自然界，一个是不可认识的、可以自由自决的、作为超感性的本体的道德界(自由界)。在康德看来，所谓理论理性和实践理性在实践理性占优先地位下的统一并不足以消除自然领域和自由领域之间的这种巨大的鸿沟，因为在这里还缺少一种从前一领域到后一领域之间的过渡，一座由此及彼的桥梁，一个把两者联结起来的中间环节。这样的过渡、桥梁或中间环节是应该有的，因为自然领域虽然绝对不能对自由领域施加影响，但自由领域却应该对自然领域有影响，就是说道德律所体现的最后目的应该在自然界、即感性现象世界里实现出来，为此自然界的合规律性就必须和道德律所体现的最后目的应该在自然界里实现的可能性互相和谐一致。现在的问题只是应该到哪里去寻找这样的过渡、桥梁或中间环节。当然、按照康德关于“纯粹理性批判”的观点，只能到人类心灵的判断能力中去寻找，而人心除去关于真的判断能力和关于善的判断能力以外，就还剩下关于美的判断能力，即鉴赏力或审美力。于是，康德在1787年6月25日致许茨的信中宣称《实践理性批判》已经大功告成的同时，又申明自己“必须马上转向《鉴赏力批判基础》”[29]，试图探索鉴赏判断是否也从属于先天的理性原理或规律的问题。两个多月以后，康德似乎已经有了肯定的答案，因此在9月11日致雅可布的信中说：“目前，我径直地转入撰写《鉴赏力批判》，我将用它结束我的批判工作”[30]。到了年底，他的新思想在致莱因霍尔德的著名书信(1787年12月28日)中就耀眼地涌现出来了：“我现在正忙于鉴赏力的批判。在这里，将揭示一种新的先天原则，它与过去所揭示的不同。因为心灵具有三种能力：认识能力，愉快与不快的感觉，欲望能力。我在纯粹(理论)理性的批判里发现了第一种能力的先天原则，在实践理性的批判里发现了第三种能力的先天原则。现在，我试图发现第二种能力的先天原则，虽然过去我曾认为，这种原则是不能发现的。”[31]同时，康德还在信中指出，知、情、意三种心灵能力构成的体系性的东西；使他改变了哲学二分为理论哲学和实践哲学的观点，而认识到哲学有三个部分——理论哲学，目的论，实践哲学，每个部分都有它自己的先天原理，三者当中自然是目的论“最缺乏先天规定根据”[32]。在这里似乎表明康德已经萌发了一种想要系统地研究自然界中的合目的性问题或自然目的论的“先天规定根据”的倾向，而这种倾向在与此信一并寄给莱因霍尔德即将在《德意志信使》上发表的同福尔斯特( J.G．Forster)论战的文稿《论目的论原则在哲学中的运用》(1788年1月刊出)中还见不到，虽然文章有针对性地讨论了生物有机体和自然史的科学研究必须以目的论原则作为指导原理的问题，而且还提出了把自然和艺术品同等地看作有机整体的思想。尽管这样，直到1788年春季，康德仍然没有把对美学的先天原理的研究和对有机体自然科学及自然目的论的先天规定根据的研究综合统一起来，因而在致莱因霍尔德的信(1788年3月7日)中他把计划在半年内完成的第三部批判著作照旧称作《鉴赏力批判》，就是说它研究的只是美学问题。[33]只是又经过了一年多的深入思索之后，康德的新思想才完全成熟和定型下来，他终于在高级认识能力中介于知性和理性之间的判断力里找到了一种把看来漠不相干的美学问题和有机体自然科学—自然目的论问题都统摄包容起来的根本原理。这样他就依然回到了他过去一贯坚持的哲学只有两个部分(理论哲学和实践哲学)的观点，认为(自然)目的论在必要时可隶属于理论哲学，而其原理则属于“纯粹理性批判”的范围。因而，康德在致莱因霍尔德的信(1789年5月12日)中就把他正在撰写的第三批判称为“判断力批判”，并注明“《鉴赏力批判》是其中的一部分”[34]。这一部分后来正式定名为“审美判断力批判”，而信中所没有提到的《判断力批判》的另一部分则是“目的论判断力批判”。全书于1790年春出版，并于1792年再版，1799年出第三版。 《判断力批判》的任务是，确定介于知性和理性之间的判断力的先天原理，这个原理是构成性的还是范导性的，它是否是判断力对介于认识能力和欲求能力之间的愉快和不快的情感能力颁布的先天规律，它是否真正能够充当从纯粹认识能力到纯粹欲求能力、即从自然领域到自由领域过渡的桥梁或中间环节。 康德认为，这里所说的判断力已不是《纯粹理性批判》中讨论的那种把特殊从属于给定的普遍的规定的判断力，而是为给定的特殊寻找、发现那可以统摄它的普遍的反思的判断力。反思判断力的先天原理不可能是规定判断力所遵循的知性范畴所体现的规律性(主要是因果必然性)，因为它所面对的特殊的经验事实或规律(例如有机体和艺术作品的各个部分之间的关系)就不是知性的因果性原理所能规定和包摄的。那么，反思判断力需要一条什么样的先天原理才能使多样性的特殊事实和规律得到统一呢?康德从人类技艺和道德中表现出来的“实践的合目的性”类推到，在自然的多样性的经验事实中也应表现出一种合目的性，这就是“自然的合目的性”的概念：“自然通过这个概念就被这样地表象着，好像有一个知性包含着自然的经验规律的多样性的统一的根据。”[35]这个知性当然不是我们人的知性(因为人的知性的先天概念是自然的因果性而不是自然的合目的性)，至于是否真有这样的知性也是无法确定的。所以，“自然的合目的性”是一个特殊的先天概念，它只是在反思判断力里有它的根源，就是说，它是反思判断力为了反思的需要而自己提供给自己的，是人用来从当前特殊的经验对象出发反思作为其基础或根据的超验本体、反思人的超验本体、暗示人的自由、道德、最后目的的前景的反思判断力的先天原理。 这个原理首先在人的审美和艺术活动中起作用。在这里，它是一个使直接同愉快和不快的情感相联系的审美判断和审美对象(美)成为可能的构成性原理，因而也就表明它是反思判断力对愉快和不快的情感能力颁立的先天法则或规律。在进行审美鉴赏时，人们着眼于一个自然对象的无目的的合目的性形式，使自己的想象力和知性能力好像趋于一个目的那样处于自由协调的活动(游戏)之中，从而无须任何概念而产生出人类共同的、无利害关系的愉快感，这就使审美对象、美在人们眼中成为了“道德的象征”，从而促进人心对于道德情绪的感受性。与审美鉴赏不同，艺术创造是带有艺术家的目的和概念的，但真正天才的艺术家并不是按照目的和概念来创造，而是好像无目的地绝对自由地进行创造，以致创造品显得好像是大自然本身的产品，但同时却又体现着天才艺术家的道德目的和理想。总之，在审美和艺术创造活动中，通过反思、象征和类比的方式，而使自然界和道德界、现象和本体、必然和自由达到了一种主观形式上的统一。 其次，反思判断力的先天原理在人们对自然的认识活动中也有其应用，不过在这里它只是作为认识能力的范导性原理而同愉快和不快的情感没有直接的关系。人们除了按照因果律等等对自然界加以认识之外，还总是倾向于对自然有机物作目的论判断。人们把有机物和无机物区别开来，把无机物视为有机物的手段，而把有机物或有机体判定为一个以自身为目的(内在目的)的统一整体，并由此而扩大到把整个自然界看作一个从低级趋向于高级的自然目的系统，其顶点则是以遵守道德律的(自由的)人为最后目的。这就使人有理由猜测到整个自然界都是从必然向自由生成的过程，从而暗示了现象和本体、认识和道德在客观质料上也是统一的。 此外，康德还从上述自然目的论进一步引申出所谓“伦理学神学”，即把整个世界看作向道德和幸福相统一的人这一最后目的前进，以实现上帝的“天意”的历程，从而为实践理性的利益展示诱人的前景。经过这样的批判考察，康德认为反思判断力的先天原理的确在分裂的自然领域和自由领域之间架起了一座由此及彼的桥梁：“判断力以其自然合目的性概念提供了自然概念和自由概念之间的中介概念，它使从纯粹理论的到纯粹实践的、从按照前者的规律性到按照后者的最后目的的过渡成为可能；因为这样一来，那只有在自然里并同自然的规律相一致才能成为现实的最终目的的可能性就被认识到了。”[36] 在这样的基础上，康德对他的全部理性批判工作进行了总结，充实、丰富、发展和完善了“纯粹理性批判”的概念。康德认为，人类心灵的一切能力都可以归结为来自我们所不知的同一根源的知、情、意这样三种能力(认识能力、愉快和不快的情感能力、欲求能力)。人作为天赋理性能力、即具有依照原理进行判断和行动的能动性的生物，他的三种心灵能力也都应该有它们必须遵循的理性先天原理或规律，为了发现它们就必须对理性能力进行批判的研究。作为理性(或知性)能力的高级认识能力的总体是由知性、判断力、理性这样的次序排列的三种能力构成的。《纯粹理性批判》确定，对于认识能力来说，只有知性是立法的，即是说知性包含着认识能力领域(科学认识和自然领域)的构成性原理。《实践理性批判》确定，对于高级欲求能力来说，只有理性是立法的，即是说理性包含着欲求能力领域(道德、自由领域)的构成性原理。现在，《判断力批判》确定，对于愉快和不快的情感能力来说，只有判断力是立法的，即是说它包含着对于审美和艺术活动中愉快和不快的情感的构成性原理，从而使自然领域和自由领域的过渡和联结成为可能。因此，康德把《判断力批判》看作完成和结束他的全部理性批判工作的最后一部著作，是他的纯粹理性批判体系中必不可少的部分，没有这一部分，对于纯粹理性的批判就是残缺不全和半途而废的。正如他在《判断力批判》里说的那样：“纯粹理性的，即依照先天原理进行判断的能力的批判将会是不完备的，如果那作为认识能力自身也要求着批判的判断力的批判不当作它的一个特殊部分来处理”；“尽管哲学只能区分为理论的和实践的两个主要部分，……可是那必须在哲学体系研究工作之前为着体系的可能性而解决一切问题的纯粹理性批判却是由三个部分构成的：纯粹知性的批判，纯粹判断力的批判和纯粹理性的批判，这些能力之所以称为纯粹，因为它们是先天立法着的。”[37]由这样三个部分构成的“纯粹理性批判”的整体也就是康德最终建成了的、完备的哲学体系，即批判哲学体系。关于这个体系的对象、任务、组成和它与诸高级心灵能力及其先天原理和原理应用范围的系统统一性可以表示如下[38]： 在康德看来，这个批判哲学体系既然已经穷尽了人类各个高级心灵能力的先天原理，解决了哲学的可能性的全部问题，它也就为纯粹理性的全部哲学知识的体系打下了深入到达原始地层的基础，从而能够保证这座哲学大厦不致因地基某一部分下陷而坍塌。它首先为康德所说的作为科学的未来形而上学体系，即由自然形而上学(理论哲学)和道德形而上学(实践哲学)两部分构成的纯粹理性体系打好了基础，因为它既提供了自然领域的先天原理和自由领域的先天原理，又为这两大部分联结为一个整体提供了可能性。它也为康德以解决关于人的四大问题为构成部分的某种哲学人类学体系提供了基础，因为不仅第一批判和第二批判由于回答了我能知道什么和我应做什么而为解决整个人的问题提供了地盘和目标，而且第三批判也为回答我可以希望什么和人是什么提供了思路和前景。因此，在这种意义上，康德的批判哲学体系也就只是他所规划的未来形而上学或哲学的一个“导论”。在以第三批判结束“批判工作”以后，康德建立自然—道德形而上学和某种哲学人类学的一系列著述活动也证明了这点。 但是，批判哲学体系虽说是哲学或形而上学的“导论”，然而正如康德在区分纯粹理性的“批判”和“体系”时也把“批判”包含在最广义的形而上学中一样，它作为“导论”无非就是形而上学的最普遍或最一般的原理的体系。这些原理在康德那里尽管是纯粹理性的先天原理，它们只涉及先天的认识的形式、道德的形式和审美的形式，而不是关于对象(对象应是形式和质料的统一)的知识。但是，它们作为这样的先天形式却是对象(认识的、道德的和审美的)成为可能的构成性原理，因而也是支配这些对象的根本特性的最普遍的规律，或者说，是一切可能的这些对象之成为实际存在的条件或规律。因此，在这种意义上，正如康德曾经在《纯粹理性批判》中把纯粹知性论称为存在论(本体论)那样，现在的批判哲学体系也就可以看作是完成了的存在论或本体论，只不过这里的存在不是指独立于人的意识的对象的存在，也不仅是作为认识对象的自然事物的存在，而是与人的意识不可分离的、包括认识、道德(意志)和审美的对象在内的一切对象的存在。正因为这样，批判哲学就不能只归结为单纯的认识论或逻辑学体系(像某些新康德主义者认为的那样)，也不能归结为认识论、逻辑学与存在论相统一的单纯自然的(仅仅关于真的)形而上学体系(康德在《纯粹理性批判》中曾这么看“批判”)，而应当看作是认识论、逻辑学、本体论相一致的关于真(自然)、善(自由)、美(艺术)的统一的形而上学体系。 同时，由于康德的批判哲学体系的任务是揭示人类知、情、意三种高级心灵能力的先天原理，而这种研究又是以阐明人类理性的最后目的在自然界里实现的可能性和条件为宗旨，所有这些都表明批判哲学是围绕着人在世界里的地位、职责和前景这个中心旋转，因而是指向“人是什么”这个人类学的根本问题的，更何况知、情、意三种能力本来也就是康德《实用人类学》的研究对象，这部著作的第一部分“人类学教授法”就是对于这三种能力的经验性的和心理学的描述。从这个角度出发，国内外一些康德哲学专家认为，批判哲学本身就具有人类学的性质，并对它进行了人类学的注释和破译。在国内，据我所知，最先把这个问题鲜明地提出来加以讨论的是康德《实用人类学》的译者邓晓芒教授。他发表了这样一种独创的观点：在康德那里有两种“人类学”，即由三大批判构成的先验人类学和经验性的实用人类学，由于康德的不可知论和批判主义最终是导向一种现象主义和实用主义，因而他就未能真正建立起一个完整的先验人类学体系，而只能以实用人类学作为其先验人类学的真正“归宿”。[39] 由此可见，康德的批判哲学体系具有多重性质的特点：就其本来的意义说，它是纯粹理性的系统批判，因而是未来形而上学的“导论”；作为“导论”，它不能不同时具有它为之奠基的由自然形而上学和道德形而上学两个部分(美学和自然目的论在康德看来不能成为哲学的一个独立的部分，而是在必要情况下临时附加在上述两个部分中的任何一个部分)构成的形而上学体系的性质；而作为以解决关于人的四大问题为构成部分的某种哲学人类学的基础或前提，它又具有哲学人类学体系的性质。 总起来说，康德发起德国哲学革命的批判时期的哲学思想是一个复杂的、矛盾的发展过程。这种复杂性和矛盾性首先表现在：在这个过程里产生和出现了三个不同的康德哲学体系：已经完成了的作为纯粹理性批判的哲学“导论”——批判哲学体系，尚未完成的作为纯粹理性体系的自然—道德形而上学体系和也没有真正完成的以回答关于人的四个问题为内容的哲学人类学体系。其次还表现在已经完成的批判哲学体系本身又具有三重不同的性质：批判主义、形而上学和人类学。康德哲学思想的这种复杂性和多重性，恰好说明了它是它那个无论在经济、政治和文化、哲学等等方面都处于新旧交替的过渡时代的产儿，也表明了它在解决哲学应走什么新的道路，应研究什么新的问题，应采取什么新的形态等等与哲学生死攸关的重大问题中的探索性和创始性。康德哲学思想的伟大历史意义和永恒历史魅力，正在于它把这种放射着探索性和创始性光辉的智慧作为最宝贵的哲学遗产传给了后世。 原载《德国哲学论文集》第16辑，北京大学出版社，1997年版，第74-108页。 ———————————————————————————————————————— [①] 《纯粹理性批判》A841＝B869(根据《哲学丛书，第37a卷，汉堡费利克斯·迈耶出版社1976版，下同)，参见蓝公武译本，商务印书馆1957年版，第570—571页。 [②]《未来形而上学导论》，庞景仁译，商务印书馆1978年版，第160—161、164页。 [③]《自然科学的形而上学基础》，邓晓芒译，三联书店1988年版，第18页。 [④]《纯粹理性批判》，A841=B869，参见蓝译本，第571页。 [⑤]《纯粹理性批判》，A846—847=B874—875，参见蓝译本，第574页。 [⑥] 见E．Adickes出版的《纯粹理性批判》一书，1889年柏林版，第631页。诺·康·斯密在其《康德&lt;纯粹理性批判)释义》中几乎逐字逐句引用了阿迪凯斯的观点，见该书，商务印书馆’1961年版，第594页。 [⑦] 按沃尔夫形而上学体系各部分排列的次序，第二部分应为理性宇宙论，第三部分为理性心理学，这里是按康德《纯粹理性批判，中的次序排的，以便对照。 [⑧]《纯粹理性批判》，A247=B303，参阅蓝译本，第213页。 [⑨] 这当然不是说物理学不属于沃尔夫包罗万象和一切知识部门的哲学体系，而是说在他那里始终没有一门作为形而上学的物理学或自然哲学。沃尔夫把上帝、人的心灵(灵魂)和物体世界看作理论哲学的对象，也就相应地把神学、心理学和物理学看作理论哲学的三个学科，而物理学又被区分为四个分枝：(1)一般物理学或关于一切物体和主要物类所共有的特性的学说；(2)宇宙论或关于世界全体的学说；(3)特殊的自然科学，如气象学、矿物学、水文学、植物学、生理学等；(4)目的论或关于自然物的目的的学说。沃尔夫把宇宙论从其他物理学分枝中抽离出来，使之与心理学、自然神学并列而为形而上学的继本体论之后的三个组成学科。(参见E. Zeller, Die Geschichte derdeutschen Philosophie seit Leibniz，München l873，S．219．)不过，沃尔夫关于物理学的意见往往是动摇或含糊不清的。他虽然认为形而上学应在物理学之先，但有时又认为形而上学的成就要以物理学为前提，因而物理学又应在形而上学之先；而在其宇宙论中却又包含有一般物体学说，即一般物理学的某些内容。(同上书，第222页)康德也许正是针对沃尔夫，强调绝不能把他所说的以外感对象的总和为对象的理性物理学理解为一般物理学，后者与其说是自然哲学，不如说是数学。(见《纯粹理性批判》，A847＝B875注。)康德认为，在一般物理学中，形而上学的建构和数学的建构是互相渗透的，为了科学的进步，有必要把形而上学的建构分离出来，连同其概念的建构原则一起，呈现在一个体系里。这个特殊的形而上学体系就是理性物理学。(参见康德：《自然科学的形而上学基础》，邓晓芒译，三联书店1988年版，第10页。) [⑩]《纯粹理性批判》，Bxlii，参见蓝译本，第25页。 [11]《康德书信百封》，李秋零编译，上海人民出版社1992年版，第137页。 [12]《哲学丛书》第39a卷，汉堡费利克斯·迈耶出版社1974年版，第5页。参见《判断力批判》上卷，宗译本，商务印书馆1964年版，第6—7页。 [13]《康德书信百封》，第243．245页。 [14]《康德书信百封》，第243．245页。 [15]《纯粹理性批判》，A804—805=B832—833，参见蓝译本，第549—550页。 [16]《康德书信百封》，第200页。 [17]《哲学丛书》第43卷，莱比锡1920年版，第27页。参见《逻辑学讲义)，许景行译，商务印书馆1991年版，第15页。 [18]《纯粹理性批判》A849＝B877，参见蓝译本，第575页。 [19]《康德书信百封》，第110页。 [20]《纯粹理性批判》，A805=B833。参见蓝译本，第550页。 [21]《康德书信百封》，第201页。 [22]《实用人类学》，邓晓芒译，重庆出版社1987年版，第1页。 [23] 同上书，第232页。 [24]《道德形而上学原理》，苗力田译，上海人民出版社1986年版，第40页。 [25] 参见上书，第99、40页。 [26]《康德书信百封》，第104页。 [27] 同上书，第106页。 [28]《哲学丛书》，第38卷，汉堡1974年版，第12页，参见《实践理性批判》，关文运译，商务印书馆1960年版，第19页。 [29]《康德书信百封》，第106页。 [30] 同上书，第107页。 [31]《康德书信百封》，第110页。康德在《纯粹理性批判》第一版(A21=B36的注)里明确地认为“使对美的东西的批判的评判从属于理性的原理，从而把这种评判的规则提高为科学’是不可能的，因为“所想到的规则或标准就其来源而论都是单纯经验的，因而不能用作鉴赏判断必须依照的先天规律”。第二版在“来源”之前增加了“最主要的”，在“先天规律’之前增加了“确定的”，语气有所缓和，但意思没变。至于鉴赏判断没有先天原理或规律可依照的根本原因则在于它同愉快和不快的情感有直接的联系，正如康德在同一著作(A801=B820的注)中所指出的：“因为情感不是表象事物的能力，而是处于全部认识能力以外的，所以凡是和愉快和不快相联系的判断的要素……都不属于只同先天纯粹知识有关系的先验哲学的主体。”现在，他的这些观点都变了。 [32]《康德书信百封》，第110页。 [33] 同上书，第113页。 [34] 同上书，第126—127页。 [35]《哲学丛书》第39a卷，汉堡1974年版，第17页，参见《判断力批判》上卷，宗白华译，商务印书馆，1964年版，第18页。 [36]《哲学丛书》，第39a卷，第34页，参见《判断力批判，上卷，宗译本，第35页。 [37]《哲学丛书》，第39a卷，第2、5页，参见《判断力批判》上卷，宗译本，第4、16页。 [38] 参见《判断力批判》上卷，宗译本，第36页。 [39] 邓晓芒：《“批判哲学”的归宿》，载《德国哲学》第2辑，北京大学出版社1986年版，第44—45、33等页。]]></content>
      <categories>
        <category>林中路</category>
      </categories>
      <tags>
        <tag>康德</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[林中路（1）：康德 | 答复这个问题：“什么是启蒙运动？”]]></title>
    <url>%2F2019%2F01%2F22%2F%E6%9E%97%E4%B8%AD%E8%B7%AF%EF%BC%881%EF%BC%89%EF%BC%9A%E5%BA%B7%E5%BE%B7%20%E7%AD%94%E5%A4%8D%E8%BF%99%E4%B8%AA%E9%97%AE%E9%A2%98%EF%BC%9A%E2%80%9C%E4%BB%80%E4%B9%88%E6%98%AF%E5%90%AF%E8%92%99%E8%BF%90%E5%8A%A8%EF%BC%9F%E2%80%9D%2F</url>
    <content type="text"><![CDATA[启蒙运动就是人类脱离自己所加之于自己的不成熟状态。不成熟状态就是不经别人的引导，就对运用自己的理智无能为力。当其原因不在于缺乏理智，而在于不经别人的引导就缺乏勇气与决心去加以运用时，那么这种不成熟状态就是自己所加于自己的了。Sapere aude！ [2] 要有勇气运用你自己的理智，这就是启蒙运动 [3] 的口号。 懒惰和怯懦乃是何以有如此大量的人，当大自然早以把他们从外界的引导之下释放出来以后(naturaliter maiorennes [4] )，却仍然愿意终身处于不成熟状态之中，以及别人何以那么轻而易举地就悍然以他们的保护人自居的原因所在。处于不成熟状态是那么安逸。如果有一部书能替我有理解，有一位牧师能替我有良心，有一位医生能替我规定食谱，等等；那么我自己就不用那么操心了。只要能对我合算，我就无需去思想；自有别人会替我去思考这类伤脑筋的事情。 绝大部分人（其中包括全部女性）都把步入成熟状态认为除了是非常之艰辛而外并且还是非常之危险；这一点老早就被每一个一片好心在从事监护他们的保护人关注到了。保护人首先是使他们的牲口愚蠢，并且小心提防着这些温驯的畜牲不要竟敢冒险从锁着他们的摇车里面迈出一步，然后就向他们指出他们企图单独行走时会威胁他们的那种危险，可是这种危险实际上并不那么大，因为他们跌过几次交之后就终于能学会走路的；然而只有有过一次这类事情，就会使人心惊胆战并且往往吓得完全不敢再去尝试了。 任何一个人要从几乎已经成为自己天性的那种不成熟状态之中奋斗出来，都是很艰难的，他甚至于已经爱好它了，并且确实暂时还不能运用他自己的理智，因为人们从来都不允许他去做这种尝试。条例和公式这类他那天分的合理运用，或者不如说误用的机械产物，就是对终古长存的不成熟状态的一副脚镣。谁要是抛开他，也就不过是在极狭窄的沟渠上做了一次不可靠的跳跃而已，因为他并不习惯于这类自由的运动，因此就只有很少数的人才能通过自己精神的奋斗而摆脱不成熟的状态，并且从而迈出切实的步伐来。 然而公众要启蒙自己，确是很可能的，只有允许他们自由，这还确实几乎是无可避免的。因为哪怕是在为广大人群所设立的保护者们中间，也总会发现一些有独立思想的人；他们自己在抛却了不成熟状态的羁绊之后，就会传播合理地估计自己的价值以及每个人的本分在于思想其自身的那种精神。这里面特别值得注意的是，公众本来是被他们套上了这种羁绊的，但当他们的保护者（其本身是不可能有任何启蒙的）中竞有一些人鼓动他们的时候，此后却强迫保护者们自身也处于其中了；种下偏见是那么有害，因为他们终于报复了本来是他们的教唆这或者是他们教唆者的先行者的那些人。因而公众只能是很缓慢地获得启蒙。通过一场革命或许很可以实现推翻个人专制以及贪婪心和权利欲的压迫，但却绝不能实现思想方式的真正改革，而新的偏见也正如旧的一样，将会成为驾驭缺少思想的广大人群的圈套。 然而，这一启蒙运动除了自由而外并不需要任何别的东西，而且还确乎是一切可以称之为自由的东西之中最无害的东西，那就是在一切事情上都有公开运用自己理性的自由。 [5] 可是我却听到从四面八方发出这样的叫喊：不许争辩！军官说：不许争辩，只许操练！税吏说：不许争辩，只许纳税。神甫说：不许争辩，只许信仰。（举世只有一位君主 [6] 说：可以争辩，随便争多少，随便争什么，但是要听话）到处都有对自由的限制。 然则，哪些限制是有碍启蒙的，哪些不是，反而是足以促进它的呢？━━我回答说：必须永远有公开运用自己理性的自由，并且唯有它才能带来人类的启蒙。私下运用自己的理性往往会被限制得很狭隘，虽则不致因此而特别妨碍启蒙运动的进步。而我所理解的对自己理性的公开运用，则是指任何人作为学者在全部听众面前所能做的那种运用。一个人在其所受任的一定公职岗位或者职务上所能运用的自己的理性，我就称之为私下的运用。 就涉及共同体利益的许多事物而言，则我们必须有一定的机器，共同体的一些成员必须靠它来保持纯粹的消极态度，以便他们由于一种人为的一致性而由政府引向公共的目的，或者至少也是防止破坏这一目的。在这上面确实是不容许有争辩的；而是人们必须服从。但是就该机器的这一部分同时也作为整个共同体的、乃至于作为世界公民社会的成员而论，从而也就是以一个学者的资格通过写作面向严格意义上的公众时，则他是绝对可以争辩的，而不致因此就有损于他作为一个消极的成员所从事的那种事业。因此，一个服务的军官在接受他的上级交下来的某项命令时，竞抗声争辩这项命令的合目的性或者有用性，那就会非常坏事；他必须服从。但是他作为学者而对军事业务上的错误进行评论并把他提交给公众来作判断时，就不能公开地加以禁止了。公民不能拒绝缴纳规定于他的税额；对所加给他的这类赋税惹事生非的擅行责难，甚至可以当作诽谤(这可能引起普遍的反抗)而加以惩处。然而这同一个人作为一个学者公开发表自己的见解，抗议这种课税的不适应与不正当不一样，他的行动并没有违背公民的义务。同样地，一个牧师也有义务按照他们所服务的那个教会的教义向他的教义问答班上的同学们和他的会众们作报告，因为他是根据这一条件才被批准的。但是作为一个学者，他却有充分自由、甚至于有责任，把他经过深思熟虑有关那种教义的缺点的全部善意的意见以及关于更好的组织宗教团体的建议传达给公众。这里面并没有任何可以给他的良心增添负担的东西，因为他把作为一个教会工作者由于自己职务的关系而讲授的东西，当作是某种他自己并没有自由的权利可以按照自己的心意进行讲授的东西：他是受命根据别人的指示并以别人的名义进行讲述的。他将要说：我们的教会教导这些或那些；这里就是他们所引用的论据。于是，他就从他自己不会以完全的信服而赞同、虽则他很可以使自己负责进行宣讲的那些条文中━━因为并非是完全不可能其中也隐藏着真理，而且无论如何至少其中不会发现有任何与内心宗教相违背的东西，━━为他的听众引绎出全部的实用价值来。因为如果他相信其中可以发现任何与内心宗教相违背的东西，那么他就不能根据良心而尽自己的职务了，他就必须辞职。一个就任的宣教师之向他的会众运用自己的理性，纯粹是一种私下的运用；因为那往往只是一种家庭式的聚会，不管是多大的聚会；而在这方面他作为一个牧师是并不自由的，而且也不能是自由的，因为他是在传达别人的委托。反之，作为一个学者通过自己的著作而向真正的公众亦即向全世界讲话时，则牧师在公开运用他的理性上便享有无限的自由可以使用他自己的理性，并以他自己本人的名义发言。因为人民（在精神事务上）的保护者而其本身居然也不成熟，那便可以归结为一种荒谬性，一种永世长存的荒谬性了。 然则一种牧师团体、一种教会会议或者一种可敬的教门法院（就象他们在荷兰人中间所自称的那样），是不是有权宣誓他们自己之间对某种不变的教义负有义务，以便对其每一个成员并且由此也就是对全体人民进行永不中辍的监护，甚至于使之永恒化呢？我要说：这是完全不可能的。这样一项向人类永远封锁了任何进一步启蒙的契约乃是绝对无效的，哪怕它被最高权力、被国会和最庄严的和平条约所确认。一个时代决不能使自己负有义务并从而发誓，要把后来的时代置于一种决没有可能扩大自己的（尤其是十分迫切的）认识、清除错误以及一般地在启蒙中继续进步的状态之中。这会是一种违反人性的犯罪行为，人性本来的天职恰好就在于这种进步；因此后世就完全有权拒绝这种以毫无根据而且是犯罪的方式所采取的规定。 凡是一个民族可以总结为法律的任何东西，其试金石都在于这样一个问题：一个民族是不是可以把这样一种法律加之于其自身？它可能在一个有限的短时期之内就好象是在期待着另一种更好的似的，为的是好实行一种制度，使得每一个公民而尤其是牧师都能有自由以学者的身份公开地，也就是通过著作，对现行组织的缺点发表自己的言论。这种新实行的制度将要一直延续下去，直到对这类事情性质的洞见已经是那么公开地到来并且得到了证实，以致于通过他们的联合（即使是并不一致）的呼声而可以向王位提出建议，以便对这一依据他们更好的洞见的概念而结合成另一种已经改变了的宗教组织加以保护，而又不致于妨碍那些仍愿保留在旧组织之中的人们。但是统一成一个固定不变的、没有人能够（哪怕在一个人的整个一生中）公开加以怀疑的宗教体制，从而也就犹如消灭了人类朝着改善前进的整整一个时代那样，并由此给后代造成损害，使得他们毫无所获，━━这却是绝对不能容许的。一个人确实可以为了他本人并且也只是在一段时间之内，推迟对自己有义务加以认识的事物的启蒙；然而逆行放弃它，那就无论是他本人，而更其是对于后代，都可以说是违反而且践踏人类的神圣权利 [7] 了。 而人民对于他们本身都不能规定的事，一个君主就更加不可以对他的人民规定了；因为他的立法威望全靠他把全体人民的意志结合为他自己的意志。只要他注意使一切真正的或号称的改善都与公民秩序结合在一起，那么此外他就可以把他的臣民发觉对自己灵魂得救所必须做的事情留给他们自己去做；这与他无关，虽则他必须防范任何人以强力防碍别人根据自己的全部才能去做出这种决定并促进这种得救。如果他干预这种事，要以政府的监督来评判他的臣民借以亮明他们自己的见识的那些作品；以及如果他凭自己的最高观点来这样做，而使自己受到“Caesar non est supra grammaticos” [8] 的这种责难；那就会有损于他的威严。如果他把自己的最高权力降低到竟至去支持自己国内的一些暴君对他其余的臣民实行精神专制主义的时候，那就更加每况愈下了。 如果现在有人问：“我们目前是不是生活在一个启蒙了的时代？”那么回答就是：“并不是，但确实是在一个启蒙运动的时代” [9] 。目前的情形是，要说人类总的说来已经处于，或者是仅仅说已经被置于，一种不需别人引导就能够在宗教的事情上确切地而又很好地使用自己的理智的状态了，则那里面还缺乏许多东西。可是现在领域已经对他们开放了，他们可以自由地在这上面工作了，而且对普遍启蒙的、或者说对摆脱自己所加给自己的不成熟状态的障碍也逐渐地减少了；关于这些我们都有着明确的信号。就这方面考虑，这个时代乃是启蒙的时代，或者说乃是腓德烈 [10] 的世纪。 一个不以如下说法为与自己不相称的国君：他认为自己的义务就是要在宗教事务方面决不对人们加以任何规定，而是让他们有充分的自由，但他又甚至谢绝宽容这个高傲的名称；这位国君本人就是启蒙了的 [11] ，并且配得上被天下后世满怀感激之忱尊之为率先使得人类，至少从政权方面而言，脱离了不成熟状态，并使每个人在任何有关良心的事务上都能自由地运用自身所固有的理性。在他的统治下，可敬的牧师们可以以学者的身份自由并且公开地把自己在这里或那里偏离了既定教义的各种判断和见解都提供给全世界来检验，而又无损于自己的职责；至于另外那些不受任何职责约束的人，那就更加是如此了。这种自由精神也要向外扩展，甚至于扩展到必然会和误解了其自身的那种政权这一外部阻碍发生冲突的地步。因为它对这种政权树立了一个范例，即自由并不是一点也不关怀公共的安宁和共同体的团结一致的。只有当人们不再有意地想方设法要把人类保持在野蛮状态的时候，人类才会由于自己的努力而使自己从其中慢慢地走出来。 我把启蒙运动的重点，亦即人类摆脱他们所加之于其自身的不成熟状态，主要是放在宗教事务方面，因为我们的统治者在艺术和科学方面并没有向他们的臣民尽监护之责的兴趣；何况这一不成熟状态既是一切之中最有害的而又是最可耻的一种。但是，一个庇护艺术与科学的国家首领，他的思想方式就要更进一步了，他洞察到：即使是在他的立法方面，容许他的臣民公开运用他们自身的理性，公开向世上提出他们对于更好地编篆法律、甚至于是直言无讳地批评现行法律的各种见解，那也不会有危险的。在这方面，我们有着一个光辉的典范，我们所尊敬的这位君主 [12] 就是没有别的君主能够超越的。 但是只有那位其本身是启蒙了的、不怕幽灵的而同时手中又掌握着训练精良的大量军队可以保障公共安宁的君主，才能够说出一个自由国家所不敢说的这种话：可以争辩，随便争多少，随便争什么；但是必须听话。这就标志着人间事务的一种可惊异的、不能意料的进程；正犹如当我们对它从整体上加以观察时，其中就几乎一切都是悖论那样。程度更大的公民自由仿佛是有利于人民精神的自由似的，然而它却设下了不可逾越的限度；反之，程度较小的公民自由却为每个人发挥自己的才能开辟了余地，因为当大自然在这种坚硬的外壳之下打开了为她所极为精心照料着的幼芽时，也就是要求思想自由的倾向与任务时，它也就要逐步地反作用于人民的心灵面貌（从而他们慢慢地就能掌握自由）；并且终于还会反作用于政权原则，使之发见按照人的尊严━━人并不仅仅是机器而已 [13] ━━去看待人，也是有利于政权本身的 [14] 。 1784年9月30日，于普鲁士哥尼斯堡。 [1] 本文写于1784年(康德60岁) ，最初刊载于《柏林月刊》1784年，第4卷，第481-494页。译文据普鲁士皇家科学院编《康德全集》(柏林，格·雷麦版，1912年) ，第8卷，第33-42页译出。━━译注 [2] [要敢于认识！] 语出诗人贺拉士(Horace，即Q. Horatius Flaccus，公元前65-8)《诗论》，I，2，40；德国启蒙运动的重要组织之一“真理之友社” 于1736年采用这句话作为该社的口号。━━译注 [3] 按启蒙运动(Aufkl?rung) 亦称“启蒙时代”或“理性时代”；这篇为当时的启蒙运动辩护的文章，发表在当时德国启蒙运动的主要刊物《柏林月刊》上。━━译注 [4] [由于自然方式而成熟] 。━━译注 [5] 此处“公开运用自己理性的自由”即指言论自由；康德在这个问题上曾和当时的普鲁士官方检查制度发生冲突。可参看本书《论一个常见的说法：这在理论上可能是正确的，但在实践上是行不通的》。━━译注 [6] 指普鲁士腓德烈大王(Frederick II，der Grosse，1740-1786) 。━━译注 [7] 按“权利”一词原文为Recht；此词相当于法文的droit，英文的right，中文的“权利”、“权”、“法律”、“法”或“正义”。一般或译作“法”，下同。━━译注 [8] [凯撒并不高于文法学家]按，此处这句话可能是针对传说中普鲁士的腓德烈大王回答伏尔泰(Voltaire，1718-1778) 的一句话：“凯撒高于文法学家”。又，传说神圣罗马帝国皇帝西吉斯蒙(Sigismund，1411-1437) 在1414年的康斯坦司会议上说过：“我是罗马皇帝并且高于文法学家”。━━译注 [9] 康德《纯粹理性批判》第1版序言：“我们的时代是一个批判的时代，一切事物都必须接受批判”。━━译注 [10] 指普鲁士腓德烈大王。━━译注 [11] “启蒙了的” 即“开明的”。━━译注 [12] 指普鲁士腓德烈大王。━━译注 [13] “人并不仅仅是机器而已”这一命题为针对拉梅特利(Julien Offray de la Mettrie，1709-1751年) 《人是机器》(1748年) 的反题。━━译注 [14] 今天我在9月13日的《布兴每周通讯》(布兴，Anton Friedrich Büsching，1724-1793年，地理学家，格廷根大学教授，当时主编《地图、地理、统计与历史新书每周通讯》━━译注) 上读到本月30日《柏林月刊》的预告，其中介绍了门德尔松先生(Moses Mendelsohn，1729-1786年，德国启蒙运动哲学家。《论“什么叫作启蒙运动”这一问题》一文刊载于《柏林月刊》1784年第4卷第9期，康德本文刊载于该刊同年同卷第12期。康德撰写本文时尚未读到门德尔松的文章，所以只在本文末尾附加了这条注释━━译注) 对于本问题的答复。我手头尚未收到该刊，否则就会扣发本文了；现在本文就只在于检验一下偶然性究竟在多大程度上能带来两个人的思想一致。 选自《历史理性批判文集》 康德著 何兆武译]]></content>
      <categories>
        <category>林中路</category>
      </categories>
      <tags>
        <tag>启蒙运动</tag>
        <tag>公开运用理智</tag>
        <tag>脱离不成熟状态</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人大应统部落（4）：2019年中国人民大学应用统计专业课真题与解析]]></title>
    <url>%2F2018%2F12%2F24%2F%E4%BA%BA%E5%A4%A7%E5%BA%94%E7%BB%9F%E9%83%A8%E8%90%BD%EF%BC%884%EF%BC%89%EF%BC%9A2019%E5%B9%B4%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E5%A4%A7%E5%AD%A6%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E4%B8%93%E4%B8%9A%E8%AF%BE%E7%9C%9F%E9%A2%98%E4%B8%8E%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[扫一扫，关注【应统联盟】公众号，或添加微信 zhanghua63170140 2019年中国人民大学应用统计初试专业课真题，之后会更新详细解答。 如有疑问或建议，可添加微信：zhanghua63170140 第一题当检验统计量没有落入拒绝域时，可不可以说“接受原假设”，请说明理由。 第二题在时间序列中，请说明严平稳和宽平稳的定义是什么？二者之间有什么联系？并说明如何判断数据的平稳性。 第三题某研究小组想要研究某城市中A、B两种疾病的发病率，其中A疾病的发病率为0.2，B疾病的发病率为0.1，为了使抽样的绝对误差不超过1%，则需要随机抽取的样本是多少？并分析简单随机抽样的可行性和效率，如果你来设计试验，你会怎样设计？ 第四题在回归分析中，说明如何判别是否存在异方差？ 第五题在多元统计分析中，$\sigma ^2$和$\sum$的用处非常多，请用至少三种不同的多元分析方法来说明$\sigma ^2$和$\sum$的应用（文字+公式） 第六题设X服从P维正态分布，即$X - N_p(\mu,\sum)$，其中$\sum$是对角矩阵，对角元素分别为$\sigma_{11} ,\sigma_{22} ,······\sigma_{pp} $，从p维正态总体中抽取一个样本量为n的样本$x_1,x_2,x_3,·····,x_n$，试估计$\mu$和$\sigma$的极大似然估计。 第七题设A、B、C、D为4个随机事件，其中$P(BC)≠0$ （1）证明，$P(A|BC) = P(A|C)$与$P(AB|C) = P(A|C)·P(B|C)$是等价的（2）如果$P(ABC|D) = P(A|D)·P(BC|D)$,证明$P(AB|D)=P(A|D)·P(B|D)$]]></content>
      <categories>
        <category>人大应统部落</category>
      </categories>
      <tags>
        <tag>人大应统</tag>
        <tag>考研真题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[增长黑客（3）：如何用增长黑客思维从0到1做一个公众号？]]></title>
    <url>%2F2018%2F11%2F25%2F%E5%A2%9E%E9%95%BF%E9%BB%91%E5%AE%A2%EF%BC%883%EF%BC%89%EF%BC%9A%E5%A6%82%E4%BD%95%E7%94%A8%E5%A2%9E%E9%95%BF%E9%BB%91%E5%AE%A2%E6%80%9D%E7%BB%B4%E4%BB%8E0%E5%88%B01%E5%81%9A%E4%B8%80%E4%B8%AA%E5%85%AC%E4%BC%97%E5%8F%B7%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[转载自Growthbox 一、前言没有预算，只有两人，如何用增长黑客思维从0到1做一个公众号？这篇文章中，Alan&amp;Yolo会抛砖引玉，用万字长文完整复盘我们的众号《增长黑盒》2018年一季度的增长过程。 简单的介绍一下：《增长黑盒》是Alan&amp;Yolo折腾了大半年的一个公众号，我们俩2018年年初才下定决心全职做这个媒体，专注研究“增长黑客”。鉴于《增长黑盒》是一个专注研究“增长”的媒体，除了写别人的案例，我们自己做到增长才有说服力。所以，我们在一季度给自己定下了不花任何预算，跑通增长黑客转化漏斗的“增长真人秀”任务。 一季度已经结束，是时候交作业了，先来关注一下一季度的粉丝数和营收（虽然我们之前一直视其为虚荣指标）： 粉丝数从4000增加到17500，增长300%以上。 不接广告，两人仅靠几篇文章，一季度被动收入在30万元左右。 二、发现问题根据艾媒报告2017年第一季度的数据，微信公众号数量的增量已经逐渐放缓，野蛮生长的年代似乎已经过了。这意味公众号越来越难做，你没有优质的内容，基本就可以放弃了。 毕竟作为媒体，内容才是核心产品，而微信公众号只不过是一个内容分发渠道。在我们决定做公众号之前，就很清楚地意识到了这个渠道的诸多问题。 我们来根据增长黑客惯用的AARRR转化漏斗来诊断一下每个环节的核心难点： 冷启动获客难： 公众号是订阅式的，没有粉丝就没有阅读。即使有好的内容，如何冷启动一个公众号也是个难点。而信息流就不用考虑粉丝数，只要内容够好就可以根据算法推荐出去，这也是很多信息流APP挤压到微信公众号的一个原因。 激活读者困难 激活已订阅的读者去阅读文章也很难。据行业人士透露，微信公众号文章的平均阅读率也就5%，打开率约为2%。不夸张的说，这个文章打开率甚至不如垃圾邮件的打开率。而且订阅号每天只能群发一波，这也减少了很多可能性。 留存读者困难： 国外的订阅阅读很多还是基于邮件或者RSS的，而中国似乎已经跨越这个阶段。但这中间的核心问题就在于，有邮件的情况下，你可以主动向读者出击，而在微信生态里你是无法主动和读者对话的。也就是说订阅号没有CRM系统，也做不到用户和线索的培育（lead nurturing），自然留存也就难做。尽管服务号有一定的推送培育用户的功能，但从直观上说，不少服务号的推送还是推式营销（outbound marketing）的一种，类似陌拜电话、垃圾短信，打开率也不会太高。 变现方式有限： 微信官方提供的变现思路就是广告，不过新号小号也是很难参与的，而公众号植入硬广软广对于读者来说也是比较糟糕的体验。 推荐环节有限： 微信自身首先是非常克制的，诱导分享是不合规的。目前大家惯用的微信裂变虽然涨粉比较有效，但是大部分的微信裂变事实上是绑架用户的一种行为，对于用户来说体验不会太好，如何做出真正的口碑营销是个难点。 三、Pre-Launch先说一下我们为什么选择做增长黑客媒体这个创业方向，没有好的选题，自媒体自然做不起来，解决再多问题也没有用。 3.1 选择垂直细分领域李开复在《创业就是要细分垄断》一书中，给出选择创业机会的4个基本条件： 风口 细分领域 社会大变 人口基数大的国家 为什么选择做增长黑客媒体，以下是具体的判断： 1) 专注增长是企业趋势：增长黑客这个概念首先由Sean Ellis在2010提出，在2015年由范冰创作同名书《增长黑客》从而正式引入中国。 在2017年的3月底，可口可乐突然宣布将取消CMO（首席营销官）一职，取而代之的是CGO（首席增长官）【1】，这也预示着公司架构变革的趋势。并且，从增长黑客的职位设置上来看，增长黑客=产品+运营+技术+营销。也就是说传统的产品经理、营销人、技术人都是被降维打击的对象，或者说以上这些职业都需要全面地补充自己的技能以应对新的竞争环境。 同时，虽然增长黑客在国外的互联网公司可以说是标配了，但国内只有少数互联网公司掌握了其中的奥秘，那就是增长其实是一门实验科学。以今日头条系为例，其增长团队有200多人，统一为旗下产品矩阵制定增长策略。而结果也很显著，今日头条、内涵段子、火山小视频、抖音APP等产品都完成了指数级的增长。 2） “增长”是被低估的垂直细分市场：《哈佛商业评论》曾写过一篇文章名叫“Every Company Needs a Growth Manager”（所有公司都需要个增长经理）【2】。事实上也是这样，增长黑客（Growth hacker）在国外已经是个Buzzword（时髦词）了，但国内却鲜有媒体报道和研究。]]></content>
      <tags>
        <tag>拌网策略</tag>
        <tag>用户分层</tag>
        <tag>用户生命周期</tag>
        <tag>交叉销售</tag>
        <tag>A/B测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[增长黑客（2）：10个月时间，CMO如何挽救这家破产的电商巨头？]]></title>
    <url>%2F2018%2F11%2F24%2F%E5%A2%9E%E9%95%BF%E9%BB%91%E5%AE%A2%EF%BC%882%EF%BC%89%EF%BC%9A10%E4%B8%AA%E6%9C%88%E6%97%B6%E9%97%B4%EF%BC%8CCMO%E5%A6%82%E4%BD%95%E6%8C%BD%E6%95%91%E8%BF%99%E5%AE%B6%E7%A0%B4%E4%BA%A7%E7%9A%84%E7%94%B5%E5%95%86%E5%B7%A8%E5%A4%B4%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[转载自Growthbox 大家好，这里是yolo@增长黑盒。很久没跟大家见面了，所以这次我来讲一个比较长的故事：美国潮牌电商巨头Karmaloop由于扩张过度，不幸破产。电商老将扛起CMO大旗，借助数据驱动实现了绝地逆转。现在，我们就一起来学习下失败的教训和增长的经验。 一、巨头陨落1999年，25岁的格雷格.赛尔克（Greg Selkoe）还没有意识到，自己即将改变美国的潮流产业。 这个患有多动症、爱好街舞和涂鸦的小伙子生活并不如意，在大学里主修了非常冷门的人类学专业，还跟父母住在波士顿的房子里【1】。这一年，互联网泡沫空前繁荣，他突然意识到自己该做点什么了。于是，他拉上自己的小伙伴，在家中的地下室成立了一个电商平台：karmaloop。格雷格的初衷很简单：提供一个便捷的通道，让所有年轻人都可以买到非常酷的衣服 – 即使身边没有潮牌店，也可以从网上订购。那个时候，“从网站上买东西”还是一件比较罕见的事。格雷格的商业模式也很简单，从当地采购街头潮流服饰（streetwear），把资料挂到网站上，然后邮寄给下单的客户【2】。 （Karmaloop创始人格雷格.赛尔克） 没想到，这生意一干就是4年 – 直到29岁，他还跟妻子在地下室发货。不过，长期对“边缘文化”的推崇，让Karmaloop在嘻哈、DJ、滑板等圈子里声名鹊起。 Karmaloop不断壮大，在2007年进入了鼎盛时期，每年的营收都是成倍增长，连嘻哈巨星Kanye West都为其站台点赞。2013年，Karmaloop终于登上顶峰 – 营业额突破1.27亿美金，远远甩开其它潮牌电商，稳居行业第一，成为了美国最大的潮牌经销平台。2014年，Karmaloop更是在“互联网零售500强”中排名134位。但是，看似一片繁荣的背后，却是1亿美元的银行债务【3】。（Karmaloop官网） Karmaloop的街头王国已经显现出崩塌的预兆。从2008年起，从来没有接受过风投的格雷格获得了Comvest等私募机构一大笔融资，有了花不完的钱。被荣誉冲昏头脑的格雷格开始大肆扩张，先后建立了PLNDR（快闪电商）、Brick Harbor（滑板电商）、MissKL（女性高端潮牌）、Boylston Trading Co（男性高端潮牌）等独立的Niche站。更让人无法理解的是，他不顾高管们的反对，筹划了一档名为KarmaloopTV的电视节目，进一步强化自己在潮流界的地位【3】。 另一方面，公司推崇张扬的嘻哈文化，所以允许员工在办公室喝酒、放音乐、养狗。但糟糕的是，这最后演变成大家经常开派对，吸大麻，在办公桌上啪啪啪 – 作为CEO，格雷格却毫不知情。 慢慢的，公司赚钱的速度已经远远赶不上烧钱的速度了。结果，Karmaloop所有扩张计划均以失败而告终，仅KarmaloopTV就烧掉了1400万美金，节目竟然没上线过。太平盛世下混乱的员工管理，也让公司经不起逆境的考验。 投资人逐渐对Karmaloop丧失了信心，公司没有了新的资金。为了偿还贷款，Karmaloop不得不采取大降价策略以清理库存，很多高端品牌经常能看到40%以上的折扣！不料，这次壮士断臂不仅没有挽回收入，还激起了众多品牌商的不满，纷纷撤出Karmaloop。 2014年末，Karmaloop背水一战，决定采用成本更低的Drop-shipping模式（即直发模式，自己不囤货，顾客下单后由品牌商直接发货）。理论上，这种模式有很大希望挽回败局，扭亏为盈。万万没想到，由于客服和物流跟不上，顾客们经常收不到货，退不了款。这下可好了，老客户们大失所望，骂声一片，Karmaloop把客户和商家两头都得罪了一遍。 2015年，局面彻底失控，Karmaloop的营业额缩水到原来的一半。一切挽救行动均宣告失败，曾经放出消息要收购Karmaloop的Kanye West也没了声音。格雷格的雄心壮志不得不画上句号 – Karmaloop正式宣告破产。Karmaloop也由此获得了一个十分尴尬的“殊荣”：美国历史上第一大破产电商【3】，仅是欠供应商的货款就高达1900万美元！格雷格本人也因欠款500万美元，至今官司缠身。 最终，投资过Karmaloop的私募机构Comvest花费1300万美元，买下了这个烂摊子 – 昔日的巨头被迫以一个月的营业额把自己卖了！ 二、新官上任Comvest接手后，第一件事就是踢掉格雷格这个CEO，并迅速邀请了时尚行业资深人士赛斯.哈勃（Seth Haber）掌管公司大权。但是，作为一家电商公司，营销的重要性自然不用多说 – Karmaloop需要一名给力的CMO，帮助他们穿过泥泞。 很快，Comvest物色到了绝佳人选：今天故事的主角杜鲁.萨诺科齐（Drew Sanocki）。他在电商零售行业也是一位传奇人物：2003年，杜鲁成立了自己的平台Design Public，以纯Dropship的形式出售高档家具，并在第一年就做到了百万美金的营收。2011年，当Karmaloop还在茁壮成长的时候，杜鲁就将自己的公司高价卖给了一家私募公司。从此之后，财务自由的他决定退居二线，成为了一家私募的合伙人 – 主要负责评估零售电商项目，以及投后顾问。 他对零售业营销有着深刻的见解。在Design Public成立之初，他就敏锐地意识到搜索引擎将成为互联网流量的入口，着手打造了一套自动化的SEO体系，成功抢占先机。面对竞争对手的追击，他竟然雇了一名喜剧作家为自己的产品撰写说明书 – 每篇出价高达数百美金！这些高质量的内容打的同行们措手不及【4】。 在Comvest的盛情邀请下，杜鲁决定接受这个挑战- 临危受命，出任Karmaloop的CMO。 Karmaloop一直拥有着顶尖的营销能力。10年前，在“增长黑客0成本指数增长”的策略。那时纸媒还占据着市场的话语权，而Karmaloop却创作了大量街头艺术家、嘻哈音乐人的专访，并通过博客、邮件推送等方式进行分发，几乎没有花钱就获得了大量用户，营收直线上升。 但是，杜鲁开始着手公司业务时，Karmaloop每个月正在流失近百万美金 。 （Karmaloop的流量和营收曲线急剧下滑【5】） 他发现，公司遭遇困境的主要原因就是错误地理解了“增长黑客”的体系：只关注用户获取，而不重视留存和变现 – 大批的用户点水而过，没有给公司创造任何价值。 2008年拿了一大笔投资之后，VC急切渴望看到成绩。因此，格雷格决定抄捷径：花钱买流量。正如前文提到的，不论是做电视节目还是开设新的分支品牌，都是为了占据流量入口。除此之外，Karmaloop还投入大量人力物力去做Google Adwords、PR、线下店、纸媒等等。这个时候，没人意识到虚荣的流量其实未曾带来收入。 不过，由于用力过猛，钱都烧光了，公司开始负债。银行还在天天催着还钱，Karmaloop才被迫想办法提升留存，促进变现。 但是，心急的格雷格干脆一刀切：使用打折促销作为激活用户的手段，直接推动留存和变现。这其实是完全错误的，因为用打折的方式吸引的用户必然是消费能力不高、生命周期价值很低。这让本来就不理想的留存率进一步恶化，获得的营收根本抵不上广告投入，还把供应商得罪了。 最后, 为了节约开支，提高变现环节的利润，Karmaloop才决定采用Dropship直发模式。不过，这也是反其道而行之 – 大批高价值的老客户流失，原本固定的收入来源也惨遭冲击。 这让Karmloop陷入了恶性循环 – 不断烧钱，低质量的新客户在增加，高质量的老客户在减少，越来越亏。 杜鲁意识到，要想扭转局势，就必须获取更多高价值的客户，并且留住他们，最终让他们带来足够的收入。 三、用户获取面对一团乱麻，杜鲁遭遇的一个问题就是：高价值的客户从哪里来？ 3.1 转化低价值用户很显然，最实际的方案就是把之前的低价值用户转化为高价值用户（或者说筛选出潜在的高价值用户）。为此，他尝试了自己践行多年的“拌网策略”【10】-这分为三个步骤： 1.对高价值客户行为进行数据建模2.找到实际数据与理想数据的偏差（即“拌网”）3.集中营销工作的时间和精力，纠正这些偏差 杜鲁自己举过一个例子： 他每个周五都要去一家名叫Slow Burn的健身房锻炼身体 – 对于健身房来说，他就是理想客户，“每周五都来健身”就是所谓的标准模型。 但是，过了几个月，他不想去了。可能的原因有三个： 自己找到了另一家更好的健身房 觉得价格太高了（可能性比较低） 自己变懒了（可能性比较高） “停止去健身”就是所谓的偏差（拌网）。 Slow Burn注意到了这个情况，开始一系列营销活动：提醒他克制懒惰，并送了优惠券给他！他收到了优惠券，并为自己的懒惰感到惭愧，于是又重新回到了健身房 – 偏差就这样被纠正了。 这个理论的核心就是， _公司不应该在所有用户上投入相同的营销成本，而是找到那些阻碍普通用户成为高价值用户的“拌网”，然后投入最大精力去解决这个障碍。_ 从入职第一天起，他就花费了整整一个月去研究Karmaloop近10年的交易数据，并利用RFM模型进行了分析。按照他的理论，首先要找出这些“高价值”的用户行为，然后不断鼓励这些行为，就可以把低价值的用户转化为高价值的用户。 他创建了两个简单的用户分层【6】： 鲸鱼：多次复购，消费额高，很少退货。代表高价值、高LTV的用户层。 鲦鱼：仅购买一次，只买便宜的商品，而且退货率高。代表低质量、低LTV的用户层，如果算上各种成本，这个分层其实在给公司亏钱。 杜鲁惊讶的发现，“鲸鱼”们只占了1.3%的访问量，却贡献了43%的收入！这些历史数据让Karmaloop的问题暴露无遗：鲦鱼太多而鲸鱼太少，所以才一直亏钱。接下来，就是寻找“鲸鱼”们的共性行为了 – 他非常想知道这些高价值用户究竟花多久来完成第二次购买。杜鲁用两个简单的步骤分析近期订单数据： 选中所有满足“鲸鱼”特征的用户； 计算第一次和第二次购买之间平均间隔天数。 分析结果如下图，绿色的条即是杜鲁想要观察的指标。 （X轴代表间隔时长，Y轴代表该有多少用户花费了这个间隔去复购） 他发现， _80%的情况下，如果一个用户要下单两次，他们都会在第一个订单之后的30天内完成第二单。_ 没错，这就是杜鲁找到的“拌网“ – 大多数“鲦鱼”并不会在30天的周期内产生复购行为。前面的数据分析表明，如果一个用户在第一天下了单，但后续的30天内没有下第二单，那他就越来越不可能成为“鲸鱼”了。但是，如果他们在30内下了单（符合理想用户行为），那就有很大机会成为“鲸鱼”。 _Karmaloop需要做的就是使用合理的营销方式与这些”鲦鱼“用户层进行沟通，引导他们在30天下第二个订单，逐步让他们转变为“鲸鱼”。_ 根据上一步的分析，他制定了两种营销策略： 1.在30天内 -&gt; 用户有机会完成二次复购 -&gt;给用户推销原价（高利润）的产品2.在30天之后-&gt;用户越来越难完成二次复购-&gt;用大额优惠（低利润）刺激用户 看到这里，大家不难理解为何Karmaloop为何会亏损了：既然30天之内用户有很大概率会复购，那为何还要给他们优惠呢？不重视精细化运营的代价是惨重的。 作为CMO， _杜鲁的第一项行动就是不再给30天周期内的用户发送优惠券。_ 另一方面，超过30天后，用户复购的几率依旧随着时间的延长而衰减。因此，杜鲁给优惠券设定了一个梯度 – _间隔时间越长，优惠额度越大_ 用户首次购买后若处于30天周期内，则给他推送原价商品； 在30天之后还未复购，则给用户推送10%折扣券； 如果超过了45天，给用户推送20%折扣券； 如果超过了60天，一律推送30%折扣券； 如果用户在任意阶段产生了复购行为，则不会触发后续的优惠； 时间周期和优惠额度会根据数据反馈进行调整。 计划制定好了，如何把营销信息推送给客户呢？在海外市场，Email的地位相当于国内的公众号，而Karmaloop凭借十几年的积累已经有了数百万Email订阅用户 – 杜鲁决定利用这些先天的优势，将Email作为营销的主战场。在团队的帮助下，他建立起了基于CRM和Klaviyo软件的自动化邮件系统。（图中为调整后的优惠梯度邮件） 当邮件取得良好的效果后，营销团队会迅速把活动更新到网站主页、Facebook定向广告、甚至是邮寄的贺卡中。 3.2 社交媒体现在，一部分有潜力的“鲦鱼”已经能够被转化为“鲸鱼”了。但是，比起之前的损失来说，目前的营收增长还远远不够。 正如增长黑盒在之前文章中提到的，社交媒体是时尚行业的必争之地。所以，接下来杜鲁开始寻找新的社交媒体渠道，进一步获取高价值客户。 1）首先是Instagram 根据杜鲁从Shopify内部得到的消息，在所有营收超过百万美金的Shopify店铺中，90%都在依赖Instagram做营销【7】。因此，杜鲁也想在Karmaloop身上尝试一下。他从团队中挑选了一位十分有干劲的小伙子，专门负责Instagram等平台的运营，不断寻求KOL合作机会。 （比如这位打call的妹子就有430万Ins粉丝！） 2）其次是Youtube 在公司倒闭之前，格雷格的KarmaloopTV项目拍摄了大量原创视频，都是放在youtube频道中。但是，现在既然没钱了，这个频道肯定也就开不下去了。于是，杜鲁放弃原创视频的想法，转而借助KOL的力量。在Youtube上，有许多被称为“hauler”的网红：他们经常拍摄潮牌的开箱评测，讲解服饰穿搭 – 自然，这些网红的粉丝一定是Karmaloop的精准客户了。杜鲁立刻安排团队拿下了大批hauler，把Karmaloop售卖的招牌产品交给他们去评测，并且跟他们联合举办抽奖活动 – 关注Instagram，留言送潮牌！ （这位花臂小哥也是潮流界名人） 四、激活留存高价值用户的增长仅仅是开端。接下来，还有更重要的一步：激活并留住他们。 这时，杜鲁应用了生命周期营销策略，即在正确的时间，把正确的信息传递给正确的人。 经过分析，他构建起了一个客户的生命周期流程 任何客户都会经历新手期，活跃期到流失期三个阶段 借助自动化的邮件系统，杜鲁让营销邮件贯穿每个客户的生命周期。从获取客户开始，5-6个邮件campaign就开始运行了。无论用户进行到哪个时间节点，都能通过邮件接收到最恰当的营销信息： 4.1 新手期这个阶段的营销目标是引导用户产生第一次购买。 当用户注册/订阅邮件后，立刻启动“欢迎系列邮件” – 即利用5-7封邮件逐步建立品牌信任度、传递Karmaloop的价值、宣传部分招牌产品。 通过欢迎阶段后，许多用户已经准备好要剁手了。但是，从建立信任到最终付款之间，还有一个“隐形杀手”：放弃购物车（cart abandonment）。2016年的数据显示，全球电商的平均弃车率高达77%！【8】因此，杜鲁特意设定了一系列邮件，唤回那些弃车的用户。 4.2 活跃期这个阶段的营销目标是让用户保持活跃，持续消费。 怎样才能让客户不断来关注Karmaloop呢？杜鲁的团队创建了一个VIP计划：如果某个用户的消费行为接近“鲸鱼”（举例来说，如果他下了单，而且其平均订单价值（AOV）或者平均订单数量超过某个固定值），就把他定义为VIP用户 – 随后会触发相应的营销活动（比如特殊折扣），而且用户会收到一封感谢邮件。 4.3 流失期这个阶段营销目的是唤回流失的老用户。 事实上，大部分Karmaloop的老用户依然停留在这个阶段。公司在倒闭时的垂死挣扎，很大程度上影响了他们的体验，导致信任度大减。 既然这些老用户都在邮件订阅列表上，继续用邮件唤回他们不是很简单吗？不过，杜鲁马上意识到自己低估这件事的难度 – 最初的唤回成功率相当低。当你失去一个人的信任之后，就很难争取第二次机会了。 碰壁之后，杜鲁马上启用了增长黑客的秘诀- A/B测试。他将流失的老用户群划分成许多个10000人的小组，逐步测试不同的方案。 他们首先尝试了10-30%范围内的折扣，随后又试了下次购买返现金，紧接着是送礼品卡，最后还试了“CEO亲笔信”，甚至是打电话…. 进过不懈的努力，在经历过20多次失败的测试后，杜鲁终于找到了一套最佳组合，老客户们渐渐认可了新生的Karmaloop。 为了能够在整个生命周期内更好的理解高价值用户的行为，杜鲁的团队还利用SurveyMonkey进行了NPS问卷，即询问用户“你有多大意愿把Karmaloop推荐给朋友（1分到10分）？”另外，问卷还包括几个核心问题： 1.你还在其它什么地方购物？2.哪些商品我们应该卖却没有卖？3.你平时都阅读哪些博客？ 正是通过不断的学习和理解，杜鲁对客户的需求有了更加精准的把握，大大改进了Karmaloop的用户体验和客户服务。 在之前的运营中，Karmaloop根本没有进行任何客户生命周期的营销，这无疑是巨大的突破。 五、提高营收在杜鲁的推动下，Karmaloop终于能够把高价值客户留存下来了。不过，要形成完整的增长闭环，还差最后的盈利阶段：把留下来的客户转化为收入。 由于之前的Dropship模式宣告失败，所以Karmaloop还要回归到传统的备货模式。因此，想通过压缩成本来提高利润的方法是行不通了。所以，杜鲁设法提高每用户平均收入（ARPU）： 5.1 提高商品单价事实上，Karmaloop之前的打折策略是严重违背其商业价值的：潮牌本身并没有太多物质上的价值，更多的是精神/文化层面的追求和认可 – 用时髦点的话来说，就是“共识”。比如Supreme从来不打折，但官网的新品总是在几十秒内售罄。 他相信，经过一番调整，Karmaloop现有的用户并不是冲着廉价而来的，而是认可潮牌的价值。那么，顾客到底愿意花多少钱买Karmaloop的产品呢？ 杜鲁继续发扬增长黑客的精神，开始了价格实验，针对不同商品供需关系，分组后逐步提价，然后观察用户的购买行为。 他首先把注意力放到了Karmaloop最热销的打底T恤上。经过一段时间提价，他惊讶的发现：价格提高30%后并不会影响销量！也就是说，这个品类的收入足足增加了30%！ 5.2 交叉销售数据显示，亚马逊35%的收入都来自于交叉销售（cross-sell）【9】。在Karmaloop，杜鲁开始实践交叉销售 – 简单来说，就是向购买过本公司A产品的客户推销本公司B产品。从心理学的角度上来说，刚刚完成一次购买的用户正处于“购物期”，心理防线非常薄弱，难以抵抗二次诱惑。借助邮件营销，Karmaloop得以大大提升客户的生命周期价值。 六、绝地逆转不到3个月时间，杜鲁就取得了卓越的成效，把公司营收提高了30%，客户生命周期营销活动的ROI高达500%！ 最终，仅用了10个月时间，杜鲁就将Karmaloop扭亏为盈。 1年半后，Karmaloop以数千万美金的价格出售给了美国球鞋零售商Shiekh Shoes【11】 – 本该从历史上抹去的巨头再次焕发了活力，实现了真正的绝地逆转。 当然，功劳不仅是杜鲁一人的。这个过程中，Karmaloop的CEO也起到了重要作用，包括与品牌方谈判，整顿公司管理，调整供应链等等。另外，杜鲁手下还有一票得力干将，包括一名邮件营销的工程师，一个设计创意团队 – 以及整个产品部门的支援。 总结这个案例为我们清晰地展示了一位CMO如何利用数据来做决策，科学推动公司的增长和提高。总结一下，杜鲁究竟做了哪些事让Karmaloop起死回生： 当然，我们能够从Karmaloop的身上吸取不少教训： 精细化运营决定成败。我们可以发现，Karmaloop之前从来不做客户分层，不分析客户生命周期，也没有自动化营销的概念。因为它依靠着互联网早期的红利起家，不懂得流量的珍贵 – 这与国内目前的形势相似，红利褪去后，野蛮生长需要转变成数据驱动。 增长是一个完整的体系。获客、留存、变现是不可割裂的。Karmaloop只关注获客，追求短期的爆发，而忽视了长期的价值积累，必然导致雪崩式的效应。 靠打折增长用户是不可取的。不得不说，利用人们贪便宜的心理是一个捷径。但是这必然导致用户本身的质量很低，难以导入后续的留存和变现环节。更重要的是，一旦你开始打折，你就再也无法回头，陷入恶性循环。 把一件事做到最好。Karmaloop失败的很大原因就是精力过于分散，急于建设各种周边品牌。说到底，都是为了增加品牌的影响力，但过于“曲线救国”。不如实际一点，比如把潮牌的SEO都做到第一位，或者把Instagram粉丝做到100万。 Dropship坑太多，没有经验不要轻易尝试。作为一个过来人，这是我个人的建议。 同时，我们也能从这位CMO身上学到很多东西： 增长是整个团队配合的结果。国外有调查显示，增长黑客们的主要阻力来自老板。所以，增长永远都是自上而下的，全员all in的。试想一下，杜鲁就算水平再高，如果CEO不支持，员工很懒惰，还能取得这样的成绩吗？ 勇于试错。公司的管理层往往都有存量的顾虑。但对于危在旦夕的Karmaloop来说，杜鲁刚好能够抛弃这种负担，从营销活动到产品价格，大胆进行各种实验 – 这也是增长黑客的精神所在。 重视VIP用户的运营。二八效应在电商行业体现的尤为明显 – 20%的人贡献了80%的收入。然而，我们大多数时候却把这20%的“金主”当做普通人来对待，这显然是不科学的。我们必须甄别出这些高价值用户，用不同的策略加以培养，从他们身上挖掘更多潜在消费价值。 重视数据和技术。如果说数据是灵魂，那么技术就是肉体。在运营Karmaloop的过程中，他都是亲自对数据进行整理和计算，而不是让手下人送个报表过来。另一方面，他非常注重自动化营销 – Karmaloop所有邮件营销都是根据客户行为自动触发的。如果没有这套系统，杜鲁的想法恐怕很难实现。 参考资料： 【1】http://nextshark.com/karmaloop-ceo-greg-selkoe-interview/【2】https://harvardmagazine.com/2012/12/karmaloop【3】http://www.complex.com/style/the-rise-and-fall-of-karmaloop【4】https://growtheverywhere.com/growth-everywhere-interview/drew-sanocki-nerd-marketing/【5】https://www.appcues.com/blog/growing-a-company-with-rapid-growth-strategies【6】https://speakerdeck.com/wooconf/drew-sanocki-the-top-growth-secrets-of-9-figure-ecommerce-retailers-all-of-which-you-can-steal【7】http://rejoiner.com/resources/data-driven-marketing-helped-karmaloop-com-claw-way-back-bankruptcy/【8】https://www.barilliance.com/cart-abandonment-rate-statistics/【9】https://www.forbes.com/sites/chuckcohn/2015/05/15/a-beginners-guide-to-upselling-and-cross-selling【10】https://conversionxl.com/blog/tripwire-marketing/【11】https://www.bostonglobe.com/business/2016/03/23/karmaloop-acquired-california-retailer]]></content>
      <tags>
        <tag>拌网策略</tag>
        <tag>用户分层</tag>
        <tag>用户生命周期</tag>
        <tag>交叉销售</tag>
        <tag>A/B测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[增长黑客（1）：Google关键词挖掘细分市场实战案例]]></title>
    <url>%2F2018%2F11%2F23%2F%E5%A2%9E%E9%95%BF%E9%BB%91%E5%AE%A2%EF%BC%881%EF%BC%89%EF%BC%9AGoogle%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8C%96%E6%8E%98%E7%BB%86%E5%88%86%E5%B8%82%E5%9C%BA%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[转载自亦无所知公众号 理论上来说，如果我们仅仅是想赚点小钱，而不是想像马云爸爸一样通过一款产品去改变世界，那么任意一个关键词，都有机会通过Google挖掘出一个可以赚钱的细分市场。而事实上，也确实可以做到这一点，下文我会一步步向大家阐述如何利用Google这个强大的工具来挖掘痛点，实现推广，在细分市场占领一席之地，进而赚点小钱（未必真的小，如果运气足够好，实现别人口中的小目标也说不定）。 受益于Google这么多年，不得不感慨Google是我最好的老师，总能在我需要的时候为我指明方向，并一步步用他的博学和细致引领我到达想去的地方。 如果说不懂编程的运营或产品有什么核心竞争力，这种信息的挖掘与整合能力便是核心竞争力之一，而对我而言，Google就是获取这种能力最重要的工具。 在细分市场里，由信息部队称造成的信息壁垒可以让我们有较长远的生存和盈利空间。反之，在一个人人都知道是巨大机会的市场中，大量的资本和明星团队进入，前期靠信息不对称造成的壁垒消失，赢得战争，得靠一场又一场硬仗，如彼时的百团大战，此时的共享单车。 在前篇文章里面分享了如何打造一台每月赚10000美元的赚钱机器，这次想在结合一个具体的案例分享一些细节，也是我通过Google挖掘细分市场思路的整理，希望能给一小部分读者带来一些价值。 今天分享的这个案例中提到的市场极不起眼，但每个月能贡献10万美元左右的利润。如果我们做出来的产品只拿到1%的市场和利润，每个月也能有1000美元的收入，对于一个自由职业者，还算不错的收入。 第一步：通过Google挖掘痛点痛点挖掘有很多种方式：从自身实际需求出发，从自己所在行业或者所关心话题出发，从互联网巨头周边出发等。 在资源有限的情况下，我常用的挖掘痛点方式是从互联网巨头产品周边挖掘出一些痛点，对于任何一个大的互联网茶农，都有很多产品需求点。 巨头考虑后不做，比如微信公众号的数据服务（新榜），微信小程序的导航（爱范儿） 巨头没考虑到，一些很小众的需求，巨头没关注到，即使关注到也不太会为了1%的人的需求去影响99%人的体验，比如允许同时在手机上开两个微信APP。 而这些被有意或者无意忽视的需求却是真实存在的，满足这些用户需求，就能获得这部分用户，体量并不一定小。 而几乎每一个互联网巨头周边都有若干产品和创业公司活的很好，甚至有一些上市公司。这次分享的是如何从Instagram上找到真实痛点和产品机会，同样的思路可以用在Facebook、WhatsApp、Bitcoin、Snapchat、微信、微博上。 1.挖掘痛点我们先Google搜索『Instagram』，在Google的下拉框和推荐搜索词提示了下面几组词 除了『instagram download』其他词看起来都很普通。 Google这个词看看有什么机会，搜索发现有以下几组关键词 除了最后两个词『instagram download video』、『 instagram download pictures』，其余的搜索官方都已经满足了需求。 2.确认痛点难道在Instagram上下载图片和视频有什么需求未被满足吗？ 果然，在网页Instagram上确实无法右键保存别人上传的图片，右键另存为的功能被限制了。这个限制其实很常见，很多网站包括知乎，出于版权保护或者其他目的，限制了直接在站内复制文本或者保存图片。 这就像， 使用微信时，收到朋友发来的语音，想转发给其他人，却发现不被支持。 Instagram的这一设置，也许就难倒了90%以上的普通用户。但事实上确实会有这样下载图片和视频的需求，这个痛点真实、有效，围绕这个痛点有没有好的解决方案？能不能赚到钱？还不清晰，继续往下Google。 3. 挖掘现有的产品继续Google『instagram download pictures』，在搜索结果的第一页出现了6个解决此痛点的小工具。 4. 商业模式研究点开排名第一的网站，功能和模式都比较简单，直接通过输入Instagram图片的链接就可以将图片直接下载下来，盈利方式是Google Adsense广告系统。 用SimilarWeb（网站流量数据统计插件）查一下，每月120万访问量，每日4万访问量。 按照千次访问2美元的收入来估算，此网站估计每天收入80美元，每个月收入2400美元，每年收入28800美元，约人民币20万元。 不错，相当于一个白领一年的收入了！ 挖掘到此，痛点真实且需求量不小，有成熟的产品上线了，不错的机会，那么继续往下考虑产品的问题。 第二步：模仿的基础上创新 『我不盲目创新，微软、谷歌做的都是别人做过的东西。最聪明的方法肯定是学习最佳案例，然后再超越』。 吴晓波新出的《腾讯传》一书中，在评价到腾讯游戏与联众游戏、《泡泡堂》和《QQ堂》的早期产品相似问题时，马化腾说了上面的话。 『紧盯市场新热点，快速跟进优化，利用自己的流量优势实现整体替代』。 腾讯在新产品开发上的逻辑可以为我们很多人借鉴，特别是小创业团队、 自由职业者、兼职创业者。 有些人在产品设计阶段会掉进一个坑，认为既然是做产品，一定要有创新的地方。 这并非不对，但既然我们的定位就是希望赚点小钱，而不是做出一款让用户尖叫的产品，那么我 _建议收起完美主义，拿出实用主义。_ 在进行这种工具类型的产品设计时，我会重点考虑这三点 综合比较现有竞争对手，梳理它们已经呈现出来全部的功能点，根据自身能力和用户的需求做出功能点的取舍，设计产品。只需要体验好一点点就够了，比如界面上更清爽些，或者功能点上更贴心一点； 提前思考流量获取方法和流量变现方法，并在产品上体现出来。前置商业模式和流量的思考，这一点很重要，一个有运营思维的产品经理和没有运营思维的的产品经理在这里会有很大的差距； 一定不要重复造轮子。能在github上找到开源的代码的尽量用现成的。 在这个案例中，我将关键词排名前两页的工具站进行了梳理，通过下面的表格， 发现Instagram pictures download这个极其细分的市场每个月能贡献10万美元左右的利润！ 结合这个具体的案例，我思考后觉得产品的设计应初步覆盖以下点 免费的基础功能点。 基于单个账号以及单个链接的图片、音频、视频下载功能，在github上搜索了下，发现已经有几百个现成的代码了，直接拿来用。 收费的增值功能点。从我的过往经验来看，在同样的流量下，一个设计的比较好的付费服务的收入是广告收入的10倍以上。基于『Instagram pictures download』付费服务有付费批量下载、付费基于关键词下载、付费基于动态提醒，付费下载年度top并生成拼图等，最终选择哪一个模式是一个需要不断去试错的过程； 增长黑客点。考虑哪些功能点可能引爆产品？ 激励功能比如分享网站到社交平台可以免费使用网站的付费功能，自动生成年度最热拼图并分享到社交网站等。这些潜在的增长黑客点在设计产品的时候一起考虑进去。 通过以上产品的系统思考后，业务逻辑、产品功能点已经基本理顺，接下来可以进入开发阶段 如果懂技术的话，尽快开发完成上线验证最重要。 如果是非程序员（我猜有很大的比例不是） 这个过程就是一个从真实的场景出发，带着明确的目的去学习编程的过程：如何爬取数据，如何设计数据库，如果设计前端，如何接入支付系统， 如何做分享功能等，这个过程一定是一个非常有意思、非常有动力的过程。 最近李笑来老师的新生大学开了一个全栈训练营，已经报名的人不妨考虑通过这篇文章提到的痛点挖掘方式，找到一个真实的需求和细分市场，做一个产品出来，也许一不小心就把2万块的学费赚到了！ 第三步：自增长痛点真实且可赚钱，产品精益开发完成，接下来怎么办呢？ 冷启动的方法有非常多，针对此类型的工具站提供一个思路。 网络上会有大量评测类型的网站以及how-to网站，类似于国内创投领域的36kr，小众软件。这些网站关注用户在使用大网站过程中的痛点，并对解决此痛点的工具性产品进行评测，导入流量。如果能将咱们的网站在这些评测类型的大网站上被评测到，就会引入大量的初始流量。 我在ahrefs.com(外链查询网站）分析了下一个竞争对手 https://vibbi.com/instaport/， 在top referring content这个页面 我们发现mashable.com、lifehacker.com、makeuseof.com、pcmag.com等流量巨头都推荐这个小工具网站，给这个网站导入了巨大流量。 显然，这些流量巨头也发现了Instagram pictures download这个真实的用户痛点，不过他们是用了更轻的方式，用内容而不是产品，来为用户解决这个问题。通过评测这些工具类型的网站，他们给用户做出指引，而用户最终被指引到这些工具类型的网站。 所以到这一步，要做的事情很清晰，梳理出报道竞争对手的网站，联系他们并请求评测。 这就像启动一个开关，一旦有一些大网站接受了评测的需求，流量就会持续进来，后面就会启动自增长，即使什么工作也不做， 过一段时间也会发现很多评测网站主动的链向我们的工具站了。 到此，基本就实现了用户需求、产品设计、运营变现从0到1的迭代。 类似的， 不少热门关键词都有不错的可盈利的细分市场，有兴趣的试试看。]]></content>
      <tags>
        <tag>增长黑客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析系列（5）：窗口函数]]></title>
    <url>%2F2018%2F08%2F25%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%EF%BC%885%EF%BC%89%EF%BC%9A%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[这篇文章梳理了几个关于分组排序的SQL语句，在查询的时候常会遇到分组排序的场景，比如找出每个群体中中排名前十的用户，找出某一类商品销量最高的几个商品等等。它们分别是： row_number() over() rank( ) over( ) dense_rank( ) over( ) 由此引申开来，讲述一下窗口函数的功用。 一、row_number() over()排序功能：1.1 row_number( ) over( )无分组排序若不对其进行分组，则此时row_number() over()的功能与rownum一致， 1234567891011121314151617SELECT empno, WORKDEPT, SALARY, Row_Number() OVER (ORDER BY salary desc) rank FROM employee -------------------------------------- 000010 A00 152750 1 000030 C01 98250 2 000070 D21 96170 3 000020 B01 94250 4 000090 E11 89750 5 000100 E21 86150 6 000050 E01 80175 7 000130 C01 73800 8 000060 D11 72250 9 但row_number() over()更强大的地方在于可以进行分组排序。 1.2 row_number( ) over( )分组排序 partition by ： 根据字段进行分组 order by desc：在分组后的各组内根据字段进行降序排序 12345678910111213141516SELECT empno, WORKDEPT, SALARY, Row_Number() OVER (partition by workdept ORDER BY salary desc) rank FROM employee -------------------------------------- 000010 A00 152750 1 000110 A00 66500 2 000120 A00 49250 3 200010 A00 46500 4 200120 A00 39250 5 000020 B01 94250 1 000030 C01 98250 1 000130 C01 73800 2 注意：使用 row_number() over()函数时候，over()里头的分组以及排序的执行晚于 where group by order by 的执行 如果没有指定partition by，那么它把整个结果集作为一个分组，与聚合函数不同的地方在于它能够返回一个分组中的多条记录，而聚合函数一般只有一个反映统计值的记录 row_number() over()返回的排名是没有重复值的，也就是即使遇到相同的值其排名也有先后，但具体哪个在前，具有不确定性，详细可见该链接1和链接2，具体的例子可以看文末的对比。 1.3 使用row_number( ) over( )去重假设表TAB中有a,b,c三列，可以使用下列语句删除a,b,c都相同的重复行。 123456789select year, QUARTER, RESULTS, row_number() over(partition by YEAR,QUARTER,RESULTS order by YEAR,QUARTER,RESULTS) AS ROW_NO FROM SALE WHERE ROW_NO=1 所以除了使用distinct进行去重，还可以用row_number( ) over( )去重。需要注意的有以下几点： 使用关键字 distinct 去重，其作用于单个字段和多个字段的时候是不同的，作用于单个字段时，其“去重”的是表中所有该字段值重复的数据；作用于多个字段的时候，其“去重”的表中所有字段值都相同的数据。 在使用函数 row_number() over() 的时候，其是按先分组排序后，再取出每组的第一条记录来进行“去重”的 二、rank( ) over( )这是跳跃排序，在同一个分组内，若有两个第一名时接下来就是第三名 123456789101112131415161718select workdept, salary, rank() over(partition by workdept order by salary) as dense_rank_order from emp order by workdept------------------ A00 39250 1 A00 46500 2 A00 49250 3 A00 66500 4 A00 152750 5 B01 94250 1 C01 68420 1 C01 68420 1 C01 73800 3 三、dense_rank( ) over( )这是连续排序，在同一个分组内，有两个第一名时仍然跟着第二名。 1234567891011121314151617181920select workdept, salary, dense_rank() over(partition by workdept order by salary) as dense_rank_order from emp order by workdept------------------ A00 39250 1 A00 46500 2 A00 49250 3 A00 66500 4 A00 152750 5 B01 94250 1 C01 68420 1 C01 68420 1 C01 73800 2 C01 98250 3 四、三者对比这个例子可以直观地看到，rank() over()是跳跃排序，dense_rank() over()是连续排序，row_number() over()是非重复排序。 1234567891011121314151617181920select region_id, customer_id, sum(customer_sales) total, rank() over(order by sum(customer_sales) desc) rank, dense_rank() over(order by sum(customer_sales) desc) dense_rank, row_number() over(order by sum(customer_sales) desc) row_numberfrom user_ordergroup by region_id, customer_id; REGION_ID CUSTOMER_ID TOTAL RANK DENSE_RANK ROW_NUMBER---------- ----------- ---------- ---------- ---------- ---------- 8 18 1253840 11 11 115 2 1224992 12 12 129 23 1224992 12 12 139 24 1224992 12 12 1410 30 1216858 15 13 15 在这个例子中也可以看到order by 的关键字可以是group by之后的聚合函数。 五、窗口函数以上我们介绍了RANK、DENSE_RANK、ROW_NUMBER这几个函数，其实在数据库中，它们被称为窗口函数，窗口函数可以进行排序、生成序列号等一般的聚合函数无法完成的操作。它也称为OLAP函数。OLAP是OnLine Analytical Processing的简称，意思是对数据库进行实时分析处理。窗口函数就是为了实现OLAP而添加的标准SQL功能。 它的基本语法是: 12&lt;窗口函数&gt; OVER ( [PARTITION BY &lt;列清单&gt;] ORDER BY &lt;排序用列清单&gt;) 其中重要的关键字是PARTITON BY 和ORDER BY，理解这两个关键字的作用是帮助我们理解窗口函数的关键。 窗口函数大致可以分为两种： RANK、DENSE_RANK、ROW_NUMBER等专用窗口函数 能够作为窗口函数的聚合函数 （SUM, AVG,COUNT,MAX,MIN） 我们上面已经介绍过RANK、DENSE_RANK、ROW_NUMBER，可以看到它们兼具了GROUP BY子句的分组功能以及ORDER BY子句的排序功能。但是PARTITION BY不具备GROUP BY子句的汇总功能。所以使用RANK函数不会减少原表中记录的行数。 通过PARTITION BY分组后的记录集合称为窗口。此处的窗口表示范围。 目前为止我们学过的函数大多数都没有使用位置的限制，最多也就是在WHERE子句不能使用聚合函数。但是，使用窗口函数的位置却有很大的限制，确切的说，窗口函数只能在SELECT子句中使用。 所有的聚合函数都能用作窗口函数，且使用语法与专用窗口函数完全相同。 5.1 窗口函数——SUM12345678--将SUM函数作为窗口函数使用 SELECT product_id, product_name, sale_price, SUM(sale_price) OVER (ORDER BY product_id) AS current_sum FROM Product; 12345678910product_id | product_name | sale_price | current_sum------------+--------------+------------+------------- 0001 | T衫 | 1000 | 1000 0002 | 打孔器 | 500 | 1500 0003 | 运动T衫 | 4000 | 5500 0004 | 菜刀 | 3000 | 8500 0005 | 高压锅 | 6800 | 15300 0006 | 叉子 | 500 | 15800 0007 | 擦菜板 | 880 | 16680 0008 | 圆珠笔 | 100 | 16780(8 行记录) 使用聚合函数作为窗口函数时，需要在其括号内指定相应的列。像上例中，使用sale_price(销售单价)作为累加的对象, current_sum的结果为在它之前的销售单价的合计。这种统计方法称为累计。 5.2 窗口函数——AVG12345678--将AVG函数作为窗口函数使用SELECT product_id, product_name, sale_price, AVG(sale_price) OVER (ORDER BY product_id) AS current_avg FROM Product; 12345678910product_id | product_name | sale_price | current_avg------------+--------------+------------+----------------------- 0001 | T衫 | 1000 | 1000.0000000000000000 0002 | 打孔器 | 500 | 750.0000000000000000 0003 | 运动T衫 | 4000 | 1833.3333333333333333 0004 | 菜刀 | 3000 | 2125.0000000000000000 0005 | 高压锅 | 6800 | 3060.0000000000000000 0006 | 叉子 | 500 | 2633.3333333333333333 0007 | 擦菜板 | 880 | 2382.8571428571428571 0008 | 圆珠笔 | 100 | 2097.5000000000000000(8 行记录) current_avg的结果为在它之前的销售单价的平均值。像这样以“自身记录”（当前记录）作为基准进行统计，就是将聚合函数作为窗口函数使用时的最大特征。 5.2 移动平均窗口函数就是将表以窗口为单位进行分割，并在其中进行排序的函数。其中还包含在窗口中指定更详细的汇总范围的备选功能，这种备选功能中的汇总范围称为框架。 例如，指定“最靠近的3行”作为汇总对象： 12345678--指定“最靠近的3行”作为汇总对象 SELECT product_id, product_name, sale_price, AVG(sale_price) OVER (ORDER BY product_id ROWS 2 PRECEDING) AS moving_avg FROM Product; 12345678910product_id | product_name | sale_price | moving_avg------------+--------------+------------+----------------------- 0001 | T衫 | 1000 | 1000.0000000000000000 0002 | 打孔器 | 500 | 750.0000000000000000 0003 | 运动T衫 | 4000 | 1833.3333333333333333 0004 | 菜刀 | 3000 | 2500.0000000000000000 0005 | 高压锅 | 6800 | 4600.0000000000000000 0006 | 叉子 | 500 | 3433.3333333333333333 0007 | 擦菜板 | 880 | 2726.6666666666666667 0008 | 圆珠笔 | 100 | 493.3333333333333333(8 行记录) 上例中，我们使用了ROWS（行）和PRECEDING(之前)两个关键字，将框架指定为“截止到之前~行”，因此，“ ROWS 2 PRECEDING”意思就是将框架指定为“截止到之前2行”，也就是“最靠近的3行”。如果将条件中的数字改为“ROWS 5 PRECEDING”，就是“截止到之前5行”（最靠近的6行）的意思。这样的统计方法称为移动平均。 使用关键字FOLLOWING(之后)替换PRECEDING，就可以指定“截止到之后~行”作为框架。 如果希望将当前记录的前后行作为汇总对象，可以同时使用PRECEDING(之前)和FOLLOWING（之后）关键字来实现。例，将当前记录的前后行作为汇总对象： 12345678 --将当前记录的前后行作为汇总对象SELECT product_id, product_name, sale_price, AVG(sale_price) OVER (ORDER BY product_id ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS moving_avg FROM Product; 12345678910product_id | product_name | sale_price | moving_avg------------+--------------+------------+----------------------- 0001 | T衫 | 1000 | 750.0000000000000000 0002 | 打孔器 | 500 | 1833.3333333333333333 0003 | 运动T衫 | 4000 | 2500.0000000000000000 0004 | 菜刀 | 3000 | 4600.0000000000000000 0005 | 高压锅 | 6800 | 3433.3333333333333333 0006 | 叉子 | 500 | 2726.6666666666666667 0007 | 擦菜板 | 880 | 493.3333333333333333 0008 | 圆珠笔 | 100 | 490.0000000000000000(8 行记录) 当前记录的前后行的具体含义就是： 之前1行的记录 自身（当前记录） 之后1行的记录 如果能够熟练掌握框架功能，就可以称为窗口函数高手了。]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>row_number( ) over( )</tag>
        <tag>rank() over()</tag>
        <tag>dense_rank() over()</tag>
        <tag>distinct</tag>
        <tag>窗口函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析系列（4）：基于ARMA模型的资金渠道流入流出预测]]></title>
    <url>%2F2018%2F05%2F23%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9A%E5%9F%BA%E4%BA%8EARMA%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%B5%84%E9%87%91%E6%B8%A0%E9%81%93%E6%B5%81%E5%85%A5%E6%B5%81%E5%87%BA%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[该数据为某资金渠道从2017/01/01至2017/07/29这段时间内的清算金额汇总信息，每5分钟按流入流出汇总交易数据，type为SE表示流入，type为RE表示流出，其中2017/01/01至2017/07/23这段时间的每天各时段的流入流出汇总数据是齐全的，而2017/07/24至2017/07/29这六天时间只有0点到早上10点的数据。现在要求我们根据这些数据来预测这六天每天从当日0点到24点的流入流出轧差值（总流入-总流出）。 通过观察我们可以得知这些资金流入流出数据是各时间点上形成的数值序列，而时间序列分析就是通过观察历史数据预测未来的值，适用于我们这次预测。 自回归移动平均模型(ARMA(p，q))是时间序列中最为重要的模型之一，它主要由两部分组成： AR代表p阶自回归过程，MA代表q阶移动平均过程，其公式如下： Z_t=\varphi _1z_{t-1}+\varphi _2z_{t-2 }+···+\varphi _pz_{t-p}+a_t-\theta_1a_{t-1}-···-\theta_qa_{t-q}可以对其简化： \varphi_p(B)z_t=\theta_q(B)a_t\\其中\varphi_p(B)=1-\varphi_1B-\varphi_2B^2-···-\varphi_pB^p\\\theta_q(B)=1-\theta_1B-\theta_2B^2-···-\theta_qB^p一、分析过程1.1 数据预处理首先要对数据进行转换处理，type为SE的amount保持不变，type为RE的amount取负数，对缺失值记为0，这样可以方便之后处理，将数据存入字典中，key为时间，value为资金（有正负）。处理脚本如下： 1234567891011121314151617181920212223def Data_Process(input_file,header= True): amount_dict=&#123;&#125; for line in open(input_file): term = line.strip().split(',') if header: header =False continue if term[1]=='SE': if term[2] =='': amount =0 else: amount = float(term[2]) elif term[1]=='RE': if term[2] =='': amount =0 else: amount = -1*float(term[2]) time = datetime.strftime(datetime.strptime(term[0],'%Y/%m/%d %H:%M'),'%Y/%m/%d %H:%M') if not time in amount_dict: amount_dict[time] = round(amount,2) elif time in amount_dict: amount_dict[time]+=round(amount,2) return amount_dict 因为我们要对天级别的数据进行预测，可以对资金按天进行聚合： 12345678def dataframe_trans_series(amount_dict,freq='D'): data_dict=&#123;&#125; data_dict['amount'] = amount_dict data = pd.DataFrame(data_dict) data.index = pd.to_datetime(data.index) df = pd.DataFrame(data['amount'].resample(freq).sum()) time_series = df['amount'] return time_series 得到的中间数据如下所示，每一天对应一条数据，表示这一天的流入流出轧差值： 1234567892017-01-01 123081.022017-01-02 150143.022017-01-03 161112.422017-01-04 138823.622017-01-05 130364.322017-01-06 85308.562017-01-07 77484.162017-01-08 93860.22······ 1.2 绘制时序图按天聚合之后，我们可以简单看一下时序图、移动平均时序图和加权移动平均时序图： 123456789101112131415def draw_trend(timeSeries, size): f = plt.figure(facecolor='white') rol_mean = timeSeries.rolling(window=size).mean() rol_weighted_mean = pd.ewma(timeSeries, span=size) timeSeries.plot(color='blue', label='Original') plt.legend(loc='best') plt.title('Original') rol_mean.plot(color='red', label='Rolling Mean') plt.legend(loc='best') plt.title('Rolling Mean') rol_weighted_mean.plot(color='black', label='Weighted Rolling Mean') plt.legend(loc='best') plt.title('Weighted Rolling Mean') plt.show() 可以从时序图看出，该时间序列存在着波动，尚无法确定是否平稳，需要进一步的统计检验。 1.3 平稳性检验序列的平稳性是进行时间序列分析的前提条件，单位根检验（ADF）是一种常用的平稳性检验方法，他的原假设为序列具有单位根，即非平稳，对于一个平稳的时序数据，就需要在给定的置信水平上显著，拒绝原假设。 123456def test_Stationarity(ts): test = adfuller(ts) output = pd.Series(test[0:4], index=['Test Statistic','p-value','Lags Used','Number of Observations Used']) for key,value in test[4].items(): output['Critical Value (%s)'%key] = value return output 以下为检验结果，其p值小于0.01，说明拒绝原假设，得出此序列具有平稳性。 1234567Test Statistic -3.779937p-value 0.003119Lags Used 2.000000Number of Observations Used 201.000000Critical Value (5%) -2.876029Critical Value (1%) -3.463309Critical Value (10%) -2.574493 当然也可以观察其自相关图，可以看到，其自相关图快速衰减，体现了平稳性序列的特点，可认为他是平稳序列。 1234567def draw_acf_pacf(ts, lags=31): f = plt.figure(figsize=(12,12),facecolor='white') ax1 = f.add_subplot(211) plot_acf(ts, lags=31, ax=ax1) ax2 = f.add_subplot(212) plot_pacf(ts, lags=31, ax=ax2) plt.show() 1.4 对数处理对数变换主要是为了减小数据的振动幅度，使其线性规律更加明显。对数变换相当于增加了一个惩罚机制，数据越大其惩罚越大，数据越小惩罚越小。虽然上一步我们验证了序列是平稳的，取对数也可以在该数据集上使用。 12def Log_Process(time_series): return np.log(time_series+1) 1.5 纯随机性检验经过平稳性检验之后，我们得到该序列为平稳性序列，接下来对其进行纯随机性检验，只有当时间序列不是一个白噪声即纯随机序列的时候，我们才可以对该序列做进一步的分析： 123def test_stochastic(ts): p_value = acorr_ljungbox(ts, lags=1)[1] return p_value 得到的p值如下，可知，该序列为非白噪声序列。 16.09097901e-21 1.6 确定ARMA的阶数由前面的平稳性检验和纯随机性检验可知，该序列为平稳序列且非白噪声序列，我们可以用ARMA(p,q)模型对其建模，这里关键的是p和q的定阶，我们可以通过观察自相关图ACF和偏自相关图PACF来识别，也可以借助AIC、BIC统计量来确定。 1.6.1 BIC识别以下是根据设定的maxlag，循环对p、q赋值，选出拟合BIC最小的p、q值作为模型的参数，并输出BIC最小的模型： 12345678910111213141516171819def choose_better_model(time_series, maxLag=5): init_bic = sys.maxint init_p = 0 init_q = 0 init_properModel = None for p in np.arange(maxLag): for q in np.arange(maxLag): model = ARMA(time_series, order=(p, q)) try: results_ARMA = model.fit(disp=-1, method='css') except: continue bic = results_ARMA.bic if bic &lt; init_bic: init_p = p init_q = q init_properModel = results_ARMA init_bic = bic return init_bic, init_p, init_q, init_properModel 1init_bic = 33.0754219416592 init_p = 1 init_q = 1 得到使BIC最小的模型，bic = 33.1，p、q分别为1。 1.6.2 ACF、BCF识别通过观察自相关图ACF和偏自相关图PACF来识别。观察下图，发现自相关和偏相系数都存在拖尾的特点，并且他们都具有明显的一阶相关性，所以我们设定p=1, q=1。 1.7 拟合ARAM模型选出BIC最小的模型，对训练集进行拟合。这里添加了一步将对数处理和差分处理还原的操作： 123bic, p, q, properModel = choose_better_model(time_series_diff) predict_time_series = properModel.predict()predict_time_series = recover(predict_time_series, time_series_train, best_diff) 1.8 训练集的拟合优度用拟合好的模型对训练集进行预测，并使用均方根误差对效果进行衡量： 12345678original_time_series = time_series[predict_time_series.index] plt.figure(facecolor='white')predict_time_series.plot(color='blue', label='Predict')original_time_series.plot(color='red', label='Original') RMSE = np.sqrt(sum((predict_time_series-original_time_series)**2)/original_time_series.size)plt.title('RMSE: %.4f'% RMSE)plt.show()print("训练集的RMSE为："+str(RMSE)) 训练集预测值和真实值拟合图如下：训练集的均方根误差为： 1训练集的RMSE为：40024.97 1.9 预测结果用拟合好的模型对测试集进行预测： 123test_predict = properModel.predict(start='2017-07-24', end='2017-07-29')test_predict = recover(test_predict, time_series_train, best_diff)print test_predict 得到最终的结果如下： 1234562017-07-24 244854.6896372017-07-25 222634.0790942017-07-26 206351.9891432017-07-27 194213.0937082017-07-28 185037.2966892017-07-29 178024.480144 二、问题2.1 请给出你的分析过程、预测结果、建模脚本（要求可复现），包含必要的说明以及截图分析过程和预测结果如上所示，建模脚本见百度网盘，密码：9okf，包括两个文件，一个为jupyter notebook【SERE_PREDICT.ipynb】，一个为python脚本【SERE_PREDICT.py】，两者均可复现。实验环境为python2.7，实验所依赖的package包括： pandas numpy datetime matplotlib statsmodels 2.2 你认为你的模型预测结果如何，附上关键说明和截图在本实验中，我使用均方根误差来评价模型预测效果，得到在训练集上的训练集的均方根误差为：40024.97。因为该数据集的数值均较大，日流入流出均达到了十万级别，这样的均方根误差相对来说效果较好，但有较大的提升空间。在分析过程中已有具体的呈现和说明。 2.3 如果需要提高预测准确率，可以从哪些方面入手，简单列举即可 在时间序列中存在的噪声，会干扰序列中每个点的波动，给预测造成难度，可以使用卡尔曼滤波来过滤这个噪声。将卡尔曼滤波算法与ARMA模型结合时，卡尔曼滤波算法可以在当获得一个新的数据点时，递归地更新状态变量（预测值）的信息，可以起到对ARMA模型的修正作用，在一定程度上提高ARMA模型的预测精度。 注意到我们需要预测的那几天在0点至10点是存在资金流动数据的，我们可以将这一部分数据作为我们预测全天数据的先验信息，提高预测精度。 如果我们具有有关资金流动的其他信息，诸如资金流动的来源、股市波动信息、宏观政策的变动、市场波动的信息等等，就可以构建一个特征集，用LSTM（长短时循环神经网络）来对序列进行建模。当然要是没有这些信息，也可以直接用历史记录 $[y_{t-1}, y_{t-2},…]$ 以及他们的各种统计量如均值、最大最小值、一阶差分、二阶差分等对 $x_t$ 做特征工程，以此丰富其维度，再使用LSTM对其进行建模。]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>时间序列</tag>
        <tag>ARMA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理系列（10）：自然语言处理的发展与趋势]]></title>
    <url>%2F2018%2F05%2F02%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%8810%EF%BC%89%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9A%84%E5%8F%91%E5%B1%95%E4%B8%8E%E8%B6%8B%E5%8A%BF%2F</url>
    <content type="text"><![CDATA[译自Deep Learning for Natural Language Processing (NLP): Advancements &amp; Trends 在过去的几年里，深度学习(DL)架构和算法在图像识别和语音处理等领域取得了令人瞩目的进展。 起初，他们在自然语言处理(NLP)上的应用起初并没有那么令人印象深刻，但现在已经被证明能够为一些常见的NLP任务提供最先进的结果。在命名实体识别(NER)，语音(POS)标记或情绪分析等领域，神经网络模型表现优于传统方法。机器翻译的进步也许是最显著的。 在本文中，我将介绍一些依赖于深度学习技术的自然语言处理领域的最新进展。我并不是在装模作样:考虑到有大量的科学论文、框架和工具，这是不可能的。我只想和大家分享一些我最喜欢的作品。我认为过去的几个月对我们的领域非常有利。深度学习在NLP中的使用不断扩大，在某些情况下产生惊人的结果，所有迹象都表明这一趋势不会停止。 From training word2vec to using pre-trained models可以说，词嵌入是最广为人知的与深度学习相关的技术。他们遵循了Harris (1954)的分布假设，根据这一理论，具有相似意义的词通常出现在类似的语境中。我建议你读一下Gabriel Mordecki的这篇文章。 Example of distributional vectors of words. 像word2vec(Mikolov et al .,2013)和GloVe(Pennington et al .,2014)已经是这个领域的先驱，尽管他们不能被视为深度学习(word2vec中的神经网络是浅层的而GloVe实现了基于计数的方法)，作为模型训练得到的中间产物，他们在很多的深度学习 NLP的方法中作为输入数据。在我们的领域使用词嵌入现在普遍认为是一种不错的实践。 一开始，对于一个给定的NLP问题，需要词嵌入，我们倾向于从一个与领域相关的大型语料库中训练我们自己的模型。当然，这并不是普遍使用词嵌入的最好方法，所以预先训练的模型开始慢慢地出现。在维基百科、推特、谷歌新闻、网络爬虫等数据集上进行了训练，这些模型允许你很容易地将词嵌入与你的深度学习算法结合起来。 最新的发展证实，预先训练的词嵌入模型仍然是NLP的一个关键问题。例如，来自Facebook人工智能研究(FAIR)实验室的fastText发布了294种语言的预先训练的向量，它对我们领域做出了巨大贡献。除了大量的不同语言之外，因为fastText使用字符级别的n-grams作为特征，这是非常有用的。这使得fastText可以避免OOV(脱离词汇)问题，因为即使是非常罕见的单词(例如特定的领域术语)，也可能会与更常见的单词共享某些字符n-grams。从这个意义上说，fastText的性能优于word2vec和GloVe，并优于小型数据集。 然而尽管我们可以看到一些进展，但在这方面还有很多工作要做。例如，很棒的 NLP框架spaCy将词嵌入和DL模型集成到诸如NER和依赖性解析等任务中，允许用户更新模型或使用他们自己的模型。 我认为这是一种会不断发展的方法。将来，在NLP框架中使用易于使用的特定领域(如生物学、文学、经济等)的预先训练的模型将是非常棒的。锦上添花的是，用最简单的方法，对我们的用例进行微调。同时，词嵌入的方法也开始出现。 Adapting generic embeddings to specific use cases也许使用预先训练的词嵌入的主要缺点是，训练数据与实际问题的数据之间存在分布的差异。假设你有一份生物学论文，食物食谱或者经济学研究论文。更有可能的是，通用的词嵌入会帮助你提高结果，因为你可能没有足够大的语料库来训练好的嵌入。但如果您可以将通用的词嵌入应用到特定的用例中呢? 这些类型的适应通常被称为NLP中的跨域或域适应技术，并且非常接近转移学习。Yang等人提出了一项非常有趣的研究。考虑到源域的词嵌入，他们提供了一个正则化的skip-gram模型，用于学习针对目标领域的词嵌入。 关键思想简单而有效。假设我们知道在源域中的单词$word_w$的词嵌入为$w_s$。为了计算$w_t$(目标域)的词嵌入，作者在两个域之间添加了一定的传输量。基本上，如果两个域中的单词都是频繁的，那就意味着它的语义不依赖于域。在这种情况下，传输量是很高的，因此在这两个域中所产生的嵌入往往是相似的。但是由于特定领域的单词在一个域中比另一个更频繁，所以传输量很小。基本上，如果两个域中的单词都是频繁的，那就意味着它的语义不依赖于域。在这种情况下，传输量是很高的，因此在这两个域中所产生的词嵌入往往是相似的。但是如果由于特定领域的单词在一个域中比另一个更频繁，则传输量很小。 这是一个关于词嵌入的研究课题，并没有得到广泛的研究，我认为它将在不久的将来得到更多的关注。 Sentiment analysis as an incredible side effect盘尼西林，x光，甚至是post-it都是出乎意料的发现。 Radford等人正在研究字节级循环语言模型的特点，目标是预测亚马逊评论文本中的下一个字符，当他们发现经过训练的模型中的单个神经元对情绪价值的预测非常准确。 Review polarity vs Value of the neuron. 在注意到这一表现之后，作者决定在斯坦福情绪树数据集上对模型进行测试，发现其准确性为91.8%，而之前最好的是90.2%。这意味着，使用明显较少的例子，他们的模型，以无监督的方式训练，至少在一个特定但广泛研究的数据集上达到了最优的情绪分析效果，。 The sentiment neuron at work由于该模型在作用在字符级之上，所以神经元在文本中改变每个字符的状态，看到它的行为如何运作是非常惊人的。 Behavior of the sentiment neuron. 例如，在单词best之后，神经元的值变得非常积极。然而，这个效应随着“可怕”这个词的出现而消失了，这是有道理的。 Generating polarity biased text当然，经过训练的模型仍然是一个有效的生成模型，因此它可以用来生成类似于Amazon评论的文本。但我发现，你可以通过简单地改写情绪神经元的值来选择生成的文本的极性。 Examples of generated texts (source). 作者选择的NN模型是由Krause等人(2016)提出的一种multiplicative LSTM，主要是因为他们观察到，对于他们所探索的超参数设置，它比普通的LSTMs收敛速度快更快。它拥有4096个单元，并接受了8200万亚马逊评论的训练。 为什么受过训练的模特如此精确地捕捉到情感的概念仍然是一个开放而迷人的问题。同时，你可以试着训练你自己的模型和实验。如果你有时间和gpu，当然这个特定模型通过四个NVIDIA Pascal gpu用了一个月的时间来进行训练。 Sentiment Analysis in Twitter无论是去了解人们对你的商业品牌的看法，还是去了解分析营销活动的影响或者在最后一场竞选活动中评估全球对希拉里•克林顿(Hillary Clinton)和唐纳德•特朗普(Donald Trump)的感觉，Twitter的情绪分析是一个非常有力的工具。 Donald Trump vs Hillary Clinton: sentiment analysis on Twitter. SemEval 2017在Twitter上的情绪分析引起了NLP研究人员的广泛关注，同时也引起了政治和社会科学的关注。这就是自2013年以来，SemEval提出了一个具体的任务的原因。 在2017年，共有48个团队参与了评估，这体现了它产生了多大的兴趣。为了让您了解对于Twitter数据SemEval是如何评估的，让我们来看看今年提出的五个子任务。 子任务 A：发一条推文，决定它表达的是积极的、消极的还是中性的情绪。 子任务 B：给定一条推文和一个主题，将对这个主题的情绪分成两部分:积极的和消极的。 子任务 C：给定一条推文和一个主题，将推文中传达的情绪分类为5个点:强阳性、弱阳性、中性、弱阴性和强阴性。 子任务 D：给定一组关于主题的tweet，估计在正和负类上的tweet的分布。 子任务 E：给定一组关于主题的推文，估计5个类的推文的分布:强阳性、弱阳性、中性、弱阴性和强阴性。 如您所见，子任务A是最常见的任务，有38个团队参与其中，但其他任务更具挑战性。组织者注意到，深度学习方法的使用非常突出，并且不断增加，今年有20个团队使用了像卷积神经网络(CNN)和长短期记忆网络(LSTM)这样的模型。此外，因为SVM模型仍然非常流行，一些参与者将它们与神经网络方法或使用的词嵌入特性结合起来。 The BB_twtr system值得注意的是，一个纯粹的深度学习系统，即_BB_twtr_系统(Cliche，2017)，在英语5个子任务中排名第一。作者结合了10个CNNs和10个biLSTMs的合集，采用不同的超参数和不同的预训练策略。您可以在本文中看到网络结构的详细信息。 为了训练模型,作者用人类标注的tweets数据(子任务A达到了49693的数量级)，并构建一个由1亿条无标注tweets组成的数据集，他提取一个distant dataset，通过将存在积极正面的表情符号如 :-) 标记为正类，反之亦然。这些推文中，小写的、标记化的、url和表情符号等等被特定的标记替换，并统一重复字符，例如，“niiice”和“niiiiiice”都变成了“niice”。 使用之前的SemEval数据集的实验表明，使用Glove会降低性能，而且对于所有的黄金标准数据集，并没有唯一的最佳模型。然后作者将所有的模型与软投票策略结合起来。结果模型比之前的2014年和2016年的最佳历史成绩要好，而且在其他年份非常接近。最后，它在2017年的5个英语子任务中排名第一。 即使组合不是以一种有机的方式进行，而是使用简单的软投票策略，这项工作显示了结合深度学习模型的潜力，同时也说明了端到端方法(输入必须预先处理)可以在Twitter的情绪分析中胜过监督学习的方法。 An exciting abstractive summarization system自动摘要，自动翻译，是NLP首要任务之一。有两种主要的方法: 基于抽取的方法，它通过从源文本中提取最重要的部分来构建摘要；基于抽象的方法，通过生成文本来构建摘要。从历史上看，基于抽取的方法是最常见的，因为它们的简单性超过了抽象的方法。 在过去的几年里，基于RNN的模型在文本生成方面取得了惊人的成果。它们在短文本的输入和输出中表现得非常好，但对于长文本来说，它们往往是不连贯和重复的。在他们的研究中，Paulus等提出了一种新的神经网络模型来克服这一局限性。结果是令人兴奋的，正如您在下图所看到的。 作者使用一个bi-LSTM编码器读取输入和一个LSTM解码器来生成输出。它们的主要贡献是一种新的内部注意力机制，即对输入和连续生成的输出进行单独的处理，以及一种新的训练方法，它结合了标准的监督词预测和强化学习。 Intra-attention strategy提出的内部注意力机制的目标是避免输出重复。为了实现这一目标，他们在解码之前对输入文本的前半部分进行解码，然后再决定下一个词的生成。这迫使模型在生成过程中使用不同的输入部分。它们还允许模型从解码器访问以前的隐藏状态。然后将这两个函数组合起来，为输出摘要选择最佳的下一个单词。 Reinforcement learning为了生成一个摘要，两个不同的人将使用不同的单词和句子顺序，这两个摘要都可能被认为是有效的。因此，一个好的摘要并不一定必须是一组序列单词来尽可能地匹配训练数据集的序列。知道了这一点，作者就避免了标准的teacher forcing 算法，这使得每个解码步骤(即每个生成的单词)的损失最小化，并且他们依赖于一个强化学习策略，这证明是一个很好的选择。 Great results for an almost end-to-end model该模型在CNN/Daily Mail数据集上进行了测试，并取得了最好的结果。此外，人类评估者的一项具体实验表明，对于人类的可读性和质量也有所提高。此外，基本的预处理的结果令人印象深刻:输入文本被标记，小写，数字被替换为“0”，数据集的某些特定实体被删除。 A first step towards fully unsupervised machine translation?双语词典构建是一项古老的NLP任务，即在两种语言中使用源和目标的单语语料库来识别词翻译匹配对。自动构建的双语词典对其他NLP任务有所帮助，如信息检索和统计机器翻译。然而，这些方法大部分时间都依赖于某种资源，通常是最初的双语词典，它并不总是可用，也不容易构建。 随着词嵌入的成功，跨语言词嵌入的思想出现了，目的是将嵌入空间而不是词汇图标对齐。不幸的是，第一种方法也依赖于双语词汇或平行语料库。在他们的研究中，Conneau等人(2018)提出了一种非常有前途的方法，它不依赖于任何特定的资源，并且对于单词翻译、句子翻译检索和跨语言单词相似性的任务，在几个语言的pairs数据上优于最先进的监督方法。 作者所开发的方法是输入两套在单语数据上单独训练的词嵌入，并在它们之间学习一种映射，这样翻译在公共空间中就很接近了。他们使用未经监督的单词向量，在维基百科上用fastText训练。下面的图片说明了这个关键的想法。 Building the mapping between two word embedding spaces. 红色的X分布是英语单词的嵌入，蓝色的Y分布是意大利语词的嵌入。 首先，他们使用对抗学习来学习一个旋转矩阵W，它将执行第一个原始的对齐。他们主要训练一个生成对抗网络(GAN)，遵循Goodfellow等人(2014)的主张。为了直观地了解甘斯的工作方式，我向您推荐巴勃罗·索托的这篇优秀文章。 为了从对抗性学习的角度对问题进行建模，他们定义了discriminator来决定，给定一些从WX和Y中随机取样的元素(见上图中的第二列)，每种语言都属于哪一种语言。然后他们训练W，以防止discriminator做出正确的预测。这在我看来是非常聪明和优雅的，最终的结果是相当不错的。 在此之后，他们再应用两个步骤来完善映射。一是避免在映射计算中引入稀疏的噪声。另一个是建立实际的翻译，主要是利用学习的映射和距离度量。 在某些情况下，结果令人印象深刻的是最先进的。例如，在英语-意大利语单词翻译的例子中，在P@10的案例中，它们的平均精度超过了1.500个源单词的最佳平均精度。 English-Italian word translation average precisions. 作者声称，他们的方法可以作为无监督机器翻译的第一步。如果真是这样，那就太好了。同时，让我们看看这个新的有希望的方法能走多远。 Specialized frameworks and tools有很多通用的深度学习框架和工具，其中一些被广泛使用，比如TensorFlow、Keras或PyTorch。然而，特定的开源NLP导向的深度学习框架和工具刚刚出现。对于我们来说，这是一个好年头，因为一些非常有用的开源框架已经向社区开放了。其中三个特别引起了我的注意，你可能也会觉得有趣。 AllenNLPAllenNLP框架是建立在PyTorch之上的一个平台，它可以在语义NLP任务中轻松使用DL方法。它的目标是让研究人员设计和评估新的模型。它包括用于常见语义NLP任务的模型的参考实现，如语义角色标记、文本蕴涵和相关解析。 ParlAIParlAI框架是一个用于对话研究的开源软件平台。它是在Python中实现的，其目标是为对话模型的共享、训练和测试提供一个统一的框架。ParlAI提供了一种与Amazon Mechanical Turk轻松集成的机制。它还提供了该领域的流行数据集，并支持多种模型，包括神经模型，如内存网络、seq2seq和专注的LSTMs。 ParlAI框架是一个用于对话研究的开源软件平台。它是在Python中实现的，其目标是为对话模型的共享、训练和测试提供一个统一的框架。ParlAI提供了一种与Amazon Mechanical Turk轻松集成的机制。它还提供了该领域的流行数据集，并支持多种模型，包括神经模型，如记忆网络、seq2seq和基于注意力机制的LSTMs。 OpenNMTOpenNMT工具包是一个专门用于序列到序列模型的通用框架。它可以用来执行诸如机器翻译、摘要、图像到文本、语音识别等任务。 Final thoughts用于NLP问题的DL技术的持续增长是不可否认的。一个很好的指标是，在过去的几年里，ACL、EMNLP、EACL和NAACL等关键NLP会议的深度学习论文百分比的变化。 Percentage of deep learning papers. 然而，真正的端到端学习才刚刚开始。我们仍在处理一些典型的NLP任务，以准备数据集，例如清理、标记或统一某些实体(例如url、数字、电子邮件地址等)。我们还使用了通用的嵌入，缺点是它们未能捕捉到特定领域术语的重要性，而且它们在多字表达式中表现不佳，这是我在我所研究的项目中反复发现的一个关键问题。 最新的进展对DL应用于NLP来说非常好。我希望未来能够带来更多的端到端学习的工作，并且特定的开放源码框架得到了更大的发展。请在评论部分与我们分享您对这些作品和框架的看法，以及您今年喜欢的和我在这里没有提到的。 Further reading关于NLP研究中深度学习方法的更多信息，我强烈推荐你在Young等人的“Recent Trends in Deep Learning Based Natural Language Processing”这篇优秀论文。 另一个有趣的阅读报告是Blunsom等人的“From Characters to Understanding Natural Language (C2NLU): Robust End-to-End Deep Learning for NLP”(2017)，研究人员在NLP，计算语言学、深度学习和通用机器学习所讨论的优势和挑战为深度学习模型使用字符作为输入而不是特定的语言符号。 为了在模型之间有一个比较的视角，我可以给你推荐一个非常有趣的CNN和RNN的比较研究，由Yin等人(2017)进行。 为了直观地了解GANs的工作原理，你可以阅读巴勃罗·索托(Pablo Soto)的这篇出色的文章，它展示了2016年深度学习的重大进展。 我建议你读一下加布里埃尔·莫德基的这篇文章。它以一种说教和娱乐的方式写成，解释了不同的方法，甚至是一些关于词嵌入的传说。 最后，Sebastian Ruder在2017年写了一篇关于词嵌入的很好的文章，你可能会觉得很有用:关于2017年的词汇嵌入: About Word embeddings in 2017: Trends and future directions Bibliography From Characters to Understanding Natural Language (C2NLU): Robust End-to-End Deep Learning for NLP Phil Blunsom, Kyunghyun Cho, Chris Dyer and Hinrich Schütze (2017) From Characters to Understanding Natural Language (C2NLU): Robust End-to-End Deep Learning for NLP Phil Blunsom, Kyunghyun Cho, Chris Dyer and Hinrich Schütze (2017) BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs Mathieu Cliche (2017) Word Translation without Parallel Data Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, Hervé Jégou (2018) Generative adversarial nets Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville and Yoshua Bengio (2014) Distributional structure Zellig Harris (1954) OpenNMT: Open-source toolkit for neural machine translation Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart and Alexander M Rush. (2017) Multiplicative lstm for sequence modelling Ben Krause, Liang Lu, Iain Murray and Steve Renals (2016) Parlai: A dialog research software platform Alexander H Miller, Will Feng, Adam Fisch, Jiasen Lu, Dhruv Batra, Antoine Bordes, Devi Parikh and Jason Weston (2017) Linguistic Regularities in Continuous Space Word Representations Tomas Mikolov, Scott Wen-tau Yih and Geoffrey Zweig (2013) Glove: Global vectors for word representation Jeffrey Pennington, Richard Socher and Christopher D. Manning (2014) Learning to Generate Reviews and Discovering Sentiment Alec Radford, Rafal Jozefowicz and Ilya Sutskever (2017) A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings Wei Yang, Wei Lu, Vincent Zheng (2017) Comparative study of CNN and RNN for Natural Language Processing Wenpeng Yin, Katharina Kann, Mo Yu and Hinrich Schütze (2017) Recent Trends in Deep Learning Based Natural Language Processing Tom Younga, Devamanyu Hazarikab, Soujanya Poriac and Erik Cambriad (2017)]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
        <tag>NLP</tag>
        <tag>Sentiment Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（13）：QCon北京2018 深度学习分享总结]]></title>
    <url>%2F2018%2F04%2F24%2F2018-04-24%2F</url>
    <content type="text"><![CDATA[参加了历时三天的QCon北京的技术分享大会，一个码农云集的盛会，各大厂拿出自己的看家技术，真的是干货满满。特别感谢金主爸爸给的免费通票和树苗苗姐姐的联络，让我这样的小白加穷学生可以有机会参加这样规模的大会，还加了很多业界大佬的微信，见到了之前膜拜已久的洪强宁老师和张俊林老师，技术交流真的获益匪浅，扩宽了眼界，看到了自己的不足，有很多不懂的地方需要自己慢慢去补课。 下面是一些个人收获比较大的分享内容记录和整理，之后还会慢慢更新。 一、达观数据 陈运文 《文本智能处理的深度学习技术》人工智能目前的三个主要细分领域为图像、语音和文本，老师分享的是达观数据所专注的文本智能处理领域。文本智能处理，亦即自然语言处理，试图让机器来理解人类的语言，而语言是人类认知发展过程中产生的高层次抽象实体，不像图像、语音可以直接转化为计算机可理解的对象，它的主要应用主要是在智能问答，机器翻译，文本分类，文本摘要，标签提取，情感分析，主题模型等等方面。 自然语言的发展历程经历了以下几个阶段。这里值得一提的是，关于语言模型，早在2000年，百度IDL的徐伟博士提出了使用神经网络来训练二元语言模型，随后Bengio等人在2001年发表在NIPS上的文章《A Neural Probabilistic Language Model》，正式提出神经网络语言模型（NNLM），在训练模型的过程中也能得到词向量。2007年，Mnih和Hinton在神经网络语言模型（NNLM）的基础上提出了log双线性语言模型（Log-Bilinear Language Model，LBL），同时，Hinton在2007年发表在 ICML 上的《Three new graphical models for statistical language modelling》初见其将深度学习搬入NLP的决心。2008年，Ronan Collobert等人 在ICML 上发表了《A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning》，其中的模型名字叫C&amp;W模型，这是第一个直接以生成词向量为目标的模型。LBL与NNLM的区别正如它们的名字所示，LBL的模型结构是一个log双线性结构；而NNLM的模型结构为神经网络结构。这些积淀也成就了Mikolov创造了实用高效的Word2Vec工具，起初，他用循环神经网络RNNLM来做语言模型，发表paper《Recurrent neural network based language model》，之后就是各种改进，博士论文研究的也是用循环神经网络来做语言模型，《Statistical Language Models based on Neural Networks》。2013年，Mikolov等人同时提出了CBOW和Skip-gram模型。使用了Hierarchial Softmax和Negative Sampling两种trick来高效获取词向量。当然这个模型不是一蹴而就的，而是对于前人在NNLM、RNNLM和C&amp;W模型上的经验，简化现有模型，保留核心部分而得到的。同时开源了Word2Vec词向量生成工具，深度学习才在NLP领域遍地开花结果。 一般地，文本挖掘各种类型应用的处理框架如下所示： 文本数据经过清洗、分词等预处理之后，传统方法通过提取诸如词频、TF-IDF、互信息、信息增益等特征形成高维稀疏的特征集合，而现在则基本对词进行embedding形成低维稠密的词向量，作为深度学习模型的输入，这样的框架可用于文本分类、情感分析、机器翻译等等应用场景，直接端到端的解决问题，也无需大量的特征工程，无监督训练词向量作为输入可带来效果的极大提升。 文本分类对于文本分类，以下列出了几种典型的深度学习模型： 序列标注序列标注的任务就是给每个汉字打上一个标签，对于分词任务来说，我们可以定义标签集合为：$LabelSet=\{ B,M,E,S\}$。B代表这个汉字是词汇的开始字符，M代表这个汉字是词汇的中间字符，E代表这个汉字是词汇的结束字符，而S代表单字词。下图为中文分词序列标注过程： 中文分词转换为对汉字的序列标注问题，假设我们已经训练好了序列标注模型，那么分别给每个汉字打上标签集合中的某个标签，这就算是分词结束了，因为这种形式不方便人来查看，所以可以增加一个后处理步骤，把B开头，后面跟着M的汉字拼接在一起，直到碰见E标签为止，这样就等于分出了一个单词，而打上S标签的汉字就可以看做是一个单字词。于是我们的例子就通过序列标注，被分词成如下形式：{跟着 Tfboys 学 左手 右手 一个 慢动作} 对于序列标注，传统的方法基本是使用大量的特征工程，进入CRF模型，但不同的领域需要进行相应的调整，无法做到通用。而深度学习模型，例如Bi-LSTM+CRF则避免了这样的情况，可以通用于不同的领域，且直接采用词向量作为输入，提高了泛化能力，使用LSTM和GRU等循环神经网络还可以学习到一些较远的上下文特征和一些非线性特征。 经典的Bi-LSTM+CRF模型如下所示： 生成式摘要对于生成式摘要，采用Encode-Decoder模型结构，两者都为神经网络结构，输入原文经过编码器编码为向量，解码器从向量中提取关键信息，组合成生成式摘要。当然，还会在解码器中引入注意力机制，以解决在长序列摘要的生成时，个别字词重复出现的问题。 此外，在生成式摘要中，采用强化学习与深度学习相结合的学习方式，通过最优化词的联合概率分布，即MLE(最大似然)，有监督进行学习，在这里生成候选的摘要集。模型图如下： 模型图中的ROUGE指标评价是不可导的，所以无法采用梯度下降的方式训练，这样我们就考虑强化学习，鼓励reward高的模型，通过给予反馈来更新模型。最终训练得到表现最好的模型。 知识图谱关系抽取对于知识图谱的关系抽取，主要有两种方法，一个是基于参数共享的方法，对于输入句子通过共用的 word embedding 层，然后接双向的 LSTM 层来对输入进行编码。然后分别使用一个 LSTM 来进行命名实体识别 (NER)和一个 CNN 来进行关系分类(RC)；另一个是基于联合标注的方法，把原来涉及到序列标注任务和分类任务的关系抽取完全变成了一个序列标注问题。然后通过一个端对端的神经网络模型直接得到关系实体三元组。 如下图所示，我们有三类标签，分别是 ①单词在实体中的位置{B(begin),I(inside),E(end),S(single)}、②关系类型{CF,CP,…}和③关系角色{1(entity1),2(entity2)}，根据标签序列，将同样关系类型的实体合并成一个三元组作为最后的结果，如果一个句子包含一个以上同一类型的关系，那么就采用就近原则来进行配对。 二、爱奇艺 方非 《爱奇艺视频信息流推荐的深度学习之路》爱奇艺的短视频内容生态主要由OGC（Occupationally-generated Content，职业生产内容）、PGC（Professionally-generated Content，专业生产内容）、UGC（User-generated Content，用户生产内容），布局到PC端、移动端、TV端和VR，通过个性化推荐并使用feed流或者多列瀑布流进行高效内容分发，它的整个内容平台如下所示： 爱奇艺的深度学习架构设计主要分为深度召回模型和深度排序模型， 深度召回模型召回，即给定上下文{ User， Context }，从所有推荐集合中过滤出具备推荐价值的内容{ Item List }，推荐价值使用兴趣相关性、热门、好友都在看、关注、LBS等等来定义。 协同过滤方法是基于用户行为来 预测/推荐 的一类算法，它基于这样一种假设：相似的视频更大概率会被同一个用户(相似的用户)看过；相似兴趣的用户会更大概率看同一个视频(相似的视频)；历史会重复发生。 基于近邻的协同过滤方法有Item-based CF和User-based CF，虽然实现简单，不依赖内容本身，能得到基本稳定可靠的结果，可解释性强；但是对稀疏性非常敏感，没有考虑行为顺序和上下文变化，难以加入其它特征，高维空间建模，难以和其他模型结合。 基于协同过滤思想的还有其他很多模型，分别如下 Matrix Factorization ：SVD &amp; ALS Bayesian Model Classification &amp; Clustering Embedding：Item2Vec、Neural CF Deep Learning：RBM、MLP、LSTM、Attention 概括一下基于Embedding和基于Deep Learning的协同过滤模型 Item2Vec由Barkan O和Koenigstein N于2016年在论文《Item2Vec: Neural Item Embedding for Collaborative Filtering》中提出， 作者受nlp中运用embedding算法学习word的latent representation的启发，特别是参考了google发布的的word2vec（Skip-gram with Negative Sampling，SGNS），利用item-based CF 学习item在低维 latent space的 embedding representation，优化 item2item的计算。其模型结构如下： Item2vec中把用户浏览的商品集合等价于word2vec中的word的序列，即句子（忽略了商品序列空间信息spatial information） ，出现在同一个集合的商品对视为 positive。对于集合$w_{1}, w_{2}, …,w_{K}$，模型的目标函数是最大化平均的log概率： \frac{1}{K}\sum_{i=1}^{K}\sum_{j≠i}^K logp(w_j|w_i)同word2vec，利用负采样，将$p(w_{j}|w_{j})$定义为：P(I_j|I_i)=\sigma (\mu_i^T v_j) \prod _{k=1}^N\sigma (-\mu_i^Tv_k)subsample的方式也类似于word2vec，丢弃某一个集合$w_i$的概率为： P(discard|w_i)=1-\sqrt{\frac{t}{f(w_i)}}最终，利用SGD方法学习参数最大化目标函数，任意Item都能 得到U和V两个一定维度的向量，这样我们得到每个商品的embedding representation，商品之间可以两两计算cosine相似度，即为商品的相似度。这个模型的缺点是相似度的计算只利用到了item的共现信息，忽略了user行为序列信息，且没有建模用户对不同item的喜欢程度高低。 Neural CFXiangnan He , Lizi Liao , Hanwang Zhang , Liqiang Nie , Xia Hu , Tat-Seng Chua, Neural Collaborative Filtering, Proceedings of the 26th International Conference on World Wide Web, April 03-07, 2017, Perth, Australia 基于MLP的协同过滤模型模型结构如下： 这个模型增加了网络深度，但缺乏时序性考虑，没有其他的特征信息。 LSTM基于LSTM的协同过滤模型，加入时序性的考虑，但缺乏对历史项重要性的学习，且没有其他的特征信息。模型结构如下： LSTM + Attention基于LSTM + Attention的协同过滤模型加入时序性考虑，且在LSTM的基础上加入对历史项Attention的学习，但还是存在没有其他的特征信息的问题。模型结构如下： 深度召回模型 深度排序模型排序算法，即从召回算法选取的内容集合中，找到最符合推荐预定目标的K个结果。主要有三种，分别是Pointwise、Pairwise、Listwise。排序模型的四个要素：目标、样本、特征和模型。 目标 最终的最大化目标有以下这些，目标有长期与短期之分，也有主次之分，每一个目标都有各自的优化偏重： 目标 特点 点击率 简单直接，偏短时长 时长 更能反应用户的兴趣，偏长时长 互动 社交属性，稀疏 关注订阅 社交属性，内容生态，稀疏 留存 最接近用户满意度，直接优化难度高 当然还会受到许多约束，诸如内容价值观、多样性、时效性、长尾分发、标题党、反作弊等 样本 然后根据目标仔细设计样本，将点击、点赞、评论、转发、关注、留存等定义为正样本，把展示不点击、不喜欢、负面评论等定义为负样本，并根据播放时长、反馈类型、样本有效性对样本进行加权。此外，处于样本有效性的考虑，会将无效样本判断过滤掉 特征 关于特征，根据特征向量的特点可分为三种：稠密特征、稀疏特征和Embedding特征。 稠密特征 稀疏特征 Embedding特征 点击率 视频 ID 标题 Embedding 播放量 视频标签 Item2Vec Embedding 播放时长 用户标签 图片 Embedding 年龄 订阅作者 ID 社交 Embedding 性别 LBS End2End Embedding 模型 关于模型这一块，已经有很成熟的模型可以使用，主要有以下几种：LR、FM、GBDT、GBDT + FM(LR)、DNN+FM（DeepFM: A Factorization-Machine based Neural Network for CTR Prediction） 这里使用的是基于Wide And Deep思想的多目标分类，模型结构如下： 最后是整个排序服务平台的搭建，其组成部分如下所示： 三、优酷 刘尚堃 《深度学习在视频搜索中的应用》视频搜索的挑战视频搜索中存在以下这些挑战： 非结构化、无组织，这就提升了召回难度 短文本、信息不充分，带来语义上的困难 海量短视频，给用户选择带来困难 针对这些挑战，提出了一些利用深度学习来解决问题的对策，分别是 基于视频内容理解的召回 语义模型、语义表征 个性化表征 视频内容理解—召回基于类目标签的召回输入任意的视频，通过内容理解的方法对视频进行类目和标签的预测。采用的方法是CNN+LSTM的端到端预测方法。 基于事件/场景的召回给定不定长视频，定位感兴趣行为发生的时间段并给出对应行为类标。采用的方法是Convolution 3D+Gated Recurrent Unit(GRU)算法，结合Single Shot Detector（SSD）框架实现行为检测功能。 基于物体/人物的召回定位和识别视频中的特定目标，并在目标生命周期内进行跟踪。采用的方法是Region fully convolution network（R-FCN）的深度学习框架，对于小物体在feature map进行了优化；跟踪采用DCF框架，结合颜色模型，并使用BACF进行候选区域扩充。 视频智能封面图UGC视频智能缩略图，目的是通过对视频进行结构化分析，对关键帧、视频镜头进行筛选和排序，选择最优的关键帧、关键片段来作为视频的展示。采用的方法是用关键帧提取+MMR优化+美学评分等方法，选择视频中最优关键帧作为该视频的首图。 语义搜索—语义表征人工审核标注ground truth，训练集测试集比例为7：3，加入语义特征后，NDCG提升1%的绝对值。 固定数据尝试不同的模型 模型 NDCG 双向LSTM+Attention 达到0.9以上 BiGRU dropout 达到0.8以上 固定模型尝试term embedding初始化方式 模型 初始化方式 长尾query NDCG Bi-LSTM+Attention 随机 baseline Bi-LSTM+Attention embedding 低于baseline 1% Bi-LSTM+Attention UC/豆瓣的FastText Embedding 高于baseline 1% 用户体验优化： 在长尾query和语义层面实现了特征增益，在长尾query相关性上有较大改善 ground truth测试集NDCG提升了1% 技术创新突破 基于FastText无监督embedding 细粒度+字切分 优酷title语料覆盖度99% 训练数据集 billion级别，模型参数量千万级别 Bi-LSTM+Attention 基于pai-tensorflow的分布式训练 排序—个性化表征特征按照field进行组织，按照特征重要性和关联性进行分域，分为query field、user field、video field、id field、统计field、用户观看序列、标签兴趣、文本等，形成超高维的稀疏编码来表征独立个体，使得深度特征的组合表达能力达到极致，其模型结构如下： 但这样对在线计算形成了挑战，特征维度高，模型存储空间大，离线训练计算时间成本高，在线实现资源占用高，前向网络计算不能满足RT的要求。就用了一系列的技术，诸如随机编码、挂靠编码、抽样技术等。 总结 百度 孙宇 《神经网络语义匹配技术及其百度搜索实践》腾讯 苏函晶 《深度学习在广告投放中的应用》PayPal 张彭善 《构建高效的风控机器学习平台》新浪微博 刘博 《深度学习在微博信息流排序的应用》腾讯 王辉 《小Q机器人的诞生之路》爱因互动 吴金龙 《深度学习在对话机器人中的应用》阿里 黄丕培 《万物皆向量—双十一淘宝首页个性化推荐的秘密》商汤科技 陈宇恒 《未来都市—智慧城市与基于深度学习的机器视觉》百分点 黄伟 《AI认知技术帮助公共安全行业》]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>视频推荐</tag>
        <tag>视频搜索</tag>
        <tag>语义匹配</tag>
        <tag>广告投放</tag>
        <tag>风险控制</tag>
        <tag>信息流排序</tag>
        <tag>对话机器人</tag>
        <tag>机器视觉</tag>
        <tag>公共安全</tag>
        <tag>个性化推荐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人大应统部落（3）：2018年中国人民大学应用统计专业课真题与解析]]></title>
    <url>%2F2018%2F04%2F05%2F%E4%BA%BA%E5%A4%A7%E5%BA%94%E7%BB%9F%E9%83%A8%E8%90%BD%EF%BC%883%EF%BC%89%EF%BC%9A2018%E5%B9%B4%E4%BA%BA%E5%A4%A7%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E4%B8%93%E4%B8%9A%E8%AF%BE%E7%9C%9F%E9%A2%98%E5%8F%8A%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[扫一扫，关注【应统联盟】公众号，或添加微信 zhanghua63170140 2018年中国人民大学应用统计初试专业课真题及详细解答，文末附专业课复试笔试真题回忆版。 如有疑问或建议，可添加微信：zhanghua63170140 第一题【10分】1.1 请说明雷达图和箱线图的基本要点。1.2 下面的数据集为8个同学的数学、语文和英语的成绩，如何利用雷达图和箱线图来描述这个数据集？ 姓名 数学 语文 英语 甲 83 86 82 乙 93 89 93 丙 85 79 90 丁 79 81 75 戊 79 81 75 己 75 70 94 庚 69 62 94 辛 67 62 94 【解答】 1.1 箱线图由一组数据的最大值、最小值、中位数、上下四分位数这五个特征值绘制而成。不仅反映一组数据分布的特征，如分布是否对称、是否存在离群点等，还可以对多组数据的分布特征进行比较。 雷达图，也称为蜘蛛图，是展示多个变量的常用方法，主要用于比较研究多个样本的相似程度。 1.2 以数学组数据为例绘制箱线图，步骤如下： 画一只箱子，箱子两端分别位于上四分位数和下四分位数。四分位数是指一组数据排序后（67 69 75 79 79 83 85 93）处于$25\%$和$75\%$位置上的值，对于数学组数据来说,下四分位数$Q_1 = 69$以及上四分位数$Q_3 = 83$。这个箱子包括中间$50\%$的数据。 在箱子中位数的位置画一条垂直线，中位数是指一组数据排序后处于中间位置上的变量值。对于数学组数据来说，中位数$M_e=79$。 用四分位数全距$IQR = Q_3 − Q_1$，确定限制线的位置。箱线图的上、下限制线分别在比$Q_1$低$1.5×IQR$和比$Q_3$高$1.5×IQR$的位置上。对于数学组数据来说，$IQR = Q_3 − Q_1 = 83-69 = 14$。因此，限制线的位置在$69− 1.5×14 = 48$和$83 + 1.5×14 =104$处。两条限制线以外的数据可以认为是异常值。但一般限制线不绘制在图上。 绘制触须线。触须线从箱子两端开始绘制，直至最小值和最大值。因此，数学组数据的触须线分别在67和93处结束。 绘制异常值。即处于内限以外位置的点。 以此类推，绘制语文组和英语组的箱线图，得到的箱线图如下： 其中中位数位置代表平均水平；箱子长短代表离散程度；分布形状可以看出是否对称。既可以研究单独分析组内数据，也可以对语文、数学、英语三组进行比较研究。我们可以看到，八位同学的英语成绩平均水平最高，数学和语文成绩的离散程度较高，三组成绩都不存在异常值。 雷达图从一个点出发，找到八条射线分别代表八位同学。八个变量的数据点链接起来，围成一个区域，三门课程围成三个区域，研究语文、数学、英语三组成绩的相关比较。从图中可知，多位同学英语成绩的高于其他两门，数学和语文成绩都比较接近。也可以找三条射线，分别代表数学英语和语文三个变量，比较每个学生三门课的成绩。乙同学的三门成绩都比较高，辛同学的三门成绩都较低，需要加倍努力哦。 第二题【20分】2.1 说明在方差已知的条件下，正态总体均值区间估计的宽度与样本量的关系。 2.2 现在有一组来自正态总体的随机样本，可以由此得到在方差已知和方差未知两种条件下的置信区间，请分析这两个置信区间的中点和宽度的异同。 【解答】 2.1 在方差已知的条件下，正态总体均值区间估计的宽度随样本量增大而减小。 在方差已知的条件下，总体分布为正态分布的均值区间估计(无论是大样本还是小样本)为 \bar {x}±z_{\frac{a}{2}}\frac{\sigma}{\sqrt{n}}其中$\bar {x}-z_{\frac{a}{2}}\frac{\sigma}{\sqrt{n}}$称为置信下限，$\bar {x}+z_{\frac{a}{2}}\frac{\sigma}{\sqrt{n}} $称为置信上限;$a$是事先所确定的一个概率值，也被称为风险值，是总体均值不包括在置信区间的概率;$1-a$称为置信水平; 是标准正态分布右侧面积为$\frac{a}{2}$的 Z 值;$z_{\frac{a}{2}}\frac{\sigma}{\sqrt{n}}$是估计总体均值的估计误差。 总体均值的置信区间由两部分组成:点估计值和描述估计量精度的±值，这个±值称为估计误差。 由估计的区间可知，当样本量$n$增大时，区间估计的宽度减小。 2.2 如果是大样本的情况下，方差已知的公式为$\bar {x}±z_{\frac{a}{2}}\frac{\sigma}{\sqrt{n}}$ ，方差未知的公式为$\bar {x}±z_{\frac{a}{2}}\frac{s}{\sqrt{n}}$ 。两个置信区间的中点相同，但是方差未知的情况下是用样本方差替代总体方差，宽度存在差异。 如果在小样本的情况下，方差已知的公式为$\bar {x}±z_{\frac{a}{2}}\frac{\sigma}{\sqrt{n}}$，方差未知的公式为$\bar {x}±t_{\frac{a}{2}}\frac{\sigma}{\sqrt{n}}$ 。两个置信区间的中点相同，但是方差未知的置信区间的宽度会更宽一些。因为选择的t分布与正态分布形状接近，但是两侧的尾端会更宽一些，所构造的置信区间宽度也会更宽。随样本量增大，宽度差异会缩小。 第三题【20分】3.1 给出一个列联表，写出可以描述上述数据的所有的图形，并说明这些图形的用途，3.2 写出可以分析上述数据所有可能的方法，并说明用途。 3.1 对于分类型数据可以选择以下图形进行描述性统计分析 复式条形图：用宽度相同的条形来表述数据的多少，观察不同类型数据的分布状况。比条形图更便于比较分析 帕累托图：按各类别出现的频数多少排序后绘制的条形图，通过排序后，容易看出哪类数据出现的多，哪类出现的少 复式饼图：用圆形及圆内扇形的角度来表示数值大小的图形，用于表示各类别的频数占全部频数的比例，适用于研究结构性问题。比之饼图更适用于展示两个或者多个类别变量的构成比较。 环形图：显示多个样本各类别频数所占的相应比例，有利于构成的比较研究 3.2 因为以上列联表中有两个分类变量，我们关心这两者是否有关联，可以进行独立性检验，也即进行列联分析。独立性检验就是分析列联表中行变量与列变量是否相互独立，也就是检验性别分布与满意度之间是否存在依赖关系。具体分析步骤如下： 提出原假设和备择假设H0:性别与满意度之间是独立的H1：性别与满意度之间不独立 计算检验统计量计算$f_o-f)e$，其中$f_e=\frac{RT}{n}×\frac{CT}{n}×n=\frac{RT×CT}{n}$计算$(f_o-f_e)^2$计算$\frac{(f_o-f_e)^2}{f_e}$计算$\sum\frac{(f_o-f_e)^2}{f_e}$ 做出决策若$\chi ^2&gt;\chi_a^2$，则拒绝原假设，认为性别与满意度两者之间存在依赖关系。如果$\chi ^2&lt;\chi_a^2$，则不拒绝原假设，认为两者之间不存在依赖关系。 若拒绝原假设，可继续使用C相关系数或V相关系数测定两者之间的相关关系到底为多少。 通过上面的方法，可以判断两个分类变量是否独立，而当拒绝原假设后，我们可以接着运用对应分析方法来了解两个分类变量即分类变量各个状态之间的相关关系。对应分析利用降维的思想进行简化数据结构，同时对数据表中的行与列进行处理，可把样本点和变量点同时反映到相同的因子轴上，用低维图形简洁明了地揭示两个分类变量之间及分类变量各状态之间的相关关系。 第四题【20分】设因变量为$y$ ,自变量为$x_1,x_2,x_3·····x_k$，写出建立多元线性回归建模的基本思路。 【解答】 提出因变量和自变量：首先根据具体问题选择合适的因变量，然后选择合理的自变量和结合问题的实际意义和专业理论知识，运用逐步回归法等选择自变量。 收集整理数据：这是一个重要环节，他直接影响模型的质量。 做相关分析，构造多元线性理论回归模型：用SPSS软件计算增广相关阵。 用软件计算，输出计算结果（参数估计有最小二乘法和极大似然法等方法）：用SPSS软件对原始数据作回归分析。 回归诊断：（1）诊断基本假定是否成立（2）相关分析：由复相关系数或决定系数判断回归方程是否显著（3）方差分析表：判断回归方程是否显著（4）回归方程系数显著性检验是否显著（5）检验异常值：判断是否符合实际意义 若未通过回归诊断，返回第一步，否则可进入回归应用，主要应用于结构分析、预测和控制三个方面。 第五题【20分】5.1 方差分析有哪些基本假定？ 5.2 简要说明检验这些假定的方法。 【解答】 5.1 方差分析有三大假定 正态性:要求每个处理所对应的总体都应服从正态分布 方差齐性：要求各处理的总体方差必须想等 独立性：要求每个样本数据是来自不同处理的独立样本 5.2 正态性检验:检验正态性可以选择图示法去绘制因变量的正态概率图。包括PP图、qq图或者绘制箱线图、直方图、茎叶图。 其中绘制箱线图、直方图、茎叶图观察与正态曲线是否接近；QQ图是根据观测值的实际分位数与理论的分位数绘制的；PP图是根据观测数据的累积概率与理论分布的累积概率的符合程度绘制的；检验正态性还包括检验法包括Shapiro-Wilk和K-S正态性检验，其中Shapiro-Wilk适用于小样本。K-S既可以用于大样本也可以用于小样本 方差齐性检验检验：方差齐性的图示法包括箱线图和残差图。检验法包括Bartlett检验和Levene检验等 独立性检验：独立性可以在实验设计之前予以确定，不需要检验第六题【20分】在同一个概率空间中是否存在三个随机事件$A,B,C$使得同时成立下面三个不等式：P(A|B,C)≤P(A|\bar {B},C) \\\ P(A|B,\bar {C})P(A|\bar B)如果存在，请列举一个例子；若不存在，证明你的结论。 【解答】 存在。该题实际是考察辛普森悖论，大家可以自行了解一下。作如下假设： A事件：被录取B事件：是男生C事件：左撇子 B事件和C事件同时发生，即是男生，左撇子有1人 B事件发生，C事件不发生，即是男生，右撇子有4人 B事件不发生，C事件发生，即是女生，左撇子的有4人 B事件不发生，C事件不发生，即是女生，右撇子的有1人 P(A|BC)=0P(A|B\bar C)=\frac{3}{4}P(A|\bar BC)=\frac{1}{4}P(A|\bar B \bar C)=1 男生B 女生$\bar B$ 左撇子C 1(录取0人) 4(录取1人) 右撇子$\bar C$ 4（录取3人） 1(录取1人) 那么 P(A|B)=\frac{3}{5}P(A|\bar B)=\frac {2}{5} 所以 P(A|B)>P(A|\bar B)第七题【20分】设$x_1,x_2······,x_n$为一个来自均值为$\mu$，方差为$\sigma^2$的分布的样本，$\mu$和$\sigma ^2$未知，考虑均值为$\mu$的线性无偏估计类 L=\{T\left(X\right)：T\left(X\right)=\sum_{i=1}^n{x_i}.c_i \} \\\ 其中c_i是常数求出$L$中$T(X)$为$\mu$的无偏估计的充要条件，并求出无偏估计类中方差一致最小的估计。 【解答】 E[T(X)]=E(\sum_{i=1}^n x_ic_i)\\=\sum_{i=1}^nEx_ic_i=\sum_{i=1}^nc_i Ex_i\\=\mu \sum_{i=1}^nc_i=\mu所以$\sum_{i=1}^nc_i=1$ var[T(X)]=\sum_{i=1}^nc_i^2var(x_i)\\=\sigma ^2 \sum_{i=1}^nc_i^2≥\frac{\sigma^2}{n}当且仅当$c_i=\frac{1}{n}, \qquad i=1,2，····，n$时,等号成立，即$var[T(X)]=\frac{1}{n}\sigma^2$所以无偏估计类方差一致最小的估计为$T(X)=\frac{1}{n}\sum_{i=1}^n x_i$ 第八题【10分】设$X$是一个正值随机变量，方差有界，证明：对于$\forall 0&lt;\lambda &lt;1$, 有 P(X>\lambda EX)\geqslant\left(1-\lambda\right)^2\frac{\left(EX\right)^2}{EX^2}【解题】要证明$P(X&gt;\lambda EX)\quad \geqslant\quad { (1-\lambda ) }^{ 2 }\frac { { (EX) }^{ 2 } }{ E{ X }^{ 2 } } $ 即证明$P(X&gt;\lambda EX)·{ EX }^{ 2 }≥\quad { (1-\lambda ) }^{ 2 }·{ (EX) }^{ 2 }$ 不等式左边，根据示性函数$I(X-Y)=\begin{cases} 1\quad X-Y&gt;0 \\ 0\quad X-Y≤0 \end{cases}$和定义$P(X&gt;Y)=E[I(X&gt;Y)]$可得 P(X>\lambda EX)·EX^2 = E[I(X>\lambda EX)]·EX^2 =E[I^2(X>\lambda EX)]·EX^2 \qquad ①根据柯西不等式$(EXY)^2≤EX^2·EY^2$，得 ①≥{E[X·I(X>\lambda EX)]}^2 \qquad ②又因为$X·I(X&gt;\lambda EX)=\begin{cases} X·1\quad X&gt;\lambda EX\quad \\ X·0\quad X≤\lambda EX \end{cases}≥X-\lambda EX$所以 ②≥[E(X-\lambda EX)]^2=(EX-\lambda EX)^2=(1-\lambda)^2(EX)^2 \qquad ③综合①和③，得到 P\left\{X>\lambda EX\right\}\geqslant\left(1-\lambda\right)^2\frac{\left(EX\right)^2}{EX^2}第九题【10分】 设地区生产总之（亿元）为因变量，固定资产投资（亿元）、社会消费品零售总额（亿元）、出口总额（亿美元）、地方财政收入（亿元）、电力消费量（亿千瓦时）、居民消费水平（元）为自变量，根据31个样本数据得到回归结果如下： Coefficients Estimate Std. Error t value Pr(t) -2.377 e+03 1.166 e+03 -2.038 0.05270 固定资产投资 4.504 e-01 8.166 e-02 5.515 1.14 e-05 * 社会消费品零售总额 1.110 e+00 1.572 e-01 7.060 2.68 e-0.7 * 出口总额 1.887 e+01 6.379 e+00 2.958 0.00686 ** 地方财政收入 9.596 e-01 6.959 e-01 1.379 0.18061 电力消费量 6.683 e-01 5.671 e-01 1.178 0.25016 居民消费水平 1.194 e-01 6.949 e-02 1.718 0.09868 Residual standard error: 1526 自由度 24 Multiple R-Squared: 0.9944 Adjusted R-squared 0.993 F -statistic: 708.8 P-Value &lt; 2.2 e-16 对该回归模型进行综合分析，评价是否需要改进，并给出思路。【10分】 【解答】 写出模型,并对每个回归系数进行解释 通过了f检验认为因变量与自变量之间线性关系显著 t检验回归系数是否显著，拒绝原假设时，表明回归系数显著 调整的多重判定系数R2=0.993，表示在用样本量和模型中自变量的个数调整后，因变量y的总变差中被多个自变量多共同解释的比例为99.3% 通过回归系数的显著性检验和回归系数的正负号判定可能存在多重共线性。还可以计算容忍度$1-R^2$、方差扩大因子$VIF=\frac{1}{1-R^2}$和特征根进一步判断，容忍度越小，多重共线性越严重，容忍度小于1时，存在严重多重共线性，VIF越大，多重共线性越严重，VIF大于10时，存在严重多重共线性；或者对模型中各对自变量之间的相关系数进行显著性检验，通常认为大于0.8高度相关，0.5-0.8是较强的相关关系 处理多重共线性可以选择以下方法 a）增加样本容量b）自变量筛选：根据容忍度和方差因子进行筛选；向前选择；向后剔除；逐步回归 前进法： 思想是变量由少到多，每次增加一个，直到没有可以引入的变量为止 后退法：首先用全部m个变量建立一个回归方程，然后在这m个变量中选择一个最不重要的变量，将它从方程中剔除 逐步回归：思想是有进有出。做法是将变量一个一个引入，每引入一个自变量后，对已选入的变量要进行逐个检验，当原引入的变量优于后面变量的引入而变得不在显著时，要将其剔除 c）放弃无偏估计选择有偏回归（岭回归；主成分回归；偏最小二乘回归） 岭回归 主成分回归: 构造前M个主成分Z1,…,Zm,然后以这些主成分作为预测变量，用最小二乘拟合线性回归模型 偏最小二乘：将原始变量的线性组合Z1,…,Zm作为新的变量集，然后用这M个新变量拟合最小二乘模型 专业课复试笔试真题回忆版 给定30个数值型数据，请问如何检验其正态性？ 给出40个男女生考试分数，其中男生25人，女生15人，问男女生考试分数是否有显著差异，说出检验思路以及该方法的假设条件。 多元线性回归模型的基本假定。 从获取数据到得出分析结果的过程中会出现误差和干扰，举例说出几种误差以及解决办法。 有些统计量中分母会出现总体或样本的方差或标准差，从z检验、t检验和方差分析的角度思考，尽量用文字表述你对这种情况的理解]]></content>
      <categories>
        <category>人大应统部落</category>
      </categories>
      <tags>
        <tag>人大应统</tag>
        <tag>考研真题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人大应统部落（2）：2018年人大应统专业硕士经验贴合集]]></title>
    <url>%2F2018%2F04%2F04%2F%E4%BA%BA%E5%A4%A7%E5%BA%94%E7%BB%9F%E9%83%A8%E8%90%BD%EF%BC%882%EF%BC%89%EF%BC%9A2018%E7%BA%A7%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E5%A4%A7%E5%AD%A6%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E4%B8%93%E4%B8%9A%E7%A1%95%E5%A3%AB%E7%BB%8F%E9%AA%8C%E8%B4%B4%E5%90%88%E9%9B%86%2F</url>
    <content type="text"><![CDATA[扫一扫，关注【应统联盟】公众号，或添加微信 zhanghua63170140 2018年中国人民大学应用统计专业硕士经验贴合集 第一篇：2018人大应统排名第三经验贴 by 肖洁琳2018年考研终于落下帷幕，如愿被人大应统拟录取，写篇经验贴还愿。背景:一战人大应统，本科应用统计学，初试总分401，政治68，英语89，数学129，专业课115，排名第三。复试后综合排名第三。 一、复习阶段和时间安排2018年考研我准备的时间是是3月-12月，历时10个月。6-7月忙着期末和搬家，复习进度完全打乱了，真正的的复习时间是3-5月和8-12月。总体来说，整个复习跨度分为三个阶段:3-6月基础阶段，8-10月强化阶段，11-12月冲刺阶段。 3-6月: 考研前期准备阶段，我给这个阶段的定位是，消除对考研的恐惧感以及打好数学和英语的基础。主要任务是翻看经验帖，了解考研应该怎样规划(做好心理建设);数学三基础复习(听课+笔记+练习);英语一基础复习(单词+长难句)。这时学校还有课，没课的时候就去图书馆坐着学习，时间不固定，每天学习3-5小时，看看视频做做题，大多数时候有种能捡到分数的满足感。 8-10月: 正式进入严肃备考状态(7月回家了，基本没复习)，掰掰手指还有五个月，有了紧迫感，但还是蜜汁自信。这一阶段我认为以提升自己各科的硬实力为要。主要任务是数学三强化(听课+笔记+习题+真题);英语一练习阅读，巩固单词;专业课复习1~2遍；政治整体复习第一遍。具体时间安排如下: 时间 学科 备注 8:30-11:30 数学 听课，看书，做题，改题 11:30-12:30 午饭 12:30-14:00 趴桌子上午休 不午休的话，下午还是会睡着 14:00-16:00 英语 阅读题 16:00-17:00 政治 英语学累了，学学政治放松放松 17:00-18:00 晚餐 去操场上散散步 18:00-19:00 政治 19:00-22:00 专业课 参考书目不少，还是会有危机感~ 大概是这么个时间安排，但不会严丝合缝地按照这个来，数学学累了的时候用软件刷刷单词，有时候会扣扣手机看看视频~算下来一天正经学习时间7~10小时。会用forest计时，看到有效学习时间增长很有成就感。 11-12月： 一进入十一月，冲刺阶段开始，需要各科都形成清晰的脉络以供按图索骥;查漏补缺，学过的基础内容丢分岂不是很可惜;做冲刺练习，临阵磨枪，不快也光。主要任务是数学三查漏补缺，做冲刺卷;英语一解决完型，新题型，翻译，作文;政治准备第二轮客观题，准备主观题，准备时政，考前抱佛脚;专业课形成自己的知识脉络，操练解题技巧。具体时间安排: 时间 学科 备注 8:30-11:30 数学 刷题，改题，抡笔记 11:30-12:30 午饭 12:30-14:00 趴桌子上午休 午休前会看看整理的数学公式啥的 14:00-16:00 英语 16:00-17:00 政治 17:00-18:00 晚餐 去操场上散散步(改善心情，不过走太久会有负罪感) 18:00-19:00 政治 19:00-22:00 专业课 22:00-24:00 图书馆闭馆了，去教学楼背诵 每天学习9-12小时，背单词也是利用早上图书馆排队或其他科目厌倦了的时间空隙。 二、数学三我本科学的数分高代，理解能力不错，反应能力比较快，计算能力一般，数三129，事实证明我栽在计算上。 2.1 复习资料和时间安排 参考书 -《李永乐复习全书》(大红皮): 知识点很全面细致，初次做会有点困难，一定要耐心全部刷完，初次做不要直接看例题答案。刷了三遍。也有人用范培华李正元的粉皮全书，据说比较难，数学基础好的小伙伴可以用这本书。 李永乐《线代辅导讲义》: 五星推荐!搞透这本书，线代满分没啥问题。大概刷了两遍。《李永乐660题》:全是选择题和填空题，需要对知识点有深入的掌握和良好的计算能力，把高数部分完整地刷了一遍，线代和概统没写完。 张宇《高数18讲》: 有很多很有意思的题，重视解题思路，还蛮好玩的，比较适合思维灵活，基础好的同学。我只写了七八章的内容，配合张宇老师的视频课食用更佳。 2000-2017年数三真题: 好像是买啥书送的试卷，解析看的是李正元的历年真题解析汤家凤最后八套卷:计算量巨大张宇最后四套卷:挑战解题思路，计算量比汤家凤八套卷稍小 2.2 辅导课程汤家凤高数基础班，强化班，概统基础班视频张宇高数强化班视频:张宇和汤家凤都是很好的老师，各有侧重，汤家凤讲的内容更细致，注重系统性，适合基础少薄弱的或者思维习惯比较规矩的同学，张宇注重数学思维，适合思维比较跳跃的同学。李永乐线代基础班，强化班视频:虽然讲课有点沉闷，但是思路清晰，内容完整，深入浅出，李永乐老师不愧被称为线代王。 阶段 时间 书目 辅导课 基础 3.1-6.30 李永乐复习全书;本科数分，高代，概统课本 汤家凤高数基础班，李永乐线代基础班，张宇概统基础班视频 强化 7.24-9.30 李永乐《线代辅导讲义》，张宇《高数18讲》、《660题》、《全书》 张宇高数强化班(一半)，汤家凤高数强化班，李永乐线代强化班，汤家凤概统强化班视频(挑着看了看) 真题 10.1-11.05 2000-2017年数三真题 无 冲刺 11.05-12.22 汤家凤8套卷，张宇4套卷 无 2.3 复习方法 3-6月 花了两个月复习高数，线代和概统各一个月。我是一边听视频课一边自己用笔记本做笔记，每学完一章，把全书上对应的例题都做一遍，有疑问的题做上标记，再刷全书的时候重点关注标记的题。注意，这一阶段不要图快，一定要扎实。我当时要求自己笔记尽量写得美观整齐些，留出一定的空间以备日后补充修正，后来证实这对逐渐形成对每个知识点的全方位的理解以及构建各个模块的联系很有帮助。听视频课的时候，遇到记忆不准确的概念和难以理解的知识点时，暂停视频，参考数学教材和网上的资料，也是为了对知识点有尽量全面的理解。这一阶段下来，全书写完第一遍，写了一本自己的数学笔记，基础阶段就算完结了。(其实概统半个月就够了，但是我六月份要准备期末，所以没太沉得下心来，进度很慢) 8-9月 全书复习完了第一遍，进入攻坚克难期。还是听课，做笔记，写练习题，再加上把新的想法补充到基础阶段写的笔记上去，按照高数-线代-概统的顺序复习。练习，学习思路，总结，再练习。练习的材料是《660》，《张宇高数18讲》，《全书》(二刷)，《线代辅导讲义》。其中二刷《全书》和《线代辅导讲义》是我规定自己必须完成的任务(我是重基础型选手，厌恶风险，宁愿难题只能做出一部分也不能容忍基础知识出错)，后来由于时间来不及了，就只把《660》的高数部分选择填空和线代概统选择题写了一次，《高数18讲》写了七八讲的样子，没有强求。 10-11月 开始真题，两天一个周期，每个周期解决一套试卷，第一天模拟考试写真题并批改打分(时间和时长建议按照考研时间来)，第二天订正错题。养成写试卷的手感。(分数大部分在140+，也有130+或者120+) 11-12月 汤家凤八套卷，张宇四套卷。和真题一样的打法，模拟分数会比真题的分数低，正常(分数一般在110-130)。另外，趁着10-12月，三刷全书(屡次做错的题)，二刷《线代辅导讲义》，进一步完善基础阶段的笔记(让它尽量体现你对知识结构的把我，同时又有全面细致的知识点供查询)。 18年数三考得比较难，客观题主观题都不是善茬，思考量和计算量都不小。写完客观题，再写完第一个主观题我就有点慌了，还好我有旺盛的求生欲，坚持到最后一秒2333，结果是客观题没扣分，所有主观题我都知道出题意图是啥，该用啥方法。但是有一半主观只写了一半，就是算不出来或者算错最终结果，想来还是没多练难题和计算能力的锅。 三、英语一3.1 总体经验我英语基础还行，四级裸考630+，六级裸考530+这样。我一开始的目标是75，冲一下80，一直按照80的预期准备的，考完预期分数80-85，真实分数89，也许有一定运气成分。主要用的参考书是朱伟的《恋练有词》，张剑黄皮书(历年真题)，张剑的黄皮英语作文书(买书送的17年版)。个人认为考研英语最基础的是词汇，次基础的是语法和长难句理解，在此基础上掌握各种题型的套路就大功告成了。我从三月份开始复习单词和长难句。单词用的朱伟的《恋词》和配套视频，因为我光背单词表会感觉很枯燥，听视频课调节一下，大概三月到五月磨磨蹭蹭地把恋词的三十个视频听完了(最后十个串讲视频太长了，懒得看了)，个人觉得词根词缀对猜词还是很有好处的，不需要刻意去记忆，听课的时候跟着理解一下形成印象就行了，以后碰到眼熟但是不确定的词根词缀或者单词，反复查几次就行了。系统地学完《恋词》之后，可以用单词软件背单词，每天坚持背，不能停下来，坚持到考研。我用的是扇贝，有的阶段一天背500个，有时候300个，有时候200个，根据其他计划酌情调整，只要每天背，坚持到最后一天就行。长难句用的是何凯文的《长难句解密》，每天翻20页左右，三四月份看完的，学怎么断句，怎样整体理解句子。不必刻意去记东西。 3.2 各个题型经验 阅读: 应该算复习性价比最高的题型?(当然也是重难点题型了)分值高，套路性特别强，英语练习我都是直接用的真题，没有另外买模拟题或者专门的书，12月最后一段时间，直接拿最近五年的真题当模拟题练。大概到七月底才拿起真题练阅读。总共约80篇真题阅读，每天干掉两篇阅读(96-11年)，大概花了40-50天。建议先读文章做题，批改，尝试理解错题，再通读全文，查单词，看全文解析，看题目解析，理解出题和答题套路，做标记，如果实在觉得理解有问题，可以听听阅读专项的视频课(我听的是唐迟的阅读强化课大概04-08年的真题，基本上包括了考研英语阅读的所有套路)。十月到十一月中旬我把阅读刷了第二次(96-11年)，直接通读全文，看标记的单词和标记的文章解析，看题目和题目解析，做标记。十一月中旬到考前，整套刷最近五年真题，三刷96-11年的阅读(直接看标记部分)。第一遍刷阅读边看解析边思考边划记，耗时会久一些，大概一篇阅读45-60分钟，第二轮第三轮就会快很多，因为已经有了适合自己的重点生词和重点解析了。不建议全文翻译，做阅读重在逻辑关系和情感倾向。 新题型: 据说新题型很简单，但是我做新题型很烂，不知道啥原因，于是我听了王晟老师的专项强化课，再不行就只能看命了。新题型放在10-11月刷完阅读后复习(因为觉得阅读是大头，其他题型都是小菜)，先做真题，再批改，再听课学思路。但是18年英语一的新题型当时我做的时候，直觉给出的答案还是和用王晟课程中的思路给出的答案有出入，最后直觉赢了，所以语感还是很重要的。 翻译: 长难句的基础在这里很重要，翻译要求句子结构切分正确，内容完整正确。我看的是唐迟的专项强化班，每次练习完一篇翻译就看一节课，放在十一月中旬到十二月刷阅读之后练习。 作文: 分两块掌握吧，一块是论述结构，一块是表达。论述的结构就那么几种，基本都是三段式，看几篇范文就能领悟，表达就需要自己整理用得顺手的句型和搭配，同时词汇量也要过关。最重要的就是练习，心里有一个论述的结构，向里面填充具体的内容(论述或例子)。作文我从十一月开始准备，从十二月才开始练习(后来还是觉得有点虚，最后主观题只扣了9分，出乎意料)，看的张剑黄皮的作文书和真题的作文解析，积累其中用得顺手的表达。 完型: 我从12月才开始准备，当时想着来不及复习就放弃，完型应该是最考验英语功底的题型了，分值小，范围广，做真题是时候可以注意积累完型的常用搭配，完型还是可以做做模拟题的。 四、政治政治分为客观题和主观题。客观题是拉分的关键。68分，说不上经验，说说个人的复习用书和复习感想。 4.1 复习用书肖秀荣强化三件套(精讲精练，1000题，真题)，《任燕翔主观题背诵宝典》《肖八》《肖四》《肖秀荣命题人时事与政策》《任燕翔四套卷》 4.2 时间安排8-10月中旬，听政治网课，看精讲精练，做1000题。10月中旬-11月中旬看第二遍精讲精练，刷第二遍1000题(只做了一半)，11月中旬-考前，背《任燕翔主观题背诵宝典》(边看视频边背，其实没有完全背下来)，做模拟卷选择题，读肖八主观题，背肖四客观题。政治最后悔的就是10月中旬-11月中旬想二刷1000题，对知识点纠结得过细，浪费了很多时间，但效果一般，导致最后一段时间政治比较被动。客观题我觉得还是快刷，多刷为好，建议最后各个政治老师的模拟卷都买来做做客观题见见世面，模拟卷的客观题错题可以考前强记一波。主观题不要太恐慌，看看任燕翔的主观题直播视频，背好肖四，考试的时候抄抄材料，背背理论就不会崩。 五、432统计学今年突然出了50分计算证明题，大概写出来了20分，拿了115分，对准备的部分还算满意，没准备的数理统计部分也怨不得人了。 5.1 参考书目 《统计学》贾俊平(第六版)及配套辅导书 《统计学》贾俊平(经管版)(有第六版中没有的内容，如实验设计部分内容，时间序列部分模型) 圣才的《统计学》考研辅导书(蓝白皮的)(用来刷选择题熟悉概念)何晓群《应用回归分析》(第四版) 王燕《应用时间序列》(第四版) 何晓群《多元统计分析》(第四版) 5.2 关于真题真题很重要，真题直接反映专业课的考察套路:考哪些内容，考哪些方面，应该怎样作答。研究历年真题，会发现有很多知识点是反复考察的，那就是复习的重点。 5.3 关于笔记不得不说学长学姐的讲义还是挺重要的，可以提供一个基本的整理框架和思路，让我不至于跑偏，但是不能完全依赖学长学姐的笔记，自己整理的东西用起来是最顺手的。我是参考学长的笔记结构和历年真题，按照自己思路整理的总结笔记，补充上我的薄弱项。最后整理了主要的八个模块，回归分析全过程的思维图，时间序列的模型总结，多元统计各个方法的联系图。到最后一个月基本就直接背笔记了，很少翻书。 5.4 复习过程专业课复习从八月开始，我把上面的这几本教材通看了两遍，第一遍是把所有内容自己理解一次(感觉比较难的章节会做笔记加深一下印象)，第二遍注意整理知识结构和在此理解难以理解的部分，这两遍是为了给后续整理自己的笔记和记忆打基础。进入十月下旬，开始一边做真题，一边整理自己的笔记。《统计学》中，概念的辨析和联系是很重要的，如假设检验中第一类第二类错误的概念，p值的含义，假设检验的思想等等。而《回归》《时序》《多元》中，各种方法的适用范围和优缺点，还有分析思路是很重要的。比如回归分析的整个过程，就要从收集数据，数据清洗开始，到设定模型形式，到参数估计拟合模型，到模型评价，到模型改进和模型选择，再到预测控制，各个环节有各自对应的不同评估指标和处理方法，整个一套过程下来理清楚，就不容易出现记忆错乱的情况;时间序列中理清楚不同类型时间序列的不同处理方法以及具体模型是主线… 5.5 关于2018年专业课今年专业课一反常态，出了三个概统的题，50分，难倒一片。第一个是辛普森悖论，第二个是证估计量的性质，第三个是证概率不等式，用到示性函数和柯西不等式的小技巧。可能专硕对数统的要求会有所提高，建议以后大家复习书目加上茆诗松的《概率论与数理统计》… 最后，决定好了的事情就用心去做，祝愿大家得偿所愿。 第二篇：2018人大应统初试第六经验贴 by 肖健一一战人大应统，政治67，英语67，数学139，专业课119，初试成绩排名第六，专业课和数学总分排名第一，重点写一下数学三和432统计学专业课的学习经验，英语和政治则稍微简要一点。(以下我说的每一句话都是重中之重，知道同学们疲于观看冗长的经验贴，我已经尽量精简了) 一、432统计学专业课我们将准备考研的过程分为三个阶段，暑假前(3-6月)暑假(7-8月)暑假后(9-12月)我的专业课复习过程贯穿于全程，但是方法不同。9月份之前，我仅仅只是将贾俊平老师的统计学看了两遍，9月份之后开始陆续把回归，时序，多元三本书看完，此时脑中有一个初步的知识架构:专业课总共有八个专题，统计学独占前五个专题(数据图表展示以及度量，统计量与抽样分布，假设检验与参数估计，方差分析，列联分析)回归，时序，多元分别是第六七八专题。但是我对八个专题的细节还不清楚，这时候我开始自己拿一个新本子，整理自己的专业课笔记，六七八专题作为重点，一至五专题则作为次要，写完了一个本子。在这里我要特别感谢人大学长学姐为我辅导专业课，由于专业课考试范围过于宏大，你很需要过来人指出考试重点以及考场感受!到了11月中旬，我已经将八个专题的笔记整理完全，之后就开始了整个复习过程中最重要的一环:默写。可以说之前你所做的全是铺垫工作，构架了知识体系，了解了各专题中常见的问题，但是我们还是需要在考场上写出大概12面A4纸的内容，八个题目，每个题目题干一两句话，答案全需要你牢牢记在脑子里并且能完整默写出来!所以我的最后一个月全在不停的默写，对照笔记本，拿出白纸，直接默写出每个专题的常见问题答案，再对照自己的笔记本，做往年真题，默写出答案，默写默写再默写，再对照自己的笔记本，总之就是默写以及围绕自己笔记本!就这样笔记本已经翻烂，笔芯不知写过多少，八个专题已经牢牢印入脑海，各种问题可以说是倒背如流了。上了考场，感觉全是老朋友，只恨不得自己能多长手，全部一顿默写，那种熟悉感觉大大增加了我的信心，学弟学妹们都知道18年专业课有50分证明题，相当于诈和，但是由于信心爆棚，思维打开，我还是做出来了近30分，做完卷子还剩5分钟，大汗淋漓，直呼痛快! 二、数学三最有感触的一门，详情见知乎回答:如何评价2018年考研数学三?https://www.zhihu.com/question/264535482/answer/283393765 从七月份开始正式准备考研以来，数学3一直是优势科目，先是把全书做了一遍，然后15年数3真题，之后660题，然后15年数1真题，这时大概到11月份了，一时兴起又补上了数三从87年到04年的真题，之后到了十二月份是每天一套模拟题，张宇12套，超越135，合工大超越卷，我做题的过程有一个特点，就是从九月份开始，每天都是以模拟考的形式做数学，每天上午8.30到11.30雷打不动，全真模拟，就是那种不准拉的模拟=(虽然好像每次都没憋住… 就这样一直做到12.22，我记得大概是从11月开始，我的模拟成绩就稳定在真题145加，模拟题130左右的水平 上了考场，看到第一个题导数不存在就看出来这套卷子不简单，第二题卡住，看到二阶可导确实想到了泰勒，但是当时也没有再深入思考，再bd中徘徊，然后随手画了个图选了d，那个时候已经有点慌了，但是由于平时模拟考场次数很多，特别是最后的各种模拟题给了我很大历练，就是那种在考场上碰到不会做的题至少不慌的能力，所以强忍着看第三题，万幸看出来了那个中间量是1，所以也就成功做出来，从第三题开始，进入了我平时模拟的状态，一路绿灯到差分方程，其实这里我要多说一句，因为我考的是应用统计，所以我是知道二阶差分的，因为时间序列分析中有多阶差分消除趋势成分的内容，但也只限于我认识那个二阶符号，我当时第一个想到的就是这题超纲了，陈独秀啊…于是没敢耽误一分一秒赶紧跳，最后花了一小时做完选择填空，平时我模拟大概也是这样的时间，于是继续往下 往下是大题，当时我应该是已经进入了状态了，后面几道高数都比较顺利，提一下级数展开an那个题，当时不知道要怎么处理x的2n次方这一项，但是想想了应该是不能再化简了，于是就暴力合并，分奇偶数情况写上了答案，答案复杂到我都不敢写，但是做过的模拟题告诉我真的有正确答案是很复杂的，所以就这么过了那道题。 之后就到了数列递推这道题，这个时候我陈独秀同志真的有话说了，因为其实在考前，我在做往年数3真题的时候，已经发现了一个特别恐怖的规律，特别是到了1617年，我发现最近的数3真的有一种怎么偏怎么考的感觉，比如我在做17年真题的时候，当时有三个点很怪： 差分方程 级数那道大题，第一问就不说了，玄学证明收敛半径大于1。。。第二问在合并的时候，考到了级数下角标以及n要变的情况，也就是n=0还是n=1要变化的情况，而以前从没有出过，数1中有考到过，但数3没有 概率最后一道大题一反常规，不是直接考估计，而是先让你推出来概率密度，也是数3以前没有考到过的 因为我当时发现了17年已经有点不对了，于是就特别注意一个冷门的考点：数列递推，单调有界准则。这一类题目，数3从来没有考过，数1倒是有考过，所以我在做模拟题的时候碰到这种题就很注意了一下，知道一般证有界需要用数学归纳法，于是在考场上那道题也就顺利做出来。 之后到了线代，线代是真的不考常规，说好的正交变换是重点，反而考了配方法，我相信我们考生最熟悉的关于二次型的题就是让你常规求正交变换，但是万幸的是，我之前有一段时间专攻了二次型，这还要归功于模拟题，因为真题大部分都是让你直接正交变换的，而不会深入考你二次型，所以第一时间我就想到了要分a的情况讨论，来看是不是可逆线性变换，于是这题也过了。 之后三道题就是真的比较常规了，但是我在做的时候也有一些小插曲。。。在做线代第二道的时候，第一问我把a算错了，到第二问的时候发现A矩阵是可逆的，我心想AP=B求P直接左乘A逆不就行了吗！！太弱智了，但是在算A逆的时候突然发现A好像不可逆！！于是又返回去算A行列式，发现第一问的a算错了，于是开始大面积划掉答案，改的时候手都在抖，心想差点，差点就凉了，10分啊！！还好悬崖勒马！然后两道概率就中规中矩了。 这个时候还剩5分钟，之前我有说过我选择题第二题是蒙的，差分方程没有做，于是这时返回去看这两个题，这时猪操作来了，因为我考前总结了大概14-17的选择题答案，发现ABCD是2231的分布，也就是说会有三个是选一样，然后是两个一样的，两个一样的，最后剩一个，但是到底是ABCD哪一个选三个是不确定的，于是我当时自以为很聪明的开始看我的其他七个确定的题，发现已经有3个A，2个D，1个B，1个C，如果第二题我再选D，就会有3个D，3个A，正好第二题我又在BD中徘徊，于是我当机立断马上改为B，没跑了！还很美滋滋。。觉得自己很帅。。。再回来看差分方程那个题，已经犯迷糊了，因为看那个yx很不爽，潜意识里告诉自己这题超纲！再加上当时就两分钟了，所以就没有做。 加一条线，显示我终于把整个考试过程说清楚了，昨天上午对完了答案，发现选择错一个（那个自以为很秀的第二题）填空错一个（差分）之后的大题好像没有错。 做一个总结吧，2018的数3，总体来说是难的，而且是历史最难，没有之一，从1987年到2017年但是我想强调点是，如果你的考研数学复习，一直是限于做真题，全书，而且做很多遍的情况下，来做2018的题，是绝对不够的，因为它和往年的真题真的很不一样，基本上是一种“怎么偏我怎么考，考的常规算我输”的状态，但是如果你有做过大量的模拟题，有经历过在难题堆里挣扎的情形，在考场上至少不会慌张，不会心态爆炸，我觉得我就是这么一种情况，当时从第二题不会做开始，就拼命告诉自己，稳住稳住，什么难题没见过之类话，所以总的来说，我认为2018年题首先是难，然后那道差分方程确实是败笔，但是其他的题，都在考纲范围内，只是比较冷门的知识点，所以说没做出来只能说自己功夫不到家，我到现在都认为我那道选择题是自己没学好才错了的。 其实在考场上，只要不出现大的失误，类似于题目抄错，考生信息填错等原则性错误，其实都属于自己的正常发挥，没考好也是自己的某方面知识薄弱。 三、英语一如果你像我一样基础不太好，可以做精翻，也就是把往年阅读做完后再翻译出来，不要做模拟题，反复做真题，做他五六次，然后每天记单词，不要断，一直到考研那天，最重要的是，不要忽视新题型，18年就是英语一新题型诈和，我估摸着我应该是全错了，不然不会这么低。我英语花的时间仅次于数学，感觉还是没考好，不然可以冲一下400分，可惜。 四、政治考前诈和，周末考试，我周一到周四都在发烧，所以完全没有背分析题，还好上了考场发现答案全在题里，相信这也是政治考试以后的发展趋势吧。不急着复习政治，结合我身边的同学复习情况来看:我女朋友五道口第17名，考前可以说是把所有类型模拟题都做了，政治投入极大，最后也没有上70，一个朋友是今年北大非法学法硕初试第一，他反而确有70多分，我室友连肖4肖8都没有买，也有66??所以我认为考研政治完全是一门玄学，你的复习时间和分数关系不大，分数高低全看天，选择题一定要多做，所有人各种版本的模拟题都可以做一做，反正也就35个题，二十分钟搞定一套，这样虽然不能保证你上70，但65肯定是有的，上不上70看天吧。 最后总结一下，既然决定了考研就要一心一意，从我身边的同学来看，考研很简单，你认真了，就能上，没有认真，就一定上不了。暑假之前，暑假，暑假之后，这三个阶段中学习强度可以逐渐加强，循序渐进，但这些都是次要的，最重要的是心诚则灵，送两个字给你们:专注。你的心要属于考研，一心向学，必然成功。最后愿所有真正努力过的同学都能取得理想的成绩！ 第三篇：2018 年人大应用统计考研经验贴 by 曾诚在 2018 年的考研中，我的分数不算太高，但也算是我自己比较没有遗憾的 一个成绩吧——375 分，也算是给两年的艰辛的考研旅程画上了一个圆满的句号。 各科分数为政治 64，英语 65，数学三 135，统计学 111。之所以写下此经验贴， 一是对自己两年考研历程的一个总结，二也是希望能够给今后考人大的师弟师妹 们提供一些帮助吧。 一、我的大致情况本人就读于天津一个非 985、211 院校，读的是统计专业精算方向。对于专 业的选择，其实没有考虑太多，由于没有读博的打算，所以直接就选择了专硕。 整个大学期间读书还算认真，但碍于非 985、211 院校，保研资格实在是太少太 少，而且就算能够保研，可能所就读的院校也不是特别特别的拔尖(除非从很早 很早就开始准备，刚进校那会就听说有个学姐保送了北大应统，据说从大一就开 始准备，发表过很多论文作品，而且大学期间就拿下了中国准精算师证书，膜拜 膜拜)，所以当时就了解了一下应统比较好的院校，这里说一下我大概了解的情 况，因为本科是在天津，所以更多的还是愿意选择北京的学校，一来北京离天津 很近，二来北京作为三大一线城市之一，就业机会远大于其他城市，其实读本科 的时候就有些后悔，应该选择一线城市的学校去读，且不谈学校如何，城市给你 的感觉截然不同。就拿天津和北京为例，天津的氛围相较于北京，实在是太过于 安逸，只能说天津是个适合安家的城市，但北京的环境所给你带来的无论是眼界、 机遇绝对是天津所给予不了的。所以当时就把目标定在了人大和北大，当然南方 也有许多不错的院校，例如说厦大，听大学一个关系不错的老师说华东师范的精 算很不错，想考精算的童鞋可以了解一下。当然这里我就重点从客观的角度说一 下人大和北大，不用说从院校实力来比较呢，自然是北大要强啦，考的难度自然 也是北大难度大些。从初试分数线来看，北大和人大的初试分数线分别是 380 和 360。从专业课来看，北大偏数理统计，而人大比较偏应用，这也是统计专业 课为数不多的专业课相对来说比较简单的院校了，但可能也是这个原因，从今年 开始人大专业课也来了个 360 度大转弯，把我们打得措手不及，估计以后就不会 那么容易了哦。从招生规模来看，都有小幅度扩招，但人大的招生人数还是远远大于北大，近年人大统招 45 人，而北大只有 17 人，招的人少，这就说明你要和 更多数学大神，专业课大神去拼，不能混入低分飘过的队伍中去了哦。虽然现在 人大的拟录取名单还没出，但从初试名单上的分数来看，低分飘过的小伙伴还是 不占少数，而北大最低的初试分数也达到了 385。从复试来看，北大的复试分差 还是相当恐怖的，最高的有 96 分的大神，在这里膜拜一下，低的才 65 分，初试 分数高的也不一定稳，这点人大还是非常占优势的，人大复试比例只占 30%，相 比其他院校还是低了很多。总之，清华北大这个大神级的学校，要考还是要多三 思。 再说说我的工作经历吧，在数学难度大增还有专业课改革的大形势下，相信 今年考研的许多小伙伴们都碰了灰。去年的这个时候我也面临着和你们一样的困 境，实习?工作?二战?这似乎是考研失败的小伙伴们避不开的选择。这里直接 给出几条建议吧。要是在纠结工作还是二战的朋友，我真的劝你好好想一想你是 否真的值得再花一年宝贵的时间来学习?今年成绩不理想的原因在哪?再给你 一年时间你在哪科能有巨大的提升?不妨再问一下自己，这个研究生是否值得读， 是否比两年或者三年的工作经历来的更重要?重新审视之后再做出决定，可以先 去认认真真地实习两个月，体会一下工作是一个什么样的状态，准备二战的也可 以实习，毕竟二战暑假再开始准备也是完全来得及的。再就是纠结全职二战还是 专心二战，本人是全职二战，不得不说当时的考虑就是想着自己毕业了，也不能 老靠着家里，就找了一个相对轻松的工作，一个月能赚足自己的生活费。但我必 须要提醒大家的是，要全职二战可是会非常的辛苦了，即使是一份轻松的工作， 你一天的学习时间也绝对不会超过 8 个小时，这可是比第一年的学习时间直接砍 掉了将近一半，而且有时候下班累了，就真的已经不想在学习了。所以我对二战 的小伙伴们的建议就是，要么就找个好点的实习，实习几个月然后专心考研;要 么就找个相对轻松的工作，但是后者会非常辛苦，建议慎重考虑。 二、各科复习进度与复习方法我主要还是讲一下我的备考经历，按时间顺序，里面会加上我的复习心得。 2.1 数学三 复习用书 复习全书、概率论与数理统计计提方法与技巧、微积分解题与技巧、线性代数辅导讲义、数学基础过关 660 题、历年真题、模拟题(李永乐)、 合工大三套卷 复习进度 由于第二年考研时间比较短，复习的时间规划不太详细，以第一年时间规划为例。 第一阶段:7 月-9 月(基础阶段) 每天 3-4 小时 建议使用李永乐复习全书、概率论与数理统计计提方法与技巧、微积分解题与技巧、线性代数辅导讲义。第一本比较系统，但是有些许定理会直接摆在上面， 并没有给予证明，可能第一遍会看不太懂，不过这也正常，可以去翻看本科的数 学课本，第一遍看的时候尽量把知识点都弄得详尽一些，最近几年对于细节的考 点越来越多，以前看似不会考的考点，比如说数列收敛性的证明，差分方程，都 成为近两年数三试卷上的宠儿(差分方程的题目连错两年，不是难，平时这类题 目练得少，公式总记不住)。后三本是分别针对数三的微积分、线代和概率论三 部分的书。其中，微积分解题技巧这本书比较难，当时也是看了 14 年考上北大 的学长经验贴买的，内容要比全书上多很多，要看不懂可以酌情跳过，但是最近 数三的微积分部分越考越难，我觉得有必要推荐一下，上面有很多题目的解题方 法。线性代数的讲义写的很有条理，思路很清晰。概率论的这本书真的是相当推 荐，有很多方法相当实用，看完这本书，基本上概率论这部分就没什么问题了。 这个阶段大家尽量将每个知识点都理解透彻，不要放过每一个细节，最好自 己列一个知识点框架，把一些重要的公式记下来，下次再记不住，就用记号笔重 点画出。刷题的时候，不会的题和猜对但不知道原理的题都要用记号标出，不要 糊弄，最后都会直接体现在你的考研分数中，这部分时间长，一定要把知识点学 扎实。 第二阶段:10 月上旬(提升阶段) 每天 3 小时 建议基础过关 660 题，这本书 20 天左右绝对刷的完。我第一年选择的是张宇 1000 题，第二年选的 660，这两本书我觉得 660 更贴近真题一些，张宇 1000 题偏难，对答案的时候非常扎心。660 只有选择和填空两种题型，1000 题所有题 型都有，但讲真，有些大题还真是有点难，大家根据自身情况进行选择。 第三阶段: 10 月下旬-12 月底(冲刺阶段) 每天 3 小时 建议历年真题、模拟题(李永乐)、合工大三套卷，很多一战的人都喜欢先 刷历年真题，再刷模拟题，没错我一战也是这样，我这个人喜欢总结套路，由于 真题简单，我就开始总结哪些题会考哪些题不会考，结果 2017 考研的时候我就 傻眼了，以为不会考的差分和数列出现在试卷上。所以大家一定要吸取我的教训， 千万不要以为某个知识点是一定不会考的，近几年的数学题是一年比一年刁钻。 我刷完真题之后发现我好像只会那么真题的那么几类题型了，做模拟题的时候好 多题型完全想不起来怎么做。所以第二年我先从模拟题刷起，把你的水平先拔高， 再去对待简单的真题。强烈推荐合工大三套卷，感觉它的出题风格非常像今年的 数学真题了，有点难，但还挺锻炼你的临场发挥能力的。每套卷子一定要掐着时 间做。我当时还专门买了考研专用的答题卡来模拟训练，因为我这个人还有个毛 病，一看见答题卡就紧张，当然我觉得这也是有必要的，因为毕竟将答案誊写到 答题卡上还要一定的时间。 12 月份我就没有再练习新的题了，这个时候在数学上的时间可以缩减到 2 个甚至 1 个小时，因为这段时间对你数学的提分作用不大，应该多放时间在其他 三门记忆性的学科上。这段时间可以看看你的错题，总结总结自己在哪方面还比 较薄弱、公式重点记忆、查缺补漏，千万不要在做题了，只会越做越紧张! 2.2 英语一复习用书:扇贝 app，历年真题，王江涛高分写作 这门学科我就不说太多了，17 年考了 76 分，18 年才考了 65 分，分析一下 分数降低的原因吧，也给不了太多的建议。 今年分数下来看见自己的英语成绩，是不能接受的。真的未免太低了，不过 后来想想也就该得这个分。近年可以说真的是一个单词都没背，这能充分说明背 单词的重要性了。学了很多阅读的技巧什么的，但终归抵不过单词不认识，句子 看不懂。所以说不要妄想走捷径，根基不稳，终究是不行滴! 英语的重心还是放在阅读上吧，阅读占的分值实在太大。历年真题即可，无 需其他的辅导书。05 年之前的真题和现在出题风格不太一样，时间不多的话就 可以不用刷了。10 年后英语开始变难。一篇一篇阅读认真刷，最好弄懂每篇文 章的每个单词和句子，对后面的翻译也是有好处的。 再就是写作，这部分真的最好要提前准备，千万不要等到 12 月份才开始准备。小作文非常有套路，我第二年因为时间不够，就把 10 年到 17 年所有小作文 考的题型列出来，发现常考的就那么 5、6 类，而且邀请信出现次数最少，结果 还真就考了，哈哈哈。大作文也是有套句的。 作文一定要背句子，不要整篇整篇的背，因为有些句子是可以通用的，文章 背完了可能只适合那一类题材，而且容易被判卷老师看出来是背的模板。平时也 要有意识的训练自己写句子的能力，毕竟不能所有的句子都写套句。 其他三部分真的是听天由命了，考的非常惨淡，就不给出建议了。 2.3 政治复习用书:肖秀荣命题人知识点精讲，肖秀荣 1000 题，肖 8，肖 4 这种学科要考好真的是完全靠天赋了，两年都是 60 几分，总之，告诫各位 前面时间真的别用太长，9 月开始准备足矣。最后一个月肖 8 肖 4，尤其是肖 4 努力背，使劲背，背着全是你的，哈哈，2017 年政治大题全压中了，可怕。其 他的不多说了~ 2.4 432 统计学复习用书:统计学第六版、统计学第四版、应用时间序列分析第四版、多元 统计分析第四版、应用回归分析第四版，今年考的证明据说是茆诗松的概率论与 数理统计的书上的，具体也不是很清楚，最好好好了解一番，不过这本书是很 多学校统计考研都会考的书，偏理论证明。 今年考研专业课画风突变，今年之前从来没有考过定理的证明，但今年考了 三道大题，一道简单，另外两道比较难，感觉有一道跟切比雪夫不等式比较像， 但怎么也证不出来，另外一道连题目都看不懂。而且今年不知道为什么初试包括 复试都只考了统计学，不知道明年还是不是这样。不管考不考，其他三本书该复 习还是得复习，再加上一本偏理论的数理统计教程。 建议大家先把历年真题都过一遍，知道大概每道题都考什么知识点，因为真 的不是所有书上的内容都会考到的，这样做非常省时间，不然毫无目的看完一遍 之后什么也记不下来。人大的专业课考研是分模块的，考试一般考 8-9 个题，全 是大题，一般分八个模块，都是按统计方法分的，每个模块一到两道题，你可以 把每个模块的题都放一起看，看看都考什么知识点。比如说第一个模块是统计图 表，知识点就是数据的分类、每类数据适用的图表以及特点。通过这些问题去找答案，看书效率事半功倍。我第二年书都没看，直接看的笔记，仅仅只花了 2 个月的时间复习专业课，最后考了 111 分，还算可以吧。不过证明题这块个人真 的不知道怎么复习，以前也没考过，这里就不给出建议了。 另外强调一下，大家在做历年真题的时候一定要自己亲自动手做，我第一年没动手做，结果到考场上专业术语全都想不起来，写的语言特别大白话，最后分 也不是很理想。不要嫌麻烦，真的，想的和写出来的不太一样的，写一遍，再对 照书本或是学长学姐的资料或笔记重新改一遍，重点记忆的部分用记号笔做标记。 另外，一个小细节，关于字母的书写的，凡是涉及字母的题目都要在开始标注清 楚每个字母都代表什么，保证做题的严谨。其他的有关于每本书的复习思路相信 在前几届学长学姐的经验贴中已经说的非常详细了，此处不再赘述。 三、结语考研除了复习方法之外，保持身体强健与心态平和也尤为重要。要知道复习 时间紧迫，只要一病就有可能耽误好几天，所以一定要保持自己身体的健康，每 天给自己半个小时或一个小时的休息锻炼时间，劳逸结合。心态平和更是重要， 第一年由于自己的心态问题导致数学相当惨烈，不仅紧张，还没休息好，考数学 前差点睡着，考试大脑一片空白，想想都可怕。考试前一定就不要熬夜了，也不 要花太多时间在复习上了，保证良好的休息时间才能有清醒的头脑对待考试。数学和专业课真的相当重要了，特别特别拉分，建议多花点时间。以上就是全部关于两年的考研历程的总结，希望大家能够多多吸取我的教训 吧，都是血和泪的教训啊，希望能对考人大的学弟学妹们有所帮助。也祝愿大家 能在 2019 年的考研中取得一个自己满意的成绩! 第四篇：2018人大应统考研经验帖 by 王雪楠大家好，我是 18 考研中国人民大学应用统计专业的小王，初试分数为政治 73、英语一 78、 数学三 124、专业课 103、总分 378 分，初试排名 15/54，复试成绩未出，成绩不算很高， 备考经验仅供参考。(注:18 考研指的是 2018 年研究生入学那一拨人的考研，实际是在 2017 年末考的) 一、 背景介绍 学业背景本人为应届生，本科读统计学专业;六级裸考520+(听力和写作较弱)，高中英语语法尚可(后面会提到)。 目标院校考研情况 人大应用统计专业最近几年的分数水涨船高，报的人多，收的人多，分数线也很高; 去年校线 390，今年校线 360，30 分主要差在专业课上，18 考研人大应统专业课难度突然 提升，导致分数线下降，个人认为分数线的下降与数学今年较难无关; 专业课按照以往的经验全部是论述题，但 18 考研突然出了 50 分的证明题，确实有些措手不 及，其中有 30 分(我猜)大部分人都不太会，所以分数线明显下降，至于明年会不会再考 证明不好说。 二、 备考经验2.1 第一阶段大三下学期即 2017.03-2017.07 大三下我还有课要上，而且课还不少，所以没有具体规划每天的时间，有大块时间就会去图 书馆，计划也是定的至少一个月以内的计划，并没有单日或单周计划。 数学三 复习资料: 李永乐复习全书(红皮)及配套分阶习题、李永乐线性代数辅导讲义、张宇高数 基础班视频 复习进度:因为我有课，所以目标是期末(2017.07)之前刷过一遍全书和分阶习题即可， 实际进度也是如此。 复习安排: 按照高数、线性代数、概率论的顺序复习。 高数部分: 按照章节看张宇高数基础班视频并做笔记。张宇老师的课讲的非常精彩，前期复习比较迷 茫的话可以一听，个人认为听了该视频做过全书的话那么可以不必再看十八讲。 每看完一章视频，去做一章全书加一章配套的分阶习题。 在全书上，会且做对的题目标1，会但做错/不太会瞎蒙对的标2，不会的题目标3，其他 部分也同样如此，方便后续复习。 线代部分: 直接做了线性代数辅导讲义。个人认为做了线代辅导讲义那么全书的线代部分和分阶习题的 线代部分可不做。 概率论:刷全书+分阶习题。 英语一 简单看了一下考研英语的语法视频，发现讲课内容主要集中在从句部分。而从句我的高中英 语老师讲得很好，我掌握得还可以，所以没有单独学习考研英语语法部分。如果同学觉得自 己从句部分掌握得不是很好，那么可以专攻一下英语的各种从句，不必听全套英语语法视 频浪费时间。 复习资料 扇贝 app 复习目标:没有具体目标，只希望自己能坚持每天打卡背单词，没有开始做英语真题。 复习安排:每天在扇贝上背 200-400 个单词，一个学期过去应该是背过了考研单词一轮。 其他:我没有使用单词书(如新东方大绿皮)和看恋练有词视频的原因: 我一开始用大绿皮背单词，每天两个新的 list，还要复习。用单词书时会看的非常细致，每 一个意思都希望自己能够记住，先背再检验是否背了下来有时可能会用一整个上午的时间， 我觉得有点浪费时间，所以最后不看单词书选择用 APP 背单词; 恋练有词里面朱伟废话连篇，虽然可以快进但是体验不太好;另一个比较主要的原因是我看 过视频后不会立刻去背单词和他讲的词根词缀等东西，过了好几天再翻发现已经忘了，最后 索性就不看视频了。如果有同学能够坚持看下来的话应该效果也不错。 432 统计学 复习资料 人大出版的 统计学第四版、统计学第六版及配套学习指导书、应用回归分析、 时间序列分析、多元统计分析 复习目标 期末前看完上面所有书第一遍，不做笔记只是读书。 复习安排:有时间就看看，没有单日计划，能在暑假开始前看完即可。 政治 没看。 2.2 第二阶段暑假即 2017.07-2017.08在家，并没有每天学习，进度不快。 数学三 复习资料 李永乐复习全书(红皮)、李永乐 660 题、李永乐线性代数辅导讲义、第一轮自己做的笔记 复习进度 暑假内二刷完全书，做完了 660 题 复习安排 按照章节看全书和自己的笔记。看全书时，第一遍标1的题目不再看，只看标2和3的题 目，从前不会做的如今会做且做对那么将标号改为1，以此类推。 每看过一章全书和笔记做一章 660 题，我当时的目标是每天 50 道。 英语一 复习资料 张剑黄皮书三本，扇贝 APP 复习目标 继续每天背单词，阅读随缘 复习安排: 每天早上在扇贝上背 700 个单词再起床，大约用时 1h; 从张剑黄皮书最早的年份开始，只做阅读题。当时是(尽量)一天精读一篇，全篇落实笔头 翻译，进度非常慢。开学前没有做完第一本阅读。 其他: 为什么不直接做第二本第三本(题型更贴合的年份)? 学习方法需要慢慢摸索，用第一本书来大胆试错，省得把第二本和第三本浪费了。当时我全 篇翻译很费时间，所以进度很慢，后来只翻译阅读中的长难句，再后来只在心里默默翻译并 在卷子上标明句子结构，慢慢才找到适合我自己的方法。全篇翻译有必要吗?个人认为没有必要。 432 统计学 资料同上，依然是每天默默看书，希望大家能够在暑假时把专业课笔记自己整理完。 政治 没看。 2.3 第三阶段2017.09-2017.12 数学三 2017.09-2017.10 复习资料 李永乐复习全书(红皮)、李永乐 660 题、李永乐线性代数辅导讲义、第一轮自己做的笔记、李永乐数学三历年真题 复习进度:三刷全书(包括笔记和线代讲义)，看完了 660 的错题，做完了 97-17 的真题。 复习安排: 大约花了一个月的时间三刷全书和从前的错题，个人认为这段时间有点浪费，错题不必太纠 结; 三刷全书以后大约以每天一套数学三真题的速度往下刷，每做五套休息一到两天，整理真题 错题，易错的知识点及时复习。个人认为真题没有必要刷两三遍，因为比较简单。 2017.11-2017.12 复习资料: 李永乐 6+2 模拟题，张宇八套卷，张宇四套卷，合工大模拟题， 复习安排 每一套模拟题都定好闹钟在三小时内完成，做一套或两套休息一天来整理，及时回顾易错知 识点直到考前。模拟题没有二刷。 (即使是现在我也可以说数学是我复习的最好用时最长的一门，没想到考试的时候考砸分数 不高，对我来说特别遗憾。但我觉得复习的过程还是可以借鉴的，因为当时真的数学复习得 最用心 T^T) 英语一 阅读、完型、新题型、翻译部分: 复习资料: 张剑黄皮书，扇贝 APP 复习安排: 单词数自己决定，不要低于 200 也不要超过 600; 第一轮:每天精读两篇阅读，不包括完型新题型，全做过一遍后进入第二轮; 第二轮:每天精读两篇阅读，不包括张剑第一本，不包括完型新题型，全做过一遍后进入第 三轮; 第三轮:每天精读四篇阅读(即一年的)，不包括张剑第一本，每天一篇完型/新题型/翻译。 如果还有时间那么继续读真题阅读和完型新题型，没有时间三轮足矣。 作文部分: 12 月份开始每天背王江涛大小作文各一篇;12 月中旬看何凯文的作文模板视频并背诵。 (英语一我完型错了一个，阅读没错，新题型五个空错了四个，最后 78 分。个人认为阅读 的复习经验可以借鉴，新题型我也很无奈，作文和翻译(扣了 13.5 分)在北京的极旱区给 分我个人觉得还算可以，但是作文准备的并不是很充分，我考前一篇作文都没写过，不要学 我。) 432 统计学 整理完笔记就是背诵，考前能够背过五轮就可以了。至于证明题由于我当时没有复习所以我 没办法给出比较好的实践过的建议。 除去背诵记得看真题。真题重要，真题参考答案也很重要。我联系了人很好的章华师兄，买了真题的参考答案，给自己做题方面指引了一些方向。 总之专业课就是一直背诵直到考前。 政治 复习资料 肖秀荣精讲精练、肖秀荣一千题、肖秀荣时事政治小册子、肖秀荣八套卷、肖秀 荣四套卷、腿姐复习宝典 复习安排 2017.09-2017.11看精讲精练做一千题，反正看完了做完了啥也没记住。2017.11-2017.12花了两天时间集中看了肖秀荣时事政治小册子，把时事政治部分搞定; 问答题背诵肖秀荣八套卷四套卷的问答题即可，18 考研肖秀荣全部命中; 选择题，看腿姐的复习宝典，有时间搭配视频来看，选择题效果拔群。 其他: 我为什么不看风中劲草: 风中劲草知识点太全了，把所有知识点都拿出来=什么都没说，背不下来浪费时间，不如多 看几遍腿姐的复习宝典(12 月份会出)(徐涛老师的也可以，总之选择题我认为不要靠风中 劲草或者一千题或者八套卷四套卷来提高，看各位老师总结的重点知识点比较有用)。我 个人的复习经验大致如此，不一定适合每个人。成绩不高写这个经验帖感觉比较羞耻，希 望大家能够多看经验帖找到适合自己的学习方法而不是照搬他人。本人自制力较差，即使是 考前一周也会因为心情不好而在宿舍躺尸一天，相信大家严格要求自己肯定可以取得不错的 成绩。 第五篇：18 年人大应用统计经验贴 by 冯亚宁我本科就读于武汉某高校经济统计学专业，说实话，当初经历了专业调剂， 转专业的风波，最后也是阴差阳错选择了统计学。起初真的没抱太多的期望， 随着接触了越来越多的统计学知识，我渐渐的迷上了统计，这大概就是所谓的 “日久生情”吧。但是，随着毕业的尾声接近，我越来越觉得自己掌握的知识 有限，因此我决定继续进行统计方面的学习。在选择学校方面，我丝毫没有犹 豫的选择了人大，一是源自我高考的执念，二是人大统计真的非常让人向往。 讲了这么多，介绍一下我的基本情况，我是二战，两年都是应用统计学专业， 去年总分 388，今年 362，刚刚过线。答应了师兄要写经验贴，想想了一想，经验谈不上，分享一下我的一点心得吧，希望对师弟师妹们有所帮助。 一、心态问题首先，我觉得一定不能偏科，要合理分配自己的时间。即使某一科学习不 太好，也不要让它成为你明显的短板。因为即使你有一科特别出彩，也很容易 被差的一科拉下来。我 17 年，政治 74，英语 62，数学 122，专业课 130，很明 显英语上就相对要低得多。我的经历就给大家作为一个反面教材，提个醒，希 望大家一定要各科均衡发力。其次，数学和专业课是大头。总看这几年的录取 名单，我们可以明显的发现数学和专业课好的同学成绩都比较靠前。希望大家 能够注重数学和专业课的学习。最后，一定要坚持。既然选择了考研，就一定 要记住自己的初心，坚持下去。期间很多同学保研、出国、工作，但是请一定 不要受他们影响。既然选择了远方，那便只顾风雨兼程。 二、时间安排我是 8 月份在人大附近租房准备的，如果有二战的小伙伴，有住宿或者相 关的问题可以问我，具体是作息时间安排，我借鉴一位师兄(花旗先生)的时 间表。 这只是我个人的时间安排，仅提供参考，希望对大家有所帮助。 三、复习进度安排因为是二战，前期复习的比较快，后期主要是整理背记，查漏补缺。这些仅是个人的一点规划，虽然没有严格执行，但是基本上相差不大，学 弟学妹可以借鉴一下，每个人的学习能力都不尽相同，还是要结合自己的实际 情况，合理的安排计划。 四、参考书目我用的书基本上跟学长的经验是一致的，详细的我就不介绍了，仅提供一 个书单，给大家参考一下。 数学: 《数学全书》 、《660 题》 《真题》、张宇《8 套卷》 、《1000 题》等; 英语: 张剑《英语一》全套，何凯文作文，《红宝书》等; 专业课: 《统计学》第六版、经管类《统计学》，《应用回归分析》，《多元统计分析》，《时间序列分析》等。注:今年专业课出现了证明题，大家可能要多看一本《概率与数理统计》， 关注一下证明题。 五、感悟、感谢考研的路上受到了很多人的关心和帮助，有家人的支持和关爱，有一个会 拿着糖葫芦坐十几站公交车来看“探视”我的闺蜜，还有学长的帮助，研友舍 友的陪伴，深深地情谊和感谢不是用言语可以表达的，只希望未来，大家都可 以心想事成，幸福安康!也希望我的经验可以为大家带来一点帮助，祝愿即将 考研的小伙伴们心想事成，金榜题名! 第六篇：2018人大应用统计考研经验分享——by wtt 第七篇：2018 年人大应用统计考研经验贴 by GGG大家好!我本人是二战的考生，本科就读于中央财经大学应用统计专业，这 次初试成绩是 381 分，政治 67，英语 76，数学 125，统计学 113。关于时间安排 的问题，其实我也看过许多学长学姐的经验贴，写得很详细。不过我认为只要把 握好大体的复习阶段和节奏就好，具体的要看个人习惯，自己尝试几天，发现最 适合的时间段，分配好各科的复习，毕竟计划赶不上变化，要随机应变。这个一 会说每一科的时候再单独说。 在开始之前，其实我最想说的是复习的心态问题。我本人一战失败的原因有 两点:1.忙于上新东方的考研班，结果忽视了自己做题练习的重要性;2.没有搞 清楚主要矛盾，一些该暂时放下的不太重要的事情没有放下比如一些娱乐项 目….;3.拖延症，把计划想的很好，然后不执行。所以可以说，我一战失败是 必然的，查到初试成绩是 375 分，去年的分数线大家都知道是 390 分，几乎不用 等通知就知道肯定是没戏了。失落消沉是肯定有过的，这些都是很正常的情绪， 大家在今后的一段漫长的复习历程中可能也难免会有。一定不要否定自己当下的 某些负面情绪!不要压抑它，一切都是正常的流露，没有人能够做到一直斗志昂 扬从来不沮丧的。你想要开始好好复习，并不需要先调整好心态。事实上，什么 样的心态并不是最重要的，对复习效果也没什么影响，只要你在做、在学习、在 努力，你就在朝着目标前进，并且不知不觉中，你就会忘记沮丧。我妈和我说: 有担心的时间，还不如多看两页书多做两道题呢!一战失败后，我决定去换个环 境换个心态，就去银河证券实习了 2 个月，一边实习一边复习，也和其他的实习 生聊聊人生。当然一战失败的同学们可以选择别的调整方式，能走出来就好。 啰啰嗦嗦说了很多，就是希望各位学弟学妹在复习过程中感到有些孤独无助 的时候能知道，其实大家都是这么度过的。每天都要在心里告诉自己，再努力一 下，再坚持一会儿，你就能胜利了。 一、数学三我本人大一时学的是数分和高代，主要侧重定理的证明和推导，和考研数学 的侧重点是有所不同。所以，如果大学期间也是学的数分和高代的同学，不要觉 得高数和线代这本书相对简单就忽视它。数学的复习时间我喜欢安排在上午，英 语在午后。政治在之后的下午，晚上安安静静地看专业课。因为是二战，可能和 大部分同学的情况不一样，我个人的复习进度是: 3 月中旬到 6 月底，按章节，先看课本做例题(没做课后习题)，然后看《李 永乐考研数学复习全书》的相应章节，做例题，做错的题标注出来，认真研究。 注意，标号章节和题号，可以拿三个本，高数、线代、概率论各一个，方便后期 看错题。这本书编得不错的，我上过新东方的考研数学班，讲得也就是这本书上 的内容。 7 月到 8 月中旬，刷《数学基础过关 660 题》，我是按科目刷的，先高数、然 后线代、最后概率论。660 题，如果基础知识比较牢固(3 月到 6 月打好基础哦) 的话其实做起来也很快的，因为就是选择和填空，但是建议要留下简单的解题步 骤。我大概是每天刷 30 道题，后面线代和概率论部分可以每天多刷一点。从 8 月中旬或者 9 月初开始，(1)首先，把之前的所有错题(大家不要忘记 标记哦)都再捋一遍，可以新找个本留下点痕迹。其实也不用抄错题，我的做法 是把每道错题都总结成一句或一小段简短的文字或公式，然后时时翻看。这个工 作可以坚持到最后一天，且各科都适用! (2)然后，就可以开始每天早 8 点到 11 点的每日一套卷刷题时光啦~回想起这 段时光、真是忙碌又充实又很有成就感!注意自己要掐好时间，一定要习惯在有 紧张感的条件下做题。一开始会觉得时间紧张，但是慢慢就学会怎么安排时间了。 用 3 个小时做完一套卷子之后，大概还要用 1 个多小时的时间来改错和总结，大 家千万不要做完就完了!至于做什么卷子，先做近 15 年的真题，然后差不多《张 宇 8 套卷》就出了!(注意，张宇的卷子会比较难、刚一接触会有受挫感，但是 没关系都这样!大家其实可以去关注一波张宇的微博，还挺正能量的。至于为什 么要推荐张宇的卷子而不是李永乐的，大家也可以买来李永乐的卷子看看，我个 人认为含金量不高。)之后还有会一段时间才会出 4 套卷，这段时间好好要回顾 一下，也可以二刷真题之类的。做完张宇的卷子再做真题就会发现真题 so easy! 真题很重要，一定要好好研究。但同时多做模拟题也是很有好处的，谁做谁知道。 注意:在此之前一直是数学占用的时间比较多，到后期可能别的科目会有很多 要背的东西比如政治。但是!大家一定不要把数学放下!必须要保持做题的手 感的，张宇老师说过，数学这东西一个月不做题就基本废了。所以，真心建议 每天一套卷子，一直到考研的前一天! 二、政治关于用书，感觉没什么好说的，就是肖秀荣的一系列全套。 4 月中旬到 7 月底，看《命题人知识点精讲精练》，看完一章，做章节后的仿 真模拟题。注意精讲精练上的题只有答案，有错的要自己回到书中去找到出处， 然后画出来。建议看书的顺序是:马原——史纲——毛中特——思修。马原比较 晦涩难懂，没关系也不用太懂，靠做题分清什么事重点!毛中特可以结合着史纲 来看，因为两者讲得基本上是同一历史时期的事件，只不过史纲偏重事件的历史 意义，毛中特偏重政策理论。思修比较简单，考试也只靠重点几章，这些肖秀荣 老师都会考诉你的~形势和当代部分可以先不看。 8 月到 10 月，刷《命题人 1000 题》。我这年的题我数了一下，选择 1514 道 (没错!命题人大约 1000 题)，分析题 43 道(分析题可以先不做)。选择题做起 来很快的，我自己是每天 80 道(马原、史纲、毛中特、思修，一共 4 部分嘛， 每部分做单选多选各 10 道)，20 天就刷完一遍了。然后，看错题，刷第二遍! 注意只做一遍是肯定不行的，按肖秀荣老师的说法，最好能刷 3 遍呢(虽然我只 刷了 2 遍，但注意我是二战的)。另外，近 10 年的真题的选择题部分(时政不用 管它)也应该做一下。注意选择的错题一定要好好看答案，搞清楚为什么! 10 月到 11 月，那本《命题人形式与政策以及当代世界经济与政治》的小册 子就出了。上编《形势与政策》包括 116 道选择题，大家看一遍然后把后边单选 题做了，时政部分就基本 OK 了。下编《当代世界经济与政治》的每个专题都有 一些分析题，这些题就可以适当地看看了，理解一下答题的逻辑，因为 10 月份 热点事件差不多都发生了，和后边的 8 套卷里的当代部分的题也都比较像了的。 之前做过的选择题的错题再好好看看!然后，看近 3 年的真题分析题，看看分析 题是怎么出的，答案应该怎么回答。11 月到 12 月，自己做《命题人 8 套卷》的选择题，选择题的答案会编的比较用心，建议都看一遍。熟读分析题答案!(能背一背最好) 12 月份(从 4 套卷出了开始，记得提前预定!)到考试前，一定要把分析题的答 案背下来!选择部分的建议和 8 套卷一样。很多人觉得难背，其实…就是很难背!我也是靠前头天晚上才差不多可以背下来。 总之，政治我考得也不算高，但是这么复习下来应该也不会考得太低。 三、英语一怎么说呢，我没有像很多人一样拿本书或者手机 app 刷单词，我看的单词都 是我在做阅读的时候自己总结的。我自认为这个方法挺好的，毕竟单词还是要在 文章中学习。用的书就是新东方的《考研英语历年真题详解及复习指南》，到后 期 10 月份以后我有刷过阅读的模拟题。 英语的复习我认为就分成两个阶段吧: 8 月底之前，做 1999-2010 的历年真题的阅读理解，每天一到两篇，也不用 严格计时，做完之后每道题都认真看答案，如果做错了，一定要搞清楚自己当时 是怎么错的。然后，仔细阅读文章，也不一定要非要翻译，但是一点要做到看懂 每一句话。这个其实好好看新东方那本书就行了，书上已经把单词、短语总结得 很好了。找个小本本，有不认识的单词一定要写下来，记好词性，至于单词对应 的中文，只写你在文中碰到的那个，不要一下写一堆中文意思，不好记的。记单 词的那个本要经常翻看! 9 月份开始，继续做 2011-2017 年的真题阅读理解，可以计时用 50 分钟做 完 4 篇阅读，对个答案，然后第二天第三天再精度文章，练习考试的感觉;也可 以一天做一到两篇，还和之前一样。总之阅读也是要保持感觉得，单词也要坚持 看，一直到考试前一天。其次，要开始练习写作了。建议先找一本比如说新东方 的写作书，背几篇范文，熟悉下考研英语写作的套路，然后再自己写。注意，自 己动手写作很重要! 四、432 统计学专业课的复习各路大神已经总结得尽善尽美了，我只是把我复习时参考的ZH 学长的经验贴贴出来!就是如下的 2000 多字，总结得可以说非常详细了! 复习用书: 贾俊平《统计学》第四版(经管类) 贾俊平《统计学》第六版(21 世纪统计学系列教材) 何晓群《多元统计分析》 王燕 《时间序列分析》 何晓群《应用回归分析》 复习方案: 贾俊平《统计学》: 一开始看的是第六版，作为门外汉的我觉得这本书还是蛮简单的，因为之前学过数理统计的一些课程，所以理解起来也不难，而且框架 体系也比较清晰，基本上一个章节一天就 OK，看了两遍，然后整理了自己的笔 记。后来了解到原来第四版的内容更饱满一些，就把第六版没有的内容补看了下， 做了笔记。而且今年出事的时候出了一道实验设计的题，最后阶段预测的时候是 万万没有想到会出这个题，所以，建议看第四版，内容全。但我又比较喜欢第六 版的表述，两本结合着看吧，但是第六版上没有的内容一定要补全，像实验设计、哑变量、指数平滑、主成分和因子分析、聚类分析(这俩个属于多元统计分析) 都要添加上去。非参数统计我看了一遍，整理了下笔记，稍微背诵了下，不过复 试的时候有人被问到了，所以也要好好看。复试的时候老师问了我关于哑变量的， 幸好看了第四版。 何晓群《多元统计分析》 吐槽一句，这本书写得像哲学，个人感觉是直接 从英文版翻译过来的，很多表述都没有做到像贾俊平那本书那么通俗易懂。我只 看到了第八章典型相关分析，真的很难说会不会考之后那几章，就目前来看是小 概率事件。这本书最关键的是统计分析方法的基本理论原理、分析步骤和以及去 对应可以解决的问题，不需要去死抠推导过程，这不是 432 需要重视的，但是对 于理解还是有帮助的，有兴趣的可以推推看。16 年没考这部分内容， 但不能预计 17 年会不会考，要复习的全面一些。一些问答题都要结合历年真题 自己根据课本进行总结。 王燕《时间序列分析》: 这本书的编写就相对好得多，条理很清晰，思路引 导很顺畅，一些例题也比较易懂，重点是各种预测描述模型，今年考了一道 08 年学硕考过的题:有趋势有季节变动可建立的模型，写出模型形式并简要说明。 可见学硕的历年真题也是很有借鉴意义的。之前也考过差分运算的，复习的时候 也要注意这种细节，但是这本书里面的例子特别好，几道题对应相应的知识点， 只要你一点点看下来理解了，然后把笔记整理好，后期再背诵下，应该没啥问题。 何晓群《应用回归分析》: 一直以为何晓群老师是个女老师，后来复试的时候 才了解到并非如此。这本书写的也很有条理，多重共线性的后果诊断处理已经多 次考到，自相关性和异方差还没出过，今年考了一个判定系数的解释，当时预测 了几道觉得会考的题，里面就有判定系数和回归模型的综合评价，初试的时候就 考到了，这个虽然比较简单，但可以尝试的方法就是在考试之前，自己预测一些 题，自己给自己出题做，涵盖面广一些，会有意想不到的结果的。 关于真题: 真题强调上百遍都不夸张，他对于你复习的方向有很大的启示作 用。我当时的做法就是把真题整理成八个专题，分别是:《专题一:图表展示与 概括性度量》、《专题二:统计量与抽样分布》、《专题三:参数估计与假设检验》、 《专题四:分类数据分析》、《专题五:方差分析与实验设计》、《专题六:回归分 析》、《专题七:时间序列分析》、《专题八:多元统计分析》，学硕和专硕的历年 真题都要整理分类，基本上人大每年考的都包含在八个专题之间，你需要做的就 是自己认认真真的从课本上找出答案来，然后总结一遍，一些学硕要求的比较偏 数理的可以忽略，需要明确的是，重点一定会反反复复的考，而且乐此不疲，像 今年时序和回归的题都是曾经考过的，几乎一模一样。 关于笔记: 自己整理的笔记的字迹一定要清晰，条理要很清楚，但这是建立 在你把书看了几遍理解透了之后才可以做到的事，当然一开始不理解，到后面反 复的背诵就会逐渐清晰起来了。当时我是和真题一样分了八个专题，参照人大大数据陈思聪学长的笔记整理了手写的笔记，学长的笔记结构完整，内容完善，当 时是如获至宝，每天看着它整理自己的笔记的心情相当愉悦，对我的专业课起到 了至关重要的作用，在这里谢谢学长。在复习过程中，我发现自己常常会对知识 的首次记忆有所偏颇，只知其一不知其二，以为已经完全理解了其确切的意思， 但其实当我在复试复习的时候再回过头来看往往会有更多新奇的发现，此时的知 识域相对来说也会完善一些。 关于背诵 心理学中有一个广为认可的记忆机制，即:我们在记忆的时候将 许多线索(诸如对一个原理的发散性理解、当时联想的事物的多样性)一并编码进入记忆中，能否长时间的保持知识的新鲜感或者说在大脑中的活跃度，取决于 这些线索是否足够丰富，这就为理解记忆提供了有力的证词。贯彻于专业课的背 诵上，其实各个统计方法知识中包含了精确的概念、严谨的逻辑、一般的原则、 生动的背景等无数的记忆线索，而并非是孤立的、任意的文本序列，各个点之间 具有并列、递进、相互排斥的种种关系，推导和演绎出这种联系，从而由点到面， 搭建成一个大的框架体系，就是我个人比较推崇的思维导图，如此进行下去到考 研前几天可以看着那张大的框架图自己逐条背诵，口头表达可以和原文范本有出 入，但是关键词必须要锁定，大致意思要接近。 第八篇：2018人大应用统计经验贴——by XXX看到今年还有去年的经验贴，我的复习策略跟他们大致相同。但也希望我的经验，能给学弟学妹们一点不一样的启示。个人基本情况：一战人大应统，总分370+ 本科北京某双非大学 数学专业。跟大多数考上的同学比，也算是个逆袭了。所以双非同学要有信心，只要持之以恒坚持下去，考上人大是可以的。复习过程大致分为三个阶段。 第一阶段大三下学期，每天大约学习考研知识5-6个小时（除了上课）。大三下学期一开始我就已经确定要考人大了。所以一切就全部针对人大应统而复习了。政治一点也没看，专业课看到人大出题相对简单，所以就把复习重点放到选修课 ： 回归分析 跟 多元统计分析，其它除了上课就没在复习专业课的知识了。英语这个学期就是把恋恋有词看了两三遍遍，感觉单词都差不多了解了，然后一个小时做了05年真题阅读错了三个，可以说这一学期英语学习取得很好效果吧。数学呢，一学期做完了36讲跟三分之二的复习全书（其实选择一个就可以了），做了17年真题大约125吧，虽然有的题之前做过，但也满意复习结果 第二阶段暑假期间（业内人士称为考研期间的遵义会议）先回家呆了一个星期看完了人民的名义。回来后就开启了全面复习计划（但从这个角度而言遵义会议好像不太合适了）。 每天学8-9小时。数学结束复习全书的内容，还完成了660题（这个跟1000题选一个就好了）。英语就是一个星期做一年真题，有听网上老师的讲解。政治一天学也就一个小时或者不到，看哲学跟政经部分。专业课在认真看课本，也买了几家学长的资料，总之，专业课下的资本比较大，舍不得孩子套不着狼，能考上就值了。 第三阶段大四上学期，每天6-7；9-10月由于开学有课学校也来了好多人加之略显疲惫，颓废了。数学上了学校里面老师讲的关于考研的课，再复习了一下前面学过的内容。政治看完了近代史，毛概的一部分（因为要开19大好多就留到后面复习了）。英语还是按着暑假的节奏走。专业课，主要复习回归跟多元部分了。10-12.23，这个时间是考研的冲刺时间，每天大概学习9-10个小时吧。数学每天一套历年真题或者模拟题，3小时左右，市面上大部分模拟题做过了。 政治，学完基础知识后，刷了好多好多选择题各种模拟题，每天学2-3小时，最后大题就是往死里背肖八肖四。专业课，3小时左右，基础知识刷好几遍，然后研究历年真题，总结出题规律。英语，时间压缩到一个小时左右，结果证明这是 一个错误之举。最后按照考试时间模拟了几场考试，为考试找感觉。 考研成绩与努力程度成正比，加油！]]></content>
      <categories>
        <category>人大应统部落</category>
      </categories>
      <tags>
        <tag>考研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理系列（9）：DCNN]]></title>
    <url>%2F2018%2F03%2F25%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%889%EF%BC%89%EF%BC%9ADCNN%2F</url>
    <content type="text"><![CDATA[这篇文章翻译自A Convolutional Neural Network for Modelling Sentences 准确地表达句子的能力是语言理解的核心。我们描述了一个被称为动态卷积神经网络(DCNN)的卷积结构，我们采用的是句子的语义模型。该网络使用动态k-Max池，一个通过线性序列的全局池操作。该网络处理不同长度的输入句子，并在句子中归纳出一个特征图，可以明确地捕捉短和长期的关系。该网络不依赖于解析树，并且很容易适用于任何语言。我们在四个实验中对DCNN进行了测试:小尺度二进制和多类情绪预测，六道问题分类和远程监控的推特情绪预测。该网络在前三项任务中取得了出色的性能，并且在最后一项任务中，在最强大的Baseline上减少了25%的错误。 1 Introduction句子模型的目的是为了分析和表征句子的语义内容，以达到分类或生成的目的。句子建模问题是许多涉及到一定程度的自然语言理解的任务的核心。这些任务包括情绪分析、意译、识别、总结、语篇分析、机器翻译、基础语言学习和图像检索。由于个别句子很少被观察到或根本没有被观察到，所以一个句子必须根据经常被观察到的句子中的单词和短的n-grams来表示一个句子。句子模型的核心是一个特征函数，它定义了从单词或n-grams的特征中提取句子特征的过程。 各种各样的模型方法已经被提出来了。基于组合的方法已经应用于从共现统计中得到的单词意义的向量表示，从而获得更长的短语的向量。在某些情况下，组合是由代数运算来定义的，而不是词义向量来生成句子的意思 (Erk and Pado ́, 2008; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Tur- ney, 2012; Erk, 2012; Clarke, 2012).。在其他情况下，一个复合函数被学习，或者与特定的句法关系有关(Guevara, 2010; Zanzotto et al., 2010) 或特定的词类型相关联(Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Kart- saklis and Sadrzadeh, 2013; Grefenstette, 2013)。另一种方法是用自动提取的逻辑形式来表示句子的意义 (Zettlemoyer and Collins, 2005)。 核心模型是基于神经网络的模型。这些范围从基本的神经词袋或n-grams模型到更结构化的递归神经网络，以及基于卷积运算的时滞神经网络(Collobert and Weston, 2008; Socher et al., 2011; Kalchbrenner and Blunsom, 2013b)。神经网络句模型有很多优点。他们可以通过预测，例如，利用单词和短语的上下文，来获得单词和短语的一般向量。通过监督训练，神经网络句模型可以将这些向量调整为特定于某项任务的信息。除了将强大的分类器作为其体系结构的一部分之外，还可以使用神经句模型来建立一个神经语言模型来逐字生成句子 (Schwenk, 2012; Mikolov and Zweig, 2012; Kalchbrenner and Blunsom, 2013a)。 我们定义了一个卷积神经网络结构，并将其应用于句子的语义建模。该网络处理不同长度的输入序列。在网络中，层间层交织着一维的卷积层和动态k-max池化层。动态k-max池是最大池操作的一般化。最大池操作符是一个非线性的子采样函数，它返回一组值的最大值 (LeCun et al., 1998)。该算子在两个方面是广义的。首先，k-max在一个线性序列上集合，返回序列中k最大值的子序列，而不是单个最大值。其次，可以通过将k作为网络的另一种功能或输入来动态选择池参数k。 卷积层在句子矩阵的每一列特征上都应用了一个可编辑的过滤器。在句子中的每一个位置上，将相同的过滤器与n-gram进行卷积，使得这些特征可以独立于句子中的位置提取出来。一个卷积层接着是一个动态池层和一个非线性形式的feature map。就像在对象识别的卷积网络中，我们通过对输入语句应用不同的过滤器来计算多个特征映射来丰富第一层的表示。后续层还具有多个特征映射，这些特征映射通过与下面一层的所有映射进行卷积来计算。这些层的权值构成一个四维张量。由此产生的结构被称为动态卷积神经网络。 卷积和动态池操作的多层结构会在输入语句中引入结构化特征图。图1展示了这样一个图。在较高层次的小筛选器可以捕获在输入句中相隔很远的非连续短语之间的句法或语义关系。特征图引入了一种类似于语法分析树的层次结构。这种结构与纯句法关系无关，是神经网络的内部结构。 我们在四种环境中进行网络实验。前两个实验包括预测电影评论的情绪(Socher et al., 2013b)。该网络在二进制和多类实验中都优于其他方法。第三个实验涉及在TREC的数据集下的六个问题类型的分类 (Li and Roth, 2002)。该网络与其他基于大量工程特性和手工编码的知识资源的先进方法的准确性相一致。第四项实验是通过远程监控来预测Twitter帖子的人气(Go et al., 2009)。该网络接受了160万条推文，根据出现在他们身上的表情符号自动标记。在手工标记的测试集上，在Go et al.(2009)中所报告的最强的unigram和bigram基线的预测误差中，该网络减少了超过25%。 论文的提纲如下。第2节描述了DCNN的背景，包括中心概念和相关的神经句模型。第3节定义了相关的操作符和网络的层。第4节介绍了网络的诱导特征图和其他属性。第5节讨论了实验，并考察了学习的特征检测器。 2 BackgroundDCNN的层是由一个卷积操作和一个池化操作形成的。我们首先回顾一下相关的神经网络句模型。然后描述了一维卷积和经典时滞神经网络的运算(TDNN) (Hinton, 1989; Waibel et al., 1990)。通过在网络中加入一个最大池化层，TDNN可以被作为一个句子模型来使用(conbert and Weston, 2008)。 2.1 Related Neural Sentence Models各种各样的神经网络语言模型已经被提出。基本句型模型的一般类型是神经网络词汇(NBoW)模型。这些通常包括一个投影层，将单词、子字单元或n-grams映射到高维的嵌入。然后将组件与操作(如求和)组合在一起。所产生的组合向量通过一个或多个全连接层进行分类。 采用外部解析树提供的更一般结构的模型是递归神经网络 (RecNN) (Pollack, 1990; Ku ̈chler and Goller, 1996; Socher et al., 2011; Hermann and Blunsom, 2013)。在树的每个节点上，节点的左右子节点都由一个经典层组合起来。该层的权值在树的所有节点上共享。在顶部节点计算的层给出了这个句子的表示。递归神经网络(RNN)是递归网络的一种特殊情况，它所遵循的结构是一个简单的线性链(Gers and Schmidhuber, 2001; Mikolov et al., 2011)。RNN主要作为一种语言模型，但也可以被看作是一个线性结构的句子模型。在最后一个单词中计算的层代表这个句子。最后，基于卷积运算和TDNN架构，进一步构建了一类神经语句模型 (Collobert and Weston, 2008; Kalchbrenner and Blunsom, 2013b)。这些模型中使用的某些概念是DCNN的核心，接下来我们将描述它们。 2.2 Convolution一维卷积是一个权重向量$m\in R^m$和一个被视为序列的输入向量$s\in R^s$之间的运算。向量m是卷积的滤波器。具体地说，我们认为s是输入语句，$s_i$是与句子中第i个单词相关的单一特征值。一维卷积背后的思想是在句子中求每个m-gram与向量m的点积，从而获得另一个序列$c$： c_j = m^T s_{j−m+1:j}\qquad (1)根据j的取值范围，公式1给出了两种类型的卷积。窄卷积要求$s≥m$，序列$c\in R^{s-m+1}$的j的范围是$m$到$s$。宽卷积对s或m没有大小要求，序列$c\in R^{s-m+1}$的索引j的范围是1到$s+m-1$。超出范围($is$)的输入值$s_i$会被设为零，狭义卷积的结果是广义卷积结果的子序列。两种一维卷积在图2中得到了说明。 过滤器m中训练的权重相当于一个语言特征检测器，它学习对一个特定的n-gram进行编码，这些n-grams为大小n≤m,m是滤波器的宽度。在一个宽的卷积中应用权重m比在一个窄的卷积中应用它们有一些优势。一个宽卷积可以确保过滤器中的所有权重达到整个句子，包括边缘的单词。当m被设置为相对较大的值(比如8或10)时，这是非常重要的。此外，宽卷积保证了滤波器m对输入语句的应用总是产生一个有效的非空结果c，独立于宽度m和句子长度s。我们接下来描述一个TDNN的经典卷积层。 2.3 Time-Delay Neural NetworksTDNN将一系列的输入序列与一组权重m进行卷积。在语音识别的TDNN (Waibel et al.， 1990)中，序列s被视为具有时间维度，而卷积应用于时间维度。每一个$s_j$往往不只是一个单一的值，而是一个d个数字组成的向量，因此$s\in R^{d×s}$。同样,m是一个大小为d×m权重矩阵。每一行m与相应的s行卷积，卷积通常是窄类型的。将生成的序列c作为输入到下一层，可以叠加多个卷积层。 Max-TDNN句子模型是基于TDNN的体系结构 (Collobert and Weston, 2008)。在模型中，窄卷积被应用到句子矩阵s上，其中每一列对应句子中的一个词的特征向量$w_i\in R^d$ s=[w_1 ... w_s] \qquad (2)为了解决不同的句子长度问题，Max-TDNN在生成的矩阵c中取每一行的最大值，从而产生一个d个数组成的向量。 { C }_{ max }=\begin{bmatrix} max({ c }_{ 1,: }) \\ ··· \\ max({ c }_{ d,: }) \end{bmatrix}\qquad (3)目标是捕捉最相关的特性，即对于生成的矩阵c的d行中的每一个具有最高值的特性。然后将固定大小的向量cmax作为输入，用全连接层进行分类。 Max-TDNN模型有许多可取的特性。它对句子中单词的顺序很敏感，它不依赖于外部语言特定的特性，如依赖关系或选区解析树。它对句子中每个单词的信号都有很大程度的统一的重要性，除了边缘上的单词，在计算窄卷积时次数更少。但该模型也有一些局限性。特征检测器的范围仅限于权重的跨度m。增加m或叠加多卷积层的窄类型使特征检测器的范围更大。与此同时，它也加剧了对句子边缘的忽视，增加了卷积所需要的输入语句的最小规格s。由于这个原因，高阶和远程特性检测器不能很容易地并入模型中。最大池操作也有一些缺点。它无法区分一个行中的相关特性是否只发生一次或多次，它会忘记特征发生的顺序。更普遍的是,池因子的减少信号矩阵的对应于s−m + 1。即使是对于中等的s，池因子也可能是过量的。下一节的目的是在保留优势的同时解决这些限制。 3 Convolutional Neural Networks with Dynamic k-Max Pooling我们使用一个卷积结构来建模句子，它通过动态的k-max池来交替使用宽卷积层和动态池层。在网络中，中间层特征图的宽度取决于输入句子的长度。所得到的体系结构是动态卷积神经网络。图3表示一个DCNN。我们开始详细描述这个网络。 3.1 Wide Convolution给定一个输入句子，获得DCNN的第一层，其中，我们得到每个句子的每个词的词嵌入表示$w_i\in R^d$，然后构造一个句子矩阵$s\in R^{d×s}$，如（2）式所示。嵌入的值$w_i$是在训练期间优化的参数。网络中的卷积层是通过对权值矩阵$m$和下一层的激活矩阵进行卷积操作得到的。例如，第二层是通过将卷积应用到句子矩阵s本身来获得的。维度d和滤波器宽度m是网络的超参数。我们让这些操作是一维宽卷积，如第2.2节所述。由此产生的矩阵c的维度为$d×(s + m−1)$ 3.2 k-Max Pooling接下来我们描述的是一个池化操作，它是在Max-TDNN语言模型中的关于时间维度的最大池化操作的一般化，不同于用于目标检测的卷积网络中使用的局部最大池化操作 (LeCun et al., 1998)。给定一个值k，和一个长度为$p$的序列$p\in R^p$ (p&gt;k)，$k-max pooling$选择了一个由向量p中最大的数值组成的子序列$p_{max}^k$。$p^k_{max}$中值的顺序对应于它们在p中的原始顺序。 k-max池操作使得选出在p中的k个最活跃特征成为可能，它们位于一些不同的位置。它保留了特征的顺序，但对它们的特定位置不敏感。它还可以更精细地分辨出在p中，特征被高度激活的次数，以及在p上的特征变化的被高度激活的状况。将k-max池运算符应用于最顶层卷积层后的网络中。这保证了全连接层的输入独立于输入句子的长度。但是，正如我们接下来看到的，在中间卷积层中，池参数k不是固定的，而是动态选择的，以便能够平滑地提取更高阶和更长的特性。 3.3 Dynamic k-Max Pooling一个动态k-max池操作是一个k-max池操作，我们让k成为句子长度和网络深度的函数。虽然可能有很多函数，但我们只是简单地将池参数建模如下： K_{l}=\max \left( k_{top}, \left \lceil \frac {L-l}{L} s \right \rceil \right)\qquad (4)其中l表示当前卷积的层数（即第几个卷积层），L是网络中总共卷积层的层数；$k_{top}$为最顶层的卷积层pooling对应的k值，是一个固定的值。举个例子，例如网络中有三个卷积层，$k_{top}=3$，输入的句子长度$s=18$；那么，对于第一层卷积层下面的pooling参数$k_1=12$，而第二层卷积层的pooling参数$k_2=6$，第三层有固定的池化参数$k_3=k_{top}=3$。方程4是描述第l部分的序列的相关部分在长度为s的句子上的相关部分所需要的数值数量的模型。对于情绪预测中的一个例子，根据方程，在长度s的句子中，一阶特征，例如一个正数，大多数出现$k_1$次，而另一个二阶特征，如否定句或子句，最多出现$k_2$次。 3.4 Non-linear Feature Function在动态池化应用于卷积的结果之后，一个偏置项$b\in R^d$和一个非线性函数$g$被应用于被池化的矩阵的每一个元素。每个集合矩阵的每一行都有一个单偏差值。如果我们暂时忽略池化层，我们可以说明在卷积层和非线性层后如何计算矩阵$a$中的d维的列a。定义M为对角矩阵。 M=[diag(m:,1),···,diag(m:,m)] \qquad (5)其中m为宽卷积的d维滤波器的权值。然后，在第一对卷积和一个非线性层之后，矩阵a中的每一列a得到如下所示，对于一些索引j： a=g\left( M\begin{bmatrix} { w }_{ j } \\ ··· \\ { w }_{ j+m-1 } \end{bmatrix}+b \right) \qquad (6)这里a是一列的一级特征。二阶特征也类似于将公式（6）应用于一个序列的一阶特性$a_j,…a_{j + m′−1}$与另一个权重矩阵$m′$。除了池，公式（6）是特征提取函数的核心，它有一个一般形式。特征函数与池化才操作引入了位置不变性，使高阶特征变量的范围变大。 3.5 Multiple Feature Maps到目前为止，我们已经描述了如何应用一个宽卷积，一个(动态的)k-max池化层和一个非线性函数到输入句子的矩阵来获得一阶特征图。这三种操作可以重复，以生成递增顺序的特征图和增加深度的网络。我们用$F_i$来表示第i阶的特征图。在对目标识别的卷积网络中，为了增加特定顺序的学习特征检测器的数量，可以在同一层并行计算多个特征映射。每一个特征映射$F_j^i$是通过不同的滤波器来对矩阵$m_{j,k}^i$和第$i-1$阶的特征映射$F_k^{i-1}$进行卷积操作，并把结果加和： F_j^i=\sum^n_{k=1}m^i_{j,k}*F_k^{i-1}\qquad (7)其中∗表示宽卷积。权重$m^i_{j,k}$构成一个4阶张量。在宽卷积之后，首先进行动态k-max池化，然后将非线性函数分别应用于每个映射。 3.6 Folding到目前为止，在网络的形成过程中，应用于句子矩阵的单个行的特征检测器可以有多种顺序，并在多个特征映射的同一行中创建复杂的依赖关系。然而，不同行的特征检测器相互独立，直到顶部的全连接层。可以通过在Eq. 5中使M变成满矩阵来实现完全依赖于不同的行，而不是一个稀疏矩阵的对角线。在这里，我们探索一种更简单的方法，称为折叠，它不引入任何其他参数。在一个卷积层之后，动态k-Max池化之前，将一个feature map组件中的每两行相加。对于d行的映射，折叠返回d/2行的映射，从而使表示的大小减半。使用折叠层之后，第i层的特征探测器现在依赖于更低的第$i-1$层的两行特征值。这样我们结束了对DCNN的描述。 4 Properties of the Sentence Model我们描述了基于DCNN的句子模型的一些属性。我们描述了由卷积和池化层的继承而导致的特征图的概念。我们简单地将这些性质与其他神经句模型的性质联系起来。 4.1 Word and n-Gram Order其中一个基本属性是对输入句子中单词顺序的敏感性。对于大多数应用，为了学习细粒度的特性检测器，对于模型来说，能够区分输入中是否存在特定的n-gram是有益的。同样，对于模型来说，能够分辨出最相关的n-grams的相对位置是有益的。该网络旨在捕捉这两个方面。第一层的宽卷积的滤波器m可以学习识别具有小于或等于滤波器宽度m的特定n-grams。正如我们在实验中看到的，在第一层中，m通常被设置为一个相对较大的值，比如10。由广义池操作提取的n-grams子序列，将不变性引入到绝对位置，但维持了它们的顺序和相对位置。至于其他的神经句模型，NBoW模型的类根据定义对词序不敏感的。一个基于递归神经网络的句子模型对词序很敏感，但它倾向于将其作为输入的最新单词 (Mikolov et al., 2011)。这使得RNN在语言建模方面表现出色，但在输入句中，它是在记忆n-grams时是最不理想的。类似地，递归神经网络对词序很敏感，但对树中最顶层的节点有偏倚。浅树可以在一定程度上缓解这种影响 (Socher et al., 2013a)。在第2.3节中，max - tdnn对词序是敏感的，但是max池只在句子矩阵的每一行中选出一个单一的n-grams特征。 4.2 Induced Feature Graph模型不需要任何的先验知识，例如句法依存树等，并且模型考虑了句子中相隔较远的词语之间的语义信息； 5 Experiments 模型训练及参数 输出层是一个类别概率分布（即softmax），与倒数第二层全连接； 代价函数为交叉熵，训练目标是最小化代价函数； L2正则化； 优化方法：mini-batch + gradient-based (使用Adagrad update rule, Duchi et al., 2011) 实验结果 在三个数据集上进行了实验，分别是(1)电影评论数据集上的情感识别，(2)TREC问题分类，以及(3)Twitter数据集上的情感识别。结果如下图： 可以看出，DCNN的性能非常好，几乎不逊色于传统的模型；而且，DCNN的好处在于不需要任何的先验信息输入，也不需要构造非常复杂的人工特征。 6 Conclusion我们描述了一个动态的卷积神经网络，它使用动态k-max池化运算符作为非线性的子采样函数。网络所引起的特征图能够捕获不同大小的单词关系。该网络在问题和情绪分类上获得了很高的性能，而不需要由解析器或其他资源提供外部特性。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
        <tag>DCNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理系列（8）：RCNN]]></title>
    <url>%2F2018%2F03%2F08%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%888%EF%BC%89%EF%BC%9ARCNN%2F</url>
    <content type="text"><![CDATA[这篇文章翻译自Recurrent Convolutional Neural Networks for Text Classification 文本分类是许多NLP应用的基础任务。传统的文本分类器通常依赖于许多人设计的特性，如字典、知识库和特殊的树内核。与传统的方法相比，我们引入了一个递归的卷积神经网络来进行文本分类，而没有人为设计的特征。在我们的模型中，我们应用了一个经常的结构，在学习单词表示法时尽可能地捕捉上下文信息，这可能大大减少了与传统的基于窗口的神经网络相比的噪音。我们还使用了一个自动判断哪些单词在文本分类中扮演关键角色，以在文本中捕获关键组件的方法。我们对四个常用的数据集进行实验。实验结果表明，该方法在多组数据集上优于最先进的方法。 一、Introduction文本分类是许多应用的重要组成部分，如web搜索、信息过滤和情绪分析(Aggarwal和Zhai 2012)。因此，它引起了许多研究者的关注。 文本分类中的一个关键问题是特征表示，这通常是基于单词bagof -words (BoW)模型，其中的unigrams、bigrams、n-grams或一些精心设计的模式通常被提取为特征。此外，还应用了频率、MI (Cover and Thomas 2012)、pLSA (Cai and Hofmann 2003)、LDA (Hingmire et al. 2013)等几种特征选择方法，以选择更具鉴别性的特征。 然而，传统的特征表示方法往往忽略文本中的上下文信息或词序，对于捕捉词的语义仍然不满意。例如，在句中，“沿着南岸的夕阳漫步提供了一系列令人惊叹的优势。”当我们分析“Bank”(unigram)这个词时，我们可能不知道它是指金融机构还是河旁。此外，“South Bank”(bigram)，尤其是考虑到两个大写字母，可能会误导那些对伦敦不太了解的人，把它当作金融机构。当我们获得更大的上下文“沿着南岸漫步”(5-gram)，我们就能很容易地辨别出它的意思。虽然高阶n-grams和更复杂的特性(如树内核(Post和Bergsma 2013))被设计用于捕获更多的上下文信息和单词序列，但它们仍然存在数据稀疏问题，这严重影响了分类的准确性。近年来，经过预先训练的word embedding和深层神经网络的快速发展，给各种NLP任务带来了新的启发。word embedding是单词的一种分布式表示，极大地缓解了数据稀疏问题(Bengio et al. 2003)。Mikolov、Yih和Zweig(2013)表明，预先训练的词嵌入可以捕捉有意义的句法和语义规律性。在word embedding的帮助下，人们提出了一些基于合成的方法来获取文本的语义表示。 Socher et al .(2011 a;2011 b;2013年提出递归神经网络(RecursiveNN)，在构建句子表示方面已被证明是有效的。然而，递归通过树结构捕获了一个句子的语义。它的性能很大程度上取决于文本树结构的性能。此外，构造这样一种文本树的时间复杂度至少为$O(n^2)$，其中n为文本的长度。当模型遇到长句或文档时，这将是非常耗时的。此外，两个句子之间的关系很难用树结构来表示。因此，递归不适合对长句或文档建模。 另一种模型，循环神经网络（RNN），模型时间复杂度为$O(n)$。该模型通过逐字分析一个文本单词，并将所有先前文本的语义存储在一个固定大小的隐藏层中(Elman 1990)。RNN的优点是能够更好地捕捉上下文信息。这可能有利于捕获长文本的语义。然而，RNN是一个有偏倚的模型，在这个模型中，后面的单词比先前的单词更具优势。因此，当它被用于捕获整个文档的语义时，它可能会降低效率，因为关键组件可能出现在文档中的任何地方，而不是最后。为了解决偏置问题，我们引入了卷积神经网络(CNN)，将一个不带偏见的模型引入到NLP任务中，它可以很好地确定文本中带有最大池化层的识别性短语。因此，与递归或循环神经网络相比，CNN可以更好地捕捉文本的语义。CNN的时间复杂度也是$O(n)$。然而，以前对CNNs的研究倾向于使用简单的卷积核，如固定窗(bert et al. 2011;Kalchbrenner和Blunsom 2013)。使用这样的内核时，很难确定窗口大小:小窗口大小可能导致一些关键信息的丢失，而大的窗口会导致巨大的参数空间(这可能很难训练)。因此，它提出了一个问题:我们能否比传统的基于窗口的神经网络学习更多的上下文信息，更准确地表示文本的语义。 为了解决上述模型的局限性，我们提出了一个循环卷积神经网络(RCNN)，并将其应用于文本分类的任务。首先，我们应用一个双向的循环结构，与传统的基于窗口的神经网络相比，它可以大大减少噪声，从而最大程度地捕捉上下文信息。此外，该模型在学习文本表示时可以保留更大范围的词序。其次，我们使用了一个可以自动判断哪些特性在文本分类中扮演关键角色的池化层，以捕获文本中的关键组件。我们的模型结合了RNN的结构和最大池化层，利用了循环神经模型和卷积神经模型的优点。此外，我们的模型显示了$O(n)$的时间复杂度，它与文本长度的长度是线性相关的。 我们用英语和汉语四种不同的任务来比较我们的模型和以前的最先进的方法。类别分类包含主题分类,情感分类和写作风格分类。实验表明，我们的模型在四种常用数据集的三种情况下比以往的先进方法更出色。 二、Related Work2.1 Text Classification传统的文本分类工作主要集中在三个主题:特征工程、特征选择和不同类型的机器学习算法。对于特征工程来说，最广泛使用的功能是bag-of-words。此外，还设计了一些更复杂的功能，比如词性标签、名词短语(Lewis 1992)和tree kernels (Post and Bergsma 2013)。特征选择旨在删除噪声特征，提高分类性能。最常见的特征选择方法是去停词(例如，“The”)。先进的方法使用信息增益、相互信息(Cover和Thomas 2012)或L1正则化(Ng 2004)以选择有用的特性。机器学习算法常用分类器，如逻辑回归(LR)、朴素贝叶斯(NB)和支持向量机(SVM)。然而，这些方法都存在数据稀疏问题。 2.2 Deep neural networks最近，深度神经网络(Hinton and Salakhutdinov 2006)和表示学习(Bengio, Courville，和Vincent 2013)提出了解决数据问题的新思路，并提出了许多学习单词表示的神经模型(Bengio et al. 2003;Mnih and Hinton 2007;Mikolov 2012;Collobert et al . 2011;huanget al . 2012;Mikolov et al . 2013)。一个单词的神经表示被称为单词嵌入，它是一个实值向量。“嵌入”一词使我们能够简单地利用两个嵌入向量之间的距离来测量单词的相关性。通过预先训练的词嵌入，神经网络在许多NLP任务中表现出色。Socher等人(2011b)使用半监督递归自动编码器来预测句子的情绪。Socher等(2011a)提出了一种用循环神经网络进行语义检测的方法。Socher等人(2013)引入递归神经张量网络分析短语和句子的情绪。Mikolov(2012)利用循环神经网络构建语言模型。Kalchbrenner和Blunsom(2013)提出了一个新的对话行为分类网络。conbert等人(2011)引入了卷积神经网络的语义角色标记。 三、Model我们提出了一个深度神经模型来捕获文本的语义。图1显示了我们模型的网络结构。网络的输入是一个文档$D$，它是一个序列的单词$w_1, w_2…w_n$。网络的输出为类别。我们使用$p(k|D，θ)$来表示文档类别$k$的概率,$θ$是网络参数。 3.1 Word Representation Learning我们结合一个词和它的上下文来呈现一个词。语境帮助我们获得更准确的词义。在我们的模型中，我们使用一个循环结构，它是一个双向的循环神经网络，用来捕获上下文。我们将$c_l(w_i)$定义为词$w_i$左边的文本，将$c_r(w_i)$定义为词$w_i$右边的文本。$c_l(w_i)$和$c_r(w_i)$都是具有|c|个实值元素的稠密向量。使用式（1）来计算词$w_i$左边的文本，$e(w_{i-1})$是词$w_{i-1}$的词嵌入，它是一个长度为$|e|$的实值向量。$c_l(w_{i-1})$是上一个词$w_{i-1}$的左半部分文本。任何文档的第一个词的左半边文本使用相同的参数$c_l(w_1)$。$W^{(l)}$是一个将隐藏层(context)转换为下一个隐藏层的矩阵。$W^{(sl)}$是一个矩阵，用来将当前单词的语义与下一个单词的左上下文结合起来。$f$是非线性激活函数。右半边文本$c_r{(w_i)}$用相同的方式计算，如公式（2）所示。一个文档的最后一个词的右半边文本共享参数$c_r(w_n)$ c_l(w_i)=f(W^{(l)}c_l(w_{i-1})+W^{(sl)}e(w_{i-1})) \qquad (1)\\c_r(w_i)=f(W^{(r)}c_r(w_{i+1})+W^{(sr)}e(w_{i+1})) \qquad (2)如式(1)和(2)所示，上下文向量捕获了所有左和右上下文的语义。例如，在图1中，$c_l (w_7)$编码了左侧上下文的语义“沿着南方漫步”以及前面的所有文本，$c_r (w_7)$编码了右侧上下文的语义“提供了一个……””。然后，我们定义单词$w_i$的表示形式为式(3)，即左侧上下文向量$c_l(w_i)$，词嵌入表示$e(w_i)$和右侧上下文向量$c_r(w_i)$的连接。用这种方式，我们的模型可以更好地消除“$w_i$”这个词的含糊含义，而不是只使用固定窗口的传统神经模型，（例如他们只使用关于文本的部分信息） x_i=[c_l(w_i);e(w_i);c_r(w_i)]\qquad (3)循环结构可以在文本的向前扫描时获取所有的$c_l$，在反向扫描时获取所有的$c_r$。时间复杂度为$O(n)$。当我们获得了单词$w_i$的表示$x_i$后，我们将一个线性变换与tanh激活函数一起应用到$x_i$，并将结果传递到下一层。 y_i^{(2)}=tanh(W^{(2)}x_i+b^{(2)})\qquad (4)$y$是一个潜在的语义向量，每一个语义因素都将被分析，以确定代表文本的最有用的因素。 3.2 Text Representation Learning我们模型中的卷积神经网络是用来表示文本的。从卷积神经网络的角度来看，我们以前所提到的重复结构是卷积层。当计算所有单词的表示时，我们应用一个max-pooling层。 y^{(3)}=max_{i=1}^ny_i^{(2)}\qquad (5)max函数是一个按元素的函数。$y^{(3)}$的第k个元素是$y_i^{(2)}$的所有向量的第k个元素的最大值。池化层将不同长度的文本转换为固定长度的向量。通过使用池化层，我们可以在整个文本中捕获信息。还有其他类型的池层，比如平均池层(Collobert et al. 2011)。我们这里不使用平均池，因为这里只有几个单词和它们的组合对于捕获文档的含义非常有用。在文档中，最大池化层试图找到最重要的潜在语义因素。池化层接受循环结构的输出作为输入。池化层的时间复杂度为$O(n)$。整体模型是一个循环结构和一个最大池化层的级联，因此，我们的模型的时间复杂度仍然是$O(n)$。我们模型的最后一部分是一个输出层。与传统的神经网络相似，它被定义为 y^{(4)}=W^{(4)}y^{(3)}+b^{(4)}\qquad (6)最后，将$softmax$函数应用于$y^{(4)}$。它可以把输出数字转换成概率。 p_i=\frac{exp(y_i^{(4)})}{\sum_{k=1}^nexp(y_k^{(4)})}\qquad (7)四、Training4.1 Training Network parameters我们定义的所有训练参数为$θ$。 \theta =\{E,b^{(4)},c_l(w_1),c_r(w_n),W^{(2)},W^{(4)},W^{(l)},W^{(r)},W^{(sl)},W^{(sr)}\}具体来说，参数$E\in R^{|e|×|V|}$是词嵌入，偏差向量$b^{(2)}\in R^{|H|}$,$b^{(4)}\in R^O$，初始文本$c_l(w_1),c_r(w_n)\in R^{(H×(|e|+2c))}$，转换矩阵$W^{(2)}\in R^{H×(|e|+2|c|)}$,$W^{(4)}\in R^{O×H}$,$W^{(4)}\in R^{(O×H)}$,$W^{(l)},W^{(r)}\in R^{|c|×|c|}$,$W^{(sl)},W^{(sr)}\in R^{|e|×|c|}$，其中$|V|$是词库中词的数量，$H$是隐藏层的规格，$O$是文档类别数。 网络的训练目标是使得$\theta$满足对数似然最大化： \theta →\sum_{D\in D}log p(class_D|D,\theta) \qquad(9)其中$D$是训练文档集，$class_D$是文档$D$的真实类别。 我们使用随机梯度下降法(Bottou 1991)来优化训练目标。在每一步中，我们随机选择一个样本(D, classD)进行一次梯度下降： \theta ← \theta+\alpha\frac{\partial log p(class_D|D,\theta)}{\partial \theta}\qquad(10)其中$\alpha$是学习率。在训练神经网络的训练过程中，我们采用了一种常用的方法来训练神经网络。我们将神经网络中的所有参数服从均匀分布初始化。最大或最小值的大小等于“fan-in”的平方根(Plaut和Hinton 1987)。数值是我们模型中前面一层的网络节点。该层的学习速率除以“fan-in”。 4.2 Pre-training Word Embedding五、Experiments5.1 Datasets5.2 Experiment Settings5.3 Comparison of Methods5.4 Results and Discussion5.5 Contextual Information六、Conclusion]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>word2vec</tag>
        <tag>RCNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日知录（5）：认知偏误列表]]></title>
    <url>%2F2018%2F03%2F02%2F%E6%97%A5%E7%9F%A5%E5%BD%95%EF%BC%885%EF%BC%89%EF%BC%9A%E8%AE%A4%E7%9F%A5%E5%81%8F%E8%AF%AF%E5%88%97%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[一、认知偏误简介认知偏误（Cognitive biases）是一种倾向，它认为在某些方面，认知会与理性标准（Standard of rationality）或者良好的判断相偏离，它经常应用于心理学（Psychology）和行为经济学（Behavioral economics）的研究中。 认知偏误的存在虽然被可重复的研究（Replicable research）证实，但是目前仍然有许多关于如何对这些偏误进行分类或如何解释它们的争论。部分偏误受到大脑用于处理判断与决策的启发式（Heuristics）信息处理规则（如心理捷径Mental shortcuts）的影响，这些影响统称为认知偏误。偏误拥有各种形式的认知（“冷性质”），比如心理噪音（Mental noise），或者动机（“热性质”）的解释（Motivational explanations），比如当理念被一厢情愿（Wishful thinking）歪曲的时候，以上两种性质的影响都可能在同时出现。 也有一些争论探讨这里面的部分偏误是否是无用的、不合理的（Irrational）或者可能会引致有效益的态度或行为。例如，当试图去了解别人的时候，人们倾向于提出确认自己关于对方的假设性引导性的问题（Leading questions）。有人认为这种确认偏误（Confirmation bias）是一种社会技能（Social skill），为了用于与其他人建立联系。 对这些偏误的研究绝大多数是关于人的，然而，一些调查显示部分偏误也存在于非人类的动物身上。例如，双曲贴现（Hyperbolic discounting）的情况存在于老鼠、鸽子和猴子身上。 二、决策、信念与行为的偏误 中文名称 英文名称 描述 模糊效应 Ambiguity effect 也称为不明确效应，倾向于选择信息量更大的选项，避免选择缺少信息或者未知的选择项 锚定效应 Anchoring or focalism 人们在对某人某事做出判断时，易受到第一印象或第一信息的支配，思想就像沉入海底的锚 一样被固定在某个点 注意力偏误 Attentional bias 人们的知觉受到再现的想法的影响，如做决定时受制于环境或情绪，忽略一些相关信息而更关注某些特定信息 自动化偏误 Automation bias 过度依赖自动化系统，这可能导致错误的自动化信息取代正确的决定 叠加效应 Availability ca 也称效用层叠，某一观点不断重复确认后会加强，如集体观念通过其在公共话语中不断地重复获得越来越多的合理性，自我认知也将随之产生强化。换言之重复足够多那么它将变成“真的”，有点像三人成虎，以及“重要的事情说三遍” 可得性启发 Availability heuristic 在记忆中高估事件可能性的倾向，它受到记忆的远近程度、事件的罕见程度以及情绪化的影响，因而在作评价时更多依赖记忆中记住的信息而不是全部信息。在使用启发法进行判断时，人们往往会依赖最先想到的经验和信息，并认定这些容易知觉到或回想起的事件更常出现，以此作为判断的依据 逆火效应 Backfire effect 也称逆反效应，人们通过强化他们的信仰来为驳斥现象做出反应。如当一个错误的信息被更正后，如果更正的信息与其原本的看法相违背，它反而会加深人们对原本错误信息的信任 从众效应 Bandwagon effect 同羊群效应，人们从事或者相信某件事时依赖于其他相同情况的人做出的选择，相关的是群体思想（Groupthink）和从众行为（Herd behavior） 基率谬误/忽视 Base rate fallacy / neglect 忽视基本的比率信息（通用的、一般的），而倾向于在某些特定情况下才适用的特定的信息，可参考统计学中的贝叶斯统计（Bayesian Statistics）理论 信念偏误 Belief bias 或称信仰偏误，人们评价论据是否合乎逻辑的可信程度受到其结论可信度的影响，因为相信结论所以相信其推理过程 偏见盲点 Bias blind spot 认为自己的偏见比别人更少，或者说认为别人的偏见比自己多的倾向 拉拉队效应 Cheerleader effect 人们在团体里面比在孤立状态下表现得更有吸引力的倾向，就像在拉拉队中更倾向于表现自己 选择支持性偏误 Choice-supportive bias 认为自己做出的选择是更好的而事实上可能并非如此（忽略不利的地方），稍有自我安慰之意 聚集性错觉 Clustering illusion 也称丛集错觉或聚类错觉，人们倾向于将大量随机小样本中不可避免的“条纹”或“聚簇”状的随机分布考虑为某种具有统计学意义的“规律”（幻象模式Phantom patterns），此处可参考知乎问题“打麻将的时候，为什么会有连续一段时间运气特别好的现象？”下面的回答 舒适区效应 Comfort zone effect 对于过去常用的方案，高估其效益或成功机会；对于过去少用的方案，低估其效益或成功机会 确认偏误 Confirmation bias 也称证实偏误，人们偏向于关注、寻找、解释和记忆能够证实自身预想的信息，而忽视事实去支持自己的成见 一致性偏误 Congruence bias 也称相合性偏误，对假设的验证完全依赖于直接的测试，而不对其替代性假设的可能性做测试；或者说只相信直接证明假说的实验，而忽视间接的证明或是证明其他假说的可能 合取谬误 Conjunction fallacy 也称联合谬误，认为特殊情况比一般情况更可能发生；认为单个条件发生的概率要小于多个条件联合发生的概率。如对某人的描述很像女性主义者时，认为某人是替妇女辩护的律师的可能性比认为某人是律师的可能性更高 保守主义 Conservatism 在新的证据出现时仍然难以改变某人观念（Revise one’s belief）的现象，相对于贝叶斯信念修正（Bayesian Belief-revision）来说，人们看重先验分布（Prior distribution）而轻视新的样本证据（Sample evidence） 对比效应 Contrast effect 同一刺激因背景不同而产生的感觉差异的现象，或者说当与最近观察到的有所差异的对象相比较时，加深或降低感知刺激的现象 知识的诅咒 Curse of knowledge 见多识广的人很难从并不那么博识的人的角度看问题，或者说知情者很难从不知情者的角度看问题 数据迁就偏误 Data-snooping bias 也称数据探查偏误、数据拟合偏误，指在基于先前得到的实证经验后对历史数据进行分析后所引起的偏误，由于模型“迁就”历史数据（即与历史数据拟合得太好），反而导致“预测”结果十分糟糕（与未来有偏差） 诱饵效应 Decoy effect 人们对两个不相上下的选项进行选择时，因为第三个新选项（诱饵）的加入，会使某个旧选项显得更有吸引力。有时“诱饵”并不需要真的存在，即“幽灵诱饵” 辩护人谬误 Defendant’s fallacy 泛指多种根据不相关资讯认定被告“犯罪的概率”很小的情况。另参见检察官谬误（Prosecutor’s fallacy） 似曾相识效应 Déjà vu effect 对某些事物有强烈的熟悉感，似乎曾经接触过，且能预先想到接下来会发生什么事 面额效应 Denomination effect 相对于大面额的钞票来说，小面额的钞票会花费得更快 处置效应 Disposition Effect 倾向于出售价值增长的资产，而抵制出售价值下降的资产 区分偏误 Distinction bias 两个事物作为备选项一起考虑时比单独考虑时区别更大。另见越少越好效应（Less-is-better effect） 丹宁克鲁格效应 Dunning-Kruger effect 也称达克效应，指无知要比知识更容易产生自信。如能力欠缺的人在自己认识不足的基础上得出错误结论，但是无法正确认识到自身的不足，察觉到自己的错误行为 时长忽视 Duration neglect 也称过程时间忽视，在确定某事件的价值时忽视该事件的持续时间。心理观察发现，人们判断某件痛苦经历的不愉快程度很少依赖于时长因素，但是经历中痛苦的最高点（Peak）和痛苦的消减速度却是两个最主要因素，一般来说消减得越快，会觉得这段经历更加痛苦 同理心断层 Empathy gap 也称移情差异、移情隔阂，人们低估情感的影响或者力量的倾向，“子非鱼，焉知鱼之乐？” 禀赋效应 Endowment effect 也称敝帚自珍效应，当个人一旦拥有某项物品，那么他对该物品价值的评价要比未拥有之前大大增加，总认为自己的东西是最好的；放弃某件物品比希望拥有它更有难度 本质主义 Exaggerated expectation 期望超出估计和现实，世界不是那么戏剧化，理想很热血，现实很平淡。相反的是保守主义偏误（Conservatism bias） 排除偏误 Exclusion bias 也称剔除偏误，研究进行时由于排除某些看似不符预期的样本产生的偏误 试验偏误或预期偏误 Experimenter’s or expectation bias 实验者相信、证实并发布那些支持他们预期结果的数据，相反地，对相矛盾的结果持怀疑、抛弃的态度或者对它们进行降权。推荐书籍《推理的迷宫》 聚焦效应 Focusing effect 对事物的某一方面过分注重的倾向，这可能导致错误的预期 福勒效应/巴纳姆效应 Forer effect / Barnum effect 人们常常认为一种笼统的、一般性的人格描述十分准确地揭示了自己的特点，而这些描述往往十分模糊及普遍，以至于能够适用到很多人身上（放之四海而皆准）。这种效应可以提供关于普遍接受的一些观念和做法的解释，如占星（Astrology）、算命（Fortune telling）、笔迹学（Graphology）和某些类型的性格测试 正向偏置 Forward Bias 模型是基于历史数据建立的，因此仅对过去的数据进行了验证，很难预测未知 框架效应 Framing effect 一个问题两种在逻辑意义上相似的说法却导致了不同的决策判断。人际交往中指关键不在于说什么，而在于怎么说；经济学中指当消费者感觉某一价格带来的是“损失”而不是“收益”时，他们对价格就越敏感 频率错觉 Functional fixedness 也称功能固着，认知限制某人只能在传统的方式上使用某种物品或对象的倾向；当需要解决问题时，思维阻断了使用新方法的可能性 经费偏误 Funding bias 也称赞助偏误，选择研究方法或诠释研究结果时，倾向于迎合经费提供者或赞助商的立场 赌徒谬误 Gambler’s fallacy 也称蒙地卡罗谬误，一种负近因效应（Negative recency），认为某事物在未来的几率会因过去的事件发生改变，事实上并非如此，统计规律不适用于个体，有些已经发生的事不会影响将要发生的事。概率上认为随机序列中一个事件发生的机率与之前发生的事件有关，即其发生的机率会随着之前没有发生该事件的次数而上升。典型的概念为大数定律（Law of large numbers）。如在某事件的一面连续发生后，人们会认为另一面在下一次“风水轮流转”或“翻盘”的几率更大。另参见逆赌徒谬误（Inverse gambler’s fallacy） 难度效应 Hindsight bias 也称后见之明偏误、后视偏误，常说的“事后诸葛亮”，事情已经发生了却说自己早已预料到 敌对媒体效应 Hostile media effect 也称敌意媒介效应，可以理解为意识形态偏误，由于强烈的阵容观念的存在，总是认为立场与自己不同的媒体有偏见、不客观 热手谬误 Hot-hand fallacy 也称热手效应、热手现象（Hot hand phenomenon），一种正近因效应（Postive recency），认为某事多次发生则未来发生的概率会较大；认为经历过成功的人物在额外的尝试之下有更大的机会获得进一步成功的可能。如人们会以为随机的连赢来自杰出的表现 双曲贴现 Hyperbolic discounting 更关注眼前利益而不是长远利益，相对于未来的回报，更倾向于现在的回报，这将导致人们的选择基于相同的原因却会随着时间的推移而发生改变。也作现时偏误（Current moment bias / Present-bias），可以参考动态不一致（Dynamic inconsistency）理论 可识别受害者效应 Identifiable victim effect 相对于处于危难中的一大群人来说，人们更倾向于为单个的可辨识的人做出响应，这里还涉及到同理心（Empathy）的问题 宜家效应 IKEA effect 当人们亲自组装、构建自己的东西（如宜家IKEA家具）时，无论最终结果如何，都会对它评价甚高，而且投入越多的劳动或情感就越容易产生依恋感和自豪感而高估物品的价值 控制错觉 Illusion of control 高估对外部事件的影响程度，人们以为自己对事物的控制能力或控制权超过实际上所拥有的控制能力，个体对自己成功的可能性的估计远高于其客观可能性的一种不合理的期望 有效性错觉 Illusion of validity 认为通过进一步地获取信息，将产生更多相关的辅助预测的数据，虽然可能并非如此，因为不一定有用 错觉关联 Illusory correlation 或称相关性错觉，错误地认为两个毫不相关的事物之间存在联系 影响偏误 Impact bias 也称影响力偏误，高估事件对未来情绪状态的影响力，如认为中大奖后会永远开心却并非如此 信息偏误 Information bias 也称资讯偏误，指人们为了获取信息会搜寻那些并不一定起作用的信息的倾向。如人们为了更好地决策，会搜集尽可能多的信息，但是并不用得上 样本大小不敏感性 Insensitivity to sample size 小样本中的变化容易被忽视的现象，人们判断样本统计（Sample statistic）中的概率时不考虑样本大小；评估统计数据时，未考虑小样本比大样本更容易观察到极端结果 逆赌徒谬误 Inverse gambler’s fallacy 认为概率很小的事发生了，一定是做了很多次。另参见赌徒谬误（Gambler’s fallacy） 不理性增值 Irrational escalation 也称沉没成本谬误（Sunk cost fallacy），人们基于累积的前期投资来证明某增加投资的决策是正确的，尽管新证据表明该决策可能是错误的；增加投资时关注已有业绩而不关注当前更有价值的信息 妄下结论 Jumping to conclusions 根据少许的信息即做出判断与决策。如诛心、预言、贴标签等等 公平世界假定 Just-world hypothesis 或称公平世界原则，人们倾向于相信自己生活在一个公平的世界里，每个人都得到他应得 领先时间偏误 Lead-time bias 也称超前时间偏误，经常出现在疾病检测中，当比较新近的或实验的测试与之前的测试时，其结果没有变化的现象，因为新近的或实验的测试只是比之前更早确定疾病，因此给人以病症时间延长的错觉 时长偏误 Length-time bias 也称时距偏误，选择偏误的一种，由于统计结果失真造成数据结论错误。当随机选择的时间点或空间点间隔参与到时长间隔的分析中时将会造成此偏误，这一分析过程倾向于选择较长的间隔，从而影响数据的准确性 越少越好效应 Less-is-better effect 当判断的时候，不联合在一起，倾向于分离出较小的集合而不是较大的集合加以考虑 损失厌恶 Loss aversion 也称损失规避，倾向于避免损失而获得收益，因为放弃某事物的负效用大于拥有它的正效用，差距可能超出1倍。可参考沉没成本效应（Sunk cost effects）和禀赋效应（Endowment effect） 戏局谬误 Ludic fallacy 一种不相干的谬误，指错误地将游戏的机率模式套用到现实世界，以及过度使用统计与概率预测未来 麦纳马拉谬误 McNamara fallacy 一种非形式谬误，指描述过度使用数据评估事情的现象，过度相信数据、依赖数据评估事情，忽略难以量化的事 单纯曝光效应 Mere exposure effect 也称曝光/熟悉/多看/重复/暴露/接触效应等，因为仅仅熟悉或亲近某一人事物而表现出对TA过分喜爱的倾向。某人事物在TA首次出现时如果没有带来厌恶感，那么随着TA出现的次数越多，对其产生的好感度也越高 货币错觉 Money illusion 也称金钱错觉，注重于货币上的名义面额价值，而不是其购买力的实际价值 道德凭证效应 Moral credential effect 道德会导致无意识的偏误，良好而平等的历史记录或看法增加了后续出现不平等偏误的可能性，受影响的人可能意识不到先前思维中已经建立的道德凭证或者说对某个事物的看法。如某人由于得到了某些高道德的评价或认证，而认为自己做得够好，反而在其他面向做了相反的事 多重比较谬误 Multiple Comparisons Fallacy 广泛比较二个群体的各种特征，从中找出有明显差异的几个，宣称它就是造成二个群体不同的原因 消极偏误 Negativity bias 也称负面/消极偏误或负性认知偏误，是一种心理现象，相比于积极的记忆，人们更容易回想起那些不愉快的记忆；相比积极的事物，人们更加注重消极的 负性效应 Negativity effect 也称负向效果，当人们评估一个他们不喜欢者的行为的原因时，倾向于将积极行为归因于周边环境，而将消极行为归因于个人的本性。人们在印象形成过程中，消极信息的作用往往大于积极信息的作用，人们根据他人的消极品质形成的印象或评价很难改变，而且更愿意相信 可能性忽视 Neglect of probability 或称概率忽视，做决定时人们会忽略掉一些可能性，如决策时完全不理会不确定条件下的可能性或几率 反安慰剂效应 Nocebo effect 也称反伪药效应，给予有效的药物或治疗，病人却相信或觉得病情有所恶化。另参见安慰剂效应（Placebo effect） 正常化偏误 Normalcy bias 高估事物正常化发展的趋势，因而在发生变故时拒绝为之前从来没有发生过的灾祸、失败（disaster）做出计划、反应或者推卸责任；对突如其来的灾难人们更难接受，更想恢复原状 非本地发明 Not invented here 也称非我发明，一种心理现象，拒绝接触、使用外部开发的产品、研究、标准和知识。另参见宜家效应 选择性观察偏误 Observation selection bias 观察时不可避免受到前置条件的限制而筛选了样本，因而得出不适当的结论。例如问卷调查到的人是个热心、愿意填问卷的人，因而其结果未必能反映不热心、不愿意填问卷的人的想法 选择性观察偏误 Observation selection bias 观察时不可避免受到前置条件的限制而筛选了样本，因而得出不适当的结论。例如问卷调查到的人是个热心、愿意填问卷的人，因而其结果未必能反映不热心、不愿意填问卷的人的想法 观察者期望效应 Observer-expectancy effect 研究员在期待给定的结果时，会因此不自觉地操纵实验或者曲解数据，以便找到它。另参见受试者期望效应（Subject-expectancy effect） 忽略偏误 Omission bias 也称忽视偏误、不作为偏误，认为错误的行为比不作为更糟糕。相比将同样有害的行为忽略或者不作为来说，将有害的行为付诸行动被看成是使情况更加糟糕的或者不道德的，因为行动比不作为更加明显 遗漏变量偏误 Omitted-variable bias 遗漏了有关变量而产生的估计量的偏误 乐观偏误 Optimism bias 也称乐观主义倾向，态度过于乐观，高估了有利和令人愉快的结果，过分相信事情会向好的一方面发展。另参见一厢情愿（Wishful thinking），情价效应（Valence effect），正面结果偏误（Positive outcome bias） 鸵鸟效应 Ostrich effect 不理会明显的消极的情形或条件，是一种逃避现实的心理，也是一种不敢面对问题的懦弱行为，就像鸵鸟被逼得走投无路时，就把头钻进沙子里。相关成语掩耳盗铃 结果偏误 Outcome bias 也称结果效应/偏好，它是发生在决策评估中的一种偏误式判断，即当决策结果与决策质量不存在实质性联系时，评估者仍根据结果信息评估决策质量，是一种不考量决策当时的状况，而以结果论成败的倾向 自负效应 Overconfidence effect 在判断、解答、决策问题时，自己往往过度自信。如人们对某些问题，回答者“99%肯定”很有可能40%是错误的 过度诊断偏误 Overdiagnosis bias 也称过度诊断偏倚，指诊断出受试者（或患者）并不会造成症状或死亡的“疾病”，经常出现于早期疾病筛查中 空想性视错觉 Pareidolia 也称空想性错视、幻想性视错觉，将模糊随机的外界刺激（声音、图像等）看成是显著的并赋予实际意义，如像动物的云朵、月球上的人脸、听到倒播影音（Records played in reverse）中不存在的隐藏信息等 悲观主义偏误 Pessimism bias 人，尤其是经历过不幸或消极的人，认为不好的事情发生在自己身上的可能性更大 安慰剂效应 Placebo effect 也称伪药效应，给予无效的药物或治疗，病人却相信或觉得病情有所改善。另参见反安慰剂效应（Nocebo effect） 计划谬误 Planning fallacy 或称规划谬误，计划时低估任务完成时间的倾向。如人们在计划未来时，往往为未来安排过多的事物，设定过于理想化的目标，而实际实施时却很难完成计划，计划赶不上变化 正面结果偏误或情价效应 Positive outcome bias or Valence effect 认为积极正面的结果比负面的更容易发生 购后合理化 Post-purchase rationalization 或称买入后理性化论、“买家的斯德哥尔摩症候群”，人们在买入某东西之后为劝说自己而加入对购买行为合理化的解释，即使买下的产品太过昂贵或发现瑕疵，以此来化解认知失调 预筛选偏误 Pre-screening bias 也称预审偏误，筛选样本时预先排除了某些不应排除的样本 首因效应 Primacy effect 也称首位效应、起始效应、第一印象作用、先入为主效应，是一种开头刺激或信息的记忆过于引人注目的认知偏误。通过“第一印象”最先输入的信息对客体以后的认知会产生显著的影响，它是由第一印象所引起的一种心理倾向，许多人习惯称之为“第一感”。如人们更容易回想起序列中起始的项目而不是后面的项目 创新偏误 Pro-innovation bias 对于发明或者创新在社会上的作用持过分乐观倾向，同时往往忽视其局限性和弱点。如只看重新意而忽视其具体应用状况 检察官谬误 Prosecutor’s fallacy 泛指多种根据不相关资讯认定被告“无辜的概率”很小的情况。另参见辩护人谬误（Defendant’s fallacy） 假确定性效应 Pseudocertainty effect 当预期的结果是正的（收益）时候作风险规避（Risk-averse）的选择，当结果是负的（损失）时候作风险寻求（Risk-seeking）的选择，在选择项内容一致的情况下，其选择受描述结果的方式影响的趋向。另参见损失厌恶（Loss aversion） 对抗心理 Reactance 也称感应抵抗、抗拒心理，当他人想要限制你自由选择或做不想做之事的时候，产生站在对立面或者持反对意见的一种冲动。另参见逆反心理（Reverse psychology） 反冲性贬低 Reactive devaluation 也称反冲性贬抑，只是因为源自对手而认为其价值不大 近因偏误 Recency bias 也称近因效应、新颖效应，与首因效应相反，它更重视近期的数据或经验，忽视早期的数据或经验（参见“峰终定律”Peak-end rule）；在多种刺激一次出现的时候，印象的形成主要取决于后来出现的刺激 新近错觉 Recency illusion 或称新词错觉，认为某词语或语法是新近才出现的，事实上它们已经存在很久了。另参见眼球效应（Frequency illusion） 复原谬误 Regression fallacy 也称还原谬误，非常态的甲事发生以后，用乙措施处理后甲事扭转，便断定乙措施可扭转甲事。然而非常态的事发生后，本来就比较容易发生较接近常态的事 可能性贝叶斯回归 Regressive Bayesian likelihood 对条件概率的估计偏于保守而不是偏激的 回归偏误 Regressive bias 或称退缩偏误、逆行偏误，思维中将高价值高可能性的情况低估，反之高估的倾向 报告偏误 Reporting bias 也称报告选择偏误，指在反映情况时，选择性披露或隐藏信息的现象 自制偏误 Restraint bias 也称压制偏误，人们高估自己面对诱惑的抵抗力 押韵效应 Rhyme as reason effect 或称押韵有理效应，押韵的语句被认为是更加真实的。典型的案例出现在辛普森审判（O.J Simpson trial）一案中，辩词说道“If the gloves don’t fit, then you must acquit”（如果手套是不合手的，那么你就是无罪的） 风险补偿/佩兹曼效应 Risk compensation / Peltzman effect 在安全性增加的情况下，人们往往倾向于做出更加危险的举动。诸如很多安全产品的发明反而增加了相关活动的风险系数 选择偏误 Selection bias 一种系统误差，来源于研究对象的选择过程以及影响研究对象参与的因素，由于选入的研究对象与未选入的研究对象在某些特征上存在差异而引起的误差，常发生于研究的设计阶段 选择性感知 Selective perception 也称预期感知，事先的期望影响了事物的感知。如打针时事先觉得很疼就真觉得很疼 自我选择偏误 Self-selection bias 也称志愿者偏误（Volunteer bias），是指由于普查组与对照组人员自身条件差异造成偏误的情况，由于志愿者的自我选择而造成统计结果产生异常或偏误 塞默尔维斯反射 Semmelweis reflex 拒绝接受与已建立的规范、信仰或价值观相矛盾的新证据，仅凭证据并不能改变心理现状 社会比较偏误 Social comparison bias 在雇佣人才的时候，倾向于选择那些在某些方面不如自己的候选者 社会期望偏误 Social desirability bias 或称社会赞许性偏误，倾向于向外人展示自己社会期许的或者社会友好型的能力和行为，而隐藏社会不太看重的或者不利的方面 现状偏误 Status quo bias 期待事物保持不变，安于现状的倾向。另参见损失厌恶（Loss aversion），禀赋效应（Endowment effect），系统正当化（System justification） 刻板印象 Stereotyping 也称定型化效应，人们对某事物具体特点形成的一种固定的看法，并把这种观看法推而广之，认为这个事物或者整体都具有该特征，而忽视个体差异，有如将心中刻好的模板套到每个个体身上 次可加性效应 Subadditivity effect 或称分开加总效应，认为整体的可能性比部分的可能性（之和）更低 受试者期望效应 Subject-expectancy effect 由于受试者期待某种结果，因而下意识地扭曲了回报内容。另参见观察者期望效应（Observer-expectancy effect），案例安慰剂效应（Placebo effect） 主观验证 Subjective validation 也称主观确认，相信某事是对的，就感觉它是对的。也会把巧合的事当作有关联 幸存者偏误 Survivorship bias 也称存活者偏误、“死人不会说话”等。当取得资讯之渠道仅来自于幸存者时（因为无法从死者获得来源），此资讯可能会存在与实际情况不同的偏误。联系成语“兼听则明，偏信则暗” 德州神枪手谬误 Texas sharpshooter fallacy 即“先射箭再画靶”，原用以形容流行病学上的群集错觉，后衍生泛指统计研究做出结果后，把其中的群集独立出来当作有统计意义，然而实际上此集群更可能是随机产生 省时偏误 Time-saving bias 低估从低速加速（减速）行进时节省（损失）的时间；而高估从高速加速（减速）行进时节省（损失）的时间 单位偏误 Unit bias 认为计量单位反映合理程度，人们倾向于认为给定的一个单位、一瓶、一罐、一盘或更多其他精细度量的单位食物，就是适当的食用份量。这个偏误强烈影响到分装食物的消费，如果分装的食物看上去应该就是一顿或一人份，那么他们就会一直吃到见底 熟悉路线效应 Well travelled road effect 低估走熟悉路线的持续时间；高估走陌生路线的时间 只看整体效应 Whole only effect 选项为整套方案时，只关注整体，而忽略个别部分有协商的可能 零风险偏误 Zero-risk bias 偏向于将小的风险降为零，虽然说高风险降低的程度可能更大。如偏向将2%降到0%，而不是20%降到5% 零和直观推断 Zero-sum heuristic 也称零和捷思，直观地判断某情形是零和的（收益和损失之和为零）或者说总体平衡的，但事实未必如此。零和（Zero-sum）概念来自博弈论（Game theory）。这种偏误的产生可能与社会主导倾向（Social dominance orientation）的个人因素有关 三、社会偏误这里的大部分偏误被称为归因偏误（Attributional biases）。 中文名称 英文名称 描述 观察者偏误 Actor-observer bias 也称行为者-观察者偏误，解释其他个体的行为时过分强调内在个性的影响，而轻视当时的外在情况（参见基本归因错误Fundamental attribution error），而在解释自己行为时则与此相反 防御性归因假定 Defensive attribution hypothesis 也称防御性归因，当后果变得越严重或者自己与受害者的情况越相似时，对加害者的责怪或谴责也就越多 自我中心偏误 Egocentric bias 相比于外部归结的责任来说，人们更倾向于将合作行动中的责任归结于自己；人们作为参与者要比作为观察者更能承担责任 外在激励偏误 Extrinsic incentives bias 一种基本归因错误，认为他人要靠外在动机如情境才能做好；而自己可以靠内在动机如个人特质做好 错误共识效应 False consensus effect 也称错误共识理论，指人们高估其他人对自己认同程度的倾向，认为别人跟自己的想法是一致的 基本归因错误 Fundamental attribution error 也叫基本归因谬误，人们在评判他人的行为时倾向于强调人格特质的影响，而低估环境在其中的角色以及给其带来的影响。另参见观察者偏误（Actor-observer bias），群体归因错误（Group attribution error），积极效应（Positivity effect），消极效应（Negativity effect） 群体归因错误 Group attribution error 认为群组成员的个体特征反映了群组整体的特征，认为群组的表现反映了群组成员个体的偏好，虽然事实所说明的可能是不同的情况 光晕效应 Halo effect 也称晕轮效应、光圈效应，人们对某件事物的总体映象会影响到人们对其具体特征或属性的看法。另参见外貌魅力偏见（Physical attractiveness stereotype）、刻板印象（Stereotyping） 非对称认知错觉 Illusion of asymmetric insight 或称非对称洞察力错觉，人们认为自己对别人的了解超过别人对自己的了解，认为自己比其他人懂得更多 外部媒介错觉 Illusion of external agency 人们通常低估自己对未来结果产生满意度的能力，当人们体会到这种自我生成的满意度时，他们会错误地认为这些是由外部的影响力（influence）、洞察力（insight）、善意（benevolence）等造成的 透明度错觉 Illusion of transparency 也称洞悉错觉，认为别人很容易看透（理解）自己，自己也很容易看透（理解）别人 优越感错觉 Illusory superiority 也称优越感偏误（Superiority bias）、乌比冈湖效应（Lake Wobegon effect）、优于常人效应（Better-than-average effect）等，相对于其他人来说，自己对个人的优秀品质评价过高，而对不良品质评价过低，即高估自己的优点，低估自己的缺点，认为自己更有水平 群内偏误 Ingroup bias 或称派系偏误，人们对认为是属于自己这一方的成员有优先或者较好待遇的倾向，即偏向某个人自己的群体 道德运气 Moral luck 基于事件结果的道德立场影响人们对事件的评判，而没有考虑到事件的意图、过程及情境 朴素犬儒主义 Naïve cynicism 认为别人的自我中心偏误或者利己行为比自己更多 朴素现实主义 Naïve realism 也称朴素实在论，相信我们的所见所闻就是真实的、客观的、不带偏见的，事实是显而易见的，理性的人一定会赞成我们，不赞成我们的人一定是无知的、懒惰的、无理的、有偏见的 外群体同质性偏误 Outgroup homogeneity bias 或称外群同质性偏误、组外一致性偏误，人们认为自己群体内的成员是比其他群体的成员更加多样化的，外部群体的成员之间彼此极其相似，比内群体成员更加“同质化”，不是自己这一方的都是一样的 投射偏误 Projection bias 也称投射效应，不自觉地以为他人（或未来的自己）和（现在的）自己有相似的情感、思想与价值观。以己度人，认为自己具有某种特性，他人也一定会有与自己相同的特性，是一种把自己的感情、意志、特性投射到他人身上并强加于人的认知障碍 自利偏误 Self-serving bias 或称自利性偏误、利己偏误，相比失败，更多地将成功归因于自己，在评价模棱两可的信息时，也倾向于描述得对自己有利。另参见利群偏误（Group-serving bias） 共有信息偏误 Shared information bias 群体成员倾向于花费更多地时间和精力讨论大家都熟悉的信息（Shared information），而对只有部分人了解的信息（Unshared information）则讨论得更少 系统正当化 System justification 也称体制合理化，倾向于保卫和巩固现状，现有的社会、经济、政治状况往往是首选，而其他方案往往遭到贬抑，即使牺牲个人或者集体利益。另参见现状偏误（Status quo bias） 性格归属偏误 Trait ascription bias 也称个性归属偏误，认为自己的个性、行为、情绪是多变的，而他人是一成不变且容易预测的 终极归因错误 Ultimate attribution error 也称最终归因错误，类似于基本归因错误（Fundamental attribution error），只是这里人们倾向于将问题归因于整个群体，而不是群内的个体 差于常人效应 Worse-than-average effect 在任务困难的时候认为自己比别人更差，与优于常人效应（Better-than-average effect）相反 四、记忆的错误与偏误心理学与认知科学（Cognitive science）中，记忆偏误（Memory bias）是一种认知偏误（Cognitive bias），它将强化或者削弱记忆，其中包括记忆回想（recall）起来的可能性以及记忆回想起来的所需时间，或者是改变记忆的内容。 以下是各种记忆偏误： 中文名称 英文名称 描述 怪异效应 Bizarreness effect 怪异的事物或情况比正常的更容易记住 改变偏误 Change bias 在参与到事物的变化中后，很难回忆起其之前的状况 童年期遗忘 Childhood amnesia 也称童年失忆症，人们很难回忆起四岁之前的情况 选择支持性偏误 Choice-supportive bias 也称支持选择偏误，在回顾自己做过的选择时，认为是明智的 保守主义或回归偏误 Conservatism or Regressive bias 记忆中将高价值高可能性的情况记成比实际低，而低价值低可能性的记成比实际高。基于证据的记忆情况是非极端化的，参见回归偏误（Regressive bias） 一致性偏误 Consistency bias 记忆中错误地将某人过去的态度和行为看成像是现在的态度和行为 情境效应 Context effect 也称语境效应，认知与记忆依赖于所处情境，回忆脱离情境的事件比处于情境内中的更难，某些情境的记忆在其他情境中也不容易回想，如在家里回忆工作内容，它所需的时间更长而准确性也更低 跨种族效应 Cross-race effect 相比辨认种族内的人来说，辨认其他种族成员的难度更高 潜隐记忆 Cryptomnesia 也称隐藏记忆，错误归因（Misattribution）的一种，当一个被遗忘的记忆无意识地出现时，会以为是自己新的原创的，是灵感的涌现。人们可能因此错误地回想或产生某个想法、观念，这在某方面可能会造成剽窃的争议 自我中心偏误 Egocentric bias 回忆中存在自利或自我美化的倾向，例如记得考试成绩比实际的好或者记得抓住的鱼比实际的大 衰退效应偏误 Fading affect bias 也称情感衰退偏误，负面情绪、不愉快的记忆比正面的积极的衰退得更快 虚假记忆/虚构症 False memory or Confabulation 也称错误记忆、伪记忆、虚谈症，记忆障碍和错误归因（Misattribution）的一种，记忆中产生空想、虚构，并非有意欺骗地捏造、歪曲、曲解关于某件事物的记忆，区别于撒谎，因为自己都不知道那些信息是虚假的 谷歌效应 Google effect 人们对于很容易用搜索引擎（如谷歌）就能从网络上搜集到的信息很容易淡忘 幽默效应 Humor effect 幽默的事物更加容易记住，可能的原因解释是幽默的事物会增加认知处理的时间以及激发情感 真相错觉效应 Illusion of truth effect 即使没有意识到听说过，人们也倾向认为听过的是真的，而不管该陈述实际上是否正确。换言之，人容易相信熟悉的话胜过陌生的 错觉关联 Illusory correlation 或称相关性错觉，错误地认为两个毫不相关的事物之间存在联系 延迟效应 Lag effec 也称滞后效应、间隔效应（Spacing effect），相对于短时间内多次接触的信息来说，人们容易学习记住那些长时间跨度但是接触少的信息。这一效应表明突击式的考前复习并不能带来很好的学习效果（质量），但是短时间内能够带来良好的记忆表现（Memory performance） 钝化与锐化 Leveling and Sharpening 可理解成强化与弱化，记忆随着时间的推移发生扭曲丢失细节的现象，特别重要的事情会得到锐化或者被选择性地想起，而其中的细节和某些方面的信息将被钝化或者遗忘。随着时间的推移以及回忆次数的增加，这两种趋势将进一步加强 处理深度效应 Levels-of-processing effect 深层次的分析产生更精细、持久、强大的记忆痕迹，且记忆中用不同方式“编码”的信息拥有不同的效力等级 列表长度效应 List-length effect 列表的长度越长，所能记住的项目的比例越少，但是随着列表长度的增加，所能记住的绝对数量也将增加 误导信息效应 Misinformation effect 也称错误信息效应，由于事后信息（Post-event information）的干扰，记忆变得愈加模糊或不准确 通道效应 Modality effect 也称模态效应，通过口语传达出来的信息，最后听到的东西会记忆得比较深刻，而书写的则相对更低 心境一致性记忆偏误 Mood-congruent memory bias 与当前心境相合的记忆更加容易回想 轮流发言效应 Next-in-line effect 当人们一个接着一个发言时，后面发言的人不太容易记住前一个人说过的话 部分项目提示效应 Part-list cueing effect 当记忆的内容是一整组的时候，提示其中的一项会使回忆起其他项目的难度加大 峰终定律 Peak-end rule 体验一项事物之后，所能记住的大约只有在高峰期与最终时的体验，而在过程中好与不好的比重和时间长短、总体的感觉，对记忆影响不大 创伤的持续性 Persistence of traumatic event 记忆中往往对创伤事件（Traumatic event）进行反复不断地回忆。另参见“创伤后压力症候群” 图画优异性效应 Picture superiority effect 也称图优效应，通过图片来传达的信息记忆效果要比通过文字传达的更好 积极效应 Positivity effect 或称正面效应，老年人的记忆偏向积极美好的倾向 首因效应、近因效应和序位效应 Primacy effect, Recency effect &amp; Serial position effect 在序列末尾的项目最容易想起，接着是起始的，而处于中间的项目最不容易想起 处理难度效应 Processing difficulty effect 处理难度更高的信息时，由于花费更多时间阅读与思考而更容易想起 怀旧凸显 Reminiscence bump 回忆人生事件时，青春期和成年早期的事比其他时期的事更容易想到 玫瑰色回忆 Rosy retrospection 也称美好的回忆，将过去事件的回忆美化得比实际更好 自我生成效应 Self-generation effect 人们记忆自己生成的信息更加清楚，而别人给出的信息则容易淡忘 自我关联效应 Self-relevance effect 与自己有关的记忆比与他人有关的记忆更加容易回想 来源混淆 Source confusion 记忆中把偶然发生的事件与其他信息相混淆，造成记忆歪曲 间隔效应 Spacing effect 长时间跨度重复暴露的信息比短时间跨度重复暴露的信息要更容易想起；比起无间隔的重复接触，有间隔的重复接触有较好的记忆与学习效果。另参见延迟效应（Lag effect） 焦点效应 Spotlight effect 也称聚光灯效应，高估别人关注自己表现和行为的错觉，认为自己是瞩目的焦点 陈规偏误 Stereotypical bias 记忆向着陈规或刻板映象扭曲，如种族和性别偏见，又如错误地认为某些名字是罪犯的 后缀效应 Suffix effect 也称附加效应，在不太需要回想起来的主题或条目后面加入合理的项目，会减少近因效应的影响，这是近因效应的一种应用 可暗示性 Suggestibility 错误归因（Misattribution）的一种，想法容易受到暗示者的提醒而发生扭曲 伸缩效应 Telescoping effect 记忆中将近期的事情往更早移，遥远的事情往更近移的倾向。因此近期的事情变得更加遥远，而遥远的事情变得更加临近 测试效应 Testing effect 或称测验效应，人们更容易记住自己反复书写过的信息，而不是反复阅读的信息 舌尖现象 Tip of the tongue phenomenon 能够想起一件事情的一部分或者相关的信息，但是难以回忆起全部。当多个相似的记忆同时出现时，会产生干扰或者阻塞，这将造成人们说话时欲言又止或者说到一半卡住 逐字效应 Verbatim effect 人们说过的话语中的大意比逐字的词语或完整的句子更容易记住，因为记忆记住的是表述而不是直接复制 雷斯多夫效应 Von Restorff effect 也称莱斯托夫效应、梵•雷斯托夫效应，突出的、醒目的、特别强调的事物更容易记住 蔡格尼克效应 Zeigarnik effect 也称蔡格尼克记忆效应、蔡格尼效应，未完成或者中断了的事物比完成了的事物更容易被记住 五、认知偏误的成因理论 有限理性（Bounded rationality）——理性与优化的限制1. 前景理论（Prospect theory） 心理账户（Mental accounting） 适应偏误（Adaptive bias）：基于有限的信息作出决策，并因错误的代价而有所偏误 属性替代（Attribute substitution）——无意识地用简单的判断替代了复杂的、困难的判断归因理论（Attribution theory） 显著性（Salience） 朴素现实主义（Naïve realism） 认知失调（Cognitive dissonance），相关的是： 印象管理（Impression management） 自我感知理论（Self-perception theory），也称自我知觉理论 启发式判断与决策（Heuristics in judgment and decision making），包括： 可得性启发（Availability heuristic）：记忆中容易想起的东西偏向于生动、不寻常和充满感情 代表性启发（Representativeness heuristic）：基于相似性来判断可能性 情绪性启发（Affect heuristic）：通过情绪反应来作出决策，而不是风险效益的考量 关于情绪（Emotion）的其他理论： 情绪二因素理论（Two-factor theory of emotion），也称情绪二因论 躯体标记假说（Somatic markers hypothesis） 内省错觉（Introspection illusion）统计（Statistics）的误解（Misinterpretations）与误用（Misuse），数字盲（Innumeracy） 2012年一份《心理学公报》（Psychological Bulletin）指出，至少有8种看似无关的偏误是由同一种信息理论（Information-theoretic）生成机制产生，它认为在人类记忆中存储与提取（Retrieval）信息时存在的杂乱信息处理过程造成了偏误。 转载自silensea的个人博客]]></content>
      <categories>
        <category>日知录</category>
      </categories>
      <tags>
        <tag>行为经济学</tag>
        <tag>认知偏误</tag>
        <tag>Cognitive biases</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理系列（7）：TextCNN调参技巧]]></title>
    <url>%2F2018%2F02%2F26%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%887%EF%BC%89%EF%BC%9ATextCNN%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[这篇文章翻译自A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification 近年来，卷积神经网络在句子分类任务上取得了显著的成绩(Kim, 2014;Kalchbrenner et al .,2014)，然而，这些模型要求从业者指定精确的模型结构与模型参数，例如，选择滤波器大小、正则化参数等等。目前尚不清楚对于句子分类的任务，不同的参数设定会对模型性能造成什么样的影响。因此，在这里我们对单层卷积神经网络进行情感分析，探索不同的参数对模型性能的影响；我们的目标是找出对语句分类来说的重要因素和不重要因素。由于一层的CNN结构简单，实验的表现也很好(Kim, 2014)，我们就着重使用这个模型来验证(而不同更复杂的模型)，从我们广泛的实证结果中得到一些了实用的建议，这些结果对于那些有兴趣用CNN对句子分类的人来说很有用。我们的实验结果所证实的一个重要结论是，研究人员应该记录性能差异，因为这可能是由于随机初始化或推理产生的。 一、Introduction在这个工作中，我们关注的是情感分类的重要任务。最近，研究表明，神经网络(CNNs)对这项任务的表现很好(Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015; Goldberg, 2015; Iyyer et al., 2015)。这类模型利用词的分布式表示，首先将包含每个句子转换成一个向量，从而形成一个矩阵作为输入给CNN(图1)。实证结果令人印象深刻。这些模型不需要太复杂就能实现强大的结果：例如，Kim(2014)提出了一种直接的单层CNN架构，它可以在多个任务中实现一致的(或类似的)结果。因此，现在有了令人信服的支持，更倾向于使用CNN而不是稀疏线性模型来进行句子分类任务。然而，CNN的一个缺点是，它们要求从业者指定要使用的精确模型架构，并设置超参数。同样的，做出这样的决定似乎是一种黑箱操作，特别是因为在模型中有许多“自由参数”可以探索。这与广泛用于文本分类的线性模型形成了鲜明的对比，例如正则化的逻辑回归和线性支持向量机(SVMs) (Joachims, 1998)。这样的模型特征通常是通过对文本的稀疏表示而产生的，并且需要相对较少的调优:通常只需要设置正则化项的系数(即:模型偏差)。使用训练数据进行线性搜索来确定参数是设置超参数的方法。 最近关于CNN的句子分类的研究，已经提供了用于实现报告结果的设置。然而这些参数设定是通过并非是特定的调参过程。但实际上，搜索CNN的参数空间是极其昂贵的，至少有两个原因:（1）训练这些模型的速度相对较慢，即使使用gpu。例如，在SST-1数据集(Socher et al.， 2013)中使用与(Kim, 2014)类似的配置，进行10倍交叉验证，需要1个小时。可能的模型架构和超参数空间是巨大的。例如，我们所讨论的简单的CNN架构，至少需要指定以下内容:输入的词向量表示;滤波器大小;特征图的数量;激活功能;池化策略;dropout比例(如果有的话);和l2范数的系数(如果有的话)。 实际上，对所有这些参数进行调优是不可行的，尤其是考虑到参数估计所需的运行时间。因此，我们的目的是要根据经验来确定那些需要花费精力进行调整的参数，以及那些在性能上无关紧要的，或者在特定的数据集上有“最佳”效果的参数。我们从前人对神经模型的经验分析中得到启发，该模型由Coates et al.(2011)和Breuel (Breuel, 2015)进行，研究了非监督特征学习效果的影响因素，以及随机梯度下降(SGD)超参数对训练的影响。在这里，我们考虑了模型结构的配置和单层CNNs的超参数值对句子分类任务的影响。我们报告了大量实验的结果，探索了不同的模型结构，运行了7个句子分类数据集。 二、背景和预备深度学习方法已在机器学习中得到很好的应用(LeCun et al.， 2015;Bengio,2009)。对于图像和语音处理任务来说，它们尤其成功(也很受欢迎)。然而，最近这些方法已经开始超越传统的自然语言处理(NLP)任务的线性模型(Goldberg, 2015)，这个领域的大部分兴趣都集中在如何得到分布式的词语表达(Bengio et al.， 2003;Mikolov et al.， 2013)并共同将这种“内部”表征嵌入到分类模型中(Col lobert and Weston, 2008;Collobert et al .,2011)或句子建模(Kalchbrenner et al.， 2014;Socher et al .,2013)。 在(Kalchbrenner et al.， 2014)中，作者构建了一个包含多个卷积层的CNN架构。他们的模型使用了动态k-max池。他们的模型假定潜在的、密集的、低维度的词向量(在推理之前初始化为随机值)。 Kim(2014)定义了一个更简单的架构，在相同的数据集上实现了类似的结果(Kalchbrenner et al.， 2014)。这个模型也将每个单词都表示为一个稠密的、低维的向量(Mikolov et al.， 2013)，他们使用预先训练的词向量，并考虑两种方法:静态和非静态。在前一种方法中，词向量被视为静态输入，而在后一种方法中，则动态调整为特定任务的词向量。 在其他地方，Johnson和Zhang(2014)引入了相似的模型，但改用了高维的one-hot向量表示。他们考虑了这一方法的两种变体，seq-CNN和bow-CNN。前者完全保留了顺序结构(以在非常高维的空间输入空间中操作的代价)，而后者保留了一些序列，但在小区域内丢失了顺序。他们的重点是更长的文本的分类，而不是句子(当然，这个模型也可以用于句子的分类)。Kim的体系结构相对简单——这与Johnson和Zhang(2014)所提出的基本相同，模块化的词向量——再加上在多个数据集上观察到的强大的经验性能，使得这是一个很有吸引力的句子分类方法。然而，在实践中，我们需要做一些模型架构决策和设置各种超参数。目前，很少有经验数据可以指导此类决定;解决这一差距是我们的目标。 2.1 CNN我们首先描述我们在本文中使用的相对简单的CNN架构。我们从一个标记化的句子开始，然后我们将它转换成一个句子矩阵，其中的行根据每个词得到的单词向量。例如，这些可能是谷歌word2vec (Mikolov et al.， 2013)或GloVe(Pennington et al.， 2014)模型的输出。我们用d表示向量的维数。如果给定句子的长度(即词汇数)是s,然后句子的维数矩阵s×d.接下来，我们可以有效地将句子矩阵作为一个“图像”，通过线性滤波器对它进行卷积操作。在NLP应用中，数据具有固有的顺序结构。直观上，因为行表示离散的符号(即单词)，所以使用宽度等于向量的维数的滤波器是合理的。（比如d)。然后我们可以考虑只改变滤波器的“高度”，它指的是共同考虑的相邻行数(词向量)。从这一点开始，我们将把滤波器的高度称为滤波器的区域大小。 假设有一个滤波器的参数化权向量$w∈R^{h×d}$和区域大小$h$;$w$包含要估计的$h·d$个参数。我们用$A\in R^{s×d}$表示句子矩阵,并使用$A[i,j]$代表从第$i$行到第$j$行的子矩阵。卷积算子的输出序列是通过对A的子矩阵进行重复的卷积操作而得到的: o_i = w·A[i:i+h-1 ] \ (1)其中，$i=1….s-h+1$，$.$表示子矩阵和滤波器的点积（先对对应元素做乘法，然后求和）,输出序列的长度为$s-h+1$。再加上一个偏置项$b$以及激活函数$f$得到对应的特征图$c\in R^{s-h+1}$： c_i=f(o_i+b)注意，我们可以使用多个滤波器来实现相同的区域大小，目标是每个滤波器从相同的区域学习互补的特性。也可以指定多个不同区域大小的过滤器(例如:“高度”)。 每个滤波器生成的特征图的维数，正好是句子长度和滤波区域大小的函数。然后，将一个池化函数应用到每个feature map中，以减少需要估计的参数的尺寸和数量。通常，池化操作为1-max池函数(Boureau et al.，2010b)，它从每个feature map生成一个一维特性。或者，可以将池化操作修改为在特征映射中在相同大小的区域内对每个区域对应的显著特征进行编码。每个滤波器映射生成的输出可以被连接到一个“顶部”特征向量，在1-max池的情况下它的大小将独立于单个的句子长度。然后通过一个softmax函数来生成这个表示，以生成最终的分类。在这个softmax层，可以选择应用“dropout策略”(Hinton et al.， 2012)作为正则化方法。这需要在向量中随机设置一些值为0。我们也可以选择施加l2范数约束，当它超过这个值时，将向量的l2范数线性扩展到一个指定的阈值。在训练过程中，最小化的目标是分类的交叉熵损失，估计的参数包括滤波器的权向量(s)、激活函数中的偏置项，以及softmax函数的权向量。请注意，我们可以选择固定词向量(我们将其称为“static”)或作为模型的附加参数，并在模型训练过程中调整(我们将把这种方法称为“non-static”)。我们探索了这两种变体。图1提供了一个简单的示意图，以说明刚刚描述的模型架构。 三、数据集我们使用同样的7个数据集(Kim, 2014)，简要总结如下: MR:句子极性数据集(Pang and Lee, 2005)。 SST-1: Stanford Sentiment Treebank (Socher et al.， 2013)。请注意，为了使输入表示在任务中一致，我们只对句子进行训练和测试。与之形成对比的是(Kim, 2014)，在这篇文章中，作者对短语和句子进行了训练。 SST-2:从SST-1派生而来，但只对两个类进行解析。我们只对句子进行训练和测试，不包括短语。 Subj:主观性数据集(Pang and Lee, 2005)。 TREC:问题分类数据集(Li and Roth, 2002)。 CR:客户审核数据集(Hu and Liu, 2004)。 MPQA:观点极性数据集(Wiebe et al.， 2005) 在表1中，我们报告了所有七个数据集的平均长度和标记化语句的最大长度。有关这些数据集的更多细节，请参考(Kim, 2014)。 四、baseline模型的性能为了给CNN的结果提供一个参考点，我们首先报告了使用稀疏正则化SVM进行句子分类的性能。我们使用unigram和bigram特性，只对所有数据集保持最频繁的3万个特征。我们还想通过将信息直接嵌入到这些模型中，来探索实现的相对收益。为此，我们用平均的单词向量(从谷歌word2vec3或GloVe4)来计算这个表达式，并计算出包含句子的单词，类似于(Lai et al.， 2015)中的方法。然后，我们使用RBF-kernel SVM作为在这个稠密特性空间中操作的分类器。我们还尝试将unigram, bi-gram和word2vec作为句子的特征，使用线性支持向量机作为分类器。我们通过嵌套的交叉折叠验证来优化正则化超参数，从而提高了精度。对所有的数据集都进行了十折交叉验证，结果如表2所示。为了保持一致性，我们对之前工作中描述的数据使用相同的预处理步骤(Kim, 2014)。从这些结果中可以立即发现的一件事是，将word2vec输出引入到特征向量中可以实现性能提升。 五、CNN情感分析我们现在报告的结果来自于我们的主要分析工作，目的是使用CNNs对句子情感分析，作为一个具体的架构和超参数设置的功能。为此，我们以baseline配置(如下所述)作为起点，该配置在之前的工作(Kim, 2014)中表现得很好。然后，我们依次探讨了修改该baseline配置组件的效果，并保持其他设置不变。 我们用“静态”和“非静态”两种词向量来进行实验。在前一种情况下，在训练过程中，单词向量不会被更新，而在后一种情况下，向量会不断调整。非静态配置优于静态配置。因此，本文只报告非静态结果，尽管我们提供了附录中静态配置的结果。 5.1 Baseline 参数设置我们现在考虑CNN的baseline模型配置的性能。具体来说，我们从之前工作中使用的模型架构和超参数开始(Kim, 2014)。为了将由于各种体系结构决策和超参数设置导致的性能差异置于环境中，必须严格评估参数估计过程中的差异。不幸的是，尽管有一个高度随机的推理过程，但大多数之前的工作并没有说明这样的差异。该方差可归因于随机梯度下降(SGD)、随机dropout和随机权值参数初始化的估计。我们表明，通过10倍交叉验证计算的平均性能在重复运行时表现出较高的方差。 我们首先使用表3中描述的原始参数设置，并为每个数据集复制实验100次，其中每一个复制都是一个10倍的CV，并且复制的折叠是固定的。表3中的“ReLU”指的是整流线性单元(Maas et al.， 2013)，这是CNN常用的激活函数。我们记录每个重复试验的10折交叉验证的平均精度，并报告超过100次重复试验的平均值、最小值和最大值。我们对静态和非静态方法都这样做。这提供了一种我们可以观察到的不改变模型的方差的感觉。结果如表4所示。图2提供了在所有数据集上对这两种方法的100次重复的平均精度的密度图。为了清晰显示，我们排除了SST-1，因为在这个数据集上，精度明显降低(但是，结果可以在表中找到)，由于我们对某些数据集进行了不同的分割和处理，正如前面所描述的那样，结果也与原来的不同。因为在这个工作中，我们只关心CNN的每个部分对性能的敏感性和影响，我们不太关心绝对的准确性，也不会比较我们在之前的作品中得到的结果。 在确定了CNNs的基准性能之后，我们现在考虑不同架构决策和超参数设置的影响。为此，我们保留所有其他的设置常量(如表3所示)，并且只改变感兴趣的组件。对于我们所考虑的每一个配置，我们重复实验10次，每一次实验都是10折交叉验证。就像原始参数设置的100次重复试验一样，我们也报告了10次10折交叉验证试验的平均均值、最小均值和最大值。对于所有的实验，我们对数据使用与(Kim, 2014)相同的预处理步骤。类似地，我们使用ADADELTA更新规则 (Zeiler, 2012)，并将minibatch大小设置为50。 5.2 word2vec句子分类模型的一个很好的特性是，它以分布式的词语作为输入的形式开始，这是一种灵活的结构，它可以在不同的预先训练的词向量中交换。因此，我们首先探讨了CNNs对所使用的输入表示的句子分类的敏感性。特别地，我们用Glove表示替换谷歌word2vec。谷歌word2vec使用了一个局部上下文窗口模型，从谷歌新闻(Mikolov et al.， 2013)中训练了1000亿单词，而GloVe则提出了一个模型，它利用了一个非常大的语料库(Pennington et al.， 2014)，利用全局单词的联合作用来统计数据。在本文中，我们使用了一个Glove版本，它是从一个包含8400亿个web数据标记的语料库中训练出来的，并且还有300个维度。我们保留所有其他设置与原始配置相同。我们的报告结果见表5。(请注意，我们还报告了SVM的结果，这些结果在表2中增加了平均Glove向量。) 作为获取对所有数据集最佳性能的潜在简单方法，我们还考虑了一种方法，该方法利用了这两种预先训练出来的表示方法。具体地说，我们将word2vec和Glove向量连接到每个单词，生成了600维的单词向量，我们将它们作为CNN的输入。预训练的向量可能并不总是适用于特定的单词(在word2vec或Glove中，或者两者都有);在这种情况下，我们随机初始化相应的子向量，如上所述。结果见表6。这里我们报告的结果只针对非静态变量，考虑到它的一般优势。 从这些结果中可以看出，使用Glove和word2vec时的相对性能取决于数据集，不幸的是，仅仅将这些表示连接起来并不一定有帮助。实际上，当面对一个新的数据集时，很可能需要使用训练数据来尝试不同的预先训练的单词向量。我们也尝试用长、稀疏的one-hot向量作为输入词表示(Johnson and Zhang, 2014)。在这个策略中，每个单词被编码成一个热矢量，它是一个稀疏的高维向量。在这种情况下，句子矩阵的宽度等于词汇量。在训练过程中，一个one-hot向量是固定的，因为这个方法就像它在一个预构建的字典中搜索每个单词一样。性能如表7所示。 将结果与word2vec和Glove的结果进行比较，我们可以看到在相同的CNN基本配置下，one-hot的性能比word2vec或Glove差。 我们不排除有特定配置的可能性，one-hot的CNN可能会比其他的输入表示的句子分类地更好。但我们这里的证据是，one-hot表示的CNN可能不适合句子分类。这可能是由于稀疏性;这些句子可能过于简短，不足以提供足够的信息来进行这种高维编码(而对于长文档来说，这可能不是一个问题)。 5.3 滤波器区域大小我们首先将区域大小设为1来看看滤波器区域大小的效果，我们将这个区域的feature map的数量设置为100(与原来的配置一样)。我们考虑区域大小为1、3、5、7、10、15、20、25和30，并记录每个区域大小的10倍交叉验证的平均值、最小值和最大精度，并将结果显示在表8中。 图3显示了每个区域大小和区域大小为3时的10次重复实验的平均精度之间的差异。因为我们只对精确的趋势感兴趣，因为我们改变了CNN的区域大小或其他组件(而不是每个任务的绝对性能)，我们只显示了从任意baseline的精度变化(这里，一个区域大小为3)。我们遵循本公约的所有数据，以方便解释。 从图中可以看出，每个数据集都有自己的最佳滤波区域大小范围。实际上，这表明在指定范围内执行粗网格搜索;这里的数据表明，句子分类的合理范围可能是2到25。然而，对于包含较长句子的数据集，例如CR(最大语句长度为105)，最优区域的大小可能更大。这也可能是由于在CR中，在更大的窗口下，更容易预测正面/负面的客户评论。 我们还探讨了合并多个不同的过滤器区域大小的效果，同时保持每个区域大小的feature map的数量为100。在这里，我们发现将几个过滤器与区域大小接近最佳的单一区域大小可以提高性能，但是在最优范围之外添加区域大小可能会损害性能。例如，从图3可以看出，MR数据集的最佳单个区域大小是7。因此，我们将几个不同的过滤器区域大小结合到这个最优范围内，并将其与在此范围之外使用区域大小的方法进行比较。从表9可以看出，使用(5,6,7)和(7,8,9)和(6,7,8,9)——靠近最佳单一区域大小的集合——产生最好的结果。当与(3,4,5)baseline设置比较时，差异尤其明显。注意，即使只使用单个良好的过滤器区域大小(这里为7)，结果也比组合不同的大小(3、4、5)更好。最佳的组合是简单地使用许多特征映射(这里为400)，以及所有区域大小等于7，即最好的区域大小。 但是，我们注意到在某些情况下(例如，对于TREC数据集)，使用多个不同的，但接近最优的区域大小表现最好。我们在表6的TREC数据集上使用几个区域大小提供了另一个示例性经验结果。从单个区域大小的性能来看，我们发现TREC的最佳单过滤区域大小是3和5，因此我们研究这些值附近的区域大小，并将其与使用多个区域大小的值进行比较。 这里我们看到(3,3,3)和(3,3,3)比(2,3,4)和(3,4,5)更差。然而，结果仍然表明，在最优的最佳区域尺寸附近的区域大小的组合比在最优的单一区域大小下使用多个区域的大小要好得多。此外，我们再次看到一个良好的区域大小(3)超过了几个次优区域大小(7、8、9)和(14、15、16)。 根据这些观察,我们认为这建议先进行粗线通过一个过滤器搜索区域大小找到最好的考虑数据集的大小,然后探索附近的几个区域大小的组合这最好的尺寸,包括结合不同的区域大小和副本的最优尺寸。 5.4 特征图数量我们再次保持其他配置不变，因此有3个过滤器区域大小:3、4和5。我们只更改每个相对于100的baseline的特征映射的数量。我们考虑大小10,50,100,200,400,600,1000,2000。报告结果如图4所示。 每个过滤器区域大小的“最佳”feature map数量取决于数据集。然而，增加超过600个feature map，充其量只能带来边际收益，而且往往会损害业绩(可能是由于过拟合)。另一个重要的事实是，当feature map的数量增加时，需要更长的时间来训练模型。实际上，这里的证据表明，搜索范围可能在100到600之间。注意，当一个人面临一个新的类似的句子分类问题时，这个范围只是提供一个可能的标准。当然，有可能在某些情况下，超过600个特征图是有益的，但这里的证据表明，花费精力去探索这一点可能是不值得的。 5.5 激活函数我们考虑了卷积层的七个不同的激活函数，包括:ReLU(根据baseline配置)，双曲正切(tanh)， Sigmoid函数(Maas et al.， 2013)， SoftPlus函数(Dugas et al.， 2001)， Cube function (Chen and Manning, 2014)和tanh Cube function (Pei et al.， 2015)。我们使用“Iden”来表示原本函数，这意味着不使用任何激活函数。表15展示了使用不同的激活函数在非静态CNN中实现的结果。在9个数据集中，最好的激活函数是Iden、ReLU和tanh。在只有一个数据集(MPQA)中，SoftPlus函数的性能超过了其他函数。Sigmoid、Cube和tanh数据集始终比其他激活函数执行得更糟糕。因此，我们在这里不报告结果。tanh函数的性能可能是由于它的零中心特性(与Sigmoid相比)。ReLU与Sigmoid相比具有非饱和形式的优点，并且已经观察到可以加速SGD的收敛(Krizhevsky等，2012)。一个有趣的结果是，不应用任何激活函数(Iden)有时会有所帮助。这表明在一些数据集上，一个线性变换足以捕获单词嵌入和输出标签之间的相关性。然而，如果存在多个隐藏层，则Iden可能比非线性激活函数更不合适。实际上，对于单层CNNs中激活函数的选择，我们的研究结果表明对ReLU和tanh进行了实验，也可能是Iden。 5.6 池化接下来我们研究了池化策略和池化区域大小的影响。我们将过滤器区域大小和特征映射的数量固定在baseline配置中，从而只改变池策略或池区域大小。在baseline参数设定中，我们对所有的feature map都使用了最大池化。得到长度为1的特征向量。但是，也可以在较小的相同大小的局部区域上执行池化操作，而不是在整个feature map (Boureau et al.， 2011)上执行。feature map上的每个小区域都经过池化操作生成单个数字，并且这些数字可以连接成一个feature map对应的特征向量。下面的步骤与1-max池相同:我们将所有的特征向量连接在一起，形成分类层的单个特征向量。我们试验了大小为3、10、20和30的局部区域，并发现1-max池比所有局部最大池配置的性能好。所有的数据集都呈现了这个结果。我们还考虑了类似于k-max池化的策略(Kalchbrenner et al.， 2014)，其中从整个feature map中提取了最大的k个值，并保留了这些值的相对顺序。我们对k进行了探索，发现1-max池的性能最好，始终优于k-max池。 接下来，我们考虑取区域的平均值，而不是区域的最大值(Boureau等人，2010a)。我们保留了其余的参数。我们尝试了区域大小为 的局部平均池化。我们发现，至少在CR和TREC数据集上，平均池化比最大池化的性能差(很多)。由于在平均池下观察到的性能和运行时间非常慢，所以我们没有对所有数据集进行完整的实验。我们对池化策略的分析表明，1-max池化对句子分类任务的效果总是优于其他策略。这可能是因为预测上下文的位置无关紧要，而句子中的某些n-grams可以比共同考虑的整个句子更具预测性。 5.7 正则化CNNs的两种常见正则化策略是dropout和l2范数。我们在这里探讨这些效应。从输入到倒数第一层应用Dropout。我们试验了从0.0到0.9的dropout比率，并根据baseline配置将l2范数约束固定到3。非静态CNN的结果如图5所示，0.5指定为baseline。我们也展示了当我们去掉了dropout和l2范数约束时(即不执行正则化时)的准确性，表示为None。另外，我们还考虑了l2正则对权重向量的影响，这些权重向量参数化了softmax函数。回想一下，当一个权重向量的l2范数超过这个阈值时，它是线性伸缩的，因此较小的c意味着更强的正则化。像dropout，这个策略只适用于倒数第一层。我们在图8中显示了不同c对非静态CNN的相对影响，我们将dropout率固定在0.5；3是这里的baseline模型的正则化参数，（再一次地，任意地)。 从图7和图8可以看出，根据数据集，非零的dropout比率可以帮助(尽管非常少)从0.1到0.5。但是，施加l2正则约束通常不会很大地提高性能(除了Opi)，甚至对至少一个数据集(CR)的性能产生负面影响。我们还研究了在增加feature map的数量时dropout比率效应。我们将每个过滤器大小的feature map的数量从100增加到500，并将max l2正则约束设置为3。dropout比率的影响如图7所示。我们看到，drouout比率的影响几乎和特征图的数量是100的时候一样，而且没有多大帮助。但是我们观察到，对于数据集SST-1来说，当它是0.7时，dropout比率实际上是有帮助的。从图4可以看出，当feature map的数量大于100时，可能由于过拟合而影响了性能，所以在这个情况下dropout将会减轻这种影响。 我们也尝试了只在卷积层上应用“dropout”，但仍然将分类层的最大标准约束设置为3，使所有其他设置完全相同。这意味着我们在训练时随机将句子矩阵的元素设置为0，然后在测试时将p与句子矩阵相乘。从图8中可以看出，dropout比率对卷积层的影响如图8所示。我们再次看到，在卷积层上运用dropout帮助很小，而且很大的dropout率极大地伤害了性能。 总之，与现有的一些文献(Srivastava et al.， 2014)相反，我们发现dropout对CNN的表现没有什么好处。我们将这一现象归因于一层CNN的参数数量比多层深度学习模型要小。另一种可能的解释是，使用词嵌入有助于防止过拟合(与基于单词的编码相比)。然而，我们并不是主张完全放弃正则化。实际上，我们建议将dropout率设置为一个小的值(0 -0.5)，并使用一个相对较大的max正则约束，同时增加feature maps的数量，以查看更多的特性是否会有所帮助。当进一步增加feature map的数量似乎会降低性能时，增加dropout比率可能是值得的。 六、结论我们对CNNs的句子分类进行了广泛的实验分析。我们总结了我们的主要发现，并从这些实际的指导中总结出了研究人员和实践者在现实场景中使用和部署cnn的方法。 6.1 主要实证结果的总结 以前的工作往往只报告模型实现的数据集的平均性能。但是，这种忽略方差完全是由于随机推理过程所使用的。这可以是相当大的:保持所有的常数(包括折叠)，因此方差是完全由随机推理过程决定的，我们发现，平均精度(通过10倍交叉验证计算)的范围可以达到1.5个点。在irony数据集上，AUC的范围甚至更大，达到3.4分(见表3)。在将来的工作中应该进行更多的复制，并且应该报告范围/方差，以防止可能的关于相对模型性能的错误结论。我们发现，即使将它们调到手边的任务，输入词向量表示(例如，在word2vec和Glove之间)的选择对性能有影响，但是不同的表示对不同的任务有更好的表现。至少对于句子分类来说，两者似乎都比直接使用one-hot向量要好。 然而,我们注意到:(1)如果有一个足够大量的训练数据，结果可能就不是这样,以及(2)与这里的简单版本相比，最近由约翰逊和张提出的semi-supervised CNN模型(Johnson and Zhang, 2015)可能提高性能(Johnson and Zhang, 2014)。 过滤区域的大小对性能有很大的影响，应该进行调整。 feature map的数量也可以在性能上扮演重要的角色，增加feature map的数量会增加模型的训练时间。 1-max池一致优于其他池化策略。 正则化对模型的性能影响较小。 6.2对从业人员的具体建议根据我们的经验结果，我们提供了关于CNN架构和超参数的指南，为那些希望在句子分类任务中部署cnnn的从业者提供参考。 考虑从表2中描述的基本配置开始，使用非静态word2vec或Golve，而不是one-hot矢量。但是，如果训练数据集的大小是非常大的，那么使用one-hot向量来探索可能是值得的。或者，如果一个人能够访问大量未标记的域内数据(Johnson和Zhang, 2015)，也可能是一个选项。 通过对单个过滤器区域大小的线性搜索来找到“最佳”的单一区域大小。一个合理的范围可能1至10。然而，对于像CR这样的长句的数据集，可能值得探索更大的过滤器区域大小。一旦确定了这一“最佳”区域大小，就可能值得探索将多个过滤器组合在一起，使用区域大小接近这个最佳大小的区域，因为根据经验，多个“好”区域大小总是优于仅使用单一最佳区域大小。 将每个过滤器区域的特征映射的数量从100个更改为600个，并且在这个过程中，使用一个小的dropout比率(0 -0.5)和一个大的max正则约束。注意，增加feature map的数量会增加运行时间，因此需要权衡考虑。还要注意发现的最佳值是否在距离边界附近(Bengio, 2012)。如果最好的值接近600，那么尝试更大的值可能是值得的。 如果可能，考虑不同的激活函数:ReLU和tanh是最好的整体候选。对于我们的一层CNN来说，完全没有激活函数是值得的。 使用1-max池;似乎没有必要花费资源来评估替代战略。 关于正则化:当增加feature map的数量开始减少性能时，试着施加更强的正则化，例如，dropout比率大于0.5。 在评估模型的性能(或其特定配置)时，必须考虑方差。因此，应该重复交叉折叠验证过程，并考虑方差和范围。 当然，以上建议仅适用于包含有相似属性的句子的数据集。也许有一些与我们的发现背道而驰的例子。尽管如此，我们相信这些建议可能会为研究人员或实践者提供一个合理的起点，他们希望将简单的一层CNN应用到现实世界的句子分类任务中。我们强调，我们选择了这个简单的单层CNN，根据观察到的强大的经验性能，它将它定位为一个新的baseline模型，类似于词袋SVM和逻辑回归。因此，在实施更复杂的模型之前，应该考虑这种方法。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>static</tag>
        <tag>word2vec</tag>
        <tag>TextCNN</tag>
        <tag>Dropout</tag>
        <tag>non-static</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理系列（6）：TextCNN]]></title>
    <url>%2F2018%2F02%2F24%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%886%EF%BC%89%EF%BC%9ATextCNN%2F</url>
    <content type="text"><![CDATA[这篇文章翻译自卷积神经网络用于文本分类的开山之作Convolutional Neural Networks for Sentence Classification，并相应的对其实现代码进行讲解。 我们进行了一系列关于卷积神经网络(CNN)的实验，这些实验是基于预先训练的词向量训练的，用于句子级别的分类任务。我们得到了一个简单的参数微调的以及静态矢量的CNN在多个基准上取得了优异的结果。通过微调学习任务特定的向量可以进一步提高性能。我们还建议对体系结构进行简单的修改，以允许使用特定于任务的和静态的向量。CNN在这里讨论了7项任务中4项的改进，包括情绪分析和问题分类。 一、导语近年来，深度学习模式在计算机视觉(Krizhevsky et al.， 2012)和语音识别(Graves et al.， 2013)中取得了显著的成绩。在自然语言处理中，许多深度学习方法都涉及通过神经语言模型学习词向量表示(Bengio et al.， 2003;Yih et al .,2011;Mikolov et al.， 2013)，并通过学习的词向量进行分类(bert et al.， 2011)。词向量，在单词中从一个稀疏的1- V编码(这里V是词汇量)到一个较低维度的向量空间，通过一个隐藏层，本质上是特征提取器，在它们的维度中编码词汇的语义特征。在这样稠密的表示法中，语义上相近的词在欧几里得或余弦距离上也同样接近（低维向量空间）。 卷积神经网络(CNN)利用具有卷积滤波器的层来应用于局部特征(LeCun等人，1998)。开端于计算机视觉的CNN模型后来被证明在自然语言处理中也很有效，在语义解析(Yih et al .,2014)、搜索查询检索(沈et al .,2014)、句子建模(Kalch -布伦纳et al .,2014)和其他传统NLP任务(Collobert et al .,2011)都取得了非常好的效果。 在目前的工作中，我们训练了一个简单的CNN，它只有一层卷积层，前面连接由无监督的神经语言模型训练的的词向量。这些向量是由Mikolov等人(2013)在谷歌新闻的1000亿字上进行训练得到，并已经公开了。我们首先保持向量静态，只学习模型的其他参数。即使对超参数进行了微调，这个简单的模型在多个基准上也都取得了出色的结果，这表明预先训练的词向量是“通用”的特征构造器，它们可以用于各种分类任务。通过微调，学习特定任务的向量可以得到进一步的改进。我们最后描述了对体系结构的一个简单的修改，允许使用多个通道来使用预先训练的和任务特定的向量。 我们的工作与Razavian et al.(2014)类似，它表明，对于图像分类，从预先训练的深度学习模型获得的特征提取器在各种任务中都表现良好，包括与最初的任务不同的任务，这些任务的特征提取器是经过训练的。 二、模型 图1所示的模型架构是CNN架构的一个小变体。$x_i\in R^k$表示一个句子中第$i$个词的$k$维词向量。一个长度为$n$的句子（可以padding）表示为 x_{1:n}=x_1⊕x_2⊕...⊕x_n \ (1)其中⊕是连接操作符。一般来讲，$x_{i:i+j}$表示词$x_i,x_{i+1}···，x_{i+j}$的连接。卷积运算涉及到一个滤波器，它被应用到一些单词的窗口，以产生一个新特征。比如，特征$c_i$产生于一个词汇窗口$x_{i:i+h-1}$，计算公式为： c_i=f(w·x_{i:i+h-1}+b) \ (2)$b∈R$是偏置项，$f$是一个非线性函数,例如双曲正切函数。该滤波器应用在句子中${x_{1∶ h}, x_{2∶h+1},…,x_{n+h-1∶n}}$每一个可能的单词窗口从而产生一个特征映射： c=[c_1,c_2,…,c_{n+h-1}]这里$c∈R^{n-h+1}$。然后我们对特征映射采用最大池化策略（Collobert et al.， 2011）即取最大的值$\hat c=max{c}$作为对应此滤波器的特征。此思路是去捕获最重要的特征——每个特征映射中最大的值。最大池化可以处理不同的句子长度。 我们已经说明了通过一个滤波器抽取一个特征的过程。当然使用多个滤波器（不同的窗口大小）可以获取多个特征。这些特征组成了倒数第二层并且传给全连接的softmax层，输出标签的概率分布。 在其中一个模型变种中，我们做了将词向量分两个“通道”的实验，一个通道中的词向量在模型训练的过程中保持不变，另一个通过BP算法（3.2节）进行细粒度的调节。在多通道架构中，如图1所示，每个滤波器应用在两个通道，再把结果加起来，然后用等式（2）计算$c_i$。除此之外模型等价于单通道的架构。 2.1 正则化为了解决过拟合的问题，我们采用正则化的方法，在倒数第二层我们使用dropout机制，并且使用L-2范数来约束权重向量。Ｄropout机制可以防止隐藏层的过拟合，通过随机的dropout：例如在反向传播的过程中将p部分的隐藏单元设置为０。即，已知，倒数第二层的特征向量z=[C1,……，Cm] (表示这里我们有m个滤波器)。经典的情况神经元的输出应该是： y=w·z+b (4)在前向传播中对输出单元y，dropout使用的是： y = w · (z ◦ r) + b \ (5)这里∘表示按元素逐个相乘操作，$r∈R^m$是一个“掩盖”向量，向量中的元素都是一个伯努利随机变量，有p的概率变为1。梯度仅仅可以通过非掩盖的单元反向传播。在测试阶段，权重向量通过因子p缩减例如$\hat w=pw$,并且$\hat w$被用来（没有使用dropout）给掩盖的句子打分。我们另外限制权重向量的二范式，在每一步梯度下降之后，如果$‖w‖_2&gt;s$，重新将w的二范式设置为$‖w‖_2=s$。 三、数据集和实验步骤 表1：分词之后的数据集的简要统计。C：目标类的个数。l：平均句子长度。N：数据集大小。|V|：单词总数。|V_pre |：出现在预训练词向量中词的个数。Test：测试集的大小（CV意味没有标准的训练/测试集并且采用十折交叉验证的方法） 我们在不同的基准上测试我们的模型。数据集的简要统计如表1所示。 MR: 一句话的电影评论。分类标签分为积极/消极的评论（Pang and Lee, 2005）。 SST-1：斯坦福情感树库——MR数据的扩展，但是包含train/dev/test数据集的划分及细粒度的标签（非常积极、积极、中立、消极、非常消极），被Socher et al.（2013）重新标记。 SST-2: 和SST-1一样，但是没有中立的评论，只有积极和消极两种标签。 Subj：主观数据集，任务是去划分一个句子是主观性的还是客观性的（Pang and Lee. 2004）。 TREC：TREC问题数据集——任务涉及到将一个问题划分为六种问题类型（人，位置，数值信息等）（Li and Roth, 2002）。 CR：不同产品（照相机、MP3s等等）的客户评论。任务是预测积极/消极的评论（Hu and Liu, 2004）。 MPQA：MPQA数据集（Wiebe et al., 2005）的观点极性检测子任务。 3.1 超参数和训练过程我们在所有的数据集中都采用下列参数： 对于所有的数据集我们使用修正线性单元(Rectified linear units)； 滤波器窗口大小h为3、4、5， 每种滤波器个数为100。； dropout的比例为0.5； l2正则化限制权值的大小为３； mini-batch大小为50； 这些值都是在SST-2 验证数据集上通过网格搜索选择得到的。 我们除了在验证集上进行early stpping外没有另外进行任何特定数据集的调节，对于没有标准验证集的数据集，我们从训练数据集中随机选择10%的数据作为验证集。通过采用Adadelta更新参数（Zeiler, 2012）及随机mini-batches策略的随机梯度下降算法进行训练。 3.2 预训练的词向量在没有大量监督训练集（Colobert et al., 2011; Socher et al., 2011; Iyyer et al., 2014）的情况下，使用从非监督神经语言模型训练得到的词向量进行初始化是用来提升结果的流行方法。我们使用公用的、由10亿Google 新闻数据中训练出来的Word2vec词向量。此向量的维度是300并且是采用连续的词袋架构（Mikolov et al., 2013）训练出来的。而将没有出现在预训练词向量中的单词随机初始化。 3.3 模型变种我们使用以下模型变种进行实验。 CNN-rand：我们的基准模型，所有的词被随机初始化，并在训练的工程中进行调节。 CNN-static：使用预训练的词向量——Word2vec。所有的词——包括随机初始化的未出现在预训练词向量中的词——保持不变而仅仅调节模型其它的参数。 CNN-non-static：和CNN-static相似，但是预训练的词向量在每个任务中会有细粒度的调节。 CNN-multichannel：有两个词向量集合的模型。将每个向量集合看作一个“通道”并且每个滤波器应用在所有的通道，但是梯度只能通过其中一个通道进行反向传播。因此，模型能够细粒度的调节其中一个向量集合，而保持另外一个不变。 为了探究上述变种对其它随机因子的影响，我们消除了其它随机化的影响——交叉验证次数赋值，未知词向量的初始化，CNN模型参数的初始化——在每个数据集上保持一致。 四、结果和讨论 RAE：由维基百科(Socher et al.， 2011)中得到预先训练的词向量的递归自编码器。 MV-RNN：句法分析树的矩阵-向量递归神经网络(Socher et al.， 2012)。 RNTN：递归神经张量网络，具有基于时态的特征函数和句法分析树(Socher et al.， 2013) DCNN: k-max汇聚的动态卷积神经网络(Kalchbrenner et al.， 2014) Paragraph-Vec：在段落向量之上的逻辑回归(Le and Mikolov, 2014) CCAE: 组合类自动编码器与组合类语法运算符(Hermann and Blunsom, 2013) Sent-Parser::情绪分析特定解析器(Dong et al.， 2014) NBSVM, MNB: 朴素贝叶斯SVM和多项朴素贝叶斯。 G-Dropout, F-Dropout: 高斯Dropout和快速Dropout Tree-CRF: 条件随机场依赖树(中川等，2010) CRF-PR：后正则化条件随机场(Yang and Cardie, 2014) SVMS: SVM使用uni-bi-trigrams、wh word、head word、POS、parser、hypernyms和60个手工编码的规则作为特性。 表2为我们模型和其它模型的结果对比。我们随机化所有词向量的基准模型（CNN-rand）就其本身而言表现的不是很好。然而我们期望通过预训练的词向量来提升效果的模型，我们对效果提升的幅度感到很吃惊。即使使用静态向量的简单模型（CNN-static）表现的相当好，产生了与利用复杂池化模式的复杂深度学习模型（Kalchbrenner et al., 2014）和需要提前计算解析树的模型（Socher et al., 2013）可抗衡的结果。这些结果说明预训练的词向量是好的，它可以作为“通用”的特征抽取器，并且可以跨数据集使用。对每个任务细粒度的调节词向量可以进一步提升结果（CNN-non-static）。 4.1 多通道与单通道模型比较我们一开始期望多通道的架构可以避免过拟合（通过确保学到词向量不会偏离初始值太远），并且比单通道的模型效果好，尤其在更小的数据集上。然而，结果是含混的，进一步的微调工作是有必要的。例如，与其使用非静态部分的额外通道，还可以维护单个通道，但使用额外的维度，在训练期间允许修改。 4.2 静态与非静态表示比较与单通道非静态模型一样，多通道模型能够对非静态通道进行微调，以使其更特定于任务。例如，good在word2vec中最类似于bad，大概是因为它们(几乎)在语法上是等价的。但是对于在SST-2数据集上的非静态信道中的向量来说，情况并不是如此(表3)。同样地，可以说在表达情感上，nice和great相比，与good更相近，这在训练的词向量中被真实的反映出来。 对于（随机初始化）不在预训练向量集合中的词，细粒度的调节允许它们学到更有意义的表示：网络训练得到感叹号经常与热情洋溢的表达联系在一起，并且逗号经常和连接副词联系在一起。 4.3 进一步观察我们做了进一步的实验和观察。 Kalchbrenner et al.（2014）使用和我们单通道架构相同的CNN架构，但是却得到了不好的结果。例如，他们采用随机初始化词向量的Max-TDNN（时间延迟神经网络）模型在SST-1数据集上获得的结果是37.4%，和我们模型获得的45%相比。我们将此差异归因于我们的CNN有更大的容量（多个滤波器和特征映射）。 Dropout是如此好的正则化方法以致于用在一个比必要网络更大的网络上并且仅仅使用dropout去正则化效果是好的。 Dropout一般可以将结果提升2%-4%。 当随机初始化不在Word2vec集合中的单词时，向量的每一维从U[-a,a]采样，结果获得了微小的提升，这里的a的选择要使随机初始化的词向量的方差和预训练的词向量有相同的方差。在初始化的过程中，尝试应用更复杂的方法反映预训练词向量的分布，看能否使结果得到提升是很有意思的事情。 我们在Collobert et al.（2011）通过维基百科训练的、另一个公用的词向量上进行试验，并且发现使用Word2vec的结果要远远优于它。目前不清楚是因为Mikovlov et al.（2013）的架构还是因为10亿的Google新闻数据集。 Adadelta（Zeiler，2012）和Adagrad（Duchi et al., 2011）的结果相似，但是需要更少的训练次数。 五、结论在这篇论文中，我们使用卷积神经网络和Word2vec进行了一系列的实验。尽管微小的超参数调节，具有一个卷积层的简单CNN就表现的非常好。我们的实验再次证明：将深度学习应用在NLP领域，非监督预训练的词向量是一个非常重要的因素。 六、代码解释待完成]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>static</tag>
        <tag>word2vec</tag>
        <tag>TextCNN</tag>
        <tag>Dropout</tag>
        <tag>non-static</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理系列（5）：FastText]]></title>
    <url>%2F2018%2F02%2F20%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%885%EF%BC%89%EF%BC%9AFastText%2F</url>
    <content type="text"><![CDATA[这篇文章翻译自Bag of Tricks for Efficient Text Classification 本文探讨了一个简单而有效的文本分类baseline。我们的实验表明，快速文本分类器在精度上通常与深度学习分类器相当，并且在训练和评估方面的速度要快得多。我们可以在不到10分钟的时间内使用标准的多核CPU对超过10亿单词的快速文本进行训练，在不到一分钟的时间内对312000类的50万个句子进行分类。 一、Introduction文本分类是自然语言处理中的一项重要任务，它有很多应用，比如web搜索、信息检索、排序和文档分类等。最近，基于神经网络的模型变得越来越流行(Kim, 2014; Zhang and LeCun, 2015; Conneau et al., 2016)。虽然这些模型在实践中取得了很好的性能，但是它们在训练和测试时都比较慢，这限制了它们在非常大的数据集上的使用。 与此同时，线性分类器通常被认为是文本分类问题的强大baseline(Joachims, 1998; McCallum and Nigam, 1998; Fan et al., 2008)。尽管它们很简单，但如果使用正确的特性，它们通常会获得最先进的性能(Wang and Manning, 2012)。它们也有可能扩展到非常大的语料库。 在这篇文章中，我们探讨了在文本分类的背景下，将这些baseline扩展到一个很大的输出空间的大型语料库。受最近的高效词汇表示法的启发(Mikolov et al.， 2013;Levy et al.， 2015)，我们展示了具有秩约束和快速损失逼近的线性模型，它在10分钟内可以训练10亿个单词，同时达到与最先进的水平相同的性能。我们在两个不同的任务上评价我们的方法FastText的质量，即标记预测和情绪分析。 二、Model architecture一个简单而有效的文本分类baseline是将文本描述成bag of word (BoW)，训练一个线性分类器，例如，逻辑回归或SVM(Joachims, 1998; Fan et al., 2008)。然而，线性分类器并不在特征和类别之间共享参数。这可能限制了它们在大的输出空间环境中的泛化，在这个大的输出空间中，一些类别的样本可能非常少。解决这个问题的常用方法是将线性分类器分解成低秩矩阵(Schutze, 1992;Mikolov et al., 2013)或者使用多层神经网络 （Collobert and Weston, 2008 ; Zhang et al., 2015)。 图1显示了一个具有秩约束的简单线性模型。第一个权重矩阵A是对单词的查找表。然后对文本的中所有词向量求平均得到一个文本的表示，依次进入线性分类器。这些文本表示是一个潜在的可重用的隐藏变量。这个架构类似于Mikolov et al.(2013)的cbow模型，只是它中间的单词被一个标签代替。我们使用将$softmax$函数$f$计算预定义类的概率分布。对一组N个文档，最小化负对数似然函数 -\frac{1}{N}\sum_{n=1}^Ny_nlog(f(BAx_n))其中$x_n$是第$n$个文档的标准化特征，$y_n$是类别标签，$A$和$B$是权重矩阵。该模型通过随机梯度下降和线性衰减学习率在多个cpu上进行异步训练。 2.1 Hierarchical softmax当类的数量很大时，计算线性分类器的计算代价很高。更准确地说，计算复杂度是$O(kh)$，$k$是类的数量，$h$是文本表示的维度。为了提高我们的运行时间，我们在Huffman编码树(Mikolov et al.， 2013)的基础上使用了一个分层的softmax (Goodman, 2001)。在训练过程中，计算复杂度下降到$O(h log_2(k))$。在搜索最有可能的类时，分层的softmax在测试时也是有利的。每个节点都与从根到该节点的路径的概率相关。如果深度$l+1$的节点，它的父节点为$n_1$、$n_2$、$n_l$，则它的概率为 P(n_{l+1})=\prod _{i=1}^lP(n_i)这意味着子节点的概率总是低于其父节点的概率。探索树的深度优先搜索和跟踪最大概率的叶子允许我们丢弃任何与一个小概率相关的分支。在实践中，我们观察到在测试时将复杂度降低到$O(hlog_2(k))$。使用二进制堆，这种方法进一步扩展到以$O(log(T))$的代价计算T-top目标。 2.2 N-gram features词袋中的词顺序是固定的，但明确地把这个顺序考虑进去通常是非常昂贵的。相反，我们使用一个n-grams作为额外的特征，以获取关于局部词序的部分信息。这在实践中非常有效，同时实现了与明确使用顺序的方法(Wang and Manning, 2012)差不多的结果。 我们通过一个哈希（hashing）技巧维持了快速且有效的 n-gram 映射（Weinberger et al., 2009），这个哈希技巧带有与 Mikolov et al.（2011）论文中相同的哈希函数和 10M bins（如果我们仅使用 bigram 的话），否则就需要 100M bin. 三、Experiments我们在两个不同的任务上评估fastText。首先，我们将其与现有的文本分类器对情绪分析问题进行比较。然后，我们评估其在标记预测数据集上扩展到大的输出空间的能力。请注意,我们的模型可以用Vowpal Wabbit库来实现,但是在实践中我们看到,我们的实现至少有2到5倍的提升。 3.1 Sentiment analysis Datasets and baselines 我们采用了同样的8个数据集，并采用了Zhangetal(2015)的评价协议。我们比较了了Zhang et al.(2015)的n-gram和TF-IDF 的baseline，以及Zhang and LeCun(2015)的字符级卷积模型(char-cnn)，基于字符的卷积循环网络(char-CRNN) (Xiao and Cho, 2016)和Conneau等人(2016)的深度卷积网络(VDCNN)。我们还比较了Tang等人(2015)的评估方案。我们比较了他们的主要baseline，以及基于循环网络的两种方法(Conv-GRNN和LSTM-GRNN)。 Results 我们在图1中展示了结果。我们使用10个隐藏单元，并在5个epochs中运行fastText，其学习率从{0.05,0.1,0.25,0.5}中选择。 在这个任务中，添加bigram信息可以提高1-4%的性能。总体而言，我们的准确性略好于char-CNN和char-CRNN，差于VDCNN。请注意，我们可以通过使用更多的n-gram来略微提高精确度，例如使用trigrams，在搜狗数据集上的性能达到了97.1%。最后，图3显示了我们的方法与Tang et al.(2015)中的方法相竞争。 我们对验证集上的超参数进行调优，并观察到使用n-grams达到5时性能最佳。与Tang et al.(2015)不同，fastText不使用预先训练的词嵌入，这可以解释1%的准确性差异。 Training time CNN和VDCNN都用NVIDIA Tesla K40 GPU训练，而我们的模型在CPU上使用了20个线程。表2显示使用卷积的方法比fastText慢几个数量级。虽然利用10倍的速度提升的CUDA来加速char-CNN的训练，FastText花了不到一分钟的训练。Tang et al.(2015)的GRNNs方法在CPU上以单个线程的时间为12小时左右。随着数据量增加，与神经网络方法相比，我们的模型加速至少达到了15000倍 3.2 Tag prediction Dataset and baselines. 为了测试我们的方法的可扩展性，在YFCC100M数据集(Thomee et al.， 2016)上进行了进一步的评估，该数据集包含了近1亿张带有说明、标题和标签的图像。我们专注于根据标题和标题预测标签(我们不使用图像)。我们删除了出现少于100次的单词和标记，并将数据分成训练集、验证集和测试集。该训练集包含91,188,648个样本(1.5B tokens)。验证集有930497个样本，测试集有543,424个样本。词汇量为297,141，有312,116个独特的标签。我们将发布一个脚本，重新创建这个数据集，以便可以复制我们的数据。我们报告的精度为1。 我们考虑一个基于频率的baseline，它预测最频繁的标记。我们也比较了Tagspace (Weston et al.， 2014)，这是一个类似于我们的标签预测模型，但是基于Weston et al.(2011)的Wsabie模型。虽然Tagspace模型是使用卷积来描述的，但是我们考虑的是线性版本，它实现了类似的性能，但是速度要快得多。 Results and training time 表5给出了fastText和baseline的比较。我们运行5个epochs的fastText，并将其与隐藏层的两个不同尺寸的Tagspace进行比较，即50和200。这两种模型都实现了一种类似的性能，它有一个隐藏的小层，但是添加了bigrams，这就大大提高了精度。在测试时，Tagspace需要计算所有类的分数，这使得它相对较慢，而我们的快速推理在类的数量很大(这里超过300K)时，会有显著的加速增长。总的来说，我们的模型速度超过了一个数量级，且获得了更高质量的效果。测试的加速阶段更显著(600倍加速)。表4显示了一些定性的例子。 四、Discussion and conclusion在本文中，我们提出了一种简单的文本分类baseline方法。不像从word2vec中训练出来的不受监督的单词向量，我们的单词特征可以被平均起来组成一个好的句子表示。在一些任务中，fastText获得的性能与最近提出的以深度学习为灵感的方法相比，速度更快。虽然深层神经网络在理论上比浅层模型具有更高的表征能力，但尚不清楚简单的文本分类问题，如情绪分析是否是正确的评价方法。我们将发布我们的代码，以便研究社区能够轻松地构建我们的工作。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
        <tag>FastText</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（21）：维特根斯坦论语言的限度（7）—— 哲学的意义]]></title>
    <url>%2F2018%2F02%2F17%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8821%EF%BC%89%EF%BC%9A%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E8%AE%BA%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6%EF%BC%887%EF%BC%89%E2%80%94%E2%80%94%20%E5%93%B2%E5%AD%A6%E7%9A%84%E6%84%8F%E4%B9%89%2F</url>
    <content type="text"><![CDATA[只有把哲学当作天下公器的时候，我们的哲学才有生命力，所以我理解的哲学是可以处理一切问题的，它不仅仅可以处理我们所面对的问题，也可以处理我们所没有面对到的问题。所以一定要把哲学看作是一个可以被适用在不同的对象上的一种工作方法，它是我们思考问题的一种方式，会带动我们去分析问题，甚至带动我们去想办法解决问题 其实我研究维特根斯坦的目的并不是要去让大家都像维特根斯坦一样，我们研究一个哲学家的目的，第一个是要了解他，第二个要通过他去了解他所给我们提供的这样一种哲学的思想方法，我们能够从中学到什么。我们当然也可以批评质疑他的一些观点，但是一定要有根据，因为维特根斯坦所有的思想都有自己的理路，如果不懂他的理路去批评他，最后可能就是一个堂吉诃德式的风车之战，你根本不是真的在跟维特根斯坦作战，所以一定要了解他，这是最基本的。但是很少有人真的去了解维特根斯坦，或者说我是按照维特根斯坦的方式去做，很少有人这样，所以这是我为什么要通过各种途径来讲解维特根斯坦，让维特根斯坦让成为大家所知道的人物，并且让大家去阅读维特根斯坦，让维特根斯坦的思想方式成为我们所熟知的方式。我们不一定按照维特根斯坦的方式去思考问题，但是我们一定要了解维特根斯坦的思维方式，这是我们了解当代哲学一个重要的组成部分，这也是我的目的之一。 别人问我江老师你是做分析哲学吗？我说我不知道。也有人问我，如果你是做分析哲学，为什么不像分析哲学家那样对一些具体的问题做出一些解读，对一些概念做逻辑上的推理？我其实做了很多的概念分析，包括真理、意义这些概念我都发过文章。他们之所以问这个问题，是因为从我的文章当中，他们看出我的思想倾向，认为我可能不像搞分析哲学的，更像搞大陆哲学的。一个搞分析的人怎么会讲拓扑学，做大陆哲学才会讲到拓扑概念。现在英国有一些哲学家他们专门做欧洲大陆的哲学家，他们做海德格尔拓扑学，有的做伽德默尔拓扑学，他们做的风格跟我做的风格不一样，我是做Philosophical Topology，我做的是哲学拓扑学，哲学拓扑学跟我们通常讲的数学的、几何学的、集合论的，或者说代数拓扑有直接关系，我更多是科学意义上来谈拓扑。 我对于整个哲学史的把握要比现在纯粹做分析的人要有优势，因为这么多年从古希腊到当代，我一直在给学生讲课，所以我对历史非常熟悉，我自己也正在写西方哲学史。我对当代哲学，欧洲大陆哲学也是非常熟的，从胡塞尔一直到德里达、福柯，读他们作品，通过阅读更多地了解他们的工作性质，我可以不同意他们的观点，但是绝不能够不读他们的书。但是现在很多搞分析的不读这些书，这就是最大的麻烦，也是有失公允的一种做法，所以我们首先需要了解。我觉得这是我们做学问的一种方法，通过广泛的了解再深入地进入讨论问题的方式。这些所有的哲学家的观点其实都是我们的一种资源，它虽然并不能够成为我们思想的一个出发点，但是通过这种思想资源我们可以了解我们究竟应当如何去做哲学。 我是一个无立场的哲学家，我既不是一个维特根斯坦的哲学家，也不是一个分析哲学家，也不是欧洲大陆哲学家，更不是中国哲学家，我是一个没有国籍没有立场的哲学家，因为哲学我把它当作天下公器，并且只有把哲学当作天下公器的时候，我们的哲学才有生命力，所以我理解的哲学是可以处理一切问题的，它不仅仅可以处理我们所面对的问题，也可以处理我们所没有面对到的问题，因为哲学只有在这个高度上，我们才能够真正理解哲学的价值。如果把哲学只限于解决某些具体的问题上，哲学的生命力或者价值就非常有限的，所以一定要把哲学看作是一个可以被适用在不同的对象上这样一种工作的方法，所以哲学是一种工作方法，或者说它是我们思考问题的一种方式，而这种思考问题的方式，会带动我们去分析问题，甚至带动我们去想办法解决问题。所以哲学不仅仅是对问题的讨论，甚至不仅仅是提出问题，我个人理解哲学应当能够解决一些问题。并不是像维特根斯坦在这本书里面说他一劳永逸地解决了哲学问题，但是它应当能够解决一些我们所讨论的某一些问题，解决的意思并不是说我能够彻底把这个问题完全让它成为在现实中，实践当中可以加以考量的问题，而是说能够消除我们思想上的困难，有一点像是每一次我的课程结束的时候，学生都会找我提问题，每一次我给学生解读完了我的理解之后，学生满意的离开了，这个时候我就想，问题解决了。哲学就是干这个事情，你能够让他的思想上解决问题，这个时候哲学的价值就出来了。当然会不断有新的问题产生，解决完这个问题，又有新的问题出来，可以进一步来解决，所以哲学是一个不断解决问题的活动。所以维特根斯坦说的很好，哲学是一种活动。这一点我接受，哲学一定是一种活动，它不是一种理论体系，不是一种观念体系，它应当是一种我们共同来践行的思想活动，我想这是最重要，只有这样我们的哲学才能有发展的可能性，这也是我力图想推广的一种哲学的观念。 并且，哲学不是一个可以用理论的方式构造出来的体系，哲学它一定是用来考察我们使用语言，使用概念，甚至是我们使用某一些思想来解决问题的活动和能力，只有在这个活动当中才能够验证哲学的价值。如果没有这样的活动，哲学就变成理论家能说会道的一种本事，而不能够成为大家普遍接受的某一些真实观念，我觉得这样的哲学就变的没有意义了。我从来认为社会对哲学的拒绝，是哲学家的耻辱，而不是社会的耻辱。换句话说，哲学在社会中的消沉是哲学家的失职，而不是社会的冷漠。你不能够把这个罪责归咎于社会，你给别人提供了什么？你不提供一个创新性的思想，一个解决问题的方案，一个能够让我们深入思考某一些重要现实问题的解决方式，甚至连一个问题都没有提出来，凭什么让人家接受？所以这个时候哲学家们需要主动的要为社会做点贡献，这是问题所在。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>维特根斯坦</tag>
        <tag>语言哲学</tag>
        <tag>逻辑哲学论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理系列（4）：深度学习解决大规模文本分类问题]]></title>
    <url>%2F2018%2F02%2F16%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%A7%A3%E5%86%B3%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[这篇文章总结了文本分类领域特别是应用深度学习解决文本分类的相关思路、做法和部分实践的经验。转载自知乎清凇撰写的文章用深度学习（CNN RNN Attention）解决大规模文本分类问题 - 综述和实践 业务问题描述： 淘宝商品的一个经典的例子见下图，图中商品的标题是“夏装雪纺条纹短袖t恤女春半袖衣服夏天中长款大码胖mm显瘦上衣夏”。淘宝网后台是通过树形的多层的类目体系管理商品的，覆盖叶子类目数量达上万个，商品量也是10亿量级，我们的任务是根据商品标题预测其所在叶子类目，示例中商品归属的类目为“女装/女士精品&gt;&gt;蕾丝衫/雪纺衫”。很显然，这是一个非常典型的短文本多酚类问题。接下来分别会介绍下文本分类传统的和深度学习的做法，最后简单梳理下实践的经验。 一、传统文本分类方法文本分类问题算是自然语言处理领域里一个非常经典的问题了，相关研究最早可以追溯到上世纪50年代，当时是通过专家规则（Pattern）进行分类，甚至在80年代初一度发展到利用知识工程监理专家系统，这样做的好处是短平快的解决top问题，但显然天花板非常低，不仅费时费力，覆盖的范围和准确性都非常有限。 后来伴随着统计学习方法的发展，特别是90年代后互联网在线文本数量增长和机器学习学科的兴起，逐渐形成了一套解决大规模文本分类问题的经典玩法，这个阶段的主要套路是人工特征+浅层分类模型。训练文本分类器过程见下图： 整个文本分类问题就拆分成了特征工程和分类器两部分，玩机器学习的同学对此自然再熟悉不过了。 1.1 特征工程特征工程在机器学习中往往是最耗时耗力的，但却及其的重要。抽象来讲，机器学习问题是把数据转换成信息再提炼到知识的过程，特征是“数据—&gt;信息”的过程，决定了结果的上限，二分类器是“信息—&gt;知识”的过程，则是去逼近这个上限。然而特征工程不同于分类器模型，不具备很强的通用性，往往需要结合对特征任务的理解。 文本分类问题所在的自然语言处理领域也有其特有的特征处理逻辑，传统文本分类任务大部分工作也在此处。文本特征工程分为文本预处理、特征提取、文本表示三个部分，最终目的是把文本转换成计算机可理解的格式，并封装足够用于分类的信息，即很强的特征表达能力。 1.1.1 文本预处理文本预处理过程是在文本中提取关键词表示文本的过程，中文文本处理中主要包括文本分词和去停用词两个阶段。之所以进行分词，是因为很多研究表明特征粒度为词粒度远好于字粒度，其实很好理解，因为大部分分类算法不考虑词序信息，基于字粒度显然随时了过多“n-gram”信息。 具体到中文分词，不同于英文有天然的空格间隔，需要设计复杂的分词算法。传统算法主要有基于字符串匹配的正向/逆向/双向最大匹配；基于理解的句法和语义分析消歧；基于统计的互信息/CRF方法。近年来随着深度学习的应用，WordEmbedding+Bi-LSTM+CRF方法逐渐成为主流，本文重点在文本分类，就不展开了。而停用词是本文中一些高频的带刺连词介词等对文本分类无意义的词，通常维护一个停用词表，特征提取过程中删除停用词表中出现的词，本质上属于特征选择的一部分。 经过文本分词和去停用词之后淘宝商品示例标题变成了下图“/”分割的一个个关键词的形式： 1夏装 / 雪纺 / 条纹 / 短袖 / t恤 / 女 / 春 / 半袖 / 衣服 / 夏天 / 中长款 / 大码 / 胖mm / 显瘦 / 上衣 / 夏 1.1.2 文本表示和特征提取 文本表示 文本表示的目的是把预处理后的文本转化成计算机可以理解的方式，是决定文本分类质量最重要的部分。传统做法常用词袋模型（BOW，Bag Of Words）或向量空间模型（Vector Space Model），最大的不足是忽略上下文关系，没歌词之间彼此独立，并且无法表征语义信息。词袋模型的示例如下： 1( 0, 0, 0, 0, .... , 1, ... 0, 0, 0, 0) 一般来说词库至少都是百万级别，因此词袋模型有两个最大的问题：高维度、高稀疏性、词袋模型是向量空间模型的基础，因此向量空间模型通过特征项选择降低纬度，通过特征权重计算增加稠密性。 特征提取 向量空间模型的文本表示方法的特征提取对应特征项的选择和特征权重计算两部分。特征选择的基本思路是根据某个评价指标独立的对原始特征项（词项）进行评分排序，从中选择得分最高的一些特征项，过滤掉其余的特征项。常用的评价有文档频率、互信息、信息增益、$\chi ^2$统计量等。 特征权重主要是经典的TF-IDF方法及其扩展方法，主要思路是一个词的重要度与在类别内的词频成正比，与所有类别出现的次数成反比。 基于语义的文本表示 传统做法在文本表示方面除了向量空间模型，还有基于语义的文本表示方法，比如LDA主题模型、LSI/PLS概率潜在语义索引等方法，一般认为这些方法得到的文本表示可以认为文档的深层表示，而Word Embedding文本分布式表示方法则是深度学习方法的重要基础，下文会展现。 1.2 分类器分类器都是统计分类方法了，基本上大部分机器学习方法都在本文分类领域有所应用，比如朴素贝叶斯分类算法、KNN、SVM、最大熵和神经网络等等，传统分类模型不是本文重点，在这里就不展开了。 二、深度学习文本分类方法上文介绍了传统的文本分类做法，传统做法主要问题的文本表示是高维度高稀疏性的，特征表达能力很弱，而且神经网络很不擅长对此类数据的处理；此外需要人工进行特征工程，成本很高。而深度学习最初之所以在图像和语音取到巨大的成功，一个很重要的原因是图像和语音原始数据是连续和稠密的，有局部相关性。应用深度学习解决大规模文本分类问题最重要的是解决文本表示，再利用CNN/RNN等网络结构自动获取特征表达能力，去掉繁杂的人工特征工程，端到端的解决问题。接下来会分别介绍： 2.1 文本的分布式表示：词向量（Word Embedding）分布式表示（Distributed Representation）其实Hinton最早在1986年就提出了，基本思想是将没歌词表达成N维稠密、连续的实数向量，与之相对的one-hot encoding向量空间只有一个维度是1，其余都是0。分布式表示最大的优点是具备非常powerful的特征表达能力，比如n维向量每一维k个值，就可以表征$k^n$个概念了。事实上，不管是神经网络的隐层，还是多个潜在变量的概率主题模型，都是应用分布式表示。下图是03年Bengio在A Neural Probabilistic Language Model 这篇文章提出的神经网络语言模型（NNLM，Neural Probabilistic Language Model），采用的是文本分布式表示，即每个词表示为稠密的实数向量。NNLM模型的目标是构建语言模型： f(w_t，···，w_{t-n+1})=P(w_t|w_1^{t-1})词的分布式表示即词向量（Word Embedding）是训练语言模型的一个附加产物，即图中的Matrix C。 尽管Hinton 86年就提出了NNLM，词向量真正火起来是google的Mikolov在13年发表的两篇Word2Vec的文章Efficient Estimation of Word Representations in Vector Space 和 Distributed Representations of Words and Phrases and their Compositionality ，更重要的是发布了简单好用的Word2Vec工具包，在语义维度上得到了很好地验证，极大的推动了文本分析的进程。下图是文中提出的CBOW和Skip-Gram两个模型的结构，基本类似于NNLM，不同的是模型去掉了非线性隐层，预测目标不同，CBOW是上下文预测当前词，Skip-Gram则相反。 除此之外，提出了Hierarchical Softmax和Negative Sample两个方法，很好地解决了计算有效性，事实上这两个方法都没有严格的理论证明，有一些trick之处，非常的实用主义。详细的过程不再阐述了，有兴趣深入理解Word2Vec的，推荐读读这篇不错的paper：word2vec Parameter Learning Explained。额外多提一点，实际上WordVec学习的向量和真正语义还有差距，更多学到的是具备相似上下文的词，比如“good”“bad”相似度也很高，反而是文本分类任务输入有监督的语义能够学到更好的语义表示，有机会后续系统分享下。 至此，文本的表示通过词向量的表示方法，把文本数据从高维度高稀疏性的神经网络难处理的方式，变成了类似图像、语言的连续稠密数据。深度学习算法本身有很强的数据迁移性，很多之前在图像领域很适用的深度学习算法比如CNN等也可以很好的迁移到文本领域了，下面一小节具体阐述下文本分类邻域深度学习的方法。 2.2 深度学习文本分类模型2.2.1 fastTextfastText是上文提到的Word2Vec作者Mikolov转战Facebook后16年7月刚发表的一篇论文Bag of Tricks for Efficient Text Classification把fastText放在此处并非因为他是文本分类的主流做法，而是他及其简单，模型图见下： 原理是把句子中所有的词向量进行平均（某种意义上可以理解为只有一个Avg Pooling的特征CNN），然后直接接SoftMax层。其实文章也加入了一些n-gram特征的tric来捕获局部序列信息。文章到没有太多的信息量，算是“水文”吧，带来的思考是文本分类问题是有一些“线性”问题的部分，也就是说不必做过多的非线性转换、特征组合即可捕获很多分类信息，因此有些任务即便简单的模型便可以搞定了。 2.2.2 TextCNN本篇文章的题图选用的就是14年这篇文章提出的TextCNN的结果（见下图）。fastText中的网络结构是完全没有考虑词序信息的，而他用的n-gram特征Trick恰恰说明了局部序列信息的重要意义。卷积神经网络（CNN Convolutional Neural Network）最初在图像领域取得了巨大成功，CNN原理就不讲了，核心点在于可以捕捉局部相关性，具体到文本分类任务重可以利用CNN来提取句子中类似n-gram的关键信息。 TextCNN的详细过程原理图见下： TextCNN详细过程：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点了。然后经过有 filter_size=(2,3,4) 的一维卷积层，每个filter_size 有两个输出 channel。第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示了，最后接一层全连接的 softmax 层，输出每个类别的概率。 特征：这里的特征就是词向量，有静态（static）和非静态（non-static）方式。static方式采用比如word2vec预训练的词向量，训练过程不更新词向量，实质上属于迁移学习了，特别是数据量比较小的情况下，采用静态的词向量往往效果不错。non-static则是在训练过程中更新词向量。推荐的方式是 non-static 中的 fine-tunning方式，它是以预训练（pre-train）的word2vec向量初始化词向量，训练过程中调整词向量，能加速收敛，当然如果有充足的训练数据和资源，直接随机初始化词向量效果也是可以的。 通道（Channels）：图像中可以利用 (R, G, B) 作为不同channel，而文本的输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法。 一维卷积（conv-1d）：图像是二维数据，经过词向量表达的文本为一维数据，因此在TextCNN卷积用的是一维卷积。一维卷积带来的问题是需要设计通过不同 filter_size 的 filter 获取不同宽度的视野。 Pooling层：利用CNN解决文本分类问题的文章还是很多的，比如这篇 A Convolutional Neural Network for Modelling Sentences 最有意思的输入是在 pooling 改成 (dynamic) k-max pooling ，pooling阶段保留 k 个最大的信息，保留了全局的序列信息。比如在情感分析场景，举个例子： “ 我觉得这个地方景色还不错，但是人也实在太多了 ” 虽然前半部分体现情感是正向的，全局文本表达的是偏负面的情感，利用 k-max pooling能够很好捕捉这类信息。 2.2.3 TextRNN尽管TextCNN能够在很多任务里面能有不错的表现，但CNN有个最大问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节也很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 “n-gram” 信息。 RNN算是在自然语言处理领域非常一个标配网络了，在序列标注/命名体识别/seq2seq模型等很多场景都有应用，Recurrent Neural Network for Text Classification with Multi-Task Learning文中介绍了RNN用于分类问题的设计，下图LSTM用于网络结构原理示意图，示例中的是利用最后一个词的结果直接接全连接层softmax输出了。 2.2.4 TextRNN + AttentionCNN和RNN用在文本分类任务中尽管效果显著，但都有一个不足的地方就是不够直观，可解释性不好，特别是在分析badcase时候感受尤其深刻。而注意力（Attention）机制是自然语言处理领域一个常用的建模长时间记忆机制，能够很直观的给出每个词对结果的贡献，基本成了Seq2Seq模型的标配了。实际上文本分类从某种意义上也可以理解为一种特殊的Seq2Seq，所以考虑把Attention机制引入近来，研究了下学术界果然有类似做法。 Attention机制介绍：详细介绍Attention恐怕需要一小篇文章的篇幅，感兴趣的可参考14年这篇paper NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE。 以机器翻译为例简单介绍下，下图中 $x_{t}$ 是源语言的一个词，$y_{t}$ 是目标语言的一个词，机器翻译的任务就是给定源序列得到目标序列。翻译 $y_{t}$ 的过程产生取决于上一个词 $y_{t-1}$ 和源语言的词的表示 $h_{j}$（$x_{j}$ 的 bi-RNN 模型的表示），而每个词所占的权重是不一样的。比如源语言是中文 “我 / 是 / 中国人” 目标语言 “i / am / Chinese”，翻译出“Chinese”时候显然取决于“中国人”，而与“我 / 是”基本无关。下图公式, $\alpha _{ij} $则是翻译英文第 i 个词时，中文第 j 个词的贡献，也就是注意力。显然在翻译“Chinese”时，“中国人”的注意力值非常大。 Attention的核心point是在翻译每个目标词（或 预测商品标题文本所属类别）所用的上下文是不同的，这样的考虑显然是更合理的。 2.2.5 TextRNN + Attention我们参考了这篇文章 Hierarchical Attention Networks for Document Classification，下图是模型的网络结构图，它一方面用层次化的结构保留了文档的结构，另一方面在word-level和sentence-level。淘宝标题场景只需要 word-level 这一层的 Attention 即可。 加入Attention之后最大的好处自然是能够直观的解释各个句子和词对分类类别的重要性。 2.2.6 TextRCNN（TextRNN + CNN）我们参考的是中科院15年发表在AAAI上的这篇文章 Recurrent Convolutional Neural Networks for Text Classification 的结构： 利用前向和后向RNN得到每个词的前向和后向上下文的表示： 这样词的表示就变成词向量和前向后向上下文向量concat起来的形式了，即： 最后再接跟TextCNN相同卷积层，pooling层即可，唯一不同的是卷积层 filter_size = 1就可以了，不再需要更大 filter_size 获得更大视野，这里词的表示也可以只用双向RNN输出。 三、一点经验理论和实践之间的Gap往往差异巨大，学术paper更关注的是模型架构设计的新颖性等，更重要的是新的思路；而实践最重要的是在落地场景的效果，关注的点和方法都不一样。这部分简单梳理实际做项目过程中的一点经验教训。 模型显然并不是最重要的：不能否认，好的模型设计对拿到好结果的至关重要，也更是学术关注热点。但实际使用中，模型的工作量占的时间其实相对比较少。虽然再第二部分介绍了5种CNN/RNN及其变体的模型，实际中文本分类任务单纯用CNN已经足以取得很不错的结果了，我们的实验测试RCNN对准确率提升大约1%，并不是十分的显著。最佳实践是先用TextCNN模型把整体任务效果调试到最好，再尝试改进模型。 理解你的数据：虽然应用深度学习有一个很大的优势是不再需要繁琐低效的人工特征工程，然而如果你只是把他当做一个黑盒，难免会经常怀疑人生。一定要理解你的数据，记住无论传统方法还是深度学习方法，数据 sense 始终非常重要。要重视 badcase 分析，明白你的数据是否适合，为什么对为什么错。 关注迭代质量 - 记录和分析你的每次实验：迭代速度是决定算法项目成败的关键，学过概率的同学都很容易认同。而算法项目重要的不只是迭代速度，一定要关注迭代质量。如果你没有搭建一个快速实验分析的套路，迭代速度再快也只会替你公司心疼宝贵的计算资源。建议记录每次实验，实验分析至少回答这三个问题：为什么要实验？结论是什么？下一步怎么实验？ 超参调节：超参调节是各位调参工程师的日常了，推荐一篇文本分类实践的论文 A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification，里面贴了一些超参的对比实验，如果你刚开始启动文本分析任务，不妨按文章的结果设置超参，怎么最快的得到超参调节其实是一个非常重要的问题，可以读读 萧瑟的这篇文章 深度学习网络调参技巧 - 知乎专栏。 一定要用 dropout：有两种情况可以不用：数据量特别小，或者你用了更好的正则方法，比如bn。实际中我们尝试了不同参数的dropout，最好的还是0.5，所以如果你的计算资源很有限，默认0.5是一个很好的选择。 fine-tuning 是必选的：上文聊到了，如果只是使用word2vec训练的词向量作为特征表示，我赌你一定会损失很大的效果。 未必一定要 softmax loss： 这取决与你的数据，如果你的任务是多个类别间非互斥，可以试试着训练多个二分类器，也就是把问题定义为multi lable 而非 multi class，我们调整后准确率还是增加了&gt;1%。 类目不均衡问题：基本是一个在很多场景都验证过的结论：如果你的loss被一部分类别dominate，对总体而言大多是负向的。建议可以尝试类似 booststrap 方法调整 loss 中样本权重方式解决。 避免训练震荡：默认一定要增加随机采样因素尽可能使得数据分布iid，默认shuffle机制能使得训练结果更稳定。如果训练模型仍然很震荡，可以考虑调整学习率或 mini_batch_size。 没有收敛前不要过早的下结论：玩到最后的才是玩的最好的，特别是一些新的角度的测试，不要轻易否定，至少要等到收敛吧。 四、写在最后几年前校招面阿里时，一面二面聊的都是一个文本分类的项目（一个新浪微博主题分类的学校课题项目），用的还是文中介绍的传统的做法。面试时对特征项处理和各个分类器可谓如数家珍，被要求在白板上写了好几个特征选择公式，短短几年传统做法已经被远远超越，不得不感慨深度学习的发展。 值得感慨的一方面是今天技术的发展非常快，故步自封自然是万万万万不可取，深知还有很多理论尚且不懂还要继续深读paper；另一方面，理解理论原理和做好项目间实际非常有巨大的gap，特别是身处工业界的同仁们，学术圈值得钻但要把握分寸，如果仅仅追逐技术深度，不免容易陷入空中阁楼。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>TextCNN</tag>
        <tag>fastText</tag>
        <tag>TextRNN</tag>
        <tag>Attention</tag>
        <tag>TextRCNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（20）：维特根斯坦论语言的限度（6）—— 维特根斯坦与罗素]]></title>
    <url>%2F2018%2F02%2F16%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8820%EF%BC%89%EF%BC%9A%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E8%AE%BA%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6%EF%BC%886%EF%BC%89%E2%80%94%E2%80%94%20%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E4%B8%8E%E7%BD%97%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[维特根斯坦的思想不是逻辑的，是形而上学的。他通过对语言的了解是要说明,其实我们人类有很多东西是隐藏在语言背后没法说出来的，而这恰恰是一种推动我们用语言去表达的动力。但是对于罗素来说是很难理解的，因为罗素是一个经验主义者，他所有的观念都是建立在我们对经验的了解和认识之上，而维特根斯坦不是一个经验主义者，他是一个真正的理性主义者，他的观念具有很强的形而上学的根据，这是罗素很难理解的。 他跟罗素的师生关系其实保持的时间并不长久，1912年他正式跟随罗素学习，到1918年以后基本就开始逐渐疏远了。他去农村教书基本上就跟学术界失去了联系，前后加起来有将近有十年的时间，那时候跟罗素也没有直接的往来。跟哥德尔的具体历史过程我不是太熟悉，不知道他到底跟哥德尔交往得有多深，不仅仅是哥德尔，在当时的时代里面有很多后来成为大家的学者们，包括经济学家凯恩斯，米塞斯他们都有很多的交往，还有图灵,本来就跟他有亲戚关系，所以他当时对这些人是有影响的，乃至于后来他们讨论说图灵从维特根斯坦那里得到什么东西.图灵肯定不会说他受了维特根斯坦的影响，但是从他们的时代来看他们两个是有交集的，我们就能够知道维特根斯坦在那个年代，他的思想的活跃程度是受益于当时他跟这些人有这样一些接触，也保证了他的思想总是站在前面，具有更广泛的影响力。 罗素对他的前期哲学是推崇有佳，这本书的导言是罗素写的，罗素写完以后维特根斯坦并不满意，我在我的《〈逻辑哲学论〉导读》里详细的分析了为什么维特根斯坦不满意罗素的导言，罗素是借用这个机会表达自己的观点，所以很多地方理解的根本不到位。到现在为止我们基本上可以判断，罗素真的没有读懂维特根斯坦。因为维特根斯坦的思想不是逻辑的，甚至不是语言的，是形而上学的。他通过对语言的了解，是要说明其实我们人类有很多东西是隐藏在语言背后的，这个隐藏在语言背后的东西是没法说出来，而这种没法说出来恰恰是一种推动力，推动我们用语言去表达，就像我们说话，其实有的时候不是我们想说话，而是我们不得不说话，当我们说我想说话的时候，其实不是你主观愿望想说话，而是有一个东西在推动你让你去说。所以海德格尔说，不是我说语言，是语言让我说。维特根斯坦实际上已经意识到这个问题了，但是对于罗素来说是很难理解的，因为罗素是一个真正的经验主义者，他所有观念的建立都是基于我们对经验的了解，所有这些都是根据我们对当下经验活动的认识，而维特根斯坦不是一个经验主义者，他是一个真正的理性主义者，或者叫逻辑主义者，所以他的这种观念具有很强的形而上学的根据，这是罗素很难理解的。罗素对他的后期哲学的评价很低，虽然前期他还能够推崇他，但是到后期哲学他就完全不理解维特根斯坦在干什么，所以罗素在他的晚年1949年所写的《我的哲学的发展》里评价维特根斯坦后期哲学的时候说，维特根斯坦后期哲学只能作为我们茶余饭后的谈资，而不能够当作深刻的思想。显然他就根本没有读懂，虽然那个时候维特根斯坦的书还没有出来，但是其实他很多的讲稿已经在外面传播开来了，所以很多人对维特根斯坦的了解不是《哲学研究》出版以后才了解的，而是之前很多人就已经知道了，只是到了他去世以后，《哲学研究》那本书才被正式的编辑出版。不仅仅是罗素，G. E. Moore这样的哲学家也完全不能理解维特根斯坦，G. E. Moore和他差了好几个档次。他从来没有觉得罗素厉害，他去找罗素也是不得已而为之，是因为弗雷格推荐，他觉得弗雷格比罗素厉害得多，弗雷格的思想实际上在很大程度上给他提供了方向，维特根斯坦所做的工作其实是沿着弗雷格的工作往前推进的，跟罗素一点关系都没有，罗素只是帮助他完成了推进，所以罗素在他的心目当中并没有那么重要，为什么一直不把罗素放在眼里就是这个原因，他仅仅把他当作一个我思考哲学问题的助手，可想而知他有多么傲气，以及他的心智水平有多么高，这不是一般人能够达到的。罗素让他读哲学史，先看历史上的那些哲学家他们是怎么来讨论问题的，维特根斯坦拒绝，罗素震惊了，说那你回去写一篇东西，维特根斯坦就利用圣诞节期间写了几篇文章，罗素看了就说，这是天才。虽然罗素自己不一定能够达到这么高的心智水平，但他还是能够判断出来维特根斯坦的确是一个心智很高的人，这一点超出了常人。所以他跟维特根斯坦的姐姐写信说，你的弟弟将会成为我们中间最棒的一个人，他将会成为伟大的哲学家。这就是罗素当年给他下的一个判断。 罗素在1918年的时候他在发表他的代表作的时候，在伦敦做了一个系列的演讲，叫“逻辑原子主义哲学”演讲，开场白就说，我以下要讲的思想不是我的，是来自于我的学生和我的朋友维特根斯坦，但是我现在不知道他的死活。因为1918年第一次世界大战结束了，维特根斯坦是被作为俘虏被收编到意大利的俘虏营。后来维特根斯坦从俘虏营里寄了个明信片告诉罗素，他写了一本书叫《逻辑哲学论》，希望他们有时间能够见面聊一聊。1918年12月份他们俩商定在海牙见面，两个人在海牙整整谈了一个星期，讨论他这本《逻辑哲学论》，最后罗素答应给这本书写一个序言，这就是整个的维特根斯坦跟他的关系。到了1919年的时候，维特根斯坦就去了奥地利南部当小学教师了，维特根斯坦跟罗素之间他们相处的时间并不是很长，但是维特根斯坦仍旧表现出他自己的那种特殊性质，这种特殊的品质就是他自己的心智之高已经完全不把罗素放在自己的考量之内。他出于尊重在书里会提到罗素，这本书提到两个哲学家，弗雷格和罗素，但是他对罗素的评价远不如对弗雷格评价高，但是罗素毕竟是他的老师。1930年维特根斯坦重返剑桥以后，罗素要给他一个博士学位，但因为剑桥大学对于所有任教的老师是要求有学位的，而维特根斯坦走的时候连硕士都没有毕业，1912年进校1914年就已经去参军了，没有毕业就参军了。 1929年重返剑桥，到了1930年初的时候，维特根斯坦参加了剑桥大学组织的博士答辩，G. E. Moore向他提议用《逻辑哲学论》当博士论文，当时这本书已经是风靡全球成为经典著作了，维特根斯坦就答应了。但其实他这个时候已经放弃了这本书里的很多观点，不再把这本书当作他主要的思想，但是为了拿到博士学位，所以就以这个作为博士论文申请去答辩，答辩的时候只有两个答辩教授，一个外请一个是自己的，一个是G. E. Moore一个是罗素，两个人简单的对个话，就算是答辩了。很快他就在学校里面任教了，后来为了拿到更高的教席，必须要有东西，所以他当时就很快就完成了《哲学评论》跟《哲学语法》这两本书，这两本书都是作为正式的出版物已经做好编辑了，但是没有付印，只是作为提交申报教职的材料，因为虽然有学位但是没有东西不行。所以他就写了这两本书，大概1930年到1932年完成的，在这个过程当中，他的思想就开始发生剧烈的变化。从1929年到1936年，大概就这五六年时间里面，他的思想就已经开始成型了，从1936年开始写《哲学研究》，一直到1945年完成，前后十年的时间完成了《哲学研究》的工作，但到他去世之前仍然是没有被出版的，本来出版社已经接受了他的出版合同，并且跟他签了约，结果他最后要出版之前又撤回来，认为好多想法还不太成熟，还希望能够再补充或者修订，结果一直到他去世这本书也没有正式出版。他去世以后，安斯康作为他的遗嘱执行人最后给他编辑完成了这本书。 维特根斯坦对自己的要求非常严格，不可能随随便便就可以把一本书交到出版社去出版，他去世之后整理他所有的笔记本，大概有几十本，全部整理出来应该有上千页的内容。但是后来实在量太大就不再整理了，全部做成微缩胶片，现在这个手稿保存在挪威，但微缩胶片在全国各个图书馆都有，有一些部分被编辑、翻译。因为他的原书大部分都是用德文写的，翻译成英文出版。如果我们要更多的了解维特根斯坦的话，需要把维特根斯坦所有的东西看过，才能够知道他在晚年的时候到底还在思考什么问题。 在2017年年初我们翻译出版了维特根斯坦跟他另外一个学生的谈话纪要，这个谈话纪要是从1932年一直持续到1946年，在过去没有出版过，后来在2015年的时候被英国的一个著名的杂志《Mind》出版，后来我们就花了一年时间翻译，一年时间校对，今年年初就全部出版出来了，通过这个对话里面就可以看出，他在这一段历史发展过程当中所考虑的问题涉及的面是相当广的，这些问题他的学生给他记录下来，后来这些东西被整理成文字。我们可以看出维特根斯坦直到去世之前都一直在思考哲学，甚至是说他认为自己还有精力（energy）,还有能力去做哲学的工作，只是因为最后身体越来越差。所以，维特根斯坦本人其实是把自己整个生命献给哲学。一个人的经历虽然是很有限的，但是如果一个人他什么事都不管，他只做哲学，你想想这个人会做出多大成就出来。但这个人得有能力去做这件事情，上天给了维特根斯坦这样的能力，只有维特根斯坦这样的人才能够真正做到这一点，这是很奇妙的一件事情，一个人只干一件事情，一辈子只做一件事情，而且把它做的这么好，觉得这个事情唯有他能做，别人真的做不了这个事情，让我们几代人可能都觉得从中受益，我觉得这种哲学家真是了不起。 我给学生放维特根斯坦照片的时候，大家都说这简直是男神，他的眼睛特别深邃，一看就是思想家，而且不是在看着现在，是望着未来，他的思想也是看着人类未来可能发生什么，这是我们常人很难达到跟做到的，这也是维特根斯坦过人之处。所以维特根斯坦英文的传记――《天才之为责任：维特根斯坦传》写的真好，揭示了他是个天才的哲学家，虽然我们讲哲学家是可以后天慢慢形成，但是像维特根斯坦这样的哲学家，大概真是很少，难得一见，空前绝后。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>维特根斯坦</tag>
        <tag>语言哲学</tag>
        <tag>逻辑哲学论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（19）：维特根斯坦论语言的限度（5）—— 尘世生活]]></title>
    <url>%2F2018%2F02%2F15%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8819%EF%BC%89%EF%BC%9A%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E8%AE%BA%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6%EF%BC%885%EF%BC%89%E2%80%94%E2%80%94%20%E5%B0%98%E4%B8%96%E7%94%9F%E6%B4%BB%2F</url>
    <content type="text"><![CDATA[维特根斯坦的生活是非常简单的，没有特别高的要求，但他对于思想和哲学上的要求却非常高，他是一个完美的理想主义者。他追求的目标是，所有的东西一定要做到最好，这是一般人做不到的。 维特根斯坦的生活经历是比较复杂的，甚至可以用坎坷这两个字，所谓坎坷说他的生活过程实际上是颠沛流离的状态。他居无定所，自从1945年他辞去了剑桥大学的教授职位以后，他就没有自己固定的住所，经常借住在他的学生家里面，或者到挪威，在山的边上盖了一个小房子，所以他基本上没有一个自己固定的生活住所，在剑桥大学的时候住自己的宿舍，宿舍陈设也非常简单、简陋。他是一个不追求物质也不追求生活情趣的人，虽然他对艺术，文学，以及宗教都有很高的鉴赏跟认识水平，但是尘世的生活并不是一个让他特别眷恋的东西，所以他也没有表现出窘迫，他仅仅表达一种个人的情绪，他在给朋友的信当中描写他当时某一种状况下心情很糟糕写不出东西，或者他觉得自己的思想已经枯竭了已经不能够做哲学了，他经常表示这样的担心。 他经常会表现出高傲的姿态，对很多人来说他是一个难以接近的人，他经常跟很多人搞不好关系，性格也比较孤僻敏感，虽然跟普通人还是比较容易融洽的，但他对跟他有思想关系的那些人反而表现出自己的独特，因为他老是怀疑他人在剽窃他的思想，尽管这些人其实在宣传他的思想，不断地在解释他的思想，但是他老觉得这些人不怀好意。 他最好的朋友罗素是他的老师，最后跟他分道扬镳，最好的朋友石里克跟卡尔纳普，当年在维也纳的时候，请大师一样的把他请去讲《逻辑哲学论》，但是最后也分道扬镳了。唯一的能够跟他在一起生活，还能够被接受的就是他的学生安斯康，她后来也是他的几本遗著的编者，因为安斯康从来不跟他计较，也从来不过分宣传他的哲学，所以他觉得安斯康比较靠谱。最后去世的时候也是在安斯康的家里，安斯康跟他的丈夫一块儿照顾他，并且请了自己的家庭医生来照顾维特根斯坦。 维特根斯坦最后去世的时候那一句话让人匪夷所思，他说，请告诉他们，我度过了一个美好的人生。当他这句话出来以后大家很奇怪说，他还美好？为什么会美好？虽然他的哲学影响很大，但他个人生活是非常落魄的，但是这个事情对于他来说真的不重要。他在早年的时候把自己继承的遗产分给了自己的家人，并且还资助了比较落魄的诗人――青年诗人里尔克，到了晚年的时候，他基本上靠一点点退休金生活，因为他到1945年算是退出教职了，但是能够从剑桥拿到一些生活费，有时候朋友也接济他。所以他自己的生活是非常的简单，也没有特别高的追求，但他对于思想上的要求，对于哲学上的要求却是非常高的，他是一个完美的理想主义者，普通人真的达不到这一点。他给他的姐姐设计过房子，自己也做过一些小小的雕塑，设计过一些小小的工艺品，这些东西都是做到极致，精巧到极致，这是他追求的目标，所有的东西一定要做到最好，这是一般人做不到的。 他的这种思想观念，主要还是来自于早年家庭的教育和熏陶，因为他出生于一个很高贵的家庭，家庭成员具有很高的艺术修养和人文关怀，他的父亲很有钱，每天来的基本上都是有很高的思想跟学术的地位的社会名流，在这样的环境中被培养出来，他会要求自己也应当像那样。他在当小学教师的时候跟学生家长经常闹别扭，因为都是农村的家庭，家长基本上都是当地农民，他认为农民一般都比较纯朴简单，所以跟农民打交道没有问题，结果没想到当他去教孩子的时候并不是这样。他对孩子有极高的要求，如果完不成任务会打手板，家长不干了，认为你完全没有必要去惩罚，孩子做不好你好好跟孩子讲，维特根斯坦认为不惩罚他记不住，因为小学生刚开始上一二年级，你跟他讲道理他不会记住的，只有通过身体的惩罚他才能够记住。那些家长就把他告到法院，经过法庭开庭经过审理，最后双方和解。但是从此他也认识到了当地人并不是他想象的那么淳朴，那么可爱，所以他跟当地人一直搞不好关系，包括他周围的人。在很大程度上是他的那种清高和完美主义导致的结果，他一直认为别人做的没有那么好，所以他一直就觉得自己跟他们有距离，这样一种心态导致他在周围很难有一个真心的，可以长久交往的一个朋友，这完全归咎于他的这种完满的理想主义，这就是他的生活。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>维特根斯坦</tag>
        <tag>语言哲学</tag>
        <tag>逻辑哲学论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理系列（3）：中文维基语料词向量训练]]></title>
    <url>%2F2018%2F02%2F15%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9A%E4%B8%AD%E6%96%87%E7%BB%B4%E5%9F%BA%E8%AF%AD%E6%96%99%E8%AF%8D%E5%90%91%E9%87%8F%E8%AE%AD%E7%BB%83%2F</url>
    <content type="text"><![CDATA[要通过计算机进行自然语言处理，首先就需要将这些文本数字化，目前用的最广泛的方法是词向量，根据训练使用算法的不同，目前主要有Word2Vec和GloVe两大方法，本文主要讲述通过这两个方法分别训练中文维基百科语料库的词向量。 一、获取并处理中文维基百科语料库1.1 下载中文维基百科语料库的下载链接为：https://dumps.wikimedia.org/zhwiki/，本试验下载的是最新的zhwiki-latest-pages-articles.xml.bz2。这个压缩包里面存的是标题、正文部分，该目录下还包括了其他类型的语料库，如仅包含标题，摘要等。 1.2 抽取内容Wikipedia Extractor是一个开源的用于抽取维基百科语料库的工具，由python携程，通过这个工具可以很容易地从语料库中抽取相关内容。使用方法如下： 12$ git clone https://github.com/attardi/wikiextractor.git wikiextractor$ wikiextractor/WikiExtractor.py -b 2000M -o zhwiki_extracted zhwiki-latest-pages-articles.xml.bz2 由于这个工具就是一个python脚本，因此无需安装，-b参数指对提取出来的内容进行切片后每个文件的大小，如果要将所有内容保存在同一个文件，那么就需要把这个参数设置地大一点，-o的参数指提取出来的文件放置的目录，抽取出来的文件的路径为zhwiki_extract/AA/wiki_00。更多的参数可参考其github主页的说明。 抽取后的内容格式为每篇文章被一对&lt;doc&gt;&lt;/doc&gt;包起来，而&lt;doc&gt;中的包含了属性有文章的id、url和title属性，如&lt;doc id=&quot;13&quot; url=&quot;https://zh.wikipedia.org/wiki?curid=13&quot; title=&quot;数学&quot;&gt; 1.3 繁简转换由上一步提取出来的中文维基百科中的语料中既有繁体字也有简体字，这里需要将其统一变为简体字，采用的工具也是开源的OpenCC转换器。安装使用方法如下： 1234$ ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; &lt; /dev/null 2&gt; /dev/null$ brew install opencc$ cd OpenCC$ opencc -i /Users/HuaZhang/Desktop/zhwiki_extracted/AA/zhwiki_extract/AA/wiki_00 -o zhwiki_extract/zhs_wiki -c /Users/HuaZhang/OpenCC/data/config/t2s.json 其中-i表示输入文件路径，-o表示输出的文件，-c表示转换的配置文件，这里使用的繁体转简体的配置文件，OpenCC自带了一系列的转换配置文件，可以参考其github主页的说明。 1.4 去除标点去除标点符号有两个问题需要解决，一个是像下面这种为了解决各地术语，名称不同的问题： 1他的主要成就包括Emacs及後來的GNU Emacs，GNU C 編譯器及-&#123;zh-hant:GNU 除錯器;zh-hans:GDB 调试器&#125;-。 另外一个就是将所有标点符号替换成空字符，通过正则表达式均可解决这两个问题，下面是具体实现的python代码： 1234567891011121314151617181920212223242526#!/usr/bin/python# -*- coding: utf-8 -*- import sysimport reimport ioreload(sys)sys.setdefaultencoding('utf-8')def pre_process(input_file, output_file): multi_version = re.compile(ur'-\&#123;.*?(zh-hans|zh-cn):([^;]*?)(;.*?)?\&#125;-') punctuation = re.compile(u"[-~!@#$%^&amp;*()_+`=\[\]\\\&#123;\&#125;\"|;':,./&lt;&gt;?·！@#￥%……&amp;*（）——+【】、；‘：“”，。、《》？「『」』]") with io.open(output_file, mode = 'w', encoding = 'utf-8') as outfile: with io.open(input_file, mode = 'r', encoding ='utf-8') as infile: for line in infile: line = multi_version.sub(ur'\2', line) line = punctuation.sub('', line.decode('utf8')) outfile.write(line)if __name__ == '__main__': if len(sys.argv) != 3: print "Usage: python script.py input_file output_file" sys.exit() input_file, output_file = sys.argv[1], sys.argv[2] pre_process(input_file, output_file) 经过该步骤处理之后，得到了简体中文的纯净文本，如下所示： &gt;doc id13 urlhttpszhwikipediaorgwikicurid13 title数学数学数学是利用符号语言研究数量结构变化以及空间等概念的一门学科从某种角度看属于形式科学的一种数学透过抽象化和逻辑推理的使用由计数计算量度和对物体形状及运动的观察而产生数学家们拓展这些概念为了公式化新的猜想以及从选定的公理及定义中建立起严谨推导出的定理·······数学奖通常和其他科学的奖项分开数学上最有名的奖为菲尔兹奖创立于1936年每四年颁奖一次它通常被认为是数学的诺贝尔奖另一个国际上主要的奖项为阿贝尔奖创立于2003年两者都颁奖于特定的工作主题包括数学新领域的创新或已成熟领域中未解决问题的解答著名的23个问题称为希尔伯特的23个问题于1900年由德国数学家大卫希尔伯特所提出这一连串的问题在数学家之间有著极高的名望且至少有九个问题已经被解答了出来另一新的七个重要问题称为千禧年大奖难题发表于2000年对其每一个问题的解答都有著一百万美元的奖金而当中只有一个问题黎曼猜想和希尔伯特的问题重复doc 1.5 jieba分词下面需要对其进行分词并且整理成每行一篇文本的格式，从而方便后续的处理。 分词采用 python 的分词工具 jieba，通过 pip install jieba安装即可。且将一篇文章分词后的结果存储在一行，由前面可知，每篇文章存储在一对&lt;doc&gt;&lt;/doc&gt;标签中，由于前面去掉了标点，所以现在变成了doc doc,所以只要判断当前行为doc时即可认为文章结束，从而开始在新的一行记录下一篇文章的分词结果。实现的python代码如下: 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/python# -*- coding: utf-8 -*-import sysimport ioimport jiebareload(sys)sys.setdefaultencoding('utf-8')def cut_words(input_file, output_file): count = 0 with io.open(output_file, mode = 'w', encoding = 'utf-8') as outfile: with io.open(input_file, mode = 'r', encoding = 'utf-8') as infile: for line in infile: line = line.strip() if len(line) &lt; 1: # empty line continue if line.startswith('doc'): # start or end of a passage if line == 'doc': # end of a passage outfile.write(u'\n') count = count + 1 if(count % 1000 == 0): print('%s articles were finished.......' %count) continue for word in jieba.cut(line): outfile.write(word + ' ') print('%s articles were finished.......' %count)if __name__ == '__main__': if len(sys.argv) &lt; 3: print "Usage: python script.py input_file output_file" sys.exit() input_file, output_file = sys.argv[1], sys.argv[2] cut_words(input_file, output_file) 二、通过Word2Vec训练词向量Word2vec中包含了两种训练词向量的方法：Continuous Bag of Words(CBOW)和Skip-gram。CBOW的目标是根据上下文来预测当前词语的概率。Skip-gram刚好相反，根据当前词语来预测上下文的概率。这两种方法都利用人工神经网络作为它们的分类算法。起初，每个单词都是一个随机N维向量。训练时，该算法利用CBOW或者Skip-gram的方法获得了每个单词的最优向量。 最初 Google 开源的 Word2Vec 是用C来写的，后面陆续有了Python ，Java 等语言的版本，这里采用的是 Python 版本的 gensim。通过 gensim 提供的 API 可以比较容易地进行词向量的训练。gensim的建议通过conda install gensim安装 下面是对上面处理后的语料库进行训练的一个简单例子。 123456789101112131415161718192021222324#!/usr/bin/python# -*- coding: utf-8 -*-import os, sysimport multiprocessingimport gensim reload(sys)sys.setdefaultencoding(&apos;utf-8&apos;)def word2vec_train(input_file, output_file): sentences = gensim.models.word2vec.LineSentence(input_file) model = gensim.models.Word2Vec(sentences, size=300, min_count=10, sg=0, workers=multiprocessing.cpu_count()) model.save(output_file) model.wv.save_word2vec_format(output_file + &apos;.vector&apos;, binary=True)if __name__ == &apos;__main__&apos;: if len(sys.argv) &lt; 3: print &quot;Usage: python script.py infile outfile&quot; sys.exit() input_file, output_file = sys.argv[1], sys.argv[2] word2vec_train(input_file, output_file) 上面的训练过程首先将输入的文件转为 gensim内部的 LineSentence对象，要求输入的文件的格式为每行一篇文章，每篇文章的词语以空格隔开。 然后通过gensim.models.Word2Vec初始化一个Word2Vec模型，size参数表示训练的向量的维数；min_count表示忽略那些出现次数小于这个数值的词语，认为他们是没有意义的词语，一般的取值范围为（0，100）；sg表示采用何种算法进行训练，取0时表示采用CBOW模型，取1表示采用skip-gram模型；workers表示开多少个进程进行训练，采用多进程训练可以加快训练过程，这里开的进程数与CPU的核数相等。 假设我们训练好了一个语料库的词向量，当一些新的文章加入这个语料库时，如何训练这些新增的文章从而更新我们的语料库？将全部文章再进行一次训练显然是费时费力的，gensim提供了一种类似于“增量训练”的方法。即可在原来的model基础上仅对新增的文章进行训练。如下所示为一个简单的例子： 12model = gensim.models.Word2Vec.load(exist_model)model.train(new_sentences) 上面的代码先加载了一个已经训练好的词向量模型，然后再添加新的文章进行训练，同样新增的文章的格式也要满足每行一篇文章，每篇文章的词语通过空格分开的格式。这里需要注意的是加载的模型只能 是通过model.save()存储的模型，从model.save_word2vec_format()恢复过来的模型只能用于查询. 三、使用词向量模型训练好的词向量可以供后续的多项自然语言处理工作使用，下面是通过gensim加载训练好的词向量模型并进行查询的例子： 123456789101112131415161718192021222324252627282930# 加载模型import gensimmodel = gensim.models.KeyedVectors.load_word2vec_format('/Users/HuaZhang/Desktop/zhwiki_extracted/output_word2vec.vector',binary = True)# 词向量维度len(model[u'黑格尔'])# 相似度model.similarity(u'叔本华',u'康德')0.62428547493158093# 找出相似度最高的词words = model.most_similar(u"哈耶克")for word in words: print word[0],word[1] 米塞斯 0.776977062225叔本华 0.731572449207黑格尔 0.723797023296巴维克 0.723039865494门格尔 0.719911754131谢林 0.714867889881波普尔 0.714080870152马克思 0.711268663406尼采 0.710071563721博姆 0.705146193504# 找出最不相关的词汇print model.doesnt_match(u"莎士比亚 卡夫卡 卢梭 爱因斯坦".split())爱因斯坦]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>Skip-gram</tag>
        <tag>CBOW</tag>
        <tag>词向量</tag>
        <tag>Wiki</tag>
        <tag>jieba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理系列（2）：Word2Vec]]></title>
    <url>%2F2018%2F02%2F14%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9AWord2Vec%2F</url>
    <content type="text"><![CDATA[这篇文章翻译自word2vec Parameter Learning Explained Mikolov等人的word2vec模型和应用在近两年受到了广泛的关注。由word2vec模型学习的单词的向量表示已经被证明具有语义意义，并且在各种NLP任务中都很有用。随着越来越多的研究人员实验word2vec或类似的技术,我注意到缺乏全面解释字嵌入模型的参数学习过程的细节的材料,从而使得非神经网络专家的研究人员无法理解此类模型的工作机制。 本文章提供了word2vec模型参数更新方程的详细推导和解释，包括原始的连续bag-of-word (CBOW)和skip-gram (SG)模型，以及优化技术，包括分层的softmax和负采样。对梯度方程的直观解释也提供了数学推导。 在附录中，提供了关于神经元网络和反向传播的基础知识的综述。我还创建了一个交互式演示，wevi，以促进对模型的直观理解。 一、Continuous Bag-of-Word Model1.1 One-word context我们从Mikolov et al. (2013a)中引入的最简单的一种连续的单词模型(CBOW)开始。我们假设每个上下文只考虑一个词，这意味着模型将根据上下文单词来预测一个目标单词，这就像一个三元模型。对于不熟悉神经网络的读者来说，建议通过附录A来快速回顾重要的概念和术语，然后再进一步讨论。 图1显示了简化的上下文定义下的网络模型。我们设定词汇量大小为V，隐藏层的大小为N。相邻层上的单元是全连接的。输入是一个one-hot编码向量，这意味着对于给定的输入上下文，在V个单元${x_1，·，·，x_V}$中只有一个单元为1，所有其他单元都是0。输入层和输出层之间的权值可以表示为一个$V×N$矩阵$W$。$W$的每一行是与输入层的相关词的N维向量表示$v_w$。形式上，$W$的第$i$行是$v_w^T$。给定一个上下文(一个单词)，假设$x_k=1 $、$x_{k’}=0$（$k≠k$）,我们有 h=W_T·x=W^{T}_{(k,:)}:=v_{w_I}^T······（1）本质上就是复制$W$的第k行至$h$。$v_{wI}$是输入词$w_I$的向量表示。这意味着隐含层单元的链接(激活)函数是线性的(即:，直接将其和输入加权求和至下一层。 从隐层到输出层,有一个不同的权重矩阵$W′= { w_{ij}^′}$，这是一个$N×V$的矩阵。使用这些权重，我们可以计算词库中每个单词的得分$u_j$。 u_j=v_{w_j}^{′T}·h······（2）$v^′_w$是矩阵$W^′$的第$j$列，然后我们可以使用softmax，一个对数线性$j$分类模型，来获取单词的后验分布，这是一个多项分布。 p(w_j|w_I) = y_j = \frac{exp(u_j)}{\sum_{j'=1}^Vexp(u_{j'})} ,······（3）其中$y_j$是输出层的第$j$个单元的输出。将(1)和(2)代入(3)，得到。 p(w_j|w_I)=\frac{exp(v_{w_j}'^Tv_{w_I})}{exp(v_{w_j'}'^Tv_{w_I})}······（4）注意$v_w$和$v’_w$是词$w$的两种表示。$v_w$来自矩阵$W$的行，是输入层至隐藏层的权重矩阵，$v’_w$来自矩阵$W$的列，是隐藏层到输出层的权重矩阵。在随后的分析中，我们把$v_w$称为“输入向量”，把$v’_w$称为词$w$的“输出向量”。 隐藏层到输出层权重的更新公式 现在让我们推导这个模型的权值更新方程。虽然实际的计算是不切实际的(解释如下)，但我们正在做以下的推导，以获得对这个原始模型的见解，并没有使用任何技巧。有关反向传播的基础知识，请参阅附录a。 训练目标(一个训练样本)是最大化(4)式，在给定输出上下文单词$w_I$的条件下，观察实际输出单词$w_o$的条件概率。 max p({ w }_{ O }|{ w }_{ I })=max { y }_{ { j }^{ * } }\qquad (5)\\ \qquad \qquad \qquad = max log { y }_{ { j }^{ * } }\qquad (6)\\ \qquad \qquad \qquad \qquad = { u }_{ { j }^{ * } }-log\sum _{ { j }^{ ' }=1 }^{ V }{ exp({ u }_{ { j }^{ ' } }):= -E } \qquad (7)其中$E=-log p(w_O|w_I)$是我们的损失函数（我们希望最小化$E$），$j^*$是输出层中的实际输出词的索引。注意，这个损失函数可以理解为两个概率分布之间的交叉熵衡量的一个特例。 现在让我们推导出隐藏和输出层之间权重的更新方程。求$E$关于第$j$个单位网络输入$u_j$的导数，我们得到： \frac{\partial E}{\partial u_j}=y_j-t_j := e_j (8)比如$t_j=1(j=j^*)$，只有当第$j$个单元是实际输出词时，$t_j$为1，否则$t_j=0$。注意，这个导数只是输出层的预测误差$e_j$。 接下来我们对$w’_{ij}$求导获得从隐藏层到输出层的梯度。 \frac{\partial E}{\partial w'_{ij}}=\frac{\partial E}{\partial u_j}·\frac{\partial u_j}{\partial w'_{ij}}=e_j·h_i \qquad (9)因此,使用随机梯度下降法,得到隐藏的权重更新方程→输出权值: w^{'(new)}_{ij}=w^{'(old)_{ij}}-\eta · e_j· h_i \qquad (10 )或者 v^{'(new)}_{j}=v^{'(old )}_{w_j}-\eta · e_j · h \qquad for j=1,2,···,V. \qquad (11 )其中$eta&gt;0$为学习率，$e_j=y_j-t_j$，$h_i$是隐藏层的第i个单元。$v’_{w_j}$是$w_j$的输出向量。注意，这个更新方程意味着我们必须遍历词汇表中的每个可能单词，检查它的输出概率$y_j$，并将$y_j$与它的期望输出$t_j$(0或1)进行比较。如果$y_j&gt;t_j$(即高估)，我们就从$v’_{w_j}$移除一部分的隐藏层向量$h$(比如$v_{w_I}$)，使得$v’_{w_j}$远离$v_{w_I}$；如果$y_j&lt;t_j$(即低估，当且仅当$t_j=1$时成立，比如$w_j=w_O$)，我们就添加一些$h$至$v’_{w_O}$，使得$v’_{w_O}$靠近$v_{w_I}$。如果$y_j$非常接近$t_j$，根据更公式，权重只会发生非常小的变化。再次注意，$v_w$(输入向量)和$v’_w$(输出向量)是词$w$两种不同的向量表示。 输入层到隐藏层的权重更新公式 已经获取了$W’$的更新公式，现在我们来看看$W$。我们对$E$求隐藏层的输出的导数，得到： \frac{\partial E}{\partial h_i}=\sum_{j=1}^V\frac{\partial E}{\partial u_j}·\frac{\partial u_j}{\partial h_i} = \sum_{j=1}^V e_j · w'_{ij}:=EH_i \qquad (12)其中$h_i$是隐藏层第i个单元的输出；$u_j$是在(2)式中被定义的，即输出层的第j个单元的网络输入；$e_j=y_j-t_j$是输出层中第j个词的预测误差。$EH$是一个N维的向量，是词库中所有单词的输出向量的预测误差加权求和。 接下来我们对$E$求$W$的导数，首先，回想一下隐藏层对输入层的值进行线性计算。展开（1）中的向量表示，我们得到 h_i=\sum_{k=1}^Vx_k · w_{k_i}现在我们可以求E关于W的每个元素的导数，得到: \frac{\partial E}{\partial w_{k_i}}=\frac{\partial E}{\partial h_i}· \frac{\partial h_i}{\partial w_{k_i}}=EH_i · x_k \qquad (14)这是$x$和$EH$的点积等式， \frac{\partial E}{\partial W}=x ⊗ EH=xEH^{T} \qquad (15)这样我们得到了一个$V×N$的矩阵。因为x向量中只有一个元素非零，$\frac{\partial E}{\partial W}$中只有一行非零，且那一行 的值为$EH^T$，一个N维的向量。我们得到了w的更新公式： v_{w_I}^{(new)}=v_{w_I}^{(old)}-\eta EH^T\qquad (16)其中$v_{w_I}$是$W$的一行，及唯一的上下文词的输入向量，也是唯一的导数不为零的$W$的行，所有其他的$W$的行在迭代后保持不变，因为他们的导数为零。 从直觉上来看，因为向量$EH$是语料库中所有单词的输出并进行预测误差加权后得到的总和，我们可以将（16）理解成为词汇中的每个输出向量的一部分添加到上下文单词的输入向量中。如果在输出层中，一个单词wj作为输出词的概率被高估$(y_j &gt; t_j)$，那么上下文单词$w_I$的输入向量将倾向于远离$w_j$的输出向量。反之，如果$w_j$作为输出词的概率被低估$(y_j &lt; t_j)$，则输入向量$w_I$将趋向于接近$w_j$的输出向量。如果$w_j$的概率相对准确的预测，那么它对$w_I$的输入向量的改变量很小。$w_I$的输入向量的变动是由词汇中所有向量的预测误差决定的。预测误差越大，一个单词对上下文单词输入向量的变动产生的影响就越大。当我们通过从训练语料库中生成的上下文目标词对来迭代更新模型参数时，对向量的影响将会累积。我们可以想象一个单词w的输出向量被w的相邻邻域的输入向量“拖拽”，就好像w的向量和它的邻域的向量之间有物理的弦。同样，输入向量也可以被认为是被许多输出向量拖拽的。这种解释可以提醒我们注意重力，或者是力向图的布局。每个虚弦的平衡长度与相关的两个单词之间的共存强度以及学习速率有关。经过多次迭代，输入和输出向量的相对位置最终会趋于稳定。 1.2 Multi-word context图2显示了带有多词上下文的CBOW模型。当计算隐层输出,而不是直接复制的输入向量输入上下文的话,CBOW模型需要的平均向量的输入上下文的话,和使用的产品输入→隐藏权重矩阵和平均向量作为输出。 h=\frac{1}{C}W^T(x_1+x_2+···+x_C)\qquad (17)\\=\frac{1}{C}(v_{w_1}+v_{w_2}+···+v_{w_C})^T\qquad (18)其中$C$是上下文中词汇的数量，$w_1,···,w_C$是上下文的词汇，$v_w$是一个词$w$的输入向量。损失函数为： E=-log p(w_O|w_{I,1},···,w_{I,C})\qquad (19)\\=-u_{j^*}+log\sum_{j'=1}^Vexp(u'_j)\qquad (20)\\=-v_{w_O}^T·h+log \sum^{V}_{j'=1}exp(v_{w_j}'^T·h)\qquad (21)可以看到和（7）式one-word-context模型的目标函数相同，除了$h$不同，将（1）式替换为（18）式。 从隐藏层到输出层权重更新公式和one-word-context模型保持一致，copy如下： v^{'(new)}_{j}=v^{'(old )}_{w_j}-\eta · e_j · h \qquad for j=1,2,···,V. \qquad (22 )注意,我们需要对隐藏层→输出层的权重矩阵的每个元素为每个训练实例进行更新。 输入层到隐藏层的权重更新公式与(16)类似,只是现在我们需要应用下列方程至上下文的每一个词: v_{w_I}^{(new)}=v_{w_I}^{(old)}-\frac{1}{C}\eta EH^T\qquad for \ j=1,2,···，C\qquad (23)其中$v_{w_{I,c}}$是在输入上下文中第c个词的输入向量；$\eta$是一个正的学习率；$EH=\frac{\partial E}{\partial h_i}$由（12）式给出。这个更新公式的直观解释和（16）式类似。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（18）：维特根斯坦论语言的限度（4）—— 语法与联系]]></title>
    <url>%2F2018%2F02%2F14%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8818%EF%BC%89%EF%BC%9A%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E8%AE%BA%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6%EF%BC%884%EF%BC%89%E2%80%94%E2%80%94%20%E8%AF%AD%E6%B3%95%E4%B8%8E%E8%81%94%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[所谓“语法”，就是看到联系。联系与结构息息相关，我们在结构当中才能为某一个具体的事物定位。我们看到任何一个游戏的时候，绝对不会想到这只是一个游戏，一定会想到这个游戏它相关的内容，因而，由这个相关的内容来理解游戏，而不是根据这个游戏来把握相关内容，不是由点到点，而是由面到点，这个面就是相互关系的面。 维特根斯坦还有一个重要的概念――语法（grammer），并且他给出了关于语法（grammer）的说明。他在中期阶段曾经留下了一些笔记，去世以后他的学生们给他编撰了一本书，这本书被取名为《哲学语法》（Philosophical grammar），除了《哲学语法》以外，他还有一本书叫《哲学评论》，这两本书通常被看作是他中期思想的两本重要的代表作。 《哲学语法》这本书里面主要给我们提供的是如何以看见或者观察到我们所有语词，或者是我们的命题之间相互关系的这样一种方式来确定我们所玩的语言游戏，或者我们所使用任何一个概念它所具有的意义和内容。有一个很著名的例子――鸭兔图，如果我们往右面我们就会看到是一个兔子，如果我们往左面看是一只鸭子，两种动物同时体现一副画上。鸭兔图实际上反映了两个不同的视角，语法（grammer)就是当我们看到一个事物的时候，我们其实不仅仅是在了解这个事物的本身，也是在了解这个事物跟其他事物之间的相互联系，因为我们只有通过相互联系，才能够对所谈到的事物本身得以确切的了解，所以我们是在关系当中去认识一个对象的。同样，对于语言游戏也是这样，我们是在各种不同的语言游戏之间的相互联系中来了解某一个语言游戏是如何完成自己的工作的，这就是我们前面讲到的整体论的观念。维特根斯坦有一个词，叫可纵观性（surveyability），是说我们可以通过鸟瞰的方式把握对象，但是我们通常把握的方式不是点到点的方式，而是线到点的方式，通过联系的方式来看到每一个点所处的位置。所以通过联系来看待事物，这是维特根斯坦通过语法的概念来解释他对于语言游戏的理解。当然这个语法不是我们讲的自然语言，他仅仅是在讲这样的一种事物与事物之间相互联系的概念，这是维特根斯坦最重要的一个观点。 语法的概念就是看到联系，联系其实就是结构的概念，我们在结构当中来为某一个具体的事物定位。你看到任何一个游戏的时候，你绝对不会想到这只是一个游戏，一定会想到这个游戏它相关的内容，因而，由这个相关的内容来理解游戏，而不是根据这个游戏来把握相关内容，不是由点到点，而是由面到点，这个面就是相互关系的面。 现代拓扑学重要的观点是，当我们谈论一个点的时候，我们往往是通过这个点所触及的面或者触及的联系来认识它。当我们能够确定网络当中任何一个点，我们都不是通过这个点来认识面的，而是相反。这对我们考察人的神经元的构造是非常有帮助的，通过显微镜看我们神经元的变化，你会发现每一个神经元的突起点实际上是不固定的，它是不断的变化的。因为它随着我们思维活动，随着我们的大脑的活动它在发生变化，随着这种变化我们产生了一个节点，这个结点可能就是思维的节点，然后产生了概念。但我们说概念、观念，这是用语言来表达出来的，但大脑当中不是一个概念，它就是一个节点，就是我们讲的那种神经与神经之间交叉的地方，而这个交叉的形成是来自于这些不同的联系，这个思维方式跟原有的思维方式是不一样的，原有的思维方式是按照传统的形而上学提倡的实体观至上的观念，实体就是所有的事物的存在都是根据实体的性质来加以解释的，而实体本身是孤立的，就像莱布尼茨的单子一样，单子与单子之间是没有联系的，然后我们根据每一个单子来理解世界的变化。 过去认为，事物的存在是世界存在的最基础的内容。但是现在认为事物的存在是以这个事物相关的各种联系所造成的结果，那个点只是造成的一个结果，它本身并不是原因，所以我们要找原因的时候，找的是网络本身给我们的结果所造成的影响，所以考察变动的关系是考察这些网络与网络之间是如何导致这个结果的。现代拓扑学讲的点面关系，点的存在是取决于面，或者取决于关系，而关系本身它是可以变化的，所以这个点是不确定的，但是点一旦被确定，就说明关系被建立起来了，所以我们又是通过这个点来了解这个关系是如何被建立的，维特根斯坦说语言关系就是那个点，所以我们通过这个点，去了解什么东西构成了这个点，然后我们就通过这个点来知道了它相关的所有这些关系，看到的这些相互的关系，如果你看不到这些关系你就没有办法理解这些点。 在人类社会中其实也是这样，人类学考察的都不是孤立的对象，你看到一个原始人类出现的时候，你绝对不会想到这只是一个个体，你一定想到这个个体它背后有一个群体存在。我们的思维一定是往外扩散的，不是往内收缩的，因为只有通过这种扩张式的方法，我们才能够发现更多的点，然后我们才能够确立这个联系是如何被建立起来，这时候我们发现我们关心的不是这些点，我们关心的是网络本身，这是现代网络最重要的一个思路，如何通过网络的建立来寻找点或者建立点，而不是通过研究点来寻找网络。 有时候我能够想象，虽然我们不一定能够看见空间真实的状况，但可以想象到空间的状况，就像我们在一些现代的美术作品当中，他们所构想的一个你看不见的世界。我想，这个看不见的世界其实是存在的，不是因为它看不见而存在，而是因为它能够很好的通过另外一种方式向我们展示它的存在。就像地球的两极以及宇宙中的射线，这些东西是看不见的，但是我们能够通过仪器找到它们，能够发现它们，而这个仪器是间接的，但是能够想象的出来这种空间中所大量分布的这样一种结构性的安排，这要回到经典力学当中去讨论作用力跟反作用力的关系。一个物体的存在本身不是单向作用的结果，一定是双向作用，一个事物就像一个人能够在地球存在，不仅仅是我们有重量，重量其实是引力的结果，任何物体都是这样。就像我们知道一个星球它能够按照某种轨道运行，只是我们不知道为什么会这样运行，我们可以看到它运行的轨迹，其实它背后有很多力量在作用，所以导致我们现在新的物理学，天体物理学研究的时候会发现新的天体存在。发现新的天体好像是说过去它不在这里，我们重新又发现它了，不是这样，其实它一直在那里，只是我们过去检测的手段并没有那么先进。门捷列夫发现元素周期表，他觉得这个地方应该有一个元素，只是我们现在不知道这个元素是什么，因为如果没有这个元素你构成不了上下前后的元素关系，所以它中间一定有一个东西，这个方法其实就是拓扑学的方式，通过这样一种不同的联系来确定一个对象所存在的位置，这就是拓扑学的基本观念。然后通过这种方式来寻找这样对象的存在，它的根据是什么，这个根据恰恰是一种不变量所决定的。 维特根斯坦之所以用《逻辑哲学论》这种方式来表达他的思想，其实里面就包含很严密拓扑学论证。他在柏林读书期间借住在德国柏林的一个专门研究拓扑学的教授家，那是十九世纪末二十世纪初，拓扑学在今天来说还是个新的学科，上个世纪六十年代才正式确立拓扑学学科，在这之前基本上属于前历史阶段，那个教授就是早期从事几何拓扑研究的一个专家，他从中得到了很多启发，所以乃至他后来写文章，写所有的笔记都是按照这种方式来编的，不是论证的方式，因为论证的方式实际上是一个线性的方式，而是一个立体而且多元的方式。所以，他的这本《逻辑哲学论》里面你可以看到整个的篇章布局都是完全按照这样的方式构建出来的，而这个构建方式在他那个时代是少见的，我估计今天也少有这样的写作方法，这对于我们了解拓扑学是如何被贯彻到他的哲学描述当中，以及理解我们所生活的世界的各种结构关系的过程都是非常有意义的。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>维特根斯坦</tag>
        <tag>语言哲学</tag>
        <tag>逻辑哲学论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（17）：维特根斯坦论语言的限度（3）—— 语言背后的基础]]></title>
    <url>%2F2018%2F02%2F13%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8817%EF%BC%89%EF%BC%9A%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E8%AE%BA%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6%EF%BC%883%EF%BC%89%E2%80%94%E2%80%94%20%E8%AF%AD%E8%A8%80%E8%83%8C%E5%90%8E%E7%9A%84%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[维特根斯坦哲学的特殊性就在于，它不是以显形的方式让人们去接受的，维特根斯坦是一个隐士，他是隐身在我们的思想背后，然后去触发人们思考很多新的问题，而这些问题实际上在维特根斯坦都已经讨论过了，只是他并没有给我们提供一个现成的结论，而是让我们不断的去思考他提出的这些问题 维特根斯坦讨论语言的界限或者语言的边界的目的是什么？我们这样处理维特根斯坦前后期的关系，并且用语言的界限这样的说法来理解维特根斯坦的时候，我们到底在做什么？我想，当维特根斯坦讨论语言问题的时候，是把这个语言看作一个连贯的整体，虽然这样的语言前期对他来说是逻辑语言，后期是自然语言，虽然看上去各不相同，但它们都是在关心我们思想的表达方式，在这一点上两者是一致的，都是讨论思想的表达方式，只是前期用逻辑的方式给出了思想的定义，后期用显示的方式通过语言游戏来表达思想的内容，所以这两者都是对思想本身的关注，因而维特根斯坦表面上看是讨论语言，他实际上关心的是思想本身，这是我们第一个想要强调的一点。 第二，我们更想强调一点是，维特根斯坦一直试图要追问：当我们谈论语言的时候，甚至在谈论思想的时候，我们背后的根据是什么？所谓背后的根据就是，我们能不能找到一个构成我们谈论语言和思想的基础的东西？他在前期哲学里讨论逻辑概念的时候是有的。因为所有的逻辑语言，其实都建立在我们对逻辑的一种自觉的先天的意识之上，这种先天的自觉意识是说，我们相信逻辑可以完成这个工作，因而逻辑就是它的基础。 到了后期哲学的时候，这个基础在哪里？这是维特根斯坦一直追问的问题。甚至不仅仅是维特根斯坦，是整个二十世纪后半叶的西方哲学家们一直追问的问题：当人类失去了一种能够追问的基础的时候，当人类失去我们共同的家园的时候，漂浮在大海上的我们，还能够做什么？ 奥托・纽拉特经常说，人类其实就是在海上一艘永远不能够靠岸的船只，人类永远是生活在这样的船只之上，要修补这个船只我们只能在大海上修补，不能等到靠岸再修补，我们人类的认识活动和知识就像是这样一艘船只，所以当这个船只出了故障的时候，我们只能够通过在大海上的漂泊来修复或者替换它。知识论里面有可替换性原则，就是当我们不能够找到一个根据的时候，我们只能利用另外的东西想办法使得我们要补充修订的这一部分内容可以得到替换，但是要替换它的东西本身也是需要修补的，我们只能够取长补短。所以我们的认识活动是通过这种方式来相互协调而共同发展的，可是我们没有那个基础存在了，我们都找不到那样一个最终的可靠的根据了。 因为自从现代科学，特别是现代的物理学产生以后，人类知识的大厦就已经受到了严重的挑战。我们几乎很难相信还有一个所谓的最确定的东西，把它当作我们一切知识的根源。在人类的认识活动，包括我们对语言的讨论当中，我们找不到这样的东西。过去我们可以把上帝当作产生语言最终根据，甚至可以把人类的理性当作我们思想的根据，但是这在今天已经被大部分人放弃了，上帝的观念已经被放弃了，而对于理性绝对的盲目崇拜也被放弃了，这导致一个结果――我们失去了家园。现在我们老讲乡愁，我们人类好像似乎总是在往回走，总是要回顾我们过去所经历的那些事情，然后把经历的历史当作我们现在的起点。但事实上经历是回忆性，甚至是幻想性的，因为这些事物其实在我们的心目当中永远只是作为一个记忆中的存在。而如果人类的认识活动建立在这样一种幻想的、回忆的，甚至是虚构的背景之上，那人类的知识就面临很大的挑战。 其实维特根斯坦整个的思考方式就是要追问有没有这个根据？能不能够找到这个根据，让我们人类的知识哪怕靠不了岸也可以就地抛锚？我们可以把我们的锚栽在河床之上，让这个船只能够稳定，而不是永远在漂浮当中。维特根斯坦试图找到这样一个河床，他把这个河床叫做思想的河床，这是维特根斯坦要做的一个重要工作。维特根斯坦在追问这样一个思想河床的时候，实际上是在为我们人类知识寻找一个根据，他试图想要找到这个东西并且他认为找到了。在《论确定性》当中，他认为这样的东西就叫做生活形式和世界图式。我自己将世界图式看成是类似于世界观的一种思想内容，跟我们通常理解斯宾格勒所说的世界观不太一样，我们通常在德国的语境当中讲世界观的时候，是指预先有一个对世界的理解，但是维特根斯坦理解的世界观是讲我们在已经具备对世界了解之后所形成的对世界的理解方式，而不是在理解世界之前，我们就预先有了某一个世界的概念，这个世界图式不是静态的，是动态的。因为在这个背景当中，有一些东西不断地会被替换掉，然后不断又有新的东西被增加进来，所以它是流动的。按照维特根斯坦的理解，这种变化的世界图景，总有一些东西是不变的，正因为这些不变的内容，使得我们把它看成是一副完整的图像，如果每一个东西都在变化，我们就没法把握这个图像。所以，在维特根斯坦心目当中，整个世界就处于这样的变动当中，而我们要把握就是那个变动当中的，某一些能够让我们把它看成完整图像的不变的内容，这是维特根斯坦要强调的一个重要的观念。 所以对他来说，他虽然反对以往传统本质主义的观念，认为人类可以有一个工作，可以把它当作一些知识的基础，但是他又试图要为我们能够去寻找这样的基础，这种心理的倾向找到一个说明，证明人类是有这个倾向的，只是找不到而已。就像我们每一个人都希望长寿，但是我们都知道这是不可能的，我们活多长时间不是由我们决定的。这个时候我们会发现，人们的这种主观愿望会直接影响到对事物的判断。在维特根斯坦看来这个倾向是实实在在的，因为实际上它确定了我们当下的这种存在方式，而当下的这种存在的方式恰恰是以这种以变化当中不变的因素来加以保障的，这是维特根斯坦要强调的一个重要的观念。 我们理解维特根斯坦的时候，会把他看作是一个比较极端的，激进的哲学家，看作是一个完全跟传统背道而驰的哲学家，他自己在《文化与价值》这本书中也多次表达过类似的观点，他反对现代科技，反对现代文化，反对整个现代主流的社会思潮，而我们认为维特根斯坦简直就是像嬉皮士一样是社会的叛逆者。其实仔细读一下他的书发现倒也未必，他思想里面有很多值得我们去进一步去追问的内容。这样就使得维特根斯坦整个的思想就变成大家思考所有当代问题的一个出发点，他不一定给出了答案，但是他的思考方式却引起大家去思索。在当代哲学当中有一个很有意思的现象，几乎每一个人言必称维特根斯坦，但是没有人承认自己是维特根斯坦的信徒，没有人承认自己是受维特根斯坦的影响，包括当代大的一些哲学家，他们在自己的书当中只是引述维特根斯坦的观点，但他们从来不认为维特根斯坦说法就是对的，或者说我认为我就是按照维特根斯坦的方式来做的。 这是维特根斯坦思想的特殊性，我曾经有文章专门讨论这个问题，维特根斯坦哲学的特殊性就在于，它不是以一个显形的方式让人们去接受的，维特根斯坦是一个隐士，他是隐身在我们的思想背后，然后去触发人们思考很多新的问题，而这些问题实际上在维特根斯坦都已经讨论过了，只是他并没有给我们提供一个现成的结论，而是让我们不断的去思考他提出的这些问题，他一辈子涉及的哲学问题实在太多了，哲学史上关注过的以及当代所面临的问题他都有所涉猎，而且几乎每一个人跟他接触的人都评价说维特根斯坦对所有事情有一个很好的感觉，他那种感觉是超人的，而且他说的话都不外行，但是他一说一表达对这个问题的看法，马上就可以表现出他自己那种天才的那一面，包括人类学、宗教、艺术、文学、诗歌、音乐，所有这些领域他都表达过自己的看法，我想恐怕在他那个时代，也少有这样的学者或者哲学家能够做到这一点，这也是维特根斯坦的过人之处。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>维特根斯坦</tag>
        <tag>语言哲学</tag>
        <tag>逻辑哲学论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（16）：维特根斯坦论语言的限度（2）—— 语言的边界]]></title>
    <url>%2F2018%2F02%2F12%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8816%EF%BC%89%EF%BC%9A%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E8%AE%BA%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94%20%E8%AF%AD%E8%A8%80%E7%9A%84%E8%BE%B9%E7%95%8C%2F</url>
    <content type="text"><![CDATA[维特根斯坦说，当我们谈到边界的时候，我们不是刻意要追问这个边界在哪里，而是用语言的方式来说明，我们所有的人类活动只有按照语言的这种表达方式来理解，我们的思想才能够被加以定位，思想的活动是通过语言游戏这种方式来加以定位甚至加以理解的，不是纯粹用逻辑的方式去规定思想的目的或方式，而是把语言完全看作是我们人类自身具有的活动能力和活动内容。语言游戏既是我们能够讨论思想边界的一种方式，同时又是我们不断突破思想边界的方式。 维特根斯坦的哲学研究是用语言游戏的方式来显示思想的内容，不是通过语言来表达思想的内容，而是通过语言游戏来显示，由此，就跟逻辑的语言没有关系了，因为逻辑的语言在他看来并不是被代替了，而是说被搁置一边。有一部分是可以用逻辑的方式来表达的，但是还有一些不能够用逻辑的语言来表达得，那关于这个不能够用逻辑语言来表达那一部分，我们只能通过语言游戏的方式来显示它们。 但是，维特根斯坦是如何用这样的语言游戏来显示他所说的那些不可能用逻辑的语言来表达的思想内容呢？大部分人都认为维特根斯坦后期对语言的批评和批判和前期不太一样，一种比较经典的观点认为，他的前期哲学是一种静态的对语言结构逻辑的分析，而到了后期是一种动态的对语言活动的一个游戏分析，一个是静态一个是动态，一个是对性质结构的了解，一个是对于活动、运动的了解，对于活动的了解，对于游戏的了解，这两个是完全不太一样，但就是这种不一样导致了维特根斯坦在后期所关心的侧重点不一样，但是并不意味着他完全背离了他前期的哲学，而只是说他关注的重点不同，所以，他只是转换了一个视角，他的研究对象并没有变化，仍旧是对于语言本身的考察。在他后期哲学里面，他对于语言的讨论就不再是界限、限度的问题，而是边界, the bounds of language or the boundary of language。维特根斯坦说，其实我们有时候很难对一个语言游戏划出一个界限，我们能够规定这个语言游戏跟其他的语言游戏是如何被加以区分的，因为我们知道，在《哲学研究》当中，维特根斯坦描述了大量的语言游戏，他并没有给出具体的哪一种语言游戏跟其他语言游戏之间有什么本质性的区别，但是他强调了语言游戏跟另外一个语言游戏之间的相似关系，他用了家族相似这个说法来说明所有的语言游戏都具有某种家族相似的特征，这个家族相似在他那里其实指的是所谓的遵守规则，无论是规则还是家族相似，在他那里都表征着一个特点――所有的语言游戏它们具有一些共同的东西，而这种共同的东西使得语言游戏可以完成它的功能，并且使得我们能够通过语言游戏去了解那些我们试图想表征的内容。 这里有一个很重要的节点，我们都认为维特根斯坦是在描述语言游戏，而并不认为语言游戏在维特根斯坦那里是被解释的，仅仅是被描述的，但是其实维特根斯坦在这里所描述的语言哲学隐含一种解释性功能，就像显示这个概念一样，显示并不是意味着这个行为本身它给我们显示出了某种特征，如果一个行为，一个活动，或者一个游戏，当它被说成是向我们显示了某一种生活形式的时候，是指这个活动本身它所显示生活形式能够被我们所理解，当我们看到一个我们完全没有见过的陌生游戏，我们没有办法通过这个游戏解读出这个游戏它所显示生活内容或者生活形式，而我们只有在知道有这样的生活形式之后，我们才能够了解这种生活形式是什么，如果完全不了解的话是没有办法解释的，所以表面上看它就只是描述性。他也反复强调说，我们的语言游戏不过就是一项跟平常的游戏一样，所以这个时候我们不需要任何解释。 按照维特根斯坦的观点，当我们要讨论语言游戏的时候，我们到底在做什么？它是以什么样的方式向我们显示它所谓的生活形式的？生活形式是一个很有意思的概念，我们需要仔细揣摩这个概念所包含的深层含义。因为我们讲的生活形式并不是大家公认的某一种生活方式，或者某一种习俗、传统、文化、历史等等，他讲的生活形式是指我们是按照这样一种方式来生活，它里面包含的内容都已经被隐含在我们的活动方式之中了，我们是通过这种活动方式来显示这样的生活内容。就像我们看到一幅绘画的时候，我们马上就能判断出这幅绘画它所包含一种历史的、文化的乃至民族的特征，这样的特征实际上并不是我们去赋予这个图画的，而是这副图画向我们显示出来的，因而我们在欣赏任何一幅作品的时候，这幅作品向我们显示出来的内涵和意义要远远多于我们赋予它解释的意义，因为解释都是依赖于它所显示的内容。所以在这个前提之下，维特根斯坦强调，我们的语言游戏是以这种显示的方式向我们呈现它的细节的，所有的语言游戏都是以其详细的活动方式和内容向我们显示它背后所隐含的生活方式和生活形式，这就是维特根斯坦一直强调的，当我们了解一个语言游戏的时候，一定要把握语言游戏它所具体的方式，而不是它一般性的内容。比如说下棋，当我们谈下棋的时候，我们一定要进一步追问什么棋、怎么下，什么棋、怎么下这里面背后追问的是：到底是什么样的规则决定了这个游戏的方式？ 因而，你可以看得出维特根斯坦所讲的细节的概念就是讲我们遵守规则的具体活动内容，通过这种细节的描述，我们才能够真正把握维特根斯坦所说游戏的概念是以什么样的形式，向我们显示它背后所给我们提供的内容。我们每一个在日常生活当中生活的人每天都会接触大量的人和事，有一些东西是我们并不熟悉的，有一些东西是我们可能通过书本去了解的，或者通过图片去了解，当我们了解这些内容的时候，我们得有一个前提，我们要知道它向我们展现的这些思想内容背后的东西是什么？ 如何去理解和把握这一点就很玄妙了，只有当我们能够真正理解这种文化所依赖根据的时候，我们才能够真正去理解这个文化向我们显示的内容，如果你不能够了解这个文化，就说明你自己没有办法把这个语言，或者这种活动看作是整个生活形式的一部分，而唯有你自己在整体上把握了这样的文化内容，你才能够真正地了解它所显示出来的文化意蕴。 后来很多哲学家就把维特根斯坦这个观点发展成了整体论的语言观。所谓整体论的语言观是说，当我们理解一种语言的时候，或者说理解一个语言当中某一些命题的时候，就意味着我们能够完整地理解这个语言和命题所在的整个的语言。就像约翰・赛尔（Jhon Searle）所举的例子，一个英国士兵在二战期间被德军抓了，他大概会说一句德语，他就用这一句德语表示自己是一个德国人。比如他说的德语是鲜花很美，这句德语跟当时的场景完全无关，但他想用这句话来表示我也会德语，我是德国人，是友军。他通过这种方式来表征自己的身份，但事实上这句话本身在这里不表征任何他能够懂这个语言本身，但这是一种特例。在这种特殊的环境下，我们能够理解这句话并不是根据这句话本身它所表达的含义，而是根据它的整个场景，他用这句话来表征他其实是一个德国人。但是我们在理解任何一个语言中一个句子的时候，当我们能够理解一个语言中的一个句子的时候，也就意味着我们能够理解这个语言所承载的这样一种文化背景，其实也就意味着你可以理解这个语言，这就是所谓的整体论的观点。 这个整体论的观点的确是从维特根斯坦这儿引发出来的。按照维特根斯坦的说法，语言游戏的理解并不是一个单纯的、简单的步骤，也不是一次性完成的工作，它是需要大量的，无数的这样的语言游戏构成一个完整的整体，这个整体就意味着说，当我们理解某一个语言游戏活动的时候，我们能够知道这个活动它所附带的，或者说它所相关的其他的游戏活动。比如当我们玩扑克牌的时候，没有一种游戏叫做玩扑克游戏，因为扑克只是一个工具，可以玩桥牌游戏，打争上游，斗地主，我们可以做各种你可以想象出来的用扑克来玩儿的游戏。这样的游戏实际上它并不依照于纸质或者材料，而是游戏规则。因而，这个时候遵守规则是整个游戏活动当中最为至关重要的一个环节，正因为遵守规则而使得维特根斯坦发现，我们通过遵守规则才能够真正地定位我们所要做的事情，因为这些规则它给我们规定了我们这个游戏的性质，同时也规定了这个游戏它所反映出来的这样一种文化特征。因而通过这个小的游戏来看整个文化的概念，小的语言活动来看整个思想的构成方式，实际上就是维特根斯坦希望通过语言游戏所要达到的目的。 在这里，我要进一步解释“边界”的概念。因为我们前面说它不是一个界限，也不是一个范围，而是一个边界。就像孙悟空一样，说要给唐僧划一个界，划的这个边界是唐僧不能够超出的范围，超出了以后他就没有安全保障了。同样，对于语言来说，如果按照逻辑的方式划一个界，只有在逻辑的范围内能够表达清楚的思想，我们都可以用逻辑的方式来表达，这是早期的一个工作。到了后期的时候，他讲的边界是说，这个边界之内的事情都没问题了，但是我们现在要做的是边界之外的事情。这个边界有多大，我们在边界之外所能够完成的工作就有多大。所以维特根斯坦说，这样的边界是没有办法确定的，因为我们的语言活动是“佛法无边”的，语言相当于是我们人类所使用一个佛杖，怎么划它都会成为一个独立的场域。因而，在这个前提下，这个边界实际上是不可能完全被划定。所以维特根斯坦说，当我们谈到边界的时候，我们不是刻意要追问这个边界在哪里，而是用语言的方式来说明，我们所有的人类活动，只有按照语言的这种表达方式来理解的话，我们的思想才能够被加以定位，思想的活动是通过语言游戏这种方式来加以定位甚至加以理解的。这样它就摆脱了早期的纯粹用逻辑的方式去规定思想的目的或方式，而是把语言完全看作是我们人类自身具有的活动能力和活动内容。所以语言游戏对他来说，既是我们能够讨论思想边界的一种方式，同时又成为不断突破思想边界的一种方式。因为，当我们想到一种新的语言游戏的时候，我们实际上就想到一种新的思想的可能性，我们只要创造一种新的游戏规则，我们就又有一种新的思想的产生，我们现代社会的发展变化如此之快，其实也在不断创造新的语言游戏，而对于维特根斯坦来说，其实我们日常生活当中的很多思想观念是伴随着这些语言游戏的生成而形成的。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>维特根斯坦</tag>
        <tag>语言哲学</tag>
        <tag>逻辑哲学论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（15）：维特根斯坦论语言的限度（1）—— 语言的限度]]></title>
    <url>%2F2018%2F02%2F11%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8815%EF%BC%89%EF%BC%9A%E7%BB%B4%E7%89%B9%E6%A0%B9%E6%96%AF%E5%9D%A6%E8%AE%BA%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%20%E8%AF%AD%E8%A8%80%E7%9A%84%E9%99%90%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[《逻辑哲学论》是维特根斯坦早期的一本代表作，也是他生前出版过的唯一的一本哲学著作。他在这本书里提出了关于哲学是一种对语言的批判的观点，即批判语言实际上是哲学的一个主要工作，甚至是哲学的全部工作。维特根斯坦认为，我们所有思想都是用语言表达的，因而我们即使是要考察思想，也只能通过语言来完成，批判语言就是在批判思想。 维特根斯坦是大家都喜欢的一个哲学家，无论是他自己的个人生平还是他的哲学思想，都引起了很多人的关注，尤其是关于他的哲学。通常会认为他有两种哲学，一个是前期的《逻辑哲学论》时期的哲学，另外一个是所谓的《哲学研究》时期的哲学，这两个时期的哲学思想，是有很大的区别的。维特根斯坦在一生当中提出两种具有深远影响的哲学思想，这个对于一般人来讲，的确很难做到，但维特根斯坦做到了，这就足以证明，维特根斯坦是一个非常具有天才性质的哲学家。 今天我其实要讲的是关于维特根斯坦整个哲学思想的理解，不仅仅是限于他的前期或后期，而是放在一个整体的发展脉络当中来加以把握。把握的方式就是通过他对语言限度的理解，来看待他的前后期之间的相互联系。谈两个哲学之间的差别比较容易，但是要谈他们两个之间的相互关系，确实需要我们去发现和寻找一些相关的线索。我们找的线索是通过对维特根斯坦语言的限度的说明，来看待作为一个整体的维特根斯坦思想的发展过程。限度的英文叫limitation，实际上讲的是在一个界限内我们所能够完成的工作，或者说我们不能够超出这个界限，只能在这个界限之内所做的工作，通常我会把这一部分的思想看作是他前期的哲学，即他在《逻辑哲学论》当中给我们提供的关于语言限度的思想。 《逻辑哲学论》是维特根斯坦早期的一本代表作，也是他生前出版过的唯一的一本哲学著作。这本书虽然篇幅不长但意义非常重大，他在这本书里首先提出了关于哲学是一种对语言的批判的观点，即批判语言实际上是哲学的一个主要工作，甚至是哲学的全部工作。维特根斯坦认为，我们所有思想都是用语言表达的，因而我们即使是要考察思想，也只能通过语言来完成，在这个意义上，批判语言就是在批判思想。 根据维特根斯坦自己的论述，我将他整个前期哲学里面关于语言界限的观点分成三个步骤来加以论述。 第一个步骤，它认为我们所有的思想不过是由有意义的命题构成的，这些有意义的命题形成了我们的思想，而所有这些命题又构成了我们的语言，我们通过了解有意义的命题来了解我们的思想，也就是说我们通过这些构成了有意义命题这样的一些语言来了解我们的思想。由此，语言本身就变成思想的全部，所以语言是思想的全部，我们只有通过考察语言才能了解思想本身。维特根斯坦相信，当我们把语言理解成能够去加以考察和思想的一种内容的时候，我们其实就是在对语言和思想本身做一种批判的工作。这种批判某种意义上意味着，当我们批判思想的时候，实际上是在批判语言，那么反过来说，当我们考察语言的时候其实是在考察思想。在维特根斯坦看来，整个哲学的工作就是要通过对有意义的命题所构成的语言的分析，来理解它所表达的思想内容，这是维特根斯坦一个非常重要的观点。对语言的考察和批判，也构成了维特根斯坦整个前期哲学主要内容。 维特根斯坦在第二步说，我们在考察语言的时候会发现，语言本身其实是我们人类机体的一个组成部分，由此，语言本身很可能会成为我们了解思想的一个障碍，因为如果我们真的可以通过语言来了解思想，语言就应该是一个透明的，毫无障碍的途径，我们可以直接通过语言去了解思想，但是现实却并非如此。我们的日常语言其实经常会出现模糊、混乱、误用等等这些情况。那么，我们在考察语言的时候如何通过语言的考察接近思想？这本身变成一个困难的事情，因而这里面是否有一个悖论——由于我们考察语言是在考察思想，而语言会妨碍思想，所以我们实际上考察语言是达不到思想的。维特根斯坦要解决这个矛盾，如果来看待我们对思想的考察是通过对语言的考察来完成的？ 维特根斯坦是怎么解决这个问题的呢？他在第一步说，所有考察语言的过程，其实就是考察思想的过程。第二步又说语言本身又会妨碍我们对思想考察，两者之间会产生一些矛盾。第三步维特根斯坦提出，我们并不是完全只是通过考察语言的表达方式去理解思想，我们能够考察的语言是很有限的，因而对这样的语言的考察，满足的是符合逻辑句法要求的语言，只有这样的语言能够给我们清楚的展现它的句子结构并且通过这样的句子结构向我们显示思想的时候，我们才能够说这样的语言考察是对思想的研究。所以，可以看得出来，维特根斯坦这里所说的考察语言本身，考察的并不是自然语言，而是逻辑语言。 这样就解释了为什么维特根斯坦会提出，对语言的批判其实就是对思想的批判，甚至说整个哲学就是对语言的批判这个观点。这个批判有两个含义，一个是要考察语言所能够发挥的作用，第二就是要考察语言的限度。语言的作用是我们通过语言能了解思想，它的限度是指我们只能通过考察那些符合逻辑要求的语言，我们才能够去达到思想。换句话说，思想是通过符合逻辑要求的语言加以表达的，这是维特根斯坦的一个观点，至少是在他前期哲学当中他是这么认为的。 这里就有一个麻烦，如果语言是通过逻辑的方式所构造出来才能表达思想的话，那些非逻辑的语言怎么办？维特根斯坦在他的这本书的最后一个命题里说，对于那些不可说的东西，也就是我们不能用清楚的逻辑语言去表达的思想，我们只能保持沉默。最后一个命题维特根斯坦试图给我们表达了一种神秘主义的思想，因为发现我们不能理解和表达的东西，似乎都是不能说的东西，既然他不能够说，按照维特根斯坦的观点它甚至连思想都不能够进行，这是维特根斯坦一个神秘和玄妙之处。 很多人在讨论维特根斯坦的时候，认为维特根斯坦在这里给我们埋下了一个伏笔，这个伏笔是说一定有一些东西是不能够用逻辑的语言去表达的，而这些不能够用逻辑的语言来表达的东西，我们一定有别的方式可以得到。这个概念在维特根斯坦前期哲学中并没有明确的给出，它只提到一个词——显示，我们可以通过显示的方式给出那些不能够用语言所表达的思想内容。由于前期哲学他主要关心的是逻辑语言，他给出的关于世界的理解，基本上都是一个逻辑世界的概念，而不是一个我们现实的经验世界，因而就使得所有的逻辑语言只能够表达我们所构造出来那个逻辑世界的内容，而不能够去理解经验世界的内容。 如果退一步我们可以说维特根斯坦实际上隐含这样一个前提，凡是我们可以用逻辑的方式去表达的，我们在经验当中其实都是可以实现的。反过来说，凡是我们在经验中可以去感受和表达的东西，如果它有意义，也应当可以用逻辑的方式加以呈现，这是两个不同的含义。如果仔细体会一下，大概我们能够理解维特根斯坦这里说的一个深刻内涵——所有关于思想的内容，当我们不能够用逻辑的语言去表达的时候，我们到底在说什么？这是一个很重要的问题，维特根思言做到这一步的时候就停止了，他没有往下追问了，因为在他看来，他能够用逻辑语言表达的思想，基本上都在这本书里已经全部说的很清楚了，他认为没有必要说那些不能够用逻辑的语言去表达的思想，所有他说凡是我们不能用逻辑语言表达的思想我们只能够保持沉默。 维特根斯坦在这本书里面给我们提供了一个如何去理解哲学工作的方式。哲学工作的方式并不是在于我们能够用某一些哲学的概念和观念去提供给人们关于世界的理解，而是相反，所有这些概念跟观念其实都包含着一些能揭示我们对世界理解思想内容的一种陈述方式，这种陈述方式是逻辑的而非经验的，这是维特根斯坦特别强调的一点。所以他从罗素和弗雷格哪里得到了他们所谓的逻辑的观念，一切哲学只有按照这样的观念来做，我们才能真正在做哲学，除此之外都是无法在哲学中加以处理的，除此之外的东西是什么，就是那些不能够说只能被显示的东西。维特根斯坦1929年重返剑桥以后，实际上他的思想发生了一个很重要的变化，他开始重新思考他之前关于逻辑语言所表达的哲学观念这样一种思想是否恰当？大部分人都认为他放弃了之前的观念，而重新给出一种新的对哲学对语言的全新理解。但是，维特根斯坦其实并没有真的完全放弃他之前的那种观念，他只是转换了一个角度，他不再坚持原来的那种以逻辑的方式去观察世界，以逻辑的语言去表达我们思想这一部分的内容，这一部分的内容在他看来并没有错，它其实仍然可以保留在我们对逻辑语言的讨论当中，但是总有一些内容它是没有办法用逻辑的语言表达的，而对于那些不能够用逻辑语言表达的那些思想内容，我们还是不能用逻辑语言表达，但是他换了一种方式，他用语言游戏的方式来显示那些不能用逻辑的语言去加以表达的重要思想。他甚至认为这一部分的思想比用逻辑的语言表达的思想更为关键，在他的哲学体系中他认为这部分更为重要。 《逻辑哲学论》这本书他所表达的并不是他的全部思想，还有一部分是他这本书还有说出来的，而他认为没有说出来的那一部分还更加重要，这种说法隐含着他后期哲学里面所做工作的意义，为他后面的工作打下很好的基础。《逻辑哲学论》这本书讨论的是在逻辑的语言之内我们所能够从事的哲学工作和所能完成的事情，这一部分包括了他对思想概念和对于世界的理解。所以七个命题里面，前两个命题是关于语言和命题的讨论，最后一个命题关于不可说。 这七个命题听上去都很简单，但是事实上理解起来却非常困难。因为一开始我们接触第一个命题的时候就不知所云。第一个命题说，世界就是所发生的一切。很多人都在问是什么意思？如果按照逻辑的语言，这句话就是重言式，世界就是他所发生的一切，就相当于A等于A，但事实上不是，那么这句话的重要意义在什么地方？只有把这七个命题颠倒过来理解我们才能真正把握这句话的含义，把它的第一个命题当做他最后一个命题来理解，这个时候我们才能真正体会到维特根斯坦到底在说什么。因为他在最后一个命题里说，一切不可说的东西我们只能保持沉默，他的第二句话应该说，以下所说的东西都是可以说的。关于命题，关于思想乃至于最后关于世界这些说法，在他看来是可以说的，但是这个可说仅仅限制在逻辑语言范围之内，只有在逻辑的概念当中我们才能够去表达我们关于命题、关于思想以及关于世界的观点。后面的这六个命题，我们往回倒着读的话，首先就是关于命题的讨论，所有的命题都是由一些形式展现它的意义的，这是可以说的。 第二个命题是关于思想。所有的思想都是那些关于事态的命题，因为思想就是关于世界和事实的逻辑图像，也就意味着我们可以通过逻辑的方式来展现这个事实本身，这个展现的图形就是命题，因而，命题就像是一幅图画，它向我们展现了世界的真实情况，这个展现的内容就是意义，或者叫做思想。所以这就是维特根斯坦告诉我们的观点。由这个思想本身再进入世界，因而世界就是我们通过思想展现命题的方式来给我们提供关于在这个命题当中向我们描述的事实的构成形式。 因为世界是由事实构成的，所有的事实都表现在命题当中，所以我们所了解的世界的概念就是所有这些事实的总和，由此，世界就是它所发生的一切，那发生的一切就是事实的总和。所以我们就可以理解，为什么维特根斯坦一开始就说，世界就是它所发生的一切，而它所发生的一切，就是事实的总和。这几句话实际上是他最后要推出的一个结果，而不是他论述的前提。通过这样的解读，我们就可以理解在维特根斯坦前期哲学当中，他所谈到的界限或者限度这个说法，实际上表征的是他在逻辑语言范围内，所能够表达的一些关于思想跟世界的理解的内容，这是他在前期哲学当中一个重要的观点。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>维特根斯坦</tag>
        <tag>语言哲学</tag>
        <tag>逻辑哲学论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（14）：康德《纯粹理性批判》句读（1）—— 第一版序]]></title>
    <url>%2F2018%2F02%2F10%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8814%EF%BC%89%EF%BC%9A%E5%BA%B7%E5%BE%B7%E3%80%8A%E7%BA%AF%E7%B2%B9%E7%90%86%E6%80%A7%E6%89%B9%E5%88%A4%E3%80%8B%E5%8F%A5%E8%AF%BB%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%20%20%E7%AC%AC%E4%B8%80%E7%89%88%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[康德《纯粹理性批判》第一版的序宏观地展示了康德在写作这部著作时所面对的问题，许诺了他将要达到的结果，制定了它在整个体系方面所依据的方法论准绳，以及该著作在他整个哲学构思中的位置。 人类理性在其知识的某个门类里有一种特殊的命运，就是它为一些无法摆脱的问题所困扰；因为这些问题是由理性自身的本性向自己提出来的，但它又不能回答它们；因为这些问题超越了人类理性的一切能力。 “某个门类”显然是指形而上学，即纯粹哲学的门类；而“特殊的命运”是指理性的那些最高问题所带来的命运，或者说是一种厄运。理性只有在形而上学中才给自己提出一些他自己不能解答的问题，这是理性的不幸，当然，也是理性的能耐，只有理性才能在他的最高形态中给自己提出超出自己能力的问题。理性的这种自然倾向或者说“本性”自有他的积极意义。不过康德在这里一开始强调的是这种命运的消极面，这也正是他进入问题的入口，即要寻找一条解决理性的困境之道。下面他就来阐明理性的这种困境和引起这种困境的原因。 人类理性陷入这种困境并不是它的罪过。它是从在经验的进程中不可避免地要运用、同时又通过经验而证明其运用的有效性的那些基本原理出发的。 理性的困境之所以是它的“命运”，就在于它并不是有意要陷入进去的，而是从一种正当的要求中不知不觉地陷入的，这种正当要求就是在那些在经验运用中已经证明了其必然性和有效性的理性原理的要求。理性的基本原理在经验中是有效的，这个谁也否认不了，所以这些原理也是不可取消的；但也正是这些原理，当我们立足于它们来追求更高的目标时，就必然把我们带入歧途。 借助这些原理，它们（正如它的本性所将导致的那样）步步高升而达到更遥远的条件。 理性的基本原理具有一种“本性”（或自然），就是要从有条件者去追溯他的条件，也即不但要知其然“而且要“知其所以然”。而这种追溯按照理性的本性来说是无穷尽的，他总是要从近到远，从已知的东西到未知的东西，从经验知识到使这种经验知识得以可能的条件，以及条件的条件。 但由于它发现，以这种方式他的工作必将永远停留在未完成状态，因为这些问题永远无法解决，这样，他就看到自己不得不求助于一些原理，这些原理超越一切可能的经验运用，却仍然显得是那么不容怀疑，以至于就连普通的人类理性也对此表示同意。 理性的这种步步高升的工作永无止境，因而将“永远停留在未完成状态”；但完全停下来又不符合理性的本性，所以理性就“不得不求助于一些原理”，这就是那些超越一切可能经验范围之上的形而上学原理。这里用“一切可能经验”，不只是现在的经验，而且包括一切过去、未来可能有的经验，就是不但从现实的范围上，而且从性质上、本质上是超经验的，也就是超时空的。形而上学原理离开了经验的领域，他们不是要运用于任何经验之上，而是仅仅按照纯粹理性本身的逻辑法则来运行；但由于这些逻辑法则是普通的人类理性中固有的法则，所以虽然没有经验的内容来充实它们，这些抽象的理性原理和法则却仍然能够得到一般人的同意，甚至在他们看来是无可置疑的。 但这样一来，人类理性也就跌入到黑暗和矛盾冲突之中，他虽然由此可以得悉，必定在某个地方隐藏着某些根本性的错误，但它无法把它们揭示出来，因为它的使用的那些原理当超出了一切经验的界限时，就不再承认什么经验的试金石了。这些无休止的争吵的战场，就叫做形而上学。 这就是人类理性的厄运的来由。也就是说，人类理性出于它的本性要求寻找一个超出一切经验之上的最高的无条件者，以便使自己不再停留于未完成状态；但一旦它超出了一切经验，它就失去了用经验来检验自己原理的真理性的试金石，从而变成了公说公有理，婆说婆有理，陷入到“无休止的争吵”和无法解决的“矛盾冲突”之中。这就使形而上学成为了一个这类矛盾冲突的战场。而且，虽然形而上学充斥着这类矛盾，人们却既没有办法解决矛盾，也不知道这类矛盾从何而来，因为他们原先正是为了避免矛盾、特别是为了避免经验的有限性和理性的无限追求之间的矛盾，才跳出一切经验的范围而寻求理性的逻辑一贯性的。但现在矛盾并不是出现在经验和理性之间，而是出现在通常认为不可能有矛盾的理想本身内部。康德在后面关于纯粹理性的“二律背反”的讨论中所揭示的正是这种情况，即两个完全相互矛盾的命题似乎都各自有自己充分的逻辑根据，谁也消灭不了谁。当然康德在这里已经预先暗示，所有这一切矛盾都是由于这些原理超越了一切经验的界限，从而抛弃了经验这个“试金石”。因为关于知识和真理的问题，虽然我们必须借助于理性的先天原理，但这些先天原理是否运用的正当，还是要凭借经验来检验，离开经验的领地来谈知识和真理是不可能的，无论我们把理性的功能发挥到何等地步，如果没有经验的内容，都免不了空谈，甚至免不了陷入自相矛盾的尴尬。由此我们可以看出，康德本身虽然出自大陆理性派传统，但却体现出力图调和经验论和唯理论的主导倾向。不过在康德的时代，还没有人能够清楚地认识到康德所指出的问题，人们已经意识到“必定在某个地方隐藏着某些根本性的错误”，因为那些矛盾冲突已经表面化和白热化了，但他们却“无法把它们揭示出来”，而是仍然固执着自己的片面观点，不是局限于经验论，并由此走向怀疑论，就是执着于唯理论的独断轮。这种分裂局面就使形而上学的名誉遭到了极大的损害。而这就是康德当时所面临的绝望处境。 曾经有一个时候，形而上学称为一切科学的女王，并且，如果把愿望当做实际的话，那么她由于其对象的突出的重要性，倒是值得这一称号。 在历史上，形而上学自亚里士多德倚赖就被称为“第一哲学”，它是“物理学之后”或物理学之上，是一切自然科学和数学的指导科学，所以它是“科学之科学”。把形而上学视为”一切科学的女王“，这是西方两千年来的传统。康德对这个传统是有保留地认同的，就是说，”如果把愿望当做实际的话“，康德认为形而上学的主观愿望和意图确实是指向理性最重要的事业的，只不过她实际上并没有能够做到她想要做的事情罢了。但康德并不因为形而上学未能达到其目的而完全抛弃她，而是致力于用新的途径恢复形而上学的权威地位，重建形而上学。所以他对形而上学今天的这种不景气的状况深表同情。 今天，时代的时髦风气导致她明显地遭到完全的鄙视，这位受到驱赶和遗弃的老妇像赫卡柏一样抱怨：”不久前我还是万人之上，以我众多的女婿和孩子而当上女王————到如今我失去了祖国，孤苦伶仃被流放他乡。“ 赫卡柏是特洛伊城最繁荣时期的王后，特洛伊城被希腊人攻陷后，她失去了丈夫和所有的子女，本人变成了俘虏，传说最后被人们用石头砸死，又说她被众神变成了一条狗。她悲惨的遭遇引起了很多诗人的同情，包括欧里彼得斯、但丁和莎士比亚。康德在这里也是以同情的笔调引用她的命运来比喻形而上学的遭遇的。有种常见的说法，说康德摧毁了形而上学，抛弃了形而上学，这是不准确的。应当说他摧毁的是传统的形而上学，但却建立起了新的形而上学，使古老的形而上学焕发了青春。 最初，形而上学的统治在独断论的管辖下是专制的。 哲学上的”独断论“相当于政治上的”专制主义“，它不由分说，也不说明理由，而是以既定的意见和信念作为不言而喻的前提。在形而上学的王国利，长期以来都是独断论站统治地位，但由于这种专制统治必然要引起内部的分裂和外部的抗议，所以在近代它的地位就遭到了不可避免的动摇。 不过，由于这种立法还带有古代野蛮的痕迹，所以它就因为内战而一步步沦为了完全的无政府状态，而怀疑论者类似于游牧民族，他们憎恨一切地面的牢固建筑，便时时来拆散市民的联盟。 “古代野蛮的痕迹”指传统的非批判的粗糙信念；”内战“指独断论的内部分裂，如经验论和唯理论、唯物主义和唯心主义的分裂，其实各方都是独断的，都想实现自己的霸权。这就导致了”完全的无政府状态“，即公说公有理，婆说婆有理，没有一个公正的法庭来对各方的主张进行裁决，从而使各方都在打一场毫无结果的消耗战。但尽管如此，这些独断论毕竟在对真理的信念还是一致的，所以尽管处于无政府状态中，却还是处于共同的利益而建立起某种”市民的联盟“，以便个人经营自己的小地盘，井水不犯河水。但怀疑论者对于这种联盟则是一种更大的威胁，他们”类似于游牧民族“，扫荡一切地面建筑，解构一切固有的联系，如休谟的怀疑论就是无论经验论还是唯理论都不能接受的，它使一切认识论的形而上学探讨都失去了意义。 但幸好他们只是少数人，所以他们不能阻止独断论者一再地试图把这种联盟重新建立起来，哪怕并不根据任何在他们中一致同意的计划。 真正像休谟一样主张彻底的怀疑论的人毕竟只是少数，一般人认为他们只是走极端而已，并不认真对待他们提出的挑战。当时盛行的所谓”健全知性“的观点就是大多数哲学家所依赖的一种权宜之计，他们借此很简单地把休谟式的怀疑主义扫到一边，认为这是一种知性的病态。但是”健全知性“本身是一个模糊概念，究竟怎样才算健全，健全知性应该包含哪些要素，人们并没有明确的规定。所以通常人们只是利用这一概念的模糊性而建立起一种认识论上的”新的联盟“，也就是”可知论“的联盟，来共同对付休谟的不可知论。但其实在他们中并没有”一致同意的计划“，同样是主张健全知性，经验派的健全知性和理性派的健全知性却大不相同，他们的共同之处仅仅在于不走极端，以及为了捍卫科学的尊严和共同抵御怀疑论的进攻。 在近代，虽然一度看来这一些争论似乎应当通过（由著名的洛克所提出的）人类知性的某种自然之学（Physiologie）来做一个了结，并对那些要求的合法性进行完全的裁决； 洛克在《人类理解论》中对人类知性的方方面面进行了详尽的分析，这些分析都是把人类的知性当做一个自然对象来看待，因此康德把它不称之为形而上学或哲学，而称之为“自然之学”，也就是用物理学或自然科学的方式来研究人的认识的结构。在这方面，洛克可以说是做大了最大可能的完备和系统化，人们似乎可以认为他已经对人类认识能力的各种要素的作用的合法性做出了“完全的裁决”。 但结果却是，尽管那位所谓的女王的出身是来自普通经验的贱民，然而，由于这一世系事实上是虚假地为她捏造出来的，而她还一味地坚持她的要求，这就使得一切又重新堕入那旧的、千疮百孔的独断论中去，并由此而陷入到人们想要使科学拜托出来的那种被蔑视的境地。 洛克的哲学实际上还是经验论哲学，康德称之为“出身”于“普通经验的贱民”，并没有任何先天的高贵之处，按照康德的看法经验派必然要称为怀疑论；但是洛克依然主张这种经验知识具有形而上学的认识论意义，即能够认识事物本身，但这种要求实际上是经验派所不能提出来的，所以康德说这种形而上学的世系“是虚假地为他捏造出来的”。但洛克的这位女王仍然一味地提出这种认识事物本身的要求，这就使得洛克的哲学又重新堕入独断论中去了，也就是没有根据地认定自己的知识就是对自在之物本身的认识。而这种无根据也正是人们蔑视形而上学的根本症结，它使形而上学名誉扫地，得到了“伪科学”的恶名。 不过，康德如此批评洛克是有些不太公平的，因为在洛克的哲学中已经包含有后来由休谟发挥出来的不可知论因素了，他首先把我们从感官直接获得的感觉，如色、声、香、味等等，称之为事物的“第二性的质”，是由我们感官的性质所决定的，因而是主观的，只有像体积、形相、运动、数量等等才属于“第一性的质”，课件他并不完全相信我们的感觉经验，其次他认为即使第一性的质，也只是我们对于客观实在所知道的“名义本质”，而不是它们的“实在本质”，实体的实在本质我们是永远也不可能知道的。这就已经有不可知论的色彩了。这正是休谟后来提出怀疑论和不可知论的最初的思想来源。当然洛克的体系中是包含尖锐的矛盾的，他甚至不太关心使自己的这种矛盾调和起来，所以康德说他是独断论也没有错，只是不全面。 今天，当一切道路都白费力气地尝试过了之后，在科学中占统治地位的是厌倦和彻底的冷淡态度，是混沌和黑夜之母，但毕竟也有这些科学临近改造和澄清的苗头，至少器=是其序幕，他们是用力用得完全不是地方而变得迷糊、混乱和不适用的。 康德的时代哲学界就是这样一片混乱的状态，但也不是毫无希望，而是在这种混乱中包含着“这些科学临近改造和澄清的苗头”，这是康德独具慧眼看出来的。“这些科学指形而上学的各种形态，唯理论和经验论，独断论和怀疑论，康德自认为负有历史使命来改造和澄清它们。但他并不想把它们全盘抛弃，而只是想把它们做一个调和，吸取它们各自的长处，批判它们的缺点。他承认，这些学说都有其合理之处，只是“用力用得完全不是地方”，从而把问题搞乱了。但只要对他们所展示出来的问题进行一番彻底的清理，康德认为将拉开真正科学的形而上学的序幕。 因此康德对当代哲学中的无所作为的态度是很瞧不起的，他说： 因此，想要对这样一些研究故意装作无所谓的态度是徒劳的，这种研究的对象对于人类的本性来说是不可能是无所谓的。 形而上学在当代出现了问题，但是这些问题是非妥善解决不可的，因为这是涉及人类理性的生死存亡的大问题。但当时哲学界人们都在装聋作哑，用一些模糊概念来回避和打发那些棘手的问题，特别是对于休谟所提出的挑战装作视而不见，从而使科学的基础面临严重的危机。 上述那些冷淡主义者也是这样，不论他们如何想通过改换学院语言而以大众化的口吻来伪装自己，只要他们在任何地方想到某物，他们就不可避免地退回到他们曾装作极为鄙视的那些形而上学主张上去。 “上述那些冷淡主义者”，指上一句话中提到的在科学中“占统治的”冷淡态度，也就是所谓“拒斥形而上学”的态度，持有这种态度的人干脆放弃一切形而上学的努力，而对任何形而上学加以拒斥，甚至在语言上也加以改换，不用形而上学的学院语言，而尽量采取大众化的语言。这种态度在康德的时代还是刚刚萌芽，但在20世纪以来则蔚然成风，一切实用主义和实证主义以及语言分析哲学都在标榜这种态度。但康德指出，这些人拒斥形而上学是虚伪的，实际上他们暗中所遵循的是最坏的形而上学，这与恩格斯的见解如出一辙。形而上学，或者说哲学，是人类逃脱不掉的命运。 然而，这种在一切科学繁盛的中心发生并恰好针对这这些科学——这些科学的知识一当它能够被拥有，人们就无论如何也不会作出丝毫放弃——的无所谓态度，毕竟是一种值得注意和深思的现象。这种态度显然不是思想轻浮的产物，而是这个时代的成熟的判断力的结果。 康德尽管对当时哲学家们“装作无所谓的态度”不满，但仍然从这种态度中看出现代哲学思想的“成熟”因为这种态度只不过是当时人们为了维护科学的尊严而临时借用的一块挡箭牌，他们显然并不为休谟所提出的怪论而受到惊扰，而是对于已经到手的科学知识怀有一种不可动摇的信念。他们之所以装出无所谓的态度，只不过是他们还没有找到可以有力地反驳对方的方法而已，但他们绝不轻易为对方的能言善辩所迷惑、所打动，这一点确实康德十分赞赏的，也是他有信心让自己的批判获得广大科学界人士赞同的理由。他相信，在“一切科学繁盛的中心”所发生的这种“无所谓态度”，其实是一种谨慎的态度，他不是“思想轻浮的产物”，而是人类理性已经成熟到可以面对任何针对科学的挑战的表现，在这样一个基础上，康德批判才能够奏效。 在这里，康德有一个注释： 人们时常听到抱怨当代思维方式的肤浅和彻底科学研究的沦落。但我看不出那些根基牢固的科学如数学和自然学说等等有丝毫值得如此责备的地方，相反，他们维护了彻底性的这种古老的荣誉，而在物理中甚至超过以往。 显然，康德心目中的数学和自然科学（物理学）的地位是神圣不可侵犯的，他熟悉当时的各门科学，并且自己就是一位杰出的科学家。数学和自然科学是他的批判哲学一切理论的基础，他对科学的信念从未动摇过。所以，他的批判哲学并不是批判这些科学，而是对这些科学的哲学解释。当休谟对科学的普遍必然性提出怀疑的时候，康德的反驳并不在于科学的普遍必然性，在它看来科学的普遍必然性是一个不容否认的事实。他要证明的是这种普遍必然性是从哪里来的，它的根据和条件是什么。所以在某种意义上说，康德其实并没有正面反驳休谟，或者说，一个彻底的休谟主义者、怀疑论者是“驳不倒的”。一个人当他连科学都不相信了，你还能向他证明什么呢？你的一切证明不都是科学吗？所以康德只是在大家公认的科学事实的基础上提出自己的一整套原理，使人们看到这些科学事实是有自己牢固的根基的，这样来维护科学的权威。在它看来，科学精神最可宝贵的就是科学中的彻底性，自从古希腊欧几里得几何学产生依赖，这种科学精神就有了自己的典范。当然，这种彻底性一直贯穿到今天，他的集中体现者是大陆理性派哲学，康德出身于理性派，他对这彻底精神比任何人都更执着。它在牛顿物理学中看到了这种彻底性的当代楷模，这种物理学以极少数的原理贯通天上地下，万事万物，其规模和深刻性显然不是任何古代的科学思想所可以比拟的，康德受这种彻底精神的熏陶，力图将它引进到形而上学中来： 而现在，正是同一个彻底精神也将在另一些知识类型中表明其作用，只要我们首先留意对他们的原则加以校正。在缺乏这种校正的情况下，冷淡、怀疑，最后是严格的批判，反倒是彻底的思维方式的证据。 “另一些知识类型”也就是指哲学、形而上学的知识类型，康德的批判哲学正是要使形而上学本身具有如同牛顿物理学那样的真正的彻底性。但前提是，必须“首先留意对他们的原则加以校正”。以往形而上学的原则本身就是建立在一个错误的基础之上的，如果这个错误基础得不到校正，那么越是彻底，这种体系就越是错的远，正所谓差之毫厘谬以千里。例如休谟的哲学够彻底了，它彻底到几乎没人能够挑出它的逻辑上的毛病，但正因为如此，他的哲学就成了一种无人能够接受的怪论。这种校正显然是康德准备留给自己来做的一件重要的工作。不过，即使这种校正还没有完成，康德已经在上述的冷淡态度和怀疑中，以及在他自己首先提出的“批判”中，看到了“彻底思维方式的证据”。冷淡态度其实也是一种不信任的态度，就是说，当人们还没有被彻底说服的时候，正是出于对彻底性的要求，反而使他们保持一种冷静，对一切奇思怪想姑妄听之。怀疑态度也是这样，就是说，你如果拿不出确凿的证据使我彻头彻尾的信服，那我就宁可停留在怀疑之中。康德的批判则是更进一步，力图对那些貌似有理的理论体系之所以可能的前提进行考察，对凡事不能彻底自圆其说的理论呢都加以清除，以便在真正经得起批判的基地上重建形而上学。其实批判本身已经是校正工作了，它是校正的第一步，但校正的完成则有待于康德自己的形而上学体系的建立。无论如何，所有这些倾向在康德看来都表明一种时代风气，即对彻底的思维方式的严格要求，他自己的批判哲学在这样一种氛围中正是基础体现了这个时代的时代精神的需要。 这就是康德的下面这句名言的意义： 我们的时代是真正批判的时代，一切都必须经受批判。 康德在他的如此晦涩艰深的哲学研究中竟然体会到了时代精神的内在的脉搏，或者说，真正深刻的哲学都是时代精神的反映，康德是有这种自觉意识的，这也可以说他殚精竭虑、穷其一生来进行哲学批判的内部热情和动力。 下面他点名了两个在他看来最需要批判的现实邻域： 通常，宗教凭借其神圣性，而立法凭借其权威，想要逃脱批判。但这样一来，他们就激起了对自身的正当的怀疑，并无法要求别人不加伪饰的敬重，理性只会把这种敬重给予那经受得住他的自由而公开的检验的事物。 一般人很难由康德的《纯粹理性批判》联想到宗教和立法这样一些现实生活的内容，虽然他在书中又对上帝存在的证明的各种批判，也经常引用法律方面的例子来说明概念的关系，但是这部著作的主题只是为科学知识和认识论提供先天根据，而并没有明确表示出某种激进的政治倾向。然而，正是这样的理性批判，对于当时统治整个社会的保守势力具有最强烈的摧毁作用。因为通过批判，康德所建立起来的是一个至高无上的理性法庭，凡是不符合理性的要求的事物都将被扫进历史地垃圾箱。从这里我们可以看到康的哲学的启蒙意义，以及对当时德国现实生活的冲击的力度。康德之所以把这样一段重要的话放在不起眼的注释中，大约连他自己都感到了他的批判的锋芒和威力，为了稳妥起见，有必要藏起来一些。 下面的正文恰好也表达了类似的意思： 这个时代不能够再被虚假的知识拖后腿了，它是对理性的吁求，要求他重新接过他的一切任务中最困难的那件任务，即自我认识的任务， 时代要求理性作出它的“自我认识”，即对理性自身进行再次的考查。自文艺复兴以来，人们已经意识到自己的理性了，莱布尼茨—沃尔夫派的启蒙理性也已经把理性摆到了一个“法庭”的位置上；但康德提出的是一个更高的要求，即理性应该对自己起诉，对自己以往所获取的那些“虚假的知识”起诉，以便对自己重新加以检查。所以康德的纯粹理性批判其实是理性的自我批判，是更高层次的理性，而这也是理性的“一切任务中最困难的那件任务”。所以康德的理性法庭是为理性自己而开的。 这就是： 委任一个法庭，这个法庭能够受理理性的合法性保障的请示，相反，对于一切无根据的非分要求，不是通过强制命令，而是能够按照理性的永恒不变的法则来处理，而这个法庭不是别的，正是纯粹理性批判。 理性要在这个法庭面前为自己的合法性取得保障，也就是排除一切由于理性的滥用而提出的“非分要求”，如何排除？仍然是“按照理性的永恒不变的法则”来排除。而这个法庭就是他的纯粹理性批判。这就开始把话题引入到正题里面来了。 以上就是康德对于他在他的时代所面临的问题的一个交代和概括，主要是当时的哲学所遇到的难题以及由此反映出来的时代精神。下面开始介绍这本书的主题内容，也就是他将要在书中所阐明和解决的问题。 但我所理解的纯粹理性批判，不是对这些书或体系的批判，而是对一般理性能力的批判，是对纯粹理性可以独立于任何经验而追求的一切知识来说的，因而是对一般形而上学的可能性或不可能性进行裁决，对他的根源、范围和界限加以规定，但这一切都是出自原则。 康德的纯粹理性批判并不是着眼于对上面所触及到的现有的那些哲学著作或哲学流派的批判，他当然要涉及这些哲学流派，而且甚至处处都在和这些哲学家对话，但它的目的并不是具体针对某个人、某本书，而是更高也更加一般的主题。康德认为他是要对“一般理性能力”进行批判，“是就纯粹理性可以独立于任何经验而追求的一切只是来说的”。通常的批判都是针对具体某本书、某个人、某种思想或某个被宣称的知识；单抗的目标超越于这一切之上，他是要一劳永逸地彻底解决问题，即针对理性能力本身，看它独立于经验而有可能追求到一些什么样的知识。通过这种批判，我们就能够对那个最高的形而上学问题，即“一般形而上学的可能性或不可能性”的问题作出裁决。这就比通常形而上学所探讨的那些内容，如形而上学的基本概念是什么，他们的关系和结构如何，他们与现实的各门科学怎样发生联系等等，要更高一层，因为它涉及任何形而上学是否以及如何可能的问题。如果形而上学根本就不可能，那么所有关于形而上学的理论再好也都是虚假的。而如果形而上学是可能的，那么它又是如何可能的？它的“根源、范围和界限”如何？这同样也是先于一切其他形而上学问题的元问题。总之，康德就是要在进行形而上学的探讨和具体规定之前，先将我们人类理性建立形而上学的能力和可能性搞清楚，或者说，要在认识之前预先清点一下我们用来认识的工具，检查一下这些工具的作用和效能，以免到头来力不从心。但康德强调“这一切都是出自原则”，也就是出自理性本身。康德在后面多次提到，理性归根到底是一种“原则的能力”。通常“原则”比“原理”和“法则”要高，比“规则”更高，是康德认为最高层次的规律。当然这些术语有时也不太严格，康德经常违反自己的规定，但大致上可以这样区分。总而言之，康德在这里想表达的是他的理性法庭是理性对理性本身的审判，最终还是要遵守理性的原则。后来黑格尔批判康德说，他就像一个游泳教练，告诫自己的学生说“未学会游泳之前切勿下水”。康德要在认识之前先对认识的工具进行检验，也就是进行认识，这本身是一个悖论。因为当你检查理性工具时，你已经运用了理性，而你所运用的这个理性必然又是没有经过检查的，这就会导致无穷后退，而一步也迈不开。不过，康德把理性提升到自我意识，这毕竟是他的一大功劳，从此理性就在通常的逻辑理性之上，又增加了一个“批判理性”，即理性的自我批判，这就为后来黑格尔的理性的自我否定运动、即“否定性的辨证论”提供了启示。黑格尔的自我否定就不再是一次性的从一个不可否定的前提出发，而是表现为一连串的否定运动，从中生发出一系列的范畴。在康德这里还未达到这种理解，理性的自我否定有一个上限，这就是理性的最高“原则”，其他的都是从那个原则降下来的。但这在当时至少也是一个很大的推进，是从来没有人做过的工作。 所以康德接下来说： 现在我走上了这条唯一留下来尚未勘察的道路，我自认为在这条道路上，我找到了迄今使理性在摆脱经验的运用中与自身想分裂的一切谬误得以消除的办法。 经验论和唯理论都不可能摆脱理性的纯粹运用时产生的自身分裂，例如二律背反的分裂，因为在涉及理性脱离经验的运用时，他们都从不同的方向不由自主地陷入了独断论；而怀疑论则只不过是发现了这种分裂，但却毫无批判地认可了这种分裂，从而停留于怀疑。迄今为止，还没有人从批判角度来对待理性的这种自我分裂，即从理性本身去寻找分裂的原因，检查理性运用的范围，它的那些原则的根源，以及他的限度。在康德看来，我们只要找到了理性在理论的运用中的范围和界限，并将它的最高原则作出定位，分清理论的理性和实践的理性的界限，分清可以认识的现象和不可认识的自在之物，理性的这些分裂自然就会烟消云散。 对于理性的这些问题，我不是例如通过借口人类理性的无能而加以回避，而是根据原则将它们完备地详细开列出来，并在把理性对它自己的误解之点揭示出来之后，对这些问题进行使理性完全满意的解决。 在康德看来，休谟式的怀疑本质上是对人类理性的无能的承认，他提出了问题却不能解决问题，而只是回避了问题。康德却试图“根据原则将他们完备地详细开列出来”搞清他们的来龙去脉，然后做出“使理性完全满意的解决”。显然，在他看来休谟之所以不能解决这些问题是由于他的方法不对，不是理性的系统的方法，只是凭借机智而纯粹经验性地发现问题，所以他不能看出问题的症结所在。康德却根据这些问题的性质而在逻辑上对他们进行了条分缕析的清理，只有这样，理性的自相冲突的根源才会显露出来，而抓住这个最终的源头，这些矛盾才有可能迎刃而解。也只有这样，这种解决问题的方式才是理性的，才是使人的理性完全信服的。 虽然对那些问题得出的回答根本不是像独断论的狂人的追求者们所可能期望的那样；因为这些人除了我所不在行的魔法的力量之外，没有什么能够使他们满足。 独断论者在这里被康德等同于魔法，是因为他们试图不根据任何确凿的出自原则的理由就断言事物出身的状况，并对理性所产生的各种幻想信以为真。在他们看来，理性的冲突如果要得到满意的解决，就必须提供出某种非理性的理由，不论是莱布尼茨的“前定和谐”，还是贝克莱德“上帝的知觉”都可以凭空想象出来，而康德则认为自己在这方面不在行，实际上带有讽刺意味。 然而，这倒也并非我们理性的自然使命原来的意图；哲学的职责曾经是：消除由误解而产生的幻觉，哪怕与此同时还要去掉很多倍高度评价和爱好的妄想。 就是说，独断论的那种狂热追求并不符合理性的“自然使命”，因为“哲学的职责”并不是去猜测和强求那种不可能获得的知识，而是要消除“由误解而产生的幻觉”，也就是一种否定性的职责。知识近代哲学在他的发端出一开始就显示出来的理性的作用，例如培根的“四假象”说，就是要把充斥在人们思想中的各种假象清除掉；笛卡尔的“怀疑一切”原则也是如此，凡是不符合理性或与理性的原则有出入的东西都一概被置于怀疑之中。在这样做时，要坚持一种彻底性，“哪怕与此同时还要去掉很多被高度评价和爱好的妄想”，这一点前人已经做出了榜样。在康德看来，理性是一切真理的标准，它的作用首先是批判，是否定那些本该否定的东西，不论他们戴着怎样的权威的面具，或是被众多的追求者所看重。单抗的自认为他在这方面比他的先驱者更上一层楼。 在这件工作中我把很大的关注放在了详尽性方面，我敢说，没有一个形而上学的问题在这里没有得到解决，或至少为其解决提供了钥匙。 也就是说，康德在清除哲学中流行的误解和幻觉的时候，首先在“详尽性”方面超过了前人。这种详尽性，康德理解为系统性，而不是细节上的繁琐性。所谓“没有一个形而上学的问题在这里没有得到解决”，就是说无一遗漏地把所有的形而上学问题都系统考察了一遍并做出了妥善的解决，”或至少为其解决提供了钥匙“。例如传统形而上学的三大问题，灵魂宇宙和上帝的问题，康德在这里都一一作了处理，揭示了它们的谬误和幻相，排除了由此形成的一系列伪科学；但同时又为它们的那些命题的真实含义留下了地盘，即认为它们作为实践理性的一些悬设还是可以接受的。当然在《纯粹理性批判》这部著作里作者讲的形而上学主要还是自然形而上学，或者说理论理性中的形而上学，而不是道德形而上学，道德形而上学是有待于康德自己去建立的，它有关实践理性。所以，，在理论理性中没有得到最终解决的形而上学问题，康德是把它们放到实践理性中去解决的，而在《纯粹理性批判》的结尾部分则对此作了明确的提示。所以康德可以大胆地说，所有的形而上学问题在他这里都得到了解决，因为他不是凭借一些偶然的经验考察或机智的联想，而是按照严格理性的原则来梳理和安排这些问题，使他们处在一个必然的关联之中，所以才能做到无一遗漏。 事实上，就连纯粹理性也是一个如此完善的统一体：只要他的原则哪怕在它凭自己的本性所提出的一切问题中的一个问题上是不充分的，人们就只好将这个原则抛弃，因为这样一来它也就无法胜任以完全的可靠性来处理任何其他问题了。 这表明康德的方法是一种严格系统化的方法，它不是到处去搜集问题，而是凭借纯粹理性的原则去推导问题，他相信由此所推导出来的问题系统不可能是不完备的，而真正的理性原则应该是放之四海而皆准的，只要有一个例证不能为这种原则所证明，这个原则就应当弃置不用。这正是理性派哲学典型的方法，罗素曾在他的《西方哲学史》中称之为”倒金字塔“的体系，这种体系只要抽掉了底下的一块砖，整座建筑就会垮下来。当然罗素所推崇的经验派的”金字塔“形的体系，即如果你从它底下抽掉一块甚至多块砖，对于整个体系并没有多大妨碍。但这种”金字塔“形的体系在康德看来根本算不上什么体系，而只是一些临时应付的偶然观点的堆积，他的解释力是有限的、就事论事的，揭示不了任何普遍的法则，当然也就不存在什么“完全的可靠性”了。康德所追求的则是一种规律性的原则，它能够以一统多，没有例外，只有这样的原则才能获得必然的可靠的知识，使问题得到彻底的解决。可见，他对他的方法的这种自信是建立在一个理性主义者对理性原则的普遍性的信念之上的，他认为它的优势就在于在形而上学这个复杂的领域内他的方法具有完全的彻底性，能够全面颠覆以往一切形而上学，而从一个崭新的基地上重建新的大厦。他对自己体系的这种划时代的重大意义是有充分的自觉和自豪的。这一段后面的这两句话实际上已经从《纯粹理性批判》一书的主题转入到写这本书的方法了。但在对他的方法展开全面阐述之前，他马上想到他对自己的方法的这种自信所可能引来的异议，于是在下面一段中先作了一番澄清。 说到这里，我相信可以在读者脸上看出对于表面上似乎如此大言不惭和却不谦虚的要求报以含有轻蔑的不满神态，然而，这些要求比起那些伪称要在其最普通的纲领中，证明例如灵魂的单纯本质或最初的世界开端的必然性的任何一个作者的要求来，还算温和无比的。 康德似乎担心自己的这种自信是否会给人带来反感，但他马上找到了辩护的理由，即他的这种自信比那些说大话的独断论者其实要谦虚得多。那些人声称他们提出的纲领是“最普通”的，因而看起来“很谦虚”，但却许诺要在关于灵魂和世界整体的方面提供出纯粹理性的知识，实际上狂妄的很。而康德的目标却只不过是指出和严格遵守人类理性的限度，对于不可知的东西就应该保持谦虚的态度。所以他说： 所以他说： 因为这种作者自告奋勇地想要把人类知识扩展到可能经验的一切界限之外，对此我谦卑地承认：这种事完全超出了我的能力。 康德自认为自己的方法是彻底的、无所不包的，只是由于他对人类的认识能力作出了严格的限制，他只是在认识能力本身的合法范围内，即“可能经验范围内”使理性的运用具有了彻底性，所以看起来好像很骄傲，其实是谦虚的，而独断论者却凭借一般常识就把理性的能力提高到与之不相称的地步。这里的“可能经验”一语是一个很有用的概念，后面还要经常遇到的，说明康德看重经验，反对理性脱离经验，但并不是狭隘的经验论，他指的经验是在先天条件下可以预见到的经验，不仅是眼前所见的经验，而且包括一切以往和将来的经验。不过即使有先天的条件，可能经验毕竟是经验，它对知识的范围做了确切的限定，康德承认，到可能经验的一切界限之外去寻求知识这件事“完全超出了我的能力”。 相反，我只想和理性本身及其纯粹思维打交道，对他的详尽的知识我不可以远离我自己去寻找，因为在我我自身中发现了它们， 康德反对独断论者把知识扩展到超出可能经验之外的理由正在于它们没有检查理性本身的能力，而是借助于某种高于人类理性的力量，相反，康德却“只想和理性本身及其纯粹思维打交道”，首先把我自己的思维结构搞清楚，在我自身内部探求纯粹理性本身的“详尽的知识”。只有这样，我们才能明智地恪守自己的本分，不去强求那些明知超出了自身能力的所谓知识。 在这方面我甚至已经有普通逻辑作为例子，即逻辑的一切简单活动都可以完备而系统地列举出来； 纯粹理性的内部结构首先体现在普通逻辑也就是传统的形式逻辑中，形式逻辑的知识有一个最突出的特点，就是它的那些基本原则自从亚里士多德以来就已经完备的形成了系统，两千年间几乎没有大的变化。在康德心目中，正如一切理性派哲学家一样，形式逻辑是任何严格科学的楷模，因为它最先全面地体现出人类理性思维的“详尽性”，也就是完备性、系统性，这种系统性是不必超出理性能力的界限之外而在自身之内就可以完成的。当然形式逻辑在康德看来也并不代表一切，而只是人类理性思维的形式规律，它的详尽性和系统性也只限于这个层面，康德只是“作为例子”而将它提出来，所以康德接下来补充说： 只是这里有一个问题，即如果我抽掉经验的一切素材和成分，我凭借逻辑可以大致希望有多大的收获。 也就是说，形式逻辑不管经验的内容，而只着眼于思维的形式，这些形式尽管可以详尽而系统地列举出来，但它们毕竟只是形式的知识，还不是我们真正想要达到的现实的知识。以往的逻辑学家们一个最大的误解就在于，他们以为单凭逻辑上的形式推理就可以获得一些现实的客观知识，这就导致了理性派的独断论的狂妄。所以，形式逻辑固然值得推崇，但是如果不考虑经验直观的内容，这些逻辑形式并不能给我们带来更大的收获，更不用说给我们带来真理性的知识了。所谓真理不仅是一种正确的思维方式，而且是思维和对象的符合，这就需要考虑经验的东西，所以在人类理性本身的系统知识中，除了形式逻辑之外应该有一种指导我们如何去获取经验知识的纯粹理性原理，这就是康德在本书中提出的所谓“先验逻辑”。当然先验逻辑也是以形式逻辑为楷模并从形式逻辑中引导出来的，这是后话了。 上面一段是插入进来的，下面才开始全面归纳他自己在本书中所采用的方法。 在达到每个目的方面注重完整性的同时，也注重在达到一切目的方面的详尽性，这些并非任意采取的决心，而是知识本身作为我们批判研究的质料的本性向我们提出的任务。 注意，这里提出了两个概念，一个是“完整性”，一个是上面说的“详尽性”。 完整性是针对“每个目的”本身而言的，详尽性是针对“一切目的”而言的。其实，就研究的对象或“质料”而言，这里的完整性和详尽性都是一个意思，就是要考虑得周全和完备。它们都属于“量”的范畴，一个是内包的量，一个是外延的量。而“内包的量”在康德后面的知性原理体系中有时候又被归于质的原理，即“知觉的预测”之下。所以这两个要求实际上分别属于质的方面和量的方面的要求，它是由康德后面范畴表上的质的范畴和量的范畴所决定的。这就可以理解，为什么康德在这里说这两个要求并不是“任意采取的决心，而是知识本身作为我们批判研究的质料的本性向我们提出的任务”，他们实际上是康德自认为放之四海而皆准的“范畴表”对于任何一个研究对象在质料上提出的要求。康德的范畴表有四大类范畴，即量、质、关系和模态，他把量和质称之为“数学性的原理”，把关系和模态称之为“力学性的原理”，有时又把前者称之为一个对象的“构成性原理”，而把关系和模态称之为“调节性原理”。而在这里，他把量和质的要求归于研究对象的质料，下面则把模态的要求归于对研究对象的形式的要求。总之，康德对于研究方法的选择不是随意的，而是根据研究对象的情况而定的；而研究对象又被纳入到他自己的那一套逻辑框架之中。所以他的方法本身是有方法的、系统化的，这是与以前一切哲学家都不大相同的地方。 下面是他对于形式方面所提出的要求 再就是确定性和明晰性这两项，这涉及这门研究的形式，它们必须被看做人们对一个敢于做这样一种难以把握的工作的作者可以正当提出的基本要求。 根据康德下面的具体解释，“确定性”属于模态范畴是没有问题的，但是“明晰性”属于什么范畴，这个下面再谈。前面两个要求都是涉及研究对象本身的构成的，即加入你连对研究对象都没有一个周全的概览，或者有些问题和目标在你的视野之外的话，那当然就不可能得出经得起推敲和质疑的正确结论了；但是光有了全景式的视野，还不足以使你的结论达到确定性和清晰性。所以后面这两种方法更具有方法论的形式法则的意义，康德指出他们“涉及到这门研究的形式”，以与上面讲的这门研究的“质料”相区别。应该说，确定性和明晰性是对作者提出了更高的要求，康德说它们“必须别看做人们对一个敢于做这样一种难以把握的工作的作者可以正当提出的基本要求”，虽然是基本要求，但是针对这样一种难以把握的工作所提出来的，而不是对一般研究提出来的；而从这两个要求的性质来看，它们不仅牵涉到研究对象的完备性，而且已经牵涉到研究对象本身的真理性了。所以康德对他们特别重视，花了更大的篇幅来谈他们。先谈确定性。 谈到确定性，那么我们曾经对我自己作过一项决定：在这类的考察中不允许任何方式的意见，一切在其中只是被视为类似于假设的东西都将是禁品，即使以最低的价格也不得出售，而必须一经发现便予以封存。 这里和“确定性”相对立的是“意见”，而在西方传统哲学中，与“意见”相对的正好是“真理”，所以他这里的“确定性”也相当于“真理性”。意见总是动摇的，真理则是确定的，这是柏拉图以来的理性派的观点。康德的这项“决定”显然是由他所出身的大陆理性派哲学所带来的，在他看来意见只是一种“被视为类似于假设的东西”，在这样的一种严格的学术研究中是必须禁止的。当然，康德自己在本书中也作了一些假设，包括纯粹实践理性的“悬设”；但他强调他的这些假设完全是建立在确定性的原理上的。他在后面的“先验方法论”部分甚至专门辟出一节来谈“纯粹理性在假设上的训练”，他在那里说：“如果想象力不应当是狂热，而应当是在理性的严格监视下的构想的话，那么就总是必须预先有某种东西是完全确定的，而不是虚构出来的或是单纯的意见，这种东西就是对象本身的可能性。这样一来就可以允许人们为了对象本身的现实性而最后求助于意见，但这种意见为了不至于是无根据的，就必须与作为解释根据的现实地给予的、因而是确定了的东西连结起来，于是这种意见就叫做假设。”并且他把这种假设的作用限定为“在纯粹理性领域内只容许作为作战武器，不是为了在这上面建立一种权利，而只是为了捍卫这种权利”。纯粹理性的权利已经由确定性建立起来了，才有可能用假设来捍卫它，来对付那些同样只是一些假设的对方观点的攻击。所以作为这项研究本身的任务只能是寻求确定性。而这种确定性在康德看来只能够通过先天性来保证： 因为每一种据认为先天地确定的知识本身都预示着它要被看做是绝对必然的，而一切纯粹先天知识的规定则更进一步，它应该是一切无可争辩（哲学上）确定性的准绳、因而甚至是范本。 先天确定的知识的性质和意见是根本不同的，它应当是“绝对必然的”，而不是偶然的、可以这样也可以那样的。为什么这里说“据认为”先天确定的知识？这里面包含着没有说出来的意思，即你认为是先天确定的，而实际上是不是这样还不一定。康德在后面曾提到一种先天必然性，例如一个人挖一栋房子的基脚，他完全可以预见到这样挖下去房子一定会倒塌，这种知识就被认为是先天确定的，而实际上它是由以往的其他经验所证实了的，所以归根结底还是后天经验的。但无论如何，凡是被认为先天确定的知识都是“预示着它要被看做是绝对必然的”，这一点却是无可怀疑的，至少你心目中是这样预计的。所以接下里康德就讲：“而一切纯粹先天知识的规定则更进一步”，所谓“纯粹先天知识”和“据认为先天地确定的知识”就不同了，后者是可以争辩和讨论的，即这种知识究竟是不是先天的还未定；它也许在这个场合下是先天的，因为房子还未倒，你已经预见到了；但它掺杂任何经验成分的知识，就比那种不纯粹的先天知识更进一步了。在什么方面进一步了呢？“它应当是一切无可争辩的（哲学上）确定的准绳，因而甚至是范本”。就是说，在这里这种确定性在没有什么可争论的了，而且不但是无可争辩的确定性，还是这种确定性的“准绳”和“范本”，也就是最高确定性，衡量一切确定性的确定性。所以这种最高确定性就是“哲学上的”确定性，它是康德在这里所努力追求的。 我在这里自告奋勇做的这件事在这一点上是否做到了，这完全要留给读者来判断，因为对于作者来说应做的只是提供根据，却不是判断这些根据在法官那里得出的结果。 这里康德表示了一点谦虚，当然实际上他是很自信的，他要由读者来判断自己的成绩正表明他的自信，即他认为任何一个有理性的人都能够通过自己的理性判断而对他所达到的确定性的知识深信不疑。 但为了不至于有什么东西不负责任地削弱了这些根据，所以倒是可以容许作者自己对那些容易引起误解的地方，即使它们知识涉及附带的目的，也加以注解，以便及时地防止在主要目的方面读者在其判断的这一点上哪怕只有丝毫的怀疑所可能产生的影响。 这里的“但”说明，虽然康德表示了自己的自信，但是他仍然担心读者会在某些地方产生误解，认为他并没有做到真正的确定性，而是自己引入了某些“意见”或“类似于假设的东西”。为此他需要对那些容易引起误会的地方加以解释，这些地方主要是那些“涉及附带目的”的地方，但在这些地方所引起的误解很可能会影响到那些“主要目的”，这是康德所要极力防止的。所以下面一大段就是专门谈“容易引起误解的地方”的，实际上只谈了一处地方，在康德看来也是最重要的地方，就是关于纯粹知性范畴的“先验演绎”的讨论。这个部分也是康德在第二版中作了大量修改的部分之一，另一个修改得更多的地方是关于理性心理学的批判。这两处修改都是为了解决同一个误解，即当时有人对《纯粹理性批判》第一版妄加解释，认为康德的体系不过是贝克莱主观唯心主义哲学的翻版。其实在第一版中，康德已经预见到了这一误解了，所以他在这个序言中提出要对书中容易引起误解之处进行注释。当然他此时所关注的还只是“先验演绎”，但他认为这是最重要的关键，只有在这里把误解澄清了，后面也就不会发生误解了。 看下面这一段。 我不知道在对我们所谓知识的能力加以探索并对其运用的规则和界限进行规定的研究中，有什么比我在题为纯粹知性概念的演绎的先验分析论第二章中所从事的研究更重要了；这些研究也是我花费了最多的、但我希望不是没有回报的精力的地方。 这就是说，在他看来他的纯粹理性批判最重要的部分就是“范畴的先验演绎”部分。要对我们的认识能力作批判的考察，要确定他运用的规则和范围，最为关键的就是要对知性范畴如何能够运用于经验性的材料之上做出说明，而这种说明主要就是在“先验演绎”部分进行的。这一部分也是康德整个《纯粹理性批判》中最为艰深难读的部分，是康德“花费了最多的、但我希望不是没有回报的精力的地方”。实际上先验演绎所涉及的是一个康德认识论的根本问题，即他的“哥白尼式的革命”何以可能的问题。康德认识论对传统认识论作了一个颠倒，即把“观念符合于对象”倒转为“对象符合于观念”，把主观符合客观变成了客观符合主观。这样一来，主观观念如何能够必然具有客观效力就是非解决不可的问题，而这正是先验演绎所要解决的问题。先验演绎的任务就是要证明主观的先验范畴所建立起来的知识不是单纯主观中的观念，而且也是有关客观经验对象的知识，因为所谓认识的对象不过是主观范畴能动地建立起来的。但正是在这一论证过程中，康德预计到有可能发生严重的误解，即把它误解成一个贝克莱式的主观唯心主义者。 他下面就对这种可能的误解加以预防。 但这一颇为深入的考察有两个方面。一方面涉及到纯粹知性的那些对象，应当对知性的先天概念的客观有效性作出阐明和把握；正因为这也是属于我的目的中本质的方面。 他首先强调他的演绎中“本质的方面”就是对知性范畴的“客观有效性作出阐明和把握”，也即是他所谓的“客观演绎”方面。这方面涉及到知性认识的对象，它解决的是“关于经验对象的知识何以可能”的问题，这个问题也等于“经验性的东西作为对象何以可能”的问题。康德对此的解答是，经验性的东西如果没有先验的范畴来规范，它们根本就是一团虚幻的过眼云烟，是完全主观的表象，哪里会有什么客观性呢？这就会堕入到贝克莱和休谟的主观唯心主义和怀疑论中去。但如果经过先验范畴的整理和规范，它们就会凝聚成一个“对象”，且只有这样它们才具有客观性，才能成为有关客观对象的知识。所以从经验的方面看，要么就没有客观对象，要有客观对象就离不开先验范畴的作用，这就是鲜艳范畴必然具有客观效力的证明。所以认识的客观性归根到底是由知性范畴所先天地带来的，只有范畴才能给经验赋予客观性，而靠经验后天地接受只能获得主观性。说明这一点就是康德先验演绎的主要目的。不过，除了这一主要目的之外，他还有一个次要目的，这就是： 另方面则是着眼于纯粹知性本身，探讨它的可能性和它自身立足其上的认识能力，因而是在主观的关系中来考察它，但即使这种讨论对我的主要目的极其重要，但毕竟不是属于主要目的的本质部分；因为主要问题仍然是：知性与理性脱离一切经验能够认识什么、认识多少？而不是：思维的能力自身是如何可能的？ 这就是他所谓的“主观演绎”的方面。简单地说，前面的客观演绎主要是探讨知性的对象何以可能，这里的主观演绎则是探讨知性本身何以可能。当然这两方面是有联系的，知性对象何以可能，这里的主观演绎则是探讨知性本身何以可能，当然这两个方面是有联系的，知性对象何以可能，康德是把它归结为知性的能动活动，那么知性到底是如何活动的，这个问题也就是必须加以探讨的了。所以后面这方面的探讨对于前一方面的目的而是“极其重要的”，但它的重要性毕竟不能和前一方面相比较。“因为主要问题仍然是：知性和理性脱离一切经验能够认识什么、认识多少？而不是：思维能力自身是如何可能的？”这里“脱离一切经验”是说，先于一切经验，即先天地能够“认识什么”。当然知性脱离一切经验实际上什么也不能认识，它只能用于经验；但康德在这里所关注的是，在经验知识中有哪些成分是知性先天赋予的，而不是从经验中来的。在这个意义上这些成分是知性“脱离一切经验”而认识到的，这就是诸范畴。至于“认识多少”，这个问题主要是针对理性的狂妄而提出来的，理性总是不满足于知性所获得的那一点点关于现象的知识，而时刻想要把知识的范围扩展到自在之物身上去，所以必须对它的这种狂妄进行批判，加以限制，树立一个界碑。这就是康德在“先验逻辑”的两个主要部分所做的工作：在先验分析论中讨论知性，他主要解决了“认识什么”的问题；在先验辩证论中讨论理性，他主要解决了“认识多少”的问题。当然实际上在每个部分中都涉及到这两个问题，但具体的解决是分两步走的。所有这些都是客观演绎所关心的主题，而这是主观演绎所不关心的，后者关心的是“思维能力自身是如何可能的？”即知性在建立客观知识的过程中，它的内部使如何运作的。这个知性内部的运作过程，康德在第一版的主观演绎中把它分为三个阶段或层次，即“直观中领会的综合”、“想象力中再生的综合”和“概念中认定的综合”，它们最终都依赖于并归结到先验自我意识的统觉的综合统一。当然这种分析就很有一些心理学色彩了，如果孤立起来看，确实也容易与贝克莱的主观唯心主义混为一谈。 对此康德解释说： 由于后一个问题仿佛是在寻找某个已给予的结果的原因，因而看起来在这里的情况似乎是，由于我允许自己发表这种意见，我也就不得不听凭读者发表另一种意见。 “后一个问题”，亦即上一句中的“思维的能力自身是如何可能的？”这一问题，看起来好像是要寻求一个假设，也就是通过假设一个先验自我意识的统觉来解释我们在认识活动中的这种构造经验对象的过程，而这种假设只不过是我对于这一过程的一种主观的意见，既然只是“意见”，则别人也完全有权采取和发表另外一种不同的意见。但康德说他“在另一个地方将要指出”，其实情况并不是如此。这里说的“另一个地方”，我们可以参看第一版演绎的最后一段，标题是“概述这个纯粹知性概念演绎的正确性和唯一可能性。”所谓“正确性和唯一可能性”，也就相当于这里所说的“确定性”的意思。康德在那段话里说道：“现在，说我们所研究的所有这一切现象、因而所有的对象全都在我们里面，亦即全都是我的同一的自身的诸规定，这种说法本身即把同一个统觉中诸现象的无例外的统一性表达为必然的了。”“通过纯粹想象力而对感性表象的综合，以及一切表象在与本源的统觉的关系的同一，是先行于一切经验性的知识的。所以，纯粹知性概念之所以是先天可能的，甚至在与经验的关系中是必然的，只是由于我们的知识仅仅与现象打交道，这些现象的可能性存在于我们自身中，它们的结合和（在一个对象中的）统一只是在我们里面才被找到，因而是必须先行于一切经验并使一切经验按其形式首次成为可能的。而从这个一切理由中唯一可能的理由中，也才引出了我们的范畴演绎。”这两段话的一个共同的意思就是，先验的演绎虽然是在主观中按照认识能力的层次（感官、想象力和知性）而展开，但实际上并不是一种心理学上的假设，而是对认识对象（现象）的可能性的一种先天必然的客观确定，所以并没有给贝克莱式的主观唯心主义留下任何可钻的空子。康德并不是从经验中“寻找某个已给予的结果的原因”，似乎这个原因在经验本身之外，我们虽然没有经验到它但可以猜测它。如果这样，这个“原因”就会仅仅是一种意见了，如同贝克莱对灵魂的一种假定一样，经不起休谟的怀疑论的攻击。相反，康德正是从经验的结构中分析出了它本身的先天形式（范畴），这个形式就体现在经验中，没有这种形式，经验本身就不可能，甚至不可想象。而一旦有了它，这个经验就借此而成了关于对象的客观知识，而不只是关于我的认识能力的主观知识。所以康德的主观演绎绝不是什么个人意见，而是任何可能的知识的先天结构的展示，当然这个要联系到客观演绎才能完成这层意思，主观演绎本来就是为客观演绎作准备的。所以他最后说： 在这种考察中我必须预先提醒读者：即使我的主观演绎不能对读者产生我所期望的全部说服力，但我在这里给予优先关注的客观演绎却会获得其全部力量，必要时单凭第92-93页所说的东西就足可以应付了。 可将康德是寄希望于它的客观演绎这一”主要目的的本质部分“能够把他的意思说明白，主观演绎只不过是一个引线，一个入口。这种情况有点像胡塞尔现象学的情况，胡塞尔也认为他虽然批判“心理主义”，但他的线性学还得要借助于心理学来进入，即从所谓“描述的心理学”、“纯粹心理学”入手，认为只要不仅仅局限于心理学的理解，就可以从中引出它的先验现象学的诸多原理。康德本人的意思也绝不是要讨论心理学问题，即人的认识能力本身的构造问题，而是要讨论一般可观知识的构造问题；所以他注重的不是思维活动的经验事实的描述，而是这种活动所构成的只是的先天必然性条件。 最后，谈到明晰性，那么读者有权首先要求有凭借概念的那种推论的（逻辑的）明晰性，淡然和也可以要求有凭借直观的直觉的（感性的）明晰性，即凭借实例或其他具体说明的明晰性。 那么，明晰性在康德这里是属于什么范畴的呢？按照康德在《逻辑学讲义》中的划分，它应该归于“质”的范畴。在《逻辑学讲义》中的“知识的特殊的逻辑完备性”这一标题之下，康德分别探讨了：“量”方面的“广泛性和彻底性或重要性和丰富性”，这相当于我们上面讲到的“外延的量和内包的量”（即上面归入“量”和“质”的要求）；“关系”方面的“真理性”，包括形式逻辑的矛盾律和同一律、充足理由律和排中律，相当于我们上面讲的由确定性引出的真理性，只不过是从形式逻辑上讲的，而不是像这个“序言”中是从先验逻辑的角度谈的；“质”方面的“明晰性”，正好相当于我们这里接触到的明晰性要求；最后是模态（中译者许景行翻译为“样式”）方面的“确定性”，它与“意见”相对立，也恰好与这里的前述对研究对象的“确定性”要求相吻合。所以这里的划分与《逻辑学讲义》中的划分大致一致，区别仅仅在于，后者把前者中的“外延的量和内包的量”全部都划归于“量”了，而把“质”的位置留给了“明晰性”；此外，这里没有特别提出“真理性”作为“关系”方面的要求，而是把它合并到“确定性”这一“模态”中，作为从中引出来的一个要求。再就是《逻辑学讲义》中主要是从形式逻辑的层次来谈方法，而这里则既有形式逻辑的要求，也有先验逻辑的要求。现在，在当前这句话中，康德又从“明晰性”中区分出了两种不同的明晰性，即一种是“逻辑的”明晰性，另一种是“感性的”明晰性。这种区分也见于《逻辑学讲义》。康德在那里说：“首先我们必须把一般逻辑的明晰性同感性的明晰区别开来。逻辑的明显以诸多特征的客观的清除为基础，感性的明晰以诸多特征的主观的清除为基础。前者是由概念而来的清楚，后者是由直观而来的清楚。”而这两种明晰是相互冲突的：“客观的明显常常引起主观的模糊，反之亦然。因此，逻辑的明晰往往只能有害于感性的明晰；相反地，借助于例证和比喻（它们并非严格地适宜，而是仅仅按照类推被采用）的感性明晰，则长城那个对于逻辑的明晰是有害的。”由此来理解下面的话就很容易了： 对于前者我已给予了充分的注意。这涉及到我的意图的本质，但它也是种偶然的原因，使得我未能考虑这第二个虽然不是那么严格但毕竟是合理的要求。 对这俩个相互冲突的要求，康德显然偏重于“前者”，即“逻辑的明晰性”，因为它“涉及到我的意图的本质”。但这样一来它也就不得不牺牲感性的明晰性了，后者本来也是一种“合理的要求”，康德对他的放弃不是有意的，而是“偶然的”，即是由于课题本身的性质所决定的，因为这个课题本身只是要搞清只是的逻辑关系。对于这种牺牲，康德自己也感到很遗憾，所以他说： 我在自己的工作进程中对于应如何处理这个问题几乎一直都是犹豫不决的。实例和说明在我看来总是必要的，因而实际上在最初构思时也附带给予了它们以适当的地位。 康德在写作方面并不缺乏感性的明晰生动的才能，这从他多年讲授并在晚年出版的《实用人类学》中可以看出来。一些康德传记也表明，康德在日常生活中经常是谈吐风趣、思想活泼的，并不是一个使周围的人感到沉闷的人。他知识丰富，博闻强识，各种逸闻趣事信手拈来，打比方生动贴切，甚至很懂得讨女人喜欢。应当说这样一个学者在构思自己的主要著作时，不可能不考虑到表达的生动和平易。但他承认，他在写作的一开始就在犹豫不决，究竟是照顾逻辑上的明晰呢，还是兼顾感性直观的明晰？ 但我马上看出我将要处理的那些课题之巨大和对象之繁多，并觉得这一切单是以枯燥的、纯粹经院的方式来陈述就已经会使这本书够庞大了，所以我感到用那些仅仅是为了通俗化的目的而必要的实例和说明来使这本书变得更加膨胀是不可取的 就是说，随着构思的深入，他马上看出要兼顾两方面几乎是不可能的。因为他所面对的课题太庞大、太复杂了，单是想要把里面的关系理清楚就已经足够繁琐的了，如果再加上一些说明性的例证和比喻，就会使这件工作超出一般人头脑的负荷。我们经常听到一些人抱怨康德这部著作的艰深难读，完全是概念到概念的抽象思辨，感受不到任何思维的乐趣。但这也正是扛得自己深感苦恼的，他不得不以这种“枯燥的、纯粹经院的方式”来写作，并不是她有意要使人读不懂，而是对象本身的性质所决定的。它以这种纯粹学院化的语言尽量简明地表达思想的内在线索，这本来是一种最节约的表达方式，但就这样也已经使这本书拥有巨大的篇幅了。如果再加上一些通俗化的例子，而为了这些例子不被误会，又必须对之加以说明，这就会使书的篇幅过于膨胀，同时也无助于逻辑的明晰。所以他忍痛牺牲掉感性的明晰性实在是不得已。 尤其是，这本书绝不会适合于大众的使用，而真正的科学内行又并不是那么迫切需要这样一种方便，尽管这种方便总是令人舒服的，但在这里甚至可能引出某种与目的相违的结果来。 也就是说，这本书的目的并不是给一般大众看的，而是一部纯学术著作，而且到了这样一种高深的层次，几乎不可能考虑“雅俗共赏”的问题。相反，为了照顾通俗化而增加一些阅读的“方便”，这往往并不能达到目的，反而会两败俱伤，既没有做到真正的通俗，有打乱了思维的逻辑线索，所以他担心“可能引出某种与目的相违背的结果来”。他把对这本书的理解寄托于“真正的科学内行”，也就是那些纯专业人士，甚至可以说，归根到底，他相信只要他严格按照逻辑的明晰性写作，就会有人理解一种深刻的思想。对学术的真诚和对纯粹真理的追求压倒了媚俗的期待。下面他引用了特拉松院长的一句话： 虽然修道院院长特拉松尝云：如果对一本书的篇幅不是按页数、而是按人们理解他所需要的时间来衡量的话，那么对有些书我们就可以说，如果它不是这么短的话，它将会短得多。 这话的意思是说，如果一本书增加一些生动有趣的例子，就便于人们很快地理解，读者就甚至会缩短阅读的时间，所以虽然书的篇幅增加了，按阅读速度算却相当于读一本篇幅更短的书。康德并不反对这种说法，但他根据自己的情况对之做了引申： 但另一方面，如果我们把目的放在对宽泛但却结合于一条原则中的那个思辨知识整体的可理解之上，那么我们就会有同样的正当理由说：有些书，如果它并不想说地如此明晰的话，它就会更加明晰得多。 这种说法与特拉松的说法实际上是对着干的，就是说，如果不增加那些说明性的生动例子，这本书反而会更加明晰，因为它的逻辑线索没有受到那些感性例证的干扰。当然这是针对着“宽泛但却结合于一条原则中的那个思辨知识整体的可理解性”而言的，即不是一般的可理解性，而是对于按照一条逻辑原则组织起来的思辨体系的可理解性，这就是康德这本书的情况。 这是因为明晰性的辅助手段虽然在部分中有效，但在整体中往往分散了，这样它们就不能足够快地让读者达到对整体的概观，倒是用它们所有那些明亮的色彩贴在体系的结合部或骨架上，使它们面目全非了，而为了能对这个体系的统一性和杰出之处下判断，最关键的却是这种骨架。 这也就是上面所说的，细节的感性的明晰性和整体的逻辑明晰性相互之间有一种冲突关系，感性的明晰性只能作为辅助手段而使部分细节突显出来，但却喧宾夺主，不仅不能达到整体的清晰，反而把整体的概观弄模糊了。康德在本书中所追求的不是这种部分的明晰性，这种表面的通俗和华丽，他不想为了讨好一般读者而使思想的逻辑骨架受到损失。他很清楚自己著作的价值和“杰出之处”在什么地方，一种对真理本身的真诚使他宁可被人抱怨，甚至由于人们无法把握而产生种种误解，而不愿放弃体系的严谨一惯性。所以我们在阅读康德的书时必须要丢掉一切幻想，不要以为他会对我们的思维能力心存怜悯，而要把这本书看作磨砺我们哲学思维最好的磨刀石。 下面： 我认为，对读者可以构成不小的诱惑的是，将他的努力和作者的努力结合起来，如果作者有希望按照所提出的构想完整地并且持之以恒地完成一部巨大而重要的著作的话。 康德相信，他的《纯粹理性批判》如果有人读懂了的话，就会产生一种“接着讲”的冲动，即以合作者的身份配合康德去共同完成一部完整的形而上学著作，或者是在康德以后继续完成康德未竟的重建形而上学的事业。显然，康德并不认为他的《纯粹理性批判》就是一个完整的体系了，虽然就其本身的任务来说他无疑是完整的，但是它的任务只是“批判”，而不是建设。批判是为建设开道的。不过批判一旦完成，地基一旦清扫干净，对人们就会形成一种诱惑，既要在这一片全新的基地上建设起一座宏伟的大厦来，这座宏伟的大厦就是未来的形而上学。 现在，形而上学，按照我们再次将给出的它的概念，是一切科学在唯一的一门这样的科学，它可以许诺这样一种完成，即在较短的时间内，只花较少的、但却是联合的力气来完成它，以至于不再给后世留下什么工作，只除了以教学法的风格按照自己的意图把一切加以编排，而并不因此就会对内容有丝毫增加。 形而上学按照康德的设想可以一劳永逸地建立起来，这种想法在今天看来十分可笑，但康德的确实认真的。他认为其他的科学都有一些无限的发展过程，比如物理学、化学、电磁学、光学等等，都总是会有新的规律发现出来，没有人敢于说他们中任何一门今天已经完成了，不需要再发展了。因为他们除了先天的认识结构之外，还需要不断涌现的偶然的经验材料，它们的那些规律而是作为偶然被我们发现的经验规律而出现在科学中的，因此总是可以不断增加的。形而上学却不同，他的那些规律和法则只是先天地存在于我们人类的纯粹理性中，因此我们可以不需要顾及到后天经验中又出现了一些什么新的情况，而是单凭反思自己先天固有的各种能力就可以找到它们的那些必然性法则，就像逻辑学自从亚里士多德以来就已经基本奠定了，不需要再作很大的修改。所以他设想，只要人们接受了他的批判哲学的原理，就有可能发挥联合的力量来完成一种最终的形而上学，来给一切科学知识提供出完整的一套形而上学原则。那样的工作不会很困难，因为最艰难的工作已经由他自己完成了，这就是纯粹理性本身进入深入的批判。地基已经打好，材料已经备齐，甚至蓝图也已经设计出来了，一切都经过精密的勘察和敲定，现在只要大家齐心协力，就可以“在较短的时间内，只花较少的、但却是联合的力气”来完成整个形而上学了。他所奠定的这份家业，后人将享用不尽，而不再有什么工作要做了。唯一可以做的只剩下“以教学法的风格按照自己的意图把一切加以编排”而已。所谓“教学法”，又译作“教授法”，德文为Didaktik，指一种通过举例说明的方式通俗地讲解一种学说的原理的方法。例如《实用人类学》的主体部分就是所谓“人类学教授法”，其中按照知情意的次序通俗地描述了人类学的各种实用的原理，这些原理的根基是奠定于他的三大批判之上的，所以有人说读康德的书最好是从《实用人类学》读起。另外在《道德形而上学》最后的“伦理学的方法论”中也有“伦理学教授法”一章，谈到教义问答和榜样的作用。可见所谓“教学法”并不给原理增加任何内容，而只是为了达到通俗易懂的效果而设计的一种策略，是对诸多原理所做的一种合乎目的的安排。所以康德接下来说： 因为这无非是对我们所拥有的一切财产的清单通过纯粹理性而加以系统地整理而已。我们在这里没有忽略任何东西，因为凡是理性完全从自身中带来的东西，都不会隐藏起来，而是只要我们揭示了它的共同原则，本身就会由理性带到光天化日之下。 理性的财产必须带上理性的形式，只有这样才能很容易地被有理性的人所理解和把握；但这种理性的形式本身就包含在理性的共同原则本身中，所以不必从外面拿来，而只需从理性的院里里面发挥出来呢就是。所以这是一种比较轻松的工作，可以由一些智力也许不如康德、但精力比康德要好的人去做。因为纯粹理性的原理经过批判已经得到了最终的确立，它们不会隐藏起来，而是必然由自身而“带到光天化日之下”，昭然于世。教学法的这种作用是一种普及作用，而有了纯粹理性作为它的原则，这种普及不必担心偏离理性的轨道。 对于出自真正纯粹概念的知识，任何经验的东西或哪怕只是应当导致确定几百个样的特殊直观都不能对之产生丝毫影响而使之扩展和增加，这类知识的完全的统一性，将会使这种无条件的完整性称为不仅是可行的，而且是必然的。”看看你自己的住所周围，你将知道你的财产是多么的简单。——波修斯”。 教学法当然要引入经验和直观，其他的各门具体的科学的知识也必须借助于经验直观，但所有这些都不会使一门“出自真正纯粹概念的知识”、即形而上学的知识有丝毫的影响和扩展，所以形而上学有望达成一种“完全的统一性”，并且必然实现体系上的“无条件的完整性”。因为形而上学的原理并不是无限增多的，它们埋藏在人类理性的深处，并不会由于经验材料的加入而增加，而一旦被人类的批判和反思精神所挖掘出来，它们就一劳永逸地摆在那里了。现在康德已经把人类理性的那些原理的“清单”开列出来了，这就是我们所拥有的一切纯粹理性的“财产”，不会再增加，也不会再减少，所以他有充分的信心在不久的将来最终完成形而上学。 但这里所指的形而上学还只是指“自然地形而上学”。 我希望这样一种纯粹的（思辨的）理性的体系在自然的形而上学这个标题下被提供出来，这个体系比起这里的批判来虽然篇幅还不及一半，但却具有无可比拟地更为丰富的内容。 按照康德的设想，《纯粹理性批判》直接为之奠基的是一门自然科学的形而上学体系，又叫做“自然的形而上学”，属于他在《任何一种能够作为科学出现的未来形而上学导论》（简称《未来形而上学导论》）中所想要建立的那种形而上学。不过这种形而上学康德自己最终并没有建立起来，而只是谢了一本《自然科学的形而上学基础》的小册子。康德《纯粹理性批判》的直接目标就是为这样一种未来的形而上学扫清地盘，但与此同时，一个间接性的、但层次更高的目标则是要建立一门《道德形而上学》。这个任务康德倒是完成了，他不但写了一本《道德形而上学基础》的小册子，而且写出了正式的《道德形而上学》。所以它的未来形而上学体系其实有两部分，一个是自然形而上学，一个是道德形而上学，而这两部分都是依次奠基在《纯粹理性批判》之上的，用康德在第二版序言中的说法，他的《纯粹理性批判》是要通过“悬置知识，以便给信仰留下位置”，具有一箭双雕的作用。不过在这段话里他并没有涉及道德形而上学，而只提到自然形而上学。他为什么没有写出《自然形而上学》来，很可能是由于他认为这种工作比较容易，属于一种事务性的工作，而不是一种创造性、开拓性的工作。最困难的工作已经由他自己做了，剩下来的事情就是按照他所提供出来的基本概念和原则而把那些派生的概念填充到框架里面去就行了。所以康德预计，“这个体系比起这里的批判来虽然篇幅不及一半，但却具有无可比拟地更为丰富的内容”。篇幅短的原因是省去了《纯粹理性批判》中所做的那些大量的繁琐论证，只需要做一些概念的组织和安排；而内容丰富则是由于它是《纯粹理性批判》中已提出的那个基本框架的扩展和充实，每个概念都可以扩展出一系列的派生概念，每个原则也可以推演出一系列的派生原则。例如，康德在后面第十节解释他的“范畴表”的时候说：“范畴作为纯粹知性的真正的主干概念，也有自己的同样纯粹的派生概念，它们在先验哲学的一个完备的体系中是绝不可以忽略的，但我在一个单纯批判性的研究中刻印满足于只要提到它们就行了。”接下来他还做了一个初步的示范：“例如把力、行动、承受的宾位词从属于因果性范畴之下，把当下、阻抗的宾位词”从属于协同性范畴之下，把产生、消失、变化的宾位词从属于模态的云谓关系之下，如此等等。把范畴和纯粹感性的样态相结合，或者也使这些范畴相互结合，就会提供大量先天的派生概念，注意到这些概念，并在可能时把它们记载下来直到完备无遗，这将是一项有用的、不无兴致的劳作，但在这里尚无必要。”显然，有了康德的范畴表，你所能够想到的任何其他的派生的概念都可以各归其位，并由此显示出它们与别的概念之间的逻辑关系。所以《纯粹理性批判》和“未来形而上学”的任务是很不相同的，康德说： 这个批判必须首先阐明形而上学之可能性的来源和条件，并清理和平整全部杂草丛生的基地。在这里我期待读者的是一个法官的耐心和不偏不倚，但在那里则是以为帮手的襄助和支持； 《纯粹理性批判》的任务是平整地基，包括批判和清算旧形而上学的杂草，顶多是在已经平整好的地基上策划未来形而上学的蓝图。这个工作是高度抽象二富有对抗性的，充满着繁琐而细致的推理和论证，必须要有法官的耐心和公正才能理解和掌握。而在“那里”，也就是在未来形而上学中，康德期待的是一位“帮手”，所做的是一种锦上添花的工作。相比较而言，后一种工作是更轻松一些，但也是必不可少的。 因为，即使把该体系的所有原则都完全在批判中陈述出来，属于该体系本身的详尽性的毕竟还有：不要缺乏任何派生出来的概念，这些概念不能先天地凭跳跃产生出来，而必须逐步逐步地去探寻， 未来形而上学的所有基本原则在《纯粹理性批判》中都已经陈述出来了，但这对于该体系的详尽性来说仍然还是不够的，这只是一个初步的蓝图，具体如何建设形而上学的大厦，用什么材料，如何作细部的加工，这些都有待于逐步地完善。一个完整的形而上学体系必须把一切基本概念和由基本概念所派生出来的概念都完全包括在自身之中，这才能达到体系的详尽性。这些派生概念当然也是纯粹的，但在层次上不如基本概念那么高，所以不能够一下子“凭跳跃”产生出来，而必须在基本概念已经提供出来的前提下逐步推演出来，以便在自然形而上学和纯粹自然科学之间形成一种更加对应的结合。例如康德前面举的例子：“力、行动和承受”的概念是从“因果性”范畴中引申出来的，它们与物理学的基本概念就处于更加直接的关系中，如此等等。由此构成的一个完备体系就能够符合形而上学所要求的详尽性，而没有任何遗漏和缺口。 同样，由于在那里概念的全部综合已被穷尽了，所以在这里就额外要求在分析方面也做到这样，这一切将是轻松的，与其说是工作，还不如说是消遣。 《纯粹理性批判》的总问题是“先天综合判断如何可能”，要解决的主要是综合的问题；而在此基础上所建立的未来形而上学则是把已经综合起来的原理加以发挥，从中分析出所有可能的原理和概念，这是一种顺水推舟的工作。综合是难的，特别是先天综合，要求人发挥自己全部的能动性努力超越，才能够找出综合之所以可能的最高条件；分析则是在已有的条件和基础上进行分解，看看从一个前提可以推出什么样的一些概念和原理。所以未来形而上学的完成不再需要像康德在《纯粹理性批判》中所做的那种自下而上的艰苦的努力，而只需要自上而下地收获那些顺理成章的理论成果就行了，所以说它是“轻松的”，甚至是一种“消遣”。这或许正是康德没有自己去建立一门“自然形而上学”的原因，他认定自己是专门对付那些困难问题的。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>康德</tag>
        <tag>纯粹理性批判</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（13）：《行为经济学讲义：演化论视角》 第二讲]]></title>
    <url>%2F2018%2F02%2F05%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8813%EF%BC%89%EF%BC%9A%E3%80%8A%E8%A1%8C%E4%B8%BA%E7%BB%8F%E6%B5%8E%E5%AD%A6%E8%AE%B2%E4%B9%89%EF%BC%9A%E6%BC%94%E5%8C%96%E8%AE%BA%E8%A7%86%E8%A7%92%E3%80%8B%20%E7%AC%AC%E4%BA%8C%E8%AE%B2%2F</url>
    <content type="text"><![CDATA[囚徒困境：故事：两位印第安人在一条狭长河谷里捉兔子，这时它们面临囚徒困境。兔子可以沿着河谷向两端跑，所以必须各有一个印第安人把守。这样两人向中间推进，可使兔子无路可逃。问题是，如果其中一个人先捉到兔子，那么他可以拒绝与另一个人瓜分这只兔子，这是不合作的策略。他也可以与对方平分这只兔子，这是合作的策略。 囚徒困境发生的前提是： 1）如果我和你同时采取合作策略，那么我们各自的福利就都会比我们同时采取不合作策略时更好。 2）如果我合作而你不合作，那么我的福利会比我们同时采取不合作策略时更差并且你的福利会比我们同时采取合作策略时更好。对称地，如果你合作而我不合作，那么你的福利会比我们同时采取不合作策略时更差并且我的福利会比我们同时采取合作策略时更好。 囚徒困境博弈的最弱版本：滚雪球博弈 你和你的邻居早晨起来发现都被昨晚的大雪困在家里，如果你和你的邻居合力铲雪，可以比你或你的邻居单独铲雪更早走出困境。当然，你或你的邻居愿意单独出来铲雪，哪怕知道对方是搭便车的人，因为，单独铲雪总比饿死在家里好得多。 介于滚雪球和囚徒困境之间的，是公共品博弈：拟合你的邻居上了修一条路，自愿出资。双方知道对方可能“免费搭车”，但最优的选择仍是修路，哪怕是独立出资。不过，有时候公共品很贵，必须有足够多的人分摊它的成本，否则公共品就不能存在。这时，公共品博弈就成为多人的囚徒困境博弈。 囚徒困境意味着社会科学基本问题，即对于一群理性人而言，社会何以可能存在并延续了许多年呢？]]></content>
      <categories>
        <category>读书记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[读书记（12）：《行为经济学讲义：演化论视角》 第一讲]]></title>
    <url>%2F2018%2F01%2F28%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8812%EF%BC%89%EF%BC%9A%E3%80%8A%E8%A1%8C%E4%B8%BA%E7%BB%8F%E6%B5%8E%E5%AD%A6%E8%AE%B2%E4%B9%89%EF%BC%9A%E6%BC%94%E5%8C%96%E8%AE%BA%E8%A7%86%E8%A7%92%E3%80%8B%20%E7%AC%AC%E4%B8%80%E8%AE%B2%2F</url>
    <content type="text"><![CDATA[本书是北大ccer教授汪丁丁2010年在北京大学讲授“行为经济学”课程的课堂讲义，书内汪教授内容旁征博引，涉及心理学、社会学、经济学、博弈论、神经脑科学等，各种各样的知识模块扑面而来，整理至此。 心智地图：可以表达网状的多因多果联系，是PPT线性表达方式的必要补充。Buzan’s iMindMap 4.0/微软Concept Draw/Mindjet Mind-manger 8.0 手写软件：“黑板” Chalk Board 1.5 熊十力倡导的读书方法：沉潜往复，从容含玩 行为经济学参考书：《行为经济学新进展》 凯莫罗（Colin Camerer）/罗文斯坦（George Loewenstein）/拉宾（Mattew Rabin） 凯莫罗（Colin Camerer）:加州理工教授，研究兴趣为实验经济学和脑科学（认知心理学），专著《行为博弈论》。1980年代芝加哥大学培养的博士，方向为“行为决策理论”。 罗文斯坦（George Loewenstein）：卡耐基梅隆大学的心理学与经济学讲座教授，CMU可谓是行为经济学的发源地，西蒙（Herbert Simon，1978年因决策过程的行为学研究获得诺贝尔经济学奖，2001年去世）在那里建立的学术传统承前启后，将1940年代的行为经济学研究传统容易CMU深厚的管理学和行为学传统，培养了诸如卢卡斯（Robert Lucas，理性预期学派领袖，获得1995年诺贝尔经济学奖）和威廉姆斯（Oliver Williamson，新制度经济学领袖之一，2009年诺贝尔经济学奖）这样的诺贝尔经济学奖。在行为经济学领域，罗文斯坦比拉宾资深，他在耶鲁的博士论文题目为“预期与跨期选择”，据拉宾介绍，他“无所不知”。 拉宾（Mattew Rabin）：伯克利经济学助教，在《美国经济评论》发表论文，研究博弈双方公平感对博弈行为的影响。2001年因行为经济学研究获得克拉克奖（两年一次，40岁以下），关注道德问题，本科数学，博士MIT经济系，伯克利任教至今，讲授“心理学与经济学的基础” 西方文献发表三年周期： 2009年：“社会仿真”、“诺瓦克”（Martin A. Nowak，哈佛教授，生物学与演化社会仿真学派的领袖人物） 2006年：“脑科学”、“费尔”（Ernst Fehr，苏黎世学派十堰经济学和神经经济学领袖人物，被认为有等于或高于凯莫罗的诺贝尔奖获奖概率） 2003年：“桑塔费学派”及其“合作的演化”研究 一、行为经济学的定义 行为: 是生命的表征。从原核生物到微生物及至人类，只要有生命，就可以有行为。个体、群体 经济学家的问题意识是个体主义的，也叫作“方法论个人主义”，优先研究个体行为，然后才考虑集结一群个体行为去试图解释群体行为。 价值、判断：行为的研究者有一个共同的假设，就是行为的主体，只要它有生命，它的行为便预设了一套价值和价值判断的评价系统。在这一体系的指引下，有“选择”行为。 选择： （1）将备选的事物，依照基于过往经验形成的某种既存标准，分别装进由于过往经验而事先存在的一套格子里的这样一种过程。 （2）可选方案相当于各种可能手段的集合，选出来的结果相当于各种可能目标的集合（希望满足的全部欲望的集合），在这两个集合之间有一种映射（选择算子，在手段的集合里确定一个子集，在这一子集上，目标集的某些目标更容易实现），称为“选择”映射。 新古典经济学派要求这样的映射必须是“最优的”，并且是“全局最优”而不是“局部最优”。根据价值来评判最优。价值是行为主体长期演化的结果（演化论），若不涉及公共选择，也可称为偏好。贝克尔（Gary Becker，芝加哥学派领袖人物，成功拓展经济学理性选择原理于广泛领域，1992年诺贝尔经济学奖）和他的一名助手在《政治经济杂志》（Journal of Political Economy，芝加哥大学经济系的机关刊物）发表了一篇论文（“Evolutionary Efficiency and Happiness”），旨在解释人类演化过程如何使我们的偏好达到均衡从而稳定地表现出今天可观测到的一些重要性质，即性状。 哈耶克论证，我们的偏好来源于三重历史：（1）种群演化的历史（2）社会与文化的历史（3）个人史 判断： 两难情境内的选择，因为有至少两种相反且相等强有力的原则将我夹在中间，我必须抉择，否则就毁灭。这就是判断不同于选择的本质之处。判断意味着创造，因为如果没有创造，选择通常不会是“两难的”。当你知道如何选择的时候，你的选择通常不是创新行为，它只是遵循以前发生过的案例而已。当你不知道怎样选择时，你有了创造的机会。 经济学家在研究任何问题时务求有可观测的数据，从而有统计关系和由统计关系推测建构因果链条的可能性。否则，经济学解释就不再是科学的，而变成一种文学解释了。文学的特征是：因刻画了不可重复的人类经验而无法接受任何科学方法的检验。 约束： 经济学千招万式最终化为一招一式，其中的那“一式”，就是“约束条件”，最优选择是那“一招”，合起来就是“约束下的最优选择” 成本： Cost is the highest alternative value，在可选方案集合里被你的选择放弃了的那些方案当中可能为你带来最高价值的那些方案的价值。最高价值的定义也就是追求最大幸福，这是经济学假设一切行为的目标函数。贝克尔强调，最大的幸福一定是行为主体想象中的最大幸福（未必是真是的），英文是“perceived maximization”（是基于真实感觉的“想象”，而不应简单译作“统觉”），为什么不是贝克尔当初使用的“imagined maximization”，因为后者有胡思乱想的意思，太不真实。例如你可以在梦境里最大化你的幸福感，但醒后就幻灭了。 道德成本： 就是当你违背了道德自律时你感受到的幸福感下降的程度。这是一种心理成本，为了补偿幸福感的下降，你就限制自己不去做违背道德自律的事情。为了界定道德成本，我们必须界定包含道德行为的可选方案集合，以及包含道德行为的目标函数最大化问题。但你诸如信仰、灵魂和道德这样的事情，通常具有“全有或全无”性质，也就是说，要么你是道德的，要么你是不道德的。59%道德和41%道德这样的状态时很可疑的。关于比道德要求更高的信仰，早如克尔凯郭尔在《或此或彼》中指出的，你要么有信仰，要么无信仰。这两种状态之间是一道难以逾越的深渊。要获得信仰，就必须有纵身一跃的勇气，这里不存在“边际”量，因此渐变是不可能的。 二、价值、认知与判断 价值： 小密尔（John Stuart Mill，1806—1873）的理解，价值就是“importance felt”（被感受到的重要性）。则新古典经济学的偏好可表达成”所有被感受到的重要性的一个排序”。“什么是重要性？”怀特海三段论——在任何理解之前先有表达，在任何表达之前先有关于重要性的感受。这里，按照怀特海的表达，关于重要性的感受总是面向实事的。金岳霖认为重要性是一种“真实感”——“真”（不假）而且“正”（不邪）。前面说过，判断必须是两难的抉择，我们之所以有判断的冲动，是因为要追求价值。故而有从“价值”到“判断”的关系箭头 重要性： 在漫长的演化过程中，我们或任何动物脑内的神经元网络形成一些专为外界刺激信号分类的“格子”。反复遇到对我们的生存具有重要性的刺激信号，就诱致相应类型的格子的形成。在这一基础上还可形成格子之间的关系，进而有更复杂的关系。一层一层地从较低级的脑区涌现到较高级的脑区，最终映射到“自我意识”脑区，被我们意识到。这就是外部世界的各种重要性在我们脑内的表达过程。《感觉的秩序》（哈耶克毕生唯一重要的学术作品） 认知： 海纳模型： 越是理性能力完备的行为主体，它的最优选择行为就越是不可预期。 设想有人对股票市场走势的洞察如上帝那样，全知全能，那么他的最优选择在我们这些智能较低的人的观察中，必是如”白噪声“那样的完全随机行为。 在智能的另一极端，是所谓的“零智能”假设，我们假设一些动物，智能远比例如蚂蚁更弱，那么这些动物的行为，在对他们而言“变幻莫测”的环境里，只好是循规蹈矩的，于是在我们这些智能较高的观察者来看，这些动物的行为具有很高的可预测性。 在得到上述结论后，海纳开始广泛运用他的结论。他试图用这一模型解释极其广泛的现象，从科学范式和技术进步路径依赖，到经济行为与文化传统。这就引起广泛的反感 “认知”依赖于“能力” 关于“判断”，真正重要的只有两大要素：价值和认知。认知能力让你能知道多少信息，并且你能处理多少信息？这样一个认知能力的问题，它导致了“海纳模型”及其结论。 从“能力”到“认知”，虽然只是一个关系箭头，却包含了例如劳动经济学家的主要努力。劳动经济学的努力表明，我们很难测定认知能力，究竟在多大程度上由先天禀赋决定？也就是在多大程度上由后天教养决定？这就是所谓“nature VS nurture”（自然对教养）的争论。 三、信息、外部环境与认知 信息： 亚当·斯密假设决策者要充当知情且公正无偏的旁观者（fully informed and impartial spectator），这一假设在他的《道德情操论》里占据核心位置，也就是正确的决策，要求决策者掌握足够的信息。哈耶克对信息的定义为生物在长期经验中感受到外界刺激信号并形成一套专用于分类的神经元网络系统。知识或分类系统可以存储在个体脑内，也可以存储在群体里面。而群体常常以社会网络的方式存在。 信息来源依赖于社会网络结构： 信息的来源，有个体也有群体。一个社会的创造性，一方面取决于这个社会里每一个体的认知能力，另一方面取决于这个社会的结构。社会结构可以用拓扑学的语言描述：社会是一个网络，网络有自己的“中心”，有“科层”，还有其他的网络拓扑结构。 社会网络方法，好处是将个体行为嵌入到群体之内。 一个群体，不是完全没有结构就可存储信息和知识的。不同的社会结构，费孝通早已论证过（皇权与绅权），一个社会有什么样的结构，决定了它可能积累什么样的知识。中国两千多年来的社会结构决定了它不可能积累下西方社会所积累的那些知识。因为，许多知识，刚开始是以信息的形式存在的，在特定社会结构里，可能不允许被传授。 信息的另一来源：认知的“外部环境”及其不确定性 当外部环境高度不确定时，最好减少动作。 随着决策环境的不确定性从小到大，根据海纳模型，个体行为表现出来的理性程度就应当是从高到底变动。当不确定性极低时，我们观察到个体理性（reason）。如果环境不确定性继续增加，那么就要有行为灵活性的大幅下降。这是出现了行为规范，尤其是存储在社会里的那些规范（群体规范）。许多社会规范，个体虽然很难理解，但因为违反规范可能遭受社会制裁，也就不得不遵守，久而久之，群体生存得越久远，对个体而言这些规范的“合理性”就变得越显明。哈耶克倾向于将这样的理性称为“演化理性”，与完全基于个体经验的“建构理性”相对峙。 社会影响通过三种途径施于个人：其一是模仿，尤其是对成功者的模仿。其二是教化，学校的教育，家庭的教育，还有在职培训和自学等方式。其三是遗传，许多行为是遗传决定的，你很难改变这些行为，例如你的人格。晚近发表的对大批受试者的长时期跟踪调查表明，一个人的性格，如果用人格五要素模型来测度，在25岁至83岁这段时间里，他在各维度上的得分，很少发生改变。 社会结构里存储了大量的决策知识，让个体能够应付极不确定的决策环节。根据海纳模型，在最高不确定的决策环境里，我们只能依靠文化传统。我们的传统延续了数千年，在这样漫长的时间里，必定会遭遇比目前严重数倍的危机。但依旧延续到今天，这就意味着它“管用”，虽然可能令人迷惑。哈耶克说，我们是我们 传统的选择，而不是我们选择了我们的传统。传统越久远，它的合理性就越不能根据一时一事是否合理来评价。我们适应了自己的传统，在这一意义上我们是被传统选择的，并因此而更可能生存下去。 传统： 希尔斯的《论传统》及其序言对传统的定义如下：“传统一词的拉丁文为traditum，意即从过去延传到现在的事物······延传三代以上的、被人类赋予价值和意义的事物都可以看作是传统。······可以看出，这种意义上的传统概念与文化人类学家所使用的“大文化”概念是一致的······不过“传统”一词还有一种更特殊的内涵，即指一条世代相传的事物之变体链，也就是说，围绕一个或几个被接受和延传的主题而形成的不同变体的事物之变体链，也就是说，围绕一个或几个被接受和延传的主题而形成的不同变体的一条时间链。这样，一种宗教信仰、一种哲学思想、一种艺术风格、一种社会制度，在其代代相传的过程中既发生了种种变异，又保持了某种共同的主题······传统是一种社会的文化遗产，是人类过去所创造的种种制度、信仰、价值观念和行为方式等构成的表意象征；它使代与代之间、一个历史阶段与另一个历史阶段之间保持了某种连续性和同一性，构成了一个社会创造与再创造自己的文化密码，并且给人类生存带来了秩序和意义。” 韦伯的“克里斯玛”学说：马克思·韦伯指出，克里斯玛式人格是历史上尤其富有创造性的革命力量。由于克里斯玛统治下的法规来自于高度个人化的对神思的体验，以及神一般的英雄力量，并且为了体现先知式的对神思的体验，以及神一般的英雄力量，并且为了体现先知式的精神气质而排除了所有外在秩序，所以克里斯玛统治以一种革命的方式转变了所有价值观，破除了所有传统的和理性的规范。这就是说，不仅创建一种传统需要非凡的克里斯玛式的想象力（当然也需要魄力，记忆力和推理能力），而且破除一种传统同样离不开克里斯玛特质，甚至需要有双倍的克里斯玛特质。因为破除一种传统必须同时创建一种更适合时宜和环境的、也更富于想象力的新传统；只有在新传统的克里斯玛力量压倒了旧传统的习惯势力之后，旧传统才会逐渐退出历史舞台。 学术界公认，希尔斯的重要贡献是论证了“传统”具有某种克里斯玛特性。 文化传统的演变，可纳入上图的框架。 物质生活：人类社会从游猎到农耕到工业社会和后工业社会的技术进步，都可置于这一维度上； 社会情感：随着社会结构的演变，例如，从皇权的社会演变为民主或现代的社会，人们的情感也发生改变。在人类情感当中，几乎没有不是社会的情感。可能有例外，就是“宗教感”。在今天，学术界达成共识：信仰是纯粹的私人事件。除了信仰之外，其他几乎可以列举出来的情感，都是社会情感。 精神生活：这里可以发生导致社会变迁的重要力量。例如在欧洲社会变迁过程中，如韦伯论证的那样，宗教和信仰的改变，有决定性的意义。 文化表达过程中社会网络和文化资本对文化表达几乎处处存在重要的影响，这就引导我们到行为经济学和社会学交叉的一个界面——所谓“社会资本”问题。 社会资本： 社会资本就是一个社会网络内存在的全部有利于囚徒困境合作解的那些因素。 全部社会科学的基本问题，在过去的一个世纪里，是要解释“社会何以可能”。全部行为经济学的基本问题，至今为止，是要解释“合作何以可能”。社会网络的社会资本定义是内生演化的社会资本，注意，这里的社会资本不再是给定的，它是内生的，是演化的。如果合作的人群规模太小，那么它会完全消失。如果合作规模超过了例如三分之一定律所要求的范围，那么它会扩展到全部网络。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>行为</tag>
        <tag>价值</tag>
        <tag>判断</tag>
        <tag>传统</tag>
        <tag>海纳模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（11）：存在主义书单]]></title>
    <url>%2F2018%2F01%2F25%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8811%EF%BC%89%EF%BC%9A%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%E4%B9%A6%E5%8D%95%2F</url>
    <content type="text"><![CDATA[存在主义相关书籍整理。 书名 作者 出版社 豆瓣评分 存在主义 [美] W. 考夫曼 编著 商务印书馆 8.3 (163人评价) 存在主义 [美]科珀 复旦大学出版社 8.4 (41人评价) 存在主义咖啡馆 [英] 莎拉·贝克韦尔 北京联合出版公司 9.0 (567人评价) 非理性的人 [美] 威廉·巴雷特 上海译文出版社 9.0(463人评价) 存在主义简论 [美]托马斯·R，弗林 外语教学与研究出版社 8.2 (284人评价) 人的奴役与自由 [俄]尼古拉·别尔嘉耶夫 贵州人民出版社 8.9 (105人评价) 思·史·诗 [中]叶秀山 人民出版社 8.6 (45人评价) 另类人 [美]科林·威尔逊 经济日报出版社 8.3 (100人评价) 从存在到存在者 [法]埃马纽埃尔·列维纳斯 江苏教育出版社 8.8 (141人评价) 在生命最深处与人相遇 [美]朱瑟琳·乔塞尔森 机械工业 7.8 (186人评价) 我与你 [德] 马丁·布伯 生活·读书·新知三联书店 8.9 (489人评价) 生命的悲剧意识 [西]乌纳穆诺 花城出版社 8.2 (258人评价) 陀思妥耶夫斯基 书名 作者 出版社 豆瓣评分 卡拉马佐夫兄弟 [俄]陀思妥耶夫斯基 上海译文出版社 9.4(7818人评价) 罪与罚 [俄] 陀思妥耶夫斯基 上海译文出版社 9.1(5075人评价) 白痴 [俄]陀思妥耶夫斯基 上海译文出版社 8.9(3775人评价) 白夜 [俄] 陀思妥耶夫斯基 上海译文出版社 8.1(2604人评价) 地下室手记 [俄] 陀思妥耶夫斯基 漓江出版社 9.1(1927人评价) 死屋手记 [俄] 陀思妥耶夫斯基 人民文学出版社 9.1(1356人评价) 被欺凌与被侮辱的 [俄] 陀思妥耶夫斯基 人民文学出版社 8.6(1075人评价) 群魔 [俄] 陀思妥耶夫斯基 译林出版社 9.3(691人评价) 作家日记（上下） [俄] 陀思妥耶夫斯基 河北教育出版社 8.4(40人评价) 穷人的美德 : 陀思妥耶夫斯基天才犯罪论集 [俄] 陀思妥耶夫斯基 天津人民出版社 7.3(270人评价) 赌徒 [俄] 陀思妥耶夫斯基 上海译文出版社 9.0(217人评价) 少年 [俄] 陀思妥耶夫斯基 上海译文出版社 8.7(165人评价) 舅舅的梦 [俄] 陀思妥耶夫斯基 山西人民出版社 8.0(45人评价) 人不单靠面包活着 : 陀思妥耶夫斯基书信选 [俄] 陀思妥耶夫斯基 上海译文出版社 8.5(121人评价) 陀思妥耶夫斯基论艺术 [俄] 陀思妥耶夫斯基 上海书店出版社 8.5(89人评价) 托尔斯泰或陀思妥耶夫斯基 [美]乔治·斯坦纳 浙江大学出版社 8.8(215人评价) 托尔斯泰与陀思妥耶夫斯基（上下） [俄]梅列日科夫斯基 华夏出版社 9.3(193人评价) 鬼 [俄] 陀思妥耶夫斯基 上海译文出版社 9.3(252人评价) 双重人格 地下室手记 [俄] 陀思妥耶夫斯基 译林出版社 9.0(687人评价) 女房东 [俄] 陀思妥耶夫斯基 文光书店 8.6(11人评价) 穷人 [俄] 陀思妥耶夫斯基 河北教育出版社 8.3(82人评价) 陀思妥耶夫斯基的世界观 [俄]尼·别尔嘉耶夫 广西师范大学出版社 9.0(187人评价) 文化的哲学 [俄]尼·别尔嘉耶夫 上海人民出版社 8.3(44人评价) 关于陀思妥耶夫斯基的六次讲座 [俄] 陀思妥耶夫斯基 广西师范大学出版社 8.6(372人评价) 陀思妥耶夫斯基诗学问题 : 复调小说理论 [俄]巴赫金 生活·读书·新知三联书店 9.2(236人评价) 读书与识字 : 陀思妥耶夫斯基读书随笔 [俄]陀思妥耶夫斯基 金城出版社 7.3(32人评价) 同时代人回忆陀思妥耶夫斯基 [俄罗斯]多利宁 广西师范大学出版社 8.7(30人评价) 陀思妥耶夫斯基的三次爱情 [美]马克・斯洛尼姆 广西师范大学出版社 7.7(88人评价) 尼采与陀思妥耶夫斯基 : 关于悲剧哲学的随笔 [俄]列夫·舍斯托夫 华东师范大学出版社 8.4(55人评价) 在约伯的天平上 [俄]舍斯托夫 上海人民出版社 9.0 (163人评价) 陀思妥耶夫斯基哲学 : 系统论述 [德] 赖因哈德·劳特 广西师范大学出版社 8.3(64人评价) 伪君子及其崇拜者 : 摘自一个无名氏的回忆录 [俄]陀思妥耶夫斯基 云南人民出版社 8.1(30人评价) 陀思妥耶夫斯基（第1卷） [美] 约瑟夫·弗兰克 广西师范大学出版社 8.8(87人评价) 陀思妥耶夫斯基诗学问题 [中]巴赫金 中央编译出版社 8.3(73人评价) 忧郁的先知:陀思妥耶夫斯基 [中]冯川 四川人民出版社 8.4(32人评价) 陀思妥耶夫斯基小说艺术研究 [中]彭克巽 北京大学出版社 7.7(32人评价) 冬天记的夏天印象 [俄]陀思妥耶夫斯基 人民文学出版社 8.1(11人评价) 诚实的贼 [俄]陀思妥耶夫斯基 新星出版社 8.6(20人评价) 宗教文化语境下的陀思妥耶夫斯基诗学 [中]王志耕 北京师范大学出版社 8.4(29人评价) 陀思妥耶夫斯基与世界文学 [俄]弗里德连杰尔 上海译文出版社 8.3(11人评价) 巴登夏日 : 关于陀思妥耶夫斯基的一切 [俄]列昂尼德·茨普金 南海出版公司 8.1(191人评价) 精神领袖 : 俄罗斯思想家论陀思妥耶夫斯基 [俄]索洛维约夫 等著 上海译文出版社 9.4(99人评价) 道德·上帝与人 : 陀思妥耶夫斯基的问题 [中]何怀宏 北京大学出版社 8.8(60人评价) 漂泊的灵魂 : 陀思妥耶夫斯基与俄罗斯传统文化 [中]赵桂莲 北京大学出版社 8.0(31人评价) 三大师 : 巴尔扎克 狄更斯 陀思妥耶夫斯基 [奥地利]茨威格 安徽文艺出版社 8.8(24人评价) 安娜·陀思妥耶夫斯卡娅回忆录 [俄]安娜·陀思妥耶夫斯卡娅 广西师范大学出版社 8.6(80人评价) 一八六七年日记 [俄]安娜·陀思妥耶夫斯卡娅 广西师范大学出版社 7.8(29人评价) 列夫·托尔斯泰 书名 作者 出版社 豆瓣评分 复活 [俄]列夫·托尔斯泰 人民文学出版社 8.0(15307人评价) 战争与和平 [俄] 列夫·托尔斯泰 人民文学出版社 8.7(4791人评价) 安娜·卡列尼娜 [俄] 列夫·托尔斯泰 上海文艺出版社 9.2(2708人评价) 伊凡·伊里奇的死 [俄] 列夫·托尔斯泰 山西人民出版社 9.2(513人评价) 生活之路 [俄] 列夫·托尔斯泰 中国人民大学出版社 8.9(274人评价) 忏悔录 [俄] 列夫·托尔斯泰 中国对外翻译 8.4(332人评价) 艺术论 [俄] 列夫·托尔斯泰 中国人民大学出版社 8.4(80人评价) 生活值得过吗 [俄] 列夫·托尔斯泰 中国发展出版社 8.2(107人评价) 克莱采奏鸣曲 [俄] 列夫·托尔斯泰 译林出版社 8.7(114人评价) 哈吉穆拉特 [俄] 列夫·托尔斯泰 上海文艺出版社 8.7(102人评价) 哥萨克 [俄] 列夫·托尔斯泰 上海文艺出版社 8.8(90人评价) 忏悔录 [俄] 列夫·托尔斯泰 译林出版社 8.9(133人评价) 童年·少年·青年 [俄] 列夫·托尔斯泰 上海文艺出版社 8.4(92人评价) 一个地主的早晨 [俄] 列夫·托尔斯泰 上海文艺出版社 8.5(59人评价) 托尔斯泰传 罗曼·罗兰 商务印书馆 7.7(93人评价) 生活之路 [俄] 列夫·托尔斯泰 商务印书馆 8.0(25人评价) 托尔斯泰中短篇小说选 [俄] 列夫·托尔斯泰 人民文学出版社 9.2(57人评价) 生活即幸福 [俄] 列夫·托尔斯泰 长江文艺出版社 8.3(33人评价) 论科学和艺术的价值 [俄] 列夫·托尔斯泰 江苏教育出版社 7.7(18人评价) 家庭的幸福 [俄] 列夫·托尔斯泰 浙江人民出版社 9.3(18人评价) 谢尔基神父 四川人民出版社 8.1(26人评价) 哥萨克 [俄] 列夫·托尔斯泰 上海文艺出版社 8.8(33人评价) 自画像 : 卡萨诺瓦、司汤达、托尔斯泰 [奥]斯蒂芬 茨威格 西苑出版社 8.5(52人评价) 一个地主的早晨 [俄] 列夫·托尔斯泰 上海文艺出版社 8.6(25人评价) 托尔斯泰忏悔录 [俄] 列夫·托尔斯泰 华文出版社 8.6(203人评价) 歌德与托尔斯泰 [德]托马斯·曼 浙江大学出版社 8.1(51人评价) 伊万·伊里奇之死 [俄] 列夫·托尔斯泰 江苏文艺出版社 9.0(104人评价) 索伦.克尔凯郭尔 书名 作者 出版社 豆瓣评分 非此即彼(上卷) [丹麦] 索伦.克尔凯郭尔 中国社会科学出版社 8.6 (306人评价) 非此即彼(下卷) [丹麦] 索伦.克尔凯郭尔 中国社会科学出版社 9.3 (91人评价) 论反讽概念 : 以苏格拉底为主线 [丹麦] 索伦.克尔凯郭尔 中国社会科学出版社 9.2 (158人评价) 畏惧与颤栗 恐惧的概念 致死的疾病 [丹麦] 索伦.克尔凯郭尔 中国社会科学出版社 9.5 (98人评价) 爱的作为 : 克尔凯郭尔文集7 [丹麦] 索伦.克尔凯郭尔 中国社会科学出版社 9.3 (29人评价) 哲学片断 [丹麦] 索伦.克尔凯郭尔 中国社会科学出版社 8.7 (48人评价) 十八训导书 [丹麦] 索伦.克尔凯郭尔 中国工人出版社 7.9 (70人评价) 哲学寓言集 [丹麦] 索伦.克尔凯郭尔 商务印书馆 8.3 (117人评价) 克尔凯郭尔 [英]加迪纳 译林出版社 7.8 (200人评价) 颤栗与不安 [丹麦] 克尔凯郭尔 陕西师范大学出版社 8.0 (275人评价) 克尔凯戈尔日记选 [丹] 彼德·P. 罗德 编 上海社会科学院出版社 8.8 (265人评价) 致死的疾病 [丹麦] 索伦.克尔凯郭尔 中国工人出版社 9.0 (255人评价) 恐惧与颤栗 [丹麦] 索伦.克尔凯郭尔 华夏出版社 8.6 (408人评价) 勾引家日记 [丹麦] 索伦.克尔凯郭尔 作家出版社 8.3(128人评价) 基督徒的激情 [丹麦] 索伦.克尔凯郭尔 中央编译出版社 7.9(203人评价) 百合·飞鸟·女演员 [丹麦] 索伦.克尔凯郭尔 华夏出版社 7.8(151人评价) 人生道路诸阶段 [丹麦] 索伦.克尔凯郭尔 商务印书馆 9.2(26人评价) 论怀疑者 [丹麦] 索伦.克尔凯郭尔 上海人民出版社 8.3(94人评价) 克尔凯郭尔日记选 [丹]彼得•P.罗德 商务印书馆 8.3(86人评价) 克尔凯郭尔：审美对象的建构 [德]T.W.阿多诺 人民出版社 7.6(30人评价) 重复 [丹麦] 索伦.克尔凯郭尔 百花文艺出版社 8.1(56人评价) 绝望的一跃 : 孤独天才克尔恺郭尔 [中]林和生 华文出版社 7.0(73人评价) 克尔凯戈尔入门 [美]唐纳德・帕尔默 东方出版社 8.7(45人评价) 旷野呼告 [俄] 列夫·舍斯托夫 华夏出版社 8.5(60人评价) 克尔恺廓尔 [美]苏珊·李·安德森 中华书局 8.6(65人评价) 不幸与幸福 [美]尼尔斯・托马森 华夏出版社 9.1(23人评价) 卡尔·雅思贝尔斯 书名 作者 出版社 豆瓣评分 生存哲学 [德]卡尔·雅斯贝尔斯 上海译文出版社 7.6 (88人评价) 时代的精神状况 [德]卡尔·雅斯贝尔斯 上海译文出版社 8.7 (230人评价) 历史的起源与目标 [德]卡尔·雅斯贝尔斯 华夏出版社 8.7(129人评价) 什么是教育 [德]卡尔·雅斯贝尔斯 生活·读书·新知三联书店 8.9(268人评价) 大学之理念 [德]卡尔·雅斯贝尔斯 上海人民出版社 8.9(228人评价) 大哲学家 [德]卡尔·雅斯贝尔斯 社会科学文献出版社 9.0(30人评价) 苏格拉底、佛陀、孔子和耶稣 [德]卡尔·雅斯贝尔斯 安徽文艺出版社 8.2(28人评价) 尼采 : 其人其说 [德]卡尔·雅斯贝尔斯 社会科学文献出版社 8.4(48人评价) 海德格尔与雅斯贝尔斯往复书简 [德]瓦尔特·比默尔 等编 上海人民出版社 8.2(37人评价) 悲剧的超越 [德]卡尔·雅斯贝尔斯 工人出版社 8.8(71人评价) 雅斯贝尔斯 : 大哲学家的生活与思想 [德]叔斯勒 中国人民大学出版社 7.5(18人评价) 尼采 书名 作者 出版社 豆瓣评分 不合时宜的沉思 [德]尼采 华东师范大学出版社 8.5 (426人评价) 悲剧的诞生 : 尼采美学文选 [德]尼采 生活·读书·新知三联书店 8.8(4002人评价) 查拉图斯特拉如是说 [德]尼采 生活·读书·新知三联书店 8.9(4885人评价) 偶像的黄昏 [德]尼采 光明日报出版社 8.6(1146人评价) 尼采生存哲学 [德]尼采 九州出版社 8.0(1476人评价) 权力意志 : 1885-1889年遗稿 [德]尼采 商务印书馆 8.9(522人评价) 人性的，太人性的 [德]尼采 华东师范大学出版社 9.1(441人评价) 快乐的科学 [德]尼采 华东师范大学出版社 8.6(667人评价) 苏鲁支语录 [德]尼采 商务印书馆 9.2(658人评价) 道德的谱系 [德]尼采 华东师范大学出版社 9.3(148人评价) 疯狂的意义 : 尼采超人哲学集 [德]尼采 陕西师范大学出版社 8.4(443人评价) 朝霞 [德]尼采 华东师范大学出版社 9.1(366人评价) 尼采诗集 [德]尼采 中国文联出版社 8.3(318人评价) 希腊悲剧时代的哲学 [德]尼采 商务印书馆 8.6(261人评价) 作为教育家的叔本华 [德]尼采 译林出版社 8.8(267人评价) 反基督 : 尼采论宗教文选 [德]尼采 河北教育出版社 8.2(140人评价) 重估一切价值 [德]尼采 华东师范大学出版社 9.1(81人评价) 论我们教育机构的未来 [德]尼采 译林出版社 9.0(140人评价) 瓦格纳事件 尼采反瓦格纳 : 尼采反瓦格纳 [德]尼采 商务印书馆 8.7(101人评价) 善恶的彼岸 [德]尼采 商务印书馆 9.3(81人评价) 狄俄尼索斯颂歌 : 经典与解释. 尼采注疏集 [德]尼采 华东师范大学出版社 9.0(50人评价) 历史对于人生的利弊 [德]尼采 商务印书馆 8.8(64人评价) 哲学与真理 : 尼采1872-1876年笔记选 [德]尼采 上海社会科学院出版社 9.2(60人评价) 瞧！这个人 [德]尼采 商务印书馆 8.8(98人评价) 生命的意志 [德]尼采 长江文艺 8.3(78人评价) 尼采读本 [德]尼采 作家出版社 8.0(77人评价) 尼采 : 在世纪的转折点上 周国平 上海人民出版社 8.6(3452人评价) 尼采 [丹]乔治·勃兰兑斯 中国社会科学出版社 8.7(177人评价) 我妹妹与我 : 尼采佚失的最后告白 [德]尼采 文化艺术出版社 8.0(448人评价) 艺术与归家 : 尼采、海德格尔、福柯 余虹 中国人民大学出版社 8.9(187人评价) 与魔鬼作斗争 : 荷尔德林、克莱斯特、尼采 [奥地利] 斯蒂芬·茨威格 译林出版社 8.8(150人评价) 尼采反卢梭 : 尼采的道德政治思想研究 凯斯.安塞尔-皮尔逊 华夏出版社 8.5(80人评价) 幻觉的哲学 : 尼采八十年代手稿研究 [丹麦]哈斯 东方出版社 8.1(34人评价) 墙上的书写 : 尼采与基督教 洛维特 华夏出版社 8.1(75人评价) 历史的用途与滥用 [德]尼采 上海人民出版社 9.0(449人评价) 尼采传 : 一个特立独行者的一生 丹尼尔・哈列维 贵州人民出版社 8.1(581人评价) 从黑格尔到尼采 : 19世纪思维中的革命性决裂 卡尔·洛维特 生活·读书·新知三联书店 8.6(233人评价) 缪斯的痛苦与激情 : 尼采、里尔克与萨乐美 周濂 社会科学文献出版社 7.3(22人评价) 解读尼采 : 尼采哲学导读图 [法]吉尔·都鲁兹 百花文艺出版社 8.6(55人评价) 尼采与柏拉图主义 : 思想与社会丛书 吴增定 上海人民出版社 8.5(335人评价) 尼采与形而上学 周国平 新世界出版社 8.4(370人评价) 尼采与哲学 [法] 吉尔·德勒兹 社会科学文献出版社 8.7(196人评价) 导读尼采 李·斯平克斯 重庆大学出版社 8.6(131人评价) 叔本华与尼采 : 一组演讲 格奥尔格·西美尔 上海译文出版社 8.8(96人评价) 尼采思想传记 萨弗兰斯基 华东师范大学出版社 8.3(178人评价) 施特劳斯与尼采 朗佩特 上海三联书店 8.5(91人评价) 尼采与身体 汪民安 北京大学出版社 7.5(107人评价) 尼采的使命 : 《善恶的彼岸》绎读 [美]朗佩特 华夏出版社 8.6(50人评价) 尼采传 玛克西米利安 云南美术出版社 9.0(42人评价) 悲剧哲学家尼采 陈鼓应 上海人民出版社 7.7(169人评价) 尼采：生命之为文学 [美] 亚历山大•内哈马斯 浙江大学出版社 9.5(15人评价) 尼采新论 陈鼓应 上海人民出版社 7.6(90人评价) 审美主义 : 从尼采到福柯 李晓林 社会科学文献出版社 8.0(35人评价) 海德格尔 书名 作者 出版社 豆瓣评分 海德格尔 乔治·斯坦纳 浙江大学出版社 8.4(66人评价) 海德格尔传 吕迪格尔·萨弗兰斯基 商务印书馆 8.5(93人评价) 海德格尔 [英国] 迈克尔·英伍德 译林出版社 7.7(50人评价) 海德格尔 帕特里夏·奥坦伯德·约翰逊 中华书局 8.2(33人评价) 存在与时间 [德] 马丁·海德格尔 生活·读书·新知三联书店 8.7 / 3305人评价 《存在与时间》释义 张汝伦 上海人民出版社 林中路 [德] 马丁·海德格尔 上海译文出版社 8.8 / 913人评价 形而上学导论 [德] 马丁·海德格尔 商务印书馆 8.6 / 565人评价 海德格尔存在哲学 [德] 马丁·海德格尔 九州出版社 8.2 (136人评价) 路标 [德] 马丁·海德格尔 商务印书馆 8.9 (352人评价) 人，诗意地安居 [德] 马丁·海德格尔 广西师范大学出版社 8.1 (539人评价) 尼采（上下） [德] 马丁·海德格尔 商务印书馆 8.8 / 423人评价 荷尔德林诗的阐释 [德] 马丁·海德格尔 商务印书馆 8.7 / 392人评价 面向思的事情 [德] 马丁·海德格尔 商务印书馆 8.7 / 350人评价 演讲与论文集 [德] 马丁·海德格尔 生活·读书·新知三联书店 9.1 / 310人评价 论真理的本质 [德] 马丁·海德格尔 华夏出版社 9.3 / 129人评价 现象学之基本问题 [德] 马丁·海德格尔 上海译文出版社 9.0 / 128人评价 尼采十讲 [德] 马丁·海德格尔 中国言实出版社 7.6 / 108人评价 同一与差异 [德] 马丁·海德格尔 商务印书馆 9.1 / 96人评价 哲学论稿 [德]马丁·海德格尔 商务印书馆 9.3 / 90人评价 物的追问 [德]马丁·海德格尔 上海译文出版社 9.1 / 85人评价 思的经验 [德]马丁·海德格尔 人民出版社 8.0 / 75人评价 康德与形而上学疑难 [德]马丁·海德格尔 上海译文出版社 9.4 / 75人评价 时间概念史导论 [德]马丁·海德格尔 商务印书馆 9.1 / 71人评价 系于孤独之途 [德]马丁·海德格尔 天津人民出版社 8.2 / 62人评价 荷尔德林的新神话 [德] 马丁·海德格尔 华夏出版社 8.2 / 37人评价 存在论 [德]马丁·海德格尔 人民出版社 8.3 / 37人评价 根据律 [德] 马丁·海德格尔 商务印书馆 9.6 / 16人评价 在通向语言的途中 [德]马丁·海德格尔 商务印书馆 9.3 / 10人评价 柏拉图的《智者》 [德]马丁·海德格尔 商务印书馆 9.4 (19人评价) 亚里士多德哲学的基本概念 [德] 马丁·海德格尔 华夏出版社 9.1(22人评价) 海德格尔文集 [德] 马丁·海德格尔 华夏出版社 9.1 / 22人评价 阿伦特与海德格尔 : 爱和思的故事 安东尼娅·格鲁嫩贝格 商务印书馆 7.6(175人评价) 《存在与时间》读本 陈嘉映 三联书店 8.5(114人评价) 海德格尔思想与中国天道 张祥龙 生活·读书·新知三联书店 8.6(153人评价) 还原与给予 : 胡塞尔、海德格尔与现象学研究 [法] 让-吕克·马里翁 上海译文出版社 9.5(52人评价) 分道而行 : 卡尔纳普、卡西尔和海德格尔 [美] 迈克尔·弗里德曼 / 张卜天 北京大学出版社 8.9(112人评价) 海德格尔哲学概论 陈嘉映 北京三联书店 8.5(144人评价) 策兰与海德格尔 : 一场悬而未决的对话：1951－1970 [美] 詹姆斯·K. 林恩 北京大学出版社 7.8(92人评价) 海德格尔与其思想的开端 : 海德格尔年鉴 第一卷 [法]阿尔弗雷德·登克尔 商务印书馆 9.0(37人评价) 海德格尔的根 : 尼采，国家社会主义和希腊人 [美]查尔斯·巴姆巴赫 上海书店出版社 8.0(56人评价) 海德格尔与伦理学问题 韩潮 同济大学出版社 8.6(65人评价) 存在的一代 : 海德格尔哲学在法国1927-1961 伊森•克莱因伯格 新星出版社 8.5(41人评价) 海德格尔与哲学的开端 王庆节 生活·读书·新知三联书店 7.9(77人评价) 说不可说之神秘 : 海德格尔后期思想研究 孙周兴 生活·读书·新知上海三联出版社 8.7(31人评价) 论精神 : 海德格尔与问题 [法]雅克·德里达 上海译文出版社 9.1(21人评价) 时间性：自身与他者 : 从胡塞尔、海德格尔到列维纳斯 王恒 江苏人民出版社 7.7(22人评价) 时间与永恒 : 论海德格尔哲学中的时间问题 黄裕生 江苏人民出版社 9.3(24人评价) 时间与存在 : 胡塞尔与海德格尔现象学的基本问题 方向红 商务印书馆 8.4(33人评价) 加缪 书名 作者 出版社 豆瓣评分 加缪全集（全四册） 柳鸣九 / 沈志明 主编 河北教育出版社 9.3 (410人评价) 置身于苦难与阳光之间 [法]加缪 上海三联书店 8.6 (1147人评价) 西西弗的神话 [法]加缪 天津人民出版社 8.6 (770人评价) 局外人 鼠疫 [法]加缪 译林出版社 9.0 (1591人评价) 加缪 理查德·坎伯 中华书局 7.4 (129人评价) 荒谬的自由 [法]加缪 江苏文艺出版社 8.2 (73人评价) 正义者 [法]加缪 漓江出版社 9.2 (121人评价) 第一个人 [法]阿尔贝・加缪 译林出版社 8.8 (172人评价) 卡里古拉 [法]阿尔贝・加缪 桂冠 9.4 (171人评价) 加缪传 [法] 奥利维·托德 商务印书馆 8.5 (76人评价) 反与正·婚礼集·夏天集 [法] 阿贝尔·加缪 译林出版社 8.8 (451人评价) 加缪文集 [法] 阿尔贝·加缪 译林出版社 9.1 (1882人评价) 加缪和萨特 [美] 罗纳德·阿隆森 华东师范大学出版社 7.8 (217人评价) 萨特 书名 作者 出版社 豆瓣评分 萨特文集（全八卷） [法] 让·保尔·萨特 人民文学出版社 9.0 (528人评价) 存在与虚无 [法] 让·保尔·萨特 生活·读书·新知三联书店 想象 [法] 让·保尔·萨特 上海译文出版社 7.3 (82人评价) 自我的超越性 [法] 让·保尔·萨特 商务印书馆 7.8 (42人评价) 存在主义是一种人道主义 [法] 让·保尔·萨特 上海译文出版社 8.5 (439人评价) 魔鬼与上帝 [法] 让·保尔·萨特 漓江出版社 9.1 (57人评价) 他人就是地狱 [法] 让·保尔·萨特 天津人民出版社 8.2 (1476人评价) 萨特读本 [法] 让·保尔·萨特 人民文学出版社 8.5 (760人评价) 词语 [法] 让·保尔·萨特 三联书店 8.5 (232人评价) 萨特自述 [法] 让·保尔·萨特 天津人民出版社 7.3 (216人评价) 寄语海狸 [法] 让·保尔·萨特 人民文学出版社 8.0 (201人评价) 萨特的世纪 [法]贝尔纳·亨利·列维 商务印书馆 8.5 (85人评价) 萨特研究 柳鸣九主编 中国社会科学出版社 7.7 (45人评价) 萨特 [美] 理查德·坎伯 百年萨特 黄忠晶 中央编译出版社 7.7 (342人评价) 萨特精选集 [法] 让·保尔·萨特 北京燕山出版社 8.9 (186人评价) 不惑之年（自由之路第一部） [法] 让·保尔·萨特 中国文学出版社等 8.5 (74人评价) 萨特论艺术 [法]萨特/ [美]韦德·巴斯金 编 中国人民大学出版社 7.6 (216人评价) 文字生涯 [法] 让·保尔·萨特 人民文学出版社 8.5 (936人评价) 厌恶及其他 [法] 让·保尔·萨特 上海译文出版社 9.1 (122人评价) 超越生命的选择 [法] 让·保尔·萨特 长江文艺 7.9 (78人评价) 萨特读本 [法] 让·保尔·萨特 人民文学出版社 8.5 (760人评价) 萨特戏剧集(上下) [法]让·保罗·萨特 安徽文艺出版社 9.1 (463人评价) 墙 [法]让·保罗·萨特 安徽文艺出版社 8.6 (489人评价) 恶心 [法]让·保罗·萨特 中国友谊出版公司 8.6 (1987人评价) 萨特传 [法] 西蒙·波伏娃 百花洲文艺出版社 8.1(169人评价) 西蒙·波伏瓦 书名 作者 出版社 豆瓣评分 第二性 [法] 西蒙·波伏娃 上海译文出版社 8.7(3568人评价) 名士风流（全二册） [法] 西蒙·波伏娃 中国书籍出版社 8.2 (78人评价) 人都是要死的 [法] 西蒙·波伏娃 上海译文出版社 8.7(1209人评价) 他人的血 [法] 西蒙·波伏娃 外国文学出版社 8.0 (13人评价) 女宾 [法] 西蒙·波伏娃 上海译文出版社 波伏娃：激荡的一生 [法] 弗朗西斯 / [法] 贡蒂埃 广西师范大学出版社 7.9 (453人评价) 波伏瓦 萨莉·J·肖尔茨 中华书局 7.7 (41人评价) 波伏瓦回忆录 [法] 西蒙·波伏娃 作家出版社 7.9(80人评价) 模糊性的道德 [法] 西蒙·波伏娃 上海译文出版社 https://book.douban.com/subject/20375518/ 长征 : 中国纪行 [法] 西蒙·波伏娃 作家出版社 7.7(59人评价) 女人是什么 [法] 西蒙·波伏娃 中国文联出版公司 8.2(52人评价) 一个与他人相当的人 [法] 西蒙·波伏娃 光明日报出版 7.3(94人评价) 面对面 : 让-保罗·萨特与西蒙娜·德·波伏瓦 [美]黑兹尔·罗利 中信出版社 8.0(85人评价) 独白 [法] 西蒙·波伏娃 上海译文出版社 7.9(521人评价) 要焚毁萨德吗 [法] 西蒙·波伏娃 上海译文出版社 8.1(228人评价) 波伏瓦 : 一位追求自由的女性 李亚凡 人民文学出版社 7.0(277人评价) 越洋情书（上下卷） [法] 西蒙·波伏娃 中国书籍出版社 7.8(165人评价) 梅洛·庞蒂 书名 作者 出版社 豆瓣评分 辩证法的历险 [法]梅洛·庞蒂 上海译文出版社 7.8 (55人评价) 知觉的首要地位及其哲学结论 [法]梅洛·庞蒂 三联书店 7.6 (44人评价) 知觉现象学 [法]梅洛·庞蒂 商务印书馆 7.5 (183人评价) 眼与心 [法]梅洛·庞蒂 商务印书馆 8.7 (148人评价) 梅洛·庞蒂 丹尼尔.托马斯.普里莫兹克 中华书局 7.6 (33人评价) 符号 [法]梅洛·庞蒂 商务印书馆 6.8(52人评价) 可见的与不可见的 [法]梅洛·庞蒂 商务印书馆 7.9(43人评价) 理解梅洛·庞蒂 : 梅洛·庞蒂在当代 [法]梅洛·庞蒂 北京大学出版社 8.4(29人评价) 世界的散文 [法]梅洛·庞蒂 商务印书馆 9.1(39人评价) 行为的结构 [法]梅洛·庞蒂 商务印书馆 7.9(27人评价) 哲学赞词 [法]梅洛·庞蒂 商务印书馆 8.0(37人评价) 模糊暧昧的哲学 : 梅洛-庞蒂传 安德烈·罗宾耐 北京大学出版社 7.3(18人评价) 隐喻的身体 : 梅洛·庞蒂身体现象学研究 张尧均 中国美术学院出版社 7.9(57人评价) 画与真 : 梅洛-庞蒂与中国山水画境 姜宇辉 上海人民出版社 8.3(28人评价) 表达与存在 : 梅洛-庞蒂现象学研究 宁晓萌 北京大学出版社 7.5(20人评价) 感性的诗学 : 梅洛-庞蒂与法国哲学主流 杨大春 人民出版社 7.7(19人评价) 梅罗-庞蒂历史现象学研究 佘碧平 复旦大学出版社 7.7(11人评价) 罗洛·梅 书名 作者 出版社 豆瓣评分 存在之发现 [美]罗洛·梅 中国人民大学出版社 8.1 (73人评价) 心理学与人类困境 [美]罗洛·梅 中国人民大学出版社 8.2 (74人评价) 存在心理学 [美]施耐德/ [美]罗洛·梅 中国人民大学出版社 8.4 (49人评价) 存在之发现 [美]罗洛·梅 中国人民大学出版社 7.9 (65人评价) 创造的勇气 [美]罗洛·梅 中国人民大学出版社 8.3 (169人评价) 自由与命运 [美]罗洛·梅 中国人民大学出版社 8.2 (175人评价) 人的自我寻求 [美] 罗洛·梅 中国人民大学出版社 8.9 (441人评价) 焦虑的意义 [美] 罗洛·梅 广西师范大学出版社 8.3 (469人评价) 爱与意志 [美] 罗洛·梅 中国人民大学出版社 8.0(160人评价) 权力与无知:寻求暴力的根源 [美] 罗洛·梅 中国人民大学出版社 8.4(71人评价) 心理学与人类困境 [美] 罗洛·梅 中国人民大学出版社 8.2(74人评价) 祈望神话 : 祈望神话 [美] 罗洛·梅 中国人民大学出版社 8.5(24人评价) 存在 : 精神病学和心理学的新方向 [美]罗洛·梅，恩斯特·安杰 中国人民大学出版社 7.6(35人评价) 欧文·亚隆 书名 作者 出版社 豆瓣评分 日益亲近 : 心理治疗师与来访者的心灵对话 [美]欧文·亚隆 中国轻工业出版社 8.5(335人评价) 妈妈及生命的意义 [美]欧文·亚隆 机械工业出版社 8.4(452人评价) 叔本华的治疗 [美]欧文·亚隆 希望出版社 9.0(928人评价) 当尼采哭泣 [美]欧文·亚隆 机械工业出版社 8.8(2983人评价) 在生命最深处与人相遇：欧文·亚隆思想传记 [美]欧文·亚隆 机械工业出版社 8.2(53人评价) 爱情刽子手 [美]欧文·亚隆 希望出版社 8.9(576人评价) 存在主义心理治疗 [美]欧文·亚隆 商务印书馆 9.5(210人评价) 诊疗椅上的谎言 [美]欧文·亚隆 四川大学出版社 8.6(1037人评价) 给心理治疗师的礼物 [美]欧文·亚隆 中国轻工业出版社 9.4(137人评价)]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>存在主义</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（10）：高宣扬讲存在主义（四）]]></title>
    <url>%2F2018%2F01%2F23%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%8810%EF%BC%89%EF%BC%9A%E9%AB%98%E5%AE%A3%E6%89%AC%E8%AE%B2%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%EF%BC%88%E5%9B%9B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一方面，存在主义试图用对存在本身的研究替代传统哲学对存在者的研究，在这个意义上说是抽象的。但是同时它又是富有生命情感的，只要我们深深的热爱自己的生命，不断地对自己的生命能够提出发问，感受自己生命经历中的一切，你就会觉得，存在主义是非常的有吸引力的，而且是跟你的生命是密切相关的 我刚才讲，这个关于语言的问题是海德格尔在谈到诠释学以后，开始进一步去展开的。所以他反复的讲“语言是存在的家”，他经常不断的去引用，运用语言运用到非常惟妙惟肖的高度的，这样一些著名的诗人的语言来说明。我这里在192页，引用了海德格尔对瑞士诗人格奥尔格•特拉克尔，Georg Trakl，这是1887年到1914年的一位诗人。他写了一首诗，题目叫做《一个冬天的夜晚》。我想从头到尾地念一下这首诗。 《一个冬天的夜晚》正当雪花临窗教堂晚钟长久回响。千家万户摆好餐桌家庭供应丰盛得当。 不只一个人还在旅程经茫茫道途终临家门。金果硕硕神恩之树，屹立大地郁郁葱葱。 游子安详入室，心情之哀痛使门槛顿时僵直。一道金光闪耀餐桌上摆设着面包和美酒。 这首诗非常深刻的，为什么？一方面这首诗用的诗的语言，把人生在世的那种曲折的历程，以及一个人怎么样经受了各种生活的煎熬、各种痛苦和快乐，然后最后在一个大雪纷飞的夜晚，在圣诞节的前夕，好不容易回到了久别的故乡，来到自己的家。这说明什么，感受到人生在世就是一种永远在旅途中游荡和飘荡的、反复不定的、充满着烦和忧虑的生活历程。这个生活历程就好像把人从自己的家给抛出去，然后在世界上经历了动荡波折以后，又到晚年把握了生命的要旨，然后要回到自己的家了，感受到原来生活就是如此。 这首诗就是以这样一种题目，来表达人生在世是怎么样通过自己的艰苦的经历，然后通过语言的表述，来说明“语言是存在的家”。你如果说往事有感想，但是找不着语言来说出来。那不行的，最后还是懵懵懂懂的。所以这点是海德格尔特别强调说，你要生活有意义，就应该像诗人那样生活在世界上。这就意味着必须要，一方面要对生活、对世界、对存在有一个深刻的把握，但另一方面又要学会通过语言去把握这个世界。这两个是同时存在的，同时必须要下功夫的。 所以海德格尔在晚年的时候，特别一再的感受到，一再的表示，他对他出生的故乡的怀念。他认为对故乡的怀念和对故乡的爱，他那种乡愁是他一生中，进行哲学思维的一个永远无止境的动力。他认为家乡给他生命的一个开端，家乡的父老兄弟给予他的各种各样，对他的关切和关注。 所以他特别强调说，在这个意义上说，哲学就是不断的回到原来的出发点，回到我当初出发的老家那边。而这个老家里面，给他感受最深的是老家的父老兄弟所说的语言，他说。那个语言是最亲切的。当我回到我的老家去，听到我老家的那个人讲的话，土话和方言的时候，我突然感受到一种非常的情感，感受到世界原来如此。正是在父老兄弟们，最早的时候自己所接受的母语的训练中，就已经包含着他此后一系列的对生命、对存在的理解，它的种子和芽，发芽的芽。 所以他特别重视这个，一再强调哲学就是要不断地回来，不断的跟原来的出发点进行对话。而且也特别提到，哲学在本质上就是感谢，就是回答所有的养育他成长，给予他营养，给予他精神启发的老师、故乡、父老兄弟的一种感谢、一种对话。 那么我想,关于存在主义这样一个东西，我在这里要特别强调说，一方面，存在主义是很抽象的。由于它试图扭转整个哲学研究的方向，把对存在的问题，把它提到本体论的高度。而且等于是把用传统哲学对存在者的研究把它代替，把它替代、改换成为对存在本身的研究，在这个意义上说是很难理解的。 但是同时它又是特别富有生命的情感的，是一个只要我们去深深的热爱我们自己的生命，只要我们不断地对自己的生命能够提出发问，不断地感受到自己生命经历中的一切一切，大大小小，从大的到最细的部分，到你的每时每刻所发生的脉搏的跳动的感受，很深地去理解的话，你就会觉得，存在主义是非常的有吸引力的，而且是跟你的生命是密切相关的。 而且人在存在的路上，在存在于世界上的路上，难免进入到危险的境界，他说。有很多风险，甚至会犯错误，会走在存在的错道上去。但是你真正的对存在，对自己的存在很关切的人，他才能够通过不断的反思。慢慢的，最后就好像前面所念的特拉克尔的诗歌《一个冬天的夜晚》所说的，一个流浪者到最后经过了千辛万苦，最后回到了家，也就是说回到了存在本身，把握了存在的奥秘，这样是不容易的。所以人生本质上是流浪的。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>海德格尔</tag>
        <tag>存在主义</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（9）：高宣扬讲存在主义（三）]]></title>
    <url>%2F2018%2F01%2F22%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%889%EF%BC%89%EF%BC%9A%E9%AB%98%E5%AE%A3%E6%89%AC%E8%AE%B2%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[海德格尔一方面强调存在必须要通过此在的自我诠释去把握，另一方面在这个基础上，进一步提出了关于“语言是存在的家”，这样一个基本论断。因为人（此在）是唯一能够通过语言去把握存在的存在者 在海德格尔进行长期的研究之后，他提出了一个非常重要的方法，就是把诠释学纳入到存在哲学里面，纳入到存在论里面的基本的内容，作为存在论的基本内容。希腊神话中赫尔墨斯在传递神谕的时候，赫尔墨斯也承担了对各种神谕进行解释的权利。所以诠释学的原意，就是Hermeneutik，就是从Hermes。这个赫尔墨斯，诠释的神的名字，作为基本的基干、词干，然后衍生出来的。 但后来，到了基督教统治整个欧洲的时候，诠释学又变成了对《圣经》文本进行诠释的一种方法。这个方法在14到15世纪的时候，由于基督教的改革，特别是后来马丁・路德对《圣经》做了新的诠释。他特别强调，基督教的原意是通过教徒根据他个人的理性和他对自己的生活的体会，对《圣经》的每一句话进行自我诠释。所以教徒不应该把天主教会对《圣经》的诠释，把它作为一个神圣不可侵犯的教条来理解。 但是到了18世纪的时候，启蒙运动和浪漫主义兴起以后，德国的神学家和哲学家施莱尔马赫（Friedrich Schleiermacher, 1768-1834）又把这样一个原来意义上的，来自古希腊和罗马教会的诠释学改为对于文本的诠释的学问。那是施莱尔马赫所确定的。但是现在到了海德格尔这里，由于海德格尔特别强调，研究存在必须通过此在对存在的体会去进入到存在的本质。所以这就涉及到此在，它作为一个特殊的一个个人的亲在，他对自己的生活经历有自己特殊的体会。而且他有一种特殊的语言表达的能力，把自己亲在所经历的感悟，通过自己的特殊的、充满着自己的情感的语言、生活的语言，来表达出他对生存的经历的感受。这也就是他对存在的理解。 所以在海德格尔那里，诠释学从此以后，就从对文本的技术性的诠释变成为存在论的诠释。这是一个重大的转折。而且在西方的思想史上，这个贡献是很大的。就是在海德格尔这样的一个思想的指导下，海德格尔的学生,Hans-Georg Gadamer,伽达默尔，后来在20世纪60年代的时候，他进一步明确的提出了关于本体论的诠释学的一个新的转折。而且伽达默尔由于发表了《真理与方法》这本书，把本体论诠释学和哲学诠释学进一步扩大，变成为人文社会科学的一个新的基本逻辑，这是后话。 但是在这里我要谈到的是此在对亲在的体会，通过他自己的自我诠释来表达对存在的理解。这是在海德格尔的《存在与时间》里面特别强调的。这一点我们要特别重视。也就是说，我们自己也必须要意识到这点，要对存在、对于人生在世有所理解，而且对这种理解能够不断的总结、不断的改进、不断的提升。通过这种提升来进一步感悟自己的对人生的的理解，也就是说提升自己的对存在的理解，那么必须要特别重视，关于对自己的在世过程的诠释，这样一个方法。 海德格尔特别提到说，“此在”是通过自我言说来表达和不断提升自己对亲在的体验的。因此此在的自我言说、自我诠释，是此在再次生存的一个基本途径、基本方法、基本态度。就在这个诠释过程中，一方面表现了此在对世界的看法，对自己的生存和存在的理解，另一方面也体现了此在通过语言这个通道跟存在发生关系。在这里，就在这一点上，海德格尔特别强调，存在和语言的关系，人是一个唯一的，能够通过存在、能够通过语言去把握存在的这样一个存在者，这样一个存在。 所以必须要看到，能够如何通过自我表述、自我展现、自我言说来去临近、靠近，越来越靠近存在的本质。这是人有这个本事。他可以通过语言的表述、通过语言的体会的诠释，越来越深入的到存在的神秘的内部，去了解存在到底是什么。因此，海德格尔后来就是，一方面强调存在必须要通过此在的自我诠释去把握，另一方面他进一步在这个基础上，提出了关于“语言是存在的家”，这样一个基本论断。他特别提到说，Die Sprache ist das Haus des Seins。 “语言是存在的家”这句话，这句话是海德格尔后来在1927年发表他的《存在与时间》之后，在30年代之后，慢慢的进一步对存在、对存在语言的问题，进行了更深入的专门的研究。海德格尔越来越意识到，通过这一点意识到，要了解存在必须研究语言。因为前面已经说了，要了解存在必须通过此在，但是此在是唯一的一种能够同时了解和把握语言的艺术的存在，所以语言就成为了此在进入到存在的内部。它的进入到语言，通过语言进入到存在的家的内部，去了解存在的这样一个唯一的存在。 他特别强调，为了了解语言是存在的家，有必要对于存在有深刻的把握，同时又对语言有深刻的把握，这样一个存在者进行研究。这样一种对语言和存在同时有深刻把握的是什么呢？是谁呢？他认为是诗人。 诗人，Le Poète。他说，诗人之所以成为诗人，就在于诗人最懂得通过语言去把握存在。诗人之所以能够通过语言去把握存在，是因为诗人他最深刻的把握了存在的本质，他同时又能够通过他的语言，去把存在的本质表达出来。所以在这个意义上说，他后来，海德格尔特别强调，要特别重视研究诗歌，研究诗的语言。在这当中，海德格尔后来就特别越来越热爱德国天才的诗人――荷尔德林（Hölderlin）。 《存在主义》这本书的第四章，海德格尔进行哲学思想的有关诗歌的这部分，我在这里写了不少。在这里我顺便列举了一下。海德格尔特别提到，他说“语言就是语言”，这句话好像是同义反复。但是他特别强调“语言就是语言”，但前面的语言和后面的语言不一样。当说“语言就是语言”的时候，前面说的语言是指的一般人所理解的语言，但是后面的“就是语言”那个语言，后一个语言，他强调的是真正的语言。 所以“语言就是语言”。但是“语言就是语言”并不是所有人都明白。为什么“语言就是语言”？在这里海德格要强调的是说，语言就是那个真正作为存在的语言。语言就是作为存在的家的语言。他要说的是这个。所以当他说，语言本身，既非语言之外之他物，使语言成为语言，语言之为语言才使我们处于无底的深渊之上。所以他说，语言它的奥秘不是通过语言之外的其它物向我们展示，而是语言本身显示了语言的奥秘。 因为语言这个东西，只要是你有思想的话，当你使用语言的时候，你会感受到语言的那种既容易又难的这样的一个悖论。在语言中，在你使用语言的艺术中，你会体会到语言跟我们的生存，跟我们的生存于世的这样一个感受，有着非常深刻的然而是非常复杂的关系。有的时候我们想要用语言表达我们对生活的感受，但是恰恰是语言本身，又限制我们去表达对生活的感受。因为我们感受到说，用语言不容易，就好像我们生活在世界上不容易一样。你不要以为说，当我遇到一种处境的时候，我好像就可以很自在地表达我的感受。其实不然。你如果要真正地表达你的感受，你就会感受到，用语言去表达你的生活感受是非常难的，难到比登天还难。 所以他前面引用了哈曼（Johann Georg Hamann）的一封信，他特别提到，语言这个东西别小看它，好像我们人人都在讲话。但实际上语言是当你好好的去体会、去理解的时候，当你把语言跟你的生命连在一起的时候，你会感受到语言是一个无底的深渊，是一个没法把握的一种东西。他引用哈曼的一封信提到说，他说，语言这种东西弄得我们无法表达我们所要表达的东西。在这段话里面，他们特别强调，语言本身的强大的威力和它的价值，就在于它能够为我们人类的思维、精神活动以及一切属于人类属性的因素，提供一个永无边界、永无止境的线索，使得我们进入到一个无限广阔的维域。但是恰恰是因为这样，所以它既没有底基，也没有“他物”的限制，它就是不受一般存在物的时空限制的“存在”本身。 因此，语言是一个什么呢？是一个既靠近你，但同时又非常远离你。你要把握它，你要使用它，好像随手可得，但是你要真正的用语言去表达你的心里的感受的时候，你就感受到语言又是很难的，是需要一个你对你的生存有深刻的理解，有一种非常恰当的，就好像前面熊伟老师所说的，要对生活对存在一种恬然澄明的一种感受、感悟。就好像佛教所说的，你要真正的能够成佛，跟那个说，人人都能成佛，但是你真的是哪一个时候，你突然感受到，原来生活就是如此的时候，这个时候你才懂得你的这个世界到底是什么。而且也就在这个时候，你才懂得，其实世界的本质，我不用语言表达，我自己都把握了。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>海德格尔</tag>
        <tag>存在主义</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（8）：高宣扬讲存在主义（二）]]></title>
    <url>%2F2018%2F01%2F21%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%888%EF%BC%89%EF%BC%9A%E9%AB%98%E5%AE%A3%E6%89%AC%E8%AE%B2%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[有一个存在者，对存在本身非常关注，而且有所体会。这个存在者就是人。海德格尔将其称之为“此在”。每个人都无可逃脱。在他亲自来到这个世界上，去经历自己的生命历程的时候，他每时每刻都遭遇到，关于你到底做什么选择，你到底要行使什么样的选择的自由的问题。此在的最根本、最基本的状态就是“烦” 因为刚才其实是把一般的问题提出来，就是关于研究存在主义，必须首先把握现象学这个方法。所谓“存在”的现象，就是它能够在被观察的时候，被观察的现象跟我们观察的主体之间的感情，和我们的生命的内部的情感、对生命的感悟，能够相互交流起来。然后使得观察过程中，被观察的对象，能够随着我们的观察者自己的生命的体验的活跃，而使被观察的对象重新地显现出来，再现出来。 这个再现是再，一再的再，再现，再次地显示出来。所以，在这点上，熊先生特别强调，要准确的、要深刻的把握，关于存在和存在者之间的区别，后来他经过很多的考察，他特别对这个，一方面强调存在者和存在的差异，但同时也去考察存在者和存在的关系。就在研究这个差异和关系的同时，他发现有这么一个存在者是非常特殊的存在者。这个存在者，他对存在本身非常关注，而且有所体会。这个存在者是什么？这个存在者就是人，但这不是传统所说的那种抽象的、一般的人，而是一个一个有特殊的生命经历，有特殊的生命感悟的那个个人。这个个体，每一个都是不一样的。就在这些不同的个体中，他对自身的存在都有自己的特殊的感悟。 举例子来说，我作为一个经过哲学训练，有过哲学思维能力，而且又对自己的经历特别关切的一个人。我一个特殊的人，不是你，不是他，而是我这个人。我作为这样一个特殊存在者，我对我的每时每刻的经历，我都有自己的经验和体会。不但有这个体会，而且我对它充满了感情，我对它不断的在生命中一再的去反思，一再的去重复的去考虑，每次考虑都有不同的结果。这样一个经历，使得我有资格、有能力、有兴趣去再现我自己的存在。 用哲学的概念和范畴来说，这样一个特殊的，对自己的存在特别关切，会不断的去关怀自己的存在的，这样一个存在者，特殊的存在者，海德格尔把他称之为“此在”，原文叫做“Dasein”。 所以每个人都无可逃脱的，在他亲自来到这个世界上，去经历自己的生命历程的时候，他每时每刻都遭遇到，关于你到底做什么选择，你到底要行使什么样的选择的自由的问题。在这个意义上说，也可以说每时每刻，对每个人来说来到这个世界上，他总是要遭遇到生和死的问题。为什么？你选择这个和选择那个，有的时候不是那么，简单地说，只是在一个具体问题上，你选择这个，选择那个，实际上你是在选择你自己要走什么路，你是要怎么生活，你离开你的死亡有多远的问题。这是一个很关键、很重要的一个问题，所以海德格尔特别强调，死亡其实不是一个只有到最后，好像是到年龄老了，好像是活到一定阶段以后，该死，要到没法逃脱自然的安排，你到八九十岁，七八十岁，可能那时候面临死亡。 其实存在主义认为不是这样，其实死亡天天每时每刻都伴随着你，就是严格意义上说，他的意思就是说，每时每刻你都有选择生命，生和死的问题，但是生和死的问题在不同的时刻，展现出不同的重要性和它的不同的意义。但总之，所有的时刻都是面临着生和死的问题。 在这个问题上，熊伟认为，他说，这个是他的原话，他说，“真自由必然是反身而诚”。什么叫反身而诚？就是对自己很忠诚、很诚实，是什么就是什么。就是自己能够对自己非常明了，我到底是什么样的人，我应该选择什么样的道路？这应该很清楚。而这种清楚只有你反复的，在每时每刻中，你都非常严格的，非常高标准的去思考自己的问题。思考你要选择什么？思考你面对的这个世界到底是什么样？你都能够不放松任何一个时候，去进行分析，进行反省，那么在这个时候，这叫做真正的，就是反身而诚，对自己诚，就是这个意思。 他说，“真自由必然是反身而诚，乐莫大焉”。“必须要从‘我在世’开始，以至于‘在到死中去’视死如归”。什么意思？就是这个“在”就是存在，存在到死中去。不但每时每刻，从我在世开始。而且必须要考虑到，每个在世同时又是面临着自己的死亡的问题，关系到自己如何死亡的问题。因为你选择什么，就是意味着你将来怎么死的问题。这是一个非常，也就是说，真正的对自己的存在负责任的一个人，当他去体验自己的亲在的时候，他必须对每一个时刻，他所面临的抉择，做出的选择，看作是自己对生死存亡的一个最大问题。以这样的严肃的心情去看待，你才能够对这个存在，对在世能够有所把握。你才真正的进入到，你是真正的生活，真正的生命，是这样。不然的话你是白过了。 所以他说，也是特别提到的，他说，我重复说，“现象学必须从‘我在世’开始，以至于‘在到死中去’视死如归，此亦即活的历史”。这也就是活的历史。“一言以蔽之，海德格尔的现象学是‘在者的在的学问，即存在论’”。“此在”是揭示存在的奥秘的一个钥匙，“此在”是进入到存在的神秘世界的一个入口，在这点上要特别注意。 所以熊伟说，“天地之大，谁能‘恬然澄明’地体会到‘我在’，谁就找到‘在’；”熊伟先生用佛教和道教，和老子所说的“恬然澄明”这四个字，来概括的说明通过对于存在，对自己的生存于世，特别关注的人的这种反思和体验，感觉到世界和我的存在的关键，因而真正的领会到原来世界就是如此，生命就是如此，到那个时候，他就是佛教说的，到了一个全然领悟的新的世界。 他说“此外在其他任何地方再也找不到‘在’”。再也不找不到存在，“而只能找到‘在者’而已”。在这里，当读者，希望你们在看海德格尔《存在与时间》的部分的时候，要把握此在，但同时也特别提到，这个此在的最根本的、最基本的状态就是“烦”，这个“烦”的原文就是，德文叫做Sorge。熊伟先生特别强调说，“如果说自由属于自己，但也不能忘记与我们‘共在’的‘他人’”。 也就是说，自由固然是自己可以做决定的时候，我面对着世界的时候，我到底做什么选择。但是当你选择的时候，固然是你自己选择，但是你不得不面对一个他人的问题。你选择你是要从你自己出发，你有你自己选择的自由。但是就是当你去选择自由的时候，你不能不考虑有他人的存在，这就是说，他人跟你永远是共在。刚才我讲到关于“此在”、“在世”、“存在”的时候，它的最基本形态叫做烦，萨特（Sartre）也同样是这样理解。 所以萨特的第一本著作，关于存在主义的第一本著作，是用小说的形式写的，叫做《呕吐》（La Nausée），在1938年写成的。在1934年以前，从1931年到1934年，萨特研究和学习海德格尔的《存在与时间》，而且也学习了胡塞尔的现象学，试图从现象学里面找到一个研究方法，去揭示人生的奥秘。 烦就是呕吐的时候的那种感觉，在萨特看来，这是活生生的人，活的个体，对于他在实际生活中所遭遇的每一个现象的这样或那样的生活感应。为什么呕吐？因为我这个个人的在世，在世的每一个时刻，我所面对的各种事情，都不是我这个人所愿意的，都不是符合我个人的喜爱，所以，一切都是令人恶心，令人作呕的。他说，我固然，我作为个体，我此在，我人生在世到这个世界上。但是，我首先是无缘无故的、不知所以然的被我的父母生出来。用他的话来说，被抛在这个世界上。我不知道怎么一回事，我也不知道我父母怎么把我生出来的。把我生出来以后，又面临着一系列的这样一个，我所遭遇到的各种各样，我所不能选择的世界，各种复杂的关系。然后当我自己去选择自己的自由的时候，我又遇到周围的各种各样的他人的约束和他们的干扰。于是，我的存在于世，就成为我不得不选择的那种选择，成为我所讨厌的选择；也就是说，我每时每刻都面临着“被强迫的选择”，我只好在恶心状态中过日子。 我无法逃脱跟他人的关系，所以他说这种无法逃脱，就好像是，萨特用一个很形象的话，叫做像黏液一样黏在你的身上。他人就像黏液一样粘在你身上，你要你逃避它不行，你关起门来，你以为你自己在屋里很孤孤单单的，你可以不去见别人，不去跟别人说话，好像你可以逃避他人。事实上你逃避不了。 但是简单的只是说“烦”还不够，所以海德格尔他的深刻之处就在于，在《存在与时间》里面，当他谈到，分析此在的、生存于世的这种“烦”的具体的过程的时候，重要的在于他描述了这个“烦”的细节。这个细节是到现在为止，任何人看了以后都会，当你对你的存在和对你的命运非常关切的话，你会感受到他的描述几乎好像就是真的是切合你的状况一样。这种烦是什么烦？就是他这么一系列的产生了，它这个概念一个一个展开，关于在烦的时候什么样的，叫做情态，感情的情，情态。 因为欧洲的启蒙运动特别重视理性。19世纪中叶，出现了像波德莱尔这样的一个作家和思想家，对理性的问题提出怀疑，要重新的思考现代性的问题，那么他特别重视关于情感的问题，特别重视情感跟理性之间的矛盾和它的相互渗透。只有通过对情感的深入的去分析，高度重视情感的成分，你才能够更好地理解什么叫理性。因为理性没有孤立的或抽象的、干巴巴的理性。在人生的生命中，所有的理性的发明、理性的思考，都伴随着一系列经历中的情感的变化，带着很深刻的情感。 所以在海德格尔的《存在与时间》里面，他尤其重视情感的问题。所以他提出了一个新的概念，德文原文叫Befindlichkeit。特别强调理性和人生在世的遭遇当中，所产生的一种非常复杂的感情，一种生活情态；使得每个人都不得不在生活中，带着不同的生活情态，去面对这个“令人烦恼”的生活世界，人们不得不处理感情因素同理性因素的相互关系，要在生活中一再地面临情感与理性的协调问题，使人陷入理性与情感之间相互协调的难题之中，把人的自由问题，变成非常复杂的生活情态难题。这一点，海德格尔在分析中进行了非常恰当的一个分析，而且可以说扭转了当时西方传统的人性论中对人的看法。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>海德格尔</tag>
        <tag>存在主义</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（7）：高宣扬讲存在主义（一）]]></title>
    <url>%2F2018%2F01%2F20%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%887%EF%BC%89%EF%BC%9A%E9%AB%98%E5%AE%A3%E6%89%AC%E8%AE%B2%E5%AD%98%E5%9C%A8%E4%B8%BB%E4%B9%89%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[海德格尔在1927年发表《存在与时间》这本书的时候特别提到，他的存在哲学最重要的贡献就在于把“存在”这样一个最基本的哲学问题重新提出来。存在主义特别强调的是研究作为活现象的存在本身的自我显现 我首先要跟大家一起分享的和讨论的，是我刚刚在去年年底出版的《存在主义》这本书，这是因为这本书所谈论的问题比较重要，而且也跟我们每个人的生活的命运和我们的生命的命运，又紧紧相关。更清楚地说，这本《存在主义》所讨论的，都是关于人生的基本问题；这个基本问题，在任何时候以及对任何一个人，都是性命攸关的。 存在主义这个思潮，早在19世纪末的时候，就已经在欧洲出现了，但那个时候还没有成为一个正式的流行的思潮；当时已经被人们提出来了，主要是结合了当时欧洲社会的动荡和文化的危机，才出现了关于存在主义的一些论题。 这些论题的提出，首先是在文学界提出来的，当时丹麦的哲学家Søren Kierkegaard（索伦・克尔凯郭尔），他提出了关于存在主义的一些新的命题。他的新命题主要是认为人生是孤独的。这一命题，他是根据基督教的思想的传统，结合当时欧洲的危机，认为人生是很孤独的，而且充满着忧虑，所以关于，存在主义，所讨论的一些关于人生的问题，特别是关于人到底应该怎么样对待自己的生活？怎么样对待人生中遇到的危机，当遭遇到了各种意想不到的命运的时候，如何去处理，如何去面对？这样一个问题，在19世纪末就被突出地提出来了。Søren Kierkegaard在当时提出了这样一个（问题），能够把它从哲学上加以总结并提出来，宣示了存在主义的最早的呼声。 与此同时，俄国的著名小说家陀思妥耶夫斯基（Фёдор Михайлович Достоевский），也在他的好几本重要的小说里面，特别是《卡拉马佐夫兄弟》（братъя карамазовы）这本书里面，已经很深刻的描述了，人生在世所遭遇的各种苦难，各种经历，以及这种经历给予人的心理的冲击，使得人对人生的基本态度，产生了所谓忧虑，所谓各种各样的烦恼。 所以存在主义应该说是很早就提出来，但是只有到了第一次世界大战爆发的时候，也就是说在1914年到1918年，第一次世界大战的战争的浩劫，给欧洲人带来了极大的冲击的情况下，存在主义才进一步被哲学家重新的进行讨论，并且加以系统的总结。 达达主义和超现实主义同样的，都是一个类似存在主义的思潮，跟存在主义几乎是有一个同样的基调，都认为人生不可把握，而且认为社会是很黑暗的。他们几乎都是采取对社会进行否定和颠覆的这样一种态度，而且对人生感到，要活着就必须要反抗，这样一个问题。 这样一个思潮产生以后，首先集中在德国和法国，一直到现在，一直在流传开来，而且它在理论上成为了哲学的一个派别以后，它更加深入人心，因为它触动了人类的灵魂本身。所以它不是一个，单纯只是在一个历史阶段中短期的一种思潮。而是已经可以说，把握了人心的最薄弱的那个环节，使得它能够很赤裸裸的，揭露人性中的那些最脆弱的部分。这一点是具有普遍意义的。 这本书是为了纪念我的老师熊伟先生。熊伟先生诞辰105年了，这本书的第一章的照片里面，我特别把熊伟老师在跟我合照的照片，把它登在那里面。因为熊伟先生是中国第一位，直接的聆听存在主义大师德国的海德格尔（Martin Heidegger）的中国学者。他30年代在弗莱堡听课以后，又在40年代回到中国。当时也是第一个，把海德格尔的基本思想、他的存在哲学传播到中国的著名学者。 熊伟先生作出了重要的贡献，就是他能够把海德格尔用德语表述的艰难的、很艰涩的一些思想，结合中国的传统思想，特别是道家的思想、老子的思想，关于自然无为的思想，结合在一起，然后用中国的语言很准确地表达了。但是现象学尽管难懂，只有把握了现象学的原则，你才能够懂得存在主义。 在传统的科学思维、传统的哲学思维中，那些现象是固定不变的一个客观的对象，它是什么就是什么。一个杯子，高度、长度、宽度、形式等等，这都是现成的在那。所以我们的任务似乎就是要去测定它到底是什么样的。是什么样，我们就说什么样。但现象学不是这样，现象学要是让它“现象”出来。所谓要“现象”出来，（熊伟）他说就是，要使得它成为我们的感性的眼睛或智慧的眼睛的对象。感性的眼睛和智慧的眼睛什么意思？ 后面他又讲，实际上所谓感性的和智慧的眼睛，就是我们的有它自身的生命力的眼睛，我们的眼睛是有它自己的生命的，这样的有生命的眼睛在观看现象的时候，总是带有感情的去看它；而且，在不同的时候看，眼睛所看到的现象，就不一样。所以，我们的眼睛所看到的现象，已经不是一个不动的“主体”去孤立地观看那些“客观”存在于外界，单纯地作为一种与我们无关的“对象”，而是通过我们的有生命力的眼睛的观看， 我们所看到的现象，实际上是已经在我们的眼睛与外界现象之间，在我们的有生命的眼睛与同样有自身生命的现象之间，建立了一种活生生的关联。用胡塞尔（Husserl）的话来说，就是构成了活的主体与同样活的客体之间的相互联系，这种联系，既不是单纯的主体性，也不是单纯的客体性，而是在两者之间的相互关系性，是在不同场合中产生的相互关系性；后来，胡塞尔由此特别强调一种“生活世界”的概念，强调任何观察，都是在特定的生活世界中发生，以此试图克服传统主观主义与客观主义的简单对立。这样一来，通过现象学的研究，把观察中的人与被观察的世界现象，搭起一个相互连接的桥梁，强调不同的人在不同的环境中所观察到的现象的差异性和连贯性。 海德格尔举例子，梵谷（梵高）画了一幅画，这幅画画得很简单，就是画了一支农靴，一个农民用过的靴子。然后，海德格尔就分析说，因为梵谷自己出身很贫寒，他自己经历了很多的曲折。他做过贫苦的工人，做过煤矿工人，做过给神父做服务的那些，在教会里面服务的小生，总之他经历了很多苦难。而且他后来到巴黎以后，住在巴黎北郊，郊区的农村里，他特别注意到，他所住的很贫寒的咖啡店的楼阁，用很便宜的房租在那边住。梵谷住在那里，对那里产生了感情，喜欢那个地方，虽然远离市中心，但是他能够亲眼看到，在他的咖啡店的小阁楼，看到的一片的麦田，他感到很亲切，因为什么？ 因为他经常看到，来了到麦田里劳作的那些农民，他们的辛劳，他们怎么样从夏天到秋天到冬天到春天，这么一年春夏秋冬的，一年又一年的辛勤劳作。经历了风雨的吹袭，然后他们的靴子里面，靴子上留下了许许多多的泥巴，这些泥巴记载了穿这个靴子的农民，他怎么样辛劳的劳动。而且同时的也在那里，等于是把辛苦劳动的农民，跟天地之间进行交流，这样一来，农民在种地的时候，他自己的这种生活的经历，他的情感，对这个世界的看法，都活生生的展现在那个靴子上。靴子上的每一个洞，每一个泥土，每个不同的痕迹，它的水汽也好，它的灰尘也好，都很形象地显示了、再现了农民的、他亲历的生活。 所以这就是，通过这个靴子，海德格尔做了很深刻的分析，说那个靴子不是死的，不是仅仅作为对象的靴子。因为作为对象的靴子，人人都看出是一个靴子，但是那个靴子当我看的时候，当梵谷看的时候，你看的时候都不一样。因为你们不同的人生、经验，使得你对出现在你面前的这个靴子，产生了不同的效果。那么这个靴子就很自然的在你面前，好像是一个活生生的在那边，一个活的物，在那边展现出来。这就是它所谓的现象，是吧？现象，每次在不同的时候，在不同的人面前，都以不同的人和不同的生活经历作为不同的背景，以不同的方式“显现”出来。这就是现象学的一个非常重要的原理。只有把握了这个原理，你才能够懂得，为什么存在主义研究人生的时候，必须要通过现象学这个方法，所以这是一个很重要的（原理）。 所以他说，这就是现象学所提到的，关于现象学研究的是“事情本身”。因为胡塞尔曾经对现象学做了一个概括，说现象学无非就是要回到“事情本身”。所以现象学的“事情本身”，就是要使各种不同的现象，在我们面前和当下“现象出来”，活生生地“显现”出来。 熊伟先生把这个“现象出来”，用他的自己的话把它说出来。让我们再说一遍，他说，所谓“现象出来”，就是，他说了，“要让现象‘在出来’”。“在出来”三个字，这个“在”就是“存在”，把它简写叫“在”。因此，“在”是活的，是因时因地而变的。要现象“在出来”，就是要现象存在出来。所谓现象存在出来，就是现象自己显现自己。所以在这个意义上说，所以，我说在，也就是存在，是自我生成的显现过程。而“在”的这种自我显现，就是存在本身。一切存在，都是自我显现的现象，也正因为这样，一切现象，通过它自己的自我显现，显示出它的活生生的性质；不仅在不同的环境，而且，对不同的观察者，面对不同的现象，这个观察者，他都以不同的姿态和不同的方式，把不同的或现象，“显现出来”。 由此可见，一切现象，由于都是自我显现，绝不是如同自然科学所研究的“客观对象”那样，都是统一的客观对象；这样一来，真正的现象，它们不可能都是千遍一律的，更不是死板不变的“客体”。 他特别强调，熊伟先生说，只有通过“在”的自我显现，一切在者才有可能存在于世。这句话又是很重要，因为在这里面涉及到，存在主义所说的“存在”这个概念和“存在者”的区别。存在者，注意，就多一个字。存在和存在者的区别等于是，现象学和存在主义同传统的形而上学思考、同传统的科学思考的区别。为什么这么说？存在主义和现象学特别强调的是，现象学和存在主义都是研究存在本身，研究作为活现象的存在的自我显现。在这里，存在本身自我显现，所以它特别强调存在是活的，是有生命的，是存在自己存在出来；它既不是存在者，也不是被传统形而上学所扭曲的那种从现实的存在抽象出来的某种“本体”。存在它自己显出来，而它这个显出来，是在我跟它的关系中发生的。这是现象学和存在主义的基本原则。 但是科学思维不是这样。科学思维把它当作认识的对象，认为这个“在”不是存在，而是存在者，它用存在者来代替存在。为什么？因为这个杯子，如果说在科学研究中，可是我一再说，这个杯子是作为对象，它不是存在本身，在科学思维面前，它是一个存在者，一个已经在那里存在的存在者，作为一个客观的对象，它存在在那里。现象学和存在主义所关心的，不是作为存在者的现象，而是作为存在本身的现象。 所以海德格尔在1927年发表《存在与时间》（SEIN UND ZEIT）这本书的时候，他特别提到，他的存在哲学的最重要的贡献就在于，把存在这样一个最基本的哲学问题，重新提出来。而“存在”这个问题，从古希腊，就是公元前六世纪以后，由于苏格拉底和柏拉图，把他们之前，希腊的自然哲学的那种自然思考的方式，把它给篡改成为，改变成为以人为主体的一种认识真理的方式，存在就因此变成了作为对象的存在者。 由于柏拉图等人的这种旋转，把“存在”最重要的一个哲学问题，把它变成为存在者的问题。这样一来，就把“存在”这样一个原来最基本的希腊的最早的问题，把它给忘记了2000年。所以存在主义的问题，就是要重新的把这个存在提出来，要去一再强调，哲学研究的对象不是存在者，不是现成的现象，而是存在本身。而所谓存在本身就是，这个“存在”是靠它自己“在”出来，靠它自己的显现显象出来，是这样的。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>海德格尔</tag>
        <tag>存在主义</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程系列（2）：JSON]]></title>
    <url>%2F2018%2F01%2F20%2Fpython%E7%BC%96%E7%A8%8B%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9AJSON%2F</url>
    <content type="text"><![CDATA[一、Json简介JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它采用完全独立于语言的文本格式，但是也使用了类似于C语言家族的习惯（包括C, C++, C#, Java, JavaScript, Perl, Python等）。这些特性使JSON成为理想的数据交换语言。易于人阅读和编写，同时也易于机器解析和生成。 存储在SQL数据库中的数据往往是规整的，比如这里有一个SQLite数据库的例子: 1234id|code|name|area|area_land|area_water|population|population_growth|birth_rate|death_rate|migration_rate|created_at|updated_at1|af|Afghanistan|652230|652230|0|32564342|2.32|38.57|13.89|1.51|2015-11-01 13:19:49.461734|2015-11-01 13:19:49.4617342|al|Albania|28748|27398|1350|3029278|0.3|12.92|6.58|3.3|2015-11-01 13:19:54.431082|2015-11-01 13:19:54.4310823|ag|Algeria|2381741|2381741|0|39542166|1.84|23.67|4.31|0.92|2015-11-01 13:19:59.961286|2015-11-01 13:19:59.961286 上述的数据由行和列组成，其中每列映射到一个已定义的属性，如id或者name。其中每一行代表一个国家，每一列代表了这个国家一些特征。但随着数据量的增加，在存储的时候我们通常不知道数据的确切的结构，这被称为非结构化数据。一个很好的例子就是网站上的访客列表，下面是发送到服务器的事件列表的示例: 123456789101112131415161718192021&#123;'event_type': 'started-mission', 'keen': &#123;'created_at': '2015-06-12T23:09:03.966Z', 'id': '557b668fd2eaaa2e7c5e916b', 'timestamp': '2015-06-12T23:09:07.971Z'&#125;, 'sequence': 1&#125; &#123;'event_type': 'started-screen', 'keen': &#123;'created_at': '2015-06-12T23:09:03.979Z', 'id': '557b668f90e4bd26c10b6ed6', 'timestamp': '2015-06-12T23:09:07.987Z'&#125;, 'mission': 1, 'sequence': 4, 'type': 'code'&#125; &#123;'event_type': 'started-screen', 'keen': &#123;'created_at': '2015-06-12T23:09:22.517Z', 'id': '557b66a246f9a7239038b1e0', 'timestamp': '2015-06-12T23:09:24.246Z'&#125;, 'mission': 1, 'sequence': 3, 'type': 'code'&#125;, 上面列出了三个独立事件。每个事件都有不同的字段，有些字段嵌套在其他字段中。这种类型的数据很难在常规的SQL数据库中存储。所以这种非结构化数据通常以JavaScript对象表示法(JSON)格式存储。JSON是一种将列表和字典等数据结构编码成字符串的方法，这样来确保它们易于被机器读取。尽管JSON以Javascript开头，但它实际上只是一种格式，可以通过任何语言读取。 Json对象是一个无序的“‘名称/值’对”集合。一个对象以“{”（左括号）开始，“}”（右括号）结束。每个“名称”后跟一个“:”（冒号）；“‘名称/值’ 对”之间使用“,”（逗号）分隔。 它的值可以是双引号括起来的字符串（string）、数值(number)、true、false、 null、对象（object）或者数组（array）。这些结构可以嵌套。 二、Python编码和解析JsonPython有很大的JSON支持。我们可以将列表和字典转换为JSON，并将字符串转换为列表和字典。JSON数据看起来很像Python中的字典（dictionary），其中存储了键和值。 使用 JSON 函数需要导入 json 库： 1import json 函数 描述 json.dumps 将 Python 对象编码成 JSON 字符串 json.loads 将已编码的 JSON 字符串解码为 Python 对象 将Python的字典结构导出到json使用json.dumps()，将json读成Python的字典结构，使用json.loads()。如果不是针对string操作而是对文件操作，分别使用json.load()函数和json.dump()函数。 1234567891011121314151617181920212223242526272829303132333435import jsonfrom pprint import pprint data = [ &#123;&apos;event_type&apos;: &apos;started-mission&apos;, &apos;keen&apos;: &#123;&apos;created_at&apos;: &apos;2015-06-12T23:09:03.966Z&apos;, &apos;id&apos;: &apos;557b668fd2eaaa2e7c5e916b&apos;, &apos;timestamp&apos;: &apos;2015-06-12T23:09:07.971Z&apos;&#125;, &apos;sequence&apos;: 1&#125;, &#123;&apos;event_type&apos;: &apos;started-screen&apos;, &apos;keen&apos;: &#123;&apos;created_at&apos;: &apos;2015-06-12T23:09:03.979Z&apos;, &apos;id&apos;: &apos;557b668f90e4bd26c10b6ed6&apos;, &apos;timestamp&apos;: &apos;2015-06-12T23:09:07.987Z&apos;&#125;, &apos;mission&apos;: 1, &apos;sequence&apos;: 4, &apos;type&apos;: &apos;code&apos;&#125;]json_str = json.dumps(data, sort_keys=True, indent=1, separators=(&apos;,&apos;, &apos;: &apos;))data = json.loads(json_str)print type(json_str)print type(data)pprint(data)# Writing JSON data to filewith open(&apos;data.json&apos;, &apos;w&apos;) as f: json.dump(data, f) # Reading data backwith open(&apos;data.json&apos;, &apos;r&apos;) as f: data = json.load(f) 1234567891011121314&lt;type &apos;str&apos;&gt;&lt;type &apos;list&apos;&gt;[&#123;u&apos;event_type&apos;: u&apos;started-mission&apos;, u&apos;keen&apos;: &#123;u&apos;created_at&apos;: u&apos;2015-06-12T23:09:03.966Z&apos;, u&apos;id&apos;: u&apos;557b668fd2eaaa2e7c5e916b&apos;, u&apos;timestamp&apos;: u&apos;2015-06-12T23:09:07.971Z&apos;&#125;, u&apos;sequence&apos;: 1&#125;, &#123;u&apos;event_type&apos;: u&apos;started-screen&apos;, u&apos;keen&apos;: &#123;u&apos;created_at&apos;: u&apos;2015-06-12T23:09:03.979Z&apos;, u&apos;id&apos;: u&apos;557b668f90e4bd26c10b6ed6&apos;, u&apos;timestamp&apos;: u&apos;2015-06-12T23:09:07.987Z&apos;&#125;, u&apos;mission&apos;: 1, u&apos;sequence&apos;: 4, u&apos;type&apos;: u&apos;code&apos;&#125;] 在编码JSON的时候，这里我们为了获得漂亮的格式化字符串，可以使用 json.dumps() 的indent参数。 它会使得输出和pprint()函数效果类似。 默认的类型对应如下： JSON Python object dict array list string unicode number (int) int, long number (real) float true True false False null None 三、其他数据类型与Json之间的编码和解码一般来讲，JSON解码会根据提供的数据创建dicts或lists。 如果你想要创建其他类型的对象，可以给 json.loads() 传递object_pairs_hook或object_hook参数。 例如，下面是演示如何解码JSON数据并在一个OrderedDict中保留其顺序的例子： 1234from collections import OrderedDicts = &apos;&#123;&quot;name&quot;: &quot;ACME&quot;, &quot;shares&quot;: 50, &quot;price&quot;: 490.1&#125;&apos;data = json.loads(s, object_pairs_hook=OrderedDict)data 1OrderedDict([(u&apos;name&apos;, u&apos;ACME&apos;), (u&apos;shares&apos;, 50), (u&apos;price&apos;, 490.1)]) 下面是如何将一个JSON字典转换为一个Python对象例子： 1234567s = &apos;&#123;&quot;name&quot;: &quot;ACME&quot;, &quot;shares&quot;: 50, &quot;price&quot;: 490.1&#125;&apos;class JSONObject: def __init__(self, d): self.__dict__ = d data = json.loads(s, object_hook=JSONObject) 这里，JSON解码后的字典作为一个单个参数传递给 __init__() 。 然后，就可以随心所欲的使用了，比如作为一个实例字典来直接使用它。 12data.namedata.price 12ACME490.1 对象实例通常并不是JSON可序列化的。例如： 1234567class Point: def __init__(self, x, y): self.x = x self.y = yp = Point(2, 3)json.dumps(p) 1TypeError: &lt;__main__.Point instance at 0x10aa97c20&gt; is not JSON serializable 如果你想序列化对象实例，你可以提供一个函数，它的输入是一个实例，返回一个可序列化的字典。例如： 1234def serialize_instance(obj): d = &#123; &apos;__classname__&apos; : type(obj).__name__ &#125; d.update(vars(obj)) return d 下面是如何使用这些函数的例子： 12345678def serialize_instance(obj): d = &#123; &apos;__classname__&apos; : type(obj).__name__ &#125; d.update(vars(obj)) return dp = Point(2,3)s = json.dumps(p, default=serialize_instance) 1&apos;&#123;&quot;y&quot;: 3, &quot;x&quot;: 2, &quot;__classname__&quot;: &quot;instance&quot;&#125;&apos;]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>JSON</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程系列（1）：正则表达式]]></title>
    <url>%2F2018%2F01%2F19%2Fpython%E7%BC%96%E7%A8%8B%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9A%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[对文本进行处理在数据科学实践中必不可少的一环，业界的文本数据往往杂乱无章，而且数量及其庞大，当我们需要对文本进行片段匹配时，就要求我们利用计算机来批量地在文本中检索某种模式。正则表达式（Regular Expression）就是可以进行文本匹配的一种高级模式，它是一些由字符和特殊符号组成的字符串，它可以按照某种模式匹配一系列有相似特征的字符串。 最简单的正则表达式就是普通字符串，它仅仅可以匹配其自身。比如正则表达式“python”只可以匹配字符串“python”。正则表达式的强大之处在于特殊符号的应用，特殊符号定义了字符集合、子组匹配以及模式的重复次数。正是这些特殊符号使得一个正则表达式可以匹配字符串集合而不只是一个字符串。 一、元字符下图列出了Python支持的正则表达式元字符和语法： 1.1 择一匹配符号“|”表示从多个模式中选择一个，用于分割不同的正则表达式，可以匹配不止一个字符串，等同于逻辑“或”。例如 1bat | bet | bit # 可以匹配字符串bat或者bet或者bit 1.2 任意字符匹配符号“.”“.”号可以匹配除了换行符以为的任何字符（Python正则表达式有一个编译标记[S或DOTALL]能够使“.”匹配换行符），要匹配“.”号自身，必须使用反斜线转译符号“.”。例如： 12f.o #能够匹配f和o之间加上任意一个字符的样式，如：fao、f9o、f#o.. #能够匹配任意两个字符 1.3 匹配字符串开始“^”或结尾“$”匹配字符串以什么开始的，可以使用脱字符“^”或\A;匹配字符串以什么结束的，可以使用美元符“$”或\Z;例如： 123^From #任何以From开始的字符串tcsh$ #任何以tcsh结尾的字符串^subject:hi$ #任何由单独的字符串subject：hi构成的字符串 1.4 匹配单词边界：“\b”、“\B”\b：匹配单词的边界（单词前或后），而不在乎单词中间的字符\B：匹配单词中间的字符，而不在乎单词边界的字符例如： 123er\b #可以匹配“never”中的“er”，但不能匹配“verb”中的“er”，只关心后边er\B #能匹配“verb”中的“er”，但不能匹配“never”中的“er”,只关心中间\bthe #匹配任何以the开头的字符串 1.5 字符集“[ ]”当想要匹配指定的某些字符的时候，使用字符集是很方便的。注意：字符集只适用于单字符的情况。也就是说[ab]表示只从ab中选择一个 12b[ae]t #匹配bat、或bet[01][ab] #匹配0a、0b、1a、1b 1.6 字符集中的范围“-”和否定“”-:表示一个字符的范围^:不匹配指定字符集里的任意字符 1234z.[0-9] #字母z后面跟着任何一个字符，然后跟着一个数字[^aeiou] #一个非元音字符[^\t\n] #不匹配制表符或\n[&quot;-a] #在一个ASCII系统中，位于“&quot;”和“a”之间的字符，即34-97之间的字符 1.7 特殊符号（*，+，？，{}）*：匹配其左边的正则表达式出现零次或多次的情况。+：匹配一次或多次出现的正则表达式。？：匹配零次或一次出现的正则表达式。{N}、{M,N}: 匹配前面的正则表达式N次或M～N次 1234[dn]ot? #字母d或n后面跟一个o，然后后面最多再跟一个t。例如：do、no、dot、not0？[1-9] #一个1到9的数字，前面跟或不跟一个0[0-9]&#123;15,16&#125; #匹配15或16个数字。例如信用卡号码&lt;/?[^&gt;]+&gt; #匹配全部有效的（和无效的）HTML标签 1.8 特殊字符\d：十进制数字，相当于[0-9]\D：非十进制数字的字符，相当于0-9\w：全部字母数字，相当于[A-Za-z0-9]\W：非字母数字的字符,相当于A-Za-z0-9\s: 空格字符\S: 非空格字符 1234\w+-\d+ #一个由字母数字组成的字符串和一串由连字符分隔的数字[A-Za-z]\w* #第一个是字母，其余是字母或数字\d&#123;3&#125;-\d&#123;3&#125;-\d&#123;4&#125; #美国电话号码格式，例如800-555-1212\w+@\w+\.com #以xxx@yyy.com格式表示的简单电子邮件地址 1.9 圆括号指定分组有时候除了进行匹配操作外，我们还想要提取所匹配的子组，例如：\w+-\d+,这个正则表达式想要分别保存第一部分的字母和第二部分的数字，该怎么实现？我们可能这样做的原因是对于任何成功的匹配，我们想要看到匹配的字符串究竟是什么。如果为两个子模块都加上圆括号，例如(\w+)-(\d),然后就能够分别访问每一个匹配的子组。 1\d+(\.\d*)? #匹配浮点数的字符串，如：“5”、“5.”、“5.009”等 1.10 扩展表示法可以参考上面表格的讲解结合下面的例子就能懂了： 12345678910111213Windows(?=95|98|NT|2000) #能匹配“Windows2000”中的“Windows”，但不能匹配“Windows3.1”中的“Windows”Windows(?!95|98|NT|2000) #能匹配“Windows3.1”中的“Windows”，但不能匹配“Windows2000”中的“Windows”(?&lt;=95|98|NT|2000)Windows #能匹配“2000Windows”中的“Windows”，但不能匹配“3.1Windows”中的“Windows”(?&lt;!95|98|NT|2000)Windows #能匹配“3.1Windows”中的“Windows”，但不能匹配“2000Windows”中的“Windows”industr(?:y|ies) #就是一个比“industry|industries”更简略的表达式 (?:\w+\.)* #以点结尾的字符串，如google.(?#comment) #不做匹配，只做注释(?=.com) #一个字符串后面跟着.com才做匹配(?!.net) #一个字符串后面跟的不是.net才做匹配(?&lt;=800-) #字符串前面出现800-才做匹配(?&lt;!192\.168\.) # 字符串前面不是192.168。才做匹配，过滤掉一类ip地址(?(1)y|x) #如果匹配组1（\1）存在，就与y匹配，否则就与x匹配 二、re模块Python语言中使用re模块的方法支持正则表达式。这里列出re模块常见的函数以方便查询（后面会介绍主要的函数使用方法） 下面将分开解释上面的部分函数： 2.1 使用match()和search()匹配字符串，使用group()查看结果re.match() :从字符串开始的位置匹配，成功返回匹配的对象，失败返回Nonere.search(): 扫描整个字符串来进行匹配，成功返回匹配的对象，失败返回None 例1：比较match() 和 search()的区别 123456789import rem = re.match(&apos;foo&apos;, &apos;seafood&apos;)if m is not None: print(&quot;match-&quot; + m.group())m = re.search(&apos;foo&apos;, &apos;seafood&apos;)if m is not None: print(&quot;search-&quot; + m.group())#结果是：search-foo 例2: match()函数从起始位开始匹配 1234567891011121314import rem = re.match(&apos;foo&apos;, &apos;foo&apos;)if m is not None: print(&quot;能匹配-&quot; + m.group())m = re.match(&apos;foo&apos;, &apos;bar&apos;)if m is not None: print(&quot;不能匹配-&quot; + m.group())m = re.match(&apos;foo&apos;, &apos;food on the table&apos;)if m is not None: print(&quot;从开始位置进行匹配-&quot; + m.group())#能匹配-foo#从开始位置进行匹配-foo 例3: 匹配多个值（使用择一表达式”|”） 123456789101112131415161718192021222324252627import rebt = &apos;bat|bet|bit&apos;m = re.match(bt, &apos;bat&apos;)if m is not None: print(&quot;1能匹配-&quot; + m.group())m = re.match(bt, &apos;blt&apos;)if m is not None: print(&quot;2能匹配-&quot; + m.group())m = re.match(bt, &apos;he bit me&apos;)if m is not None: print(&quot;3能匹配-&quot; + m.group())m = re.search(bt, &apos;he bit me&apos;)if m is not None: print(&quot;4能匹配-&quot; + m.group())#结果：# 1能匹配-bat# 4能匹配-bit 例4: 匹配任何单个字符。点号”.”除了换行符\n和非字符，都能匹配 1234567891011121314151617181920212223242526import rebt = &quot;.end&quot;m = re.match(bt, &apos;bend&apos;)if m is not None: print(&quot;bend能匹配-&quot; + m.group())m = re.match(bt, &apos;end&apos;)if m is not None: print(&quot;end能匹配-&quot; + m.group())m = re.match(bt, &apos;\nend&apos;)if m is not None: print(&quot;\nend能匹配-&quot; + m.group())m = re.search(bt, &apos;the end.&apos;)if m is not None: print(&quot;the end.能匹配-&quot; + m.group())#结果：# bend能匹配-bend# the end.能匹配- end 例5: 匹配小数点 123456789101112131415161718192021222324import rebt = &quot;3.14&quot;pi_bt = &quot;3\.14&quot; #表示字面量的点号 （dec.point）m = re.match(bt, &apos;3.14&apos;) #点号匹配if m is not None: print(&quot;3.14能匹配-&quot; + m.group())m = re.match(pi_bt, &apos;3.14&apos;) #精确匹配if m is not None: print(&quot;精确匹配-&quot; + m.group())m = re.match(bt, &apos;3014&apos;) #点号匹配0if m is not None: print(&quot;3014能匹配-&quot; + m.group())#结果：# 3.14能匹配-3.14# 精确匹配-3.14# 3014能匹配-3014 例6： 使用字符集”[ ]” 1234567891011import rebt = &quot;[cr][23][dp][o2]&quot;m = re.match(bt, &apos;c3po&apos;) #点号匹配if m is not None: print(&quot;c3po能匹配-&quot; + m.group())#结果：# c3po能匹配-c3po 例7: 重复、特殊字符 正则表达式: \w+@\w+.com可以匹配类似nobody@xxx.com的邮箱地址，但是类似nobody@xxx.yyy.aaa.com的地址就不能匹配了。这时候我们可以使用 操作符来表示该模式出现零次或者多次：\w+@(\w+.)\w+.com 例8: 分组 group()可以访问每个独立的子组groups()获取一个包含所有匹配子组的元组 12345678910111213141516&gt;&gt;&gt; import re&gt;&gt;&gt; m = re.match(&apos;(\w\w\w)-(\d\d\d)&apos;, &apos;abc-123&apos;)&gt;&gt;&gt; m.group()&apos;abc-123&apos;&gt;&gt;&gt; m.group(1)&apos;abc&apos;&gt;&gt;&gt; m.group(2)&apos;123&apos;&gt;&gt;&gt; m.groups()(&apos;abc&apos;, &apos;123&apos;)&gt;&gt;&gt; m = re.match(&apos;ab&apos;, &apos;ab&apos;)&gt;&gt;&gt; m.group()&apos;ab&apos;&gt;&gt;&gt; m.groups()( ) 例9: 匹配字符串起始和结尾 123456789101112131415161718192021m = re.search(&apos;^the&apos;,&apos;the end.&apos;)&gt;&gt;&gt; m.group()&apos;the&apos;&gt;&gt;&gt; m = re.search(&apos;^the&apos;,&apos;sthe end.&apos;)&gt;&gt;&gt; m.group()Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: &apos;NoneType&apos; object has no attribute &apos;group&apos;&gt;&gt;&gt; m = re.search(r&apos;\bthe&apos;,&apos;bite the dog&apos;)&gt;&gt;&gt; m.group()&apos;the&apos;&gt;&gt;&gt; m = re.search(r&apos;\bthe&apos;,&apos;bitethe dog&apos;)&gt;&gt;&gt; m.group()Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: &apos;NoneType&apos; object has no attribute &apos;group&apos;&gt;&gt;&gt; m = re.search(r&apos;\Bthe&apos;,&apos;bitethe dog&apos;)&gt;&gt;&gt; m.group()&apos;the&apos; 2.2 使用findall()、finditer()查找每一次出现的位置final() 以列表的形式返回所有能匹配的结果 123&gt;&gt;&gt; import re&gt;&gt;&gt; re.findall(&apos;car&apos;, &apos;car sscare&apos;)[&apos;car&apos;, &apos;car&apos;] finaliter()返回一个顺序访问每一个匹配结果（Match对象）的迭代器 1234&gt;&gt;&gt; re.finditer(r&apos;(th\w+) and (th\w+)&apos;,s, re.I).next().group(1)&apos;This&apos;&gt;&gt;&gt; re.finditer(r&apos;(th\w+) and (th\w+)&apos;,s, re.I).next().group(2)&apos;That&apos; 2.3 使用sub()和subn()搜索和替换两个函数都可以实现搜索和替换功能，将某字符串中所有匹配正则表达式的部分进行某种形式的替换。不同点是subn()还返回一个表示替换了多少次的总数，和返回结果一起以元组的形式返回。 1234&gt;&gt;&gt; re.sub(&apos;[ae]&apos;,&apos;X&apos;,&apos;abcdef&apos;)&apos;XbcdXf&apos;&gt;&gt;&gt; re.subn(&apos;[ae]&apos;,&apos;X&apos;,&apos;abcdef&apos;)(&apos;XbcdXf&apos;, 2) 进行替换的时候，还可以指定替换的顺序，原理是使用匹配对象的group()方法除了能够获取匹配分组编号外，还可以使用\N，其中N表示要替换字符串中的分组的编号，通过编号就能指定替换的顺序。例如：将美式日期MM/DD/YY{,YY}格式转换成DD/MM/YY{,YY}格式 1234&gt;&gt;&gt; re.sub(r&apos;(\d&#123;1,2&#125;)/(\d&#123;1,2&#125;)/(\d&#123;2&#125;|\d&#123;4&#125;)&apos;,r&apos;\2/\1/\3&apos;,&apos;2/20/91&apos;)&apos;20/2/91&apos;&gt;&gt;&gt; re.sub(r&apos;(\d&#123;1,2&#125;)/(\d&#123;1,2&#125;)/(\d&#123;2&#125;|\d&#123;4&#125;)&apos;,r&apos;\2/\1/\3&apos;,&apos;2/20/1991&apos;)&apos;20/2/1991&apos; 2.4 在限定模式上使用split()分隔字符串re模块的split（）可以基于正则表达式的模式分隔字符串。但是当处理的不是特殊符号匹配多重模式的正则表达式时，re.split()和str.split()的工作方式相同，如下所示： 1234&gt;&gt;&gt; re.split(&apos;:&apos;, &apos;str1:str2&apos;)[&apos;str1&apos;, &apos;str2&apos;]&gt;&gt;&gt; &apos;str1:str2&apos;.split(&apos;:&apos;)[&apos;str1&apos;, &apos;str2&apos;] 但当处理复杂的分隔时，就需要比普通字符串分隔更强大的处理方式,例如下面匹配复杂情况： 12345678&gt;&gt;&gt; DATA = (&apos;Mountation View, CA 94040&apos;, &apos;sunnyvale, CA&apos;, &apos;Los Altos, 94023&apos;, &apos;Palo Alto CA&apos;,&apos;Cupertino 95014&apos;)&gt;&gt;&gt; for datum in DATA: print(re.split(&apos;, |(?= (?:\d&#123;5&#125;|[A-Z]&#123;2&#125;)) &apos;,datum))... [&apos;Mountation View&apos;, &apos;CA&apos;, &apos;94040&apos;][&apos;sunnyvale&apos;, &apos;CA&apos;][&apos;Los Altos&apos;, &apos;94023&apos;][&apos;Palo Alto&apos;, &apos;CA&apos;][&apos;Cupertino&apos;, &apos;95014&apos;] 上述的正则表达式：当一个空格紧跟在5个数字或2个字母后面时就用split语句分隔。当遇到“，”也用split函数分隔。 2.5 扩展符号通过使用(?iLmsux)系列选项，可以直接在正则表达式里面指定一个活着多个标记。以下是使用re.I/IGNORECASE的示例，第二个是使用re.M/MULTILINE实现多行混合。 12345678910&gt;&gt;&gt; re.findall(r&apos;(?i)yes&apos;,&apos;yes? Yes. YES!!!&apos;)[&apos;yes&apos;, &apos;Yes&apos;, &apos;YES&apos;]&gt;&gt;&gt; re.findall(r&apos;(?i)th\w+&apos;,&apos;The quickest way is through this tunnel.&apos;)[&apos;The&apos;, &apos;through&apos;, &apos;this&apos;]&gt;&gt;&gt; re.findall(r&apos;(?im)(^th[\w ]+)&apos;, &quot;&quot;&quot;... This is the first,... another line,... that line,it&apos;s the best... &quot;&quot;&quot;)[&apos;This is the first&apos;, &apos;that line&apos;] 通过使用“多行”，能够在目标字符串中实现跨行搜索，而不必将整个字符串视为单个实体。 下一个例子用来演示re.S/DOTALL，该标记表示点号（.）能够用来表示\n符号。 123456789101112&gt;&gt;&gt; re.findall(r&apos;th.+&apos;,&quot;&quot;&quot;... The first line... the second line... the third line... &quot;&quot;&quot;)[&apos;the second line&apos;, &apos;the third line&apos;]&gt;&gt;&gt; re.findall(r&apos;(?s)th.+&apos;,&quot;&quot;&quot;... The first line... the second line... the third line... &quot;&quot;&quot;)[&apos;the second line\nthe third line\n&apos;] re.X/VERBOSE标记允许用户通过抑制在正则表达式中使用空白符来创建更易读的正则表达式。 12345678&gt;&gt;&gt; re.search(r&apos;&apos;&apos;(?x)... \((\d&#123;3&#125;)\) #区号... [ ] #空白符... (\d&#123;3&#125;) #前缀... - #横线... (\d&#123;4&#125;) #终点数字... &apos;&apos;&apos;,&apos;(800) 555-1212&apos;).groups()(&apos;800&apos;, &apos;555&apos;, &apos;1212&apos;) (?:…)符号可以对部分正则表达式进行分组，但是不会保存该分组用于后续的检索或应用。 123456&gt;&gt;&gt; re.findall(r&apos;http://(?:\w+\.)*(\w+\.com)&apos;,... &apos;http://google.com http://www.google.com http://code.google.com&apos;)[&apos;google.com&apos;, &apos;google.com&apos;, &apos;google.com&apos;]&gt;&gt;&gt; re.search(r&apos;\((?P&lt;areacode&gt;\d&#123;3&#125;)\) (?P&lt;prefix&gt;\d&#123;3&#125;)-(?:\d&#123;4&#125;)&apos;,... &apos;(800) 555-1212&apos;).groupdict()&#123;&apos;areacode&apos;: &apos;800&apos;, &apos;prefix&apos;: &apos;555&apos;&#125; 可以同时使用(?P)和(?P=name)符号。前者通过使用一个名称标识符而不是使用从1开始增加到N的增量数字来保存匹配，如果使用数字来保存匹配结果，我们就可以通过使用\1、\2、…,\N来索引，如下所示，可以使用一个类似风格的\g来检索它们。 123&gt;&gt;&gt; re.sub(r&apos;\((?P&lt;areacode&gt;\d&#123;3&#125;)\) (?P&lt;prefix&gt;\d&#123;3&#125;)-(?:\d&#123;4&#125;)&apos;,... &apos;(\g&lt;areacode&gt;) \g&lt;prefix&gt;-xxxx&apos;, &apos;(800) 555-1212&apos;)&apos;(800) 555-xxxx&apos; 使用后者，可以在同一个正则表达式中重用模式。例如，验证一些电话号码的规范化。 12bool(re.match(r&apos;\((?P&lt;areacode&gt;\d&#123;3&#125;)\) (?P&lt;prefix&gt;\d&#123;3&#125;)-(?P&lt;number&gt;\d&#123;4&#125;) (?P=areacode)-(?P=prefix)-(?P=number) 1(?P=areacode)(?P=prefix)(?P=number)&apos;, &apos;(800) 555-1212 800-555-1212 18005551212&apos;))True 使用（？x）使代码更易读： 12345678&gt;&gt;&gt; bool(re.match(r&apos;&apos;&apos;(?x)... \((?P&lt;areacode&gt;\d&#123;3&#125;)\)[ ](?P&lt;prefix&gt;\d&#123;3&#125;)-(?P&lt;number&gt;\d&#123;4&#125;)... [ ]... (?P=areacode)-(?P=prefix)-(?P=number)... [ ]... 1(?P=areacode)(?P=prefix)(?P=number)... &apos;&apos;&apos;,&apos;(800) 555-1212 800-555-1212 18005551212&apos;))True 可以使用(?=…)和(?!…)符号在目标字符串中实现一个前视匹配： (?=…)字符串后面跟着…才适配 123456789&gt;&gt;&gt; re.findall(r&apos;\w+(?= van Rossum)&apos;,... &apos;&apos;&apos;... Guido van Rossum... Tim Peters... Alex Martelli... Just van Rossum... Raymond Hettinger... &apos;&apos;&apos;)[&apos;Guido&apos;, &apos;Just&apos;] (?!…)字符串后面不跟着…才适配： 123456789&gt;&gt;&gt; re.findall(r&apos;(?m)^\s+(?!noreply|postmaster)(\w+)&apos;,... &apos;&apos;&apos;... sales@phptr.com... postmaster@phptr.com... eng@phptr.com... noreply@phptr.com... admin@phptr.com... &apos;&apos;&apos;)[&apos;sales&apos;, &apos;eng&apos;, &apos;admin&apos;] 比较re.findall()和re.finditer() 123456789&gt;&gt;&gt; [&apos;%s@awcom&apos; % e.group(1) for e in re.finditer(r&apos;(?m)^\s+(?!noreply|postmaster)(\w+)&apos;,... &apos;&apos;&apos;... postmaster@phptr.com... noreply@phptr.com... admin@phptr.com... eng@phptr.com... sales@phptr.com... &apos;&apos;&apos;)][&apos;admin@awcom&apos;, &apos;eng@awcom&apos;, &apos;sales@awcom&apos;] 条件正则表达式匹配，假定拥有一个特殊字符，它仅仅包含字母x和y，两个字母必须由一个跟着另外一个，不能同时拥有相同的两个字母： 1234&gt;&gt;&gt; bool(re.search(r&apos;(?:(x)|y)(?(1)y|x)&apos;, &apos;xy&apos;))True&gt;&gt;&gt; bool(re.search(r&apos;(?:(x)|y)(?(1)y|x)&apos;, &apos;xx&apos;))False 三、实例在UNIX系统中，who命令会展示登录的用户信息。例如： 12345➜ ~ whosl console Nov 21 08:59 sl ttys000 Nov 21 09:09 sl ttys001 Nov 21 10:30 ➜ ~ 如果想按照空格（多个，数量不确定）分隔的话，可以使用\s\s+，下面创建一个程序，将保存在文件whodata.txt中的数据读出来：先将who的数据保存在whodata.txt文件中： 1➜ ~ who &gt; /Users/sl/Desktop/whodata.txt 然后执行下面的程序： 123456import ref = open(&apos;whodata.txt&apos;,&apos;r&apos;)for eachLine in f: print(re.split(r&apos;\s\s+&apos;, eachLine))f.close() 执行结果： 123[&apos;sl&apos;, &apos;console&apos;, &apos;Nov 21 08:59&apos;, &apos;&apos;][&apos;sl&apos;, &apos;ttys000&apos;, &apos;Nov 21 09:09&apos;, &apos;&apos;][&apos;sl&apos;, &apos;ttys001&apos;, &apos;Nov 21 10:30&apos;, &apos;&apos;] 优化上面的程序： 上面的程序，who命令是在脚本外部执行的，每次手动重复做这件事让人很厌倦，我们可以通过调用os.popen()命令（现在已经被subprocess模块替代）将这个命令的执行在脚本内部实现。另外我们使用str.rstrip()去除尾部的\n，程序如下： 1234567891011import reimport osf = os.popen(&apos;who&apos;, &apos;r&apos;)for eachLine in f: print(re.split(r&apos;\s\s+|\t&apos;, eachLine.rstrip()))f.close()#结果：[&apos;sl&apos;, &apos;console&apos;, &apos;Nov 21 08:59&apos;][&apos;sl&apos;, &apos;ttys000&apos;, &apos;Nov 21 09:09&apos;][&apos;sl&apos;, &apos;ttys001&apos;, &apos;Nov 21 10:30&apos;] 还可以使用with语句，可以使上下文管理对象变得更简易： 123456import reimport oswith os.popen(&apos;who&apos;, &apos;r&apos;) as f: for eachLine in f: print(re.split(r&apos;\s\s+|\t&apos;, eachLine.rstrip())) 如果要适配python2和python3的话，可以避免使用print（）,而使用两个版本中都有的函数distutils.log.warn()，并将其转换成printf名来使用。 123456import reimport osfrom distutils.log import warn as printfwith os.popen(&apos;who&apos;, &apos;r&apos;) as f: for eachLine in f: printf(re.split(r&apos;\s\s+|\t&apos;, eachLine.rstrip())) 生成随机数的例子，用于希望练习从中匹配、搜索正则表达式使用： 12345678910111213141516171819202122232425from random import randrange, choicefrom string import ascii_lowercase as lcfrom sys import maxsizefrom time import ctimetlds = (&apos;com&apos;, &apos;edo&apos;, &apos;net&apos;, &apos;org&apos;, &apos;gov&apos;)for i in range(randrange(5, 11)): dtint = randrange(maxsize) / 3000000000 dtstr = ctime(dtint) llen = randrange(4, 8) login = &apos;&apos;.join(choice(lc) for j in range(llen)) dlen = randrange(llen, 13) dom = &apos;&apos;.join(choice(lc) for j in range(dlen)) print(&apos;%s::%s@%s.%s::%d-%d-%d&apos; % (dtstr, login, dom, choice(tlds), dtint, llen, dlen))#随机产生的结果Mon Jun 24 08:07:21 2024::nunzkre@iqdhccpw.gov::1719187641-7-8Sun May 21 12:23:33 2062::dxlyq@kupbixskweqj.edo::2915411013-5-12Thu Sep 2 18:27:12 1999::vuhihly@hdgaimdma.com::936268032-7-9Mon Jul 30 16:45:03 2007::vygxw@diwdeqkq.net::1185785103-5-8Thu Jul 8 01:50:54 1971::mjxs@dmcuo.com::47757054-4-5Thu Sep 1 03:02:30 2005::djld@eohculuz.gov::1125514950-4-8Sun Nov 27 01:23:35 2011::cjvf@atvmdgxupi.gov::1322328215-4-10Thu Aug 1 18:36:36 2024::phasko@flcfkvb.org::1722508596-6-7]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>re</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（6）：西方哲学前沿~新古典自由主义的破冰之旅]]></title>
    <url>%2F2018%2F01%2F17%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%886%EF%BC%89%EF%BC%9A%E8%A5%BF%E6%96%B9%E5%93%B2%E5%AD%A6%E5%89%8D%E6%B2%BF%20%E2%80%94%E2%80%94%20%E6%96%B0%E5%8F%A4%E5%85%B8%E8%87%AA%E7%94%B1%E4%B8%BB%E4%B9%89%E7%9A%84%E7%A0%B4%E5%86%B0%E4%B9%8B%E6%97%85%2F</url>
    <content type="text"><![CDATA[时间：2018年3月12日地点：中国人民大学国学馆228主讲：周濂 德鲁克在《经济人的末日》1994年的再版序言中说：“可以确定的是，我们现在的世界，或许跟之前的所有社会一样，疯狂错乱。但偏执不是治愈疯狂世界的良方。相反，要在疯狂的环境中生存，更需要保持清醒。”现代性的根本宗旨就是凭借人类理性去营建和维系一个理性、有序。可控和可理解的社会秩序。虽然二十世纪的历史充满灾难和无序，但在一个上帝盾形的时代，我们只能将现代性的自由平等理想坚持下去，让自己的行为理性起来，正如的德鲁克所言，唯有如此，我们才有机会拥有一个正直的、有意义的、有成就感的人生和一个正直的社会。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>自由</tag>
        <tag>哈耶克</tag>
        <tag>德鲁克</tag>
        <tag>新古典自由主义</tag>
        <tag>罗尔斯</tag>
        <tag>诺齐克</tag>
        <tag>平等</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（5）：《伯林谈话录》摘]]></title>
    <url>%2F2018%2F01%2F15%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%885%EF%BC%89%EF%BC%9A%E3%80%8A%E4%BC%AF%E6%9E%97%E8%B0%88%E8%AF%9D%E5%BD%95%E3%80%8B%E6%91%98%2F</url>
    <content type="text"><![CDATA[其一他反对那种以为可以依据科学的、政治的、甚至美学的价值在人世间创造一个乌托邦的主张。鉴于人类历史实际上是各种经常相互碰撞的价值和思想的产生地及其变化发展的实验场所这一事实，伯林追溯多元论在伦理学、政治学和美学等领域的出现。 但是，如果我说的不错，不光最终解决这个概念本身是不切实际的，而且，各种价值之间也不可避免地是相互碰撞的、不可协调的。最终解决的可能性（即使我们忘记了这个词组带有希特勒时期的恐怖感）会制造出一种幻觉，一种非常危险的幻觉。因为，如果人们真的相信这种解决是可能的，那么，为了达到这个目标付出多少都绝不为过：为了使人类永远公正、幸福、富于创造性以及和谐协调，有什么不可以为此付出的呢？为了做成这样的蛋卷，我们可以打破无限数量的鸡蛋，这就是列宁、托洛茨基以及我们所了解的波尔布特的信念。既然我知道通往社会问题最终解决的唯一正确道路，我也就知道人类车队必须沿着什么路线走；因为你没有我这种知识，你就不能有选择自由，哪怕是最低限度的选择自由，否则你就达不到目的地。你声明采取某种方式将使你更幸福、更自由，或将使你有自由呼吸的空间，而我知道你这样想是错误的。我知道你需要什么，人民大众需要什么。如果出现由于无知或恶意而酿成的反抗，那就必须振亚下去，为了大多数人永远幸福，消灭成千上万人也许是必要的。除了心甘情愿地将他们全都牺牲掉，我们，明白此中道理的我们，又有什么选择？“（扭曲的人性之材） 《往事与随想》赫尔岑 选择概念（关键地位） 在思想史的工作中，伯林研究了那些勇敢地、公开地跟占统治地位的理性体系作斗争的思想，赞赏他们的观点和立场。他特别重视这些思想家的自由思想。正式对这种普遍存在的自由思想的肯定和褒赏，显示了他的思想史研究具有重要意义。通过他的研究工作，伯林向我们宣示，在人类历史上没有绝对的价值，而且，人类历史与众多悲剧性后果相伴，充满着那些企图通过坚信最终绝对真理而避免做出悲剧性选择的人们的困苦。 普列汉诺夫 伯林：”普列汉诺夫的确是一位富有才华的马克思主义著作家。我完全被他的书迷住了，因为他学识渊博，说理精辟，行文机智，情趣横生，极富吸引力。他是真正的马克思主义之父“ 赫尔岑 伯林：”赫尔岑成了我的人生楷模。他是一个非常杰出的作家，一个敏锐的真正的政治思想家，非常有独创性。他的自传大概是我一生中度过的最精彩的自传，比卢梭的自传还要好。正是赫尔岑使我爱上了社会思想史和政治思想史，这就是我研究思想史的真正的开端。“ 《日瓦戈医生》 帕斯捷尔纳克 伯林：”他当然是一位伟大的诗人。这么说吧，诗人可以有两种类型。第一种诗人，写诗时是诗人，而写散文诗是作家，像普希金。第二种诗人，写诗时是诗人，而写散文时也是诗人。帕斯捷尔纳克就属于第二种类型的诗人。他的散文总是诗化的散文，我看他本质上不是一个散文作家。他是晚近俄罗斯伟大的诗人之一，他的小说是伟大的诗化小说。尽管他置身于那虚浮造作的时代中心，却能诚实地描写爱——男主人公对女主人公的爱，极少有作家能够做到这点。正是他的诗使他赢得俄国人和阅读俄文作品的外国人的广泛钦佩，实际上，只有约瑟夫·布罗茨基的成就可与他相媲美。阿赫玛托娃和曼德尔施塔姆都差得远。依我看，帕斯捷尔纳克在各方面都堪称是活着的最优秀的俄国诗人。但是并非所有天才都表里如一，帕斯捷尔纳克也同样如此。他谈起话来稀奇古怪，经常让听者捉摸不透，但总让你感到才气逼人。再也没有比听他谈话更迷人的事了。据我的体会，只有弗吉尼亚·伍尔夫谈到某些东西时像他那样迷人。当然弗吉尼亚·伍尔夫有点狂妄。“ 马雅可夫斯基 伯林：”他是一个大胆的革新者，一个惊人的雄辩家，一个真正的革命者，但是，他的诗作我看比不上帕斯捷尔纳克、曼德尔施塔姆和阿赫玛托娃。“ 如果你对某些思想有兴趣，并引起你思考一些问题，那么你就不能不考虑这些思想的历史。因为思想不是单子，他们不是在真空中产生的，而跟人们的信念、生活方式、人生观和世界观紧密相连。思想之间相互碰撞和影响，并不断呈现，成为所谓”智性气候“的组成部分，他和物质因素一样，形成人们的行为和感情，并且历史地变迁着。 哲学不是一种积累性的学科。古代那些基本的哲学思想、观点、理论和见解现在仍然是哲学的中心内容。他们有其特定的横贯历史的生命。 哲学，如果教得好的话，其用处之一就是让人透过政治上冠冕堂皇的辞藻，识别各种谬论、欺骗、恶作剧、赘疣、感情上的讹诈，以及各种各样的诡辩和伪装，他能大大增强人们的批判能力。 哲学不外是要在看不到办法的地方力图去寻找问题的答案。自我理解是哲学的主要目的之一，哲学的目标就是要理解人、事物、词语三者之间的相互关系。 在叔本华那里我没有发现包罗万象的形而上学大厦那样的东西。人们可能对叔本华的体系一无所知，但照样能从他许多尖锐的有时是深刻的见解中获得教益。黑格尔的体系在我看来，似乎是希腊神话中的独眼巨人波吕斐摩斯的阴森森的黑洞，一进去就出不来，每一个脚印都指示一条道路，正如拉丁诗人所说的那样。 伯林：”他（指施特劳斯）和他的门生都相信，善于恶、对与错都直接得自某种先天的启示，某种”形而上学之眼“，也就是靠使用柏拉图式的那种无缘分享的理性官能。柏拉图、亚里士多德、《圣经》、犹太教法典、迈蒙尼德，也许还有阿奎那和中世纪的其他经院哲学家，都通晓什么样的人生才是最美好的，他们的门生现在也执着于此，而我却没有这种荣幸。“ 其二世界上存在的一切不外乎就是人、物和人脑中的观念————目标、情感、希望、畏惧、选择、想象的情景和所有其他形式的人类经验。这就是我所认识的全部东西。我无法做到无所不知。也许有一个永恒真理和永恒价值的世界，有一种只有真正的思想家才能具有的魔眼，而这只属于恐怕我永远无法进入的极少数精英的领地。 在关键时期，在历史转折关头，当各种因素大体上平衡地出现的时候，个人以及他们的抉择的行动，本身不一定可被预见（确实很少被预见），但却能决定历史的进程。我不相信历史是一部戏剧（这是赫尔岑使用的概念，他认为历史不是一部多幕剧，一部由上帝或大自然赋予主题的演出，不是有图像可辨的地毯）。而马克思和黑格尔都认为历史是一部有结局的多幕剧，它在达到高潮之后（在马克思看来，要经过可怕的冲突、苦难、灾祸）天堂之门将会启开，那就是戏剧的收场，历史（马克思称为史前时期）便会从此结束，一切事物都永恒协调，人们将合乎理性地合作共事。 维科和赫尔德相信历史进程有一定的形式，但不相信历史是一部有结局的戏剧。 我感兴趣的是维科和赫尔德的文化多元性的信念。实际上，每种文化都有自己的重心，各种文化有着各不相同的、新颖的、不可预见的思想及其互相冲突的倾向。维科最先理解到，文化就是世界相对于社会的意义，就是男男女女对于他们自身与别人和环境发生关联的集体意识；文化影响思想、情感、行为、举动的特定形式；文化是多种多样的。维科划分不同时期的文化，赫尔德则对不同时代的不同民族的文明做了区分。 我认为历史不是一个呆板的单线条的进步过程。伏尔泰说，历史是理性、知识和艺术品创造不断进步的过程，有时被可怕的干扰所打破，突然陷入野蛮状态。 预言是一种普遍却难以信赖的活动。我在维科和赫尔德的著作中经所读到的，是人类历史固有的文化多样性的观点。历史并不直线行走，不同文化之间相互作用，有时就是因果性质的作用。通向未来或过去的道路不存在唯一的钥匙，他不好跟自然科学作类比。后者的定律对重复出现的因果链是开放的，这样的因果链能总结为一般的规律。 我们能够理解不同民族和地区人们的生活方式（即使他们的生活方式跟我们的差异很大，即使他们憎恨我们或有时候被我们所谴责），这样的事实表明，我们大家能够穿越时空进行沟通。当我们认了理解了那些与我们在文化上有很大差别的群体的时候，即意味着某种强大的富于同情心的理解、洞察和共感的存在。即便其他文化排斥我们，依靠移情的想象力，我们也可以设想，为什么他们会产生这样的思想和感情，并采取相应的行动达到预定的目标。 一般性的价值观是有的，这是关于人类的经验事实，莱布尼茨称为事实的真理而不是原理性的真理。不同时空的芸芸众生，绝大多数人都共同拥有某些价值观，不论这些价值观是否自觉明晰，也不论他们在态度、举止和行动上的表现如何。另一方面，人与人之间、社会与社会之间，又确实存在着很大的差异。如果你确实了解了个人之间、团体之间、民族之间、各个完整的文明之间所存在的差异，运用想象进入他们的思想、情感世界、设想你自己置身于他们的生存环境中会怎样认知世界并审视自己与他们的关系，那么，即使你对所观察到的东西很反感（全部了解当然并不等于全部谅解），也肯定会减少盲目的偏执和狂热。想象会产生狂热，但通过想象洞察了不同于自己的境况，结果必定能减少狂热。 我认为，人们在把一个有思考里的人称为疯子或神经错乱者时务必小心谨慎。迫害不是来自神经错乱，而产生于把骇人听闻的谬误深信为真理，进而导致罄竹难书的恶果。 了解自己及他人，懂得理性的方法，掌握作为知识和全部科学基础的证据，以及力图验证直观确定性，这些对我们来说都有着根本的重要性。人权这个观念建立在一个正确的信念之上，那就是普遍存在着某种特定的品性。自由、正义、对幸福的追求、真诚、爱。这符合整个人类的利益，而不只是符合作为这个或那个民族、宗教、职业、身份的成员的利益。满足这些要求，保护人们这些要求不被忽视或否认，都是正当的。 你必须懂得什么是正义，什么是自由，什么是社会契约，并对不同类别的自由、权威、义务作出区分等等。政治理论的分野往往围绕“为什么有些人要服从理你些人”这个中心问题来展开。多数政治理论都是对这个问题的回答。实质上不是为什么服从，而是为什么应该服从和服从到什么程度。 关于消极自由的问题是：拦在我面前有什么障碍要排除？其他人怎样妨碍着我？其他人这样做是有意的还是无意的？是间接的还是有制度依据的？ 关于积极自由的问题是：谁管我？别人管还是自己管？如果是别人，他凭借什么权利？什么权威？如果我有权自主，自己管自己，那么，我会不会失去这个权利？能不能丢掉这个权利？放弃这个权利再恢复这个权利？具体怎么做？还有，谁制定法律？或谁执行法律？征求过我的意见吗？是多数人在统治吗？为什么？是因为上帝、牧师、还是党？是出于公共舆论的压力？传统的压力？还是摄于什么权威？ 积极自由在正常生活中虽然更重要，但与消极自由相比更频繁地被歪曲和滥用。历史上虚伪的积极自由所造成的危害比现代虚伪的消极自由所造成的危害更大。 真诚地相信错误的东西是很危险的，是没有道德价值或精神价值的，至少是令人遗憾的。 自由社会的好处在于容许各种各样相互冲突的意见存在而不被压制。 多元论确认：既然对于道德和政治问题以至任何价值问题不可能有一个最终的解答，并且，人们给出的或有权给出的某些解答是相互矛盾的，那么，在实际生活的某些领域，有些价值便可能变得互不相容，这样，如果要避免破坏性的冲突的话，就应该妥协，而最低限度的宽容，不管你情不情愿，都是必不可少的。 人们可以选择一种生活或另一张生活，而不能同时过两种生活；没有更为根本的标准用来决定正确的选择；既然选择这种也行，选择那种也行，在客观上就不能说一种生活优于另一种生活。它是人们想做什么和成为什么的问题。 浪漫主义认为，价值不是发现而是创造出来的，生活的目的就是生活本身。生活就是生活，没有目的。 政治哲学的任务是审视生活。要做的事就是审查为实现各种社会目标而提出的种种主张的合理性，检查为确定和实现这些目标而采取的种种方法的正当性。政治哲学要力图澄清构成有关观点的词和概念，使人们能理解自己相信的是什么，自己的行动表示什么。政治哲学还对那些维护或者反对人们所追求的各种目标的辩论作出评价，并防止麦克米兰所引述的胡说八道。 对人类的问题，追求一种唯一的、最后的、普遍的解决，无异于是追求海市蜃楼。对于人类生活破坏严重的，莫过于那种迷信了：凡美好生活都是跟政治或者军事力量相联结的。 多数英国哲学家似乎都太单薄、太技术化；跟英国哲学家相比，多数法国哲学家似乎都太含糊、太夸饰。 我认为马基雅维利是指出现实的各种价值是相互冲突的第一人。依照马基雅维利的观点，你可以选择做一个罗马人或一个基督教徒和殉道者，或者起码可以做一个当权者统治下的受害者。 任何真正的问题在某种意义上都是当代的问题。 我认为维科是理解了并告诉我们什么是人类文化的第一人。他不自觉地确立了文化的观念。就我所知，在他之前没有谁有过这样的想法，要努力去重构人们是如何看待生活在周围环境中的自己，如何看待（或感受）与自己发生关联的自然界和其他人————作为在时间中持续存在下来的一类生灵。他反思思想、情感、世界观等各类行为以及肉体的、情绪的、理智的、精神的等多种反应的本质，而正是这些行为和反应构成了文化。如果你想了解人们怎样生活，你必须了解他们的崇拜仪式，文字的内涵，他们通常运用什么类型的想象、明喻、隐喻，他们如何吃、喝、抚养小孩，如何看待自己，如何过私人的、社会的、经济的和政治的生活等等。作为一种模式的文化不是一个孤立的有机体，而是一种存在方式，树立这种理念正是维科对思想史的主要贡献。维科的值得重视的观点是：各不相同的人类思想、行为、感情和行动是互相联结和互相启发的。 米什莱按照维科的思路，认为历史就是社会跟自然力量作斗争并力图运用自然力量去创造让人们能生存和发展的生活方式的历史。人的历史是跟自然界，跟各种力量，跟一切人为的和非人为的障碍进行斗争的历史，这就是米什莱关于人类从各种羁绊中朝向自我解放不断进步的观点。 我们谈论自然界，但我们所知道的自然界仅是我们在外部世界所发现的东西。我们也看见和感触我们的身体，但我们还能说出他作为人之具体化有什么样的感受，这是一种“内在的审视”；人既是观察者，又是行动者。这就是“新科学”的大概意思。理解对于意图、感情、希望、恐惧、努力、意识和无意识的认识，而科学是对处在空间中的物体的认识。换句话说，我们可以看见桌子是什么样的，但我们看不见桌子为什么是这样的。理解过去的文化就是去理解前任所追求的东西；他们怎样看待与他人发生关联的自己，怎样看待世界以及生活在这个世界中的自己。 其三在赫尔德看来，“归属于”的意思是，你说什么，不必多做解释，大家就能了解；你的姿态、语言、所有参与交流的因素，不需敬你熟悉的人作介绍，大家都能把握。是语言、习惯、姿态或本能的反应创造了联合和团结，即创造了具有自己特色的观点、文化和社会共同体。 我一开始读维科的著作简直就被他迷住了。我总是从接受邀请做讲演或写文章开始研究的。 唯一真实的东西是精神，是人与上帝的关系，人与人的关系，别无其他了。内在的精神，个人的灵魂的底蕴，内心世界，这是唯一实在的东西，至于礼节、学问和教阶制度，统统不在话下。 赫尔德乐观地相信，人类大花园中的所有花卉都能和谐地生长，各种文化都能相互激励，为创造这种和谐的境界作出自己的贡献。绝不主张政治上的民族主义，政治上的民族主义必然大致侵略和培植民族自豪感。一个民族不是一个国家，而是一个文华实体，同一民族的人说共同的语言，生活在共同的地域，有着共同的习惯、共同的历史额共同的传统。 依我的看法，强烈的民族主义不过是耻辱心理的表现。高度发达的民族不会产生民族主义。民族主义是对伤害的反应。民资注意对一切事物均构成威胁。民族主义就等于我们对自己说，因为我们是德国人或法国人，所以我们是最优秀的人，我们完全有权做我们要做的事，一旦你把一切行为的根据放在民族这个超越个人的权威上，那就会扩展到政党，到阶级，到教会，通往压迫的道路便从此打开了。 你不能阻止科学的进步，造成灾难的不是武器，而在于使用武器的人。智能的进步是不能阻止的，人们所能做的是防止科学的滥用。廓清腐败的社会，荡涤一切污泥浊水，然后再向前进。 熊彼特正确的说过，那些相信观念必定绝对不变的人是偶像崇拜者。文明意味着必须允许变化的可能，意味着永不停息地去追求自己信奉的理想，为之献身也在所不惜。 不同的个人、集团。文化之间可以沟通，因为人的价值并非无限地多；他们共属于一条水平线，即客观的常常又相互矛盾的人类价值，在他们之间必须进行（常常是痛苦的）选择。 我自己感觉不到有这种既在现实生活之内又超越现实生活的实在。我不是宗教徒。但我对信教者的宗教体验评价颇高。我深深地被犹太教堂也包括基督教堂和伊斯兰教寺院中的宗教仪式所打动。我想，不理解信教是怎么一回事的人恐怕也不理解人为什么而活着。因此干巴巴的无神论者都是瞎子聋子，不了解人生的深刻体验，或者说不了解人生的内在底蕴，就像瞎子不能欣赏美景一样。光有感觉能力的人不能充分理解他人，包括信教者、不信教者、神秘主义者、儿童、诗人、艺术家等等。 我有一种深信不疑的看法，有些道德的、社会的和政治的价值是相互抵触的，任何一个社会总有一些价值是不能彼此调和的。换句话说，人们爱以生存的某种最终的价值，不光在实践上而且在原则上、在概念上都是不可兼得的，或者说不可彼此结合的。没有哪一个精于心机的人，同时又是无所计较，一切都听其自然的人。你不能把充分的自由跟充分的平等结合起来。给狼充分自由就不能同时也给羊有充分自由。正义和慈悲，知识和幸福，如此等等，都可能相互冲突，不可兼得。既然是这样，人类的问题（归根到底是如何生活的问题）就不可能全都求得完满的解决。这不是因为实际上有困难，找不到妥善的解决方法，而是因为这些价值本身在概念性质上都是有缺陷的。乌托邦式的解决在原则上没有缺陷，可以成立，但这样的解决是企图把不可结合的东西结合起来。某些人类的价值之所以不能相互结合，就因为他们本身是不能并存的。因此只能在彼此之间进行选择。选择可能很痛苦，如果你选择A，你就得忍痛失去B。在最终的各种人类价值之间不可避免要作出这样的选择。在任何可以想象的社会，选择都可能是痛苦而且是不可避免的。互不相容的价值本身始终是不能相容的。我们所能做的是防止选择太痛苦，这就意味着，我们需要有一种机制，使得人们对各种价值的追求尽可能不违背自己深刻的道德信念。在多元化的自由社会里，不可避免要作出各种妥协和折中，经过权衡利弊而避免最坏的情况。再三斟酌，取其一方。平等多重要？自有多重要？正义多重要？慈悲多重要？善良对重要？真理多重要？掂量掂量就知道了。知识和幸福也不总是牢牢结合的。一个知道自己患了癌症的人不会因为有了这种知识而感到幸福；无知会使他少一些自由，但同时却使他觉得多了一些幸福。这就是说，人生问题的某种最终解决，没有普遍适用的始终不变的可行保准。那些相信可能有完美无缺的社会的人必定以为，为了实现这种美好的社会，作出多大牺牲都是必要的，为了达到这种理想的目标，付出多大的代价都是值得的。他们想，如果必须要流血才能创造这种美好的社会，那么就流血吧，不管流谁的血，也不管流多少血。不打破鸡蛋怎么能做出上等的蛋卷，课时人们一旦养成打破鸡蛋的习惯，他们久不久罢手，鸡蛋打破了，蛋卷却没有做成。凡是以为对人生问题可以求得最终解决的这种狂热的信念，不能把导致灾难、痛苦、流血和可怕的压迫。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>以赛亚·伯林</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（4）：增长黑客的炼成]]></title>
    <url>%2F2018%2F01%2F13%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%884%EF%BC%89%EF%BC%9A%E5%A2%9E%E9%95%BF%E9%BB%91%E5%AE%A2%E7%9A%84%E7%82%BC%E6%88%90%2F</url>
    <content type="text"><![CDATA[《增长黑客》思维导图整理。]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>增长黑客</tag>
        <tag>产品</tag>
        <tag>用户</tag>
        <tag>活跃</tag>
        <tag>留存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（3）：娱乐时代的疾呼]]></title>
    <url>%2F2018%2F01%2F11%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%883%EF%BC%89%EF%BC%9A%E5%A8%B1%E4%B9%90%E6%97%B6%E4%BB%A3%E7%9A%84%E7%96%BE%E5%91%BC%2F</url>
    <content type="text"><![CDATA[在地铁上看完了《娱乐至死》，写点感想，做个串讲。 如果仅仅从书名“娱乐至死”的字面意思理解，我们或许都会误解，以为作者在书中定会着力指责那些活跃于电视荧幕的娱乐节目的缔造者们，将这个时代的浅薄与庸俗归咎于嬉皮笑脸的闪亮明星们，因为他们集体攻陷了普罗大众的闲暇时间，把他们一个个变成了“娱乐时代”的俘虏，从而变相地“奴役”了我们。 但其实波兹曼真正的意图并不在此。 开篇，波兹曼满怀激情地拥泵并阐释了麦克卢汉的“媒介即隐喻”的理论，我们会因为媒介的存在形式的更替变化（诸如纸媒、电媒）而在一个相当长的时期内受到其影响，这个过程是潜移默化的，最终触及到人类社会的生活方式。因为特定的媒介形式会偏好特定的内容，长期来讲，这些“特定媒介形式提供的特定内容”就会慢慢地塑造出人的新的生活方式甚至是整个人类社会的文化特征。 再者，波兹曼在此基础上阐发了“媒介即认识论”，我们每个人日常获得的信息大都是经过媒介报道的，因为自身的限制我们无法跳脱出媒介所呈现的信息，久而久之，就会在不知不觉中认为媒介中传递的信息便是对我们身处的社会真实的描绘，但事实上这仅仅是我们的一厢情愿，那些有态度的媒体们会因为其价值倾向或者利益诱导等等原因，对取材于“客观世界”的信息进行层层筛选把关之后，再进行“滤镜式”报道。这就导致了我们的“内心直观世界”和“客观的社会环境”之间不仅仅隔着不可知的成分，还因为有偏倚的媒体中介的存在而变得“扑朔迷离”。因此“真理的定义至少有一部分来自传递信息的媒体的性质，进而影响我们对“真理”或者是“外部世界”的定义与评价。 随后，波兹曼用了大量的篇幅描述了在印刷机统治下的社会环境与思想市场，在那个年代，文学家狄更斯访问美国时所得到的待遇简直可以和当代最引人注目的明星相媲美；开创美国的元勋都是学养高深的知识分子，他们深受印刷术带来的深度思考的福利；整个社会环境推崇的是严肃的、有序的、观点明确的、具有逻辑性的公众对话；美国人用白纸黑字来表明态度、表达思想、制定法律、销售商品、创造文学和宣扬宗教。而这一切都是通过印刷术实现的，印刷文字有语义、可释义的、有逻辑，建立在其上文化也正因为如此而变得稳重而成熟。波兹曼把那个时代叫做“阐释年代”，阐释是一种思想的模式，一种学习的方法，一种表达的图解。所有成熟的话语所拥有的特征，都被偏爱阐释的印刷术发扬光大；富有逻辑的复杂思维、高度的理性和秩序、超常的冷静与客观都在印刷术的带领下变得触手可及。印刷机不仅仅是一种机器，它更代表着一种话语结构。 然而让波兹曼深感缅怀的是，印刷术媒介在时代更迭中逐渐被电报、电视取代，再也回不到那个“阐释时代”。电报出现之时，便携带着无知无序无能的基因，它让公众话语变得无聊散乱，让我们的时间支离破碎，使我们的注意力被割裂。因为电报本身的性质就注定了它只适合于传播转瞬即逝的信息，各种各样的信息排着队被报道，一个报道完了便是下一个，很快就会被取代。新闻就是电报这个媒介所衍生出来的媒体内容，新闻往往是危言耸听、结构零散、没有目标受众，它就像一个个口号，毫无连贯性地进入读者的视野。在这样的语言中，没有关联，没有语境，没有历史，没有任何意义，它们拥有的是用趣味代替复杂而连贯的思想。读者们都随之变成了“知道分子”，他们知道很多事，但他们并不深入分析。 随后，电视来了，它成了新认识论的指挥中心。所有人都可以在电视中寻找一隅之地，所有公众感兴趣的话题都可以在电视上呈现。而电视的思维方式和印刷术的思维方式是格格不入的，电视对话会助长语无伦次和无聊琐碎；“严肃的电视”本来就是一个自相矛盾的表述，因为电视因为其本身的性质，注定了它只有一个声音——娱乐。 对，电视是娱乐性的，但这完全不会对文化造成任何的威胁，如果电视只是单纯地展示娱乐的内容，这是一件皆大欢喜的事情；但潜伏在背后的危机，我们或许很难察觉，那就是当所有的内容都已娱乐的方式表现出来，就完全变味了。这种变味发生在当严肃的思想被搬到荧幕面前被娱乐性地思考，发生在当需要严谨对话的社会话题以娱乐化的形式呈现在电视前。当思想、社会、文化、哲学、政治、经济这些厚重的语词和娱乐性的电视挂起钩的时候，“虚无主义”便出现了。波兹曼毫不客气地以轻蔑的语调嘲笑那些妄想利用电视机来提高文化修养的人，电视无法延伸或扩展文字文化，相反，电视只能攻击他们。毫不掩饰的揭示掩藏在电视节目超现实外壳下的，是反交流的理论，它抛弃逻辑、理性和秩序。他戏谑地说道，在美学中，这种理论被称为“达达主义”；在哲学中，它被称为“虚无主义”；在精神病学中，它被称为“精神分裂症”；如果用舞台术语来说，它可以被称为“杂耍”。 究竟电视是如何引致这种娱乐性的？因为人们喜欢看到有动感的画面，稍纵即逝但却斑斓夺目，正因为此决定了电视无法容忍思想，它必须来迎合人们对视觉快感的需求，来适应娱乐业的发展。电视新闻一切以简短为宜，做到不让观众有精神紧张之感，反之，要以富于变化和新奇的动作不断刺激观众的感官。你不必注意概念和角色，不要在同一个问题上多停留几秒。因为这种种的原因，严肃、连贯、缜密的思考天生与电视就是不相容的。 电视的这种娱乐性渗透进了教育：电视通过控制人们的时间、注意力和认知习惯获得了控制人们教育的权力，但这种控制是无力，因为不像阅读获得的往往和我们原来储存的知识相关，而由电视上获得的往往是一些具体的片段，不具备推断性。它也同样渗透进了宗教：电视最大的长处是它让具体的形象进入我们的心里，电视里的宗教形象往往会因为电视的这种长处而惹人注目，这就违背了传教的本意：他本是将一些教义深入人心，而电视压根没有这种将抽象的概念留在我们脑海中的功能，他把我们的目光转置那些传教者的仪态和举止，他们成了受教者的上帝，而不是在教义中的上帝。 在最后波兹曼借由赫胥黎发出警告，有两种方法可以让文化精神枯萎，一种是奥威尔式的——文化成为一个监狱，另一种是赫胥黎式的——文化成为一场滑稽戏。我们应该借助赫胥黎而不是奥威尔来理解电视和其他图像形式对于民主国家的基础所造成的威胁，更明确地说，是对信息自由所造成的威胁。我们要担心的是电视信息的过剩，而不是政府的限制；我们的文化部是赫胥黎式的，而不是奥威尔式的，它想尽一切办法让我们不断地看电视，但是我们看到的是使信息简单化的一种媒介，它使信息变得没有内容、没有历史、没有语境，也就是说，信息被包装成为娱乐。在一个科技发达的时代里，造成精神毁灭的敌人更可能是一个满面笑容的人，而不是那种一眼看上去就让人心生怀疑和仇恨的人。 波兹曼开始大声呼喊：在弥尔顿、培根、伏尔泰、歌德和杰弗逊这些前辈的精神的激励下，我们一定会拿起武器保卫和平。但是，如果我们没有听到痛苦的哭声呢？谁会拿起武器去反对娱乐？当严肃的话语变成了玩笑，我们该向谁抱怨，该用什么样的语气抱怨？对于一个因为大笑过度而体力衰竭的文化，我们能有什么救命良方？ 但似乎只有深刻而持久地意识到信息的结构和效应，消除对媒介的神秘感，我们才有可能对电视，或电脑，或任何其他媒介获得某种程度的控制。刻意地疏远那些信息形式还不足够，我们要学会用理性去解读文化中的象征，用思维去抵御那些虚假的思想面孔，当这个时代每个人都自觉的意识到，赫胥黎的预言正在悄然上演，那电视就或许真的可以做回它最擅长的事情——真正的娱乐，而严肃的思想也会回归本位，躺在在厚重的书卷里等待读者的共鸣。 补： 前几天在看菲利普·罗斯的文章时，也看到类似的担忧：“捷克斯洛伐克成了一个自由民主的消费社会的时候，你们这些作家就会发现自己受到了许多新的对手的折磨，而再奇怪不过的是，那个压迫人的、枯燥无味的极权社会曾保护你们免于这些折磨。特别让人不安的将是威胁文学、教育和语言的那个无处不在的、威力无比的主要对手。我可以向你保证，不会再有对抗的人群在瓦茨拉夫广场集合起来去推翻暴政，也不会有哪位剧作家知识分子被愤怒的群众抬举来救赎民族之魂，使其脱离因对手将所有人类话语减弱之后而使之所处的昏聩状态。我正在谈论的是将所有一切变得浅薄无聊的商业电视——并不是说因为被愚蠢的国家审查所控制才没有人想看的少数频道，而是说十几、二十几个因为娱乐性才被众多人整日观看的乏味的陈词滥调的频道。你和作家同行们最终摆脱了极权体制下的知识监狱。欢迎你们进入全部娱乐的世界。你们不知道失去了什么。或者说你们知道吗？”]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>娱乐至死</tag>
        <tag>波兹曼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（2）：一七年十二月读书月记]]></title>
    <url>%2F2018%2F01%2F05%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%882%EF%BC%89%EF%BC%9A%E9%98%85%E8%AF%BB%E6%9C%88%E8%AE%B0%E3%80%902017.12%E3%80%91%2F</url>
    <content type="text"><![CDATA[这个月利用上下班乘坐地铁的时间阅读了五本书，分别是波兹曼的《娱乐至死》、黑塞的《悉达多》、菲利普·罗斯的《人性的污秽》和《行话》，还有一本池建强的《MacTalk：人生元编程》。 这篇文章只对《娱乐至死》写了一些感想和串讲，其他几本就相应做了一下摘录。 一、波兹曼《娱乐至死》二、黑塞《悉达多》摘录了一些句子： 读海涅的诗，我要成为社会民主党人，我要学习海涅 他的杯子没有满，他的智怀没有饱，他的灵魂不安宁，他的心情不平静。 在他的身上看不到寻求，看不到欲望，看不到刻意，没有任何努力的痕迹——有的只是光明与安详。 但是这个清晰可敬的教导有一样东西没有包含到，它没有包含大智大慧的世尊自己经历的秘密——有别于千千万万个人的他独自的秘密。这就是在听您讲道的时候我想到并意识到的。这就是为什么我要继续走我自己的路的原因。我不再寻找另外的更好的教导了，因为我知道这样的教导是没有的。我要离开所有的教导和所有的教师，我要单独去达到我的目标，不然，就死掉。 思考的本质就是认识根源，只有通过思考，种种感觉才会变成知识，变成真实，开始成熟，不致迷失。 他必须自己去亲身体验。 除了一件小小的东西，一件极其微小的东西之外，凡是圣贤和思想家所有的一切，他们一概都不缺乏。他所缺乏的那个小小的东西，就是对于所有生命圆融统一的认识。有许多次，悉达多甚至怀疑这个知识，这个思想，怀疑它是不是真有这么大的价值，是不是它或许仅仅是思想家们天真的自我陶醉的说辞，因为思想家们也许不过是一批会思想的孩子而已。 他感觉到这位一动也不动的倾听者是在吸收他的自白，如同一棵大树吸收雨水一样。他感觉到这位静坐倾听的人就是河流的本身，就是神与梵天永恒的化身。 它们互相融入彼此，思念者的悲叹、聪明者的欢笑、愤慨者的哭泣、垂死者的呻吟，全都是互相交织在一起，互相连锁在一起，以千百种不同的方式纠缠在一起。所有的声音、所有的目标、所有的怀念、所有的悲哀、所有的欢乐、所有的善良和邪恶，这一切一切交汇在一起，就是生活之流，就是人生的音乐。 每一个真理的反面也同样是真实的。举例来说，一个真理，如果它是片面的真理，就只能以文字表达出来，也只能局限在文字能力的范围内。每一样事物，要是用思想来思考和用文字来表达，都是单方面的，都只是一半的真理，它缺乏完全、圆满和统一。 歌文达，这个世界不是不完美的，也不是在沿着一条漫长的途径逐渐向着完美发展。不，每个瞬间世界都是完美的，每一个罪恶都已经在它之中携含了恩赦，所有的孩童都是潜在的老人，所有的婴儿在身上都带着死亡，所有垂死的人必获得永恒的生命。 语言无法将思想完全表达出来，思想一旦被语言表达出来，总是会与心中的原意有所不同，有所曲解，显得有点愚蠢。 每一张面孔都是最后难免一死的生物，都是整个空幻界中的一个激情的、痛苦的例子。然而他们却没有一个死掉，他们只是不断地变幻，不断地重生，不断地有新的面孔，只有作为纽带的时间站在一张面孔和另一张面孔之间。 三、菲利普·罗斯《人性的污秽》摘录了一些句子： 1998年的夏天,在新英格兰应该是酷暑加骄阳,而在棒球场上,则该是一个白色本垒打战神和一个褐色本垒打战神之间所进行的神话般的比拼,然而那个夏天席卷全美的却是虔诚与贞洁的大狂欢,因为突然,恐怖主义被口交所代替:一位精力旺盛、面相年轻的中年总统和一个举止轻狂、使人神魂颠倒的二十一岁雇员在白宫的椭圆形办公室里,像两个十来岁孩子在停车场上调情似的。这使得美国最古老的公众再次燃起了激情,从历史的角度来看,也许是它最为不可靠、最具颠覆性的快感:伪君子的狂喜。 我自己则梦到一面大旗,仿佛是一幅基督画像,以达达派手法,将白宫从东到西包裹起来,上面撰写着如下铭文:这里住着的是一个人。这就是那个夏天,即使破烂摊、残害他人肢体罪或大杂烩都被十亿次地证明比这个人的思想或那个人的道德更为精妙。就是在那个夏天,人们脑子里想的都是一位总统的阳具,而他的生活,以其所有无耻的污秽,又一次使得美国不知所措。 我把所有一切的颠鸾倒凤和快乐都归功于伟哥。没有伟哥,这一切都不会发生。没有伟哥,我就会对世界有一个与我年龄相称的看法以及全然不同的生活目标。没有伟哥,我就不会受情欲干扰,而拥有举止规范的年长绅士的尊严。我就不会做没意思的事。我就不会做不体面的、草率的、考虑不周的,而且对所有相关的人都有着潜在危害的事。没有伟哥,我就可以继续在我的晚年发展一个有经验的、受过教育的、荣誉退休的,并早已放弃声色犬马享乐的老年人的那种客观、包容的视角,我就可以继续做深刻的哲理性总结,并一如既往地对青年人进行坚定不移的道德感化,而不至于将自己推回到不断出现的性冲动的紧急状态之中。感谢伟哥,我终于明白了宙斯缘何需要各种多情的化身。他们应当给伟哥起那个名字。他们应当叫伟哥宙斯。” 诀窍便是从(又是霍桑)“一个孤独的大脑与它自己的交流”中寻得养分。秘诀在于从诸如霍桑那样的人身上觅得养分,从才华横溢的已逝者身上觅得养分。 越战老兵是些在战后岁月里亲身经受了生活中一切罪孽的人:离婚、酗酒、毒品、犯罪、警察、牢房、毁灭性的精神压抑、无可控制的哭泣、想尖叫、要砸东西、双手颤抖、身体痉挛、面部紧绷、从头到脚大汗淋漓。由于重温枪林弹雨、刺眼的爆炸、血肉横飞的场面 父亲从不发脾气。父亲有另外的办法叫你服输。用言词。用话语。用他所谓的“乔叟的、莎士比亚的、狄更斯的语言”,用任何人都别想从你身上夺走的英语,用西尔克先生以浑厚的嗓音说出的始终完美、清晰、满怀激情的英语道白,仿佛即使在日常对话中他也是在朗诵马克·安东尼在恺撒尸体旁发表的演说。 有些人只是一味捶打沙袋,科尔曼不,科尔曼思索,与他在学校里或在赛跑时所用的方式一样:把一切不相干的东西都排除出去,不让任何不相干的东西钻进来,一心一意只关注这一件事,题目,比赛,考试——不论必须掌握的是什么,一律成为这一件事。他能够在学习生物学时那样做,他能在短跑时那样做,他能在拳击时那样做。不仅不受任何外部动静的干扰,任何内心活动也都置之度外。如果赛场上人群中有人冲他喊叫,他能充耳不闻,如果与之相斗的人是他最好的朋友,他也可以视而不见。比赛过后,他们有的是时间重修旧好。他设法强制自己无视感情,不论是恐惧、犹豫,甚至友谊——要有这些感情,但和他自己脱钩。比方说,当他进行假想拳斗时,他并不仅仅是全身放松,他同时还设想有另外一个人存在,在脑子里和另外一个人进行一场秘密打斗。临赛时,即使另外那人完全是真实的——臭气熏天的,鼻涕满脸的,汗流浃背的,正在眼前挥拳的——那家伙仍然无从得知你在想什么。没有一名教师要求得到对这个问题的答案。你在场上获得的答案你秘而不宣,当你的秘密大白于天下时,它可以出自各种渠道,唯独不经过你的嘴巴。 懦夫在未死以前,就已经死过好多次;勇士一生只死一次。在我所听到过的一切怪事之中,人们的贪生怕死是一件最奇怪的事情,因为死本来是一个人免不了的结局,它要来时谁也不能叫它不来。 随着两大防护墙的消失——大哥在海外,父亲死了——他重新充电,自由自在地想当什么就当什么,自由自在地追求最高的目标,他骨子里有信心当独特的我,自由到他父亲无从想象的地步,自由得正如他父亲不自由一般。不仅摆脱了他父亲,而且摆脱了他父亲忍受的一切。强迫。羞辱。阻挠。伤痛和故作姿态和羞耻。内心饱尝的失败及挫折的煎熬。自由地走上大舞台。自由地勇往直前,从事大事业。自由地上演无拘无束、自我定位的有关我们、他们和我的戏剧。 他自童年起所向往的就是自由:不当黑人,甚至不当白人——就当他自己,自由自在。他不想以自己的选择侮辱任何人,也不是在企图模仿他心目中的哪一位优等人物,或对他的或她的种族提出某种抗议。他知道,在循规蹈矩的人眼中,世上的一切都早有安排,都是一成不变的,他们永远也不会认为他做得对。但不敢越雷池一步、固守正确的界限向来不是他的目标,他的目标是决不将自己的命运交由一个敌视他的世界以愚昧和充满仇恨的意图主宰,必须由他自己的意志决定。 社会之所以成其为社会的一切——它不断变动的力量、无处不在的利与害的潜网、激烈的争权夺利的战斗、无休无止的吞并降服、派系的纵横捭阖、狡诈的道德术语、习以为常的仁厚独裁、变幻不定的稳定的幻觉——社会之成其为社会的一切,始终如此、必须如此的一切,对他们而言,居然跟康涅狄格的扬基人眼中的亚瑟王朝一样陌生。 心平气和地接受不如自我放逐宏伟辉煌的东西,并放弃对自身力量压倒性的挑战。与自己的失败以一种谦和的态度共处,重新以理性作为生活准则,抹去伤痛和愤怒。倘若不屈服,则静静地不屈服——平静地。带着尊严的沉思冥想 欲望乃万变之源。一切毁灭都可从中找到答案。 突然开始用傻瓜的思维模式思考问题:突然把每件事和每个人都往最好的方面想,完全抛弃对别人的怀疑、自我谨慎、自我怀疑,以为自己的一切困难都迎刃而解了,一切的困扰都不复存在了,不仅忘记了自己身在何处,而且忘记了自己是如何到达的,拱手交出勤勉、纪律、寸土必争的韧劲……就好像每个人的单斗都可以放弃了,就好像一个人可以随手捡回和扔掉自我——独具个性的、不可改变的、从一开始战斗就是为了它而进行的自我。 因为他的信条,因为他目空一切的、傲慢的“我不是你们中的一分子,我不能容忍你们,我不属于你们黑人的我们”的信条。反对他们的我们的伟大英勇的斗争——瞧他现在的德性!为争取宝贵的个性而进行的激烈斗争,他为反对黑人命运所进行的单枪匹马的反抗——瞧,这个蔑视一切的伟人落到了什么地步!这就是你,科尔曼,来寻找生活深层意义的地方。一个充满爱的世界,那是你原来拥有的,可是你却为了这个而抛弃了那个!你所作的悲剧性、鲁莽的行为!而且不仅对你自己——对我们大家,对欧内斯廷,对瓦特,对母亲,对我,对在坟墓中的我,对在坟墓中的我父亲。 “美国海军”,文身只说了这些,高度仅有四分之一英寸的几个字,用蓝色颜料刺在一个蓝色铁锚的两个蓝色臂膀之间,铁锚本身有两三英尺长。就军人的文身而言,是个非常简朴的图案,而且谨慎地、恰恰安置在右胳膊肩关节下,无疑是个相当容易隐藏的文身。但当他回想他如何将它刺上去时,它不仅成为一个唤起他生命中最糟糕夜晚狂乱情景的标记,而且成为一个唤起潜伏在狂乱背后之一切的标记——它是他全部的历史,他的英雄主义与羞耻不可分割性的缩影。镶嵌在那个文身里的正是他的一个真实、完整的自我形象,其中可见无法磨灭的身世,如同根深蒂固事物的原型,因为文身恰恰象征着永远无可变更的一切,其中也包含着巨大的业绩,包含着外部势力,不可预知未来的整个链接,一切暴露的危险,以及一切隐藏的危险——甚至生命的无意义性都隐含在那个小小的、傻乎乎的蓝色文身之中。 她在他们那个年龄早看过了所有的黑泽明,所有的塔科夫斯基,所有的费利尼,所有的安东尼奥尼,所有的法斯宾德,所有的沃特缪勒,所有的撒提亚吉特·雷,所有的雷内·克莱尔,所有的文·温德斯,所有的特吕福、戈达尔、夏布罗尔、利斯奈、罗米尔、雷诺阿,而这些孩子只看过星球大战。 他在里面,正和福妮雅一起,相互保护着对方,不受任何外人的侵扰——彼此,对对方而言,构成了整个世界。他们在里面跳舞,很可能光着身子,超越人世的苦难,置身于一个植根于世俗欲望的非凡的天堂里,在那里他们的结合是一出他们倾注生命中所有的愤怒与失望的戏剧。 音乐家们的确揭示了我们生命中最年轻、最天真的思想以及对于非现实、不可能实现的东西的根深蒂固的渴望。 他将假装说世界属于我们,而我会让他这么假装,然后我也会假装。但是,为什么不呢？我能跳……但他得记住。 我们留下污秽,我们留下踪迹,我们留下我们的印记。污染、残酷、欺凌、谬误、粪便、精液——要待在这儿就别无二致。和反抗无关。和恩赐或救赎无关。在每个人的身上。存储于内心。与生俱来。无可描述。污秽先于印记。没有留下印记之前便已存在。污秽完全是内在的,不需留印记。污秽先于反抗,是包围反抗并扰乱一切的解释与理解。这就是为什么所有的净化行为纯属玩笑,而且还是个野蛮的玩笑。纯洁的幻想是极其可怕的,是疯狂的。对纯洁的追求其实质倘若不是更严重的不纯洁,又会是什么呢？她所有关于污秽的话归结起来无非是说它是不可逃避的。这,自然,便是福妮雅的阐释:我们无可避免地都是被污染的角色。心甘情愿地接受这可怕的、原始的不纯净状态吧。她像希腊人,像科尔曼的希腊人,像他们供奉的神。无人不是小心眼。争吵。械斗。恨。谋杀。交媾。他们的宙斯成天只想操女的——女神,女人,母牛,母熊——不仅以他自身的形象出现,还更为令人兴奋地将自己装扮成兽类,作为一头公牛气势雄劲地凌驾于女性之上,化做一只扑打着双翼的白天鹅以异乎寻常的方式进入她的身体。对这位众神之王而言,肌肤之乐永无穷尽,花样翻新也层出不穷。欲望所带来的一切疯狂。放荡。堕落。最粗野的欢乐。还有妻子的怒气。不要那绝对孤独,绝对隐晦,作为现在、过去以及永远唯一主宰的穷极无聊地整日为犹太人操心的希伯来上帝。不要那完全无性别的基督神人和他降孕怀胎的母亲以及所有某种精致的超凡性所激发的罪恶感与羞耻感,而选择纠缠于冒险之中、具有鲜活表达力、朝秦暮楚、沉醉于声色犬马、精力充沛地与他丰富多彩的生活联姻、从不单独行事、从不偷偷摸摸的希腊的宙斯。而选择神圣的污秽。对福妮雅·法利来说,伟大的反映现实的宗教,倘若,通过科尔曼她有所了解的话,如同希伯来幻象所言,是以上帝的形象创造的,好吧,但并不是我们的上帝——是他们的。上帝淫荡。上帝腐败。如果真有过上帝的话,是个活生生的神,是以人的形象出现的上帝。 虽然世上满是那种自以为他们将你或你的邻居看透了的人,实际上未知的东西却深不可测。关于我们的真相是无穷无尽的,谎言也同样如此。 四、菲利普·罗斯《行话》 与我们闭着眼睛什么也看不见，光溜溜地呱呱坠地不同，罗斯先生一出场，指甲、毛发、牙齿都已长齐，他说话流利，技巧娴熟，机智幽默，富有生气，具有名家风范。 卡夫卡的“小说一直所坚持的就是，看上去似乎难以想象的幻觉和毫无希望的诡论其实正是构成我们现实的东西”。 他们都在小说里直露地描写了性，但并不认为那是渲染色情，而是认为性是人的最深层的东西，最能体现人的本质，最能放射出异常强烈的光芒。 他在倾听时专心、安静，就像金花鼠在石墙上发现了陌生的东西一样。 正常人在生理构造上注定要从事具有目的的活动，无所事事或者无目标的工作（如奥斯威辛集中营的工作）则导致痛苦和萎缩症。 这部分及整部书给我留下深刻印象的是，一颗实际、高尚的科学心灵的思考在多大程度上使你得以生存。在我看来，你的幸存并非由兽性的生物力量或者难以置信的运气所决定，而是由你的专业性格所决定：讲究精确的人，追求秩序原理的实验控制者，他所重视的一切都被颠覆。就算你是恶魔般的机器中一个编了号的部件，你也是一个总在用一颗系统化的心灵去理解的部件。在奥斯威辛集中营里，你自语道“我思考得太多”以抵制，“我过于开化”。但对我而言，那位思考太多的开化人与那位幸存者是分不开的。科学家与幸存者是同一个人。 卡夫卡依仗的是内在的精神世界，企图对现实达到某种掌握，而我经历了集中营和森林这样实实在在、具体细致的经验世界。 第二次世界大战中的犹太人经历并非属于“历史”范畴。我们遭遇到了原始神秘的力量，一种神秘的潜意识。我们对其中所含的意义没有任何了解，时至今日我们仍然无法了解。这个世界似乎是理性的（有火车、发车时间、火车站和工程师），但这些只是想象的旅行、谎言和诡计，只有深奥的非理性冲动才能虚构出来。 他们的目盲和耳聋，以及他们只全神贯注于自己的事务等构成了他们单纯的一部分。那些凶手是功利主义者，知道他们需要什么。单纯的人总是不幸、滑稽的受害者，从没有及时听到危险的信号，混杂起来，乱糟糟一堆，最后掉在了陷阱里。那些缺陷使我感到陶醉。我爱上了它们。犹太人施用诡计统治世界的神话原来是被在某种程度上夸大了。 捷克斯洛伐克地下出版所产生的环境有其独特性。由外国军队支持的政权——由占领者建立的政权知道，这个政权只有符合占领者的意愿才能存在——害怕批评。它还知道，任何一种精神生活最终都导致对自由的向往。所以，它毫不犹豫地禁止实际上所有捷克斯洛伐克文化，使作家无法写作，画家无法展览，科学家——特别是社会科学领域——无法进行独立的研究；它摧毁大学，大多数情况下任命俯首帖耳的职员当教授。突然间被投入到这个大灾难之中的国家被动地接受着这一切，至少一度如此，无奈地看着最近崇拜的人物一个接一个地消失。 文学没有必要四处搜寻政治现实或者担忧兴衰更替的统治体系；文学可以超越这些，而且仍然可以回答统治体系在人们心中所激发出来的问题。 捷克斯洛伐克成了一个自由民主的消费社会的时候，你们这些作家就会发现自己受到了许多新的对手的折磨，而再奇怪不过的是，那个压迫人的、枯燥无味的极权社会曾保护你们免于这些折磨。特别让人不安的将是威胁文学、教育和语言的那个无处不在的、威力无比的主要对手。我可以向你保证，不会再有对抗的人群在瓦茨拉夫广场集合起来去推翻暴政，也不会有哪位剧作家知识分子被愤怒的群众抬举来救赎民族之魂，使其脱离因对手将所有人类话语减弱之后而使之所处的昏聩状态。我正在谈论的是将所有一切变得浅薄无聊的商业电视——并不是说因为被愚蠢的国家审查所控制才没有人想看的少数频道，而是说十几、二十几个因为娱乐性才被众多人整日观看的乏味的陈词滥调的频道。你和作家同行们最终摆脱了极权体制下的知识监狱。欢迎你们进入全部娱乐的世界。你们不知道失去了什么。或者说你们知道吗？ 如同卡夫卡独身一人一样，据说舒尔茨与女性保持着长期热烈的联系，通过信函过着大部分的色情生活。 作为一个文化史概念，东欧是俄罗斯，其具体的历史位居拜占庭世界。波希米亚、波兰、匈牙利，就如奥地利一样，从来就不是东欧的一部分。从最初起，他们就参与了西欧文明伟大的冒险，如哥特文化、文艺复兴、宗教改革——这一运动的摇篮之地确切地说就在这一地区。而现代文化的最伟大的脉搏在中欧跳动——心理分析、结构主义、十二音技术、巴托克音乐、卡夫卡和穆西尔的小说新美学等。战后中欧被俄罗斯文明吞并（或者至少是其主要部分），致使西方文化失去了其关键的重心 幽默感是辨认的可靠标志。从那时起，我就对一个没有幽默感的世界感到恐惧。 人们的愚蠢在于为一切都提供一个答案，小说的智慧在于对一切都提出一个问题 小说家教育读者把世界当成一个问题来看。这种态度中包含着智慧和宽容。在一个建立于极度神圣的肯定之上的世界里，小说就无法存在。极权主义的世界，无论是建立在什么主义之上，都是一个答案的世界，而不是问题的世界。在这个世界里，小说没有地位。不管怎么说，在我看来，似乎全世界的人当今都喜欢判断而不喜欢理解，喜欢回答而不喜欢提问，结果小说的声音被人类吵闹的、愚蠢的肯定声音所淹没。 这就是作为作家的代价，对过去始终难以忘怀——痛苦、激动、拒绝，所有的一切。我坚信，这种抱着过去不放的思想，虽然没有希望，但它充满热情地渴望重构过去，使它发生变化。医生、律师，还有其他许多稳定的公民不会被一种持之以恒的记忆所折磨。他们以自己的方式和我们一样受到烦扰，只不过他们不知就里，因为他们不探究。 我秘密参与周围世界的一切，关注每一个人的每一点历史，这些材料都是故事和小说的必备基础。 你一旦长大成人，离开家庭，选择了作家这种孤独的生活，性爱就不可避免地成为你可以继续进入的最重要经验领域？ 性爱的兴奋在很大程度上是与痛苦和分离联系在一起的。我的性生活对我来说是最重要的，相信对于每个人也如此。思考和完成它要花费很多的时间，但思考性爱是令人自豪的部分。对于我来说，它主要是秘密的，包含着神秘和劫掠的成分。我的日常生活和性生活不是一个整体——它们是分离的 贝娄推翻了一切：基于和谐、有序的叙述原则之上的写作选择，受惠于卡夫卡的《审判》、陀思妥耶夫斯基的《双重人格》（The Double）和《永久的丈夫》（The Eternal Husband）的小说家社会精神气质，以及一种很难说是因为喜欢闪光、色彩和足够的实体而产生的道德视角 《赫索格》是贝娄写作生涯中首部涉及性这个广泛领域并进行长时间探讨的小说。赫索格的女人对他来说至关重要，因为她们勾起了他的虚荣，激发了他的肉欲，引导了他的爱，引起了他的好奇。还因为可以表现出男人的聪明、魅力和俊美容颜从而使他滋生了男童般的快乐和喜悦——在她们的爱慕中他得到了确认。她们骂出的每一句难听的话，杜撰的每一个称号，头颅的每一次迷人的转动，手的每一次安慰性的触摸，嘴巴每一次愤怒的扭曲，他的女人们都以异性特有的他者性使赫索格神魂颠倒。 对无法预料的展现就是一切。把错误转变方向，那残酷的难料之事就是我们学童所学的无害的‘历史’，一切当时的难料之事被当做历史的必然编入历史，难以名状的恐怖被历史所掩盖，灾难转换成了史诗。 五、池建强《MacTalk：人生元编程》日知录（8）：MacTips]]></content>
      <categories>
        <category>读书记</category>
      </categories>
      <tags>
        <tag>娱乐至死</tag>
        <tag>悉达多</tag>
        <tag>人生的污秽</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日知录（4）：启发式认知偏差]]></title>
    <url>%2F2018%2F01%2F02%2F%E6%97%A5%E7%9F%A5%E5%BD%95%EF%BC%884%EF%BC%89%EF%BC%9A%E5%90%AF%E5%8F%91%E5%BC%8F%E8%AE%A4%E7%9F%A5%E5%81%8F%E5%B7%AE%2F</url>
    <content type="text"><![CDATA[在不确定条件下，人类会采用一种启发式的思维方法，即根据以往（相同的或类似的甚至无关的）的经验来对当前情况进行判断。这是一种思考上的捷径，是解决问题的简单、笼统的策略，也称之为经验法则或拇指法则。 因为人类是认知的“吝啬鬼”，任性、懒惰。启发式推理会简化信息处理的过程。因此，当人们面对大量的信息和不确定性进行判断时，往往并不遵循贝叶斯法则，而是凭借直觉或者以往的经验进行判断。依赖“启发法”做出的决策带有不确定性，只能说可能是正确的结论，但如果所遗漏的因素和现象很重要，那么信息的缺损就会导致产生判断和估计上的严重偏差。 Kahneman和Tversky指出人们通常具有三种启发式推理方式： 代表性启发法 可得性启发法 锚定与调整启发法 这三种方法既可以得出正确的推理结果,也有可能导致错误的结论。 一、代表性启发法在使用启发法时，首先会考虑到借鉴要判断事件本身或事件的同类事件以往的经验即以往出现的结果，这种推理过程称之为代表性启发法。 一般情况下，代表性是一个有用的启发法，但在分析以往经验，寻找规律或结果的概率分布的过程中,可能会产生严重的偏差,从而得到错误的启示，导致判断错误。 1.1 代表性启发法的定义 代表性会导致忽略样本大小。在分析事件特征或规律时，人们往往不能正确理解统计样本大小的意义，对总体进行统计的结果才是真正的结果，样本的数量愈接近真实的数量，统计的结果也就愈可信，样本愈小,与真实数量相差愈大,统计的结果愈不能反映真实的结果情况。代表性启发法是对同类事件以往所出现的各种结果进行统计分析，得到结果的概率分布，从而找出发生概率最大的结果即最可能发生的结果。 因此必须考察所有同类事件这个总体或者考察尽量多同类事件(大样本)但人们往往趋向于在很少的数据基础上很快地得出结论。 代表性会忽略判断的难易程度，即使面对的是一个复杂的难以判断的问题，也简单地去作出判断，或经常根据不规范的和与判断无关的描述轻易地作出判断，或经常会忽略掉不熟悉或是看不懂的信息，只凭自己能够理解和熟悉的信息去作出判断，这些忽略掉的信息可能对判断是关键的。 代表性启发出现的背景 没有时间认真思考某个问题 负载信息过多以至于无法充分对其进行加工 认为问题不十分重要以至于不必太过思虑 缺乏做决策所需的可靠知识或者信息 1.2 Tom W 实验1973年Kahneman及Tversky进行了一个名为“Tom W ”的著名实验，大概如下：给被试以下一段关于Tom W.的描述：“Tom W.智商很高，但是缺乏真正的创造力。他喜欢按部就班，把所有事情都安排得井然有序，写的文章无趣、呆板，但有时也会闪现一些俏皮的双关语和科学幻想。他很喜欢竞争，看起来不怎么关心别人的感情，也不喜欢和其他人交往。虽然以自我为中心，但也有很强的道德感。” 然后要被试估计，Tom W.最有可能是以下哪个专业的学生：企业管理，工程，教育，法律，图书，医学，社会学？想象一下如果你是其中一名被试，你会怎么回答。 结果，绝大多数被试都认为Tom W.最有可能是工程系学生。为什么呢？很有可能是因为Tom W.最像一个学工程学的学生。也就是说，对Tom W.的以上描述，与我们心目中一个理工科学生所应当具有的形象完全吻合（或者说代表了一个理工科学生的形象），所以我们认为Tom W.最有可能是工程系的学生。这就是典型的代表性启发式思维方式。当面对不确定的事件，我们往往根据其与过去经验的相似程度来进行判断或预测。说简单一点，就是基于（过去经验的）相似性来预测（当前事件的）可能性。到底个体A是否归属于群体B？如果个体A具有群体B的某些特征（具有相似性、代表性），则认为个体Ａ归属于群体Ｂ。 如果我们在公共汽车上看到一个人鬼鬼祟祟，像个小偷，则我们会认为他就是一个小偷，并提高警惕性。有时相似性确实和可能性有关，因此这种判断是正确的，但有时则可能会因此忽略其它相关信息而做出错误的判断。 比如在Tom W.的实验当中，被试就完全忽略了学生在各个专业中的基础比率（base rate）。就算上述7个专业的学生都一样多，那么任何一个学生是工程系的学生的概率和他是其它任何一个专业的学生的概率是一样的，即1/7。根据另外一组被试对所有学生在各个专业中所占的比率的估计，学工程学的学生应该比学其他专业的学生要更少，即还占不到1/7。如果考虑到这一点，那么任意抽一个学生出来（比如Tom W.），他是学工程学的可能性应该是很低的。这种在判断时忽略基础比率而导致的谬误就是所谓的基础比率谬误（base rate fallacy） 。 再看另外一个问题 ，Linda，31岁，单身、坦率，活泼，她学的专业是哲学。当她还是个学生时，就非常关注歧视和社会公正的问题，并且参加过反核武的示威游行活动。（Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.）问Linda更有可能是什么样的人？ Linda是一个银行出纳员。 Linda是一个崇尚女权主义的银行出纳员。 很多人都会选第二项。因为从对Linda描述更符合我们心目中女权主义者的形象（或者说代表了我们心目中女权主义者的形象），所以我们就更倾向于认为Linda是一个崇尚女权主义的银行出纳员。我们在这里就运用了启发式的判断，却没有注意到这样一个基本道理：两个独立的事件同时发生的概率不可能高于其中单个事件单独发生的概率，从而犯了一个所谓的结合谬误（conjunction falalcy）。Linda是崇尚女权主义的概率可能很高，Linda是一个银行出纳员的概率可能不高，但Linda同时既是银行出纳员又崇尚女权主义的概率就肯定低于前二者的概率了 。这个道理说出来很简单，大家心里都清楚，但一到实际中人们往往就不会运用。我们会犯这样一种错误的原因可能是因为对事件描述得越详尽，就越容易让我们产生联想，进而导致我们误以为事件越容易发生。 1.3 赌徒谬误在运用代表性启发法进行判断时还有可能会导致赌徒谬误（gambler’s fallacy），也称为蒙地卡罗谬误（The Monte Carlo Fallacy ），主要来源于这样一个故事 ： 1913年8月18日，在蒙地卡罗的一间赌场里的轮盘 游戏中，黑色不可思议的连续出现了十五次，人们开始近乎疯狂的冲着去押红色。当黑色连续出现了二十次以后，人们还进一步加大了他们的赌注，因为大家都认为在黑色连续出现了二十次以后再出现黑色的可能性已经不到百万分之一了。结果黑色是创纪录的连续出现了二十六次！这间赌场因此挣得盆缽满盈。 大家都有这种感觉：似乎黑色已经连续出现太多次，不可能再出现了。这种想法很普遍。比如玩抛硬币，我告诉你前面抛的五次结果都是“正”，要你猜一猜，下一次会出现哪一面？肯定很多人会倾向于“反”面。前面已经出现过那么多次“正”面了，不可能还是“正”吧？连续出现6次“正”面的概率太低了。又比如人们在买彩票的时候，一般都不会选择上一次中奖已经出现过的号码，不可能连续两次中奖都有同一个号码。其实这也是支撑赌徒一直赌下去的重要心理原素之一：我已经输了那么多次了，无论怎么样也应该会赢一次吧。 这其实也是一种启发式的思维模式。我们认为不可能连续出现6次“正”面或是极端的连续26次黑色，或者连续两次中奖号码都有同一个数字，因为抛硬币、赌博、彩票等事件是随机的，这样的概率实在是太低了，根本就不像是随机事件，一个随机事件怎么可能有这么多巧合？ 但到底怎么样才叫“随机”？到底是“正反反正反正”还是”正正正正正正”更有可能出现？其实现实生活中随机事件看起来往往都不像是随机的，或者说随机事件并没有你想象的那么随机。所谓随机也就意味着事件与事件之间在统计学意义上是独立的（what makes a sequence random is that its members are statistically independent of each other），一件事情的发生在统计学意义上对另一件事情的发生没有任何影响。（the occurrence of one has no statistical effect upon the occurrence of the other）。随机事件是没有倾向性的，是不可预测的，是没有记忆功能的。因此，就算黑色已经连续出现了N次，下一次是红还是黑都是随机的，认为黑色不太可能再出现而疯狂的去押红色是没有道理的；不管“正”面已经连续出现了多少次，下一次的结果要不是正面就是反面，二者出现的概率都是50％。 但是竟然会出现连续26次黑色或者连续6次正面这种情况，还是让人难以接受。这是因为有时小样本不具有代表性，样本越小，与真实的数量相差越大，统计的结果越不能反映真实的情况。只有对总体进行统计的结果才是真正的结果，也就是说样本的数量越接近真实的数量，统计的结果也就越可信。如果只抛十次硬币，正反面出现的概率不一定是50%，什么情况都有可能发生，只有抛足够多次，才能得出正反面的概率是50%的结果。这提醒我们有时不要匆忙的作出判断或下结论，很有可能你看到的只不过是一个小样本。 二、可得性启发法人们在什么情况下更愿意买地震保险？想象在汶川地震以前有位保险销售人员向你推销地震保险，相信你是打死都不会买。因为地震这个事情离你的生活实在是太遥远了，那是八竿子打不着的事情。但在发生汶川地震以后，情况就截然不同了，特别是你在电视上看到了地震的各种惨状以后，如果再有保险销售人员向你推销地震保险的话，你肯定会比以前更愿意掏钱。现在很多人去买房，总喜欢问这样一个问题：这房子能抗几级地震啊？如果我们仔细想想的话，人们的这种反应其实是非常可笑的，因为恰恰是因为刚刚发生过地震，所以在一段时间内再次发生的地震的可能性是很小很小的。那么为什么人们反而会在地震以后更愿意买地震保险呢？这就是可得性启发法（Availability heuristic）的影响。 很多时候我们做出判断是情绪性的、无意识的，非理性的。人们对于越容易想起来的事情（即在脑海中的印象更为深刻），会觉得越容易发生，这就是所谓的可得性启发，通过易得性来判断其可能性，即“人们倾向于根据客体或事件在知觉或记忆中的可得性程度来评估其相对频率，容易知觉到的或回想起的客体或事件被判定为更常出现 ”。比如我们在判断是否要买地震保险时，脑海中首先浮现到的就是不久前发生的地震以及我们在电视上目睹的各种惨状，于是就会觉得地震确实很可怕，并且似乎离我们很近或者说很有可能会发生在我们自己的身上，因此就更愿意购买地震保险了。 对于可得性启发经常被提及的一个实验是这样的，问：以字母k开头的英文单词和第三个字母是K的英文单词相比，二者谁更多？大多数人认为以字母k开头的英文单词更多。因为人们很容易就想到以字母k开头的英文单词，比如keep,kill,kitchen等，但要想起第三个字母是K的英文单词就有些困难了，于是人们就会认为以字母k开头的英文单词会更多。实际上，第三个字母是k的单词是以k字母开头的单词的3倍。 有时候可得性启发式是正确的，越容易回想起来的事情确实越有可能发生，因为不断重复（容易）发生的事情，自然更容易在我们的脑海中留下深刻的印象。但是我们不仅仅只会记住那些经常发生的事情，事实证明，我们更喜欢那些生动的、形象的、具有情绪感染力的事件。 大家都应该见过这张希望工程的宣传图片，相信许多人在看到这张照片以后，都会被深深的感动，可能本来没打算捐钱的结果也捐一点钱，本来打算只捐5块钱的结果捐了10块钱。我们通过这张图片，对贫困失学儿童有了更为直观的、生动的认识。 这就是所谓的一张图片胜过千言万语。不管我们是说帮助中国的失学儿童有多么多么的重要，还是说中国有多少多少的失学儿童并列举一堆统计数字，都不如一张具有视觉冲击力的图片更能打动我们的心。我们的行为很容易因此而受到影响——比如多捐一点钱，当然这是好的方面。（这也就是为什么在禁毒宣传或者交通安全宣传的时候总是喜欢摆一些恶心的照片出来） 此外，我们对自己亲身经历过的事情印象也会特别深刻。一个死于骑摩托车的亲戚比大量的统计数据更能影响你对摩托车的态度。结了婚的年轻人经常会为干家务的事情发生争吵，总认为自己干的家务活要比对方干得多。美国的心理学家做过这样一个调查，让妻子和丈夫各自评估自己所干的事占所有家务的比例，然后将两人的比例加起来，结果总是超过100%。这其实很好理解，这一方面是因为自利归因，另一方面也是因为自己干的家务活自己总是能记住，所以就总觉得自己干的要比对方多。 可得性启发会导致我们做出错误的决策。因为受可得性启发的影响，我们做出判断决策时，依据的可能就仅仅是一些经过我们头脑选择过的、印象最为深刻的例子。正如道金斯在《自私的基因》里说到的：经过选择的例子对于任何有价值的概括从来就不是重要的证据（Chosen examples are never serious evidence for any worthwhile generalization.）。因为例子永远都只能是一个例子，不具有任何的代表性。如果你遇到一个河南人把你的钱都骗光了，你能因此而得出结论说所有的河南人都是骗子，都是不可靠的吗？如果你不是遇到一个河南人如此，而是十个、一百个河南人都如此呢？那么也仅仅说明这十个、一百个河南人是不可靠的，但不能说所有河南人都是不可靠。 此外我们在举例时很容易忽略那些与我们已有观念不一致的信息，从而导致所谓的证实性偏差。塔布勒在《黑天鹅》中把过度举例称为“无知的经验主义”。想象一只火鸡，每天都被喂得饱饱的，每次的喂食都使它更加相信生命的一般法则在于每天得到“为他的最大利益着想”的友善人类的喂食。日子一天天的过去，它的信心也越来越充足。但感恩节也是一天天的临近，它被屠杀的危险也越来越大。当危险最大时它的安全感却达到了最大值。直到感恩节的前一天，它才真正的明白过来，它过去获得的知识和经验都是无关痛痒的甚至是虚假的。在某种程度上，我们所有人都是这只火鸡。 所以例子不应该作为论据来使用的。那么有人会说，你这篇文章里不就通篇都是例子吗？例子在这里只能是为了把更好的把一个道理解释清楚、使人更容易理解。也就是说我们应该用例子来阐释说明而非证明。但现实恰恰是人们很喜欢用例子来证明自己的观点——特别是那些最容易想到的例子。 比如有一次你乘坐A航空公司的航班，结果A航空公司把你的行李给弄丢了，你火冒三丈，觉得A公司的服务怎么这么差，以后再也不要坐这间公司的航班了。这其实和河南人的例子差不多，一次不愉快的经历可能只是一宗个案，并不具有任何代表性。 但有的人就会认为，保障乘客的行李安全是航空运输的最基本要求之一，如果连这一点都做不到的话，那么我们还有什么理由来相信它，就算是个案，这样的事情只要经历一次就足够了，难道还要再丢一次行李吗！这种想法是可以理解的，但却是没有道理的。在某种程度上，错误其实是不可避免的。管理学上有所谓的六西格玛质量管理法，我总是很怀疑这是否能够真的做到。就算是DNA复制这种高度精确的事件，都会不可避免的出现偏差（这正是进化的源泉之一），更何况人为的事件？因此，航空公司把乘客的行李弄丢其实是一个不可避免的错误，只是很不幸碰巧被你遇上了。这是一个负面的影响，导致你对整体产生了负面的认识。但是想象如果是一件正面的事件，比如你买彩票，一不小心中了五百万，那么你会如何认识中奖这样一件事情呢？难道你会因此而觉得彩票中奖是很容易，或者别人也应该中奖，或者你以后会更容易中奖吗？ 当然我不是说航空公司把你行李弄丢是和中彩票一样的低概率事件，或者说是可以原谅的。而是要看到，不能仅仅因为一次丢失行李的不愉快的经历而对该公司的服务水平完全否定。如果真的要想知道A公司的服务水平到底怎么样？我们就需要调查不同航线准时到达率和行李丢失率的统计数据。只有统计数据才能相对真实的反映出客观情况。（这也让我想到了所谓的一票否决制，某些领导如果在某些事情上没有达标或者出了什么问题，那么就一概否决，这是不够科学的。没有被否决可能仅仅是运气好罢了。） 在现代传媒的作用下，可得性启发对人们判断决策的影响更加的大。因为媒体为了追求收视率，吸引眼球，总是喜欢过度报道一些特别的事件。正如人们戏言：狗咬人不是新闻，人咬狗才是新闻。但是如果你看电视看的太多并且懒得去想的话，那么你就会很容易认为满大街的人都在咬狗。比如人们总认为坐飞机很危险，事实上大量的统计数据表明，飞机并不会比坐汽车更危险，但为什么人们会有这样一种错误的判断？很大程度上是因为飞机失事更有新闻价值。如果是一宗普通的交通事故，媒体就不会有太多的报道，但如果是一架飞机坠毁了，那么媒体必定是连篇累牍的大肆报道。结果这些事情在人们的脑海里留下了极其深刻的印象，当我们需要作出判断时脑海中首先浮现的就是这些飞机失事的场景，因此就会得出结论：飞机非常的危险。当然人么更害怕坐飞机也有其他方面的原因，比如人类的生物本能，人们对于自己不能控制的事物的恐惧等等。 《魔鬼经济学》里有一个例子：假设有个一个8岁大的叫莫莉的孩子，她有两个最好的朋友，一个叫艾米，一个叫伊玛尼，两个朋友都住在附近。莫莉的父母知道艾米的家里放着一把枪，于是他们不许莫莉到艾米家玩。所以莫莉就经常跑到伊玛尼家玩，伊玛尼家的后院有个游泳池。莫莉的父母觉得自己的做法上是在保护莫莉，这样做是对的。 可根据统计资料显示，这种做法一点都不明智。平均来说，美国每1.1万个家庭游泳池就能溺死一个孩子。美国一共有600万个这样的游泳池，这也就是说，每年将近有550个不到10岁的孩子是溺死在游泳池里。相比之下，在美国，每100多万支枪才会杀死一个孩子。据估计，美国一共有2亿支枪，这就是说美国平均每年死于枪口下的孩子数量大约为175名。所以对于美国孩子来说，他们死于游泳池里的概率（1：11000）要远远大于死于枪口的概率（1：1000000）：也就是说莫莉在伊玛尼家的危险程度是在艾米家的100倍。 问题就在于为什么莫莉的父母会觉得枪会比游泳池更危险？这和人们对飞机的态度的原因是一样的。媒体更愿意报道一名丧心病狂的家伙开枪打死一名孩子而不是一名孩子在游泳池里淹死。各种奇闻异事更容易在人们的脑海中留下深刻的印象。因此在某种程度上我们对世界的认识是扭曲的，我们的身边充斥着各路媒体和大量信息，我们以为自己很清楚自己在想什么，实际上我们的世界观价值观已被他人所左右。从这个角度来看，信息越多误差越大。 我们如何摆脱可得性启发带来的偏差？这其实是不可能的，因为启发法是一种无意识的思维，也就意味着有时候我们连自己已经运用了启发法都不知道，更别说避免其错误了。但如果我们对启发法有更深入的了解，那么还是有助于我们减少在这一方面的错误的。感觉往往是不可靠的，不要轻易的下结论。当我们做出判断或得出一个结论时，必须要问自己：我的依据（论据）是什么？这些依据是否足以支撑我的判断？我们对那些感人或者骇人的一切生动的故事都必须保持足够的警惕，不要被我们的情绪所主宰，要更加注重统计数据，很多时候只有统计数据才能相对真实的反映出客观情况。 三、锚定与调整启发法这是一个地球人都知道的故事。一条巷子里有两家卖粥的小店。左边一个，右边一个，两家店的生意都很好，每天都是顾客盈门。可是，晚上盘点的时候，左边这个店总是比右边那个店每天多赚两三百块钱，而且每天都是这样，让人心生不解。 细心的人终于发现了其中的秘密。如果你走进右边那个粥店，服务员微笑着把你迎进去，给你盛好一碗粥，热情地问你：“您好!加不加鸡蛋?”一般情况下，喜欢吃鸡蛋的人，就会说加一个吧!于是服务员就会拿来一个鸡蛋;不喜欢吃鸡蛋的人，就会说不加，喝完粥结了帐就走了。可是，如果你走进左边那个粥店，服务员同样也是微笑着把你迎进去，给你盛好一碗粥，然后热情地问你：“您好!加一个鸡蛋还是加两个鸡蛋?”一般情况下，喜欢吃鸡蛋的人，就会说加两个;不喜欢吃鸡蛋的人，就会说加一个。就这样，一天下来，左边的这个粥店比右边的那个粥店每天要多卖出很多个鸡蛋，这就是它每天多出多出两三百块的原因。 大凡讲这个故事的人，都是在讲左边那家粥店的生意经。其实，粥店的生意经里蕴含着十分深刻的心理学道理。左边的那个粥店其实是在运用心理学中的锚定法，诱导消费者在不经意间做出有利于店家的选择。如果你是它的顾客，你的决策则是受到了心理锚定效应的影响，你不自觉地在粥店设定的条件下进行了决策选择。 也许，多吃一个鸡蛋并没有什么大的问题，但是，如果在商业交易或谈判中，你多付出了10万或者100万，可能就是一个大问题，或者说是一个大损失。左边小店的服务员把顾客“锚定”在“加几个鸡蛋”上，而右边小店的服务员则把顾客“锚定”在“要不要加鸡蛋”上。在前一种情况下，顾客是在“加一个鸡蛋还是加两个鸡蛋”上进行选择或调整，而后一种情况下，顾客是在“加不加鸡蛋”上进行选择或调整，顾客有限的理性使很多的顾客没有充分地调整，使两个小店的生意大相径庭。 心理学家曾经运用一个随机转盘对人们进行测试。当转盘的指针停留在65%这个刻度时，要求被试回答：非洲国家的数量在联合国国家总数中，所占的百分比是大于65%还是小于65%?对于这个常识性的问题，大部分被试都会回答小于65%。实验者接着又问：“具体的比例是多少?”多数被试回答45%左右。 接下来，研究者又对一些从来没有参加这类测试活动的人进行了测试。当转盘的指针指向10%的时候，要求被试回答：非洲国家的数量在联合国国家总数中，所占的百分比是大于10%还是小于10%? 这也是一个常识性的问题，大部分被试会回答大于10%。实验者接着又问：“具体的比例是多少?”多数被试回答在25%左右。所有的被试都知道，转盘的数字是随机出现的，但是，他们的回答却明显地受到转盘先前给出的数字的影响——即使这些数字是无关的。也就是说，被试的答案被“锚定”在先前给出的无关数字上了。 外出旅游的时候，你看中了一件标价为3000元的紫砂壶，但你对紫砂壶的情况又不是很了解，结果你动用了所有的智慧与店主讨价还价，最终以 1500元成交，你感到很满意。因为，你花了1500元的价钱买了3000元的东西，而不是花了4000元的价钱买了3000元的东西。店主也很高兴，因为他把价值500元的东西随意标成了3000元，而店主的这个前置标价，对于你来说就是一种“锚定”，他让你始终围绕着这个似乎是随机的数字3000元来思维，这也就是为什么商家在一开始就标价很高的原因，因为，这种方法能够实现“双赢”：商家赚了钱，顾客“捡了便宜”。 这些现象也会出现在商业谈判之中，在信息不对称的情况下，你可能会“锚定”谈判对手，也可能会被对方所“锚定”。有一种十分简单的方法就可以打破这种思维锚定，那就是货比三家，充分地了解标的物的相关信息。 Kahneman和Tversky认为，无论是初出茅庐的新手，还是经验丰富的决策者，在面对复杂和模糊的问题时，经常会发生启发式认知偏差，只是偏差的几率、幅度大小不同而已。]]></content>
      <categories>
        <category>日知录</category>
      </categories>
      <tags>
        <tag>启发式认知偏差</tag>
        <tag>代表性启发法</tag>
        <tag>可得性启发法</tag>
        <tag>锚定与调整启发法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书记（1）：读书清单]]></title>
    <url>%2F2018%2F01%2F01%2F%E8%AF%BB%E4%B9%A6%E8%AE%B0%EF%BC%881%EF%BC%89%EF%BC%9A%E8%AF%BB%E4%B9%A6%E6%B8%85%E5%8D%95%2F</url>
    <content type="text"><![CDATA[这是我本科实习期间利用闲暇时间整理出来的一系列清单，包含涉及人文社科各个学科的经典书籍。 我有很多渴望，而这其中尤为让我感受到歌德所说的“静谧的激情”的是这样一种渴望。我渴望深入人类文明最澄明精要的思想脉络，渴望与人类历史上最迷人的灵魂相伴，渴望在漫长的岁月中被这些浪漫的精神“缠绕”，而我相信，它们留下的书，正是我们通向这种渴望的路途中最朴实的一条。这份清单正是始于这种渴望，我期许通过它们去谋合我对文字的控制欲以及统领知识的野心，形成人文社科的阅读图景与思想路径，依循这条规定好的路径，边走边想，争取到那半点光辉。 翁贝托·艾科在他的《无限的清单》里，带着极度的痴狂带领着我们发现隐藏在伟大的作品中的迷人的清单，将那些本来漫无秩序的事物赋予秩序。在第十七章《混乱的枚举》中提及到博尔赫斯式的作品清单时说：“他们之所以开清单，并不是因为他们技穷，不晓得要如何说他们想说的事情，他们以开清单的方式来说他们想说的话，是出于他们对过度的喜爱，是出于骄傲，以及对文字的贪婪，还有，对多元、无限的知识——快乐的知识——贪求。”正是这些伟大的人和他们伟大的作品，在引诱着我们去探求更为鲜活的真理、更为纯粹的思想，也正因为每个人身上都携带着对知识渴求的基因，才会让这些随着伟大的头脑的消逝便无法生效的纸质文字在每个时代熠熠生辉。 但之后的事情并没有按照规定好的方式前行，我时常感觉到我被这些“狂妄的清单”奴役了，甚至很长时间无法摆脱他们整齐划一的身影，它们赤裸裸地暴露了我对多元、无限和过度的探求欲望，他们就像是一位看似博学实则浅薄的教授在我指手画脚，指挥我、支使我、调度我的行动，让我失去了阅读的趣味，失去那种在无边无际的森林中肆意游荡然后发现原本看不见的风景的趣味，我就像是一只困兽，被这些并非观念、并非思想的索然无味的家伙给囚禁了。 我也曾有过很多类似的美学体验，当我一人徜徉于图书馆之中时，看到成千上万本归好类别的书层层叠叠的放置在书架上，尽管这是一个有限的空间，只能容纳下有限的纸张，但面对这些扑面而来的携带着无限知识模块的书籍，我感受到的是一种“不安的快感”，甚至是一种“绝望的快感”，一种即将窒息的无力感，就如同溺水之际一眼看不到海岸之时的垂死挣扎。于是索性就暂时“抛弃”了它们，用力挣脱它们的身影，离开它们，把它们闲置、一劳永逸地拖入垃圾箱，让他们荒芜，让他们丢失我最初给予它们的热忱与痴迷。 之后在没有清单梦魇困扰的的日子里，我逐渐醒悟，原来清单它只是一种有限的表达形式，它仅仅代表了一种无序生活中寻找各种秩序生活的可能性。而思想与精神无法像清单一样清晰地罗列排开，它们是无需安排的，它们有其内在的环路，我开始告诫自己，沿着思想走，而不是沿着书本走。 但如果这些清单最终的用途仅仅是满足我的私欲和揭露我狂妄的野心，那它的命运也过于浅薄灰暗了，我不该让它们被我自私的占有，它们的命运也不绝于此，它们本该有更多的触角去和更多的读者相遇，见到更多的光与热爱，与更多美妙的灵魂互动。于是便有了这一系列的整理。 下面是它们的目录及链接，简单介绍一下，我把他们归类成了以下几个篇章：哲学篇、文学篇、诗歌篇、社会学篇、政治学篇、心理学篇、人类学篇、经济学篇、法学篇，每一个类目包含了该领域下经典的作品，主要按照作者来进行细分，当然也包含了一些比较不错的系列丛书，每一个作者或者系列都链接到了百度网盘，里面就是那些伟大的作品了，基本上是PDF格式，当初选择PDF格式也是因为阅读的时候可以看到纸质书的原貌，在选择的时候也尽量挑选了相对比较清晰的版本，尽量提升阅读体验。 最后，我想引用歌德的一句话：“阅读是读者与作者间的一次合谋。书已经翻开，你已经边缘性地进入这场阴谋，除了主动乃至假装愉快地参与，似乎别无选择······” 如果您有任何问题，请联系我的邮箱&#49;&#56;&#x38;&#49;&#x30;&#54;&#57;&#56;&#57;&#50;&#x33;&#x40;&#x31;&#x36;&#x33;&#x2e;&#x63;&#111;&#109;。 清单的艺术：哲学篇 阿多诺 阿尔都塞 阿甘本 阿奎那 艾耶尔 奥古斯丁 巴迪欧 巴赫金 柏格森 柏拉图 查尔斯.泰勒 陈嘉映 德勒兹 德里达 詹姆士 张汝伦 邓安庆 邓晓芒 狄尔泰 笛卡尔 蒂利希 杜威 福柯 伽达默尔 哈贝马斯 海德格尔 何怀宏 何兆武 黑格尔 胡塞尔 怀特海 张世英 霍克海默 江怡 金观涛 卡西尔 康德 柯林伍德 克尔凯郭尔 克里希那穆提 莱布尼茨 赖尔 朗西埃 勒维纳斯 利科 刘小枫 卢克莱修 罗蒂 罗兰.巴特 罗素 洛克 马克思 梅洛.庞蒂 莫罗阿 尼采 张一兵 倪梁康 帕斯卡尔 培根 皮亚杰 齐泽克 萨特 塞涅卡 舍斯托夫 石里克 叔本华 斯宾诺莎 唐君毅 王元化 维科 维特根斯坦 俞吾金 西蒙娜·薇依 西塞罗 休谟 雅斯贝尔斯 亚当斯密 亚里士多德 杨祖陶 于连 美学 科学哲学 政治哲学 道德哲学 逻辑学 现象学 心灵哲学 分析哲学 二十世纪西方哲学译丛 国外经典哲学教材译丛 世纪人文系列丛书 西方名著入门 西方社会科学读本 人文译丛 人文与社会译丛 后现代交锋丛书 当代西方哲学家评传 德国哲学 辑刊 汉译哲学 哲学工具书系列 西方哲学史 通识系列 现代性研究译丛 余英时 孔子 老子 梁漱溟 孟子 牟宗三 王阳明 熊十力 朱熹 庄子 其他 清单的艺术：文学篇 D.H.劳伦斯 E.M.福斯特 J.M.库切 T·S·艾略特 V.S.奈保尔 阿·托尔斯泰 阿波利奈尔 阿多尼斯 阿赫玛托娃 阿克萨科夫 阿来 德波顿 阿摩司·奥兹 阿瑟·库斯勒 阿斯塔菲耶夫 耶利内克 埃利蒂斯 艾丽丝·默多克 艾丽斯·沃克 艾特玛托夫 爱伦堡 爱伦坡 爱默生 安.兰德 安德烈.纪德 安德烈耶夫 安东尼·伯吉斯 安妮·普鲁 安徒生 奥尔罕.帕慕克 巴别尔 巴尔扎克 巴塞尔姆 邦达列夫 保罗.策兰 保罗·科埃略 朱利安.巴恩斯 朱天文 张炜 张贤亮 贝克特 本·奥克瑞 本哈德·施林克 本雅明 彼得·阿克罗伊德 别尔嘉耶夫 波德莱尔 波德里亚 勃兰兑斯 勃留索夫 勃洛克 博尔赫斯 布尔加科夫 布莱希特 布朗肖 蔡骏 陈忠实 迟子建 川端康成 茨维塔耶娃 村上春树 达夫妮.杜穆里埃 达里奥·福 大江健三郎 但丁 狄更斯 东野圭吾 董桥 杜鲁门·卡波特 多丽丝莱辛 菲利普·罗斯 费定 马卡宁 迪伦马特 孚希特万格 福克纳 冈察尔 冈察洛夫 高尔基 高尔斯华绥 高行健 戈迪默 歌德 格非 格雷厄姆·格林 格列科娃 果戈理 哈珀·李 海明威 海涅 海因里希.伯尔 韩松 荷尔德林 赫尔岑 赫塔·米勒 黑塞 亨利.米勒 亨利希·曼 科塔萨尔 华兹华斯 惠特曼 霍达 霍夫曼 霍普特曼 济慈 加缪 加西亚.马尔克斯 贾平凹 蒋勋 杰弗里·尤金尼德斯 芥川龙之介 金庸 聚斯金德 君特.格拉斯 卡夫卡 富恩特斯 卡内蒂 卡萨诺瓦 卡赞扎基斯 科尔姆.托宾 科马克·麦卡锡 克莱斯特 克里斯塔·沃尔夫 冯内古特 库普林 拉·艾里森 拉斯普京 莱蒙特 莱蒙托夫 莱辛 兰波 劳伦斯·斯特恩 勒克莱齐奥 勒萨日 雷巴科夫 雷马克 雷蒙德卡佛 李欧梵 里尔克 理查·赖特 列夫.托尔斯泰 列斯科夫 林贤治 刘慈欣 鲁迅 路遥 伦茨 罗伯.格里耶 罗伯特.波拉尼奥 罗曼罗兰 洛扎诺夫 骆以军 马丁·瓦尔泽 马尔罗 马家辉 马拉默德 略萨 马雅可夫斯基 玛·金·罗琳斯 阿特伍德 杜拉斯 迈克尔·翁达杰 迈克尔·坎宁安 麦尔维尔 麦家 曼德尔施塔姆 毛姆 梅列日科夫斯基 梅特林克 蒙田 弥尔顿 米兰·昆德拉 米沃什 莫迪亚诺 莫里亚克 穆齐尔 那多 纳博科夫 纳丁·戈迪默 娜塔莉·萨洛特 涅克拉索夫 聂鲁达 诺曼·梅勒 帕斯捷尔纳克 帕特里克·怀特 帕乌斯托夫斯基 潘海天 培根 菲兹杰拉德 皮兰德娄 蒲宁 普里什文 普鲁斯特 普吕多姆 普希金 契诃夫 恰佩克 乔纳森·弗兰岑 乔伊斯·奥兹 丘特切夫 邱妙津 裘帕・拉希莉 让.雅克.卢梭 萨冈 萨特 塞弗尔特 塞林格 塞普尔维达 塞万提斯 三岛由纪夫 莎士比亚 舍斯托夫 沈从文 施尼茨勒 施托姆 张爱玲 石黑一雄 菲茨杰拉德 司汤达 斯蒂芬.茨威格 斯特林堡 苏珊.桑塔格 苏童 苏伟贞 梭罗 索尔·贝娄 索尔仁尼琴 太宰治 泰戈尔 唐·德里罗 唐诺 特兰斯特勒默 铁凝 屠格涅夫 托马斯.哈代 托马斯曼 托马斯·品钦 托妮·莫里森 陀思妥耶夫斯基 瓦·格罗斯曼 王安忆 王德威 王尔德 王晋康 王蒙 王小波 威廉·格纳齐诺 维·贡布罗维奇 维科 翁贝托.艾柯 伍尔夫 西奥多.德莱塞 西尔维娅・普拉斯 西格斯 张大春 张洁 希梅内斯 席勒 夏多布里昂 萧伯纳 萧红 新井一二三 雪莱 亚历克斯·哈里 亚马多 阎连科 叶赛宁 叶芝 伊迪丝·华顿 伊恩·麦克尤恩 伊凡·克里玛 伊夫林·沃 内米洛夫斯基 伊姆雷 卡达莱 卡尔维诺 伊沃.安德里奇 易卜生 尤瑟纳尔 约翰.厄普代克 约翰·肯尼迪·图尔 约翰·韦恩 约瑟夫.海勒 布罗茨基 扎米亚京 詹姆斯.乔伊斯 鲍德温 文化生活译丛 文学批评理论 清单的艺术：诗歌篇 阿多尼斯 阿赫玛托娃 艾略特 奥登 拜伦 波德莱尔 茨维塔耶娃 张枣 狄金森 谷川俊太郎 哈维尔 海涅 荷尔德林 惠特曼 济慈 亚非诗选 莱蒙托夫 兰波 里尔克 马雅可夫斯基 曼德尔施塔姆 聂鲁达 帕斯捷尔纳克 泰戈尔 托马斯.哈代 王尔德 希梅内斯 席勒 雪莱 叶赛宁 叶芝 域外诗丛 域外诗歌精品评析 20世纪世界诗歌译丛 西葡诗选 德语诗选 英诗选集 拉美诗选 俄罗斯诗选 诺贝尔获得者诗选 日本诗选 法国诗选 美国诗选 二十世纪外国大诗人丛书 十位外国诗人 外国文学名家诗篇 世界诗库 诗苑译林 外国诗歌丛书 清单的艺术：法学篇 波斯纳 德沃金 德肖维茨 哈特 贺卫方 卡尔·施米特 梁治平 刘星 苏力 其他 清单的艺术：心理学篇 荣格 阿德勒 陈鹤琴 弗洛姆 弗洛伊德 卡伦霍妮 拉康 马斯洛 20世纪西方现代心理学 其他 清单的艺术：社会学篇 鲍德里亚 鲍曼 彼得·伯格 布迪厄 迪尔凯姆 福柯 福山 格尔茨 哈耶克 吉登斯 雷蒙.阿隆 马克思.舍勒 马克思.韦伯 曼海姆 米德 西美尔 殷海光 郑杭生 亚历山大 科塞 其他 社会学家茶座 清单的艺术：政治学篇 德沃金 福山 国际政治 哈贝马斯 哈耶克 汉娜.阿伦特 亨廷顿 吉登斯 刘军宁 刘瑜 卢梭 罗尔斯 马克思.韦伯 秦晖 施特劳斯 汪晖 许纪霖 以赛亚.伯林 政治法律社会 周保松 刘擎 其他 清单的艺术：人类学篇 博厄斯 布迪厄 迪尔凯姆 格尔茨 马林诺夫斯基 马塞尔·莫斯 米德 普理查德 人物传记 萨林斯 斯特劳斯 特纳 王铭铭 庄孔韶 其他 清单的艺术：经济学篇 阿玛蒂亚.森 博弈论 布坎南 丹尼尔·卡尼曼 周其仁 哈耶克 加里.贝克尔 卡尔·门格尔 科斯 约瑟夫·阿罗 茅于轼 米尔顿.弗里德曼 米塞斯 庞巴维克 钱颖一 斯蒂格勒 斯蒂格利茨 托马斯·谢林 汪丁丁 韦森 行为经济学 张维迎 张五常 新制度经济学 当代世界十大经济学派丛书 其他]]></content>
      <categories>
        <category>读书记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[日知录（3）：心理账户]]></title>
    <url>%2F2017%2F12%2F12%2F%E6%97%A5%E7%9F%A5%E5%BD%95%EF%BC%883%EF%BC%89%EF%BC%9A%E5%BF%83%E7%90%86%E8%B4%A6%E6%88%B7%2F</url>
    <content type="text"><![CDATA[2002年10月9日，瑞典皇家科学院将诺贝尔经济学奖授予普林斯顿大学心理学教授DannielKahneman和乔治·梅森大学经济学教授VernonSmith。根据瑞典皇家科学院的新闻公报，卡尼曼“将心理学的深入分析融入到了经济学中，从而为一个崭新的经济学研究领域奠定了基础”。1981年，DannielKahneman及其合作者AmosTversky在《科学》杂志发表论文，研究人们决策过程的认知心理规律。文章介绍了“演出实验” 【实验情境A】：你打算去剧院看一场演出，票价是10美元，在你到达剧院的时候，发现自己丢了一张10美元的钞票。你是否会买票看演出？实验表明：88%的调查对象选择会；12%的调查对象选择不会。（调查对象为183人） 【实验情境B】：你打算去看一场演出而且花10美元钱买了一张票。在你到达剧院的时候，发现门票丢了。如果你想看演出，必须再花10美元，你是否会买票？实验结果表明：46%的调查对象选择会，54%的调查对象不会。（调查对象为200人） Kahneman和Tversky认为:两种实验情境出现明显不同结果的原因在于:在考虑情境A的决策结果时，人们把丢失的10美元钞票和买演出票的10美元分别考虑；而在情境B中，则把已经购买演出票的钱和后来买票的钱放在同一个账户估价，一部分人觉得“太贵了”改变自己的选择。为此，Kahneman和Tversky引入理查德·萨勒教授（RichardThaler）提出的“心理账户”概念，对实验结果进行了深入的分析。 一、概念发展1.1 理查德·萨勒1980年，芝加哥大学著名行为金融和行为经济学家理查德·萨勒（RichardThaler）首次提出“Psychic Accounting（心理账户）”概念，用于解释个体在消费决策时为什么会受到“沉没成本效应（sunk cost effert）”的影响。萨勒认为：人们在消费行为中之所以受到“沉没成本”的影响，一个可能的解释是卡尼曼教授等提出的“前景理论”，另一个可能的解释就是推测个体潜意识中存在的“心理账户系统”（Psychic Accounting system）。人们在消费决策时把过去的投入和现在的付出加在一起作为总成本，来衡量决策的后果。这种对金钱分门别类的分账管理和预算的心理过程就是“心理账户”的估价过程。 1.2 丹尼尔·卡尼曼1981年，丹尼尔·卡尼曼和特韦尔斯基(Amos Tversky)在对“演出实验”的分析中使用“Psychological Account(心理账户)”概念，表明消费者在决策时根据不同的决策任务形成相应的心理账户。卡尼曼认为，心理账户是人们在心理上对结果(尤其是经济结果)的分类记账、编码、估价和预算等过程。 1984年，卡尼曼教授和特韦尔斯基教授认为“心理账户”概念用“mental account”表达更贴切。卡尼曼认为:人们在做出选择时，实际上就是对多种选择结果进行估价的过程。究竟如何估价，最简单也最基本的估价方式就是把选择结果进行获益与损失(得失)的评价。因此，他提出了“值函数”假设和“决策权重”函数来解释人们内在的得失评价机制。 1.3 理查德·萨勒1985年，萨勒教授发表“心理账户与消费者行为选择”一文，正式提出“心理账户”理论，系统地分析了心理账户现象，以及心理账户如何导致个体违背最简单的经济规律。萨勒认为:小到个体、家庭，大到企业集团，都有或明确或潜在的心理账户系统。在作经济决策时，这种心理账户系统常常遵循一种与经济学的运算规律相矛盾的潜在心理运算规则，其心理记账方式与经济学和数学的运算方式都不相同。因此经常以非预期的方式影响着决策，使个体的决策违背最简单的理性经济法则。萨勒列举了4个典型现象阐明心理账户对传统经济规律的违背，并提出了心理账户的“非替代性”特征。 1.4 特韦尔斯基1996年Tversky提出，心理账户是一种认知幻觉，这种认知幻觉影响金融市场的投资者，使投资者们失去对价格的理性关注，从而产生非理性投资行为。Kivetz(1999)认为，心理账户是人们根据财富的来源不同进行编码和归类的心理过程，在这一编码和分类过程中“重要性-非重要性”是人们考虑的一个维度。有学者从行为的角度对“心理账户”进行定义，认为心理账户是个人或家庭用来管理、评估和记录经济活动的一套认知操作系统，这套认知操作系统导致一系列非理性的“心理账户”决策误区。 1.5 理查德·萨勒1999年，萨勒发表“mental accounting matters”一文，这是对近20年“心理账户”研究的一个总结。在文章中，萨勒认为:心理账户的三个部分最受关注，首先是对于决策结果的感知以及决策结果的制定及评价，心理账户系统提供了决策前后的损失——获益分析;第二个部分涉及特定账户的分类活动，资金根据来源和支出划分成不同的类别(住房、食物等)，消费有时要受制于明确或不明确的特定账户的预算;第三个部分涉及账户评估频率和选择框架，账户可以是以每天、每周或每年的频率进行权衡，时间限定可宽可窄。因此，“心理账户”是人们在心理上对结果(尤其是经济结果)的编码、分类和估价的过程，它揭示了人们在进行(资金)财富决策时的心理认知过程。 二、心理账户的非替代性（non-fungibility）按照传统的微观经济学理论，金钱不会被贴上标签，它具有替代性(fungibility)，事实上，越来越多的实证研究表明: 人们并不是把所有的财富放在一个整体账户进行管理，每一元钱与每一元钱可以很好的替换与转移。相反，人们根据财富来源与支出划分成不同性质的多个分账户，每个分账户有单独的预算和支配规则，金钱并不能容易地从一个账户转移到另一个账户。 萨勒将这种金钱不能很好转移，不能完全替换的特点称之为“非替代性”。萨勒教授在研究中发现金钱非替代性的一些表现: 【不同来源】：由不同来源的财富而设立的心理账户之间具有非替代性，例如意外之财和辛苦得来的钱不具替代性。一般来说，人们会把辛苦挣来的钱存起来不舍得花，而如果是一笔意外之财，可能很快就花掉。 【不同消费项目】：不同消费项目而设立的心理账户之间具有非替代性。我们来看一个案例:王先生非常中意商场的一件羊毛衫，价格为1250元，他觉得贵而舍不得买。月底的时候他妻子买下羊毛衫作为生日礼物送给他，他非常开心。尽管王先生的钱和他的妻子的钱是同一家庭的钱，为什么同样的钱以不同的理由开支心理感觉不同?研究表明:自己花费购买羊毛衫，属于生活必需开支，1250元太贵了;而作为生日礼物送给丈夫，属于情感开支。因此人们欣然接受昂贵的礼品却未必自己去买昂贵的物品。可见，为不同的消费项目设立的心理账户之间具有非替代性。 【不同存储方式】：不同存储方式导致心理账户的非替代性。萨勒教授举的一个实例。约翰先生一家存了15000美元准备买一栋理想的别墅，他们计划在5年以后购买，这笔钱放在商业账户上的利率是10%;可最近他们刚刚贷款11000美元买了一部新车，新车贷款3年的利率是15%，为什么他不用自己的15000美元存款买新车呢?通常，人们对已经有了预定开支项目的金钱，不愿意由于临时开支挪用这笔钱，对这个家庭来说，存起来买房的钱，已经放在了购房这一预定账户上，如果另外一项开支(买车)挪用了这笔钱，这笔钱就不存在了。从理性上说，家庭的总财富不变。但因为财富改变了存放的位置，固定账户和临时账户具有非替代性，人们的心理感觉不一样。 三、心理账户的运算规则在日常经济活动中，人们是如何操纵和管理心理账户，这些经济交易在人们心里是如何评估和被体验的呢?萨勒认为:人们在进行各个账户的心理运算时，实际上就是对各种选择的损失-获益进行估价，称之为“得与失的构架(the framing of gains and losses)”，人们在心理运算的过程中并不是追求理性认知上的效用最大化，而是追求情感上的满意最大化。 情感体验在人们的现实决策中起着重要的作用，他将这种运算称之为“享乐主义的加工”（hedonic editing） 3.1 值函数的假设为了更好地探讨心理账户的价值运算如何影响人们的经济决策行为，卡尼曼教授在“前景理论”中提出了“值函数”(value function)这一概念。与以往经济理论中的“效用函数”(utility function)相比，值函数有三个重要的特征。 值函数是人们在决策行为时对于某个参照点的相对得失的详细说明，人们的“得与失”是个相对概念而不是期望效用理论的绝对概念。人们对某一决策结果的主观判断是相对于某个自然参照点而言，而不是绝对的财富或经济。因此，参照点的变化会引起人们主观估价的变化，人们更关注的是围绕参照点引起的改变而不是绝对水平。 “得与失”都表现出敏感性递减的规律。值函数的曲线是一条近似“S”形的曲线，右上角的盈利曲线为下凹形(concave)，左下角的亏损曲线为上凸形(convex)(如图所示)。离参照点(坐标交叉的原点)愈近的差额人们愈加敏感，越是远离参照点的差额越不敏感。因此，不管是获得还是损失，人们感觉到10元到20元的差额似乎比1000元到1010元的差额更大，这反映了价值曲线的边际递减特征。 损失规避。卡尼曼教授认为:同等数量的损失比获益对人的影响更大，因此在决策的时候人们尽量回避损失，表现在价值函数曲线上，损失曲线的斜率比获益曲线的斜率更大(如图1所示)，用公式表示为V(X)&lt;-V(-X)。例如损失1000元钱所带来的痛苦比获得1000元奖金而带来的愉悦更强烈。因此，面临损失时，人们是风险偏好的;面临获得时，人们是风险规避的。 由于值函数的三个典型特征，对心理账户的运算规则至少有三个启示: 相同的决策结果表述为损失或者获益会改变人们的风险决策偏好; 设计不同的参照点会改变人们对决策结果的认知; 同样的价格差额在不同的原始价格下，影响作用是不同的。 3.2 得与失的编码规则根据上述值函数的特点，萨勒在关于心理账户的研究中，将值函数在得与失的不同组合结果中的偏好情况作了分析。 规则一：两笔盈利应分开 假如两笔收入X、Y均为正，分开价值为$V(X)+V(Y)$，整合值为$V(X+Y)$。因价值曲线在右上角为凸形，所以$V(X)+V(Y)&gt;V(X+Y)$，个体更偏好分开体验(如图)。假如想送朋友两件礼物——一套衣服和一个健身器，最好分两次送。每次送一件礼物所带来的心理体验比一次送两件礼物的心理体验高。 规则二：两笔损失应整合 两笔支出对个体而言是“损失”，因价值曲线在左下角为凹形，所以$V(-X)+V(-Y)&lt;V(-X-Y)$，个体更偏好整合价值。这一规律可以解释生活中的很多现象，比如开会收取会务费时，最好一次收齐并留有余地，若有额外开支一次次增收，虽然数量不多，会员仍会牢骚满腹。 规则三:大得小失应整合 两笔收入一正一负:X，-Y，且余额为正，即$X&gt;Y$，从价值曲线看应是$V(X)+V(-Y)&lt;V(X-Y)$，所以人们更偏好整合。这条规则给人们的启示是，如果你有一个大的好消息和一个小的坏消息，应该把这两个消息一起告诉别人。如此整合，坏消息带来的痛苦会被好消息带来的快乐所冲淡，负面效应也就小得多。 规则四:小得大失应具体分析 两笔收入一正一负:X，-Y，且余额为负，即X&lt;Y，此时应分两种情况: 其一，小得大失且悬殊很大，应分开估价。从图中看出$V(X)+V(-Y)&gt;V(X-Y)$，因此，分开估价的心理体验要好，这种现象称为“银衬里(silver lining)”规则。例如(40，-6000)，人们更愿意分开估价，因为价值曲线在-6000元附近相对较平缓，40元的获得与6000元的损失相比几乎没有减少损失的作用，分开估价还能得到40元收益的感觉。 其二，小得大失且悬殊不大，应整合。如(40，-50)，人们更偏好整合价值。表现为$V(X-Y)&gt;V(X)+V(-Y)$。整合估价时，人们在心理会把损失从50元降低到10元，这样的损失就显得小了，心理体验更好，整合估价的作用体现出来。 萨勒进一步把这四条规则概括为: 分离收益; 整合损失; 把小损失与大收益整合一起; 把小收益从大损失中分离出来。 以上心理账户的运算规则对于理解和解释现实经济决策行为有重要的指导意义。 四、应用研究4.1 价格感知——绝对值优惠与相对值优惠1982年，特维尔斯基教授和卡尼曼教授通过设计以下情景实验引入“心理账户”与消费者购买决策行为的研究。 【实验情景A】:假定你要买一件夹克和一个计算器。在某商场夹克的价格是125美元，计算器的价格是15美元。这时候有人告诉你，开车二十分钟后另一个街区的一家商场计算器的价格是10美元。请问:你会去另一个商场买计算器吗? 【实验情景B】:假定你要买一件夹克和一个计算器。在某商场夹克的价格是15美元，计算器的价格是125美元。这时候有人告诉你，开车二十分钟后另一个街区的一家商场计算器的价格是120美元。请问:你会去另一个商场买计算器吗? 在这两个情境中，其实都是对“是否开车20分钟从140美元的总购物款中节省5美元”做出选择。然而，实验对象在两个情境中的回答却不一样。在情境A中，68%的实验对象选择去另一家商场;而在情境B中，只有29%的实验对象选择开车去另一家商场。选择偏好发生了逆转。 卡尼曼提出，消费者在感知价格的时候，是从三个不同的心理账户进行得失评价的。一个是最小账户(minimal account)，就是不同方案所优惠的绝对值。在本实验中的最小账户就是5美元。另一个是局部账户(topical account)，也可称为相对值账户。例如，在实验情境A中开车前往另一家店的“局部账户”表现为计算器价格从15美元降为10美元(相对差额为1/3);而在实验情境B中的“局部账户”表现为计算器价格从125美元降为120美元(相对差额为1/25)。第三个是综合账户(comprehensive account)，综合账户就是总消费账户，该实验的综合账户为140美元。 卡尼曼认为，在上面的实验中，消费者是自发运用了局部账户，即通过相对优惠值来感知价格。情境A有33.3%的优惠;而情境B仅有4%的优惠。因此，人们的购买行为发生了反转。表现为在实验情境A中，68%的实验对象选择去另一家商场;而在实验情境B中，却只有29%。 此后，Philip Moon,Kevin Keasey,Darren Duxbury对卡尼曼的研究进行了重复实验并且提出，当优惠超过某个阈限值的时候，消费者对绝对优惠值同样非常敏感。绝对值优惠与相对值优惠之间存在一种关系。 4.2 行为生命周期理论——心理账户在消费领域的应用经典的生命周期假说和持久收入假说是凯恩斯以后消费函数理论最重要的发展，但他们的理论是建立在完全理性人的假设之上的。例如，生命周期假说就认为:人总是能够深谋远虑，在任何时候都会考虑几十年以后的长远利益，并站在这种高度，根据一生的总财富来合理安排一生中每个阶段的消费，使一生的总效用达到最大。这显然和人们实际的消费行为不符，这种过于理性化的理论也无法解释现实中的许多经济现象。 1988年Shefrin和Thaler提出行为生命周期理论(behavior life cycle hypothesis)修正了传统的生命周期假说，使之能更好地描述现实中人们的消费行为。行为生命周期理论的两个最重要的概念是自我控制和心理账户。 行为生命周期理论引入“心理账户”理论解释消费行为。消费者根据生命周期不同财富的来源和形式，将它们划分为三个心理账户:现期可花费的现金收入账户(I)，现期资产账户(A)和未来收入账户(F)。行为生命周期理论认为:不同账户的财富对消费者的决策行为是不同的。现金收入账户消费的诱惑力最大，因此，将这个账户的收入不消费而储蓄起来的心理成本也最大;现期资产账户的诱惑力和储蓄的心理成本居中;未来收入账户的诱惑力和储蓄的心理成本最小。由于不同的心理账户对消费者的诱惑不同，所以，消费者倾向于较多地通过现金收入账户消费，而较少通过现期资产账户消费，几乎不通过未来收入账户消费。不仅不同的心理账户对消费者的诱惑是不同的，而且同一个心理账户，其中的财富余额不同，对消费者的诱惑也不同。财富余额越多，诱惑越大。 行为生命周期理论的消费函数可表示为$C=f(I,A,F)$，且有:$1≈C/I&gt;C/A&gt;C/F≈0$。这就是说，现金收入账户的边际消费倾向最大，接近于1;现期资产账户次之;未来收入账户最小，接近0。和生命周期持久收入假说的消费函数相比，行为生命周期理论在分析消费者行为时强调的是心理方面的因素，这些心理因素主要是通过心理账户加以描述。所以，心理账户的划分及其性质是理解行为生命周期理论的关键。 4.3 关于消费预算的研究1994年Heath和Soll发现，消费者有为不同的消费支出账户设置心理预算的倾向，并且严格控制该项目支出不超过合适的预算。例如，每个月的娱乐支出300元，每个月的日常餐饮消费1000元等。如果一段时间购买同一支出项目的总消费额超过了预算，人们会停止购买该类产品。即使在同一个消费项目中，不同的消费有不同的预算标准，同是娱乐消费，看电影的消费是200元人民币，买一本武打小说的消费是50元人民币。他们通过实验证明:人们当前在某一类项目的消费支出会减少他们未来在同一类项目的支出，而对其他项目的支出几乎没有什么影响。这是心理账户对每个消费项目会设定一个预算控制。 1996年，Chip和Soll研究认为，心理账户通过心理预算调节人们的消费行为。表现在:人们会为不同的消费设置预算，但预算通常会低估或者高估购买特定商品的价格，因此常使人们产生“穷鬼”和“大富翁”的认知错觉，从而出现消费不足和过度消费的消费误区。他们通过三个实验证明了心理账户的分类预算对消费决策的重要作用。 2006年，EldarShafir和RichardThaler发表Investnow,drinklater,spendnever一文，研究表明:在购买和消费暂时分离的商品交易中，人们会建构多种框架的心理账户。奢侈品的购买更多的被认为是一种“投资”而不是一种消费，因此，当消费很早以前购买的高档产品时，通常被编码为“免费”的或者是储蓄。但如果消费方式不是按原意愿进行时，对该产品的消费预算就会发挥作用。 4.4 行为资产组合理论（BPT）——心理账户在金融投资领域的应用心理账户在金融投资决策领域最广泛的应用是投资组合结构的运用。根据理性投资组合理论，投资者应该只关心他们投资组合的期望收益，而不应该关注某个特定投资部分的收益。可事实相反，投资者倾向于把他们的资金分成安全账户(保障他们的财富水平)和风险账户(试图作风险投机的买卖)。 1997 年 Fisher 和 Statman 提出:人们在投资时会把 资金分别放在不同的投资账户中，即使是基金公司 也建议投资者建立一个资产投资的金字塔，把现金放在金字塔的最低层，把基金放在中间层，把股票放在金字塔的最高层。2000年，Shefrin和Statman提出了行为资产组合理论（Behavioral portfolio theory，BPT-MA）下图就是一个典型的分层金字塔结构，从底端到顶端是按照其风险程度由低到高排列的，从右到左是按其收入价值由低到高 的顺序排列[14]。模型中的每层是根据安全性、潜力 性和期望值这三者相关的投资需求设计的。底层是 为投资者提供安全性而设计的证券，包括货币市场 基金和银行存款保证，上一层是债券，再上一层是 股票和房地产。 在行为金融理论中，行为投资组合理论是建立 在卡尼曼和特维尔斯基的前景理论之上的一个框架 体系。它认为投资者的资产结构应该是金字塔式的 分层结构(这里的层就是心理账户)，投资者对其 资产分层进行管理，每一层对应投资者的一个目标。 底层是投资者为避免贫穷而设立的，所以，其投资 对象通常是短期国债、大额可转让存单、货币市场 基金等有稳定收益、风险小的证券;高层是为使其 富有而设立的，其投资对象通常是外国股票、成长 性股票、彩票等高风险、高收益证券。Shefrin 和 Statman 设计了投资者只有一个心理账户和两个心 理账户的行为资产组合模型，并给出了模型的最优 解。当投资者有两个心理账户时，他们分别在低期 望水平和高期望水平两个心理账户建立投资模型， 并在两个账户之间分配资金。 此外，巴比雷斯和黄明(Barberis and Ming Huang)于 2001 年发表了题为“心理账户、损失规 避与个股回报”的论文，提出了一个较为完整的、 具体的刻画投资者心态的投资模型。并研究了在 两种心理账户下公司股票的均衡回报:一种是投资 者只对所持有的个股价格波动损失规避;另一种是 投资者对所持有的证券组合价格波动损失规避。该 模型在结合心理学、信息学和社会学研究成果的基 础上，对投资者与外部信息之间的互动关系做了崭 新的诠释，对投资者的心态及其决策过程做了具体 的刻画。为人们对投资决策的研究和资产定价的研 究提供了新的思路。]]></content>
      <categories>
        <category>日知录</category>
      </categories>
      <tags>
        <tag>卡尼曼</tag>
        <tag>行为经济学</tag>
        <tag>心理账户</tag>
        <tag>理查德·萨勒</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日知录（2）：前景理论]]></title>
    <url>%2F2017%2F12%2F10%2F%E6%97%A5%E7%9F%A5%E5%BD%95%EF%BC%882%EF%BC%89%EF%BC%9A%E5%89%8D%E6%99%AF%E7%90%86%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[所谓决策，就是在几个方案中选择一个方案，分为风险决策和非风险决策。在非风险决策中，各个方案的结果都是确定的。在风险决策中，有的方案有的结果是不确定的，即可能发生，也可能不发生。我们这里谈及的就是风险决策。 一般来说，风险决策研究有两个途径： 一个是规范性途径，其基本问题是：人类的风险决策应该遵循怎样的规则？ 一个是描述性途径，其基本问题是：人类的风险决策实际遵循怎样的规则？ 规范性途径往往被叫做决策逻辑学，描述性途径往往被叫做决策行为学或决策心理学。我们这里关注的是描述性途径。 关于风险决策的最早理论是期望效用理论，它最初只是规范性理论，但经济学家们逐渐把它当做描述性理论来使用——把它作为经济学中对人的决策行为的基本假定，即认为人的实际决策行为遵循期望效应理论，直到上世纪70年代处有的著名经济学家（如Arrow）仍然采取这种做法。 自1979年以来，Kahneman &amp; Tversky的一系列著作，对期望效用理论作为描述性理论的有效性提出了严峻的挑战，并提出前景理论作为合适的描述理论。由于前景理论能够精确解释、预言许多风险决策行为，它的影响越来越大；后来Kahneman把它应用于经济学领域，对经济学产生了深远的影响，为此，Kahneman于2002年获得了诺贝尔经济学奖。 这里首先介绍期望效用理论；然后介绍Kahneman等以实验事实对期望效用理论作为描述性理论的批评；最后介绍前景理论的主要内容。 一、期望效用理论期望效用函数理论是20世纪50年代，冯·纽曼和摩根斯坦（von Neumann and Morgenstem）在公理化假设的基础上，运用逻辑和数学工具，建立了不确定条件下对理性人（rational actor）选择进行分析的框架。不过，该理论是将个体与群体合而为一的。后来，阿罗和德布鲁（Arrow and Debreu）将其吸收进瓦尔拉斯均衡的框架中，成为处理不确定决策问题的分析范式，进而构建起现代微观经济学并由此展开的包括宏观、金融、计量等在内的宏伟而又优美的理论大厦。 期望效用理论本来是作为规范性理论提出的，但后来在许多经济学著作中被应用为描述性理论，直到上个世纪70年代初仍然如此。 经济学上所使用的期望效用理论包括三方面的内容：Bayes框架；Savage公理；Bernoulli原则。它们分别由不同的人在不同的时期创建。以下分述之。 1.1 Bayes框架早在1662年，Antoine Arnauld就写道：决定一个人必须做什么以获得好处或避免坏处，不仅必须考虑好处和坏处本身，而且必须考虑它发生或不发生的概率。 大约在100年之后出版的Bayes的遗著（1763）年把这个思想系统化、精确化，形成了所谓的风险决策的Bayes框架，其核心思想可以概括为两点： $ED(A_i)=\sum _jP_{ij}·D_{ij}$：即行动$A_i$的估计渴望度（estimated desirability，简称ED；后来改称期望效用，expected utility，EU）等于它的各个可能结果的渴望度$D_{ij}$乘以该可能结果出现的概率所得的积的和；或者说，行动$A_i$的估计渴望度等于它的各个结果的渴望度的加权和，权重为各个结果的概率。（注意，前提是各个可能结果互不相容。） 根据贝叶斯原则进行选择：选择有着最大估计渴望度的一个方案。 请注意到Bayes决策框架所包含的两点假设： 决策权重=概率本身 渴望度（效用）不依赖于参考点。Bayes决策框架并没有要求以一个参考点来衡量渴望度（效用）；实际上，在经济学中人们往往以财富的最终状态来计算效用。 1.2 Savage公理Bayes决策框架一直沿用下来，并有进一步的发展。 Von Neumann &amp; Morgenstern（1944）发展出关于偏好（选择）的公理系统；而Ramsey（1931）和Savage（1954）继续发展了该公理系统，用主观概率代替客观概率，从而使概率理论和决策理论可以适用于更广泛的事情。Savage公理系统是这些公理系统中最为成熟的。其中的公理有： 不变性公理（Invariance Axiom）：方案间的偏好顺序不依赖于方案的描述方式。 优势性公理（Dominance Axiom）：如果方案A在每个方面至少跟方案B一样好，而在至少一个方面比B更好，那么A应该比B更可取。 如果方案B由于方案A，那么它们与任一概率$p≠0$的结合所得的$(B,p)$一定优于$(A,p)$ 这些公理与直觉十分一致；也与Bayes决策框架完全一致。 1.3 Bernoulli原则很早以前Bernoulli就指出人们通常是风险回避的。一个决策是风险回避的，是指：按照结果的表面值（如金额）计算，在确定的结果与有着相等或更高的期望值的不确定的结果之间，决策者选择了确定的结果，如，A. 确定得到80元；B.81%的可能得到100元，按结果的表面值计算，B的期望值高于A，按理应该选择B，但事实上大多数人们会选择A。 为了解释这个现象，Bernoulli提出：人们评价方案，不是用方案的金钱结果值，而是用这些金钱结果值的主观价值，而这个主观价值对于金钱值的函数曲线（效用曲线）是一条凹形的曲线，即$u’’(x)&lt;0$。通俗地说，随着x的不断增大，u的增长越来越慢。用这个原理能轻易解释上段提及的现象。假设80元的主观价值是72，由此可以推出100元的主观价值应该小于90（因为前面80元中每20元的主观价值是18，根据u增长越来越慢的原理，100元超出80元的那20元的主观价值应该小于18，因而100元的主观价值小于90），假设是85.于是，A方案的期望值是72，而B的期望值是$85*81%=68.85$。所以大多数人们选择了A方案。 于是，风险回避和$u’’(x)&lt;0$也成了一些经济学著作对人的决策行为的假定之一。 1.4 小结与问题经济学理论中常常把期望效用理论的上述Bayes框架，Savage公理和Bernoulli原则中的全部或部分作为对人的风险决策行为的基本假定，在此假定和其他假定的基础上构建经济学理论。 这种做法有明显的方法论问题：这些经验假定（经验命题）的真假并没有经过经验方法的系统判明。逻辑学家们提出Bayes框架，Savage公理的初衷是：高斯人们应该怎样决策，或者说怎样决策才是合乎理性的；他们并未考察人们的决策行为是否恰好符合这些规则。而Bernoulli原则知识根据某类风险决策事实归纳得出；并未建立在对各种类型的风险决策事实的全面考察上。人们的实际决策行为是否遵循上述规则，显然是个经验命题；而一个经验命题只有通过经验的方法才能判明它的真伪。而经济学家们，未经经验方法的判定，就把上述规则当做经验命题来使用，这种做法就存在方法论上的问题。 这种方法论上的问题使得采用期望效用理论作为描述性理论可能是错误的。而Kahneman &amp; Tversky的一系列实验表明：采用期望效用理论作为描述性理论确实是错误的。 二、对期望效用理论的实验挑战Kahneman &amp; Tversky的一系列实验对期望效用理论作为描述理论的有效性提出了挑战。下面是他们的部分实验。 2.1 实验一 期望效用理论的“决策权重=概率”成立吗？ 问题一：请选择： A. 有80%的可能性得到4000元； 【20%】 B. 确定地得到3000元。【80%】 方括号指的是该项被试的百分比，以下皆同。 问题二： 请选择： C. 有20%的可能性得到4000元；【65%】 D. 有25%的可能性得到3000元。【35%】 分析： 如前1.1 所述，期望效用理论中，决策权重=概率。假设这点成立，那么有： U(A)=0.80*u(4000);U(B)=u(3000)U(C)=0.2*u(4000)U(D)=0.25*u(3000)注意到$U(A),U(B)$分别乘以0.25就相应得到$U(C),U(D)$，所以有：如果$U(A)]]></content>
      <categories>
        <category>日知录</category>
      </categories>
      <tags>
        <tag>前景理论</tag>
        <tag>卡尼曼</tag>
        <tag>行为经济学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理系列（1）：词向量和语言模型]]></title>
    <url>%2F2017%2F12%2F10%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9A%E8%AF%8D%E5%90%91%E9%87%8F%E5%92%8C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[转载自LICSTAR的博客 这篇博客是我看了半年的论文后，自己对Deep Learnimng在NLP领域中应用的理解和总结，在此分享。其中必然有局限性，欢迎各种交流，随便拍。 Deep Learning算法已经在图像和音频领域取得了惊人的成果，但是在NLP领域中尚未见到如此激动人心的结果。关于这个原因，引一条我比较赞同的微博。 @王威廉：Steve Renals算了以下icassp录取文章题目中包含deep learning 的数量，发现有44偏，而naacl则有0篇。有一种说法是，语言（词、句子、篇章等）属于人类认知过程中产生的高层认知抽象实体，而语言和图像属于较为底层的原始输入信号，所以后两者更适合做deep learning来学习特征。 第一句就先不用管了，毕竟今年的 ACL 已经被灌了好多 Deep Learning 的论文了。第二句我很认同，不过我也有信心以后一定有人能挖掘出语言这种高层次抽象中的本质。不论最后这种方法是不是 Deep Learning，就目前而言，Deep Learning 在 NLP 领域中的研究已经将高深莫测的人类语言撕开了一层神秘的面纱。 我觉得其中最有趣也是最基本的，就是“词向量”了。 将词用“词向量”的方式表示可谓是将 Deep Learning 算法引入 NLP 领域的一个核心技术。大多数宣称用了 Deep Learning 的论文，其中往往也用了词向量。 一、词向量是什么？自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。 NLP中最直观，也是到目前为止最常用的词表示方法是One-Hot Representation，这种方法把没歌词表示为一个很长的向量，这个向量的维度是词表大小，其中绝大多数元素为0，只有一个维度的值为1，这个维度就代表了当前的词。 举个例子： “话筒”表示为[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0····]“麦克”表示为[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0·····]每个词都是茫茫0海中的一个1。 这种One-Hot Representation如果采用稀疏方式存储，会是非常的简介：也就是给每个词分配一个数字ID。比如刚才的例子中，话筒记为3，麦克记为8（假设从0开始记）。如果要编程实现的话，用Hash表给每个词分配一个编号就可以了。这么简洁的表示方式配合上最大熵、SVM、CRF等等算法已经很好地完成了NLP领域的各种主流任务。 当然这种表示方法也存在一个重要的问题就是“词汇鸿沟”现象：任意两个词之间都是孤立的。光从这俩个向量中看不出两个词是否有关系，哪怕是话筒和麦克这样的同义词也不能幸免于难。 Deep Learning中一般用到的词向量并不是刚才提到的用One-Hot Representation表示的那种很长很长的词向量，而是用Distrubuted Representation（不知道这个该怎么翻译，因为还存在一种叫“Distributional Representation”的表示方法，又是另一个不同的概念）表示的一种低维实数向量、这种向量一般长这个样子：[0.792,-0.177,-0.107,0.109,-0.542,…]。维度以50维和100维比较常见。这种向量的表示不是唯一的，后文会提到目前计算这种向量的主流方法。 （个人认为）DIstributed representation最大的贡献是让相关或者相似的词，在距离上更接近了。向量的距离可以用最传统的欧氏距离来衡量，也可以用cos夹角来衡量。用这种方式表示的向量，“麦克”和“话筒”的距离会远远小于“麦克”和“天气”。可能理想情况下“麦克”和“话筒”的表示应该是完全一样的，但是由于有些人会把英文名“迈克”也写成“麦克”，导致“麦克”一词带上了一些人名和语义，因此不会和“话筒”完全一致。 二、词向量的来历Distributed representation最早是Hinton在1986年的论文《Learning distributed representation of concepts》中提出的。虽然这篇文章没有说要将词做Distributed representation，（甚至我很无厘头地猜想那篇文章是为了给他刚提出的BP网络打广告）但至少这种先进的思想在那个时候就在人们的心中埋下了火种，到2000年之后开始逐渐被人重视。 Distributed representation用来表示词，通常被称为“Word Representation”或“Word Embedding”，中文俗称“词向量”。这的只能叫俗称，算不上翻译。半年前我本想翻译的，但是硬是想不出Embedding应该怎么翻译的，后来就这么叫习惯了，如果有好的翻译欢迎提出。（更新：@南大周志华给出了一个合适的翻译：词嵌入）Embedding一次的意义可以参考维基百科的相应页面。后文提到的所有“词向量”都是指用Distributed Representation表示的词向量。 如果用传统的稀疏表示法表示词，在解决某些任务的时候（比如构建语言模型）会造成维数灾难[Bengio 2003]。使用低维的词向量就没这样的问题。同时从实践上看，高维的特征如果要套用Deep Learning，其复杂度几乎是难以接受的，因此低维的词向量在这里也饱受追捧。 同时如上一节提到的，相似词的词向量距离相近，这就让基于词向量设计的一些模型自带平滑功能，让模型看起来非常的漂亮。 三、词向量的训练要介绍词向量是怎么训练的到的，就不得不提到语言模型。到目前为止我了解到的所有训练方法都是在训练语言模型的同时，顺便得到词向量的。 这也比较容易理解，要从一段无标注的自然文本中学习出一些东西，无非就是统计出词频、词的共现、词的搭配之类的信息。而要从自然文本中统计并建立一个语言模型，无疑是要求最为精确的一个任务（也不排除以后有人创造出更好更有用的方法）。既然构建语言模型这一任务要求这么高，其中必然也需要对语言进行更精细的统计和分析，同时也会需要更好的模型，更大的数据来支撑。目前最好的词向量都来自于此，也就不难理解了。 这里介绍的工作均为从大量未标注的普通文本数据中无监督地学习出词向量（语言模型本来就是基于这个想法而来的），可以猜测，如果用上了有标注的语料，训练词向量的方法肯定会更多。不过视目前的语料规模，还是使用未标注的方法靠谱一些。 词向量的训练最经典的有三个工作，C&amp;W 2008、M&amp;H 2008、Mikolov 2010。当然在说这些工作之前，不得不介绍一下这一系列中Bengio的经典之作。 3.1 语言模型简介语言模型其实就是看一句话是不是正常人说出来的。这玩意儿很有用，比如机器翻译、语音识别得到若干候选之后，可以利用语言模型挑一个尽量靠谱的结果。在NLP的其他任务里也都能用到。 语言模型形式化的描述就是给定一个字符串，看它是自然语言的概率$P(w_1,w_2,···,w_t)$。$w_1$到$w_t$依次表示这句话中的各个词。有个很简单的推论是$P(w_1,w_2,…,w_t)=P(w_1)✖️P(w_2|w_1)✖️P(w_3|w_1,w_2)✖️…✖️P(w_t|w_1,w_2,…w_{t-1})$ 常用的语言模型都是在近似地求$P(w_t|w_1,w_2,···,w_{t-1})$，比如$n-gram$模型就是用$P(w_t|w_{t-n+1},…,w_{t-1})$近似表示前者。 顺便提一句，由于后面要接受煎熬的每篇论文使用的符号差异太大，本博文尝试统一使用Bengio 2003的符号系统（略作简化），以便在各方法之间做对比和分析。 3.2 Bengio的经典之作用神经网络训练语言模型的思想最早由百度IDL的徐伟于2000提出。其论文《Can Artificial Neural Networks Learn Language Model？》提出一种用神经网络构建二元语言模型（即$P(w_t|w_{t-1})$）的方法。文中的基本思路和后续的语言模型的差别已经不大了。 训练语言模型的最经典之作，要数Bengio等人在2001年发表在NIPS上的文章《A Neural Probabilistic Language Model》。当然现在要看的话，肯定是要看他在2003年投到JMLR上的同名论文了。 Bengio用了一个三层的神经网络来构建语言模型，同样也是n-gram模型。 图中最下方的$w_{t-n+1}，…，w_{t-2},w_{t-1}$就是前$n-1$个词。现在需要根据这已知的$n-1$个词预测下一个词$w_t$。$C(w)$表示词$w$所对应的词向量，整个模型中使用的是一套唯一的词向量，存在矩阵$C$(一个$|V|×m$的矩阵)中。其中$|V|$表示词表的大小（语料中的总词数），m表示词向量的维度。$w$到$C(w)$的转化就是从矩阵中取出一行。 网络的第一层（输入层）是将$C(w_{t-n+1})，…，C(w_{t-2}),C(w_{t-1})$这n-1个向量首尾相拼接起来，形成一个$(n-1)m$维的向量，下面记为x。 网络的第二层（隐藏层）就如同普通的神经网络，直接使用$d+Hx$计算得到。d是一个偏置项。在此之后，使用tanh作为激活函数。 网络的第三层（输出层）一共$|V|$个节点，每个节点$y_i$表示下一个词为$i$的未归一化log概率。最后使用softmax激活函数将输出值y归一化成概率。最终，y的计算公式为： y=b+Wx+Utanh(d+Hx)式子中的$U$(一个$V×h$的矩阵)是隐藏层到输出层的参数，整个模型的多数计算集中在U和隐藏层的矩阵乘法中。后文的提到的3个工作，都有对这一环节的简化，提升计算的速度。 式子中还有一个矩阵$W(|V|×(n-1)m)$,这个矩阵包含了从输入层到输出层的直连边。直连边就是从输入层直接到输出层的一个线性变换，好像也是神经网络中的一种常用技巧。如果不需要直连边的话，将W置为0就可以了。在最后的实验中，Bengio发现直连边虽然不能提升模型效果，但是可以少一半的迭代次数。同时他也猜想如果没有直连边，可能可以生成更好的词向量。 现在万事俱备，用随机梯度下降法把这个模型优化出来就可以了。需要注意的是，一般神经网络的输入层只是一个输入值，而在这里，输入层x也是参数（存在C中），也是需要优化的。优化结束之后，词向量有了，语言模型也有了。 这样得到的语言模型自带平滑，无需传统n-gram模型中那些复杂的平滑算法。Bengio在APNews数据集上做的对比试验也表明他的模型效果比精心设计平滑算法的普通n-gram算法要好10%到20%。 在结束介绍 Bengio 大牛的经典作品之前再插一段八卦。在其 JMLR 论文中的未来工作一段，他提了一个能量函数，把输入向量和输出向量统一考虑，并以最小化能量函数为目标进行优化。后来 M&amp;H 工作就是以此为基础展开的。 他提到一词多义有待解决，9 年之后 Huang 提出了一种解决方案。他还在论文中随口（不是在 Future Work 中写的）提到：可以使用一些方法降低参数个数，比如用循环神经网络。后来 Mikolov 就顺着这个方向发表了一大堆论文，直到博士毕业。 大牛就是大牛。 3.3 C&amp;W的SENNARonan Collobert 和 Jason Weston 在 2008 年的 ICML 上发表的《A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning》里面首次介绍了他们提出的词向量的计算方法。和上一篇牛文类似，如果现在要看的话，应该去看他们在 2011 年投到 JMLR 上的论文《Natural Language Processing (Almost) from Scratch》。文中总结了他们的多项工作，非常有系统性。这篇 JMLR 的论文题目也很霸气啊：从头开始搞 NLP。 他们还把论文所写的系统开源了，叫做 SENNA（主页链接），3500 多行纯 C 代码也是写得非常清晰。我就是靠着这份代码才慢慢看懂这篇论文的。可惜的是，代码只有测试部分，没有训练部分。 实际上 C&amp;W 这篇论文主要目的并不是在于生成一份好的词向量，甚至不想训练语言模型，而是要用这份词向量去完成 NLP 里面的各种任务，比如词性标注、命名实体识别、短语识别、语义角色标注等等。 由于目的的不同，C&amp;W 的词向量训练方法在我看来也是最特别的。他们没有去近似地求 $P(w_t|w_1,w_2,…,w_{t−1})$，而是直接去尝试近似 $P(w_1,w_2,…,w_t)$。在实际操作中，他们并没有去求一个字符串的概率，而是求窗口连续 n 个词的打分 $f(w_{t−n+1},…,w_{t−1},w_t)$。打分 f 越高的说明这句话越是正常的话；打分低的说明这句话不是太合理；如果是随机把几个词堆积在一起，那肯定是负分（差评）。打分只有相对高低之分，并没有概率的特性。 有了这个对 f 的假设，C&amp;W 就直接使用 pair-wise 的方法训练词向量。具体的来说，就是最小化下面的目标函数。 ∑_{x∈𝔛}∑_{w∈𝔇}max \{0,1−f(x)+f(x^{(w)})\}$𝔛$为训练集中的所有连续的n元短语，$𝔇$是整个字典。第一个求和枚举了训练语料中的所有的n元短语，作为正样本。第二个对字典的枚举是构建负样本。$x^{(w)}$是将短语x的最中间的那个词，替换成w。在大多数情况下，在一个正常短语的基础上随便找个词替换掉中间的词，最后得到的短语肯定不是正确的短语，所以这样构造的负样本是非常可用的（多数情况下确实是负样本，极少数情况下把正常短语当做负样本也不影响大局）。同时，由于负样本仅仅是修改了正样本中的一个词，也不会让分类面距离负样本太远而影响分类。再回顾这个式子，x是正样本，$x^{(w)}$是负样本，$f(x)$是对正样本的打分，$f(x^{(w)})$是对负样本的打分。最后希望正样本的打分要比负样本的打分至少高一分。 $f$函数的结构和Bengio 2003中提到的网络结构基本一致。同样是把窗口中的 nn 个词对应的词向量串成一个长的向量，同样是经过一层网络（乘一个矩阵）得到隐藏层。不同之处在于 C&amp;W 的输出层只有一个节点，表示得分，而不像 Bengio 那样的有 $|V|$个节点。这么做可以大大降低计算复杂度，当然有这种简化还是因为 C&amp;W 并不想做一个真正的语言模型，只是借用语言模型的思想辅助他完成 NLP 的其它任务。（其实 C&amp;W 的方法与 Bengio 的方法还有一个区别，他们为了程序的效率用 HardTanh代替 tanh 激活函数。） 他们在实验中取窗口大小 n=11，字典大小 |V|=130000，在维基百科英文语料和路透社语料中一共训练了 7 周，终于得到了这份伟大的词向量。 如前面所说 C&amp;W 训练词向量的动机与其他人不同，因此他公布的词向量与其它词向量相比主要有两个区别： 1.他的词表中只有小写单词。也就是说他把大写开头的单词和小写单词当作同一个词处理。其它的词向量都是把他们当作不同的词处理的。 2.他公布的词向量并不直接是上述公式的优化结果，而是在此基础上进一步跑了词性标注、命名实体识别等等一系列任务的 Multi-Task Learning 之后，二次优化得到的。也可以理解为是半监督学习得到的，而非其他方法中纯无监督学习得到的。 不过好在 Turian 在 2010 年对 C&amp;W 和 M&amp;H 向量做对比时，重新训练了一份词向量放到了网上，那份就没上面的两个“问题”（确切的说应该是差别），也可以用的更放心。后面会详细介绍 Turian 的工作。 关于这篇论文其实还是有些东西可以吐槽的，不过训练词向量这一块没有，是论文其他部分的。把吐槽机会留给下一篇博文了。 3.4 M&amp;H 的 HLBLAndriy Mnih 和 Geoffrey Hinton 在 2007 年和 2008 年各发表了一篇关于训练语言模型和词向量的文章。2007 年发表在 ICML 上的《Three new graphical models for statistical language modelling》表明了 Hinton 将 Deep Learning 战场扩展到 NLP 领域的决心。2008 年发表在 NIPS 上的《A scalable hierarchical distributed language model》则提出了一种层级的思想替换了 Bengio 2003 方法中最后隐藏层到输出层最花时间的矩阵乘法，在保证效果的基础上，同时也提升了速度。下面简单介绍一下这两篇文章。 Hinton 在 2006 年提出 Deep Learning 的概念之后，很快就来 NLP 最基础的任务上试了一把。果然，有效。M&amp;H 在 ICML 2007 上发表的这篇文章提出了“Log-Bilinear”语言模型。文章标题中可以看出他们其实一共提了 3 个模型。从最基本的 RBM 出发，一点点修改能量函数，最后得到了“Log-Bilinear”模型。 模型如果用神经网络的形式写出来，是这个样子： h = ∑_{i=1}^{t-1}H_iC(w_i)y_j=C(w_j)^Th这里的两个式子可以合写成一个$y_j=∑_{i=1}^{n-1}C(w_j)^TH_iC(w_i)$。$C(w)$是词 w 对应的词向量，形如 $x^TMy$ 的模型叫做 Bilinear 模型，也就是 M&amp;H 方法名字的来历了。 为了更好地理解模型的含义，还是来看这两个拆解的式子。h 在这里表示隐藏层，这里的隐藏层比前面的所有模型都更厉害，直接有语义信息。首先从第二个式子中隐藏层能和词向量直接做内积可以看出，隐藏层的维度和词向量的维度是一致的（都是 m维）。$H_i$就是一个 $m×m$的矩阵，该矩阵可以理解为第 i个词经过 $H_i$ 这种变换之后，对第 t个词产生的贡献。因此这里的隐藏层是对前 $t−1$个词的总结，也就是说隐藏层 $h$ 是对下一个词的一种预测。 再看看第二个式子，预测下一个词为 $w_j$ 的 log 概率是 $y_j$，它直接就是 $C(w_j)$和 $h$ 的内积。内积基本上就可以反应相似度，如果各词向量的模基本一致的话，内积的大小能直接反应两个向量的 cos 夹角的大小。这里使用预测词向量 h 和各个已知词的词向量的相似度作为 log 概率，将词向量的作用发挥到了极致。这也是我觉得这次介绍的模型中最漂亮的一个。 这种“Log-Bilinear”模型看起来每个词需要使用上文所有的词作为输入，于是语料中最长的句子有多长，就会有多少个 H 矩阵。这显然是过于理想化了。最后在实现模型时，还是迫于现实的压力，用了类似 n-gram 的近似，只考虑了上文的 3 到 5 个词作为输入来预测下一个词。 M&amp;H 的思路如前面提到，是 Bengio 2003 提出的。经过大牛的实现，效果确实不错。虽然复杂度没有数量级上的降低，但是由于是纯线性模型，没有激活函数（当然在做语言模型的时候，最后还是对 $y_j$ 跑了一个 softmax），因此实际的训练和预测速度都会有很大的提升。同时隐藏层到输出层的变量直接用了词向量，这也就几乎少了一半的变量，使得模型更为简洁。最后论文中 M&amp;H 用了和 Bengio 2003 完全一样的数据集做实验，效果有了一定的提升。 2008 年 NIPS 的这篇论文，介绍的是“hierarchical log-bilinear”模型，很多论文中都把它称作简称“HLBL”。和前作相比，该方法使用了一个层级的结构做最后的预测。可以简单地设想一下把网络的最后一层变成一颗平衡二叉树，二叉树的每个非叶节点用于给预测向量分类，最后到叶节点就可以确定下一个词是哪个了。这在复杂度上有显著的提升，以前是对 $|V|$ 个词一一做比较，最后找出最相似的，现在只需要做 $log_2(|V|)$ 次判断即可。 这种层级的思想最初可见于 Frederic Morin 和 Yoshua Bengio 于 2005 年发表的论文《Hierarchical probabilistic neural network language model》中。但是这篇论文使用 WordNet 中的 IS-A 关系，转化为二叉树用于分类预测。实验结果发现速度提升了，效果变差了。 有了前车之鉴，M&amp;H 就希望能从语料中自动学习出一棵树，并能达到比人工构建更好的效果。M&amp;H 使用一种 bootstrapping 的方法来构建这棵树。从随机的树开始，根据分类结果不断调整和迭代。最后得到的是一棵平衡二叉树，并且同一个词的预测可能处于多个不同的叶节点。这种用多个叶节点表示一个词的方法，可以提升下一个词是多义词时候的效果。M&amp;H 做的还不够彻底，后面 Huang 的工作直接对每个词学习出多个词向量，能更好地处理多义词。 3.5 Mikolov 的 RNNLM前文说到，Bengio 2003 论文里提了一句，可以使用一些方法降低参数个数，比如用循环神经网络。Mikolov 就抓住了这个坑，从此与循环神经网络结下了不解之缘。他最早用循环神经网络做语言模型是在 INTERSPEECH 2010 上发表的《Recurrent neural network based language model》里。Recurrent neural network 是循环神经网络，简称 RNN，还有个 Recursive neural networks 是递归神经网络（Richard Socher 借此发了一大堆论文），也简称 RNN。看到的时候需要注意区分一下。不过到目前为止，RNNLM 只表示循环神经网络做的语言模型，还没有歧义。 在之后的几年中，Mikolov 在一直在RNNLM 上做各种改进，有速度上的，也有准确率上的。现在想了解 RNNLM，看他的博士论文《Statistical Language Models based on Neural Networks》肯定是最好的选择。 循环神经网络与前面各方法中用到的前馈网络在结构上有比较大的差别，但是原理还是一样的。网络结构大致如图。 左边是网络的抽象结构，由于循环神经网络多用在时序序列上，因此里面的输入层、隐藏层和输出层都带上了“(t)”。$w(t)$ 是句子中第 t 个词的 One-hot representation 的向量，也就是说 $w$ 是一个非常长的向量，里面只有一个元素是 1。而下面的 $s(t−1)$ 向量就是上一个隐藏层。最后隐藏层计算公式为： s(t)=sigmoid(Uw(t)+W_s(t−1))从右图可以看出循环神经网络是如何展开的。每来一个新词，就和上一个隐藏层联合计算出下一个隐藏层，隐藏层反复利用，一直保留着最新的状态。各隐藏层通过一层传统的前馈网络得到输出值。 $w(t)$ 是一个词的 One-hot representation，那么 $Uw(t)$ 也就相当于从矩阵 $U $中选出了一列，这一列就是该词对应的词向量。 循环神经网络的最大优势在于，可以真正充分地利用所有上文信息来预测下一个词，而不像前面的其它工作那样，只能开一个 n 个词的窗口，只用前 n 个词来预测下一个词。从形式上看，这是一个非常“终极”的模型，毕竟语言模型里能用到的信息，他全用上了。可惜的是，循环神经网络形式上非常好看，使用起来却非常难优化，如果优化的不好，长距离的信息就会丢失，甚至还无法达到开窗口看前若干个词的效果。Mikolov 在 RNNLM 里面只使用了最朴素的 BPTT 优化算法，就已经比 n-gram 中的 state of the art 方法有更好的效果，这非常令人欣慰。如果用上了更强的优化算法，最后效果肯定还能提升很多。 对于最后隐藏层到输出层的巨大计算量，Mikolov 使用了一种分组的方法：根据词频将$|V|$个词分成 $\sqrt{|V|}$组，先通过$\sqrt{|V|}$次判断，看下一个词属于哪个组，再通过若干次判断，找出其属于组内的哪个元素。最后均摊复杂度约为 $o(\sqrt{|V|})$，略差于 M&amp;H 的 $o(log(|V|))$，但是其浅层结构某种程度上可以减少误差传递，也不失为一种良策。 Mikolov 的 RNNLM 也是开源的（网址）。非常算法风格的代码，几乎所有功能都在一个文件里，工程也很好编译。比较好的是，RNNLM 可以完美支持中文，如果语料存成 UTF-8 格式，就可以直接用了。 最后吐槽一句，我觉得他在隐藏层用 sigmoid 作为激活函数不够漂亮。因为隐藏层要和输入词联合计算得到下一个隐藏层，如果当前隐藏层的值全是正的，那么输入词对应的参数就会略微偏负，也就是说最后得到的词向量的均值不在 0 附近。总感觉不好看。当然，从实验效果看，是我太强迫症了。 3.6 Huang的语义强化与前几位大牛的工作不同，Eric H. Huang 的工作是在 C&amp;W 的基础上改进而成的，并非自成一派从头做起。他这篇发表在 ACL 2012 上的《Improving Word Representations via Global Context and Multiple Word Prototypes》试图通过对模型的改进，使得词向量富含更丰富的语义信息。他在文中提出了两个主要创新来完成这一目标：（其实从论文标题就能看出来）第一个创新是使用全文信息辅助已有的局部信息，第二个创新是使用多个词向量来表示多义词。下面逐一介绍。 Huang 认为 C&amp;W 的工作只利用了“局部上下文（Local Context）”。C&amp;W 在训练词向量的时候，只使用了上下文各 5 个词，算上自己总共有 11 个词的信息，这些局部的信息还不能充分挖掘出中间词的语义信息。Huang 直接使用 C&amp;W 的网络结构计算出一个得分，作为“局部得分”。 然后 Huang 提出了一个“全局信息”，这有点类似传统的词袋子模型。词袋子模型是把文章中所有词的 One-hot Representation 加起来，形成一个向量（就像把词全都扔进一个袋子里），用来表示文章。Huang 的全局模型是将文章中所有词的词向量求个加权平均（权重是词的 idf），作为文章的语义。他把文章的语义向量和当前词的词向量拼接起来，形成一个两倍长度的向量作为输入，之后还是用 C&amp;W 的网络结构算出一个打分。 有了 C&amp;W 方法的得到的“局部得分”，再加上在 C&amp;W 方法基础上改造得到的“全局得分”，Huang 直接把两个得分相加，作为最终得分。最终得分使用 C&amp;W 提出的 pair-wise 目标函数来优化。 加了这个全局信息有什么用处呢？Huang 在实验中发现，他的模型能更好地捕捉词的语义信息。比如 C&amp;W 的模型中，与 markets 最相近的词为 firms、industries；而 Huang 的模型得到的结果是 market、firms。很明显，C&amp;W 的方法由于只考虑了临近词的信息，最后的结果是词法特征最相近的词排在了前面（都是复数形式）。不过我觉得这个可能是英语才有的现象，中文没有词形变化，如果在中文中做同样的实验还不知道会有什么效果。 Huang 论文的第二个贡献是将多义词用多个词向量来表示。Bengio 2003 在最后提过这是一个重要的问题，不过当时他还在想办法解决，现在 Huang 给出了一种思路。 将每个词的上下文各 5 个词拿出来，对这 10 个词的词向量做加权平均（同样使用 idf 作为权重）。对所有得到的上下文向量做 k-means 聚类，根据聚类结果给每个词打上标签（不同类中的同一个词，当作不同的词处理），最后重新训练词向量。 当然这个实验的效果也是很不错的，最后 star 的某一个表示最接近的词是 movie、film；另一个表示最接近的词是 galaxy、planet。 这篇文章还做了一些对比实验，在下一章评价里细讲。 3.7 总结讲完了大牛们的各种方法，自己也忍不住来总结一把。当然，为了方便对比，我先列举一下上面提到的各个系统的现有资源，见下表。对应的论文不在表中列出，可参见最后的参考文献。 Turian 的工作前面只是提了一下，他在做 C&amp;W 向量与 H&amp;M 向量的对比实验时，自己按照论文重新实现了一遍他们的方法，并公布了词向量。后来 C&amp;W 在主页上强调了一下：尽管很多论文把 Turian 实现的结果叫做 C&amp;W 向量，但是与我发布的词向量是不同的，我这个在更大的语料上训练，还花了两个月时间呢！ Turian 公布的 H&amp;M 向量是直接请 Andriy Mnih 在 Turian 做好的语料上运行了一下 HLBL，所以没有代码公布。同时 Turian 自己实现了一份 LBL模型，但是没有公布训练出来的词向量。（这是根据他主页上描述推测的结果，从 Turian 的论文中看，他应该是实现了 HLBL 算法并且算出词向量的。） RCV1 的词数两篇文章中所写的数据差距较大，还不知道是什么原因。 Holger Schwenk 在词向量和语言模型方面也做了一些工作，看起来大体相似，也没仔细读过他的论文。有兴趣的读者可以直接搜他的论文。 事实上，除了 RNNLM 以外，上面其它所有模型在第一层（输入层到隐藏层）都是等价的，都可以看成一个单层网络。可能形式最为特别的是 M&amp;H 的模型，对前面的每个词单独乘以矩阵 HiHi，而不是像其它方法那样把词向量串接起来乘以矩阵 HH。但如果把 HH 看成 HiHi 的拼接： $[H1H2…Ht]$，则会有以下等式： 这么看来还是等价的。 所以前面的这么多模型，本质是非常相似的。都是从前若干个词的词向量通过线性变换抽象出一个新的语义（隐藏层），再通过不同的方法来解析这个隐藏层。模型的差别主要就在隐藏层到输出层的语义。Bengio 2003 使用了最朴素的线性变换，直接从隐藏层映射到每个词；C&amp;W 简化了模型（不求语言模型），通过线性变换将隐藏层转换成一个打分；M&amp;H 复用了词向量，进一步强化了语义，并用层级结构加速；Mikolov 则用了分组来加速。 每种方法真正的差别看起来并不大，当然里面的这些创新，也都是有据可循的。下一章就直接来看看不同模型的效果如何。 四、词向量的评价词向量的评价大体上可以分成两种方式，第一种是把词向量融入现有系统中，看对系统性能的提升；第二种是直接从语言学的角度对词向量进行分析，如相似度、语义偏移等。 4.1 提升现有系统词向量的用法最常见的有两种： 1）直接用于神经网络模型的输入层。如 C&amp;W 的 SENNA 系统中，将训练好的词向量作为输入，用前馈网络和卷积网络完成了词性标注、语义角色标注等一系列任务。再如 Socher 将词向量作为输入，用递归神经网络完成了句法分析、情感分析等多项任务。 2）作为辅助特征扩充现有模型。如 Turian 将词向量作为额外的特征加入到接近 state of the art 的方法中，进一步提高了命名实体识别和短语识别的效果。 具体的用法理论上会在下一篇博文中细讲。 C&amp;W 的论文中有一些对比实验。实验的结果表明，使用词向量作为初始值替代随机初始值，其效果会有非常显著的提升（如：词性标注准确率从 96.37% 提升到 97.20%；命名实体识别 F 值从 81.47% 提升到 88.67%）。同时使用更大的语料来训练，效果也会有一些提升。 Turian 发表在 ACL 2010 上的实验对比了 C&amp;W 向量与 M&amp;H 向量用作辅助特征时的效果。在短语识别和命名实体识别两个任务中，C&amp;W 向量的效果都有略微的优势。同时他也发现，如果将这两种向量融合起来，会有更好的效果。除了这两种词向量，Turian 还使用 Brown Cluster 作为辅助特征做了对比，效果最好的其实是 Brown Cluster，不过这个已经超出本文的范围了。 4.2 语言学评价Huang 2012 的论文提出了一些创新，能提升词向量中的语义成分。他也做了一些实验对比了各种词向量的语义特性。实验方法大致就是将词向量的相似度与人工标注的相似度做比较。最后 Huang 的方法语义相似度最好，其次是 C&amp;W 向量，再然后是 Turian 训练的 HLBL 向量与 C&amp;W 向量。这里因为 Turian 训练词向量时使用的数据集（RCV1）与其他的对比实验（Wiki）并不相同，因此并不是非常有可比性。但从这里可以推测一下，可能更大更丰富的语料对于语义的挖掘是有帮助的。 还有一个有意思的分析是 Mikolov 在 2013 年刚刚发表的一项发现。他发现两个词向量之间的关系，可以直接从这两个向量的差里体现出来。向量的差就是数学上的定义，直接逐位相减。比如 $C(king)−C(queen)≈C(man)−C(woman)$。更强大的是，与 $C(king)−C(man)+C(woman)$最接近的向量就是 $C(queen)$。 为了分析词向量的这个特点， Mikolov 使用类比（analogy）的方式来评测。如已知 a 之于 b 犹如 c 之于 d。现在给出 a、b、c，看 $C(a)−C(b)+C(c)$最接近的词是否是 d。 在文章 Mikolov 对比了词法关系（名词单复数 good-better:rough-rougher、动词第三人称单数、形容词比较级最高级等）和语义关系（clothing-shirt:dish-bowl）。 在词法关系上，RNN 的效果最好，然后是 Turian 实现的 HLBL，最后是 Turian 的 C&amp;W。（RNN-80:19%；RNN-1600:39.6%；HLBL-100:18.7%；C&amp;W-100:5%；-100表示词向量为100维） 在语义关系上，表现最好的还是 RNN，然后是 Turian 的两个向量，差距没刚才的大。（RNN-80:0.211；C&amp;W-100:0.154；HLBL-100:0.146） 但是这个对比实验用的训练语料是不同的，也不能特别说明优劣。 这些实验结果中最容易理解的是：语料越大，词向量就越好。其它的实验由于缺乏严格控制条件进行对比，谈不上哪个更好哪个更差。不过这里的两个语言学分析都非常有意思，尤其是向量之间存在这种线性平移的关系，可能会是词向量发展的一个突破口。 参考文献Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of Machine Learning Research (JMLR), 3:1137–1155, 2003. [PDF] Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research (JMLR), 12:2493-2537, 2011. [PDF] Andriy Mnih &amp; Geoffrey Hinton. Three new graphical models for statistical language modelling. International Conference on Machine Learning (ICML). 2007. [PDF]Andriy Mnih &amp; Geoffrey Hinton. A scalable hierarchical distributed language model. The Conference on Neural Information Processing Systems (NIPS) (pp. 1081–1088). 2008. [PDF] Mikolov Tomáš. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of Technology. 2012. [PDF] Turian Joseph, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for semi-supervised learning. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL). 2010. [PDF] Eric Huang, Richard Socher, Christopher Manning and Andrew Ng. Improving word representations via global context and multiple word prototypes. Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. 2012. [PDF] Mikolov, Tomas, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. Proceedings of NAACL-HLT. 2013. [PDF]]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
        <tag>Skip-gram</tag>
        <tag>CBOW</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日知录（1）：MacTips]]></title>
    <url>%2F2017%2F12%2F05%2F%E6%97%A5%E7%9F%A5%E5%BD%95%EF%BC%881%EF%BC%89%EF%BC%9AMacTips%20%2F</url>
    <content type="text"><![CDATA[1.终端输入说英语 说英语时我们当然希望有标准发音。在Mac中不需要字典，直接在终端里输入say yes，Mac就会说英语了。 2.Spotlight快速打开程序 很多刚开始使用Mac的用户，一般都知道Spotlight检索功能。事实上，用这个功能还可以快速打开程序。通过ctrl+space呼出，或者输入通讯或cont都可以找到通讯录这个程序，回车即可打开。 3.Spotlight注释功能定位文件 OS X的文件系统提供了Spotlight注释功能，可以帮助用户更有针对性地定位文件。选中一个文件或文件夹，command+i打开简介，在Spotlight注释功能中加入自己特定的关键词。关掉简介窗口，呼出Spotlight并输入刚才的关键词，可以准确定位到相关的文件或文件夹。 4.使用sips命令批量处理图片 如果你想批量修改一批图片（尺寸、旋转、反转等），但你不会或没有PS，可以使用sips命令高效完成这些功能，例如： 123456##把当前用户图片文件夹下的所有JPG图片宽度缩小为800px，高度按比例缩放sips -Z 800～/Pictures/.JPG##顺时针旋转90˚sips -r 90～/Pictures/.JPGwdhjf##垂直反转sips -f vertical ～/Pictures/.JPG 更多命令可以用sips -h查看。 5.把当前网页附加到待发送邮件中 使用Safari浏览网页的时候，如果你想把当前页面通过邮件发送给自己或别人，使用command+i可以直接打开邮件并把当前网页附加到待发送的邮件中。 6.快速删除文件和清空废纸篓 在Finder中选中文件，使用command+delete删除文件。如果想彻底清除，使用shift+command+delete就会自动清空废纸篓。 7.获悉目录空间 在Mac下想知道某个目录下各个文件和子目录各占多少空间，不需要一个一个去查看。打开终端，在该目录下输入du-sh*，结果一目了然。 8.英文自动完成 当使用系统软件文本编辑、Pages和Keynote时，输入英文按esc键，系统会帮助你自动完成单词。比如你想输入“brilliance”，只需输入“brill”，按esc键，系统就会出现自动提示。如果某个应用，比如Safari的搜索框里esc是取消输入，那么使用fn+F5也可以达到这个效果。对于常写英文文档的人比较有帮助。 9.文件操作 在Finder中打开文件使用鼠标双击或按command+o键。和Windows不一样的是，选中文件回车是对文件重命名，而不是打开文件。 10.显示隐藏文件 在终端里输入ls-a，可以显示该目录下的隐藏文件。在Finder中按shift+command+.键可以显示隐藏文件，想恢复原来的设置，再按一遍shift+command+.即可。 11.利用你的触发角 OS X系统为用户提供了强大的Mission Control功能，今天为大家介绍其中的触发角。打开“系统偏好设置”→“Mission Control”→“触发角”，就可以对屏幕的四个角进行设置了。比如把左上角设置为将显示器置为睡眠状态，当我们暂时离开电脑时，顺手把鼠标移到左上角，屏幕就变黑了，非常方便。 12.维护你的Mac Mac的OS X是一个使用起来非常简单的操作系统，一般情况下不需要装杀毒工具，大部分程序安装都非常简单，直接把后缀为app的程序拖进应用程序文件夹就可以了。但是，当你在使用系统时如果发现出现异常，那么就该进行日常维护了。打开磁盘管理，选中你的系统盘，单击“修复磁盘权限”，对磁盘权限进行检查和修复。完成之后还可以手动执行维护脚本： 123sudo periodic dailysudo periodic weeklysudo periodic monthly 也可以一次全部执行： 1sudo periodic daily weekly monthly 一般执行完这些操作后，你的Mac就会充满活力，可以继续上路了。这些操作可以定期执行。 13.Mission Control 设置中把“使窗口按应用程序成组”关掉，Mission Control的行为就会跟10.7以前的Exposé一样，不会把同一个程序的多个窗口叠在一起。对经常一个程序开很多窗口的程序员来说很有用。 14.截图 OS X提供了非常方便的截图工具，你可以随时随地截取屏幕画面。shift+command+3：全屏幕截图；shift+command+4：通过鼠标选取截图。截取的图片默认存放在桌面上，以时间命名。系统默认截图格式是png，你可以通过如下命令修改截图文件类型，例如： 1defaults write com.apple.screencapture type -string JPEG 15.推荐几个有用的小工具 ◆TotalFinder：Finder的增强插件，Finder的插件，为Finder增加多标签（类似Chrome的多页签）、双面板、UI设置等功能。收费软件。◆Breeze：窗口管理软件，option+1/2/3分别对应最大化窗口/左半屏幕窗口/右半屏幕窗口。收费软件。◆Trillian：整合了MSN、GTalk、Twitter等，表现稳定，用户体验也不错。免费软件，可以从AppStore直接下载。◆smcFancontrol：风扇控制软件，免费。OS X对风扇控制不敏感，CPU温度很高时才会增加风扇转速，那时机器表面已经比较热了。用这个软件可以自由控制风扇转速。夏天空调屋里一般3000～4000转就够了，冬天一般不需要开启。 16.Mac的原生输入法 我在Mac下曾经使用过很多输入法，包括百度、FIT、搜狗等，这是因为之前Mac的原生输入法太不给力了。不过现在的版本已经有了很大的改进，慢慢地也变成了常用输入法之一。今天就为大家介绍一些Mac输入法的操作技巧。◆中英文混合输入：输入中文的时候，打开caps lock键，可以直接输入英文，关掉又切换回中文。◆选词：通过-+号可以切换字或词，通过[]可以展开候选词列表并进行切换。◆打开输入法偏好设置，可以设置自动校正模糊音。◆用’可以进行手动分词，比如“fang’an（方案）”。◆使用shift+6可以输入表情符号，比如“（☆_☆）”和“凸^-^凸”。用习惯了，你会离不开这个输入法的。 17.Safari的标签 Safari是我在Mac上最常用的浏览器，Chrome也不错，但我更偏爱Safari。今天为大家介绍一下这个浏览器的标签使用。当你想在新的标签页打开网页时，只需要按住command键，单击链接即可。使用Multi-Touch手势在标签页中切换。在触控板上，双指开合即可显示你打开的标签页。在标签视图中，双指轻扫可浏览不同标签页。通过shift+command+左右方向键，可以快速在Safari中打开的标签中进行切换。 18.监控Mac的运行状况 ◆top：打开终端输入top，可以显示目前系统的进程情况、CPU使用情况、内存使用情况、磁盘使用情况和进程的详细列表等信息，输入“?”会显示帮助信息，参考帮助你还可以自定义top显示的信息，输入“q”退出监控界面。◆htop：htop是更聪明更高级的top，虽然不是Mac原生的，但安装非常方便。打开终端输入： 1sudo port install htop 命令结束就安装完成了。然后键入htop，你会看到一个更丰富的彩色的top，多个CPU、内存统计、uptime，以及更详细的进程信息。参考界面最底部的帮助信息还可以进行排序、展开和Kill。输入q退出监控界面。◆系统的活动监视器：这个非常适合不喜欢终端的用户。从应用程序→实用工具可以找到活动监视器，打开后你会发现很类似Windows下的任务管理器，相信这个不需要给大家介绍了。 19.批量复制文件 例如你在一个目录下林林总总放了几百个文件，有图片、pdf、zip、doc等，你想把后缀为png、jpeg、gif的图片复制到另一个文件夹去，最简单的方式是什么？不是通过搜索把这些文件找出来，再全选复制到另一个文件夹下。而是进入该目录，执行这样一条命令： 1cp *.png *.jpeg *.gif /destpath 如果想剪切，就把cp改为mv。 20.程序切换 在OS X中程序切换可以通过command+tab进行，command+tab进行顺序切换，command+shift+tab进行逆序切换，功能类似Win7的alt+tab。OS X还提供了同组程序的切换，比如你打开了多个预览程序阅读pdf，你想在这些pdf之间切换阅读，这时候就可以使用command+`（esc下面的键）进行同组程序切换。 21.远程复制 OS X提供了基于SSH的远程复制命令scp，这个命令大部分Linux和Unix系统都会提供，使用该命令可以非常方便地在两台机器之间安全地复制文件，具体命令为： 1scp ./testfile.txt username@10.10.10.22:/tmp 回车后会要求你输入username的密码，只会将当前目录下的testfile.txt复制到另一台机器的tmp目录下。 1scp username@10.10.10.22:/tmp/testfile.txt ./ 从远端复制到本地。 22.OS X中的ftp 这个问题有订阅者问过，总结一下，以下三种方式就够用了。◆直接在命令行使用。打开终端输入ftp anonymous@ftp.mozilla.org，或者使用sftp通过SSH完成ftp的功能，例如sftp user@10.10.10.11。◆使用第三方工具。比如FileZilla，用法和Windows类似。◆利用OS X原生FTP工具。从Finder菜单栏中进入“前往”→“连接服务器…”，输入FTP服务器地址,如 ftp://ftp.mozilla.org , 单击地址栏右侧的“+”号按钮可以将当前地址加入“个人收藏服务器”，单击“连接”按钮，按照提示进行身份验证，成功后即可连接到FTP服务器。 23.备份 OS X提供了非常方便的备份工具TimeMachine（时间机器），我第一台Mac用的操作系统是Leopard，后来升级到Snow Leopard→Lion→MountainLion，再后来换新机器，但从未重装过系统。这对于Windows系统来说是不可想象的，这都得益于时间机器。我个人每周会备份一次，如果你觉得自己资料非常重要，可以每隔几小时备份一次。具体的用法我就不介绍了，可以参考官方介绍：http://support.apple.com/kb/HT1427?viewlocale=zh_CN。 24.inode和history ◆inode：Mac的文件系统和Windows完全不同，文件所需信息都包含在这个inode（索引节点）里。每个文件都有inode，文件系统用inode来标识文件。简单来说就是inode包含了文件的元数据信息，文件名、文件内容，但不包含任何控制信息。inode是Unix/Linux系列文件系统设计的核心，有兴趣的同学可以上网查阅相关资料。对于普通用户用来，最直观的表现是，在Mac里，你可以对正在使用的文件改名，换目录，甚至放到废纸篓，都不会影响当前文件的使用。◆history：打开终端输入history，所有的历史命令都会显示出来，想找某一条执行过的命令，还可以这样： 1history | grep apache 找到左边的命令编号（例如1001），在终端输入 1!1001 就可以执行原来那条命令了。 25.Go2Shell 通过Finder浏览文件的时候，常常需要在浏览的文件目录中打开终端进行操作，Go2Shell能够自动做到这一点。从App Store下载这个免费软件 https://itunes.apple.com/us/app/go2shell/id445770608?mt=12 ,下载完成后从应用程序文件夹把Go2Shell拖到Finder工具栏上，然后随便进入一个目录，单击Go2Shell图标，即可打开终端进入该目录。Go2Shell支持原生终端、iTerm2和xterm，在终端输入open-a Go2Shell—args config即可进入配置界面，选择你喜欢的终端。 26.Safari的阅读器 Safari的阅读器是浏览器创新之一，在Safari之前，没有其他浏览器提供过这样的功能。当Safari发现结构优良的网页文档时，就会在地址栏右侧显示“阅读器”，单击就可以进入简洁的阅读模式，通过shift+command+r也可以进入。 阅读器已经提供了良好的网页阅读体验，对于分页文档甚至能够自动翻页阅读，但是我们还可以更进一步。比如我就觉得阅读器太窄了，视野不够宽阔。有类似需求的同学就可以通过Safari的扩展插件CustomReader进行个性化定义。 从http://canisbos.com/customreader 下载CustomReader ,双击即可安装。安装之后到任何一个支持阅读器的网页，按下shift+command+r激活阅读器，再用ctrl+r调出配置页面，就可以配置你自己独享的个性化阅读器了。 27.Remote Desktop Connection for Mac 很多读者询问如何在Mac中通过远程桌面连接到Windows，这次统一答复一下，微软提供了专门的Remote Desktop Connection for Mac，免费，下载链接：http://www.microsoft.com/mac/remote-desktop-client。 28.文档的版本控制 经常使用Keynote、Pages、Numbers和原生文本编辑器的用户，可以尝试使用文档的版本控制功能。对于经常编写文档的人来说，这个功能非常有用。大家可能没有注意到，当你把鼠标移至文档标题的时候，会出现一个小箭头，下拉可以看到浏览所有版本的选项，单击进入该文档的时间线，界面与Time Machine一模一样，你可以非常方便地找到任何一时间点你编辑过的内容，也可以随意恢复到任何一个版本而不会影响其他版本。非常酷的功能，并且好用。 29.如何快速发送带附件的邮件 在Windows中，我们可以右键单击文件发送到邮箱即可发送带附件的邮件。OS X也有类似功能，只不过叫共享。右键单击要发送的文件→“共享”→“电子邮件”即可。 30.如何快速创建便笺 便笺是我们很常用的功能，可以把一些临时性的文字内容贴到桌面上，大家是如何做的呢？复制文字，打开便笺程序，新建便笺，粘贴文字！Too young too complicated，我们只需要选中文字，然后shift+command+y，就行了。 31.Mac的通用快捷键 这部分内容之前陆续介绍过，但还是有人希望有个汇总，基于二八原则，我把最常用的快捷键罗列一下，对于非开发者，应该够用了。 1234567891011121314command+tab 任意情况下切换应用程序，向前循环shift+command+tab 切换应用程序，向后循环command delete 把选中的资源移到废纸篓shift+command+delete 清倒废纸篓（有确认）shift+option+command+delete 直接清倒废纸篓command+～ 同一应用程序多窗口间切换command+f 呼出大部分应用程序的查询功能command+c/v/x 复制/粘贴/剪切command+n 新建应用程序窗口command+q 彻底退出当前应用程序command+l 当前程序是浏览器时，可以直接定位到地址栏command+&quot;+/-&quot; 放大或缩小字体control+space 呼出Spotlightcommand+space 切换输入法 对于最后两个快捷键，我个人比较习惯control+space切换输入法，所以做了自定义的配置。 32.终端命令open 我们之前介绍过如何在Finder中浏览文件时进入当前目录的shell界面，那个插件叫做Go2Shell。当然我们也会有在shell下打开当前目录的Finder的需求，运行如下命令即可： 12345678open .#当然open也可以打开其他目录，比如open /Users#open还可以直接打开文件、打开程序、指定程序打开文件、打开网址等，例如open a.txtopen -a Safariopen -a TextMate a.txtopen http://news.sina.com.cn 33.介绍小软件——CatchMouse 可以自定义快捷键快速在多个显示器内切换鼠标，非常方便，链接附上：https://itunes.apple.com/cn/app/catchmouse/id439700005?mt=12。 34.激活窗口 如果你在一个屏幕内打开了多个程序，除了当前激活的软件窗口，你还想看看其他窗口的内容，这时你直接单击其他窗口的话，原来的窗口就可能被遮挡或消失。如何保持原来的窗口一直处于最上层呢？非常简单，拖曳其他窗口的时候按住command键即可，原来的窗口会永远在最上面。 35.文件检查器 在Windows中大家经常选中多个文件，单击右键→“属性”可以查看这些文件的大小。在Mac里同样的操作（选中多个文件，单击右键→“显示简介”）弹出的是各个文件或文件夹的简介，这让很多人困惑不解。其实我们只要在单击右键的同时按住option键，“显示简介”就会变成“显示检查器”，单击“显示检查器”即可查看和操作批量文件。另外，我还经常用这种方式浏览图片，比如选中多张图片，按住option键单击鼠标右键，选中“幻灯片显示xx项”，就可以全屏浏览图片了。 36.屏幕放大镜 有时我们需要放大屏幕做一些精细的操作，Ctrl+鼠标滚轮可以实现这一效果，如果你是键盘控，用option+command加上加减号也可以实现。 37.语音识别 Mountain Lion增加了语音识别的功能，具体的设置在“系统偏好设置”→“听写与语音”，可以设置听写语言、呼出窗口的快捷键等。我采用的是默认的快捷键，连续按fn键两次即可呼出语音识别窗口，这时候你就可以对Mac说话了。如果你想让Mac把你说的写下来，最好打开一个的文本编辑器并让光标处于可编辑状态。注意，该功能需要联网。 38.time命令 如果你想知道在终端执行的某个程序耗时多久，对CPU等的使用情况，可以输入： 1time python fib.py 输出结果： 1python fib.py 0.02s user 0.02s system 50% cpu 0.094 total 39.特殊字符输入 12345678910111213141516171819202122美元（$） shift+4美分（¢） option+4英镑（£） option+3人民币（¥） option+Y欧元（€） shift+option+2连接号（–） option+-破折号（—） shift+option+-省略号（...） option+;约等于（≈） option+X度（°） shift+option+8除号（÷） option+/无穷大（∞） option+5小于等于（≤） option+,大于等于（≥） option+.不等于（≠） option+=圆周率（π） option+P正负（±） shift+option+=平方根（√） option+V总和（∑） option+W商标（™） option+2注册（®） option+R版权（©） option+G 40.OS X三指轻拍查找功能 OS X提供了三指轻拍查找的功能，什么意思呢？把光标移到一个单词上面，无需选中，三指轻拍，系统就会弹出词典显示相关单词的释义，非常方便。该功能可以在“系统偏好设置”→“触控板”里进行设置。 41.F.lux调节屏幕色温 推荐一款免费小软件F.lux。这个软件功能类似系统的亮度自动调节，不同的是它调节的是屏幕的色温。该软件能够根据时间来调节屏幕色温以达到保护眼睛的目的。有数据表明，4600～5000K的暖色有缓解眼部疲劳的作用。下载地址：http://stereopsis.com/flux/。第一次打开应用，需要输入当前城市名称，搜索定位用户的当地时间。F.lux将根据日出及日落的时间来调节色温。在日出日落期间，屏幕色温和平时一样，对于RMBP来时就是6500K；日落之后，F.lux会逐渐地暖化你的屏幕。具体色温可以自定义。我用了之后感觉还不错，大家可以试用下。 42.让Mac不进入休眠状态 如果你想离开电脑一段时间，又不想让电脑进入休眠状态，有个简单的命令可以帮助你做到这一点。在终端中输入：pmset noidle，即可。只要该命令一直运行，Mac就不会进入睡眠状态。关掉终端或ctrl+C可以取消该命令。pmset是OS X提供的命令行管理电源的工具，其功能远不止于此。 123pmset -g #查看当前电源的使用方案sudo pmset -b displaysleep 5 #设置电池供电时，显示器5分钟内进睡眠sudo pmset schedule wake &quot;02/01/13 20:00:00&quot; #设置电脑在2013年2月1日20点唤醒电脑 …… 感兴趣的可以使用man pmset查看详细信息。 43.网络共享 Mac提供了非常简单易用的Internet共享功能，可以作为一个轻量级的家庭无线路由使用。只要你的Mac能够上网，那么Phone和Pad等设备就都可以通过Wi-Fi共享Mac的网络，实现无线上网。具体的设置非常简单，打开“系统偏好设置”→“共享”→“互联网共享”，选择共享源（网卡或AirPort），并设置Wi-Fi的名称、密码、安全级别等属性，最后勾选左侧列表的“互联网共享”，根据提示操作即可。这是一个我曾经认为大部分用户都知道的功能，后来发现几乎很少人使用或会用。 44.快速查看 OS X提供了非常方便的预览文件内容的功能。在Finder或桌面上，选中一个文件并按空格键，系统就会弹出预览界面。对于很多文件我们仅仅使用快速查看功能就可以浏览文件内容了，比如iWorks的Keynote、Pages和Numbers的文档，微软Office的文档，pdf，图片，视频，各类文本文件等。除了在Finder和桌面快速查看文件，我们还可以快速浏览邮件的附件。打开邮件程序，找到一个带有附件的邮件，选中附件并按空格键，就可以快速浏览附件内容。我们还可以在终端操作的时候使用这个功能，例如qlmanage-p文件名，系统就会弹出快速查看窗口。 45.显示桌面 我们下载文件或临时文件经常会放到桌面上，在Windows里通过alt+d或点击显示桌面的图标即可，在Mac里如何实现呢？有两种方式，都很方便。第一种是四指划开，该功能可以在触控板里设置。还有一种方式是通过快捷键command+F3，即可实现移开程序显示桌面的功能。当我们想把桌面的文件放入某个程序（比如当做邮件附件）时，可以配合command+tab实现。用鼠标拖动桌面文件，command+tag切换程序，然后把文件拖入该程序即可。 46.应用程序的安装和卸载 OS X中的应用程序和OSGi中使用的Bundle类似，都是把配置文件和程序封装在一个包里。对于普通用户来说，你在Launchpad中看到的所有程序都像一个图标，但这个图标不是Windows中的快捷方式，而是封装好的Bundle，从程序角度而言这是一个文件夹，对普通用户来说，知道点这个图标运行程序就行了。这种设计方式使得OS X中95%以上的软件的安装变得十分简单。如果你是从Windows转过来的话，你会认为安装和卸载简单得令人发指。安装程序就是把XXX.app拖进/Applications（应用程序文件夹），卸载就是把程序从该目录删掉。好吧，你可以这么理解，OS X中95%以上的软件都是Windows中的“绿色软件”。 47.磁盘映像 磁盘映像类似Windows中的iso，不过文件后缀为dmg。磁盘映像可以直接挂接到OS X中，其表现形式就像是磁盘分区。双击文件可以直接打开，打开后在Finder左边栏的设备中可以找到挂接好的磁盘映像。dmg是Mac下最常用的文件组织方式，几乎所有的安装程序都是以dmg方式发布的。一般情况下安装程序就是打开相关程序的dmg文件，里面有一个app和应用程序文件夹，把app拖入应用程序即可。另外我们也可以使用磁盘工具把dmg里的文件恢复为真正的硬盘文件，也可以制作dmg文件。 48.复制目录下文件名列表 如何复制某个目录下所有文件的文件名列表呢？非常简单，command+a，command+c。然后打开一个文本编辑器（比如TextMate），command+v即可。 49.多点触控手势 当我们用Safari浏览网页时，经常想回到之前浏览过的历史页面。使用多点触控手势可以非常容易直观地实现该功能。打开Safari浏览多个页面，然后使用双指左右轻扫，可以来回切换浏览页面。 另外，如果你在浏览时不小心关掉了一个标签页，使用command+z可以恢复最后关闭的那个标签页。 50.OS X的预览程序 OS X的预览程序可以打开各类图片和pdf等类型的文件，当你想查看某个图片或pdf的细节时，没必要用command++/-来缩放整个文件，使用`键可以呼出放大镜，细节一览无遗。 51.显示/隐藏桌面内容快捷键 我们经常会在桌面上堆满文件夹和文件，有时候会很方便，有时候会觉得很乱。其实我们可以通过以下命令来决定什么时候显示，什么时候隐藏： 12chflags hidden ～/Desktop/* //隐藏桌面内容chflags nohidden ～/Desktop/* //显示桌面内容 如果觉得输入麻烦，用TextExpander或Alfred设置成snippet即可。 52.按住option的快捷键 OS X设置了一些快捷键用来快速打开显示器、Mission Control、键盘、声音等系统设置，具体是什么呢？你只要按住option，轮番把键盘最上方的那排键试一下就知道了，一般人我不告诉他。 53.Space（空间） 使用OS X，我们可以充分利用系统提供的多个Space，把不同的程序放到不同的Space，让我们的系统更有扩展性。如何增加Space呢？四指上推，在桌面的最上方会出现当前的Space，把鼠标移到Space列表的右侧，会出现一个带+号的空间，单击加号，即可增加一个Space。那么如何把某个程序固定在某个Space打开呢？在某个Space打开程序，在Dock中找到这个程序图标，鼠标长按会出现一个菜单，“选项”→“分配给”，选“这个桌面”，下次再打开这个程序，就会自动进入设定的Space。 Space的排列方式可以在Mission Control里设置，比如选择按照使用情况自动排列等。 54.隐藏程序 当我们不想在使用当前程序的时候看到其他程序的时候，可以使用快捷键option+command+h，这时除了你正在使用的程序，其他所有的程序都会被隐藏起来，有助于你专心工作。想切换到其他程序时，可以使用command+tab。 55.文件颜色标签的使用 OS X的Finder提供了颜色标签的功能，可以直接为文件和文件夹标记颜色。我在很长一段时间都没有注意到这个功能，一次偶然的机会开始使用颜色标记文件，感觉非常方便。比如我会在Finder的主目录下用颜色标明最常访问的文件夹。如果是电子书，可以用颜色表示阅读状态，例如绿色表示正在阅读，灰色表示读完了，橙色表示待阅读，等等。大家可以根据自己的习惯使用颜色标签，提高效率。 56.利用邮件中的日期创建日历事件 工作中我们总是通过邮件来通知会议和活动，这时邮件中往往有日期信息。我们可以利用这个信息直接创建日历事件。打开邮件，把鼠标移动到有效的日期信息上，会出现下拉菜单的按钮，单击后可以为日历添加事件，事件标题默认为邮件标题。 57.AppleScript小程序 今天为大家介绍用AppleScript实现一个示例小功能：清空废纸篓。打开AppleScript编辑器，输入如下代码： 1234567891011--操作对象是Findertell application &quot;Finder&quot;--为isEmpty变量赋值set isEmpty to &quot;是否清空废纸篓！&quot;--显示确认对话框，单击“确认”程序继续执行，单击“取消”终止程序display dialog isEmpty--清空废纸篓empty the trash--通过语音说这事搞定了say &quot;It is done!&quot;end tell 单击工具栏的“编译”按钮，检查没有错误后，单击“运行”即可，大家可以看看发生了什么。 58.Homebrew Homebrew的功能和OS X自带的MacPorts很像，但是更为轻量级，由于大量利用了系统自带的库，安装方便，编译快速，实在是OS X系统开发中之必备工具。安装方式： 1ruby-e&quot;$（curl-fsSL https://raw.github.com/mxcl/homebrew/go）&quot;。 使用方式： 1brew install wget//安装wget工具。 具体的使用请参考：https://github.com/mxcl/homebrew/wiki。 59.根据文件名快速查找文件 我们在OS X中查找文件或文件内容一般使用Spotlight或Alfred，这些功能在前面的Mac Tips中都介绍过，不过，如果你知道文件名的一部分，想更加快速地定位文件，那么就会用到命令行工具locate。 locate是Unix/Linux下的命令工具，基本原理就是通过定期更新系统的文件和文件名并把索引信息放入系统的数据库中，当通过locate查找文件时直接从数据库里取数据。而且locate可以查到Spotlight查不到的系统文件。基本的使用方法非常简单，比如你想找Nignx的配置文件在哪，只需输入：locate nginx.conf。 60.设置用户登录选项 OS X系统登录后会自动启动一些程序，比如Alfred、拼音输入法、风扇控制软件等等。有时我们会嫌多，有时又想增加一些启动项，在哪设置呢？打开“系统偏好设置”→“用户与群组”，选中“当前用户”，单击右边的“登录项”，你就会看到系统启动时加载的程序，可以随意删减，还能够设置启动后隐藏，非常方便。 61.修改你的登录窗口 我们默认登录OS X时，系统会显示登录用户列表，你需要用鼠标点一下要使用的用户，或者用光标键选择用户，出现登录框后输入密码登录。如果我们想不显示用户列表，直接输入用户名和密码登录怎么办呢？打开“系统偏好设置”→“用户与群组”，单击左侧下方的“登录选项”（很奇怪很多人找不到这个），在右侧修改登录窗口为“名称和密码”。注销登录，这次大家就满意了。 62.Mac的键盘 很多人第一次用Mac的键盘时会发现，苹果也太抠门儿了，退格键没了，PageUP/PageDown/Home/End也没了。别担心，您不是还有delete键和上下左右方向键么？delete相对于退格键，fn+delete可以往前删，fn+上下左右方向键可以实现PageUP/PageDown/Home/End的功能，一个功能都不少。 63.QuickTime 很多人都会使用QuickTime Player看mp4或mov视频文件，但其功能远不止于此。option+command+n可以打开录像功能，ctrl+option+command+n可以打开录音功能，ctrl+command+n可以打开录制屏幕功能。最后一个功能非常适合做产品介绍或产品演示，大家可以试一试。遇到快捷键冲突的，在QuickTime的文件菜单中也可以找到这三项。 64.Dropbox快速导入Mac 有读者问如何把iPhone或iPad里的照片导入Mac，我自己用的办法是Dropbox，安装了Dropbox之后，每次用USB连接iPhone或iPad时，程序都会提示是否有新照片需要导入，导入后自动云端同步。不用Dropbox的同学，另外一个简单的方式是连接移动设备时，打开预览程序，单击文件，可以看到一个“从iPhone/iPad导入”的菜单，单击一下，后续你基本就知道该怎么做了。 当然还有其他方法，比如打开图像捕捉或iPhoto程序等。 65.快速创建日历事件 OS X提供了智能创建日历事件的功能。打开日历程序，单击左上角的“+”号，在弹出的输入框里输入：“明天上午9点到13点参加公司年会。”回车，看看效果如何？日历程序会准确地创建你想要的事件。大家可以试试其他写法。 66.显卡监控软件gfxCardStatus 现在大部分Mac都有两块显卡，集成显卡和独立显卡。OS X会根据不同的程序自动切换显卡，但有时候我们在电池供电的情况下会由于某些程序的原因一直使用独立显卡，会大大缩短待机时间，这时候就能用到这个软件了。gfxCardStatus能做的事情有两件。一件是手动切换显卡。另一件是监控现在系统在使用哪块显卡，如果是独立显卡的话，是因为哪个程序导致必须使用独显。下载地址：http://gfx.io。 67.创建智能文件夹 Finder提供了智能文件夹的功能，简单来说就是固化你的搜索条件，并形成文件夹存放在左侧边栏。 例如你想建一个文件大小大于1G的智能文件夹，使用快捷键option+command+n呼出新建智能文件夹界面，单击最右侧的加号，在条件选择第一栏选择“大小”，第二栏选择“大于”，第三栏输入“1G”，你就可以看到你的Mac上文件大于1G的列表，单击“存储”，命名后该文件夹就会出现在左侧边栏。随时单击随时动态监控自己的硬盘上有哪些超过1G的大文件。试试其他搜索条件吧！ 68.自动打开程序文稿 OS X提供了自动恢复上次关闭程序时打开的文稿和窗口的功能。这就是说，如果你使用预览程序打开了5个PDF文件，用command+q关闭了预览程序，下次打开预览程序时，会自动恢复这5个PDF程序，包括文字选中的状态，阅读进度等信息。这个功能我非常喜欢，但有时候我们并不希望自动恢复，那么有两种方式可以关闭这个功能。 第一种：打开“系统偏好设置”→“通用”，选中“退出应用时关闭窗口”，这样所有的程序都不再具备恢复功能。 第二种：退出程序时使用option+command+q而不是command+q，相当于关闭所有文件并退出程序，下次打开时，这些文件就不会自动打开了. 69.智能邮箱 邮箱账户的创建相信一般的用户都可以正常操作，不知道你是否使用过OS X中Mail的智能邮箱功能呢？ 打开邮件程序，单击“邮箱”→“新建智能邮箱”，在弹出的窗口中选择你的过滤条件。过滤条件非常灵活，可以定义与或关系，增加多个过滤条件，设置完成后保存即可。你会发现左侧栏多了一个智能邮箱，单击即可根据你设置的过滤条件找到那些符合条件的邮件。 70.隐藏的VIP 如果你的系统版本是10.8以上，那么你就会发现邮件程序中多了一个隐藏的VIP功能。随便找封邮件，把鼠标放在发件人或收件人的邮件地址上，会出现一个蓝色的选择框，单击其中的白色箭头，在下拉菜单中单击“添加到VIP”，你就会发现左边栏多了一个VIP分栏，单击加入的VIP用户，可以直接查看他们发送的邮件。 71.在Finder中打开某个文件夹下所有子文件夹 有时候我们希望在Finder中查看某个文件夹下的所有文件和子文件夹，怎么做到呢？把文件切换到列表视图（command+2），把排序方式设置为“不排序”，这时文件夹左侧会出现一个箭头。按住option键单击文件夹左侧的箭头，你就会发现所有的文件和文件夹都展现在眼前了。注意，如果该文件夹下文件太多，那么不建议使用，因为打开会需要很长时间。 72.慢速动画 所有具备动画效果的操作，按住shift键，会播放慢速动画。大家可以试试按住shift键的时候最小化窗口，效果非常酷。 73.XtraFinder 这个插件具备和TotalFinder类似的功能，支持tab、文件夹置顶、多窗口、剪切、全局热键等功能，重要的是这是一个完全免费的自由软件。下载网址：http://www.trankynam.com/xtrafinder/。 74.MacBook待机仍可为iPhone供电 我们平时会把iPhone接到MacBook上充电。事实上，把MacBook合上待机时，仍然可以为iPhone供电，大家可以试一下。如果你出游时会带上你的Mac，别忘了这也是一块大的移动电池。 75.恢复截屏图片默认保存路径 截屏图片存哪了？OS X自带截屏不好使了，截屏之后有“咔嚓”的程序运行声，但图片不知道去哪里了，如何修复？ OS X自带的截图文件是存储在桌面上的，你的可能是被修改过了，我们可以通过以下命令恢复默认路径： 1defaults delete com.apple.screencapture location 注销，重新登录，再次截屏看看文件是否保存在桌面上了。 76.如何为OS X自带的字典增加中文字典 目前OS X自带的字典程序是没有中文的，不过我们很容易为其扩展新字典。操作如下： ◆如果打开了字典程序，关闭。◆到以下网址下载《朗道英汉字典》和《朗道汉英字典》，解压缩得到两个后缀为dictionary的文件：http://pan.baidu.com/share/link?shareid=249542&amp;uk=2617481269。◆把这两个文件复制到～/Library/Dictionaries下。◆启动字典程序，你就会看到增加了《朗道英汉字典》和《朗道汉英字典》。 77.文件共享 在Mac之间进行文件共享有很多种方式，介绍两个最简单的，具备AirDrop功能的两台或多台Mac，在开着Wi-Fi的情况下打开AirDrop，就会找到同样打开AirDrop的Mac，把想传送的文件拖放到其他人的Mac头像上即可。 另一个就是利用系统的共享功能。打开“系统偏好设置”→“共享”，单击左侧栏的“文件共享”，在右侧区域配置即可。 78.删除程序 删除Mac上的程序有很多种方法，比如直接去应用程序文件夹下删除，用CleanApp删除，等等，今天介绍一个最好玩的。 打开Launchpad，按住option键，就会看到所有的程序图标都会像iOS图标那样晃动起来，单击图标左上角的叉，即可删除程序，和操作iOS一样。 79.command+上下方向键 这两个快捷键很多应用程序都支持，具体功能就是屏幕滚动到应用程序的顶部或底部，类似很多网站提供的“回到顶部/底部”功能。Safari、Chrome、Firefox、Pages、Evernote等默认都支持这样的功能。 在使用快捷键呼出Spotlight的时候，使用command+上下方向键还可以在搜索分组之间切换，非常方便。 80.Mac上的阅读笔记类软件 ◆Kindle for Mac：支持视网膜屏，支持本地阅读和Amazon商店，支持中英文字典，电子阅读体验一流。遗憾的是不能整合中国和美国Amazon的账户，导致电子书商品也没法使用同一个账户阅读。免费。◆Caffeinated：优秀的RSS阅读器，如果你还喜欢博客和传统阅读，那么推荐使用。◆Pocket：免费。最好的稍后读App，支持标签分类、编辑等功能，支持afari、Chrome等插件，非常适合知识积累。收费。◆Evernote：很好的笔记类App，5.0之后UI有了很大的改进，目前我所有的文章都是用Evernote管理。免费，有收费版本。以上4个App在iPad、iPhone上也有相关应用，并且都支持云同步，合理使用对提高读写效率非常有帮助。 81.查看电源状况 按住option键，单击右上角的苹果图标，选择“系统信息”，在打开窗口的左侧栏中找到“电源”，单击即可查看电源的详细信息。主要的指标包括电池循环计数、状况等信息。如果您安装了Alfred，呼出后直接输“sys”，也可以找到系统信息。 如果想简单查看一下电池的使用状况，按住option键单击顶部工具栏上的电池图标，可以显示电池使用状况。如果出现“尽快更换”、“修理电池”等信息，那么有可能是电池出了问题，建议先重置系统管理控制器（SMC），如何重置可以去Apple的官方支持网站查一下。还没效果的话，可能就需要换电池了。 82.Pixelmator 这款图像处理软件号称是Mac上的精简版Photoshop，而且更为人性化，适合非专业人士使用，不是平面设计人员也可以作出非常专业的图像设计。MacTalk里很多配图我都使用这款软件加工过，很好用。收费软件，但值得拥有。推 荐 一 个Podcast视 频 教 程（RSS）：http://www.pixelmator.com/tutorials/itunes/。 83.搜索命令mdfind mdfind是一个非常灵活的全局搜索命令，类似Spotlight的命令行模式，可以在任何目录对文件名、文件内容进行检索，例如： 12345678//搜索文件内容或文件名包含“苹果操作系统”的文件mdfind苹果操作系统//在桌面上搜索文件内容或文件名包含“苹果操作系统”的文件mdfind -onlyin ～/Desktop苹果操作系统//统计搜索到的结果mdfind -count -onlyin ～/Desktop苹果操作系统//搜索文件名包含“苹果操作系统”的文件mdfind -name苹果操作系统 84.元信息命令mdls mdls可以列出某个文件或文件夹的所有元数据信息，针对不同文件显示不同的元数据信息，例如文件创建时间、类型、大小等。如果是图片或音视频文件，则会显示更多元数据信息。使用方式非常简单： 1mdls ～/Desktop/a.jpg 如果想查看图片的ISO数据，可以使用如下命令： 1mdls ～/Desktop/a.jpg|grep ISO 85.功能键 很多程序员在调试程序的时候总会用到F7、F8这些键，但在OS X里这些功能键默认分配了一些功能，想使用的话需要同时按fn+F8…… 如果希望将F1～F12这些按键用作标准功能键而且不需要按fn，可以执行以下操作：打开“系统偏好设置”→“键盘”，选中“将F1、F2等键用作标准功能键”。启用此选项时，顶部一行按键将用作标准功能键（F1～F12），而不执行音量控制等特殊功能。启用此选项后，若要使用这些按键的特殊功能，请按fn，比如按fn+F8来播放音乐。 86.查看文件信息的命令：file file可以查看相关文件的类型和属性，相对于mdls，这个更亲民一些，基本用法file xxx.png，大家感受一下。 87.如何配置多种网络环境 我自己无论在公司还是家里都是DHCP自动分配IP，所以不需要进行网络环境切换。但有些用户有时自动有时手动，需要多套网络配置方案，每次修改实在是太麻烦了。曾经有人问我Mac上是否有这样的第三方软件？我说没有，因为OS X的网络设置本身就提供了这样的功能。 打开“系统偏好设置”→“网络”，单击“位置”下拉菜单，找到“编辑位置”，打开后即可增删编辑多套网络设置，设置完成后保存。 这时单击屏幕左上角的苹果图标，在下拉菜单里增加了一个位置选项，里面就是你配置好的多种网络设置，单击切换即可。 88.生成man page的PDF文档 打开OS X的终端，通过man命令可以直接查看该命令的使用手册。但有时我们会觉得在命令行查看不太方便，如果可以提供一个PDF文档就完美了。这很容易做到，在终端输入如下命令，即可在预览程序打开grep的使用手册，另存为你需要的文件名即可： 1man -t grep | open -f -a Preview 89.虚拟机 2006年Mac的硬件进行了重大的架构调整，开始全面采用Intel系列的CPU，Power渐行渐远。架构的调整和Bootcamp的推出，使得在Mac上安装双系统变得触手可及。基于Mac的虚拟机应用也开始出现。我刚开始使用Mac时是双系统的支持者，后来Windows用得越来越少，就比较推荐使用虚拟机了。 在OS X上主要有三款虚拟机软件：Parallels Desktop、Vmware Fusion和VirtualBox。简单给大家介绍一下。 ◆Parallels Desktop：Parallels是OS X上一款优秀的虚拟机软件，最新版本是8。它支持多种操作系统，并对Windows有完美的支持。通过融合模式，可以让Windows程序运行起来象Mac的应用。提供把Vmware Fusion虚拟机迁移到PD上的功能。收费。 ◆Vmware Fusion：Vmware在Windows和Linux下大名鼎鼎，Fusion是Mac版本，功能同样强大。收费。 ◆VirtualBox：Sun推出的一款开源虚拟机，现在归Oracle了，未来走势不明。免费。 我个人首推Parallels Desktop，功能、性能和价格都不错，专注于桌面版，属上乘之选。我自己虚拟了Win7、Redhat Linux和Ubuntu等环境，作软件测试和搭建多机开发环境。 90.如何开启root用户 用过Linux/Unix系统的都知道root用户，它具备读写文件系统所有区域的特权，是最高级别的用户。OS X一样有root用户，只不过默认情况是不开启的。我们想在命令行执行需要root权限的操作时，可以在命令之前增加sudo指令，比如执行每日维护指令：sudo periodic daily，系统会提示你输入用户密码，执行root权限。在GUI（图形界面）执行root级别的命令时也会提示输入用户密码。一般情况下我们是不需要开启root用户的。 用惯了Linux系统的用户有时很想启用root用户，其实也很简单，打开Finder，输入shift+command+g，在前往文件夹中输入：/System/Library/CoreServices，然后在目录中找到“目录实用工具”并打开，解开左下角的小锁，然后单击顶部菜单的“编辑”，你就会看到启用或停用root用户的选项了。然后我们在命令行下执行su-，就可以切换到root目录下，root的默认目录是/var/root。 root有风险，启用须谨慎！ 91.隐藏的空间切换功能 以前介绍过OS X中Space的使用，我们可以定义多个Space，每个程序都可以在特定的Space中打开，多手势上推下滑选择程序，也可以通过ctrl+数字切换Space，很方便。今天再为大家介绍一个隐藏的功能，就是通过四指双击触控板，可以在你最近使用的两个Space之间切换，这个功能就类似电视频道中的返回功能，当你使用了Space1中的一些App，切换到了Space4之后，就可以通过四指双击在Space1和Space4之间切换了，对于协同工作非常有效。典型的应用场景：Space1里编码，在Space4里参考各类文档。 功能开启，打开终端程序，输入： 12defaults write com.apple.dock double-tap-jump-back -bool TRUE; #功能开启killall Dock; #重启Dock 92.免费的文本编辑器Imagine 我个人觉得Imagine比OS X自带的TextEdit好，除了目前不支持iCloud外，基本涵盖了TE的功能，而且排版简约美观，可更换柔和的背景色，全屏写字非常舒服，对字体样式的支持很好，在富文本和纯文本间切换方便，我基本用Imagine替代了TextEdit。 下载地址：https://itunes.apple.com/cn/app/imagine/id566877440?mt=12。 93.去除右键菜单的重复项 OS X系统有个问题，某个程序反复安装后，选中某种类型的文件，单击右键→“打开方式”，你会看到不少重复的选项，我们可以用以下命令的除重复项。 1/System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/LaunchServices.framework/Versions/A/Support/lsregister -kill -r -domain local -domain system -domain user 94.如何分别设置Mac的鼠标和触控板的滚动方向 很多人习惯鼠标使用相反的滚动方向，而触控板类似iPad那样自然滚动，问如何设置，当时我的回答是不知道，因为目前OS X的系统设置里，鼠标和触控板的设置是统一的。今天发现了一个免费的软件Scroll Reverser，可以实现鼠标和触控板的分别设置。 下载地址：http://www.macupdate.com/app/mac/37872/scroll-reverser。 启动后程序显示在顶部菜单栏，设置简单明了，有需要的用户体验一下吧。 95.如何让不支持Retina的Mac软件变成Retina App 前两天有读者求推荐Mac下的FTP软件，我推荐了FileZilla，但这个软件是不支持Retina屏的，Retina用户使用这个软件会感觉整个世界都模糊了，结果搜索之下，发现了一个小软件，叫做Retinizer，顾名思义，就是把非Retina的软件Retina化，我用了一下，完美支持FileZilla。下载地址：http://retinizer.mikelpr.com/。 96.文件比较 ◆对于单个文件的比较，一般使用diff或vimdiff就可以了，比如：vimdiff destfile.txt sourcefile.txt vim会非常清晰地显示出文件的不同，还有很多快捷方式帮助你查看和操作文件，这个命令比较适合命令行爱好者。 ◆对于大批量文件的比较，还是图形化比较工具更合适一些。OS X自带了FileMerge比较工具，可以满足部分需求，但对于中文编码文件或大文件经常会崩溃，很奇怪Apple一直不解决这个问题。 ◆推荐一款收费软件：VisualDiffer（25元），UI、功能和稳定性都非常不错，实在是居家旅行、代码比较、查找问题的必备利器。 97.FTP工具Cyberduck 之前介绍Retinizer（普通软件Retina化）的时候提到了FTP软件FileZilla，我个人一般使用命令行下的ftp/sftp/scp等实现FTP软件的功能，但普通用户还是用图形界面的更方便些。今天再给大家介绍一个可以实现远程同步文件的FTP工具：Cyberduck。Cyberduck除了可以实现FTP的基本功能外，还能支持远程同步。所谓同步，就是把远程和本地的两个目录进行比较，然后自动找出修改的文件上传到服务器。 具体操作就是通过ftp或sftp的方式登入远端服务器，选中某个文件夹，右键菜单里选择“同步”，再选择本地文件夹，就可以进行同步比较上传了，上传之前你最好确认下，更稳妥。 同样，这个软件也可以用Retinizer实现高清显示效果。 98.文件重命名 文件重命名的问题以前说过，但最近又有些用户问起，就再说一下。如果你没有装任何插件的话，在Finder中重命名文件或文件夹的快捷键就是回车。打开文件用command+O，返回上级目录用command+向上的方向键。如果你装了原来推荐过的XtraFinder，可以把回车改为打开文件（与Windows操作类似），把option+R设置为文件重命名。如果你在命令行下重命名文件，命令是这样的： 1mv oldname newname 99.多个用户登录一个程序 Mac下有很多程序默认是单进程的，比如你不能打开多个邮件程序，不能打开多个Evernote，但有时我们可能会有这样的需求，那么用如下命令可以实现： 1open -n /Applications/XXX.app -n的含义是：Open a new instance of the application（s）even if one is already running.意思就是为正在运行的应用程序再开一个新实例。常用于多个账户登录一个程序，或软件比较等场景。 100.强制关闭程序 总有程序关闭不了，这时候我们就需要以下方法。 方法一：option+command+esc，调出强制退出应用程序的窗口，选择要退出的进程即可。 方法二：打开活动监视器，类似windows的任务管理器一样操作就好了。 方法三：命令行下的kill命令，比如想杀掉TextMate，首先用ps-ax|grep TextMate找到进程号，然后用kill-9进程号，即可。 至此，天下无杀不掉的进程。 101.用AppleScript实现打开多实例程序 之前介绍了通过open-n/Applications/XXX.app的方式打开多实例程序，有人在微博上问如何选中一个文件或程序，通过右键菜单打开新实例，而不是每次都去命令行操作。 我们可以通过Automator+AppleScript实现这个功能。打开Automator，选择创建“服务”，在左侧选择“运行AppleScript”，双击打开程序窗口，在“（Yourscriptgoeshere）”处输入如下代码： 123456789tell application &quot;Finder&quot; try set filename to POSIX path of（selection as text） set fileType to（do shell script &quot;file -b &quot; &amp; filename） if（fileType does not end with &quot;directory&quot;）or（filename end with &quot;app&quot;）then do shell script &quot;open -n &quot; &amp; filename end if end tryend tell 在程序上方的选择框设定“文件和文件夹”、“任何应用程序”，然后保存，起个你喜欢的名字，比如叫“以新实例运行”。退出Automator。选中文件或程序，右键→“服务”→“以新实例运行”，即可实现类似open-n的方式。 102.Automator Automator是苹果公司为其操作系统OS X开发的一款软件。通过单击拖曳鼠标等操作就可以将一系列动作组合成一个工作流，从而帮助你自动完成一些复杂的重复工作。Automator还能横跨很多不同种类的程序，包括：查找器、Safari网络浏览器、iCal、地址簿或者其他的一些程序。在Automator中可以运行AppleScript。 在上一个技巧中我们通过Automator创建了一个服务，当你在Finder或桌面上选中文件时，在右键的“服务”菜单里增加了一个选项“以新实例运行”，是通过AppleScript实现的。下面说明一下程序功能： 1234567891011121314--通知Findertell application &quot;Finder&quot;--异常处理try --获取选中文件的全路径 set filename to POSIX path of（selection as text） --通过脚本file -b获取文件类型 set fileType to（do shell script &quot;file -b &quot; &amp; filename） --如果不是文件夹或以app结尾，执行open -n脚本 if（fileType does not end with &quot;directory&quot;）or（filename ends with &quot;app&quot;）then do shell script &quot;open -n &quot; &amp; filename end if end tryend tell 这里考虑到了选中程序直接打开，或选中文件以默认程序打开的情况。 103.用Safari默认查询引擎查询应用软件里的文字 如果你想通过Safari的默认查询引擎查询某个应用软件里的文字，那么选中文字，然后使用shift+command+L即可跳转到Safari的搜索页面，非常方便。大部分应用都支持这个快捷键。 104.旋转屏幕 打开“系统偏好设置”，已经打开了的，退出重新打开。按住option+command键，单击显示器，在原来的亮度选项下方会出现一个“旋转”的选项，这时候你就可以旋转你的屏幕了。 105.最近尝试录制视频时在屏幕上显示键盘快捷键的操作， ScreenFlow固然可以实现这个功能，不过99美元的价格让人感觉得不偿失。搜索之下找到了KeyCastr，简单设置了一下发现可以实现我需要的功能，项目托管在GitHub上，网址：https://github.com/sdeken/keycastr, 可以直接下载DMG包。还有一种方案是使用OS X原生的键盘显示，打开“系统偏好设置”→“语言与文本”→“输入源”，选中左边栏的第一项“键盘与字符显示程序”。关闭偏好设置，这时单击顶部Menu Bar的“语言”，会多出两项功能，单击“显示键盘显示程序”，就会在屏幕上出现一个模拟键盘。 这个方案的缺点是没法区分快捷键和普通字符输入，而且显示速度太快，不够醒目。 106.复制截屏图片到剪贴板 以前介绍过如何通过苹果自带的快捷键截屏并存储图片文件，例如shift+command+3和shift+command+4。现在发现如果在以上两个截屏动作中加入ctrl键，可以实现直接把图片保存在剪贴板中，而不是实体文件中，这样就可以通过command+v直接把截取的图片内容复制到图像处理软件或Pages、Keynote等文件中了。 107.CheatSheet 一生要记住多少快捷键呢？我都不知道自己记住了多少快捷键，很多快捷键是到了那个环境下才能想起来。但是毋庸置疑，快捷键可以大大提高我们的工作效率，在Mac环境下使用快捷键和不使用，几乎是两种体验。如何记住这些快捷键呢，有人开发了一款软件叫做CheatSheet，安装并打开之后，当你记不住快捷键的时候，按住command键两秒钟，就会弹出一个当前应用软件快捷键列表，不全，但是对大部分用户都够用了。 下载地址：http://www.cheatsheetapp.com/CheatSheet/ 108.HTML5Player 现在越来越多的人开始看在线视频，目前大部分视频网站的播放器都是基于Flash技术的，而苹果一直对Flash很抵触，支持得也不好，Flash播一会Mac机身就会变热。另外现在的视频网站广告太多，页面花里胡哨也不适合观看。于是有位无聊的程序员做了一个HTML5播放器，可以把在线视频的播放转化成HTML5方式，并且去除广告。使用起来非常简单，只要把{原文}里的HTML5Player链接拖曳到Safari的书签栏，播放视频时单击书签栏上的HTML5Player书签，播放器就会自动转换，效果自己看吧。目前支持优酷、土豆、搜狐视频、爱奇艺、乐视网、QQ、迅雷离线，以及56视频的单视频播放页面。 相关链接：http://zythum.sinaapp.com/youkuhtml5playerbookmark/ 109.重建Spotlight索引 以前给大家介绍过，在OS X中几乎不需要进行文档和文件夹管理，因为有Spotlight机制，可以瞬间找到你想要的文件，只要你记得这个文件的一点蛛丝马迹。 但是Spotlight也有出问题的时候，就是它的索引文件出事了，比如查找速度变慢，某些文件明明在硬盘上就是检索不到，等等，这时候就需要重建索引了。 打开终端程序，输入如下命令： 123456sudo mdutil -i off /#该命令用来关闭索引sudo mdutil -E /#该命令用来删除索引sudo mdutil -i on /#该命令用来重建索引 然后用快捷键呼出Spotlight菜单，随便输入一个词，就能看到提示，正在进行索引，并且显示完成重建索引需要的时间。完成之后，Spotlight又可以运转如飞了。 有时候人在某个阶段也需要重建索引，保持初心。什么是初心，空空如也！不要成天得瑟你知道的那点事，多琢磨那些你还不知道的事儿。 110.用键盘操作Dock和Menu Bar的菜单 当我们想操作Dock或顶部菜单栏的时候，往往需要鼠标去选中Dock或菜单栏，但是我们往往是不希望去碰鼠标的，这时候快捷键就又开始发挥作用了。使用control+F2可以选中Menu Bar的菜单，通过左右键选择功能，回车执行；使用control+F3可以选中并显示Dock，通过左右键选择功能，回车执行。 该功能在全屏操作时尤其有效。对于F1、F2等不是标准功能键的设置，增加fn键即可。 111.定义自己的快捷键 我认为OS X是一个把GUI、程序进程和脚本结合得最好的操作系统，当然这样说可能有些读者不是很明白，这么说吧，OS X是一个定制化非常强的系统，很多人说OS X封闭，事实上OS X为用户预留了非常多的入口和切面，让你能够通过简单、简洁的办法进入系统做你想做的事情。举例来说，对于普通用户，你可以通过键盘的快捷键设置定义自己的常用操作。对于程序员，你可以自己通过AppleScript、Shell、Automator等创建自己的服务，也可通过类似Alfred2这样的优秀工具编写自己的workflow。 今天给大家说说第一种。打开“系统偏好设置”→“键盘”→“键盘快捷键”，左侧栏里列出了各种功能的快捷键，比如Launchpad、Dock、Mission Control、截屏、服务等，大家可以在这些选项中定义和修改自己常用的快捷键，增加右键菜单，等等。 112.选择文本 用command+鼠标，可以选中不同位置的文本内容。用option+鼠标，可以对文本进行块选。 113.Dock中的文件夹 这个功能非常适合普通用户使用。一般安装了系统后Dock右边会有几个默认的文件夹，事实上你可以把任何常用的文件夹拖到这个位置，不想要的拖到废纸篓即可移除。Dock文件夹的显示方式提供了扇状、网格和列表三种方式，我一般使用网格和列表。但是还有一个隐藏的列表功能，更为实用些，可以在命令行输入如下命令开启： 1defaults write com.apple.dock use-new-list-stack -bool TRUE; killall Dock 这时候你再启动列表模式，就会发现列表显示方式不一样了，变得更加容易操作了。另外，在列表和网格模式下，还可以通过command+/-来放大和缩小图标，非常方便。 114.Finder的宽度 Finder是OS X的默认文件管理器，它提供了多种显示方式，包括图标、列表、分栏和Cover Flow。其中分栏最为常用，通过键盘的方向键浏览多层级的文件非常方便。不过每个分栏的宽度都是系统默认宽度，如何改变这个默认宽度呢？用鼠标拖动分栏线时同时按住option键，这个默认宽度就随之改变了。 115.Dashboard 顾名思义，Dashboard就是OS X系统中的仪表盘，它可以在桌面上显示各种小功能块，比如字典、便笺、系统状态、天气预报等。使用快捷键F12或单击Dock中的Dashboard可以运行Dashboard，运行方式有两种，可以在一个新的Space里打开，也可以在当前的Space里打开，可以在“偏好设置”→“Mission Control”中设置。我一般使用在当前Space里打开。 单击左下角的“+”号，可以为Dashboard添加功能块，“-”号可以删除已经添加的功能块。把鼠标移动到某个功能块时按住option键，该功能块会出现一个删除图标，单击也可删除。 如果你想添加更多的功能块，在单击“+”号时，右侧会显示更多的Widget，单击可以到网络上下载你需要的功能。 Dashboard还有一个Web Clip的功能，如果你添加了这个功能块，浏览网页看到特别喜欢的词句或图片，可以单击右键→“在Dashboard打开”，把这部分内容放入Dashboard。 116.Dock文件夹的使用小技巧 前面介绍过Dock文件夹的使用问题，再说一个小技巧。当我们打开Dock文件夹后，先打开某个文件所在文件夹时，按住command，单击该文件，就会打开Finder文件夹，并选中你刚才单击的文件。 117.几个简单的命令 介绍几个简单的命令。打开终端程序，输入date会显示当前日期，输入cal会显示日历，输入uptime会显示系统从开机到现在所运行的时间。 118.神奇的option键 前面介绍过option相关的快捷键和功能，比如选中多个文件，然后按option+右键，可以显示检查器，按住option单击顶部菜单的电池图标会显示电池状况，单击Wi-Fi图标会显示网络状况，单击备份……可以自己试试。别忘了最左边的苹果按钮，option+单击，在下拉菜单击关机、重启都是不提示的。 option+拖曳文件可以复制，按住option输入“=”，输出是“≠”，按住option和shift输入“=”，输出是“±”。 还有好多，没事的时候多按按option键，你会有很多意外的发现。 119.音乐处理软件XLD XLD全程是X Lossless Decoder，是Mac平台上无损音乐播放、编码和转换工具，不仅支持APE、FLAC等无损音频，还支持读取音频CD，将音轨抓取出来之后创建音乐文件。 免费软件，喜欢的可以捐赠。 官网地址：http://tmkk.undo.jp/xld/index_e.html。 120.保护你的数据文件 在Mac下对某些文件或数据进行加密操作有两种方式： 第一种：“系统偏好设置”→“安全性与隐私”→“FileVault”→“打开FileVault…”，即可。FileVault是全盘加密技术，可以对磁盘上的所有文件进行加密，后果是系统速度会稍微变慢一点点，一般不建议采用。 第二种：创建磁盘映像文件，对磁盘映像进行加密处理，然后把需要保护的数据和文件放到这个磁盘映像中即可。具体方式如下：打开“应用程序”→“实用工具”→“磁盘工具”，单击“新建映像”，在“加密”选项处选择“256位AES加密（更安全但速度较慢）”，这种加密算法是极其安全的。创建映像时输入两次密码，即可创建加密的磁盘映像文件。在创建时最好不要选择“在我的钥匙串中记住密码”，这样可以每次打开这个磁盘映像文件时都需要输入密码，可以达到最佳保护数据的作用。 121.如何禁用通知 很多时候写作或写代码时，不希望被打扰，这时候就需要把OX S的通知关掉。双指从触控板右侧滑入，呼出“通知中心”，在最顶部有一个“显示提示和横幅”的开关，关掉就会禁止通知，不过第二天会自动恢复这个通知设置。 更简单的做法是按住option键单击屏幕右上角的“通知”图标。 122.Finder的工具栏 我们可以把文件和程序拖到Finder的工具栏上，以便随时打开。但是想移除时会发现单击鼠标拖动是没法把这些图标移除的，这时候只要在单击拖动时加上command，你就会发现这些图标被销毁了。 123.Spotlight搜索时文件的定位 用Spotlight搜索的时候，搜到文件时，我们有时候会需要打开该文件所在的文件夹，这时候按住command*键，单击文件即可打开Finder，并定位到该文件所在文件夹。 124.重新启动Finder Finder是OS X系统中的常驻程序，一般不需要退出，如果想重新启动Finder，有一个简单的方式：按住option键，右键单击Dock上的Finder图标，底部菜单会出现“重新开启”的选项，单击即可。同样的操作对其他Dock上的程序是强制退出。 125.屏幕画中画 之前介绍过屏幕放大功能，也就是通过option+command++/-可以放大和缩小屏幕，使用control+滚轮也可以。 不过这只是放大屏幕方式的一种表现形式，我们还可以通过辅助设置改为画中画模式，打开“系统偏好设置”→“缩放”→“缩放样式”，把全屏幕改为画中画即可，效果大家自己看吧。 126.粘贴纯文本 我们在网页或其他文档上复制文字的时候，会把文字格式一并复制下来，command+v会把文字格式都粘贴过去，如果我们只想粘贴纯文本，可以使用shift+option+command+v，大部分软件都支持这种方式复制纯文本。 127.终端命令lsof 有用户问，在倾倒废纸篓的时候，经常会提示该文件还在使用，不能删除，但是又不知道哪个程序在用，怎么办？ Unix下有一个命令叫做lsof，名字是list open files的缩写，顾名思义，就是查看打开的文件，在终端里输入lsof文件名，就可以找到打开这个文件的程序。关掉程序，就可以正常删除文件。当然lsof还有很多丰富的指令，感兴趣的用户自行Google吧。 128.AirDrop的有线传输 AirDrop默认只能通过Wi-Fi来传文件，如果电脑已经连了网线，但是没开Wi-Fi就不能用AirDrop了，有一个办法可以打开AirDrop通过有线传文件的特征。打开终端输入： 1defaults write com.apple.NetworkBrowser BrowseAllInterfaces 1 然后选中Dock栏的Finder，按住option键，然后右键单击Finder图标，单击底部菜单项“重新开启”，Finder重启之后，即使你的电脑没开Wi-Fi，也可以用AirDrop给别人分享传文件了。 129.切换程序时实现预览功能 通过command+tab可以实现程序之间的切换，如果我们想在切换到某个程序的时候看看该程序组都在显示什么，可以按住command的同时按数字键1或上下方向键，系统会调出该程序的Exposé模式，这时你可以放开所有按键，用鼠标或方向键选择显示哪个程序窗口。 130.Spotlight检索的高级技巧 ◆通过文件类型搜索文件，搜索格式是：kind:文件类型——搜索关键字比如： 123456789101112131415kind:app——搜索应用程序kind:bookmark——搜索书签和历史记录kind:contact——搜索联系人kind:document——搜索各类文档kind:word——搜索wordkind:pages——搜索pageskind:key——搜索keynotekind:email——搜索emailkind:event——搜索日历事件kind:folder——搜索文件夹kind:movies——搜索视频kind:music——搜索音乐kind:pdf——搜索pdf文件kind:pic——搜索图片…… ◆通过标签颜色搜索。如果你喜欢使用各种颜色的标签标注不同的文件夹，那么这个功能就用得上了。“label:红”就可以找到红色标签的文件和文件夹。◆通过日期搜索。 123date:today——查看今天创建或修改的文件date:yesterday——查看昨天创建或修改的文件date:2013-05-01——查看2013年5月1日创建或修改的文件 ◆条件表达式。想搜索包含Mac不包含Windows的Keynote，可以这样写： 1kind:key Mac -Windows 也可以这样写： 1kind:key Mac NOT Windows 我们可以使用+或-进行条件表达式求值，也可以通过NOT、AND和OR来检索，不过后者一定要大写，否则会被当做搜索内容处理。有了以上4种搜索方式，天下再无搜不到的文档！]]></content>
      <categories>
        <category>日知录</category>
      </categories>
      <tags>
        <tag>macbook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（40）：机器学习中的数据清洗与特征处理综述]]></title>
    <url>%2F2017%2F09%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8840%EF%BC%89%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E4%B8%8E%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[一、背景随着美团交易规模的逐步增大，积累下来的业务数据和交易数据越来越多，这些数据是美团做为一个团购平台最宝贵的财富。通过对这些数据的分析和挖掘，不仅能给美团业务发展方向提供决策支持，也为业务的迭代指明了方向。目前在美团的团购系统中大量地应用到了机器学习和数据挖掘技术，例如个性化推荐、筛选排序、搜索排序、用户建模等等，为公司创造了巨大的价值。本文主要介绍在美团的推荐与个性化团队实践中的数据清洗与特征挖掘方法。主要内容已经在内部公开课”机器学习InAction系列”讲过，本博客的内容主要是讲座内容的提炼和总结。 二、综述如上图所示是一个经典的机器学习问题框架图。数据清洗和特征挖掘的工作是在灰色框中框出的部分，即“数据清洗=&gt;特征，标注数据生成=&gt;模型学习=&gt;模型应用”中的前两个步骤。灰色框中蓝色箭头对应的是离线处理部分。主要工作是 从原始数据，如文本、图像或者应用数据中清洗出特征数据和标注数据。 对清洗出的特征和标注数据进行处理，例如样本采样，样本调权，异常点去除，特征归一化处理，特征变化，特征组合等过程。最终生成的数据主要是供模型训练使用。 灰色框中绿色箭头对应的是在线处理的部分。所做的主要工作和离线处理的类似，主要的区别在于1.不需要清洗标注数据，只需要处理得到特征数据，在线模型使用特征数据预测出样本可能的标签。2.最终生成数据的用处，最终生成的数据主要用于模型的预测，而不是训练。在离线的处理部分，可以进行较多的实验和迭代，尝试不同的样本采样、样本权重、特征处理方法、特征组合方法等，最终得到一个最优的方法，在离线评估得到好的结果后，最终将确定的方案在线上使用。另外，由于在线和离线环境不同，存储数据、获取数据的方法存在较大的差异。例如离线数据获取可以将数据存储在Hadoop，批量地进行分析处理等操作，并且容忍一定的失败。而在线服务获取数据需要稳定、延时小等，可以将数据建入索引、存入KV存储系统等。后面在相应的部分会详细地介绍。 本文以点击下单率预测为例，结合实例来介绍如何进行数据清洗和特征处理。首先介绍下点击下单率预测任务，其业务目标是提高团购用户的用户体验，帮助用户更快更好地找到自己想买的单子。这个概念或者说目标看起来比较虚，我们需要将其转换成一个技术目标，便于度量和实现。最终确定的技术目标是点击下单率预估，去预测用户点击或者购买团购单的概率。我们将预测出来点击或者下单率高的单子排在前面，预测的越准确，用户在排序靠前的单子点击、下单的就越多，省去了用户反复翻页的开销，很快就能找到自己想要的单子。离线我们用常用的衡量排序结果的AUC指标，在线的我们通过ABTest来测试算法对下单率、用户转化率等指标的影响。 三、特征使用方案在确定了目标之后，下一步，我们需要确定使用哪些数据来达到目标。需要事先梳理哪些特征数据可能与用户是否点击下单相关。我们可以借鉴一些业务经验，另外可以采用一些特征选择、特征分析等方法来辅助我们选择。具体的特征选择，特征分析等方法我们后面会详细介绍。从业务经验来判断，可能影响用户是否点击下单的因素有： 距离，很显然这是一个很重要的特征。如果购买一个离用户距离较远的单子，用户去消费这个单子需要付出很多的代价。 当然，也并不是没有买很远单子的用户，但是这个比例会比较小。 用户历史行为，对于老用户，之前可能在美团有过购买、点击等行为。用户实时兴趣。 单子质量，上面的特征都是比较好衡量的，单子质量可能是更复杂的一个特征。 是否热门，用户评价人数，购买数等等。 在确定好要使用哪些数据之后，我们需要对使用数据的可用性进行评估，包括数据的获取难度，数据的规模，数据的准确率，数据的覆盖率等， 数据获取难度：例如获取用户id不难，但是获取用户年龄和性别较困难，因为用户注册或者购买时，这些并不是必填项。即使填了也不完全准确。这些特征可能是通过额外的预测模型预测的，那就存在着模型精度的问题。 数据覆盖率：数据覆盖率也是一个重要的考量因素，例如距离特征，并不是所有用户的距离我们都能获取到。PC端的就没有距离，还有很多用户禁止使用它们的地理位置信息等。 用户历史行为，只有老用户才会有行为。 用户实时行为，如果用户刚打开app，还没有任何行为，同样面临着一个冷启动的问题。数据的准确率 单子质量，用户性别等，都会有准确率的问题。 四、特征获取方案Ok，在选定好要用的特征之后，我们需要考虑一个问题。就是这些数据从哪可以获取？只有获取了这些数据我们才能用上。否则，提一个不可能获取到的特征，获取不到，提了也是白提。下面就介绍下特征获取方案。 离线特征获取方案：离线可以使用海量的数据，借助于分布式文件存储平台，例如HDFS等，使用例如MapReduce，Spark等处理工具来处理海量的数据等。 在线特征获取方案：在线特征比较注重获取数据的延时，由于是在线服务，需要在非常短的时间内获取到相应的数据，对查找性能要求非常高，可以将数据存储在索引、kv存储等。而查找性能与数据的数据量会有矛盾，需要折衷处理，我们使用了特征分层获取方案，如下图所示。出于性能考虑。在粗排阶段，使用更基础的特征，数据直接建入索引。精排阶段，再使用一些个性化特征等。 五、特征与标注数据清洗在了解特征数据放在哪儿、怎样获取之后。下一步就是考虑如何处理特征和标注数据了。下面3节都是主要讲的特征和标注处理方法 5.1 标注数据清洗首先介绍下如何清洗特征数据，清洗特征数据方法可以分为离线清洗和在线清洗两种方法。 离线清洗数据：离线清洗优点是方便评估新特征效果，缺点是实时性差，与线上实时环境有一定误差。对于实时特征难以训练得到恰当的权重。 在线清洗数据：在线清洗优点是实时性强，完全记录的线上实际数据，缺点是新特征加入需要一段时间做数据积累。 5.2 样本采样与样本过滤特征数据只有在和标注数据合并之后，才能用来做为模型的训练。下面介绍下如何清洗标注数据。主要是数据采样和样本过滤。 数据采样，例如对于分类问题：选取正例，负例。对于回归问题，需要采集数据。对于采样得到的样本，根据需要，需要设定样本权重。当模型不能使用全部的数据来训练时，需要对数据进行采样，设定一定的采样率。采样的方法包括随机采样，固定比例采样等方法。 除了采样外，经常对样本还需要进行过滤，包括 1.结合业务情况进行数据的过滤，例如去除crawler抓取，spam，作弊等数据。 2.异常点检测，采用异常点检测算法对样本进行分析，常用的异常点检测算法包括偏差检测，例如聚类，最近邻等。 基于统计的异常点检测算法 例如极差，四分位数间距，均差，标准差等，这种方法适合于挖掘单变量的数值型数据。全距(Range)，又称极差，是用来表示统计资料中的变异量数(measures of variation) ，其最大值与最小值之间的差距；四分位距通常是用来构建箱形图，以及对概率分布的简要图表概述。 基于距离的异常点检测算法，主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，主要使用的距离度量方法有绝对距离 ( 曼哈顿距离 ) 、欧氏距离和马氏距离等方法。 基于密度的异常点检测算法，考察当前点周围密度，可以发现局部异常点，例如LOF算法 六、特征分类在分析完特征和标注的清洗方法之后，下面来具体介绍下特征的处理方法，先对特征进行分类，对于不同的特征应该有不同的处理方法。 根据不同的分类方法，可以将特征分为(1)Low level特征和High level特征。(2)稳定特征与动态特征。(3)二值特征、连续特征、枚举特征。 Low level特征是较低级别的特征，主要是原始特征，不需要或者需要非常少的人工处理和干预，例如文本特征中的词向量特征，图像特征中的像素点，用户id，商品id等。Low level特征一般维度比较高，不能用过于复杂的模型。High level特征是经过较复杂的处理，结合部分业务逻辑或者规则、模型得到的特征，例如人工打分，模型打分等特征，可以用于较复杂的非线性模型。Low level 比较针对性，覆盖面小。长尾样本的预测值主要受high level特征影响。 高频样本的预测值主要受low level特征影响。 稳定特征是变化频率(更新频率)较少的特征，例如评价平均分，团购单价格等，在较长的时间段内都不会发生变化。动态特征是更新变化比较频繁的特征，有些甚至是实时计算得到的特征，例如距离特征，2小时销量等特征。或者叫做实时特征和非实时特征。针对两类特征的不同可以针对性地设计特征存储和更新方式，例如对于稳定特征，可以建入索引，较长时间更新一次，如果做缓存的话，缓存的时间可以较长。对于动态特征，需要实时计算或者准实时地更新数据，如果做缓存的话，缓存过期时间需要设置的较短。 二值特征主要是0/1特征，即特征只取两种值：0或者1，例如用户id特征：目前的id是否是某个特定的id，词向量特征：某个特定的词是否在文章中出现等等。连续值特征是取值为有理数的特征，特征取值个数不定，例如距离特征，特征取值为是0~正无穷。枚举值特征主要是特征有固定个数个可能值，例如今天周几，只有7个可能值：周1，周2，…，周日。在实际的使用中，我们可能对不同类型的特征进行转换，例如将枚举特征或者连续特征处理为二值特征。枚举特征处理为二值特征技巧：将枚举特征映射为多个特征，每个特征对应一个特定枚举值，例如今天周几，可以把它转换成7个二元特征：今天是否是周一，今天是否是周二，…，今天是否是周日。连续值处理为二值特征方法：先将连续值离散化（后面会介绍如何离散化)，再将离散化后的特征切分为N个二元特征，每个特征代表是否在这个区间内。 6.1 特征归一化，离散化，缺省值处理主要用于单个特征的处理。 归一化不同的特征有不同的取值范围，在有些算法中，例如线性模型或者距离相关的模型像聚类模型、knn模型等，特征的取值范围会对最终的结果产生较大影响，例如二元特征的取值范围为[0，1]，而距离特征取值可能是[0，正无穷)，在实际使用中会对距离进行截断，例如[0，3000000]，但是这两个特征由于取值范围不一致导致了模型可能会更偏向于取值范围较大的特征，为了平衡取值范围不一致的特征，需要对特征进行归一化处理，将特征取值归一化到［0，1］区间。常用的归一化方法包括1.函数归一化，通过映射函数将特征取值映射到［0，1］区间，例如最大最小值归一化方法，是一种线性的映射。还有通过非线性函数的映射，例如log函数等。2.分维度归一化，可以使用最大最小归一化方法，但是最大最小值选取的是所属类别的最大最小值，即使用的是局部最大最小值，不是全局的最大最小值。3.排序归一化，不管原来的特征取值是什么样的，将特征按大小排序，根据特征所对应的序给予一个新的值。 离散化在上面介绍过连续值的取值空间可能是无穷的，为了便于表示和在模型中处理，需要对连续值特征进行离散化处理。常用的离散化方法包括等值划分和等量划分。等值划分是将特征按照值域进行均分，每一段内的取值等同处理。例如某个特征的取值范围为[0，10]，我们可以将其划分为10段，[0，1)，[1，2)，…，[9，10)。等量划分是根据样本总数进行均分，每段等量个样本划分为1段。例如距离特征，取值范围［0，3000000］，现在需要切分成10段，如果按照等比例划分的话，会发现绝大部分样本都在第1段中。使用等量划分就会避免这种问题，最终可能的切分是[0，100)，[100，300)，[300，500)，..，[10000，3000000]，前面的区间划分比较密，后面的比较稀疏。 缺省值处理有些特征可能因为无法采样或者没有观测值而缺失，例如距离特征，用户可能禁止获取地理位置或者获取地理位置失败，此时需要对这些特征做特殊的处理，赋予一个缺省值。缺省值如何赋予，也有很多种方法。例如单独表示，众数，平均值等。 6.2 特征降维在介绍特征降维之前，先介绍下特征升维。在机器学习中，有一个VC维理论。根据VC维理论，VC维越高，打散能力越强，可容许的模型复杂度越高。在低维不可分的数据，映射到高维是可分。可以想想，给你一堆物品，人脑是如何对这些物品进行分类，依然是找出这些物品的一些特征，例如：颜色，形状，大小，触感等等，然后根据这些特征对物品做以归类，这其实就是一个先升维，后划分的过程。比如我们人脑识别香蕉。可能首先我们发现香蕉是黄色的。这是在颜色这个维度的一个切分。但是很多东西都是黄色的啊，例如哈密瓜。那么怎么区分香蕉和哈密瓜呢？我们发现香蕉形状是弯曲的。而哈密瓜是圆形的，那么我们就可以用形状来把香蕉和哈密瓜划分开了，即引入一个新维度：形状，来区分。这就是一个从“颜色”一维特征升维到二维特征的例子。 那问题来了，既然升维后模型能力能变强，那么是不是特征维度越高越好呢？为什么要进行特征降维&amp;特征选择？主要是出于如下考虑：1. 特征维数越高，模型越容易过拟合，此时更复杂的模型就不好用。2. 相互独立的特征维数越高，在模型不变的情况下，在测试集上达到相同的效果表现所需要的训练样本的数目就越大。 3. 特征数量增加带来的训练、测试以及存储的开销都会增大。4.在某些模型中，例如基于距离计算的模型KMeans，KNN等模型，在进行距离计算时，维度过高会影响精度和性能。5.可视化分析的需要。在低维的情况下，例如二维，三维，我们可以把数据绘制出来，可视化地看到数据。当维度增高时，就难以绘制出来了。在机器学习中，有一个非常经典的维度灾难的概念。用来描述当空间维度增加时，分析和组织高维空间，因体积指数增加而遇到各种问题场景。例如，100个平均分布的点能把一个单位区间以每个点距离不超过0.01采样；而当维度增加到10后，如果以相邻点距离不超过0.01小方格采样单位超一单位超正方体，则需要10^20 个采样点。 正是由于高维特征有如上描述的各种各样的问题，所以我们需要进行特征降维和特征选择等工作。特征降维常用的算法有PCA，LDA等。特征降维的目标是将高维空间中的数据集映射到低维空间数据，同时尽可能少地丢失信息，或者降维后的数据点尽可能地容易被区分 PCA算法通过协方差矩阵的特征值分解能够得到数据的主成分，以二维特征为例，两个特征之间可能存在线性关系（例如运动的时速和秒速度），这样就造成了第二维信息是冗余的。PCA的目标是发现这种特征之间的线性关系，并去除。 LDA算法考虑label，降维后的数据点尽可能地容易被区分 6.3 特征选择特征选择的目标是寻找最优特征子集。特征选择能剔除不相关(irrelevant)或冗余(redundant )的特征，从而达到减少特征个数，提高模型精确度，减少运行时间的目的。另一方面，选取出真正相关的特征简化模型，协助理解数据产生的过程。 特征选择的一般过程如下图所示：主要分为产生过程，评估过程，停止条件和验证过程。 6.3.1 特征选择-产生过程和生成特征子集方法 完全搜索(Complete) 广度优先搜索( Breadth First Search )：广度优先遍历特征子空间。枚举所有组合，穷举搜索，实用性不高。 分支限界搜索( Branch and Bound )：穷举基础上加入分支限界。例如：剪掉某些不可能搜索出比当前最优解更优的分支。 其他，如定向搜索 (Beam Search )，最优优先搜索 ( Best First Search )等 启发式搜索(Heuristic) 序列前向选择( SFS ， Sequential Forward Selection )：从空集开始，每次加入一个选最优。 序列后向选择( SBS ， Sequential Backward Selection )：从全集开始，每次减少一个选最优。 增L去R选择算法 ( LRS ， Plus-L Minus-R Selection )：从空集开始，每次加入L个，减去R个，选最优（L&gt;R)或者从全集开始，每次减去R个，增加L个，选最优(L&lt;R)。 其他如双向搜索( BDS ， Bidirectional Search )，序列浮动选择( Sequential Floating Selection )等 随机搜索(Random) 随机产生序列选择算法(RGSS， Random Generation plus Sequential Selection) 随机产生一个特征子集，然后在该子集上执行SFS与SBS算法。 模拟退火算法( SA， Simulated Annealing )：以一定的概率来接受一个比当前解要差的解，而且这个概率随着时间推移逐渐降低 遗传算法( GA， Genetic Algorithms )：通过交叉、突变等操作繁殖出下一代特征子集，并且评分越高的特征子集被选中参加繁殖的概率越高。 随机算法共同缺点:依赖随机因素，有实验结果难重现。 6.3.2 特征选择－有效性分析对特征的有效性进行分析，得到各个特征的特征权重，根据是否与模型有关可以分为1.与模型相关特征权重，使用所有的特征数据训练出来模型，看在模型中各个特征的权重，由于需要训练出模型，模型相关的权重与此次学习所用的模型比较相关。不同的模型有不同的模型权重衡量方法。例如线性模型中，特征的权重系数等。2.与模型无关特征权重。主要分析特征与label的相关性，这样的分析是与这次学习所使用的模型无关的。与模型无关特征权重分析方法包括(1)交叉熵，(2)Information Gain，(3)Odds ratio，(4)互信息，(5)KL散度等 七、特征监控在机器学习任务中，特征非常重要。 个人经验，80%的效果由特征带来。下图是随着特征数的增加，最终模型预测值与实际值的相关系数变化。 对于重要的特征进行监控与有效性分析，了解模型所用的特征是否存在问题，当某个特别重要的特征出问题时，需要做好备案，防止灾难性结果。需要建立特征有效性的长效监控机制 我们对关键特征进行了监控，下面特征监控界面的一个截图。通过监控我们发现有一个特征的覆盖率每天都在下降，与特征数据提供方联系之后，发现特征数据提供方的数据源存在着问题，在修复问题之后，该特征恢复正常并且覆盖率有了较大提升。 在发现特征出现异常时，我们会及时采取措施，对服务进行降级处理，并联系特征数据的提供方尽快修复。对于特征数据生成过程中缺乏监控的情况也会督促做好监控，在源头解决问题。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（39）：实例详解机器学习如何解决问题]]></title>
    <url>%2F2017%2F09%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8839%EF%BC%89%EF%BC%9A%E5%AE%9E%E4%BE%8B%E8%AF%A6%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[一、前言随着大数据时代的到来，机器学习成为解决问题的一种重要且关键的工具。不管是工业界还是学术界，机器学习都是一个炙手可热的方向，但是学术界和工业界对机器学习的研究各有侧重，学术界侧重于对机器学习理论的研究，工业界侧重于如何用机器学习来解决实际问题。我们结合美团在机器学习上的实践，进行一个实战（InAction）系列的介绍（带“机器学习InAction系列”标签的文章），介绍机器学习在解决工业界问题的实战中所需的基本技术、经验和技巧。本文主要结合实际问题，概要地介绍机器学习解决实际问题的整个流程，包括对问题建模、准备训练数据、抽取特征、训练模型和优化模型等关键环节；另外几篇则会对这些关键环节进行更深入地介绍。 下文分为 1）机器学习的概述， 2）对问题建模， 3）准备训练数据， 4）抽取特征， 5）训练模型， 6）优化模型， 7）总结 共7个章节进行介绍。 二、机器学习的概述：2.1 什么是机器学习？随着机器学习在实际工业领域中不断获得应用，这个词已经被赋予了各种不同含义。在本文中的“机器学习”含义与wikipedia上的解释比较契合，如下：Machine learning is a scientific discipline that deals with the construction and study of algorithms that can learn from data. 机器学习可以分为无监督学习（unsupervised learning）和有监督学习（supervised learning），在工业界中，有监督学习是更常见和更有价值的方式，下文中主要以这种方式展开介绍。如下图中所示，有监督的机器学习在解决实际问题时，有两个流程，一个是离线训练流程（蓝色箭头），包含数据筛选和清洗、特征抽取、模型训练和优化模型等环节；另一个流程则是应用流程（绿色箭头），对需要预估的数据，抽取特征，应用离线训练得到的模型进行预估，获得预估值作用在实际产品中。在这两个流程中，离线训练是最有技术挑战的工作（在线预估流程很多工作可以复用离线训练流程的工作），所以下文主要介绍离线训练流程。 2.2 什么是模型？模型，是机器学习中的一个重要概念，简单的讲，指特征空间到输出空间的映射；一般由模型的假设函数和参数w组成（下面公式就是Logistic Regression模型的一种表达，在训练模型的章节做稍详细的解释）；一个模型的假设空间（hypothesis space），指给定模型所有可能w对应的输出空间组成的集合。工业界常用的模型有Logistic Regression（简称LR）、Gradient Boosting Decision Tree（简称GBDT）、Support Vector Machine（简称SVM）、Deep Neural Network（简称DNN）等。 模型训练就是基于训练数据，获得一组参数w，使得特定目标最优，即获得了特征空间到输出空间的最优映射，具体怎么实现，见训练模型章节。 2.3 为什么要用机器学习解决问题？ 目前处于大数据时代，到处都有成T成P的数据，简单规则处理难以发挥这些数据的价值； 廉价的高性能计算，使得基于大规模数据的学习时间和代价降低； 廉价的大规模存储，使得能够更快地和代价更小地处理大规模数据； 存在大量高价值的问题，使得花大量精力用机器学习解决问题后，能获得丰厚收益。 2.4 机器学习应该用于解决什么问题？ 目标问题需要价值巨大，因为机器学习解决问题有一定的代价； 目标问题有大量数据可用，有大量数据才能使机器学习比较好地解决问题（相对于简单规则或人工）； 目标问题由多种因素（特征）决定，机器学习解决问题的优势才能体现（相对于简单规则或人工）； 目标问题需要持续优化，因为机器学习可以基于数据自我学习和迭代，持续地发挥价值。 三、对问题建模本文以DEAL（团购单）交易额预估问题为例（就是预估一个给定DEAL一段时间内卖了多少钱），介绍使用机器学习如何解决问题。首先需要： 收集问题的资料，理解问题，成为这个问题的专家； 拆解问题，简化问题，将问题转化机器可预估的问题。 深入理解和分析DEAL交易额后，可以将它分解为如下图的几个问题： 3.1 单个模型？多个模型？如何来选择？按照上图进行拆解后，预估DEAL交易额就有2种可能模式，一种是直接预估交易额；另一种是预估各子问题，如建立一个用户数模型和建立一个访购率模型（访问这个DEAL的用户会购买的单子数），再基于这些子问题的预估值计算交易额。 不同方式有不同优缺点，具体如下： 选择哪种模式？ 1）问题可预估的难度，难度大，则考虑用多模型； 2）问题本身的重要性，问题很重要，则考虑用多模型； 3）多个模型的关系是否明确，关系明确，则可以用多模型。 如果采用多模型，如何融合？ 可以根据问题的特点和要求进行线性融合，或进行复杂的融合。以本文问题为例，至少可以有如下两种： 3.2 模型选择对于DEAL交易额这个问题，我们认为直接预估难度很大，希望拆成子问题进行预估，即多模型模式。那样就需要建立用户数模型和访购率模型，因为机器学习解决问题的方式类似，下文只以访购率模型为例。要解决访购率问题，首先要选择模型，我们有如下的一些考虑： 主要考虑 1）选择与业务目标一致的模型； 2）选择与训练数据和特征相符的模型。 训练数据少，High Level特征多，则使用“复杂”的非线性模型（流行的GBDT、Random Forest等）； 训练数据很大量，Low Level特征多，则使用“简单”的线性模型（流行的LR、Linear-SVM等）。 补充考虑 1）当前模型是否被工业界广泛使用；2）当前模型是否有比较成熟的开源工具包（公司内或公司外）；3）当前工具包能够的处理数据量能否满足要求；4）自己对当前模型理论是否了解，是否之前用过该模型解决问题。 为实际问题选择模型，需要转化问题的业务目标为模型评价目标，转化模型评价目标为模型优化目标；根据业务的不同目标，选择合适的模型，具体关系如下： 通常来讲，预估真实数值（回归）、大小顺序（排序）、目标所在的正确区间（分类）的难度从大到小，根据应用所需，尽可能选择难度小的目标进行。对于访购率预估的应用目标来说，我们至少需要知道大小顺序或真实数值，所以我们可以选择Area Under Curve（AUC）或Mean Absolute Error（MAE）作为评估目标，以Maximum likelihood为模型损失函数（即优化目标）。综上所述，我们选择spark版本 GBDT或LR，主要基于如下考虑： 1）可以解决排序或回归问题；2）我们自己实现了算法，经常使用，效果很好；3）支持海量数据；4）工业界广泛使用。 四、准备训练数据深入理解问题，针对问题选择了相应的模型后，接下来则需要准备数据；数据是机器学习解决问题的根本，数据选择不对，则问题不可能被解决，所以准备训练数据需要格外的小心和注意： 4.1 注意点：待解决问题的数据本身的分布尽量一致；训练集/测试集分布与线上预测环境的数据分布尽可能一致，这里的分布是指（x,y）的分布，不仅仅是y的分布；y数据噪音尽可能小，尽量剔除y有噪音的数据；非必要不做采样，采样常常可能使实际数据分布发生变化，但是如果数据太大无法训练或者正负比例严重失调（如超过100:1）,则需要采样解决。 4.2 常见问题及解决办法待解决问题的数据分布不一致：1）访购率问题中DEAL数据可能差异很大，如美食DEAL和酒店DEAL的影响因素或表现很不一致，需要做特别处理；要么对数据提前归一化，要么将分布不一致因素作为特征，要么对各类别DEAL单独训练模型。数据分布变化了：1）用半年前的数据训练模型，用来预测当前数据，因为数据分布随着时间可能变化了，效果可能很差。尽量用近期的数据训练，来预测当前数据，历史的数据可以做降权用到模型，或做transfer learning。y数据有噪音：1）在建立CTR模型时，将用户没有看到的Item作为负例，这些Item是因为用户没有看到才没有被点击，不一定是用户不喜欢而没有被点击，所以这些Item是有噪音的。可以采用一些简单规则，剔除这些噪音负例，如采用skip-above思想，即用户点过的Item之上，没有点过的Item作为负例（假设用户是从上往下浏览Item）。采样方法有偏，没有覆盖整个集合：1）访购率问题中，如果只取只有一个门店的DEAL进行预估，则对于多门店的DEAL无法很好预估。应该保证一个门店的和多个门店的DEAL数据都有；2）无客观数据的二分类问题，用规则来获得正/负例，规则对正/负例的覆盖不全面。应该随机抽样数据，进行人工标注，以确保抽样数据和实际数据分布一致。 4.3 访购率问题的训练数据收集N个月的DEAL数据（x）及相应访购率（y）；收集最近N个月，剔除节假日等非常规时间 （保持分布一致）；只收集在线时长&gt;T 且 访问用户数 &gt; U的DEAL （减少y的噪音）；考虑DEAL销量生命周期 （保持分布一致）；考虑不同城市、不同商圈、不同品类的差别 （保持分布一致）。 五、抽取特征完成数据筛选和清洗后，就需要对数据抽取特征，就是完成输入空间到特征空间的转换（见下图）。针对线性模型或非线性模型需要进行不同特征抽取，线性模型需要更多特征抽取工作和技巧，而非线性模型对特征抽取要求相对较低。 通常，特征可以分为High Level与Low Level，High Level指含义比较泛的特征，Low Level指含义比较特定的特征，举例来说： 1234DEAL A1属于POIA，人均50以下，访购率高；DEAL A2属于POIA，人均50以上，访购率高；DEAL B1属于POIB，人均50以下，访购率高；DEAL B2属于POIB，人均50以上，访购率底； 基于上面的数据，可以抽到两种特征，POI（门店）或人均消费；POI特征则是Low Level特征，人均消费则是High Level特征；假设模型通过学习，获得如下预估： 12如果DEALx 属于POIA（Low Level feature），访购率高；如果DEALx 人均50以下（High Level feature），访购率高。 所以，总体上，Low Level 比较有针对性，单个特征覆盖面小（含有这个特征的数据不多），特征数量（维度）很大。High Level比较泛化，单个特征覆盖面大（含有这个特征的数据很多），特征数量（维度）不大。长尾样本的预测值主要受High Level特征影响。高频样本的预测值主要受Low Level特征影响。 对于访购率问题，有大量的High Level或Low Level的特征，其中一些展示在下图： 非线性模型的特征 1）可以主要使用High Level特征，因为计算复杂度大，所以特征维度不宜太高； 2）通过High Level非线性映射可以比较好地拟合目标。 线性模型的特征 1）特征体系要尽可能全面，High Level和Low Level都要有； 2）可以将High Level转换Low Level，以提升模型的拟合能力。 5.1 特征归一化特征抽取后，如果不同特征的取值范围相差很大，最好对特征进行归一化，以取得更好的效果，常见的归一化方式如下： Rescaling：归一化到[0,1] 或 [-1，1]，用类似方式： Standardization：设为x分布的均值，为x分布的标准差； Scaling to unit length：归一化到单位长度向量 5.2 特征选择特征抽取和归一化之后，如果发现特征太多，导致模型无法训练，或很容易导致模型过拟合，则需要对特征进行选择，挑选有价值的特征。 Filter：假设特征子集对模型预估的影响互相独立，选择一个特征子集，分析该子集和数据Label的关系，如果存在某种正相关，则认为该特征子集有效。衡量特征子集和数据Label关系的算法有很多，如Chi-square，Information Gain。 Wrapper：选择一个特征子集加入原有特征集合，用模型进行训练，比较子集加入前后的效果，如果效果变好，则认为该特征子集有效，否则认为无效。 Embedded：将特征选择和模型训练结合起来，如在损失函数中加入L1 Norm ，L2 Norm。 六、训练模型完成特征抽取和处理后，就可以开始模型训练了，下文以简单且常用的Logistic Regression模型（下称LR模型）为例，进行简单介绍。 设有m个（x,y）训练数据，其中x为特征向量，y为label，；w为模型中参数向量，即模型训练中需要学习的对象。所谓训练模型，就是选定假说函数和损失函数，基于已有训练数据（x,y），不断调整w，使得损失函数最优，相应的w就是最终学习结果，也就得到相应的模型。 6.1 模型函数假说函数，即假设x和y存在一种函数关系： 损失函数，基于上述假设函数，构建模型损失函数（优化目标），在LR中通常以（x,y）的最大似然估计为目标： 6.2 优化算法 梯度下降（Gradient Descent）即w沿着损失函数的负梯度方向进行调整，示意图见下图，的梯度即一阶导数（见下式），梯度下降有多种类型，如随机梯度下降或批量梯度下降。随机梯度下降（Stochastic Gradient Descent），每一步随机选择一个样本，计算相应的梯度，并完成w的更新，如下式， 批量梯度下降（Batch Gradient Descent）,每一步都计算训练数据中的所有样本对应的梯度，w沿着这个梯度方向迭代，即 牛顿法（Newton’s Method） 牛顿法的基本思想是在极小点附近通过对目标函数做二阶Taylor展开，进而找到L(w)的极小点的估计值。形象地讲，在wk处做切线，该切线与L(w)=0的交点即为下一个迭代点wk+1（示意图如下）。w的更新公式如下，其中目标函数的二阶偏导数，即为大名鼎鼎的Hessian矩阵。 拟牛顿法（Quasi-Newton Methods）：计算目标函数的二阶偏导数，难度较大，更为复杂的是目标函数的Hessian矩阵无法保持正定；不用二阶偏导数而构造出可以近似Hessian矩阵的逆的正定对称阵，从而在”拟牛顿”的条件下优化目标函数。 BFGS： 使用BFGS公式对H(w)进行近似，内存中需要放H(w),内存需要O(m2)级别； L-BFGS：存储有限次数（如k次）的更新矩阵，用这些更新矩阵生成新的H(w),内存降至O(m)级别； OWLQN: 如果在目标函数中引入L1正则化，需要引入虚梯度来解决目标函数不可导问题，OWLQN就是用来解决这个问题。 Coordinate Descent对于w，每次迭代，固定其他维度不变，只对其一个维度进行搜索，确定最优下降方向（示意图如下），公式表达如下： 七、优化模型经过上文提到的数据筛选和清洗、特征设计和选择、模型训练，就得到了一个模型，但是如果发现效果不好？怎么办？ 【首先】反思目标是否可预估，数据和特征是否存在bug。 【然后】分析一下模型是Overfitting还是Underfitting，从数据、特征和模型等环节做针对性优化。 7.1 Underfitting &amp; Overfitting所谓Underfitting，即模型没有学到数据内在关系，如下图左一所示，产生分类面不能很好的区分X和O两类数据；产生的深层原因，就是模型假设空间太小或者模型假设空间偏离。所谓Overfitting，即模型过渡拟合了训练数据的内在关系，如下图右一所示，产生分类面过好地区分X和O两类数据，而真实分类面可能并不是这样，以至于在非训练数据上表现不好；产生的深层原因，是巨大的模型假设空间与稀疏的数据之间的矛盾。 在实战中，可以基于模型在训练集和测试集上的表现来确定当前模型到底是Underfitting还是Overfitting，判断方式如下表： 7.2 怎么解决Underfitting和Overfitting问题？ 八、总结综上所述，机器学习解决问题涉及到问题建模、准备训练数据、抽取特征、训练模型和优化模型等关键环节，有如下要点： 理解业务，分解业务目标，规划模型可预估的路线图。 数据：y数据尽可能真实客观；训练集/测试集分布与线上应用环境的数据分布尽可能一致。 特征：利用Domain Knowledge进行特征抽取和选择；针对不同类型的模型设计不同的特征。 模型：针对不同业务目标、不同数据和特征，选择不同的模型；如果模型不符合预期，一定检查一下数据、特征、模型等处理环节是否有bug；考虑模型Underfitting和Qverfitting，针对性地优化。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（38）：外卖订单量预测异常报警模型实践]]></title>
    <url>%2F2017%2F09%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8838%EF%BC%89%EF%BC%9A%E5%A4%96%E5%8D%96%E8%AE%A2%E5%8D%95%E9%87%8F%E9%A2%84%E6%B5%8B%E5%BC%82%E5%B8%B8%E6%8A%A5%E8%AD%A6%E6%A8%A1%E5%9E%8B%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[一、前言外卖业务的快速发展对系统稳定性提出了更高的要求，每一次订单量大盘的异常波动，都需要做出及时的应对，以保证系统的整体稳定性。如何做出较为准确的波动预警，显得尤为重要。 从时间上看，外卖订单量时间序列有两个明显的特征（如下图所示）： 周期性。每天订单量的变化趋势都大致相同，午高峰和晚高峰订单量集中。实时性。当天的订单量可能会受天气等因素影响，呈现整体的上涨或下降。订单量波动预警，初期外卖订单中心使用的是当前时刻和前一时刻订单量比较，超过一定阈值就报警的方式，误报率和漏报率都比较大。后期将业务数据上传到美团点评的服务治理平台，使用该平台下的基线报警模型进行监控报警。基线数据模型考虑到了订单量时间序列的周期性特征，但是忽略了实时性特征，在实际使用中误报率依然很高，大量的误报漏报导致RD对于报警已经麻木，出现问题时不能及时响应，因此，急需一种新的异常检测模型，提高报警的准确率。 二、异常检测的定义异常，意为“异于正常”。异常检测，就是从一组数据中寻找那些和期望数据不同的数据。监控数据都是和时间相关的，每一个监控指标只有和时间组合一起才有其具体的含义。按照时间顺序，将监控指标组成一个序列，我们就得到了监控指标的时间序列。 基于预测的异常检测模型如下图所示，xt是真实数据，通过预测器得到预测数据，然后xt和pt分别作为比较器的输入，最终得到输出yt。yt是一个二元值，可以用+1（+1表示输入数据正常），-1（-1表示输入数据异常）表示。 异常检测主要有两种策略： 异常驱动的异常检测（敏感性）：宁愿误报，也不能错过任何一个异常，这适用于非常重要的检测。简单概括，就是“宁可错杀一千，不能放过一个”。 预算驱动的异常检测（准确性）：这种策略的异常检测，从字面理解就是只有定量的一些预算去处理这些报警，那么只能当一定是某种问题时，才能将报警发送出来。 这两种策略不可兼容的。对于检测模型的改善，可以从两个方面入手，一是预测器的优化，二是比较器的优化。我们从这两个方面描述模型的改善。 三、预测器设计预测器，就是用一批历史数据预测当前的数据。使用的历史数据集大小，以及使用的预测算法都会影响最终的预测效果。 外卖订单量具有明显的周期性，同时相邻时刻的订单量数据也有很强的相关性，我们的目标，就是使用上面说的相关数据预测出当前的订单量。下面，我们分析几种常用的预测器实现。 3.1 同比环比预测器同比环比是比较常用的异常检测方式，它是将当前时刻数据和前一时刻数据（环比）或者前一天同一时刻数据（同比）比较，超过一定阈值即认为该点异常。如果用图2.1模型来表示，那么预测器就可以表示为用当前时刻前一时刻或者前一天同一时刻数据作为当前时刻的预测数据。 假如需要预测图中黄色数据，那么环比使用图中的蓝色数据作为预测黄点的源数据，同比使用图中红色数据作为预测黄点的源数据。 3.2 基线预测器同比环比使用历史上的单点数据来预测当前数据，误差比较大。t时刻的监控数据，与$t-1,t-2,…$时刻的监控数据存在相关性。同时，与$t-k,t-2k,…$时刻的数据也存在相关性（k为周期），如果能利用上这些相关数据对t时刻进行预测，预测结果的误差将会更小。 比较常用的方式是对历史数据求平均，然后过滤噪声，可以得到一个平滑的曲线（基线），使用基线数据来预测当前时刻的数据。该方法预测t时刻数据（图中黄色数据）使用到的历史数据如下图所示（图中红色数据）： 基线数据预测器广泛应用在业务大盘监控中，预测效果如图3.3所示。从图中可以看出，基线比较平滑，在低峰期预测效果比较好，但是在外卖的午高峰和晚高峰预测误差比较大。 3.3 Holt-Winters预测器同比环比预测到基线数据预测，使用的相关数据变多，预测的效果也较好。但是基线数据预测器只使用了周期相关的历史数据，没有使用上同周期相邻时刻的历史数据，相邻时刻的历史数据对于当前时刻的预测影响是比较大的。如外卖订单量，某天天气不好，很多用户不愿意出门，那么当天的外卖的订单量就会呈现整体的上涨，这种整体上涨趋势只能从同一周期相邻时刻的历史数据中预测出来。如图3.4所示，预测图中黄色数据，如果使用上图中所有的红色数据，那么预测效果会更好。 本文使用了Holt-Winters来实现这一目标。 Holt-Winters是三次指数滑动平均算法，它将时间序列数据分为三部分：残差数据a(t)，趋势性数据b(t)，季节性数据s(t)。使用Holt-Winters预测t时刻数据，需要t时刻前包含多个周期的历史数据。相关链接：Exponential smoothing、Holt-Winters seasonal method。 各部分的迭代计算公式（周期为k）： 如图所示，(a)显示了某一段时间内外卖订单的原始提单监控数据（分钟统计量，周期为1天），图(b)显示了其Holt-Winters的分解图（四幅图分别对应原始数据、残差数据分量、趋势数据分量、周期数据分量）。将订单量时间序列分解为残差数据a(t)，趋势数据b(t)，周期数据s(t)后，就可以使用下面的公式预测未来不同时刻时刻的订单量，其中h表示未来时刻距离当前时刻的跨度。 外卖订单量，是按分钟统计的离散时间序列，所以如果需要预测下一分钟的订单量，令h=1。 3.4 外卖报警模型中的预测器在外卖订单量异常检测中，使用Holt-Winters预测器实时预测下一分钟订单量，每次需要至少5天以上的订单量数据才能有较好的预测效果，数据量要求比较大。 在实际的异常检测模型中，我们对Holt-Winters预测器进行了简化。预测器的趋势数据表示的是时间序列的总体变化趋势，如果以天为周期看待外卖的订单量时间序列，是没有明显的趋势性的，图3.5(b)的分解图也证明了这一点。因此，我们可以去掉其中的趋势数据部分。 各部分的迭代公式简化为(3-1)： 预测值： h越大，预测值Yhat[t+h] 的误差也就越大。实时的订单流监控，令h=1，每当有新的监控数据时，更新输入序列，然后预测下一分钟数据。 Holt-Winters每一次预测都需要大量的输入数据序列。从上面模型的简化公式可以看出，对残差数据a(t)的预测是对序列(a(t-m),a(t-m+1),…a(t-2),a(t-1))的一次指数滑动平均，对周期数据s(t)的预测是对序列（s(t-mk) ,s(t-(m-1)k),…s(t-k)）的一次滑动平均，大量的输入数据是用于周期数据s(t)的计算。 a(t)和s(t)是互相关联的迭代计算过程，如果从周期性角度看公式(3-1)，可以发现：计算当前周期内的a(t)时，使用的是上一周期计算出来的s(t-k)，当前周期计算出的s(t)是用于下一周期a(t+k)的计算。为了将算法应用到线上的实时预测，我们可以将Holt-Winters算法拆分为两个独立的计算过程： 定时任务计算序列的周期数s(t)。 对残差序列做实时预测。 下面就分别从这两个步骤介绍外卖报警模型中的预测器实现。 3.4.1 计算序列的周期性数据时间序列的周期性数据不需要实时计算，按周期性更新即可，如外卖订单大盘监控，s(t)只需要每天更新一次即可。对于s(t)的计算，可以有多种方法，可以使用上面提到的Holt-Winters按公式(3-1)计算出时间序列的周期性数据（如图3.5b所示），或直接使用前一天的监控数据作为当天的周期数据（这两种方式都需要对输入序列进行预处理，保证算法的输入序列不含有异常数据）。也可以用上面3.2节提到的，将历史数据做平均求出基线作为序列的周期性数据。 目前外卖订单中心报警模型采用的是Holt-Winters计算周期数据的方式。在将该模型推广到外卖其他业务线监控时，使用了计算基线数据作为周期数据的方式，这里简单对比一下两种方式的优劣。 使用Holt-Winters算法计算周期数据 优点：如果序列中含有周期性的陡增陡降点，Holt-Winters计算出的周期数据中会保留这些陡增陡降趋势，因此可以准确的预测出这些趋势，不会产生误报。比如外卖订单的提单数据，在每天的某个时刻都有一个定期陡降，使用该方式可以正确的预测出下降的趋势。如图3.6所示，蓝色线是真实数据，棕色线是预测数据，在该时刻，棕色线准确的预测出了下降点。 缺点：需要对输入数据进行预处理，去除异常数据。如果输入序列中含有异常数据，使用Holt-Winters时可能会把这些异常数据计算到周期数据中，影响下一周期的预测从而产生误报（Holt-Winters理论上也只是滑动平均的过程，因此如果输入数据中含有比较大的异常数据时，存在这种可能性，实际应用中订单的报警模型也出现过这种误报）。 历史数据平均求基线 优点：计算出的周期数据比较平滑，不需要对输入序列进行预处理，计算过程中可以自动屏蔽掉异常数据的影响，计算过程简单，如图3.3所示的基线数据。缺点：周期数据比较平滑，不会出现陡增陡降点，因此对于周期性出现的陡增陡降不能很好的预测，出现误报。比如外卖活动的大盘（如图3.7所示，红线是真实数据，黑线是预测数据），提前下单优惠在每天某个时刻会出现周期性陡降，使用该方式会出现误报。 两种求周期数据的方式各有优劣，可以根据各自的监控数据特点选择合适的计算方式。如果监控数据中含有大量的周期性的陡增陡降点，那么推荐使用方式1，可以避免在这些时间点的误报。如果监控数据比较平滑，陡增陡降点很少，那么推荐方式2，计算简单的同时，也能避免因输入数据预处理不好而造成的意料之外的误报。 3.4.2 残差数据实时预测计算出周期数据后，下一个目标就是对残差数据的预测。使用下面的公式，实际监控数据与周期数据相减得到残差数据，对残差数据做一次滑动平均，预测出下一刻的残差，将该时刻的残差、周期数据相加即可得到该时刻的预测数据。残差序列的长度设为60，即可以得到比较准确的预测效果。 对于实时预测，使用的是当天的周期数据和前60分钟数据。最终的预测结果如图3.8(a)(b)所示，其中蓝色线是真实数据，红色线是预测数据。 四、比较器设计预测器预测出当前时刻订单量的预测值后，还需要与真实值比较来判断当前时刻订单量是否异常。一般的比较器都是通过阈值法，比如实际值超过预测值的一定比例就认为该点出现异常，进行报警。这种方式错误率比较大。在订单模型的报警检测中没有使用这种方式，而是使用了两个串联的Filter（如图4.1所示），只有当两个Fliter都认为该点异常时，才进行报警，下面简单介绍一下两个Filter的实现。 离散度Filter：根据预测误差曲线离散程度过滤出可能的异常点。一个序列的方差表示该序列离散的程度，方差越大，表明该序列波动越大。如果一个预测误差序列方差比较大，那么我们认为预测误差的报警阈值相对大一些才比较合理。离散度Filter利用了这一特性，取连续15分钟的预测误差序列，分为首尾两个序列（e1,e2），如果两个序列的均值差大于e1序列方差的某个倍数，我们就认为该点可能是异常点。 阈值Filter：根据误差绝对值是否超过某个阈值过滤出可能的异常点。利用离散度Filter进行过滤时，报警阈值随着误差序列波动程度变大而变大，但是在输入数据比较小时，误差序列方差比较小，报警阈值也很小，容易出现误报。所以设计了根据误差绝对值进行过滤的阈值Filter。阈值Filter设计了一个分段阈值函数y=f(x)，对于实际值x和预测值p，只有当|x-p|&gt;f(x)时报警。实际使用中，可以寻找一个对数函数替换分段阈值函数，更易于参数调优。 五、报警模型最终效果最终的外卖订单异常报警模型结构图如图5.1所示，每天会有定时Job从ETL中统计出最近10天的历史订单量，经过预处理模块，去除异常数据，经过周期数据计算模块得到周期性数据。对当前时刻预测时，取60分钟的真实数据和周期性数据，经过实时预测模块，预测出当前订单量。将连续15分钟的预测值和真实值通过比较器，判断当前时刻是否异常。 新的报警模型上线后，外卖订单量的异常检测的漏报率和误报率都有显著的提升，上线半年以来，对于每一次的异常都能准确的检测出来，漏报率近乎为0。误报率在通常情况下限制在了每周0~3次误报。 报警模型目前应用在外卖订单量的异常检测中，同时推广到了外卖业务的其他各种大盘监控中，取得了不错的效果。在报警模型上线后，我们发现并解决了一些系统隐患点，如： 点评侧外卖提单量在每天定时有一个下降尖刺，经过排查是因为客户端冷启动短时间内大量的请求，导致SLB性能达到瓶颈，从而导致接口成功率下降。 点评侧外卖订单取消量经常会有尖刺，经过排查发现是由于在支付时，需要进行跨机房的账号转换，专线网络抖动时造成接口超时。 外卖订单量在每天某些时刻都有陡降趋势，经过排查，是因为这些点大量商家开始休息导致的。 六、总结将机器学习中的预测算法运用到外卖订单的异常检测中，极大的提高了异常检测的准确性和敏感性，提升了系统稳定运维的效率。该报警模型也有很广泛的应用场景，美团点评的各个业务线的监控数据，绝大多数都是含有明显周期性的时间序列，本文提出的模型都能运用到这些监控数据的异常检测中。 当然，模型还有进一步完善的空间，如： 历史数据的预处理优化。在进行周期数据计算时，对于输入序列的预处理，如果能够排除绝大部分的异常数据，那么最终检测的误报率将会进一步的降低。 在不会产生持续误报的情况下替换有异常的实时数据。对于当前数据的预测，利用的都是前60分钟的真实数据，但是这些数据可能本身就存在异常数据，那么就存在一种情况，当出现异常时，真实数据开始下跌，预测数据紧接着也会下跌（如图3.8b所示）。这种情况有时候可能满足需求（比如只在异常开始的时候进行报警，异常持续时间内不再报警，防止报警太多造成的信息轰炸），有时候可能不满足需求（比如要求预测数据不跟随异常变化而变化，这种情况可以应用在故障期间的损失统计中）。如果需要预测值不随异常变化而变化，一种可能的方法是，当检测到当前数据是异常数据时，将预测数据替换当前的真实数据，作为下一时刻预测器的输入，这样可以防止异常数据对于下一时刻预测值的影响。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>异常检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（37）：外卖O2O的用户画像实践]]></title>
    <url>%2F2017%2F09%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8837%EF%BC%89%EF%BC%9A%E5%A4%96%E5%8D%96O2O%E7%9A%84%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[美团外卖经过3年的飞速发展，品类已经从单一的外卖扩展到了美食、夜宵、鲜花、商超等多个品类。用户群体也从早期的学生为主扩展到学生、白领、社区以及商旅，甚至包括在KTV等娱乐场所消费的人群。随着供给和消费人群的多样化，如何在供给和用户之间做一个对接，就是用户画像的一个基础工作。所谓千人千面，画像需要刻画不同人群的消费习惯和消费偏好。 外卖O2O和传统的电商存在一些差异。可以简单总结为如下几点： 1）新事物，快速发展：这意味很多用户对外卖的认知较少，对平台上的新品类缺乏了解，对自身的需求也没有充分意识。平台需要去发现用户的消费意愿，以便对用户的消费进行引导。 2）高频：外卖是个典型的高频O2O应用。一方面消费频次高，用户生命周期相对好判定；另一方面消费单价较低，用户决策时间短、随意性大。 3）场景驱动：场景是特定的时间、地点和人物的组合下的特定的消费意图。不同的时间、地点，不同类型的用户的消费意图会有差异。例如白领在写字楼中午的订单一般是工作餐，通常在营养、品质上有一定的要求，且单价不能太高；而到了周末晚上的订单大多是夜宵，追求口味且价格弹性较大。场景辨识越细致，越能了解用户的消费意图，运营效果就越好。 4）用户消费的地理位置相对固定，结合地理位置判断用户的消费意图是外卖的一个特点。 一、外卖产品运营对画像技术的要求如下图所示，我们大致可以把一个产品的运营分为用户获取和用户拓展两个阶段。在用户获取阶段，用户因为自然原因或一些营销事件（例如广告、社交媒体传播）产生对外卖的注意，进而产生了兴趣，并在合适的时机下完成首购，从而成为外卖新客。在这一阶段，运营的重点是提高效率，通过一些个性化的营销和广告手段，吸引到真正有潜在需求的用户，并刺激其转化。在用户完成转化后，接下来的运营重点是拓展用户价值。这里有两个问题：第一是提升用户价值，具体而言就是提升用户的单均价和消费频次，从而提升用户的LTV（life-time value)。基本手段包括交叉销售（新品类的推荐）、向上销售（优质高价供给的推荐）以及重复购买（优惠、红包刺激重复下单以及优质供给的推荐带来下单频次的提升）；第二个问题是用户的留存，通过提升用户总体体验以及在用户有流失倾向时通过促销和优惠将用户留在外卖平台。 所以用户所处的体验阶段不同，运营的侧重点也需要有所不同。而用户画像作为运营的支撑技术，需要提供相应的用户刻画以满足运营需求。根据上图的营销链条，从支撑运营的角度，除去提供常规的用户基础属性（例如年龄、性别、职业、婚育状况等）以及用户偏好之外，还需要考虑这么几个问题：1）什么样的用户会成为外卖平台的顾客（新客识别）；2）用户所处生命周期的判断，用户是否可能从平台流失（流失预警）；3）用户处于什么样的消费场景（场景识别）。后面“外卖O2O的用户画像实践”一节中，我们会介绍针对这三个问题的一些实践。 二、外卖画像系统架构下图是我们画像服务的架构：数据源包括基础日志、商家数据和订单数据。数据完成处理后存放在一系列主题表中，再导入kv存储，给下游业务端提供在线服务。同时我们会对整个业务流程实施监控。主要分为两部分，第一部分是对数据处理流程的监控，利用用内部自研的数据治理平台，监控每天各主题表产生的时间、数据量以及数据分布是否有异常。第二部分是对服务的监控。目前画像系统支持的下游服务包括：广告、排序、运营等系统。 三、外卖O2O的用户画像实践3.1 新客运营新客运营主要需要回答下列三个问题： 1）新客在哪里？ 2）新客的偏好如何？ 3）新客的消费力如何？ 回答这三个问题是比较困难的，因为相对于老客而言，新客的行为记录非常少或者几乎没有。这就需要我们通过一些技术手段作出推断。例如：新客的潜在转化概率，受到新客的人口属性（职业、年龄等）、所处地域（需求的因素）、周围人群（同样反映需求）以及是否有充足供给等因素的影响；而对于新客的偏好和消费力，从新客在到店场景下的消费行为可以做出推测。另外用户的工作和居住地点也能反映他的消费能力。对新客的预测大量依赖他在到店场景下的行为，而用户的到店行为对于外卖是比较稀疏的，大多数的用户是在少数几个类别上有过一些消费行为。这就意味着我们需要考虑选择什么样的统计量描述：是消费单价，总消费价格，消费品类等等。然后通过大量的试验来验证特征的显著性。另外由于数据比较稀疏，需要考虑合适的平滑处理。 我们在做高潜新客挖掘时，融入了多方特征，通过特征的组合最终作出一个效果比较好的预测模型。我们能够找到一些高转化率的用户，其转化率比普通用户高若干倍。通过对高潜用户有针对性的营销，可以极大提高营销效率。 3.2 流失预测新客来了之后，接下来需要把他留在这个平台上，尽量延长生命周期。营销领域关于用户留存的两个基本观点是（引自菲利普.科特勒 《营销管理》）： 获取一个新顾客的成本是维系现有顾客成本的5倍！ 如果将顾客流失率降低5%，公司利润将增加25%~85% 用户流失的原因通常包括：竞对的吸引；体验问题；需求变化。我们借助机器学习的方法，构建用户的描述特征，并借助这些特征来预测用户未来流失的概率。这里有两种做法: 第一种是预测用户未来若干天是否会下单这一事件发生的概率。这是典型的概率回归问题，可以选择逻辑回归、决策树等算法拟合给定观测下事件发生的概率；第二种是借助于生存模型，例如COX-PH模型，做流失的风险预测。下图左边是概率回归的模型，用户未来T天内是否有下单做为类别标记y，然后估计在观察到特征X的情况下y的后验概率P(y|X)。右边是用COX模型的例子，我们会根据用户在未来T天是否下单给样本一个类别，即观测时长记为T。假设用户的下单的距今时长t&lt;T，将t作为生存时长t’；否则将生存时长t’记为T。这样一个样本由三部分构成：样本的类别(flag)，生存时长(t’)以及特征列表。通过生存模型虽然无法显式得到P(t’|X)的概率，但其协变量部分实际反映了用户流失的风险大小。 生存模型中，βTx反映了用户流失的风险，同时也和用户下次订单的时间间隔成正相关。下面的箱线图中，横轴为βTx，纵轴为用户下单时间的间隔。 我们做了COX模型和概率回归模型的对比。在预测用户XX天内是否会下单上面，两者有相近的性能。 美团外卖通过使用了用户流失预警模型，显著降低了用户留存的运营成本。 3.3 场景运营拓展用户的体验，最重要的一点是要理解用户下单的场景。了解用户的订餐场景有助于基于场景的用户运营。对于场景运营而言，通常需要经过如下三个步骤： 场景可以从时间、地点、订单三个维度描述。比如说工作日的下午茶，周末的家庭聚餐，夜里在家点夜宵等等。其中重要的一点是用户订单地址的分析。通过区分用户的订单地址是写字楼、学校或是社区，再结合订单时间、订单内容，可以对用户的下单场景做到大致的了解。 上图是我们订单地址分析的流程。根据订单系统中的用户订单地址文本，基于自然语言处理技术对地址文本分析，可以得到地址的主干名称（指去掉了楼宇、门牌号的地址主干部分）和地址的类型（写字楼、住宅小区等）。在此基础上通过一些地图数据辅助从而判断出最终的地址类型。另外我们还做了合并订单的识别，即识别一个订单是一个人下单还是拼单。把拼单信息、地址分析以及时间结合在一起，我们可以预测用户的消费场景，进而基于场景做交叉销售和向上销售。 四、总结外卖的营销特征，跟其他行业的主要区别在于： 外卖是一个高频的业务。由于用户的消费频次高，用户生命周期的特征体现较显著。运营可以基于用户所处生命周期的阶段制定营销目标，例如用户完成首购后的频次提升、成熟用户的价值提升、衰退用户的挽留以及流失用户的召回等。因此用户的生命周期是一个基础画像，配合用户基本属性、偏好、消费能力、流失预测等其他画像，通过精准的产品推荐或者价格策略实现运营目标。 用户的消费受到时间、地点等场景因素驱动。因此需要对用户在不同的时间、地点下消费行为的差异做深入了解，归纳不同场景下用户需求的差异，针对场景制定相应的营销策略，提升用户活跃度。 另外由于外卖是一个新鲜的事物，在用户对一些新品类和新产品缺乏认知的情况下，需要通过技术手段识别用户的潜在需求，进行精准营销。例如哪些用户可能会对小龙虾、鲜花、蛋糕这样的相对低频、高价值的产品产生购买。可以采用的技术手段包括用户分群、对已产生消费的用户做look-alike扩展、迁移学习等。 同时我们在制作外卖的用户画像时还面临如下挑战： 1）数据多样性，存在大量非结构化数据例如用户地址、菜品名称等。需要用到自然语言处理技术，同时结合其他数据进行分析。 2）相对于综合电商而言，外卖是个相对单一的品类，用户在外卖上的行为不足以全方位地描述用户的基本属性。因此需要和用户在其他场合的消费行为做融合。 3）外卖单价相对较低，用户消费的决策时间短、随意性强。不像传统电商用户在决策前有大量的浏览行为可以用于捕捉用户单次的需求。因此更需要结合用户画像分析用户的历史兴趣、以及用户的消费场景，在消费前对用户做适当的引导、推荐。 面临这些挑战，需要用户画像团队更细致的数据处理、融合多方数据源，同时发展出新的方法论，才能更好地支持外卖业务发展的需要。而外卖的上述挑战，又分别和一些垂直领域电商类似，经验上存在可以相互借鉴之处。因此，外卖的用户画像的实践和经验累积，必将对整个电商领域的大数据应用作出新的贡献！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>用户画像</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（36）：GBDT算法原理深入解析]]></title>
    <url>%2F2017%2F09%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8836%EF%BC%89%EF%BC%9AGBDT%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[梯度提升（Gradient boosting）是一种用于回归、分类和排序任务的机器学习技术[1]，属于Boosting算法族的一部分。Boosting是一族可将弱学习器提升为强学习器的算法，属于集成学习（ensemble learning）的范畴。Boosting方法基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断要好。通俗地说，就是“三个臭皮匠顶个诸葛亮”的道理。梯度提升同其他boosting方法一样，通过集成（ensemble）多个弱学习器，通常是决策树，来构建最终的预测模型。 Boosting、bagging和stacking是集成学习的三种主要方法。不同于bagging方法，boosting方法通过分步迭代（stage-wise）的方式来构建模型，在迭代的每一步构建的弱学习器都是为了弥补已有模型的不足。Boosting族算法的著名代表是AdaBoost，AdaBoost算法通过给已有模型预测错误的样本更高的权重，使得先前的学习器做错的训练样本在后续受到更多的关注的方式来弥补已有模型的不足。与AdaBoost算法不同，梯度提升方法在迭代的每一步构建一个能够沿着梯度最陡的方向降低损失（steepest-descent）的学习器来弥补已有模型的不足。经典的AdaBoost算法只能处理采用指数损失函数的二分类学习任务[2]，而梯度提升方法通过设置不同的可微损失函数可以处理各类学习任务（多分类、回归、Ranking等），应用范围大大扩展。另一方面，AdaBoost算法对异常点（outlier）比较敏感，而梯度提升算法通过引入bagging思想、加入正则项等方法能够有效地抵御训练数据中的噪音，具有更好的健壮性。这也是为什么梯度提升算法（尤其是采用决策树作为弱学习器的GBDT算法）如此流行的原因，有种观点认为GBDT是性能最好的机器学习算法，这当然有点过于激进又固步自封的味道，但通常各类机器学习算法比赛的赢家们都非常青睐GBDT算法，由此可见该算法的实力不可小觑。 基于梯度提升算法的学习器叫做GBM(Gradient Boosting Machine)。理论上，GBM可以选择各种不同的学习算法作为基学习器。现实中，用得最多的基学习器是决策树。为什么梯度提升方法倾向于选择决策树（通常是CART树）作为基学习器呢？这与决策树算法自身的优点有很大的关系。决策树可以认为是if-then规则的集合，易于理解，可解释性强，预测速度快。同时，决策树算法相比于其他的算法需要更少的特征工程，比如可以不用做特征标准化，可以很好的处理字段缺失的数据，也可以不用关心特征间是否相互依赖等。决策树能够自动组合多个特征，它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）不过，单独使用决策树算法时，有容易过拟合缺点。所幸的是，通过各种方法，抑制决策树的复杂性，降低单颗决策树的拟合能力，再通过梯度提升的方法集成多个决策树，最终能够很好的解决过拟合的问题。由此可见，梯度提升方法和决策树学习算法可以互相取长补短，是一对完美的搭档。至于抑制单颗决策树的复杂度的方法有很多，比如限制树的最大深度、限制叶子节点的最少样本数量、限制节点分裂时的最少样本数量、吸收bagging的思想对训练样本采样（subsample），在学习单颗决策树时只使用一部分训练样本、借鉴随机森林的思路在学习单颗决策树时只采样一部分特征、在目标函数中添加正则项惩罚复杂的树结构等。现在主流的GBDT算法实现中这些方法基本上都有实现，因此GBDT算法的超参数还是比较多的，应用过程中需要精心调参，并用交叉验证的方法选择最佳参数。 本文对GBDT算法原理进行介绍，从机器学习的关键元素出发，一步一步推导出GBDT算法背后的理论基础，读者可以从这个过程中了解到GBDT算法的来龙去脉。对于该算法的工程实现，本文也有较好的指导意义，实际上对机器学习关键概念元素的区分对应了软件工程中的“开放封闭原则”的思想，基于此思想的实现将会具有很好的模块独立性和扩展性。 一、机器学习的关键元素先复习下监督学习的关键概念：模型（model）、参数（parameters）、目标函数（objective function） 模型就是所要学习的条件概率分布或者决策函数，它决定了在给定特征向量x时如何预测出目标y。定义$x_i\in R^d$为训练集中的第$i$个训练样本，则线性模型（linear model）可以表示为：$\hat{y}=\sum_jw_jx_{ij }$。模型预测的分数$\hat{y_i}$在不同的任务中有不同的解释。例如在逻辑回归任务中，$1/(1+exp(-\hat{y}_i))$表示模型预测为正例的概率；而在排序学习任务中，$\hat{y_i}$表示排序分。 参数就是我们要从数据中学习得到的内容。模型通常是由一个参数向量决定的函数。例如，线性模型的参数可以表示为：$\Theta=\{w_j|j=1,\cdots,d\}$ 目标函数通常定义为如下形式： Obj(\Theta)=L(\Theta)+\Omega(\Theta)其中，$L(\Theta)$是损失函数，用来衡量模型拟合训练数据的好坏程度；$\Omega(\Theta)$称之为正则项，用来衡量学习到的模型的复杂度。训练集上的损失（Loss）定义为：$L=\sum_{i=1}^n l(y_i, \hat{y}_i)$。常用的损失函数有平方损失（square loss）：$l(y_i, \hat{y}_i)=(y_i - \hat{y}_i)^2$；Logistic损失： $l(y_i, \hat{y}_i)=y_i ln(1+e^{y_i}) + (1-y_i)ln(1+e^{\hat{y}_i})$。常用的正则项有L1范数$\Omega(w)=\lambda \Vert w \Vert_1$和L2范数$\Omega(w)=\lambda \Vert w \Vert_2$。Ridge regression就是指使用平方损失和L2范数正则项的线性回归模型；Lasso regression就是指使用平方损失和L1范数正则项的线性回归模型；逻辑回归（Logistic regression）指使用logistic损失和L2范数或L1范数正则项的线性模型。 目标函数之所以定义为损失函数和正则项两部分，是为了尽可能平衡模型的偏差和方差（Bias Variance Trade-off）。最小化目标函数意味着同时最小化损失函数和正则项，损失函数最小化表明模型能够较好的拟合训练数据，一般也预示着模型能够较好地拟合真实数据（groud true）；另一方面，对正则项的优化鼓励算法学习到较简单的模型，简单模型一般在测试样本上的预测结果比较稳定、方差较小（奥坎姆剃刀原则）。也就是说，优化损失函数尽量使模型走出欠拟合的状态，优化正则项尽量使模型避免过拟合。 从概念上区分模型、参数和目标函数给学习算法的工程实现带来了益处，使得机器学习的各个组成部分之间耦合尽量松散。 二、加法模型GBDT算法可以看成是由K棵树组成的加法模型： \hat{y}_i=\sum_{k=1}^K f_k(x_i), f_k \in F \tag 0其中$F$为所有树组成的函数空间，以回归任务为例，回归树可以看作为一个把特征向量映射为某个score的函数。该模型的参数为：$\Theta=\{f_1,f_2, \cdots, f_K \}$。于一般的机器学习算法不同的是，加法模型不是学习d维空间中的权重，而是直接学习函数（决策树）集合。 上述加法模型的目标函数定义为：$Obj=\sum_{i=1}^n l(y_i, \hat{y}_i) + \sum_{k=1}^K \Omega(f_k)$，其中表示决策树的复杂度，那么该如何定义树的复杂度呢？比如，可以考虑树的节点数量、树的深度或者叶子节点所对应的分数的L2范数等等。 如何来学习加法模型呢？ 解这一优化问题，可以用前向分布算法（forward stagewise algorithm）。因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数（结构），逐步逼近优化目标函数，那么就可以简化复杂度。这一学习过程称之为Boosting。具体地，我们从一个常量预测开始，每次学习一个新的函数，过程如下： \begin{split} \hat{y}_i^0 &= 0 \\ \hat{y}_i^1 &= f_1(x_i) = \hat{y}_i^0 + f_1(x_i) \\ \hat{y}_i^2 &= f_1(x_i) + f_2(x_i) = \hat{y}_i^1 + f_2(x_i) \\ & \cdots \\ \hat{y}_i^t &= \sum_{k=1}^t f_k(x_i) = \hat{y}_i^{t-1} + f_t(x_i) \\ \end{split}那么，在每一步如何决定哪一个函数$f$被加入呢？指导原则还是最小化目标函数。在第$t$步，模型对$x_i$的预测为：$\hat{y}_i^t= \hat{y}_i^{t-1} + f_t(x_i)$，其中$f_t(x_i)$为这一轮我们要学习的函数（决策树）。这个时候目标函数可以写为： \begin{split} Obj^{(t)} &= \sum_{i=1}^nl(y_i, \hat{y}_i^t) + \sum_{i=i}^t \Omega(f_i) \\ &= \sum_{i=1}^n l\left(y_i, \hat{y}_i^{t-1} + f_t(x_i) \right) + \Omega(f_t) + constant \end{split}\tag{1}举例说明，假设损失函数为平方损失（square loss），则目标函数为： \begin{split} Obj^{(t)} &= \sum_{i=1}^n \left(y_i - (\hat{y}_i^{t-1} + f_t(x_i)) \right)^2 + \Omega(f_t) + constant \\ &= \sum_{i=1}^n \left[2(\hat{y}_i^{t-1} - y_i)f_t(x_i) + f_t(x_i)^2 \right] + \Omega(f_t) + constant \end{split}\tag{2}其中$(\hat{y}_i^{t-1} - y_i)$，称之为残差（residual）。因此，使用平方损失函数时，GBDT算法的每一步在生成决策树时只需要拟合前面的模型的残差。 泰勒公式：设$n$是一个正整数，如果定义在一个包含$a$的区间上的函数$f$在$a$点处$n+1$次可导，那么对于这个区间上的任意$x$都有： \displaystyle f(x)=\sum _{n=0}^{N}\frac{f^{(n)}(a)}{n!}(x-a)^ n+R_ n(x)，其中的多项式称为函数在$a$处的泰勒展开式，$R_n(x)$是泰勒公式的余项且是$(x-a)^n$的高阶无穷小。——维基百科 根据泰勒公式把函数$f(x+\Delta x)$在点处二阶展开，可得到如下等式： f(x+\Delta x) \approx f(x) + f'(x)\Delta x + \frac12 f''(x)\Delta x^2 \tag 3由等式(1)可知，目标函数是关于变量$\hat{y}_i^{t-1} + f_t(x_i)$若把变量$\hat{y}_i^{t-1}$看成是等式(3)中的$x$，把变量$f_t(x_i)$看成是等式(3)中的$\Delta x$，则等式(1)可转化为： Obj^{(t)} = \sum_{i=1}^n \left[ l(y_i, \hat{y}_i^{t-1}) + g_if_t(x_i) + \frac12h_if_t^2(x_i) \right] + \Omega(f_t) + constant \tag 4其中，$g_i$定义为损失函数的一阶导数，即$g_i=\partial_{\hat{y}^{t-1}}l(y_i,\hat{y}^{t-1})$；$h_i$定义为损失函数的二阶导数，即$h_i=\partial_{\hat{y}^{t-1}}^2l(y_i,\hat{y}^{t-1})$。 假设损失函数为平方损失函数，则$g_i=\partial_{\hat{y}^{t-1}}(\hat{y}^{t-1} - y_i)^2 = 2(\hat{y}^{t-1} - y_i)$，$h_i=\partial_{\hat{y}^{t-1}}^2(\hat{y}^{t-1} - y_i)^2 = 2$，把$g_i$和$h_i$代入等式(4)即得等式(2)。由于函数中的常量在函数最小化的过程中不起作用，因此我们可以从等式(4)中移除掉常量项，得： Obj^{(t)} \approx \sum_{i=1}^n \left[ g_if_t(x_i) + \frac12h_if_t^2(x_i) \right] + \Omega(f_t) \tag 5由于要学习的函数仅仅依赖于目标函数，从等式(5)可以看出只需为学习任务定义好损失函数，并为每个训练样本计算出损失函数的一阶导数和二阶导数，通过在训练样本集上最小化等式(5)即可求得每步要学习的函数$f(x)$，从而根据加法模型等式(0)可得最终要学习的模型。 二、GBDT算法一颗生成好的决策树，假设其叶子节点个数为$T$，该决策树是由所有叶子节点对应的值组成的向量$w \in R^T$，以及一个把特征向量映射到叶子节点索引（Index）的函数$q:R^d \to \{1,2,\cdots,T\}$组成的。因此，决策树可以定义为$f_t(x)=w_{q(x)}$。 决策树的复杂度可以由正则项$\Omega(f_t)=\gamma T + \frac12 \lambda \sum_{j=1}^T w_j^2$来定义，即决策树模型的复杂度由生成的树的叶子节点数量和叶子节点对应的值向量的L2范数决定。 定义集合$I_j=\{ i \vert q(x_i)=j \}$为所有被划分到叶子节点的训练样本的集合。等式(5)可以根据树的叶子节点重新组织为T个独立的二次函数的和： \begin{split} Obj^{(t)} &\approx \sum_{i=1}^n \left[ g_if_t(x_i) + \frac12h_if_t^2(x_i) \right] + \Omega(f_t) \\ &= \sum_{i=1}^n \left[ g_iw_{q(x_i)} + \frac12h_iw_{q(x_i)}^2 \right] + \gamma T + \frac12 \lambda \sum_{j=1}^T w_j^2 \\ &= \sum_{j=1}^T \left[(\sum_{i \in I_j}g_i)w_j + \frac12(\sum_{i \in I_j}h_i + \lambda)w_j^2 \right] + \gamma T \end{split}\tag 6定义$G_j=\sum_{i \in I_j}g_i$，$H_j=\sum_{i \in I_j}h_i$，则等式(6)可写为： Obj^{(t)} = \sum_{j=1}^T \left[G_iw_j + \frac12(H_i + \lambda)w_j^2 \right] + \gamma T假设树的结构是固定的，即函数$q(x)$确定，令函数$Obj^{(t)}$的一阶导数等于0，即可求得叶子节点对应的值为：$w_j^*=-\frac{G_j}{H_j+\lambda} \tag 7$此时，目标函数的值为 Obj = -\frac12 \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T \tag 8综上，为了便于理解，单颗决策树的学习过程可以大致描述为： 枚举所有可能的树结构$q$ 用等式(8)为每个$q$计算其对应的分数$Obj$，分数越小说明对应的树结构越好。 根据上一步的结果，找到最佳的树结构，用等式(7)为树的每个叶子节点计算预测值 然而，可能的树结构数量是无穷的，所以实际上我们不可能枚举所有可能的树结构。通常情况下，我们采用贪心策略来生成决策树的每个节点。 从深度为0的树开始，对每个叶节点枚举所有的可用特征 针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益） 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集 回到第1步，递归执行到满足特定条件为止 在上述算法的第二步，样本排序的时间复杂度为$O(nlogn)$，假设共用K个特征，那么生成一颗深度为K的树的时间复杂度为$O(dKnlogn)$。具体实现可以进一步优化计算复杂度，比如可以缓存每个特征的排序结果等。 如何计算每次分裂的收益呢？假设当前节点记为,分裂之后左孩子节点记为，右孩子节点记为，则该分裂获得的收益定义为当前节点的目标函数值减去左右两个孩子节点的目标函数值之和：$Gain=Obj_C-Obj_L-Obj_R$，具体地，根据等式(8)可得： Gain=\frac12 \left[ \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma \tag 9其中，$-\gamma$项表示因为增加了树的复杂性（该分裂增加了一个叶子节点）带来的惩罚。等式(9)还可以用来计算输入特征的相对重要程度，具体见下一节 最后，总结一下GBDT的学习算法： 算法每次迭代生成一颗新的决策树 在每次迭代开始之前，计算损失函数在每个训练样本点的一阶导数$g_i$和二阶导数$h_i$ 通过贪心策略生成新的决策树，通过等式(7)计算每个叶节点对应的预测值 把新生成的决策树$f_t(x)$添加到模型中：$\hat{y}_i^t = \hat{y}_i^{t-1} + f_t(x_i)$ 通常在第四步，我们把模型更新公式替换为：$\hat{y}_i^t = \hat{y}_i^{t-1} + \epsilon f_t(x_i)$，其中$\epsilon$称之为步长或者学习率。增加因子的目的是为了避免模型过拟合。 三、特征重要度集成学习因具有预测精度高的优势而受到广泛关注，尤其是使用决策树作为基学习器的集成学习算法。树的集成算法的著名代码有随机森林和GBDT。随机森林具有很好的抵抗过拟合的特性，并且参数（决策树的个数）对预测性能的影响较小，调参比较容易，一般设置一个比较大的数。GBDT具有很优美的理论基础，一般而言性能更有优势。 基于树的集成算法还有一个很好的特性，就是模型训练结束后可以输出模型所使用的特征的相对重要度，便于我们选择特征，理解哪些因素是对预测有关键影响，这在某些领域（如生物信息学、神经系统科学等）特别重要。本文主要介绍基于树的集成算法如何计算各特征的相对重要度。 3.1 优势 使用不同类型的数据时，不需要做特征标准化/归一化 可以很容易平衡运行时效率和精度；比如，使用boosted tree作为在线预测的模型可以在机器资源紧张的时候截断参与预测的树的数量从而提高预测效率 学习模型可以输出特征的相对重要程度，可以作为一种特征选择的方法 模型可解释性好 对数据字段缺失不敏感 能够自动做多组特征间的interaction，具有很好的非性线性 3.2 特征重要度的计算Friedman在GBM的论文中提出的方法： 特征$j$的全局重要度通过特征$j$在单颗树中的重要度的平均值来衡量： \hat{J_{j}^2}=\frac1M \sum_{m=1}^M\hat{J_{j}^2}(T_m)其中，M是树的数量。特征$j$在单颗树中的重要度的如下： \hat{J_{j}^2}(T)=\sum\limits_{t=1}^{L-1} \hat{i_{t}^2} 1(v_{t}=j)其中，$L$为树的叶子节点数量，$L-1$即为树的非叶子节点数量（构建的树都是具有左右孩子的二叉树），是和节点$t$相关联的特征，$\hat{i_t^2}$是节点分裂之后平方损失的减少值。 3.3 实现代码为了更好的理解特征重要度的计算方法，下面给出scikit-learn工具包中的实现，代码移除了一些不相关的部分。 下面的代码来自于GradientBoostingClassifier对象的feature_importances属性的计算方法： 123456def feature_importances_(self): total_sum = np.zeros((self.n_features, ), dtype=np.float64) for tree in self.estimators_: total_sum += tree.feature_importances_ importances = total_sum / len(self.estimators_) return importances 其中，self.estimators_是算法构建出的决策树的数量，tree.feature_importances_ 是单棵树的特征重要度向量，其计算方法如下： 1234567891011121314cpdef compute_feature_importances(self, normalize=True): &quot;&quot;&quot;Computes the importance of each feature (aka variable).&quot;&quot;&quot; while node != end_node: if node.left_child != _TREE_LEAF: # ... and node.right_child != _TREE_LEAF: left = &amp;nodes[node.left_child] right = &amp;nodes[node.right_child] importance_data[node.feature] += ( node.weighted_n_node_samples * node.impurity - left.weighted_n_node_samples * left.impurity - right.weighted_n_node_samples * right.impurity) node += 1 importances /= nodes[0].weighted_n_node_samples return importances 上面的代码关键点是两个： 第一点：weighted_n_node_samples : array of int, shape [node_count] weighted_n_node_samples[i] holds the weighted number of training samples reaching node i. 第二点：impurity : array of double, shape [node_count] impurity[i] holds the impurity (i.e., the value of the splitting criterion) at node i. 当然上面的代码经过了简化，保留了核心思想。计算所有的非叶子节点在分裂时加权不纯度的减少，减少得越多说明特征越重要 不纯度的减少实际上就是该节点此次分裂的收益，因此我们也可以这样理解，节点分裂时收益越大，该节点对应的特征的重要度越高。关于收益的定义就是上一节中等式(9)的定义。 参考资料[1] Gradient Boosting 的更多内容[2] XGBoost是一个优秀的GBDT开源软件库，有多种语言接口[3] Pyramid是一个基于Java语言的机器学习库，里面也有GBDT算法的介绍和实现[4] Friedman的论文《Greedy function approximation: a gradient boosting machine》是比较早的GBDT算法文献，但是比较晦涩难懂，不适合初学者，高阶选手可以进一步学习[5] “A Gentle Introduction to Gradient Boosting”是关于Gradient Boosting的一个通俗易懂的解释，比较适合初学者或者是已经对GBDT算法原理印象不深的从业者[6] 关于GBDT算法调参的经验和技巧可以参考这两篇博文：《GBM调参指南》、《XGBoost调参指南》，作者使用的算法实现工具来自于著名的Python机器学习工具scikit-learn[7] GBDT算法在搜索引擎排序中的应用可以查看这篇论文《Web-Search Ranking with Initialized Gradient Boosted Regression Trees 》，这篇论文提出了一个非常有意思的方法，用一个已经训练好的随机森林模型作为GBDT算法的初始化，再用GBDT算法优化最终的模型，取得了很好的效果 [1] Feature Selection for Ranking using Boosted Trees [2] Gradient Boosted Feature Selection[3] Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（35）：使用Sklearn进行集成学习（实践）]]></title>
    <url>%2F2017%2F09%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8835%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Sklearn%E8%BF%9B%E8%A1%8C%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AE%9E%E8%B7%B5%EF%BC%89%2F</url>
    <content type="text"><![CDATA[待更新]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（34）：使用Sklearn进行集成学习（理论）]]></title>
    <url>%2F2017%2F09%2F02%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8834%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Sklearn%E8%BF%9B%E8%A1%8C%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89%2F</url>
    <content type="text"><![CDATA[转载自jasonfreak的个人博客 一、前言很多人在竞赛（Kaggle，天池等）或工程实践中使用了集成学习（例如，RF、GTB等），确实也取得了不错的效果，在保证准确度的同时也提升了模型防止过拟合的能力。但是，我们真的用对了集成学习吗？ sklearn提供了sklearn.ensemble库，支持众多集成学习算法和模型。恐怕大多数人使用这些工具时，要么使用默认参数，要么根据模型在测试集上的性能试探性地进行调参（当然，完全不懂的参数还是不动算了），要么将调参的工作丢给调参算法（网格搜索等）。这样并不能真正地称为“会”用sklearn进行集成学习。 我认为，学会调参是进行集成学习工作的前提。然而，第一次遇到这些算法和模型时，肯定会被其丰富的参数所吓到，要知道，教材上教的伪代码可没这么多参数啊！！！没关系，暂时，我们只要记住一句话：参数可分为两种，一种是影响模型在训练集上的准确度或影响防止过拟合能力的参数；另一种不影响这两者的其他参数。模型在样本总体上的准确度（后简称准确度）由其在训练集上的准确度及其防止过拟合的能力所共同决定，所以在调参时，我们主要对第一种参数进行调整，最终达到的效果是：模型在训练集上的准确度和防止过拟合能力的大和谐！ 本篇博文将详细阐述模型参数背后的理论知识，在下篇博文中，我们将对最热门的两个模型Random Forrest和Gradient Tree Boosting（含分类和回归，所以共4个模型）进行具体的参数讲解。如果你实在无法静下心来学习理论，你也可以在下篇博文中找到最直接的调参指导，虽然我不赞同这么做。 二、集成学习是什么？我们还是花一点时间来说明一下集成学习是什么，如果对此有一定基础的同学可以跳过本节。简单来说，集成学习是一种技术框架，其按照不同的思路来组合基础模型，从而达到其利断金的目的。 目前，有三种常见的集成学习框架：bagging，boosting和stacking。国内，南京大学的周志华教授对集成学习有很深入的研究，其在09年发表的一篇概述性论文《Ensemble Learning》对这三种集成学习框架有了明确的定义，概括如下： bagging：从训练集从进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果： boosting：训练过程为阶梯状，基模型按次序一一进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化。对所有基模型预测的结果进行线性综合产生最终的预测结果： stacking：将训练好的所有基模型对训练基进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测： 有了这些基本概念之后，直觉将告诉我们，由于不再是单一的模型进行预测，所以模型有了“集思广益”的能力，也就不容易产生过拟合现象。但是，直觉是不可靠的，接下来我们将从模型的偏差和方差入手，彻底搞清楚这一问题。 三、偏差和方差广义的偏差（bias）描述的是预测值和真实值之间的差异，方差（variance）描述距的是预测值作为随机变量的离散程度。《Understanding the Bias-Variance Tradeoff》当中有一副图形象地向我们展示了偏差和方差的关系： 3.1 模型的偏差和方差模型的偏差是一个相对来说简单的概念：训练出来的模型在训练集上的准确度。 定义随机变量的值的差异是计算方差的前提条件，通常来说，我们遇到的都是数值型的随机变量，数值之间的差异再明显不过（减法运算）。但是，模型的差异性呢？我们可以理解模型的差异性为模型的结构差异，例如：线性模型中权值向量的差异，树模型中树的结构差异等。在研究模型方差的问题上，我们并不需要对方差进行定量计算，只需要知道其概念即可。 研究模型的方差有什么现实的意义呢？我们认为方差越大的模型越容易过拟合：假设有两个训练集A和B，经过A训练的模型Fa与经过B训练的模型Fb差异很大，这意味着Fa在类A的样本集合上有更好的性能，而Fb反之，这便是我们所说的过拟合现象。 我们常说集成学习框架中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型。但是，并不是所有集成学习框架中的基模型都是弱模型。bagging和stacking中的基模型为强模型（偏差低方差高），boosting中的基模型为弱模型。 在bagging和boosting框架中，通过计算基模型的期望和方差，我们可以得到模型整体的期望和方差。为了简化模型，我们假设基模型的权重、方差及两两间的相关系数相等。由于bagging和boosting的基模型都是线性组成的，那么有： 3.2 bagging的偏差和方差对于bagging来说，每个基模型的权重等于1/m且期望近似相等（子训练集都是从原训练集中进行子抽样），故我们可以进一步化简得到： 根据上式我们可以看到，整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似。同时，整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。但是，模型的准确度一定会无限逼近于1吗？并不一定，当基模型数增加到一定程度时，方差公式第二项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。另外，在此我们还知道了为什么bagging中的基模型一定要为强模型，否则就会导致整体模型的偏差度低，即准确度低。 Random Forest是典型的基于bagging框架的模型，其在bagging的基础上，进一步降低了模型的方差。Random Fores中基模型是树模型，在树的内部节点分裂过程中，不再是将所有特征，而是随机抽样一部分特征纳入分裂的候选项。这样一来，基模型之间的相关性降低，从而在方差公式中，第一项显著减少，第二项稍微增加，整体方差仍是减少。 3.3 boosting的偏差和方差对于boosting来说，基模型的训练集抽样是强相关的，那么模型的相关系数近似等于1，故我们也可以针对boosting化简公式为： 通过观察整体方差的表达式，我们容易发现，若基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，boosting框架中的基模型必须为弱模型。 因为基模型为弱模型，导致了每个基模型的准确度都不是很高（因为其在训练集上的准确度不高）。随着基模型数的增多，整体模型的期望值增加，更接近真实值，因此，整体模型的准确度提高。但是准确度一定会无限逼近于1吗？仍然并不一定，因为训练过程中准确度的提高的主要功臣是整体模型在训练集上的准确度提高，而随着训练的进行，整体模型的方差变大，导致防止过拟合的能力变弱，最终导致了准确度反而有所下降。 基于boosting框架的Gradient Tree Boosting模型中基模型也为树模型，同Random Forrest，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。 3.4 模型的独立性聪明的读者这时肯定要问了，如何衡量基模型的独立性？我们说过，抽样的随机性决定了模型的随机性，如果两个模型的训练集抽样过程不独立，则两个模型则不独立。这时便有一个天大的陷阱在等着我们：bagging中基模型的训练样本都是独立的随机抽样，但是基模型却不独立呢？ 我们讨论模型的随机性时，抽样是针对于样本的整体。而bagging中的抽样是针对于训练集（整体的子集），所以并不能称其为对整体的独立随机抽样。那么到底bagging中基模型的相关性体现在哪呢？在知乎问答《为什么说bagging是减少variance，而boosting是减少bias?》中请教用户“过拟合”后，我总结bagging的抽样为两个过程： 样本抽样：整体模型F(X1, X2, …, Xn)中各输入随机变量（X1, X2, …, Xn）对样本的抽样 子抽样：从整体模型F(X1, X2, …, Xn)中随机抽取若干输入随机变量成为基模型的输入随机变量 假若在子抽样的过程中，两个基模型抽取的输入随机变量有一定的重合，那么这两个基模型对整体样本的抽样将不再独立，这时基模型之间便具有了相关性。 3.5 小结还记得调参的目标吗：模型在训练集上的准确度和防止过拟合能力的大和谐！为此，我们目前做了一些什么工作呢？ 使用模型的偏差和方差来描述其在训练集上的准确度和防止过拟合的能力 对于bagging来说，整体模型的偏差和基模型近似，随着训练的进行，整体模型的方差降低 对于boosting来说，整体模型的初始偏差较高，方差较低，随着训练的进行，整体模型的偏差降低（虽然也不幸地伴随着方差增高），当训练过度时，因方差增高，整体模型的准确度反而降低 整体模型的偏差和方差与基模型的偏差和方差息息相关 这下总算有点开朗了，那些让我们抓狂的参数，现在可以粗略地分为两类了：控制整体训练过程的参数和基模型的参数，这两类参数都在影响着模型在训练集上的准确度以及防止过拟合的能力。 四、Gradient Boosting对基于Gradient Boosting框架的模型的进行调试时，我们会遇到一个重要的概念：损失函数。在本节中，我们将把损失函数的“今生来世”讲个清楚！ 基于boosting框架的整体模型可以用线性组成式来描述，其中$hi$为基模型与其权值的乘积： 根据上式，整体模型的训练目标是使预测值F(x)逼近真实值y，也就是说要让每一个基模型的预测值逼近各自要预测的部分真实值。由于要同时考虑所有基模型，导致了整体模型的训练变成了一个非常复杂的问题。所以，研究者们想到了一个贪心的解决手段：每次只训练一个基模型。那么，现在改写整体模型为迭代式： 这样一来，每一轮迭代中，只要集中解决一个基模型的训练问题：使$Fi$逼近真实值y。 4.1 拟合残差使Fi逼近真实值，其实就是使hi逼近真实值和上一轮迭代的预测值Fi-1之差，即残差（y-Fi-1）。最直接的做法是构建基模型来拟合残差，在博文《GBDT（MART） 迭代决策树入门教程 | 简介》中，作者举了一个生动的例子来说明通过基模型拟合残差，最终达到整体模型F(x)逼近真实值。 研究者发现，残差其实是最小均方损失函数的关于预测值的反向梯度： 也就是说，若$Fi-1$加上拟合了反向梯度的$hi$得到$Fi$，该值可能将导致平方差损失函数降低，预测的准确度提高！这显然不是巧合，但是研究者们野心更大，希望能够创造出一种对任意损失函数都可行的训练方法，那么仅仅拟合残差是不恰当的了。 4.2 拟合反向梯度4.2.1 契机：引入任意损失函数引入任意损失函数后，我们可以定义整体模型的迭代式如下： 在这里，损失函数被定义为泛函。 4.2.2 难题一：任意损失函数的最优化对任意损失函数（且是泛函）的最优化是困难的。我们需要打破思维的枷锁，将整体损失函数L’定义为n元普通函数（n为样本容量），损失函数L定义为2元普通函数（记住！！！这里的损失函数不再是泛函！！！）： 我们不妨使用梯度最速下降法来解决整体损失函数L’最小化的问题，先求整体损失函数的反向梯度： 假设已知样本x的当前预测值为$Fi-1$，下一步将预测值按照反向梯度，依照步长为$r[i]$，进行更新： 步长r[i]不是固定值，而是设计为： 4.2.3 难题二：无法对测试样本计算反向梯度问题又来了，由于测试样本中y是未知的，所以无法求反向梯度。这正是Gradient Boosting框架中的基模型闪亮登场的时刻！在第i轮迭代中，我们创建训练集如下： 也就是说，让基模型拟合反向梯度函数，这样我们就可以做到只输入x这一个参数，就可求出其对应的反向梯度了（当然，通过基模型预测出来的反向梯度并不是准确的，这也提供了泛化整体模型的机会）。 综上，假设第i轮迭代中，根据新训练集训练出来的基模型为$fi$，那么最终的迭代公式为： 4.3 常见的损失函数ls：最小均方回归中用到的损失函数。在之前我们已经谈到，从拟合残差的角度来说，残差即是该损失函数的反向梯度值（所以又称反向梯度为伪残差）。不同的是，从拟合残差的角度来说，步长是无意义的。该损失函数是sklearn中Gradient Tree Boosting回归模型默认的损失函数。 deviance：逻辑回归中用到的损失函数。熟悉逻辑回归的读者肯定还记得，逻辑回归本质是求极大似然解，其认为样本服从几何分布，样本属于某类别的概率可以logistic函数表达。所以，如果该损失函数可用在多类别的分类问题上，故其是sklearn中Gradient Tree Boosting分类模型默认的损失函数。 exponential：指数损失函数，表达式为： 对该损失函数求反向梯度得： 这时，在第i轮迭代中，新训练集如下：脑袋里有什么东西浮出水面了吧？让我们看看Adaboost算法中，第i轮迭代中第j个样本权值的更新公式：样本的权值什么时候会用到呢？计算第i轮损失函数的时候会用到：让我们再回过头来，看看使用指数损失函数的Gradient Boosting计算第i轮损失函数： 天呐，两个公式就差了一个对权值的归一项。这并不是巧合，当损失函数是指数损失时，Gradient Boosting相当于二分类的Adaboost算法。是的，指数损失仅能用于二分类的情况。 4.4 步子太大容易扯着蛋：缩减缩减也是一个相对显见的概念，也就是说使用Gradient Boosting时，每次学习的步长缩减一点。这有什么好处呢？缩减思想认为每次走一小步，多走几次，更容易逼近真实值。如果步子迈大了，使用最速下降法时，容易迈过最优点。将缩减代入迭代公式： 缩减需要配合基模型数一起使用，当缩减率v降低时，基模型数要配合增大，这样才能提高模型的准确度。 4.5 初始模型还有一个不那么起眼的问题，初始模型$F0$是什么呢？如果没有定义初始模型，整体模型的迭代式一刻都无法进行！所以，我们定义初始模型为： 根据上式可知，对于不同的损失函数来说，初始模型也是不一样的。对所有的样本来说，根据初始模型预测出来的值都一样。 4.5 Gradient Tree Boosting终于到了备受欢迎的Gradient Tree Boosting模型了！但是，可讲的却已经不多了。我们已经知道了该模型的基模型是树模型，并且可以通过对特征的随机抽样进一步减少整体模型的方差。我们可以在维基百科的Gradient Boosting词条中找到其伪代码实现。 4.6 小结到此，读者应当很清楚Gradient Boosting中的损失函数有什么意义了。要说偏差描述了模型在训练集准确度，则损失函数则是描述该准确度的间接量纲。也就是说，模型采用不同的损失函数，其训练过程会朝着不同的方向进行！ 五、总结 磨刀不误砍柴功，我们花了这么多时间来学习必要的理论，我强调一次：必要的理论！集成学习模型的调参工作的核心就是找到合适的参数，能够使整体模型在训练集上的准确度和防止过拟合的能力达到协调，从而达到在样本总体上的最佳准确度。有了本文的理论知识铺垫，在下篇中，我们将对Random Forest和Gradient Tree Boosting中的每个参数进行详细阐述，同时也有一些小试验证明我们的结论。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Scikit-Learn</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（33）：特征处理（Feature Processing）]]></title>
    <url>%2F2017%2F08%2F31%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8833%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86%EF%BC%88Feature%20Processing%EF%BC%89%2F</url>
    <content type="text"><![CDATA[特征工程（Feature Engineering）经常被说为机器学习中的black art，这里面包含了很多不可言说的方面。怎么处理好特征，最重要的当然还是对要解决问题的了解。但是，它其实也有很多科学的地方。这篇文章我之所以命名为特征处理（Feature Processing），是因为这里面要介绍的东西只是特征工程中的一小部分。这部分比较基础，比较容易说，所以由此开始。 单个原始特征（或称为变量）通常属于以下几类之一： 连续（continuous）特征； 无序类别（categorical）特征； 有序类别（ordinal）特征。 本文中我主要介绍针对单个特征的处理方法，虽然也会附带介绍基础的特征组合方法。同时处理多个特征，以及更复杂的特征处理方法介绍，以后我再另外细说。下面我由浅入深地逐渐说明针对这三类特征的常用处理方法。 一、初级篇这节要讲的处理技术，应该刚接触机器学习不久的同学都会知道。 1.1 连续特征1.2 无序特征可以使用One-hot（也叫One-of-k）的方法把每个无序特征转化为一个数值向量。比如一个无序特征color有三种取值：red，green，blue。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于red，green，blue。如： color取值 向量表示 red (1, 0, 0) green (0, 1, 0) blue (0, 0, 1) 这种方法在NLP里用的很多，就是所谓的词向量模型。变换后的向量长度对于词典长度，每个词对应于向量中的一个元素。 机器学习书籍里在讲这个的时候介绍的处理方法可能跟我上面说的有点差别。上面说的表达方式里有一个维度是可以省略的。 既然我们知道color一定是取3个值中的一个，那么我们知道向量的前两个元素值，就能推断第3个值是多少。所以，其实用下面的方式就可以表达到底是哪种颜色： color取值 向量表示 red (1, 0) green (0, 1) blue (0, 0) 这样表达的好处是少用了一个维度，降低了转化后特征之间的相关性。但在实际问题中特征基本都或多或少会有些缺失。使用第一种表达方式就可以用全0的向量来表示值缺失，而第二种表达方式是没法表达缺失的。 1.3 有序特征有些特征虽然也像无序特征那样只取限定的几个值，但是这些值之间有顺序的含义。例如一个人的状态status有三种取值：bad, normal, good，显然bad &lt; normal &lt; good。 当然，对有序特征最简单的处理方式是忽略其中的顺序关系，把它看成无序的，这样我们就可以使用处理无序特征的方式来处理它。在实际问题中，这种处理方式其实用的很多。 当然有些问题里有序可能会很重要，这时候就不应该把其中的顺序关系丢掉。一般的表达方式如下： status取值 向量表示 bad (1, 0, 0) normal (1, 1, 0) good (1, 1, 1) 上面这种表达方式很巧妙地利用递进表达了值之间的顺序关系。 二、中级篇最容易让人掉以轻心的，往往就是大家觉得最简单的事。在特征处理中，最容易让刚入门同学忽略的，是对连续特征的处理方式。 以线性分类器Linear Regression (LinearReg)为例，它是通过特征的线性加权来预测因变量y： y=w^Tx但大部分实际情况下，y与x都不会是这么简单的线性关系，甚至连单调关系都不会有。举个只有一个特征的例子，如果y与x的实际关系如下图： 那么直接把x扔进LinearReg模型是怎么也得不到好结果的。很多人会想着既然线性分类器搞不定，那就直接找个非线性的好了，比如高斯核的SVM。我们确实可以通过这种简单换算法的方式解决这个简单的问题。但对于很多实际问题（如广告点击率预测），往往特征非常多，这时候时间约束通常不允许我们使用很复杂的非线性分类器。这也是为什么算法发展这么多年，广告点击率预测最常用的方法还是Logistic Regression (LogisticReg)。 对于上面这个问题，有没有什么办法使得LinearReg也能处理得不错？当然是有，就是对原始特征x做转化，把原来的非线性关系转化为线性关系。 2.1 方法一：离散化最常用的转化方式是对xx做离散化(discretization)，也就是把原来的值分段，转化成一个取值为0或1的向量。原始值落在某个段里，向量中此段对应的元素就为1，否则为0。 离散化的目标是y与转化后向量里的每个元素都保持比较好的线性关系。 比如取离散点{0.5,1.5,2.5}，通过判断xx属于(−∞,0.5)，[0.5,1.5)，[1.5,2.5)，[2.5,+∞)中哪段来把它离散化为4维的向量。下面是一些例子的离散结果： 原始值xx 离散化后的值 0.1 (1, 0, 0, 0) 1.3 (0, 1, 0, 0) 3.2 (0, 0, 0, 1) 5.8 (0, 0, 0, 1) 离散化方法的关键是怎么确定分段中的离散点。下面是常用的选取离散点的方法： 等距离离散： 顾名思义，就是离散点选取等距点。我们上面对xx取离散点{0.5,1.5,2.5}就是一种等距离散，见下图。图中垂直的灰线代表离散点。 等样本点离散 选取的离散点保证落在每段里的样本点数量大致相同，见下图。 画图观察趋势 以x为横坐标，y为纵坐标，画图，看曲线的趋势和拐点。通过观察下面的图我们发现可以利用3条直线（红色直线）来逐段近似原来的曲线。把离散点设为两条直线相交的各个点，我们就可以把xx离散化为长度为3的向量。 上面介绍的这种离散化为0/1向量的方法有个问题，它在离散时不会考虑到具体的xx到离散边界的距离。比如等距离散中取离散点为{0.5,1.5,2.5}{0.5,1.5,2.5}，那么1.499，1.501和2.49分别会离散为(0, 1, 0, 0)，(0, 0, 1, 0)和(0, 0, 1, 0)。1.499和1.501很接近，可是就因为这种强制分段的离散导致它们离散的结果差距很大。 针对上面这种硬离散的一种改进就是使用软离散，也就是在离散时考虑到xx与附近离散点的距离，离散出来的向量元素值可以是0/1之外的其他值。有兴趣的同学可以去ESL1这本书中找点感觉。 2.2 函数变换函数变换直接把原来的特征通过非线性函数做变换，然后把原来的特征，以及变换后的特征一起加入模型进行训练。常用的变换函数见下表，不过其实你可以尝试任何函数。 常用非线性函数f(x) x的取值范围 $x^α; α∈(−∞,+∞)$ $(−∞,+∞)$ $log(x)$ $(0,+∞)$ $log(\frac{x}{1−x})$ $(0,1)$ 这个方法操作起来很简单，但记得对新加入的特征做归一化。 对于我们前面的问题，只要把$x^2$，$x^3$也作为特征加入即可，因为实际上y就是x的一个三次多项式。 三、高级篇3.1 笛卡尔乘积我们可以使用笛卡尔乘积的方式来组合2个或更多个特征。比如有两个类别特征color和light，它们分别可以取值为red，green，blue和on, off。这两个特征各自可以离散化为3维和2维的向量。对它们做笛卡尔乘积转化，就可以组合出长度为6的特征，它们分别对应着原始值对(red, on)，(red, off)，(green, on)，(green, off)，(blue, on)，(blue, off)。下面的矩阵表达方式更清楚地说明了这种组合。 X on off red green blue 对于3个特征的笛卡尔乘积组合，可以表达为立方的形式。更多特征的组合依次类推。这个方法也可以直接用于连续特征与类别特征之间的组合，只要把连续特征看成是1维的类别特征就好了，这时候组合后特征对应的值就不是0/1了，而是连续特征的取值。 3.2 离散化续篇在上节中我已经介绍了一些常用的离散化单个连续特征的方法，其中一个是画图观察趋势。画图观察趋势的好处是直观、可解释性强，坏处是很麻烦。当要离散化的特征很多时，这种方法可操作性较差。 机器学习中有个很好解释，速度也不错的模型——决策树模型。大白话说决策树模型就是一大堆的if else。它天生就可以对连续特征分段，所以把它用于离散化连续特征合情合理。我称这种方法为决策树离散化方法。例如Gmail在对信件做重要性排序时就使用了决策树离散化方法 决策树离散化方法通常也是每次离散化一个连续特征，做法如下： 单独用此特征和目标值yy训练一个决策树模型，然后把训练获得的模型内的特征分割点作为离散化的离散点。 这种方法当然也可以同时离散化多个连续特征，但是操作起来就更复杂了，实际用的不多。 3.3 核方法核方法经常作为线性模型的一种推广出现。以线性回归模型为例，它对应的核方法如下： f_\theta (x)=\sum_{i=1}^n\theta _iK(x,x_i)其中$\{x_i\}^n_{i=1}$为训练样本点，$K(x_i，x_j)$为核函数，比如常用的高斯核函数为： K(x_i，x_j)=exp(-\frac{||x_i-x_j||_2^2}{2h^2})如果我们把上面模型里的${K(x,x_i)}^n_{i=1}$看成特征，而$θ$看成模型参数的话，上面的模型仍旧是个线性模型。所以可以认为核方法只是特征函数变换的一种方式。 当然，如果把核函数$K(x_i,x_j)$看成一种相似度的话，那上面的模型就是kNN模型了，或者叫做加权平均模型也可以。 因为核方法在预测时也要用到训练样本点，耗内存且计算量大，所以在数据量较大的实际问题中用的并不多。 到此，我已经介绍了不少针对单个特征的处理方法。这些处理方法很难说哪个好哪个不好。有些问题这个好，有些问题那个好，也没什么绝招能直接判断出哪种方法能适合哪些问题。唯一的招就是：Experiment a lot! 参考 Trevor Hastie et al. The Elements of Statistical Learning, 2001. ↩ Douglas Aberdeen et al. The Learning Behind Gmail Priority Inbox, 2010. ↩]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（12）：单例模式]]></title>
    <url>%2F2017%2F08%2F30%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%8812%EF%BC%89%EF%BC%9A%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[一、设计模式设计模式（Design pattern）是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结。使用设计模式是为了可重用代码、让代码更容易被他人理解、保证代码可靠性。 面向对象最基本的设计原则有5条，分别是：单一职责原则、开放封闭原则、依赖倒置原则、接口隔离原则和Liskov替换原则。 设计模式就是实现了这些原则，从而达到了代码复用、增加可维护性的目的。 设计模式分为三种类型，共23类。 创建型模式： 是处理对象创建的设计模式，试图根据实际情况使用合适的方式创建对象。基本的对象创建方式可能会导致设计上的问题，或增加设计的复杂度。创建型模式通过以某种方式控制对象的创建来解决问题。 创建型模式由两个主导思想构成。一是将系统使用的具体类封装起来，二是隐藏这些具体类的实例创建和结合的方式。 创建型模式包括：单例模式、抽象工厂模式、建造者模式、工厂模式、原型模式。 结构型模式： 适配器模式、桥接模式、装饰模式、组合模式、外观模式、享元模式、代理模式。 行为型模式： 模板方法模式、命令模式、迭代器模式、观察者模式、中介者模式、备忘录模式、解释器模式、状态模式、策略模式、职责链模式、访问者模式。 二、单例模式单例模式是较为常用的模式之一，且经常作为考题进行考察。单例模式的意图：保证一个类仅有一个实例，并提供一个访问它的全局访问点。 单例模式的结构图： 使用单例的优点： 单例类只有一个实例 共享资源，全局使用 节省创建时间，提高性能 单例模式有多种写法各有利弊，现在我们来看看各种模式写法。 2.1 饿汉式12345678public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123; &#125; public static Singleton getInstance() &#123; return instance; &#125; &#125; 这种方式和名字很贴切，饥不择食，在类装载的时候就创建，不管你用不用，先创建了再说，如果一直没有被使用，便浪费了空间，典型的空间换时间，每次调用的时候，就不需要再判断，节省了运行时间。 Java Runtime就是使用这种方式，它的源代码如下： 1234567891011121314151617public class Runtime &#123; private static Runtime currentRuntime = new Runtime(); /** * Returns the runtime object associated with the current Java application. * Most of the methods of class &lt;code&gt;Runtime&lt;/code&gt; are instance * methods and must be invoked with respect to the current runtime object. * * @return the &lt;code&gt;Runtime&lt;/code&gt; object associated with the current * Java application. */ public static Runtime getRuntime() &#123; return currentRuntime; &#125; /** Don't let anyone else instantiate this class */ private Runtime() &#123;&#125; //以下代码省略&#125; 总结：「饿汉式」是最简单的实现方式，这种实现方式适合那些在初始化时就要用到单例的情况，这种方式简单粗暴，如果单例对象初始化非常快，而且占用内存非常小的时候这种方式是比较合适的，可以直接在应用启动时加载并初始化。 但是，如果单例初始化的操作耗时比较长而应用对于启动速度又有要求，或者单例的占用内存比较大，再或者单例只是在某个特定场景的情况下才会被使用，而一般情况下是不会使用时，使用「饿汉式」的单例模式就是不合适的，这时候就需要用到「懒汉式」的方式去按需延迟加载单例。 2.2 懒汉式（非线程安全）1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123; &#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 懒汉模式申明了一个静态对象，在用户第一次调用时初始化，虽然节约了资源，但第一次加载时需要实例化，反映稍慢一些，而且在多线程不能正常工作。在多线程访问的时候，很可能会造成多次实例化，就不再是单例了。 「懒汉式」与「饿汉式」的最大区别就是将单例的初始化操作，延迟到需要的时候才进行，这样做在某些场合中有很大用处。比如某个单例用的次数不是很多，但是这个单例提供的功能又非常复杂，而且加载和初始化要消耗大量的资源，这个时候使用「懒汉式」就是非常不错的选择。 2.3 懒汉式（线程安全）1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123; &#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 这两种「懒汉式」单例，名字起的也很贴切，一直等到对象实例化的时候才会创建，确实够懒，不用鞭子抽就不知道走了，典型的时间换空间，每次获取实例的时候才会判断，看是否需要创建，浪费判断时间，如果一直没有被使用，就不会被创建，节省空间。 因为这种方式在getInstance()方法上加了同步锁，所以在多线程情况下会造成线程阻塞，把大量的线程锁在外面，只有一个线程执行完毕才会执行下一个线程。 Android中的 InputMethodManager 使用了这种方式，我们看看它的源码： 12345678910111213141516171819public final class InputMethodManager &#123; static InputMethodManager sInstance; /** * Retrieve the global InputMethodManager instance, creating it if it * doesn&apos;t already exist. * @hide */ public static InputMethodManager getInstance() &#123; synchronized (InputMethodManager.class) &#123; if (sInstance == null) &#123; IBinder b = ServiceManager.getService(Context.INPUT_METHOD_SERVICE); IInputMethodManager service = IInputMethodManager.Stub.asInterface(b); sInstance = new InputMethodManager(service, Looper.getMainLooper()); &#125; return sInstance; &#125; &#125;&#125; 2.4 双重校验锁（DCL）上面的方法「懒汉式（线程安全）」毫无疑问存在性能的问题 — 如果存在很多次getInstance()的调用，那性能问题就不得不考虑了！ 让我们来分析一下，究竟是整个方法都必须加锁，还是仅仅其中某一句加锁就足够了？我们为什么要加锁呢？分析一下出现lazy loaded的那种情形的原因。原因就是检测null的操作和创建对象的操作分离了。如果这两个操作能够原子地进行，那么单例就已经保证了。于是，我们开始修改代码，就成了下面的双重校验锁（Double Check Lock）： 1234567891011121314151617181920public class Singleton &#123; /** * 注意此处使用的关键字 volatile， * 被volatile修饰的变量的值，将不会被本地线程缓存， * 所有对该变量的读写都是直接操作共享内存，从而确保多个线程能正确的处理该变量。 */ private volatile static Singleton singleton; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if (instance == null) &#123; synchronized(Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125; 这种写法在getSingleton()方法中对singleton进行了两次判空，第一次是为了不必要的同步，第二次是在singleton等于null的情况下才创建实例。在这里用到了volatile关键字，不了解volatile关键字的可以查看 Java多线程（三）volatile域 和 java中volatile关键字的含义 两篇文章，可以看到双重检查模式是正确使用volatile关键字的场景之一。 「双重校验锁」：既可以达到线程安全，也可以使性能不受很大的影响，换句话说在保证线程安全的前提下，既节省空间也节省了时间，集合了「饿汉式」和两种「懒汉式」的优点，取其精华，去其槽粕。 对于volatile关键字，还是存在很多争议的。由于volatile关键字可能会屏蔽掉虚拟机中一些必要的代码优化，所以运行效率并不是很高。也就是说，虽然可以使用“双重检查加锁”机制来实现线程安全的单例，但并不建议大量采用，可以根据情况来选用。 还有就是在java1.4及以前版本中，很多JVM对于volatile关键字的实现的问题，会导致“双重检查加锁”的失败，因此“双重检查加锁”机制只只能用在java1.5及以上的版本。 2.5 静态内部类另外，在很多情况下JVM已经为我们提供了同步控制，比如： 在static {…}区块中初始化的数据 访问final字段时 因为在JVM进行类加载的时候他会保证数据是同步的，我们可以这样实现：采用内部类，在这个内部类里面去创建对象实例。这样的话，只要应用中不使用内部类 JVM 就不会去加载这个单例类，也就不会创建单例对象，从而实现「懒汉式」的延迟加载和线程安全。 12345678910public class Singleton &#123; private Singleton()&#123; &#125; public static Singleton getInstance()&#123; return SingletonHolder.sInstance; &#125; private static class SingletonHolder &#123; private static final Singleton sInstance = new Singleton(); &#125; &#125; 第一次加载Singleton类时并不会初始化sInstance，只有第一次调用getInstance方法时虚拟机加载SingletonHolder 并初始化sInstance ，这样不仅能确保线程安全也能保证Singleton类的唯一性，所以推荐使用静态内部类单例模式。 然而这还不是最简单的方式，《Effective Java》中作者推荐了一种更简洁方便的使用方式，就是使用「枚举」。 2.6 枚举《Java与模式》中，作者这样写道，使用枚举来实现单实例控制会更加简洁，而且无偿地提供了序列化机制，并由JVM从根本上提供保障，绝对防止多次实例化，是更简洁、高效、安全的实现单例的方式。 12345678public enum Singleton &#123; //定义一个枚举的元素，它就是 Singleton 的一个实例 INSTANCE; public void doSomeThing() &#123; // do something... &#125; &#125; 使用方法如下： 1234public static void main(String args[]) &#123; Singleton singleton = Singleton.instance; singleton.doSomeThing();&#125; 枚举单例的优点就是简单，但是大部分应用开发很少用枚举，可读性并不是很高，不建议用。 2.7 使用容器12345678910111213public class SingletonManager &#123; private static Map&lt;String, Object&gt; objMap = new HashMap&lt;String,Object&gt;(); private Singleton() &#123; &#125; public static void registerService(String key, Objectinstance) &#123; if (!objMap.containsKey(key) ) &#123; objMap.put(key, instance) ; &#125; &#125; public static ObjectgetService(String key) &#123; return objMap.get(key) ; &#125;&#125; 这种是用SingletonManager 将多种单例类统一管理，在使用时根据key获取对象对应类型的对象。这种方式使得我们可以管理多种类型的单例，并且在使用时可以通过统一的接口进行获取操作，降低了用户的使用成本，也对用户隐藏了具体实现，降低了耦合度。 总结 对于以上七种单例，分别是「饿汉式」、「懒汉式(非线程安全)」、「懒汉式(线程安全)」、「双重校验锁」、「静态内部类」、「枚举」和「容器类管理」。很多时候取决人个人的喜好，虽然双重检查有一定的弊端和问题，但我就是钟爱双重检查，觉得这种方式可读性高、安全、优雅（个人观点）。所以代码里常常默写这样的单例，写的时候感觉自己是个伟大的建筑师。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>单例模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（11）：进程与线程]]></title>
    <url>%2F2017%2F08%2F30%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[操作系统可以同时运行多个任务。打个比方，你一边在用浏览器上网，一边在听MP3，一边在用Word赶作业，这就是多任务，至少同时有3个任务正在运行。还有很多任务悄悄地在后台同时运行着，只是桌面上没有显示而已。 操作系统轮流让各个任务交替执行，任务1执行0.01秒，切换到任务2，任务2执行0.01秒，再切换到任务3，执行0.01秒……这样反复执行下去。表面上看，每个任务都是交替执行的，但是，由于CPU的执行速度实在是太快了，我们感觉就像所有任务都在同时执行一样。真正的并行执行多任务只能在多核CPU上实现，但是，由于任务数量远远多于CPU的核心数量，所以，操作系统也会自动把很多任务轮流调度到每个核心上执行。 对于操作系统来说，一个任务就是一个进程（Process），比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。 有些进程还不止同时干一件事，比如Word，它可以同时进行打字、拼写检查、打印等事情。在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的这些“子任务”称为线程（Thread）。 由于每个进程至少要干一件事，所以，一个进程至少有一个线程。当然，像Word这种复杂的进程可以有多个线程，多个线程可以同时执行，多线程的执行方式和多进程是一样的，也是由操作系统在多个线程之间快速切换，让每个线程都短暂地交替运行，看起来就像同时执行一样。当然，真正地同时执行多线程需要多核CPU才可能实现。 一、进程1.1 定义狭义定义：进程是计算机中正在运行的程序的实例（an instance of a computer program that is being executed）。 程序本身只是指令的集合，进程才是程序（那些指令）的真正运行。用户下达运行程序的命令后，就会产生进程。同一程序可产生多个进程（一对多关系），以允许同时有多位用户运行同一程序，却不会相互冲突。进程需要一些资源才能完成工作，比如CPU使用时间、存储器、文件以及I/O设备，且为依序逐一进行，也就是任何时间内仅能运行一项进程。 1.2 基本状态通常进程有如下5种状态，其中前三种是进程的基本状态。 1）运行状态（执行窗台）：进程正在处理器上运行。在单处理器环境下，每一时刻最多只有一个进程处于运行状态。 2）就绪状态：进程已处于准备运行的状态，即进程获得了除处理器之外的一切所需资源，一旦得到处理器即可运行。 3）阻塞状态：又称为等待状态，进程正在等待某一事件而暂停运行，如等待某资源为可用（不包括处理器）或等待输入/输出完成。即使处理器空闲，该进程也不能运行。 4）创建状态：进程正在被创建，尚未转到就绪状态。 5）结束状态：进程正从系统中消失。可能是进程正常结束或其他原因中断退出运行。 进程的三个基本状态之间只可以相互转换的，如图所示。具体的说： 当一个就绪状态获得处理机时，其状态由就绪变为执行； 当一个运行进程被剥夺处理机时，如用完系统分给它的时间片、出现更高优先级别的其他进程，其状态由运行变为就绪； 当一个运行进程因某事件受阻时，如所申请资源被占用、启动I/O传输未完成，其状态由执行变为阻塞； 当所等待事件发生时，如得到申请资源、I/O传输完成，其状态由阻塞变为就绪 1.3 进程与程序的区别 进程是程序及其数据在计算机上的一次运行活动，是一个动态的概念。进程的运行实体是程序、离开程序的进程没有存在的意义。从静态角度看，进程是由程序、数据和进程控制块（PCB）三部分组成的。而程序时一组有序的指令集合，是一种静态的概念。 进程是程序的一次执行过程，它是动态地创建和消亡的，具有一定的生命期，是暂时存在的；而程序则是一组代码的集合，它是永久存在的，可长期保存。 一个进程可以执行一个或几个程序，一个程序也可以构成多个进程。进程可创建进程，而程序不可能形成新的程序。 二、线程2.1 定义线程，有时被称为轻量级进程（Lightweight Process，LWP），是程序执行流的最小单元。一个标准的线程由线程ID、当前指令指针（PC）、寄存器集合和堆栈（stack）组成。另外，线程是进程中的一个实体，是被系统独立调度和分派的基本单位，线程自己不拥有系统资源，只拥有一点在运行中必不可少的资源，但它可与同属一个进程的其他线程共享进程所拥有的资源。 线程共享的进程环境包括：进程代码段、进程的共有数据（如全局变量，利用这些共享的数据，线程很容易的实现相互之间的通信）、进程打开的文件描述符、信号的处理器、进程的当前目录和进程用户ID与进程组ID。 线程拥有这许多共性的同时，还拥有自己的个性。有了这些个性，线程才能够实现并发性。这些个性包括： 线程ID：每个线程都有自己的线程ID，这个ID在本进程中是唯一的。进程用此来标识线程； 寄存器的值：由于线程间是并发运行的，每个线程有自己不同的运行线索，当从一个线程切换到另一个线程上时，必须将原有线程的寄存器集合的状态进行保存，以便将来该线程在被重新切换时能得以恢复。 线程的堆栈（Stack）：堆栈是保证线程独立运行所必须的。线程函数可以调用函数，而被调用函数中又是可以层层嵌套的，所以线程必须拥有自己的函数堆栈，使得函数调用可以正常执行，不受其他线程的影响。在一个进程的线程共享堆区（heap）。 错误返回码 线程的信号屏蔽码 线程的优先级 一个线程可以创建和撤销另一个线程，同一进程的多个线程之间可以并发执行。由于县城之间的相互制约，致使线程在运行中呈现间断性。 线程也有就绪、阻塞和运行三种基本状态。每一个程序都至少有一个线程，若程序只有一个线程，那就是程序本身。 线程是程序中一个单一的顺序控制流程。在单个程序中同时运行多个线程完成不同的工作，称为多线程。 引入线程后，进程的内涵发生了变化，进程只作为除CPU以外系统资源的分配单元，线程则作为处理器的分配单元。在同一进程中，线程的切换不会引起进程的切换，但从一个进程中的线程切换到另一个进程中的线程时，将会引起进程的切换。 1.2 进程与线程的区别 调度：在传统操作系统中，拥有资源和独立调度的基本单位都是进程。引入线程后，线程是独立调度的基本单位，进程是拥有资源的基本单位。在同一进程中，线程的切换不会引起进程切换。在不同进程中进行的线程切换，则会引起进程切换。 拥有资源：不论是传统的还是引入线程的操作系统，进程都是拥有资源的基本单位，线程不拥有资源（也有一点必不可少的资源），但线程可以共享其隶属进程的系统资源。 并发性：在引入线程的操作系统中，不仅进程可以并发执行，而且同一进程内的多个线程也可以并发执行，从而使操作系统具有具有更好的并发性，大大提高了系统的吞吐量。 系统开销：创建和撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O设备等等，因此操作系统所付出的开销远大于创建或撤销线程的开销。类似地，在进程切换时，涉及当前执行进程CPU环境的保存以及新调度的进程CPU环境的设置；而线程切换时只需保存和设置少量寄存器内容，因此开销很小。另外，由于同一进程内的多个线程共享进程的地址空间，因此这些线程之间的同步与通信比较容易实现，甚至无需操作系统的干预。 地址空间和其他资源（如打开的文件）：进程的地址空间之间相互独立，同一进程的各线程间共享进程的资源，某进程内的线程对于其他进程不可见。 通信方面：进程间通信需要借助操作系统，而线程间可以直接读、写进程数据段（如全局变量）来进行通信。 三、进程通信与进程同步多个进程可以共享系统中的各种资源，但其中许多资源一次为能为一个进程使用，我们把一次仅允许一个进程使用的资源成为临界资源。许多物理设备都属于临界资源，如打印机等。 对临界资源的访问，必须互斥的进行，在每个进程中，访问临界资源的那段代码成为临界区（Critical Section）。 进程通信与同步有如下一些目的。 数据传输 共享数据 通知数据 资源共享 进程控制 Linux进程间通信的几种主要手段简介： 管道（Pipe）及有名管道（named Pipe） 信号（Signal） Message（消息队列） 共享内存（Shared Memory） 信号量 套接口 Linux线程间通信：互斥体（互斥量）、信号量、条件变量Windows进程间通信：管道、共享内存、消息队列、信号量、socketwindows线程间通信：临界区（Critical Section）、互斥量（Mutex）、信号量（信号灯）（Semaphore）、事件（Event）。 四、调度算法调度的基本准则包括CPU利用率、系统吞吐量、周转时间、等待时间、响应时间等。 系统吞吐量：表示单位时间内CPU完成作业的数量 周转时间：作业完成时刻减去作业到达的时刻 等待时间：进程处于等处理器状态的时间之和，等待时间越长，用户满意度越低。 响应时间：从用户提交请求到系统首次产生响应所用的时间。 典型的调度算法包括： 实时系统中：FIFO(First Input First Output，先进先出算法)，SJF(Shortest Job First，最短作业优先算法)，SRTF(Shortest Remaining Time First，最短剩余时间优先算法）。 交互式系统中：RR(Round Robin，时间片轮转算法)，HPF(Highest Priority First，最高优先级算法)，多级队列，最短进程优先，保证调度，彩票调度，公平分享调度。 多级反馈队列调度算法。其中SJF的平均等待时间、平均周转时间最少。 五、死锁所谓死锁是指多个进程因竞争资源而造成的一种僵局（互相等待），若无外力作用，这些进程都将无法向前推进。现实生活中简单的例子：交通阻塞，两股相向而行的车流都想通过已被对方占用的道路，结果双方都不能前进。 死锁产生的原因：系统资源的竞争、进程推进顺序非法 死锁产生的必要条件：产生死锁必须同时满足以下四个条件，只要其中任一条件不满足，死锁就不会发生。 互斥条件：进程要求对所分配的资源进行排他性控制，即在一段时间内某资源仅为一个进程所占有。此时若有其他进程请求该资源，则请求进程只能等待； 不剥夺条件：进程所获得的资源在未使用完毕之前，不能被其他资源强行夺走，即只能由获得该资源的进程自己来释放。 请求和保持条件：又称为部分分配条件。进程每次申请它所需要的一部分资源，在等待新资源的同时，进程继续占有已分配到的资源。 循环等待条件：存在一种进程资源的循环等待链，链中每个进程已获得的资源同时被链中下一个进程所请求。即存在一个处于等待状态的进程集合$\{P_1,P_2,….P_n\}$其中$P_i$等待的资源被$P_{i+1}(i=0,1,2,…n-1)$占有，$P_n$等待资源被$P_0$占有。 死锁处理策略 预防死锁：设置某些限制条件，破坏产生死锁的四个必要条件中的一个或几个 避免死锁：在资源的动态分配过程中，用某种方法防止系统进入不安全状态。银行家算法是著名的死锁避免算法。 死锁的检测及解除：无需采取任何限制性措施，允许进程在运行过程中发生死锁，通过系统的检测机制及时地检测出死锁的发生，然后采取某种措施解除死锁。死锁可利用资源分配图来描述。死锁的解除主要方法：资源剥夺法、撤销进程法、进程回退法]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>操作系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（10）：QA]]></title>
    <url>%2F2017%2F08%2F30%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%8810%EF%BC%89%EF%BC%9AQA%2F</url>
    <content type="text"><![CDATA[一、什么是Java虚拟机？为什么Java被称作是“平台无关的编程语言”？Java虚拟机是一个可以执行Java字节码的虚拟机进程。Java源文件被编译成能被Java虚拟机执行的字节码文件。 二、JDK和JRE的区别是什么？Java运行时环境(JRE)是将要执行Java程序的Java虚拟机。它同时也包含了执行applet需要的浏览器插件。Java开发工具包(JDK)是完整的Java软件开发包，包含了JRE，编译器和其他的工具(比如：JavaDoc，Java调试器)，可以让开发者开发、编译、执行Java应用程序。 三、static关键字是什么意思？Java中是否可以覆盖(override)一个private或者是static的方法？第一小问：static 修饰符能够与变量、方法一起使用，表示是“静态”的。静态变量和静态方法能够通过类名来访问，不需要创建一个类的对象来访问该类的静态成员，所以static修饰的成员又称作类变量和类方法。静态变量与实例变量不同，实例变量总是通过对象来访问，因为它们的值在对象和对象之间有所不同。 静态变量属于类，不属于任何独立的对象，所以无需创建类的实例就可以访问静态变量。之所以会产生这样的结果，是因为编译器只为整个类创建了一个静态变量的副本，也就是只分配一个内存空间，虽然有多个实例，但这些实例共享该内存。实例变量则不同，每创建一个对象，都会分配一次内存空间，不同变量的内存相互独立，互不影响，改变 a 对象的实例变量不会影响 b 对象。 static 的变量是在类装载的时候就会被初始化。也就是说，只要类被装载，不管你是否使用了这个static 变量，它都会被初始化。 小结：类变量(class variables)用关键字 static 修饰，在类加载的时候，分配类变量的内存，以后再生成类的实例对象时，将共享这块内存（类变量），任何一个对象对类变量的修改，都会影响其它对象。外部有两种访问方式：通过对象来访问或通过类名来访问。 123456789101112131415public class Demo &#123; static int i; int j; public static void main(String[] args) &#123; Demo obj1 = new Demo(); obj1.i = 10; obj1.j = 20; Demo obj2 = new Demo(); System.out.println("obj1.i=" + obj1.i + ", obj1.j=" + obj1.j); System.out.println("obj2.i=" + obj2.i + ", obj2.j=" + obj2.j); &#125;&#125; 运行结果： 12obj1.i=10, obj1.j=20obj2.i=10, obj2.j=0 第二小问：被覆盖的方法不能为static。如果父类中的方法为静态的，而子类中的方法不是静态的，但是两个方法除了这一点外其他都满足覆盖条件，那么会发生编译错误；反之亦然。即使父类和子类中的方法都是静态的，并且满足覆盖条件，但是仍然不会发生覆盖，因为方法覆盖是基于运行时动态绑定的，而static方法是编译时静态绑定的。static方法跟类的任何实例都不相关，所以概念上不适用。 四、是否可以在static环境中访问非static变量？非静态的既可以访问静态的，也可以访问非静态的，而静态的只能访问静态的。 不可以在静态环境中访问非静态。因为静态的成员属于类，随着类的加载而加载到静态方法区内存，当类加载时，此时不一定有实例创建，没有实例，就不可以访问非静态的成员。类的加载先于实例的创建，因此静态环境中，不可以访问非静态！ static变量在Java中是属于类的，它在所有的实例中的值是一样的。当类被Java虚拟机载入的时候，会对static变量进行初始化。如果你的代码尝试不用实例来访问非static的变量，编译器会报错，因为这些变量还没有被创建出来，还没有跟任何实例关联上。 五、Java支持的数据类型有哪些？什么是自动拆装箱？ 基本数据类型 对应的包装类 byte Byte short Short int Integer long Long char Character float Float double Double boolean Boolean 虽然 Java 语言是典型的面向对象编程语言，但其中的八种基本数据类型并不支持面向对象编程，基本类型的数据不具备“对象”的特性——不携带属性、没有方法可调用。 沿用它们只是为了迎合人类根深蒂固的习惯，并的确能简单、有效地进行常规数据处理。这种借助于非面向对象技术的做法有时也会带来不便，比如引用类型数据均继承了 Object 类的特性，要转换为 String 类型（经常有这种需要）时只要简单调用 Object 类中定义的toString()即可，而基本数据类型转换为 String 类型则要麻烦得多。为解决此类问题 ，Java为每种基本数据类型分别设计了对应的类，称之为包装类(Wrapper Classes)，也有教材称为外覆类或数据类型类。 每个包装类的对象可以封装一个相应的基本类型的数据，并提供了其它一些有用的方法。包装类对象一经创建，其内容（所封装的基本类型数据值）不可改变。 基本类型和对应的包装类可以相互装换： 由基本类型向对应的包装类转换称为装箱，例如把 int 包装成 Integer 类的对象； 包装类向对应的基本类型转换称为拆箱，例如把 Integer 类的对象重新简化为 int。 八个包装类的使用比较相似，下面是常见的应用场景。 1）实现 int 和 Integer 的相互转换 可以通过 Integer 类的构造方法将 int 装箱，通过 Integer 类的 intValue 方法将 Integer 拆箱。例如： 1234567891011public class Demo &#123; public static void main(String[] args) &#123; int m = 500; Integer obj = new Integer(m); // 手动装箱 int n = obj.intValue(); // 手动拆箱 System.out.println(&quot;n = &quot; + n); Integer obj1 = new Integer(500); System.out.println(&quot;obj 等价于 obj1？&quot; + obj.equals(obj1)); &#125;&#125; 运行结果： 12n = 500obj 等价于 obj1？true 2）将字符串转换为整数 Integer 类有一个静态的 paseInt() 方法，可以将字符串转换为整数，语法为： 1parseInt(String s, int radix); s 为要转换的字符串，radix 为进制，可选，默认为十进制。 下面的代码将会告诉你什么样的字符串可以转换为整数： 1234567891011121314public class Demo &#123; public static void main(String[] args) &#123; String str[] = &#123;&quot;123&quot;, &quot;123abc&quot;, &quot;abc123&quot;, &quot;abcxyz&quot;&#125;; for(String str1 : str)&#123; try&#123; int m = Integer.parseInt(str1, 10); System.out.println(str1 + &quot; 可以转换为整数 &quot; + m); &#125;catch(Exception e)&#123; System.out.println(str1 + &quot; 无法转换为整数&quot;); &#125; &#125; &#125;&#125; 1234123 可以转换为整数 123123abc 无法转换为整数abc123 无法转换为整数abcxyz 无法转换为整数 3）将整数转换为字符串 Integer 类有一个静态的 toString() 方法，可以将整数转换为字符串。例如： 1234567public class Demo &#123; public static void main(String[] args) &#123; int m = 500; String s = Integer.toString(m); System.out.println(&quot;s = &quot; + s); &#125;&#125; 运行结果： 1s = 500 上面的例子都需要手动实例化一个包装类，称为手动拆箱装箱。Java 1.5(5.0) 之前必须手动拆箱装箱。 Java 1.5 之后可以自动拆箱装箱，也就是在进行基本数据类型和对应的包装类转换时，系统将自动进行，这将大大方便程序员的代码书写。例如： 1234567891011public class Demo &#123; public static void main(String[] args) &#123; int m = 500; Integer obj = m; // 自动装箱 int n = obj; // 自动拆箱 System.out.println("n = " + n); Integer obj1 = 500; System.out.println("obj 等价于 obj1？" + obj.equals(obj1)); &#125;&#125; 运行结果： 12n = 500obj 等价于 obj1？true 自动拆箱装箱是常用的一个功能，需要重点掌握。 六、Java中的方法覆盖(Overriding)和方法重载(Overloading)是什么意思？1、方法覆盖（Overriding）： 在类继承中，子类可以修改从父类继承来的方法，也就是说子类能创建一个与父类方法有不同功能的方法，但具有相同的名称、返回值类型、参数列表。如果在新类中定义一个方法，其名称、返回值类型和参数列表正好与父类中的相同，那么，新方法被称做覆盖旧方法。参数列表又叫参数签名，包括参数的类型、参数的个数和参数的顺序，只要有一个不同就叫做参数列表不同。被覆盖的方法在子类中只能通过super调用。 注意：覆盖不会删除父类中的方法，而是对子类的实例隐藏，暂时不使用。 123456789101112131415161718192021222324252627282930public class Demo&#123; public static void main(String[] args) &#123; Dog myDog = new Dog("花花"); myDog.say(); // 子类的实例调用子类中的方法 Animal myAnmial = new Animal("贝贝"); myAnmial.say(); // 父类的实例调用父类中的方法 &#125;&#125;class Animal&#123; String name; public Animal(String name)&#123; this.name = name; &#125; public void say()&#123; System.out.println("我是一只小动物，我的名字叫" + name + "，我会发出叫声"); &#125;&#125;class Dog extends Animal&#123; // 构造方法不能被继承，通过super()调用 public Dog(String name)&#123; super(name); &#125; // 覆盖say() 方法 public void say()&#123; System.out.println("我是一只小狗，我的名字叫" + name + "，我会发出汪汪的叫声"); &#125;&#125; 运行结果： 12我是一只小狗，我的名字叫花花，我会发出汪汪的叫声我是一只小动物，我的名字叫贝贝，我会发出叫声 方法覆盖的原则： 覆盖方法的返回类型、方法名称、参数列表必须与原方法的相同。 覆盖方法不能比原方法访问性差（即访问权限不允许缩小）。 覆盖方法不能比原方法抛出更多的异常。 被覆盖的方法不能是final类型，因为final修饰的方法是无法覆盖的。 被覆盖的方法不能为private，否则在其子类中只是新定义了一个方法，并没有对其进行覆盖。 被覆盖的方法不能为static。如果父类中的方法为静态的，而子类中的方法不是静态的，但是两个方法除了这一点外其他都满足覆盖条件，那么会发生编译错误；反之亦然。即使父类和子类中的方法都是静态的，并且满足覆盖条件，但是仍然不会发生覆盖，因为静态方法是在编译的时候把静态方法和类的引用类型进行匹配。 2、方法重载(Overloading)在Java中，同一个类中的多个方法可以有相同的名字，只要它们的参数列表不同就可以，这被称为方法重载(method overloading)。参数列表又叫参数签名，包括参数的类型、参数的个数和参数的顺序，只要有一个不同就叫做参数列表不同。 重载是面向对象的一个基本特性。 下面看一个详细的实例。 123456789101112131415161718192021222324252627public class Demo&#123; // 一个普通的方法，不带参数 void test()&#123; System.out.println(&quot;No parameters&quot;); &#125; // 重载上面的方法，并且带了一个整型参数 void test(int a)&#123; System.out.println(&quot;a: &quot; + a); &#125; // 重载上面的方法，并且带了两个参数 void test(int a,int b)&#123; System.out.println(&quot;a and b: &quot; + a + &quot; &quot; + b); &#125; // 重载上面的方法，并且带了一个双精度参数 double test(double a)&#123; System.out.println(&quot;double a: &quot; + a); return a*a; &#125; public static void main(String args[])&#123; Demo obj= new Demo(); obj.test(); obj.test(2); obj.test(2,3); obj.test(2.0); &#125;&#125; 运行结果： 1234No parametersa: 2a and b: 2 3double a: 2.0 通过上面的实例，读者可以看出，重载就是在一个类中，有相同的函数名称，但形参不同的函数。重载的结果，可以让一个程序段尽量减少代码和方法的种类。说明： 参数列表不同包括：个数不同、类型不同和顺序不同。 仅仅参数变量名称不同是不可以的。 跟成员方法一样，构造方法也可以重载。 声明为final的方法不能被重载。 声明为static的方法不能被重载，但是能够被再次声明。 方法的重载的规则： 方法名称必须相同。 参数列表必须不同（个数不同、或类型不同、参数排列顺序不同等）。 方法的返回类型可以相同也可以不相同。 仅仅返回类型不同不足以成为方法的重载。 方法重载的实现： 方法名称相同时，编译器会根据调用方法的参数个数、参数类型等去逐个匹配，以选择对应的方法，如果匹配失败，则编译器报错，这叫做重载分辨。 3、覆盖和重载的不同： 方法覆盖要求参数列表必须一致，而方法重载要求参数列表必须不一致。 方法覆盖要求返回类型必须一致，方法重载对此没有要求。 方法覆盖只能用于子类覆盖父类的方法，方法重载用于同一个类中的所有方法（包括从父类中继承而来的方法）。 方法覆盖对方法的访问权限和抛出的异常有特殊的要求，而方法重载在这方面没有任何限制。 父类的一个方法只能被子类覆盖一次，而一个方法可以在所有的类中可以被重载多次 七、Java中，什么是构造方法？什么是构造方法重载？什么是复制构造方法？1、Java中，什么是构造方法？ 在类实例化的过程中自动执行的方法叫做构造方法，它不需要你手动调用。构造方法可以在类实例化的过程中做一些初始化的工作。 构造方法的名称必须与类的名称相同，并且没有返回值。 每个类都有构造方法。如果没有显式地为类定义构造方法，Java编译器将会为该类提供一个默认的构造方法。 下面是一个构造方法示例： 12345678910111213141516171819202122232425public class Dog&#123; String name; int age; // 构造方法，没有返回值 Dog(String name1, int age1)&#123; name = name1; age = age1; System.out.println("感谢主人领养了我"); &#125; // 普通方法，必须有返回值 void bark()&#123; System.out.println("汪汪，不要过来"); &#125; void hungry()&#123; System.out.println("主人，我饿了"); &#125; public static void main(String arg[])&#123; // 创建对象时传递的参数要与构造方法参数列表对应 Dog myDog = new Dog("花花", 3); &#125;&#125; 运行结果： 1感谢主人领养了我 说明： 构造方法不能被显示调用。 构造方法不能有返回值，因为没有变量来接收返回值。 2、什么是构造方法重载？ 跟成员方法一样，构造方法也可以重载。Java中构造函数重载和方法重载很相似。可以为一个类创建多个构造函数。每一个构造函数必须有它自己唯一的参数列表。 3、什么是复制构造方法？Java不支持像C++中那样的复制构造函数，这个不同点是因为如果你不自己写构造函数的情况下，Java不会创建默认的复制构造函数. 八、Java支持多继承么？单继承性：Java 允许一个类仅能继承一个其它类，即一个类只能有一个父类，这个限制被称做单继承性。 但是java中的接口支持多继承，，即一个子接口可以有多个父接口。（接口的作用是用来扩展对象的功能，一个子接口继承多个父接口，说明子接口扩展了多个功能，当类实现接口时，类就扩展了相应的功能）。 九、接口和抽象类的区别是什么？Java提供和支持创建抽象类和接口。它们的实现有共同点，不同点在于： 接口中所有的方法隐含的都是抽象的。而抽象类则可以同时包含抽象和非抽象的方法。 类可以实现很多个接口，但是只能继承一个抽象类 类可以不实现抽象类和接口声明的所有方法，当然，在这种情况下，类也必须得声明成是抽象的。 抽象类可以在不提供接口方法实现的情况下实现接口。 Java接口中声明的变量默认都是final的。抽象类可以包含非final的变量。 Java接口中的成员函数默认是public的。抽象类的成员函数可以是private，protected或者是public。 接口是绝对抽象的，不可以被实例化。抽象类也不可以被实例化，但是，如果它包含main方法的话是可以被调用的。 也可以参考JDK8中抽象类和接口的区别 十、什么是值传递和引用传递?值传递是对基本型变量而言的,传递的是该变量的一个副本,改变副本不影响原变量. 引用传递一般是对于对象型变量而言的,传递的是该对象地址的一个副本, 并不是原对象本身 。 所以对引用对象进行操作会同时改变原对象. 一般认为,java内的传递都是值传递. 十一、进程与线程的区别概述: 进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动,进程是系统进行资源分配和调度的一个独立单位. 线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源. 一个线程可以创建和撤销另一个线程;同一个进程中的多个线程之间可以并发执行. 相对进程而言，线程是一个更加接近于执行体的概念，它可以与同进程中的其他线程共享数据，但拥有自己的栈空间，拥有独立的执行序列。 在串行程序基础上引入线程和进程是为了提高程序的并发度，从而提高程序运行效率和响应时间。 区别:进程和线程的主要差别在于它们是不同的操作系统资源管理方式。进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。 进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动，是操作系统进行资源分配和调度的一个独立单位；线程是进程的一个实体，是CPU调度和分派的基本单位，是比进程更小的能独立运行的基本单位。线程的划分尺度小于进程，这使得多线程程序的并发性高；进程在执行时通常拥有独立的内存单元，而线程之间可以共享内存。使用多线程的编程通常能够带来更好的性能和用户体验，但是多线程的程序对于其他程序是不友好的，因为它可能占用了更多的CPU资源。当然，也不是线程越多，程序的性能就越好，因为线程之间的调度和切换也会浪费CPU时间。时下很时髦的Node.js就采用了单线程异步I/O的工作模式。 简而言之,一个程序至少有一个进程,一个进程至少有一个线程. 线程的划分尺度小于进程，使得多线程程序的并发性高。 另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。 线程在执行过程中与进程还是有区别的。每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 从逻辑角度来看，多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理以及资源分配。这就是进程和线程的重要区别。 优缺点: 线程和进程在使用上各有优缺点：线程执行开销小，但不利于资源的管理和保护；而进程正相反。同时，线程适合于在SMP机器上运行，而进程则可以跨机器迁移。 十二、为什么集合类没有实现Cloneable和Serializable接口？克隆(cloning)或者是序列化(serialization)的语义和含义是跟具体的实现相关的。因此，应该由集合类的具体实现来决定如何被克隆或者是序列化 实现Serializable序列化的作用：将对象的状态保存在存储媒体中以便可以在以后重写创建出完全相同的副本；按值将对象从一个从一个应用程序域发向另一个应用程序域。 实现Serializable接口的作用就是可以把对象存到字节流，然后可以恢复。所以你想如果你的对象没有序列化，怎么才能进行网络传输呢？要网络传输就得转为字节流，所以在分布式应用中，你就得实现序列化。如果你不需要分布式应用，那就没必要实现实现序列化。 十三、什么是迭代器(Iterator)？Iterator接口提供了很多对集合元素进行迭代的方法。每一个集合类都包含了可以返回迭代器实例的迭代方法。迭代器可以在迭代的过程中删除底层集合的元素,但是不可以直接调用集合的remove(Object Obj)删除，可以通过迭代器的remove()方法删除。 十四、 Iterator和ListIterator的区别是什么？Iterator是遍历集合的迭代器（不能遍历Map，只用来遍历Collection），Collection的实现类都实现了iterator()函数，它返回一个Iterator对象，用来遍历集合，ListIterator则专门用来遍历List。 Iterator对集合只能是前向遍历，ListIterator既可以前向也可以后向。 ListIterator实现了Iterator接口，并包含其他的功能，比如：增加元素，替换元素，获取前一个和后一个元素的索引，等等。 十五、快速失败(fail-fast)和安全失败(fail-safe)的区别是什么？Iterator的安全失败是基于对底层集合做拷贝，因此，它不受源集合上修改的影响。java.util包下面的所有的集合类都是快速失败的，而java.util.concurrent包下面的所有的类都是安全失败的。快速失败的迭代器会抛出ConcurrentModificationException异常，而安全失败的迭代器永远不会抛出这样的异常。 十六、final的作用在 Java 中，声明类、变量和方法时，可使用关键字 final 来修饰。final 所修饰的数据具有“终态”的特征，表示“最终的”意思。具体规定如下： final 修饰的类不能被继承。 final 修饰的方法不能被子类重写。 final 修饰的变量（成员变量或局部变量）即成为常量，只能赋值一次。 final 修饰的成员变量必须在声明的同时赋值，如果在声明的时候没有赋值，那么只有 一次赋值的机会，而且只能在构造方法中显式赋值，然后才能使用。 final 修饰的局部变量可以只声明不赋值，然后再进行一次性的赋值。 final 一般用于修饰那些通用性的功能、实现方式或取值不能随意被改变的数据，以避免被误用，例如实现数学三角方法、幂运算等功能的方法，以及数学常量π=3.141593、e=2.71828 等。 事实上，为确保终态性，提供了上述方法和常量的 java.lang.Math 类也已被定义为final 的。 需要注意的是，如果将引用类型（任何类的类型）的变量标记为 final，那么该变量不能指向任何其它对象。但可以改变对象的内容，因为只有引用本身是 final 的。 如果变量被标记为 final，其结果是使它成为常数。想改变 final 变量的值会导致一个编译错误。下面是一个正确定义 final 变量的例子： 1public final int MAX_ARRAY_SIZE = 25; // 常量名一般大写 常量因为有 final 修饰，所以不能被继承。请看下面的代码： 12345678910111213141516public final class Demo&#123; public static final int TOTAL_NUMBER = 5; public int id; public Demo() &#123; // 非法，对final变量TOTAL_NUMBER进行二次赋值了 // 因为++TOTAL_NUMBER相当于 TOTAL_NUMBER=TOTAL_NUMBER+1 id = ++TOTAL_NUMBER; &#125; public static void main(String[] args) &#123; final Demo t = new Demo(); final int i = 10; final int j; j = 20; j = 30; // 非法，对final变量进行二次赋值 &#125;&#125; final 也可以用来修饰类（放在 class 关键字前面），阻止该类再派生出子类，例如 Java.lang.String 就是一个 final 类。这样做是出于安全原因，因为要保证一旦有字符串的引用，就必须是类 String 的字符串，而不是某个其它类的字符串（String 类可能被恶意继承并篡改）。 方法也可以被 final 修饰，被 final 修饰的方法不能被覆盖；变量也可以被 final 修饰，被 final 修饰的变量在创建对象以后就不允许改变它们的值了。一旦将一个类声明为 final，那么该类包含的方法也将被隐式地声明为 final，但是变量不是。 被 final 修饰的方法为静态绑定，不会产生多态（动态绑定），程序在运行时不需要再检索方法表，能够提高代码的执行效率。在Java中，被 static 或 private 修饰的方法会被隐式的声明为 final，因为动态绑定没有意义。 由于动态绑定会消耗资源并且很多时候没有必要，所以有一些程序员认为：除非有足够的理由使用多态性，否则应该将所有的方法都用 final 修饰。 这样的认识未免有些偏激，因为 JVM 中的即时编译器能够实时监控程序的运行信息，可以准确的知道类之间的继承关系。如果一个方法没有被覆盖并且很短，编译器就能够对它进行优化处理，这个过程为称为内联(inlining)。例如，内联调用 e.getName() 将被替换为访问 e.name 变量。这是一项很有意义的改进，这是由于CPU在处理调用方法的指令时，使用的分支转移会扰乱预取指令的策略，所以，这被视为不受欢迎的。然而，如果 getName() 在另外一个类中被覆盖，那么编译器就无法知道覆盖的代码将会做什么操作，因此也就不能对它进行内联处理了。 十七、String 与StringBuffer的区别，有什么好处？String 的值是不可变的，每次对String的操作都会生成新的String对象，不仅效率低，而且耗费大量内存空间。 StringBuffer类和String类一样，也用来表示字符串，但是StringBuffer的内部实现方式和String不同，在进行字符串处理时，不生成新的对象，在内存使用上要优于String。 StringBuffer 默认分配16字节长度的缓冲区，当字符串超过该大小时，会自动增加缓冲区长度，而不是生成新的对象。 StringBuffer不像String，只能通过 new 来创建对象，不支持简写方式，例如： 1234StringBuffer str1 = new StringBuffer(); // 分配16个字节长度的缓冲区StringBuffer str2 = =new StringBuffer(512); // 分配512个字节长度的缓冲区// 在缓冲区中存放了字符串，并在后面预留了16个字节长度的空缓冲区StringBuffer str3 = new StringBuffer(&quot;www.baidu.com&quot;); 17.1 StringBuffer类的主要方法StringBuffer类中的方法主要偏重于对于字符串的操作，例如追加、插入和删除等，这个也是StringBuffer类和String类的主要区别。实际开发中，如果需要对一个字符串进行频繁的修改，建议使用 StringBuffer。 1) append() 方法 append() 方法用于向当前字符串的末尾追加内容，类似于字符串的连接。调用该方法以后，StringBuffer对象的内容也发生改变，例如： 12StringBuffer str = new StringBuffer(“biancheng100”);str.append(true); 则对象str的值将变成”biancheng100true”。注意是str指向的内容变了，不是str的指向变了。 字符串的”+“操作实际上也是先创建一个StringBuffer对象，然后调用append()方法将字符串片段拼接起来，最后调用toString()方法转换为字符串。 这样看来，String的连接操作就比StringBuffer多出了一些附加操作，效率上必然会打折扣。 但是，对于长度较小的字符串，”+“操作更加直观，更具可读性，有些时候可以稍微牺牲一下效率。 2) deleteCharAt() deleteCharAt() 方法用来删除指定位置的字符，并将剩余的字符形成新的字符串。例如： 12StringBuffer str = new StringBuffer(&quot;abcdef&quot;);str. deleteCharAt(3); 该代码将会删除索引值为3的字符，即”d“字符。 你也可以通过delete()方法一次性删除多个字符，例如： 12StingBuffer str = new StringBuffer(&quot;abcdef&quot;);str.delete(1, 4); 该代码会删除索引值为1~4之间的字符，包括索引值1，但不包括4。 3) insert()方法 insert() 用来在指定位置插入字符串，可以认为是append()的升级版。例如： 12StringBuffer str = new StringBuffer(&quot;abcdef&quot;);str.insert(3, &quot;xyz&quot;); 最后str所指向的字符串为 abcdxyzef。 4) setCharAt() 方法 setCharAt() 方法用来修改指定位置的字符。例如： 12StringBuffer str = new StringBuffer(&quot;abcdef&quot;);str.setCharAt(3, &apos;z&apos;); 该代码将把索引值为3的字符修改为 z，最后str所指向的字符串为 abczef。 以上仅仅是部分常用方法的简单说明，更多方法和解释请查阅API文档。 17.2 String和StringBuffer的效率对比为了更加明显地看出它们的执行效率，下面的代码，将26个英文字母加了10000次。 123456789101112131415161718192021222324public class Demo &#123; public static void main(String[] args)&#123; String fragment = "abcdefghijklmnopqrstuvwxyz"; int times = 10000; // 通过String对象 long timeStart1 = System.currentTimeMillis(); String str1 = ""; for (int i=0; i&lt;times; i++) &#123; str1 += fragment; &#125; long timeEnd1 = System.currentTimeMillis(); System.out.println("String: " + (timeEnd1 - timeStart1) + "ms"); // 通过StringBuffer long timeStart2 = System.currentTimeMillis(); StringBuffer str2 = new StringBuffer(); for (int i=0; i&lt;times; i++) &#123; str2.append(fragment); &#125; long timeEnd2 = System.currentTimeMillis(); System.out.println("StringBuffer: " + (timeEnd2 - timeStart2) + "ms"); &#125;&#125; 运行结果： 12String: 5287msStringBuffer: 3ms 结论很明显，StringBuffer的执行效率比String快上千倍，这个差异随着叠加次数的增加越来越明显，当叠加次数达到30000次的时候，运行结果为： 12String: 35923msStringBuffer: 8ms 所以，强烈建议在涉及大量字符串操作时使用StringBuffer。 17.3 StringBuilder类StringBuilder类和StringBuffer类功能基本相似，方法也差不多，主要区别在于StringBuffer类的方法是多线程安全的，而StringBuilder不是线程安全的，相比而言，StringBuilder类会略微快一点。 StringBuffer、StringBuilder、String中都实现了CharSequence接口。CharSequence是一个定义字符串操作的接口，它只包括length()、charAt(int index)、subSequence(int start, int end) 这几个API。 StringBuffer、StringBuilder、String对CharSequence接口的实现过程不一样，如下图所示： 可见，String直接实现了CharSequence接口；StringBuilder 和 StringBuffer都是可变的字符序列，它们都继承于AbstractStringBuilder，实现了CharSequence接口。 总结一下： 线程安全： StringBuffer：线程安全 StringBuilder：线程不安全 速度： 一般情况下，速度从快到慢为 StringBuilder &gt; StringBuffer &gt; String，当然这是相对的，不是绝对的。 使用环境： 操作少量的数据使用 String； 单线程操作大量数据使用 StringBuilder； 多线程操作大量数据使用 StringBuffer。 十九、异常处理机制，，try和finally里面都有return的时候，会不会执行finally的return。二十、 synchronized和lock的区别 动态绑定与静态绑定： volatile是什么，threadlocal是什么 volatile和synchronized的区别 垃圾回收机制 1.进程与线程的区别，线程的同步问题，两个线程访问一个临界资源该怎么做？线程什么时候终止？（还问到一个daemon函数，我当时完全不知道这是什么） 1.什么是面向对象？JAVA与C相比有什么区别？JAVA的对象与c的结构体有什么区别？ 2.JAVA的IO有哪些类？接口？关系是啥？（谁继承谁之类的） 3.你用过哪些JAVA的库？（java.io, java.util, 等等）你什么时候开始用JAVA的？（这个问题是最开始问的，我说14年开始的。。后面又问了一遍。。） 然后问了高并发问题，讲讲CurrentHashMap原理，可是我并不了解，就大概说了下自己想法]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>QA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（9）：内部类、抽象类、接口]]></title>
    <url>%2F2017%2F08%2F28%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%889%EF%BC%89%EF%BC%9A%E5%86%85%E9%83%A8%E7%B1%BB%E3%80%81%E6%8A%BD%E8%B1%A1%E7%B1%BB%E3%80%81%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[一、Java内部类及其实例化在 Java 中，允许在一个类（或方法、语句块）的内部定义另一个类，称为内部类(Inner Class)，有时也称为嵌套类(Nested Class)。 内部类和外层封装它的类之间存在逻辑上的所属关系，一般只用在定义它的类或语句块之内，实现一些没有通用意义的功能逻辑，在外部引用它时必须给出完整的名称。 使用内部类的主要原因有： 内部类可以访问外部类中的数据，包括私有的数据。 内部类可以对同一个包中的其他类隐藏起来。 当想要定义一个回调函数且不想编写大量代码时，使用匿名(anonymous)内部类比较便捷。 减少类的命名冲突。 请看下面的例子： 123456789101112131415161718public class Demo &#123; private int size; public class Inner &#123; private int counter = 10; public void doStuff() &#123; size++; &#125; &#125; public static void main(String args[]) &#123; Demo outer = new Demo(); Inner inner = outer.new Inner(); inner.doStuff(); System.out.println(outer.size); System.out.println(inner.counter); // 编译错误，外部类不能访问内部类的变量// System.out.println(counter); &#125;&#125; 这段代码定义了一个外部类 Outer，它包含了一个内部类 Inner。将错误语句注释掉，编译，会生成两个 .class 文件：Outer.class 和 Outer`Inner.class。也就是说，内部类会被编译成独立的字节码文件。内部类是一种编译器现象，与虚拟机无关。编译器将会把内部类翻译成用` 符号分隔外部类名与内部类名的常规类文件，而虚拟机则对此一无所知。注意：必须先有外部类的对象才能生成内部类的对象，因为内部类需要访问外部类中的成员变量，成员变量必须实例化才有意义。 内部类是 Java 1.1 的新增特性，有些程序员认为这是一个值得称赞的进步，但是内部类的语法很复杂，严重破坏了良好的代码结构， 违背了Java要比C++更加简单的设计理念。 内部类看似增加了—些优美有趣，实属没必要的特性，这是不是也让Java开始走上了许多语言饱受折磨的毁灭性道路呢？本教程并不打算就这个问题给予一个肯定的答案。 二、内部类的分类内部类可以是静态(static)的，可以使用 public、protected 和 private 访问控制符，而外部类只能使用 public，或者默认。 2.1 成员式内部类在外部类内部直接定义（不在方法内部或代码块内部）的类就是成员式内部类，它可以直接使用外部类的所有变量和方法，即使是 private 的。外部类要想访问内部类的成员变量和方法，则需要通过内部类的对象来获取。 请看下面的代码： 三、抽象类的概念和使用在自上而下的继承层次结构中，位于上层的类更具有通用性，甚至可能更加抽象。从某种角度看，祖先类更加通用，它只包含一些最基本的成员，人们只将它作为派生其他类的基类，而不会用来创建对象。甚至，你可以只给出方法的定义而不实现，由子类根据具体需求来具体实现。 这种只给出方法定义而不具体实现的方法被称为抽象方法，抽象方法是没有方法体的，在代码的表达上就是没有“{}”。包含一个或多个抽象方法的类也必须被声明为抽象类。 使用 abstract 修饰符来表示抽象方法和抽象类。 抽象类除了包含抽象方法外，还可以包含具体的变量和具体的方法。类即使不包含抽象方法，也可以被声明为抽象类，防止被实例化。 抽象类不能被实例化，抽象方法必须在子类中被实现。请看下面的代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243import static java.lang.System.*;public class Demo&#123; public static void main(String[] args) &#123; Teacher t = new Teacher(); t.setName(&quot;王明&quot;); t.work(); Driver d = new Driver(); d.setName(&quot;小陈&quot;); d.work(); &#125;&#125;// 定义一个抽象类abstract class People&#123; private String name; // 实例变量 // 共有的 setter 和 getter 方法 public void setName(String name)&#123; this.name = name; &#125; public String getName()&#123; return this.name; &#125; // 抽象方法 public abstract void work();&#125;class Teacher extends People&#123; // 必须实现该方法 public void work()&#123; out.println(&quot;我的名字叫&quot; + this.getName() + &quot;，我正在讲课，请大家不要东张西望...&quot;); &#125;&#125;class Driver extends People&#123; // 必须实现该方法 public void work()&#123; out.println(&quot;我的名字叫&quot; + this.getName() + &quot;，我正在开车，不能接听电话...&quot;); &#125;&#125; 运行结果：我的名字叫王明，我正在讲课，请大家不要东张西望…我的名字叫小陈，我正在开车，不能接听电话… 关于抽象类的几点说明： 抽象类不能直接使用，必须用子类去实现抽象类，然后使用其子类的实例。然而可以创建一个变量，其类型是一个抽象类，并让它指向具体子类的一个实例，也就是可以使用抽象类来充当形参，实际实现类作为实参，也就是多态的应用。 不能有抽象构造方法或抽象静态方法。 在下列情况下，一个类将成为抽象类： 当一个类的一个或多个方法是抽象方法时； 当类是一个抽象类的子类，并且不能为任何抽象方法提供任何实现细节或方法主体时； 当一个类实现一个接口，并且不能为任何抽象方法提供实现细节或方法主体时；注意： 这里说的是这些情况下一个类将成为抽象类，没有说抽象类一定会有这些情况。一个典型的错误：抽象类一定包含抽象方法。 但是反过来说“包含抽象方法的类一定是抽象类”就是正确的。事实上，抽象类可以是一个完全正常实现的类 四、接口的概念和使用4.1 接口的概念在抽象类中，可以包含一个或多个抽象方法；但在接口(interface)中，所有的方法必须都是抽象的，不能有方法体，它比抽象类更加“抽象”。 接口使用 interface 关键字来声明，可以看做是一种特殊的抽象类，可以指定一个类必须做什么，而不是规定它如何去做。 现实中也有很多接口的实例，比如说串口电脑硬盘，Serial ATA委员会指定了Serial ATA 2.0规范，这种规范就是接口。Serial ATA委员会不负责生产硬盘，只是指定通用的规范。 希捷、日立、三星等生产厂家会按照规范生产符合接口的硬盘，这些硬盘就可以实现通用化，如果正在用一块160G日立的串口硬盘，现在要升级了，可以购买一块320G的希捷串口硬盘，安装上去就可以继续使用了。 下面的代码可以模拟Serial ATA委员会定义以下串口硬盘接口： 12345678public interface Demo&#123; //连接线的数量 public static final int CONNECT_LINE=4; //写数据 public void writeData(String data); //读数据 public String readData();&#125; 注意：接口中声明的成员变量默认都是 public static final 的，必须显式地初始化。因而在常量声明时可以省略这些修饰符。 接口是若干常量和抽象方法的集合，目前看来和抽象类差不多。确实如此，接口本就是从抽象类中演化而来的，因而除特别规定，接口享有和类同样的“待遇”。比如，源程序中可以定义多个类或接口，但最多只能有一个public 的类或接口，如果有则源文件必须取和public的类和接口相同的名字。和类的继承格式一样，接口之间也可以继承，子接口可以继承父接口中的常量和抽象方法并添加新的抽象方法等。 但接口有其自身的一些特性，归纳如下。 1) 接口中只能定义抽象方法，这些方法默认为 public abstract 的，因而在声明方法时可以省略这些修饰符。试图在接口中定义实例变量、非抽象的实例方法及静态方法，都是非法的。例如： 12345678910public interface SataHdd&#123; //连接线的数量 public int connectLine; //编译出错，connectLine被看做静态常量，必须显式初始化 //写数据 protected void writeData(String data); //编译出错，必须是public类型 //读数据 public static String readData()&#123; //编译出错，接口中不能包含静态方法 return &quot;数据&quot;; //编译出错，接口中只能包含抽象方法， &#125;&#125; 2) 接口中没有构造方法，不能被实例化。 3) 一个接口不实现另一个接口，但可以继承多个其他接口。接口的多继承特点弥补了类的单继承。例如： 123456789101112131415//串行硬盘接口public interface SataHdd extends A,B&#123; // 连接线的数量 public static final int CONNECT_LINE = 4; // 写数据 public void writeData(String data); // 读数据 public String readData();&#125;interface A&#123; public void a();&#125;interface B&#123; public void b();&#125; 4.2 为什么使用接口大型项目开发中，可能需要从继承链的中间插入一个类，让它的子类具备某些功能而不影响它们的父类。例如 A -&gt; B -&gt; C -&gt; D -&gt; E，A 是祖先类，如果需要为C、D、E类添加某些通用的功能，最简单的方法是让C类再继承另外一个类。但是问题来了，Java 是一种单继承的语言，不能再让C继承另外一个父类了，只到移动到继承链的最顶端，让A再继承一个父类。这样一来，对C、D、E类的修改，影响到了整个继承链，不具备可插入性的设计。 接口是可插入性的保证。在一个继承链中的任何一个类都可以实现一个接口，这个接口会影响到此类的所有子类，但不会影响到此类的任何父类。此类将不得不实现这个接口所规定的方法，而子类可以从此类自动继承这些方法，这时候，这些子类具有了可插入性。 我们关心的不是哪一个具体的类，而是这个类是否实现了我们需要的接口。 接口提供了关联以及方法调用上的可插入性，软件系统的规模越大，生命周期越长，接口使得软件系统的灵活性和可扩展性，可插入性方面得到保证。 接口在面向对象的 Java 程序设计中占有举足轻重的地位。事实上在设计阶段最重要的任务之一就是设计出各部分的接口，然后通过接口的组合，形成程序的基本框架结构。 4.3 接口的使用接口的使用与类的使用有些不同。在需要使用类的地方，会直接使用new关键字来构建一个类的实例，但接口不可以这样使用，因为接口不能直接使用 new 关键字来构建实例。 接口必须通过类来实现(implements)它的抽象方法，然后再实例化类。类实现接口的关键字为implements。 如果一个类不能实现该接口的所有抽象方法，那么这个类必须被定义为抽象方法。 不允许创建接口的实例，但允许定义接口类型的引用变量，该变量指向了实现接口的类的实例。 一个类只能继承一个父类，但却可以实现多个接口。 实现接口的格式如下：修饰符 class 类名 extends 父类 implements 多个接口 {实现方法} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import static java.lang.System.*;public class Demo&#123; public static void main(String[] args) &#123; SataHdd sh1=new SeagateHdd(); //初始化希捷硬盘 SataHdd sh2=new SamsungHdd(); //初始化三星硬盘 &#125;&#125;//串行硬盘接口interface SataHdd&#123; //连接线的数量 public static final int CONNECT_LINE=4; //写数据 public void writeData(String data); //读数据 public String readData();&#125;// 维修硬盘接口interface fixHdd&#123; // 维修地址 String address = &quot;北京市海淀区&quot;; // 开始维修 boolean doFix();&#125;//捷硬盘class SeagateHdd implements SataHdd, fixHdd&#123; //希捷硬盘读取数据 public String readData()&#123; return &quot;数据&quot;; &#125; //希捷硬盘写入数据 public void writeData(String data) &#123; out.println(&quot;写入成功&quot;); &#125; // 维修希捷硬盘 public boolean doFix()&#123; return true; &#125;&#125;//三星硬盘class SamsungHdd implements SataHdd&#123; //三星硬盘读取数据 public String readData()&#123; return &quot;数据&quot;; &#125; //三星硬盘写入数据 public void writeData(String data)&#123; out.println(&quot;写入成功&quot;); &#125;&#125;//某劣质硬盘，不能写数据abstract class XXHdd implements SataHdd&#123; //硬盘读取数据 public String readData() &#123; return &quot;数据&quot;; &#125;&#125; 4.4 接口作为类型使用接口作为引用类型来使用，任何实现该接口的类的实例都可以存储在该接口类型的变量中，通过这些变量可以访问类中所实现的接口中的方法，Java 运行时系统会动态地确定应该使用哪个类中的方法，实际上是调用相应的实现类的方法。 示例如下： 12345678910111213141516171819public class Demo&#123; public void test1(A a) &#123; a.doSth(); &#125; public static void main(String[] args) &#123; Demo d = new Demo(); A a = new B(); d.test1(a); &#125;&#125;interface A &#123; public int doSth();&#125;class B implements A &#123; public int doSth() &#123; System.out.println("now in B"); return 123; &#125;&#125; 运行结果：now in B 大家看到接口可以作为一个类型来使用，把接口作为方法的参数和返回类型。 五、接口和抽象类的区别类是对象的模板，抽象类和接口可以看做是具体的类的模板。 由于从某种角度讲，接口是一种特殊的抽象类，它们的渊源颇深，有很大的相似之处，所以在选择使用谁的问题上很容易迷糊。我们首先分析它们具有的相同点。 都代表类树形结构的抽象层。在使用引用变量时，尽量使用类结构的抽象层，使方法的定义和实现分离，这样做对于代码有松散耦合的好处。 都不能被实例化。 都能包含抽象方法。抽象方法用来描述系统提供哪些功能，而不必关心具体的实现。 下面说一下抽象类和接口的主要区别。 1) 抽象类可以为部分方法提供实现，避免了在子类中重复实现这些方法，提高了代码的可重用性，这是抽象类的优势；而接口中只能包含抽象方法，不能包含任何实现。 12345678910111213141516public abstract class A&#123; public abstract void method1(); public void method2()&#123; //A method2 &#125;&#125;public class B extends A&#123; public void method1()&#123; //B method1 &#125;&#125;public class C extends A&#123; public void method1()&#123; //C method1 &#125;&#125; 抽象类A有两个子类B、C，由于A中有方法method2的实现，子类B、C中不需要重写method2方法，我们就说A为子类提供了公共的功能，或A约束了子类的行为。method2就是代码可重用的例子。A 并没有定义 method1的实现，也就是说B、C 可以根据自己的特点实现method1方法，这又体现了松散耦合的特性。 再换成接口看看： 1234567891011121314151617181920public interface A&#123; public void method1(); public void method2();&#125;public class B implements A&#123; public void method1()&#123; //B method1 &#125; public void method2()&#123; //B method2 &#125;&#125;public class C implements A&#123; public void method1()&#123; //C method1 &#125; public void method2()&#123; //C method2 &#125;&#125; 接口A无法为实现类B、C提供公共的功能，也就是说A无法约束B、C的行为。B、C可以自由地发挥自己的特点现实 method1和 method2方法，接口A毫无掌控能力。 2) 一个类只能继承一个直接的父类（可能是抽象类），但一个类可以实现多个接口，这个就是接口的优势。 12345678910111213141516171819202122232425262728293031interface A&#123; public void method2();&#125;interface B&#123; public void method1();&#125;class C implements A,B&#123; public void method1()&#123; //C method1 &#125; public void method2()&#123; //C method2 &#125;&#125;//可以如此灵活的使用C，并且C还有机会进行扩展，实现其他接口A a=new C();B b=new C();abstract class A&#123; public abstract void method1();&#125;abstract class B extends A&#123; public abstract void method2();&#125;class C extends B&#123; public void method1()&#123; //C method1 &#125; public void method2() &#123; //C method2 &#125;&#125; 对于C类，将没有机会继承其他父类了。 综上所述，接口和抽象类各有优缺点，在接口和抽象类的选择上，必须遵守这样一个原则： 行为模型应该总是通过接口而不是抽象类定义，所以通常是优先选用接口，尽量少用抽象类。 选择抽象类的时候通常是如下情况：需要定义子类的行为，又要为子类提供通用的功能。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>内部类</tag>
        <tag>抽象类</tag>
        <tag>接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（8）：常用库类、向量与哈希]]></title>
    <url>%2F2017%2F08%2F27%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%888%EF%BC%89%EF%BC%9A%E5%B8%B8%E7%94%A8%E5%BA%93%E7%B1%BB%E3%80%81%E5%90%91%E9%87%8F%E4%B8%8E%E5%93%88%E5%B8%8C%2F</url>
    <content type="text"><![CDATA[一、Java基础类库Java 的类库是 Java 语言提供的已经实现的标准类的集合，是 Java 编程的 API（Application Program Interface），它可以帮助开发者方便、快捷地开发 Java 程序。这些类根据实现的功能不同，可以划分为不同的集合，每个集合组成一个包，称为类库。Java 类库中大部分都是由Sun 公司提供的，这些类库称为基础类库。 Java 语言中提供了大量的类库共程序开发者来使用，了解类库的结构可以帮助开发者节省大量的编程时间，而且能够使编写的程序更简单更实用。Java 中丰富的类库资源也是 Java 语言的一大特色，是 Java 程序设计的基础。 Java 常用包的简单介绍如下： java.lang 包：主要含有与语言相关的类。java.lang 包由解释程序自动加载，不需要显示说明。 java.io 包：主要含有与输入/输出相关的类，这些类提供了对不同的输入和输出设备读写数据的支持，这些输入和输出设备包括键盘、显示器、打印机、磁盘文件等。 java.util 包：包括许多具有特定功能的类，有日期、向量、哈希表、堆栈等，其中 Date类支持与时间有关的操作。 java.swing 包和 java.awt 包：提供了创建图形用户界面元素的类。通过这些元素，编程者可以控制所写的 Applet 或 Application 的外观界面。包中包含了窗口、对话框、菜单等类。 java.net 包：含有与网络操作相关的类，如 TCP Scokets、URL 等工具。 java.applet 包：含有控制 HTML 文档格式、应用程序中的声音等资源的类，其中 Applet类是用来创建包含于 HTML 的 Applet 必不可少的类。 java.beans 包：定义了应用程序编程接口（API），Java Beans 是 Java 应用程序环境的中性平台组件结构。 二、Java语言包(java.lang)简介Java语言包（java.lang）定义了Java中的大多数基本类，由Java语言自动调用，不需要显示声明。该包中包含了Object类，Object类是整个类层次结构的根结点，同时还定义了基本数据类型的类，如：String、Boolean、Byter、Short等。这些类支持数字类型的转换和字符串的操作等，下面将进行简单介绍。 Math类提供了常用的数学运算方法以及Math.PI和Math.E两个数学常量。该类是final的，不能被继承，类中的方法和属性全部是静态，不允许在类的外部创建Math类的对象。因此，只能使用Math类的方法而不能对其作任何更改。表8-1列出了Math类的主要方法。 2.1 Math类 Math类的主要方法 方法 功能 int abs(int i) 求整数的绝对值（另有针对long、float、double的方法） double ceil(double d) 不小于d的最小整数（返回值为double型） double floor(double d) 不大于d的最大整数（返回值为double型） int max(int i1,int i2) 求两个整数中最大数（另有针对long、float、double的方法） int min(int i1,int i2) 求两个整数中最小数（另有针对long、float、double的方法） double random() 产生0~1之间的随机数 int round(float f) 求最靠近f的整数 long round(double d) 求最靠近d的长整数 double sqrt(double a) 求平方根 double sin(double d) 求d的sin值（另有求其他三角函数的方法如cos，tan，atan） double log(double x) 求自然对数 double exp(double x) 求e的x次幂（ex） double pow(double a, double b) 求a的b次幂 例如：产生10个10~100之间的随机整数。 123456789101112//********** ep8_2.java **********class Demo&#123; public static void main(String args[])&#123; int a; System.out.print(&quot;随机数为：&quot;); for(int i=1;i&lt;=10;i++)&#123; a=(int)((100-10+1)*Math.random()+10); System.out.print(&quot; &quot;+a); &#125; System.out.println(); &#125;&#125; 2.2 字符串类字符串是字符的序列。在 Java 中，字符串无论是常量还是变量都是用类的对象来实现的。java.lang 提供了两种字符串类：String 类和 StringBuffer 类。 1.String 类 按照 Java 语言的规定，String 类是 immutable 的 Unicode 字符序列，其作用是实现一种不能改变的静态字符串。例如，把两个字符串连接起来的结果是生成一个新的字符串，而不会使原来的字符串改变。实际上，所有改变字符串的结果都是生成新的字符串，而不是改变原来字符串。 字符串与数组的实现很相似，也是通过 index 编号来指出字符在字符串中的位置的，编号从0 开始，第 2 个字符的编号为 1，以此类推。如果要访问的编号不在合法的范围内，系统会产生 StringIndexOutOfBoundsExecption 异常。如果 index 的值不是整数，则会产生编译错误。 String 类提供了如下表所示的几种字符串创建方法。 方法 功能 String s=”Hello!” 用字符串常量自动创建 String 实例。 String s=new String(String s) 通过 String 对象或字符串常量传递给构造方法。 public String(char value[]) 将整个字符数组赋给 String 构造方法。 public String(char value[], int offset, int count) 将字符数组的一部分赋给 String 构造方法，offset 为起始下标，count为子数组长度。 2.StringBuffer 类 String 类不能改变字符串对象中的内容，只能通过建立一个新串来实现字符串的变化。如果字符串需要动态改变，就需要用 StringBuffer 类。StringBuffer 类主要用来实现字符串内容的添加、修改、删除，也就是说该类对象实体的内存空间可以自动改变大小，以便于存放一个可变的字符序列。 StringBuffer 类提供的三种构造方法： 构造方法 说明 StringBuffer() 使用该无参数的构造方法创建的 StringBuffer 对象，初始容量为 16 个字符，当对象存放的字符序列大于 16 个字符时，对象的容量自动增加。该对象可以通过 length()方法获取实体中存放的字符序列的长度，通过 capacity()方法获取当前对象的实际容量。 StringBuffer(int length) 使用该构造方法创建的 StringBuffer 对象，其初始容量为参数 length 指定的字符个数，当对象存放的字符序列的长度大于 length 时，对象的容量自动增加，以便存放所增加的字符。 StringBuffer(Strin str) 使用该构造方法创建的 StringBuffer 对象，其初始容量为参数字符串 str 的长度再加上 16 个字符。 几种 StringBuffer 类常用的方法 方法 说明 append() 使用 append() 方法可以将其他 Java 类型数据转化为字符串后再追加到 StringBuffer 的对象中。 insert(int index, String str) insert() 方法将一个字符串插入对象的字符序列中的某个位置。 setCharAt(int n, char ch) 将当前 StringBuffer 对象中的字符序列 n 处的字符用参数 ch 指定的字符替换，n 的值必须是非负的，并且小于当前对象中字符串序列的长度。 reverse() 使用 reverse()方法可以将对象中的字符序列翻转。 delete(int n, int m) 从当前 StringBuffer 对象中的字符序列删除一个子字符序列。这里的 n 指定了需要删除的第一个字符的下标，m 指定了需要删除的最后一个字符的下一个字符的下标，因此删除的子字符串从 n~m-1。 replace(int n, int m, String str) 用 str 替换对象中的字符序列，被替换的子字符序列由下标 n 和 m 指定。 三、日期和时间类Java 的日期和时间类位于 java.util 包中。利用日期时间类提供的方法，可以获取当前的日期和时间，创建日期和时间参数，计算和比较时间。 3.1 Date 类Date 类是 Java 中的日期时间类，其构造方法比较多，下面是常用的两个： Date()：使用当前的日期和时间初始化一个对象。 Date(long millisec)：从1970年01月01日00时（格林威治时间）开始以毫秒计算时间，计算 millisec 毫秒。如果运行 Java 程序的本地时区是北京时区（与格林威治时间相差 8 小时），Date dt1=new Date(1000);，那么对象 dt1 就是1970年01月01日08时00分01秒。 请看一个显示日期时间的例子： 123456789import java.util.Date;public class Demo&#123; public static void main(String args[])&#123; Date da=new Date(); //创建时间对象 System.out.println(da); //显示时间和日期 long msec=da.getTime(); System.out.println(&quot;从1970年1月1日0时到现在共有：&quot; + msec + &quot;毫秒&quot;); &#125;&#125; 运行结果： 12Mon Feb 05 22:50:05 CST 2007从1970年1月1日0时到现在共有：1170687005390 毫秒 一些比较常用的 Date 类方法： 方法 功能 boolean after(Date date) 若调用 Date 对象所包含的日期比 date 指定的对象所包含的日期晚，返回 true，否则返回 false。 boolean before(Date date) 若调用 Date 对象所包含的日期比 date 指定的对象所包含的日期早，返回 true，否则返回 false。 Object clone() 复制调用 Date 对象。 int compareTo(Date date) 比较调用对象所包含的日期和指定的对象包含的日期，若相等返回 0；若前者比后者早，返回负值；否则返回正值。 long getTime() 以毫秒数返回从 1970 年 01 月 01 日 00 时到目前的时间。 int hashCode() 返回调用对象的散列值。 void setTime(long time) 根据 time 的值，设置时间和日期。time 值从 1970 年 01 月 01 日 00 时开始计算。 String toString() 把调用的 Date 对象转换成字符串并返回结果。 public Static String valueOf(type variable) 把 variable 转换为字符串。 Date 对象表示时间的默认顺序是星期、月、日、小时、分、秒、年。若需要修改时间显示的格式可以使用“SimpleDateFormat(String pattern)”方法。 例如，用不同的格式输出时间： 123456789101112import java.util.Date;import java.text.SimpleDateFormat;public class Demo&#123; public static void main(String args[])&#123; Date da=new Date(); System.out.println(da); SimpleDateFormat ma1=new SimpleDateFormat(&quot;yyyy 年 MM 月 dd 日 E 北京时间&quot;); System.out.println(ma1.format(da)); SimpleDateFormat ma2=new SimpleDateFormat(&quot;北京时间：yyyy 年 MM 月 dd 日 HH 时 mm 分 ss 秒&quot;); System.out.println(ma2.format(-1000)); &#125;&#125; 运行结果： 123Sun Jan 04 17:31:36 CST 20152015 年 01 月 04 日 星期日 北京时间北京时间：1970 年 01 月 01 日 07 时 59 分 59 秒 3.2 Calendar 类抽象类 Calendar 提供了一组方法，允许把以毫秒为单位的时间转换成一些有用的时间组成部分。Calendar 不能直接创建对象，但可以使用静态方法 getInstance() 获得代表当前日期的日历对象，如： 1Calendar calendar=Calendar.getInstance(); 该对象可以调用下面的方法将日历翻到指定的一个时间： 123void set(int year,int month,int date);void set(int year,int month,int date,int hour,int minute);void set(int year,int month,int date,int hour,int minute,int second); 若要调用有关年份、月份、小时、星期等信息，可以通过调用下面的方法实现： 1int get(int field); 其中，参数 field 的值由 Calendar 类的静态常量决定。其中：YEAR 代表年，MONTH 代表月，HOUR 代表小时，MINUTE 代表分，如： 1calendar.get(Calendar.MONTH); 如果返回值为 0 代表当前日历是一月份，如果返回 1 代表二月份，依此类推。 由 Calendar 定义的一些常用方法如下表所示： 方法 功能 abstract void add(int which,int val) 将 val 加到 which 所指定的时间或者日期中，如果需要实现减的功能，可以加一个负数。which 必须是 Calendar 类定义的字段之一，如 Calendar.HOUR boolean after(Object calendarObj) 如果调用 Calendar 对象所包含的日期比 calendarObj 指定的对象所包含的日期晚，返回 true，否则返回 false boolean before(Object calendarObj) 如果调用 Calendar 对象所包含的日期比 calendarObj 指定的对象所包含的日期早，返回 true，否则返回 false final void clear() 对调用对象包含的所有时间组成部分清零 final void clear(int which) 对调用对象包含的 which 所指定的时间组成部分清零 boolean equals(Object calendarObj) 如果调用 Calendar 对象所包含的日期和 calendarObj 指定的对象所包含的日期相等，返回 true，否则返回 false int get(int calendarField) 返回调用 Calendar 对象的一个时间组成部分的值，这个组成部分由 calendarField指定，可以被返回的组成部分如：Calendar.YEAR，Calendar.MONTH 等 static Calendar getInstance() 返回使用默认地域和时区的一个 Calendar 对象 final Date getTime() 返回一个和调用对象时间相等的 Date 对象 final boolean isSet(int which) 如果调用对象所包含的 which 指定的时间部分被设置了，返回 true，否则返回 false final void set(int year,int month) 设置调用对象的各种日期和时间部分 final void setTime(Date d) 从 Date 对象 d 中获得日期和时间部分 void setTimeZone(TimeZone t) 设置调用对象的时区为 t 指定的那个时区 3.3 GregorianCalendar 类GregorianCalendar 是一个具体实现 Calendar 类的类，该类实现了公历日历。Calendar 类的 getInstance() 方法返回一个 GregorianCalendar，它被初始化为默认的地域和时区下的当前日期和时间。 GregorianCalendar 类定义了两个字段：AD 和 BC，分别代表公元前和公元后。其默认的构造方法 GregorianCalendar() 以默认的地域和时区的当前日期和时间初始化对象，另外也可以指定地域和时区来建立一个 GregorianCalendar 对象，例如： 123GregorianCalendar(Locale locale);GregorianCalendar(TimeZone timeZone);GregorianCalendar(TimeZone timeZone,Locale locale); GregorianCalendar 类提供了 Calendar 类中所有的抽象方法的实现，同时还提供了一些附加的方法，其中用来判断闰年的方法为： 1Boolean isLeapYear(int year); 如果 year 是闰年，该方法返回 true，否则返回 false。 四、哈希表及其应用哈希表也称为散列表，是用来存储群体对象的集合类结构。 4.1 什么是哈希表数组和向量都可以存储对象，但对象的存储位置是随机的，也就是说对象本身与其存储位置之间没有必然的联系。当要查找一个对象时，只能以某种顺序（如顺序查找或二分查找）与各个元素进行比较，当数组或向量中的元素数量很多时，查找的效率会明显的降低。 一种有效的存储方式，是不与其他元素进行比较，一次存取便能得到所需要的记录。这就需要在对象的存储位置和对象的关键属性（设为 k）之间建立一个特定的对应关系（设为 f），使每个对象与一个唯一的存储位置相对应。在查找时，只要根据待查对象的关键属性 k 计算f(k)的值即可。如果此对象在集合中，则必定在存储位置 f(k)上，因此不需要与集合中的其他元素进行比较。称这种对应关系 f 为哈希（hash）方法，按照这种思想建立的表为哈希表。 Java 使用哈希表类（Hashtable）来实现哈希表，以下是与哈希表相关的一些概念： 容量（Capacity）：Hashtable 的容量不是固定的，随对象的加入其容量也可以自动增长。 关键字（Key）：每个存储的对象都需要有一个关键字，key 可以是对象本身，也可以是对象的一部分（如某个属性）。要求在一个 Hashtable 中的所有关键字都是唯一的。 哈希码（Hash Code）：若要将对象存储到 Hashtable 上，就需要将其关键字 key 映射到一个整型数据，成为 key 的哈希码。 项（Item）：Hashtable 中的每一项都有两个域，分别是关键字域 key 和值域 value（存储的对象）。Key 和 value 都可以是任意的 Object 类型的对象，但不能为空。 装填因子（Load Factor）：装填因子表示为哈希表的装满程度，其值等于元素数比上哈希表的长度。 4.2 哈希表的使用哈希表类主要有三种形式的构造方法： Hashtable(); //默认构造函数，初始容量为 101，最大填充因子 0.75 Hashtable(int capacity); Hashtable(int capacity,float loadFactor) 哈希表类的主要方法如下表所示。 方法 功能 void clear() 重新设置并清空哈希表 boolean contains(Object value) 确定哈希表内是否包含了给定的对象，若有返回 true，否则返回 false boolean containsKey(Object key) 确定哈希表内是否包含了给定的关键字，若有返回 true，否则返回 false boolean isEmpty() 确认哈希表是否为空，若是返回 true，否则返回 false Object get(Object key) 获取对应关键字的对象，若不存在返回 null void rehash() 再哈希，扩充哈希表使之可以保存更多的元素，当哈希表达到饱和时，系统自动调用此方法 Object put(Object key,Object value) 用给定的关键字把对象保存到哈希表中，此处的关键字和元素均不可为空 Object remove(Object key) 从哈希表中删除与给定关键字相对应的对象，若该对象不存在返回 null int size() 返回哈希表的大小 String toString() 将哈希表内容转换为字符串 哈希表的创建也可以通过 new 操作符实现。其语句为： 1HashTable has=new HashTable(); 哈希表的遍历 123456789101112131415//*** ep8_12.java **********import java.util.*;class Demo&#123; public static void main(String args[])&#123; Hashtable has=new Hashtable(); has.put(&quot;one&quot;,new Integer(1)); has.put(&quot;two&quot;,new Integer(2)); has.put(&quot;three&quot;,new Integer(3)); has.put(&quot;four&quot;,new Double(12.3)); Set s=has.keySet(); for(Iterator&lt;String&gt; i=s.iterator();i.hasNext();)&#123; System.out.println(has.get(i.next())); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>常用库类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（7）：深入理解java异常处理机制]]></title>
    <url>%2F2017%2F08%2F26%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%887%EF%BC%89%EF%BC%9A%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3java%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[一、java异常异常指不期而至的各种状况，如：文件找不到、网络连接失败、非法参数等。异常是一个事件，它发生在程序运行期间，干扰了正常的指令流程。Java通 过API中Throwable类的众多子类描述各种不同的异常。因而，Java异常都是对象，是Throwable子类的实例，描述了出现在一段编码中的 错误条件。当条件生成时，错误将引发异常。 Java异常类层次结构图： 在 Java 中，所有的异常都有一个共同的祖先 Throwable（可抛出）。Throwable 指定代码中可用异常传播机制通过 Java 应用程序传输的任何问题的共性。 1.1 Exception（异常）和 Error（错误） Throwable： 有两个重要的子类：Exception（异常）和 Error（错误），二者都是 Java 异常处理的重要子类，各自都包含大量子类。 Error（错误）:是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。这些错误表示故障发生于虚拟机自身、或者发生在虚拟机试图执行应用时，如Java虚拟机运行错误（Virtual MachineError）、类定义错误（NoClassDefFoundError）等。这些错误是不可查的，因为它们在应用程序的控制和处理能力之 外，而且绝大多数是程序运行时不允许出现的状况。对于设计合理的应用程序来说，即使确实发生了错误，本质上也不应该试图去处理它所引起的异常状况。在 Java中，错误通过Error的子类描述。 Exception（异常）:是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 类及其子类表示“JVM 常用操作”引发的错误。例如，若试图使用空值对象引用、除数为零或数组越界，则分别引发运行时异常（NullPointerException、ArithmeticException）和 ArrayIndexOutOfBoundException。 注意：异常和错误的区别：异常能被程序本身可以处理，错误是无法处理。 1.2 可查异常（checked exceptions）和不可查异常（unchecked exceptions）Java的异常(包括Exception和Error)分为可查的异常（checked exceptions）和不可查的异常（unchecked exceptions）。 可查异常（编译器要求必须处置的异常）： 正确的程序在运行中，很容易出现的、情理可容的异常状况。可查异常虽然是异常状况，但在一定程度上它的发生是可以预计的，而且一旦发生这种异常状况，就必须采取某种方式进行处理。 除了RuntimeException及其子类以外，其他的Exception类及其子类都属于可查异常。这种异常的特点是Java编译器会检查它，也就是说，当程序中可能出现这类异常，要么用try-catch语句捕获它，要么用throws子句声明抛出它，否则编译不会通过。 不可查异常(编译器不要求强制处置的异常): 包括运行时异常（RuntimeException与其子类）和错误（Error）。 1.3 运行时异常和非运行时异常Exception 这种异常分两大类运行时异常和非运行时异常(编译异常)。程序中应当尽可能去处理这些异常。 运行时异常： 都是RuntimeException类及其子类异常，如NullPointerException(空指针异常)、IndexOutOfBoundsException(下标越界异常)等，这些异常是不检查异常，程序中可以选择捕获处理，也可以不处理。这些异常一般是由程序逻辑错误引起的，程序应该从逻辑角度尽可能避免这类异常的发生。运行时异常的特点是Java编译器不会检查它，也就是说，当程序中可能出现这类异常，即使没有用try-catch语句捕获它，也没有用throws子句声明抛出它，也会编译通过。 非运行时异常 （编译异常）： 是RuntimeException以外的异常，类型上都属于Exception类及其子类。从程序语法角度讲是必须进行处理的异常，如果不处理，程序就不能编译通过。如IOException、SQLException等以及用户自定义的Exception异常，一般情况下不自定义检查异常。 二、处理异常机制在Java应用程序中，异常处理机制为：抛出异常，捕捉异常。 抛出异常： 当一个方法出现错误引发异常时，方法创建异常对象并交付运行时系统，异常对象中包含了异常类型和异常出现时的程序状态等异常信息。运行时系统负责寻找处置异常的代码并执行。 捕获异常： 在方法抛出异常之后，运行时系统将转为寻找合适的异常处理器（exception handler）。潜在的异常处理器是异常发生时依次存留在调用栈中的方法的集合。当异常处理器所能处理的异常类型与方法抛出的异常类型相符时，即为合适 的异常处理器。运行时系统从发生异常的方法开始，依次回查调用栈中的方法，直至找到含有合适异常处理器的方法并执行。当运行时系统遍历调用栈而未找到合适 的异常处理器，则运行时系统终止。同时，意味着Java程序的终止。 对于运行时异常、错误或可查异常，Java技术所要求的异常处理方式有所不同。 由于运行时异常的不可查性，为了更合理、更容易地实现应用程序，Java规定，运行时异常将由Java运行时系统自动抛出，允许应用程序忽略运行时异常。 对于方法运行中可能出现的Error，当运行方法不欲捕捉时，Java允许该方法不做任何抛出声明。因为，大多数Error异常属于永远不能被允许发生的状况，也属于合理的应用程序不该捕捉的异常。 对于所有的可查异常，Java规定：一个方法必须捕捉，或者声明抛出方法之外。也就是说，当一个方法选择不捕捉可查异常时，它必须声明将抛出异常。 能够捕捉异常的方法，需要提供相符类型的异常处理器。所捕捉的异常，可能是由于自身语句所引发并抛出的异常，也可能是由某个调用的方法或者Java运行时 系统等抛出的异常。也就是说，一个方法所能捕捉的异常，一定是Java代码在某处所抛出的异常。简单地说，异常总是先被抛出，后被捕捉的。 任何Java代码都可以抛出异常，如：自己编写的代码、来自Java开发环境包中代码，或者Java运行时系统。无论是谁，都可以通过Java的throw语句抛出异常。 从方法中抛出的任何异常都必须使用throws子句。 捕捉异常通过try-catch语句或者try-catch-finally语句实现。 总体来说，Java规定，对于可查异常必须捕捉、或者声明抛出。允许忽略不可查的RuntimeException和Error。 2.1 捕获异常：try、catch和finally2.1.1 try-catch语句在Java中，异常通过try-catch语句捕获。其一般语法形式为： 12345678try &#123; // 可能会发生异常的程序代码 &#125; catch (Type1 id1)&#123; // 捕获并处置try抛出的异常类型Type1 &#125; catch (Type2 id2)&#123; //捕获并处置try抛出的异常类型Type2 &#125; 关键词try后的一对大括号将一块可能发生异常的代码包起来，称为监控区域。Java方法在运行过程中出现异常，则创建异常对象。将异常抛出监控区域之 外，由Java运行时系统试图寻找匹配的catch子句以捕获异常。若有匹配的catch子句，则运行其异常处理代码，try-catch语句结束。 匹配的原则是：如果抛出的异常对象属于catch子句的异常类，或者属于该异常类的子类，则认为生成的异常对象与catch块捕获的异常类型相匹配。 例1 捕捉throw语句抛出的“除数为0”异常。 123456789101112131415public class TestException &#123; public static void main(String[] args) &#123; int a = 6; int b = 0; try &#123; // try监控区域 if (b == 0) throw new ArithmeticException(); // 通过throw语句抛出异常 System.out.println(&quot;a/b的值是：&quot; + a / b); &#125; catch (ArithmeticException e) &#123; // catch捕捉异常 System.out.println(&quot;程序出现异常，变量b不能为0。&quot;); &#125; System.out.println(&quot;程序正常结束。&quot;); &#125; &#125; 运行结果： 12程序出现异常，变量b不能为0。程序正常结束。 在try监控区域通过if语句进行判断，当“除数为0”的错误条件成立时引发ArithmeticException异常，创建 ArithmeticException异常对象，并由throw语句将异常抛给Java运行时系统，由系统寻找匹配的异常处理器catch并运行相应异 常处理代码，打印输出“程序出现异常，变量b不能为0。”try-catch语句结束，继续程序流程。 事实上，“除数为0”等ArithmeticException，是RuntimException的子类。而运行时异常将由运行时系统自动抛出，不需要使用throw语句。 例2 捕捉运行时系统自动抛出“除数为0”引发的ArithmeticException异常。 1234567891011public static void main(String[] args) &#123; int a = 6; int b = 0; try &#123; System.out.println(&quot;a/b的值是：&quot; + a / b); &#125; catch (ArithmeticException e) &#123; System.out.println(&quot;程序出现异常，变量b不能为0。&quot;); &#125; System.out.println(&quot;程序正常结束。&quot;); &#125; &#125; 运行结果： 12程序出现异常，变量b不能为0。 程序正常结束。 例2中的语句System.out.println(“a/b的值是：” + a/b);在运行中出现“除数为0”错误，引发ArithmeticException异常。运行时系统创建异常对象并抛出监控区域，转而匹配合适的异常处理器catch，并执行相应的异常处理代码。由于检查运行时异常的代价远大于捕捉异常所带来的益处，运行时异常不可查。Java编译器允许忽略运行时异常，一个方法可以既不捕捉，也不声明抛出运行时异常。 例3 不捕捉、也不声明抛出运行时异常。 12345678public class TestException &#123; public static void main(String[] args) &#123; int a, b; a = 6; b = 0; // 除数b 的值为0 System.out.println(a / b); &#125; &#125; 运行结果： 12Exception in thread &quot;main&quot; java.lang.ArithmeticException: / by zeroat Test.TestException.main(TestException.java:8) 例4 程序可能存在除数为0异常和数组下标越界异常。 123456789101112131415161718public class TestException &#123; public static void main(String[] args) &#123; int[] intArray = new int[3]; try &#123; for (int i = 0; i &lt;= intArray.length; i++) &#123; intArray[i] = i; System.out.println(&quot;intArray[&quot; + i + &quot;] = &quot; + intArray[i]); System.out.println(&quot;intArray[&quot; + i + &quot;]模 &quot; + (i - 2) + &quot;的值: &quot; + intArray[i] % (i - 2)); &#125; &#125; catch (ArrayIndexOutOfBoundsException e) &#123; System.out.println(&quot;intArray数组下标越界异常。&quot;); &#125; catch (ArithmeticException e) &#123; System.out.println(&quot;除数为0异常。&quot;); &#125; System.out.println(&quot;程序正常结束。&quot;); &#125; &#125; 运行结果： 1234567intArray[0] = 0intArray[0]模 -2的值: 0intArray[1] = 1intArray[1]模 -1的值: 0intArray[2] = 2除数为0异常。程序正常结束。 例4中程序可能会出现除数为0异常，还可能会出现数组下标越界异常。程序运行过程中ArithmeticException异常类型是先行匹配的，因此执行相匹配的catch语句： (ArithmeticException e)&#123; 12 System.out.println(&quot;除数为0异常。&quot;); &#125; 需要注意的是，一旦某个catch捕获到匹配的异常类型，将进入异常处理代码。一经处理结束，就意味着整个try-catch语句结束。其他的catch子句不再有匹配和捕获异常类型的机会。 Java通过异常类描述异常类型，异常类的层次结构如图1所示。对于有多个catch子句的异常程序而言，应该尽量将捕获底层异常类的catch子 句放在前面，同时尽量将捕获相对高层的异常类的catch子句放在后面。否则，捕获底层异常类的catch子句将可能会被屏蔽。 RuntimeException异常类包括运行时各种常见的异常，ArithmeticException类和ArrayIndexOutOfBoundsException类都是它的子类。因此，RuntimeException异常类的catch子句应该放在 最后面，否则可能会屏蔽其后的特定异常处理或引起编译错误。 2.1.2 try-catch-finally语句 try-catch语句还可以包括第三部分，就是finally子句。它表示无论是否出现异常，都应当执行的内容。try-catch-finally语句的一般语法形式为：123456789try &#123; // 可能会发生异常的程序代码 &#125; catch (Type1 id1) &#123; // 捕获并处理try抛出的异常类型Type1 &#125; catch (Type2 id2) &#123; // 捕获并处理try抛出的异常类型Type2 &#125; finally &#123; // 无论是否发生异常，都将执行的语句块 &#125; 例5 带finally子句的异常处理程序。 1234567891011121314151617public class TestException &#123; public static void main(String args[]) &#123; int i = 0; String greetings[] = &#123; &quot; Hello world !&quot;, &quot; Hello World !! &quot;, &quot; HELLO WORLD !!!&quot; &#125;; while (i &lt; 4) &#123; try &#123; // 特别注意循环控制变量i的设计，避免造成无限循环 System.out.println(greetings[i++]); &#125; catch (ArrayIndexOutOfBoundsException e) &#123; System.out.println(&quot;数组下标越界异常&quot;); &#125; finally &#123; System.out.println(&quot;--------------------------&quot;); &#125; &#125; &#125; &#125; 运行结果：12345678Hello world !--------------------------Hello World !!--------------------------HELLO WORLD !!!--------------------------数组下标越界异常-------------------------- 在例5中，请特别注意try子句中语句块的设计，如果设计为如下，将会出现死循环。如果设计为： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647try &#123; System.out.println (greetings[i]); i++; &#125; ``` 小结：try 块：用于捕获异常。其后可接零个或多个catch块，如果没有catch块，则必须跟一个finally块。catch 块：用于处理try捕获到的异常。finally 块：无论是否捕获或处理异常，finally块里的语句都会被执行。当在try块或catch块中遇到return语句时，finally语句块将在方法返回之前被执行。在以下4种特殊情况下，finally块不会被执行：1. 在finally语句块中发生了异常。2. 在前面的代码中用了System.exit()退出程序。3. 程序所在的线程死亡。4. 关闭CPU。#### 2.1.3 try-catch-finally 规则1. 必须在 try 之后添加 catch 或 finally 块。try 块后可同时接 catch 和 finally 块，但至少有一个块。2. 必须遵循块顺序：若代码同时使用 catch 和 finally 块，则必须将 catch 块放在 try 块之后。3. catch 块与相应的异常类的类型相关。4. 一个 try 块可能有多个 catch 块。若如此，则执行第一个匹配块。即Java虚拟机会把实际抛出的异常对象依次和各个catch代码块声明的异常类型匹配，如果异常对象为某个异常类型或其子类的实例，就执行这个catch代码块，不会再执行其他的 catch代码块5. 可嵌套 try-catch-finally 结构。6. 在 try-catch-finally 结构中，可重新抛出异常。7. 除了下列情况，总将执行 finally 做为结束：JVM 过早终止（调用 System.exit(int)）；在 finally 块中抛出一个未处理的异常；计算机断电、失火、或遭遇病毒攻击。#### 2.1.4 try、catch、finally语句块的执行顺序1. 当try没有捕获到异常时：try语句块中的语句逐一被执行，程序将跳过catch语句块，执行finally语句块和其后的语句；2. 当try捕获到异常，catch语句块里没有处理此异常的情况：当try语句块里的某条语句出现异常时，而没有处理此异常的catch语句块时，此异常将会抛给JVM处理，finally语句块里的语句还是会被执行，但finally语句块后的语句不会被执行；3. 当try捕获到异常，catch语句块里有处理此异常的情况：在try语句块中是按照顺序来执行的，当执行到某一条语句出现异常时，程序将跳到catch语句块，并与catch语句块逐一匹配，找到与之对应的处理程序，其他的catch语句块将不会被执行，而try语句块中，出现异常之后的语句也不会被执行，catch语句块执行完后，执行finally语句块里的语句，最后执行finally语句块后的语句；### 2.2 抛出异常任何Java代码都可以抛出异常，如：自己编写的代码、来自Java开发环境包中代码，或者Java运行时系统。无论是谁，都可以通过Java的throw语句抛出异常。从方法中抛出的任何异常都必须使用throws子句。#### 2.2.1 throws抛出异常如果一个方法可能会出现异常，但没有能力处理这种异常，可以在方法声明处用throws子句来声明抛出异常。例如汽车在运行时可能会出现故障，汽车本身没办法处理这个故障，那就让开车的人来处理。throws语句用在方法定义时声明该方法要抛出的异常类型，如果抛出的是Exception异常类型，则该方法被声明为抛出所有的异常。多个异常可使用逗号分割。throws语句的语法格式为： methodname throws Exception1,Exception2,..,ExceptionN {}12方法名后的throws Exception1,Exception2,...,ExceptionN 为声明要抛出的异常列表。当方法抛出异常列表的异常时，方法将不对这些类型及其子类类型的异常作处理，而抛向调用该方法的方法，由他去处理。例如： import java.lang.Exception;public class TestException { static void pop() throws NegativeArraySizeException { // 定义方法并抛出NegativeArraySizeException异常 int[] arr = new int[-3]; // 创建数组 } public static void main(String[] args) { // 主方法 try { // try语句处理异常信息 pop(); // 调用pop()方法 } catch (NegativeArraySizeException e) { System.out.println(&quot;pop()方法抛出的异常&quot;);// 输出异常信息 } } }12345678910使用throws关键字将异常抛给调用者后，如果调用者不想处理该异常，可以继续向上抛出，但最终要有能够处理该异常的调用者。 pop方法没有处理异常NegativeArraySizeException，而是由main函数来处理。 Throws抛出异常的规则：1. 如果是不可查异常（unchecked exception），即Error、RuntimeException或它们的子类，那么可以不使用throws关键字来声明要抛出的异常，编译仍能顺利通过，但在运行时会被系统抛出。2. 必须声明方法可抛出的任何可查异常（checked exception）。即如果一个方法可能出现受可查异常，要么用try-catch语句捕获，要么用throws子句声明将它抛出，否则会导致编译错误3. 仅当抛出了异常，该方法的调用者才必须处理或者重新抛出该异常。当方法的调用者无力处理该异常的时候，应该继续抛出，而不是囫囵吞枣。4. 调用方法必须遵循任何可查异常的处理和声明规则。若覆盖一个方法，则不能声明与覆盖方法不同的异常。声明的任何异常必须是被覆盖方法所声明异常的同类或子类。例如： void method1() throws IOException{} //合法 //编译错误，必须捕获或声明抛出IOExceptionvoid method2(){ method1();} //合法，声明抛出IOExceptionvoid method3()throws IOException { method1();} //合法，声明抛出Exception，IOException是Exception的子类void method4()throws Exception { method1();} //合法，捕获IOExceptionvoid method5(){ try{ method1(); }catch(IOException e){…}} //编译错误，必须捕获或声明抛出Exceptionvoid method6(){ try{ method1(); }catch(IOException e){throw new Exception();}} //合法，声明抛出Exceptionvoid method7()throws Exception{ try{ method1(); }catch(IOException e){throw new Exception();}}1234567891011判断一个方法可能会出现异常的依据如下：1. 方法中有throw语句。例如，以上method7()方法的catch代码块有throw语句。2. 调用了其他方法，其他方法用throws子句声明抛出某种异常。例如，method3()方法调用了method1()方法，method1()方法声明抛出IOException，因此，在method3()方法中可能会出现IOException。#### 2.2.2 throw抛出异常throw总是出现在函数体中，用来抛出一个Throwable类型的异常。程序会在throw语句后立即终止，它后面的语句执行不到，然后在包含它的所有try块中（可能在上层调用函数中）从里向外寻找含有与其匹配的catch子句的try块。我们知道，异常是异常类的实例对象，我们可以创建异常类的实例对象通过throw语句抛出。该语句的语法格式为： throw new exceptionname;1例如抛出一个IOException类的异常对象： throw new IOException;1要注意的是，throw 抛出的只能够是可抛出类Throwable 或者其子类的实例对象。下面的操作是错误的： throw new String(“exception”);12345这是因为String 不是Throwable 类的子类。如果抛出了检查异常，则还应该在方法头部声明方法可能抛出的异常类型。该方法的调用者也必须检查处理抛出的异常。如果所有方法都层层上抛获取的异常，最终JVM会进行处理，处理也很简单，就是打印异常消息和堆栈信息。如果抛出的是Error或RuntimeException，则该方法的调用者可选择处理该异常。 package Test;import java.lang.Exception;public class TestException { static int quotient(int x, int y) throws MyException { // 定义方法抛出异常 if (y &lt; 0) { // 判断参数是否小于0 throw new MyException(“除数不能是负数”); // 异常信息 } return x/y; // 返回值 } public static void main(String args[]) { // 主方法 int a =3; int b =0; try { // try语句包含可能发生异常的语句 int result = quotient(a, b); // 调用方法quotient() } catch (MyException e) { // 处理自定义异常 System.out.println(e.getMessage()); // 输出异常信息 } catch (ArithmeticException e) { // 处理ArithmeticException异常 System.out.println(“除数不能为0”); // 输出提示信息 } catch (Exception e) { // 处理其他异常 System.out.println(“程序发生了其他的异常”); // 输出提示信息 } } }class MyException extends Exception { // 创建自定义异常类 String message; // 定义String类型变量 public MyException(String ErrorMessagr) { // 父类方法 message = ErrorMessagr; } public String getMessage() { // 覆盖getMessage()方法 return message; } }12345678### 2.3 异常链&gt; 如果调用quotient(3,-1)将发生MyException异常，程序调转到catch (MyException e)代码块中执行；&gt; 如果调用quotient(5,0)将会因“除数为0”错误引发ArithmeticException异常，属于运行时异常类，由Java运行时系统自动抛出。quotient（）方法没有捕捉ArithmeticException异常，Java运行时系统将沿方法调用栈查到main方法，将抛出的异常上传至quotient（）方法的调用者： int result = quotient(a, b); // 调用方法quotient()由于该语句在try监控区域内，因此传回的“除数为0”的ArithmeticException异常1由Java运行时系统抛出，并匹配catch子句： catch (ArithmeticException e) { // 处理ArithmeticException异常System.out.println(“除数不能为0”); // 输出提示信息}``` 处理结果是输出“除数不能为0”。Java这种向上传递异常信息的处理机制，形成异常链。 Java方法抛出的可查异常将依据调用栈、沿着方法调用的层次结构一直传递到具备处理能力的调用方法，最高层次到main方法为止。如果异常传递到main方法，而main不具备处理能力，也没有通过throws声明抛出该异常，将可能出现编译错误。 如还有其他异常发生 将使用catch (Exception e)捕捉异常。由于Exception是所有异常类的父类，如果将catch (Exception e)代码块放在其他两个代码块的前面，后面的代码块将永远得不到执行，就没有什么意义了，所以catch语句的顺序不可掉换。 2.4 Throwable类中的常用方法注意：catch关键字后面括号中的Exception类型的参数e。Exception就是try代码块传递给catch代码块的变量类型，e就是变量名。catch代码块中语句”e.getMessage();”用于输出错误性质。通常异常处理常用3个函数来获取异常的有关信息: getCause()：返回抛出异常的原因。如果 cause 不存在或未知，则返回 null。 getMeage()：返回异常的消息信息。 printStackTrace()：对象的堆栈跟踪输出至错误输出流，作为字段 System.err 的值。 有时为了简单会忽略掉catch语句后的代码，这样try-catch语句就成了一种摆设，一旦程序在运行过程中出现了异常，就会忽略处理异常，而错误发生的原因很难查找。 三、Java常见异常 在Java中提供了一些异常用来描述经常发生的错误，对于这些异常，有的需要程序员进行捕获处理或声明抛出，有的是由Java虚拟机自动进行捕获处理。Java中常见的异常类: 3.1 runtimeException子类: java.lang.ArrayIndexOutOfBoundsException：数组索引越界异常。当对数组的索引值为负数或大于等于数组大小时抛出。 java.lang.ArithmeticException：算术条件异常。譬如：整数除零等。 java.lang.NullPointerException：空指针异常。当应用试图在要求使用对象的地方使用了null时，抛出该异常。譬如：调用null对象的实例方法、访问null对象的属性、计算null对象的长度、使用throw语句抛出null等等 java.lang.ClassNotFoundException：找不到类异常。当应用试图根据字符串形式的类名构造类，而在遍历CLASSPAH之后找不到对应名称的class文件时，抛出该异常。 java.lang.NegativeArraySizeException：数组长度为负异常 java.lang.ArrayStoreException：数组中包含不兼容的值抛出的异常 java.lang.SecurityException：安全性异常 java.lang.IllegalArgumentException：非法参数异常 3.2 IOException IOException：操作输入流和输出流时可能出现的异常。 EOFException：文件已结束异常 FileNotFoundException ：文件未找到异常3.3 其他 ClassCastException：类型转换异常类 ArrayStoreException：数组中包含不兼容的值抛出的异常 SQLException：操作数据库异常类 NoSuchFieldException：字段未找到异常 NoSuchMethodException：方法未找到抛出的异常 NumberFormatException：字符串转换为数字抛出的异常 StringIndexOutOfBoundsException：字符串索引超出范围抛出的异常 IllegalAccessException：不允许访问某类异常 InstantiationException：当应用程序试图使用Class类中的newInstance()方法创建一个类的实例，而指定的类对象无法被实例化时，抛出该异常 四、自定义异常使用Java内置的异常类可以描述在编程时出现的大部分异常情况。除此之外，用户还可以自定义异常。用户自定义异常类，只需继承Exception类即可。 在程序中使用自定义异常类，大体可分为以下几个步骤。 创建自定义异常类。 在方法中通过throw关键字抛出异常对象。 如果在当前抛出异常的方法中处理异常，可以使用try-catch语句捕获并处理；否则在方法的声明处通过throws关键字指明要抛出给方法调用者的异常，继续进行下一步操作。 在出现异常方法的调用者中捕获并处理异常。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>异常处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（6）：异常处理]]></title>
    <url>%2F2017%2F08%2F25%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%886%EF%BC%89%EF%BC%9A%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[一、异常处理基础Java异常是一个描述在代码段中发生的异常（也就是出错）情况的对象。当异常情况发生，一个代表该异常的对象被创建并且在导致该错误的方法中被抛出（throw）。该方法可以选择自己处理异常或传递该异常。两种情况下，该异常被捕获（catch）并处理。异常可能是由Java运行时系统产生，或者是由你的手工代码产生。被Java抛出的异常与违反语言规范或超出Java执行环境限制的基本错误有关。手工编码产生的异常基本上用于报告方法调用程序的出错状况。 Java异常处理通过5个关键字控制：try、catch、throw、throws和 finally。下面讲述它们如何工作的。程序声明了你想要的异常监控包含在一个try块中。如果在try块中发生异常，它被抛出。你的代码可以捕捉这个异常（用catch）并且用某种合理的方法处理该异常。系统产生的异常被Java运行时系统自动抛出。手动抛出一个异常，用关键字throw。任何被抛出方法的异常都必须通过throws子句定义。任何在方法返回前绝对被执行的代码被放置在finally块中。 下面是一个异常处理块的通常形式： 12345678910111213try&#123; // block of code to monitor for errors&#125;catch (ExceptionType1 exOb) &#123; // exception handler for ExceptionType1&#125;catch (ExceptionType2 exOb) &#123; // exception handler for ExceptionType2&#125;// ...finally &#123; // block of code to be executed before try block ends&#125; 这里，ExceptionType 是发生异常的类型。下面将介绍怎样应用这个框架。 二、异常类型所有异常类型都是内置类Throwable的子类。因此，Throwable在异常类层次结构的顶层。紧接着Throwable下面的是两个把异常分成两个不同分支的子类。一个分支是Exception。该类用于用户程序可能捕捉的异常情况。它也是你可以用来创建你自己用户异常类型子类的类。在Exception分支中有一个重要子类RuntimeException。该类型的异常自动为你所编写的程序定义并且包括被零除和非法数组索引这样的错误。 另一类分支由Error作为顶层，Error定义了在通常环境下不希望被程序捕获的异常。Error类型的异常用于Java运行时系统来显示与运行时系统本身有关的错误。堆栈溢出是这种错误的一例。本章将不讨论关于Error类型的异常处理，因为它们通常是灾难性的致命错误，不是你的程序可以控制的。 三、未被捕获的异常在你学习在程序中处理异常之前，看一看如果你不处理它们会有什么情况发生是很有好处的。下面的小程序包括一个故意导致被零除错误的表达式。 123456class Exc0 &#123; public static void main(String args[]) &#123; int d = 0; int a = 42 / d; &#125;&#125; 当Java运行时系统检查到被零除的情况，它构造一个新的异常对象然后抛出该异常。这导致Exc0的执行停止，因为一旦一个异常被抛出，它必须被一个异常处理程序捕获并且被立即处理。该例中，我们没有提供任何我们自己的异常处理程序，所以异常被Java运行时系统的默认处理程序捕获。任何不是被你程序捕获的异常最终都会被该默认处理程序处理。默认处理程序显示一个描述异常的字符串，打印异常发生处的堆栈轨迹并且终止程序。 下面是由标准javaJDK运行时解释器执行该程序所产生的输出： 12java.lang.ArithmeticException: / by zeroat Exc0.main(Exc0.java:4) 注意，类名Exc0，方法名main，文件名Exc0.java和行数4是怎样被包括在一个简单的堆栈使用轨迹中的。还有，注意抛出的异常类型是Exception的一个名为ArithmeticException的子类，该子类更明确的描述了何种类型的错误方法。本章后面部分将讨论，Java提供多个内置的与可能产生的不同种类运行时错误相匹配的异常类型。 堆栈轨迹将显示导致错误产生的方法调用序列。例如，下面是前面程序的另一个版本，它介绍了相同的错误，但是错误是在main( )方法之外的另一个方法中产生的： 123456789class Demo &#123; static void subroutine() &#123; int d = 0; int a = 10 / d; &#125; public static void main(String args[]) &#123; Exc1.subroutine(); &#125;&#125; 默认异常处理器的堆栈轨迹结果表明了整个调用栈是怎样显示的： 123java.lang.ArithmeticException: / by zeroat Exc1.subroutine(Exc1.java:4)at Exc1.main(Exc1.java:7) 如你所见，栈底是main的第7行，该行调用了subroutine( )方法。该方法在第4行导致了异常。调用堆栈对于调试来说是很重要的，因为它查明了导致错误的精确的步骤。 四、try和catch的使用尽管由Java运行时系统提供的默认异常处理程序对于调试是很有用的，但通常你希望自己处理异常。这样做有两个好处。第一，它允许你修正错误。第二，它防止程序自动终止。大多数用户对于在程序终止运行和在无论何时错误发生都会打印堆栈轨迹感到很烦恼（至少可以这么说）。幸运的是，这很容易避免。为防止和处理一个运行时错误，只需要把你所要监控的代码放进一个try块就可以了。紧跟着try块的，包括一个说明你希望捕获的错误类型的catch子句。完成这个任务很简单，下面的程序包含一个处理因为被零除而产生的ArithmeticException 异常的try块和一个catch子句。 12345678910111213class Demo &#123; public static void main(String args[]) &#123; int d, a; try &#123; // monitor a block of code. d = 0; a = 42 / d; System.out.println(&quot;This will not be printed.&quot;); &#125; catch (ArithmeticException e) &#123; // catch divide-by-zero error System.out.println(&quot;Division by zero.&quot;); &#125; System.out.println(&quot;After catch statement.&quot;); &#125;&#125; 该程序输出如下： 12Division by zero.After catch statement. 注意在try块中的对println( )的调用是永远不会执行的。一旦异常被引发，程序控制由try块转到catch块。执行永远不会从catch块“返回”到try块。因此，“This will not be printed。”将不会被显示。一旦执行了catch语句，程序控制从整个try/catch机制的下面一行继续。 一个try和它的catch语句形成了一个单元。catch子句的范围限制于try语句前面所定义的语句。一个catch语句不能捕获另一个try声明所引发的异常（除非是嵌套的try语句情况）。 被try保护的语句声明必须在一个大括号之内（也就是说，它们必须在一个块中）。你不能单独使用try。 构造catch子句的目的是解决异常情况并且像错误没有发生一样继续运行。例如，下面的程序中，每一个for循环的反复得到两个随机整数。这两个整数分别被对方除，结果用来除12345。最后的结果存在a中。如果一个除法操作导致被零除错误，它将被捕获，a的值设为零，程序继续运行。 123456789101112131415161718192021//Handle an exception and move on.import java.util.Random;class Demo &#123; public static void main(String args[]) &#123; int a=0, b=0, c=0; Random r = new Random(); for(int i=0; i&lt;30; i++) &#123; try &#123; b = r.nextInt(); c = r.nextInt(); a = 12345 / (b/c); &#125; catch (ArithmeticException e) &#123; System.out.println(&quot;Division by zero.&quot;); a = 0; // set a to zero and continue &#125; System.out.println(&quot;a: &quot; + a); &#125; &#125;&#125; Throwable重载toString( )方法（由Object定义），所以它返回一个包含异常描述的字符串。你可以通过在println( )中传给异常一个参数来显示该异常的描述。例如，前面程序的catch块可以被重写成 123456789101112131415161718192021//Handle an exception and move on.import java.util.Random;class Demo &#123; public static void main(String args[]) &#123; int a=0, b=0, c=0; Random r = new Random(); for(int i=0; i&lt;30; i++) &#123; try &#123; b = r.nextInt(); c = r.nextInt(); a = 12345 / (b/c); &#125; catch (ArithmeticException e) &#123; System.out.println(&quot;Exception: &quot; + e); a = 0; // set a to zero and continue &#125; System.out.println(&quot;a: &quot; + a); &#125; &#125;&#125; 当这个版本代替原程序中的版本，程序在标准javaJDK解释器下运行，每一个被零除错误显示下面的消息： 1Exception: java.lang.ArithmeticException: / by zero 尽管在上下文中没有特殊的值，显示一个异常描述的能力在其他情况下是很有价值的——特别是当你对异常进行实验和调试时。 五、多重catch语句的使用某些情况，由单个代码段可能引起多个异常。处理这种情况，你可以定义两个或更多的catch子句，每个子句捕获一种类型的异常。当异常被引发时，每一个catch子句被依次检查，第一个匹配异常类型的子句执行。当一个catch语句执行以后，其他的子句被旁路，执行从try/catch块以后的代码开始继续。下面的例子设计了两种不同的异常类型： 1234567891011121314151617//Demonstrate multiple catch statements.class Demo &#123; public static void main(String args[]) &#123; try &#123; int a = args.length; System.out.println(&quot;a = &quot; + a); int b = 42 / a; int c[] = &#123; 1 &#125;; c[42] = 99; &#125; catch(ArithmeticException e) &#123; System.out.println(&quot;Divide by 0: &quot; + e); &#125; catch(ArrayIndexOutOfBoundsException e) &#123; System.out.println(&quot;Array index oob: &quot; + e); &#125; System.out.println(&quot;After try/catch blocks.&quot;); &#125;&#125; 该程序在没有命令行参数的起始条件下运行导致被零除异常，因为a为0。如果你提供一个命令行参数，它将幸免于难，把a设成大于零的数值。但是它将导致ArrayIndexOutOf BoundsException异常，因为整型数组c的长度为1，而程序试图给c[42]赋值。下面是运行在两种不同情况下程序的输出： 123456C:\&gt;java MultiCatcha = 0Divide by 0: java.lang.ArithmeticException: / by zero After try/catch blocks.C:\&gt;java MultiCatch TestArga = 1Array index oob: java.lang.ArrayIndexOutOfBoundsException After try/catch blocks. 当你用多catch语句时，记住异常子类必须在它们任何父类之前使用是很重要的。这是因为运用父类的catch语句将捕获该类型及其所有子类类型的异常。这样，如果子类在父类后面，子类将永远不会到达。而且，Java中不能到达的代码是一个错误。例如，考虑下面的程序： 123456789101112131415161718//This program contains an error.//A subclass must come before its superclass in a series of catch statements. If not,unreachable code will be created and acompile-time error will result.//*/class Demo &#123; public static void main(String args[]) &#123; try &#123; int a = 0; int b = 42 / a; &#125; catch(Exception e) &#123; System.out.println(&quot;Generic Exception catch.&quot;); &#125; /* This catch is never reached because ArithmeticException is a subclass of Exception. */ catch(ArithmeticException e) &#123; // ERROR - unreachable System.out.println(&quot;This is never reached.&quot;); &#125; &#125;&#125; 如果你试着编译该程序，你会收到一个错误消息，该错误消息说明第二个catch语句不会到达，因为该异常已经被捕获。因为ArithmeticException 是Exception的子类，第一个catch语句将处理所有的面向Exception的错误，包括ArithmeticException。这意味着第二个catch语句永远不会执行。为修改程序，颠倒两个catch语句的次序。 六、try语句的嵌套Try语句可以被嵌套。也就是说，一个try语句可以在另一个try块内部。每次进入try语句，异常的前后关系都会被推入堆栈。如果一个内部的try语句不含特殊异常的catch处理程序，堆栈将弹出，下一个try语句的catch处理程序将检查是否与之匹配。这个过程将继续直到一个catch语句匹配成功，或者是直到所有的嵌套try语句被检查耗尽。如果没有catch语句匹配，Java的运行时系统将处理这个异常。下面是运用嵌套try语句的一个例子： 123456789101112131415161718192021222324//An example of nested try statements.class Demo &#123; public static void main(String args[]) &#123; try &#123; int a = args.length; /* If no command-line args are present,the following statement will generate a divide-by-zero exception. */ int b = 42 / a; System.out.println(&quot;a = &quot; + a); try &#123; // nested try block /* If one command-line arg is used,then a divide-by-zero exception will be generated by the following code. */ if(a==1) a = a/(a-a); // division by zero /* If two command-line args are used,then generate an out-of-bounds exception. */ if(a==2) &#123; int c[] = &#123; 1 &#125;; c[42] = 99; // generate an out-of-bounds exception &#125; &#125; catch(ArrayIndexOutOfBoundsException e) &#123; System.out.println(&quot;Array index out-of-bounds: &quot; + e); &#125; &#125; catch(ArithmeticException e) &#123; System.out.println(&quot;Divide by 0: &quot; + e); &#125; &#125;&#125; 如你所见，该程序在一个try块中嵌套了另一个try块。程序工作如下：当你在没有命令行参数的情况下执行该程序，外面的try块将产生一个被零除的异常。程序在有一个命令行参数条件下执行，由嵌套的try块产生一个被零除的错误。因为内部的块不匹配这个异常，它将把异常传给外部的try块，在那里异常被处理。如果你在具有两个命令行参数的条件下执行该程序，由内部try块产生一个数组边界异常。下面的结果阐述了每一种情况： 12345678C:\&gt;java NestTryDivide by 0: java.lang.ArithmeticException: / by zeroC:\&gt;java NestTry Onea = 1Divide by 0: java.lang.ArithmeticException: / by zeroC:\&gt;java NestTry One Twoa = 2Array index out-of-bounds: java.lang.ArrayIndexOutOfBoundsException 当有方法调用时，try语句的嵌套可以很隐蔽的发生。例如，你可以把对方法的调用放在一个try块中。在该方法内部，有另一个try语句。这种情况下，方法内部的try仍然是嵌套在外部调用该方法的try块中的。下面是前面例子的修改，嵌套的try块移到了方法nesttry( )的内部： 12345678910111213141516171819202122232425262728//Try statements can be implicitly nested via calls to methods. */class Demo &#123; static void nesttry(int a) &#123; try &#123; // nested try block /* If one command-line arg is used,then a divide-by-zero exception will be generated by the following code. */ if(a==1) a = a/(a-a); // division by zero /* If two command-line args are used,then generate an out-of-bounds exception. */ if(a==2) &#123; int c[] = &#123; 1 &#125;; c[42] = 99; // generate an out-of-bounds exception &#125; &#125; catch(ArrayIndexOutOfBoundsException e) &#123; System.out.println(&quot;Array index out-of-bounds: &quot; + e); &#125; &#125; public static void main(String args[]) &#123; try &#123; int a = args.length; /* If no command-line args are present,the following statement will generate a divide-by-zero exception. */ int b = 42 / a; System.out.println(&quot;a = &quot; + a); nesttry(a); &#125; catch(ArithmeticException e) &#123; System.out.println(&quot;Divide by 0: &quot; + e); &#125; &#125;&#125; 该程序的输出与前面的例子相同。 八、throws子句如果一个方法可以导致一个异常但不处理它，它必须指定这种行为以使方法的调用者可以保护它们自己而不发生异常。做到这点你可以在方法声明中包含一个throws子句。一个 throws 子句列举了一个方法可能抛出的所有异常类型。这对于除Error或RuntimeException及它们子类以外类型的所有异常是必要的。一个方法可以抛出的所有其他类型的异常必须在throws子句中声明。如果不这样做，将会导致编译错误。 下面是包含一个throws子句的方法声明的通用形式： 123type method-name(parameter-list) throws exception-list&#123; // body of method&#125; 这里，exception-list是该方法可以抛出的以有逗号分割的异常列表。 下面是一个不正确的例子。该例试图抛出一个它不能捕获的异常。因为程序没有指定一个throws子句来声明这一事实，程序将不会编译。12345678910//this program contains an error and will not compile.class ThrowsDemo &#123; static void throwOne() &#123; System.out.println(&quot;Inside throwOne.&quot;); throw new IllegalAccessException(&quot;demo&quot;); &#125; public static void main(String args[]) &#123; throwOne(); &#125;&#125; 为编译该程序，需要改变两个地方。第一，需要声明throwOne( )引发IllegalAccess Exception异常。第二，main( )必须定义一个try/catch 语句来捕获该异常。正确的例子如下：1234567891011121314//This is now correct.class Demo &#123; static void throwOne() throws IllegalAccessException &#123; System.out.println(&quot;Inside throwOne.&quot;); throw new IllegalAccessException(&quot;demo&quot;); &#125; public static void main(String args[]) &#123; try &#123; throwOne(); &#125; catch (IllegalAccessException e) &#123; System.out.println(&quot;Caught &quot; + e); &#125; &#125;&#125; 下面是例题的输出结果： 12inside throwOnecaught java.lang.IllegalAccessException: demo 九、finally块当异常被抛出，通常方法的执行将作一个陡峭的非线性的转向。依赖于方法是怎样编码的，异常甚至可以导致方法过早返回。这在一些方法中是一个问题。例如，如果一个方法打开一个文件项并关闭，然后退出，你不希望关闭文件的代码被异常处理机制旁路。finally关键字为处理这种意外而设计。finally创建一个代码块。该代码块在一个try/catch 块完成之后另一个try/catch出现之前执行。finally块无论有没有异常抛出都会执行。如果异常被抛出，finally甚至是在没有与该异常相匹配的catch子句情况下也将执行。一个方法将从一个try/catch块返回到调用程序的任何时候，经过一个未捕获的异常或者是一个明确的返回语句，finally子句在方法返回之前仍将执行。这在关闭文件句柄和释放任何在方法开始时被分配的其他资源是很有用的。finally子句是可选项，可以有也可以无。然而每一个try语句至少需要一个catch或finally子句。 下面的例子显示了3种不同的退出方法。每一个都执行了finally子句： 12345678910111213141516171819202122232425262728293031323334353637383940//Demonstrate finally.class Demo &#123; // Through an exception out of the method. static void procA() &#123; try &#123; System.out.println("inside procA"); throw new RuntimeException("demo"); &#125; finally &#123; System.out.println("procA's finally"); &#125; &#125; // Return from within a try block. static void procB() &#123; try &#123; System.out.println("inside procB"); return; &#125; finally &#123; System.out.println("procB's finally"); &#125; &#125; // Execute a try block normally. static void procC() &#123; try &#123; System.out.println("inside procC"); &#125; finally &#123; System.out.println("procC's finally"); &#125; &#125; public static void main(String args[]) &#123; try &#123; procA(); &#125; catch (Exception e) &#123; System.out.println("Exception caught"); &#125; procB(); procC(); &#125;&#125; 该例中，procA( )过早地通过抛出一个异常中断了try。Finally子句在退出时执行。procB( )的try语句通过一个return语句退出。在procB( )返回之前finally子句执行。在procC（）中，try语句正常执行，没有错误。然而，finally块仍将执行。 注意：如果finally块与一个try联合使用，finally块将在try结束之前执行。 下面是上述程序产生的输出： 1234567inside procAprocA’s finallyException caughtinside procBprocB’s finallyinside procCprocC’s finally 十、内置异常在标准包java.lang中，Java定义了若干个异常类。前面的例子曾用到其中一些。这些异常一般是标准类RuntimeException的子类。因为java.lang实际上被所有的Java程序引入，多数从RuntimeException派生的异常都自动可用。而且，它们不需要被包含在任何方法的throws列表中。Java语言中，这被叫做未经检查的异常（unchecked exceptions ）。因为编译器不检查它来看一个方法是否处理或抛出了这些异常。 java.lang中定义的未经检查的异常列于表10-1。表10-2列出了由 java.lang定义的必须在方法的throws列表中包括的异常，如果这些方法能产生其中的某个异常但是不能自己处理它。这些叫做受检查的异常（checked exceptions）。Java定义了几种与不同类库相关的其他的异常类型。 Java 的 java.lang 中定义的未检查异常子类 异常 说明 ArithmeticException 算术错误，如被0除 ArrayIndexOutOfBoundsException 数组下标出界 ArrayStoreException 数组元素赋值类型不兼容 ClassCastException 非法强制转换类型 IllegalArgumentException 调用方法的参数非法 IllegalMonitorStateException 非法监控操作，如等待一个未锁定线程 IllegalStateException 环境或应用状态不正确 IllegalThreadStateException 请求操作与当前线程状态不兼容 IndexOutOfBoundsException 某些类型索引越界 NullPointerException 非法使用空引用 NumberFormatException 字符串到数字格式非法转换 SecurityException 试图违反安全性 StringIndexOutOfBounds 试图在字符串边界之外索引 UnsupportedOperationException 遇到不支持的操作 java.lang 中定义的检查异常 异常 意义 ClassNotFoundException 找不到类 CloneNotSupportedException 试图克隆一个不能实现Cloneable接口的对象 IllegalAccessException 对一个类的访问被拒绝 InstantiationException 试图创建一个抽象类或者抽象接口的对象 InterruptedException 一个线程被另一个线程中断 NoSuchFieldException 请求的字段不存在 NoSuchMethodException 请求的方法不存在 十一、创建自己的异常子类尽管Java的内置异常处理大多数常见错误，你也许希望建立你自己的异常类型来处理你所应用的特殊情况。这是非常简单的：只要定义Exception的一个子类就可以了（Exception当然是Throwable的一个子类）。你的子类不需要实际执行什么——它们在类型系统中的存在允许你把它们当成异常使用。Exception类自己没有定义任何方法。当然，它继承了Throwable提供的一些方法。因此，所有异常，包括你创建的，都可以获得Throwable定义的方法。这些方法显示在表10-3中。你还可以在你创建的异常类中覆盖一个或多个这样的方法。 Throwable 定义的方法 方法 描述 Throwable fillInStackTrace( ) 返回一个包含完整堆栈轨迹的Throwable对象，该对象可能被再次引发。 String getLocalizedMessage( ) 返回一个异常的局部描述 String getMessage( ) 返回一个异常的描述 void printStackTrace( ) 显示堆栈轨迹 void printStackTrace(PrintStreamstream) 把堆栈轨迹送到指定的流 void printStackTrace(PrintWriterstream) 把堆栈轨迹送到指定的流 String toString( ) 返回一个包含异常描述的String对象。当输出一个Throwable对象时，该方法被println( )调用 下面的例子声明了Exception的一个新子类，然后该子类当作方法中出错情形的信号。它重载了toString( )方法，这样可以用println( )显示异常的描述。 1234567891011121314151617181920212223242526272829//This program creates a custom exception type.class MyException extends Exception &#123; private int detail; MyException(int a) &#123; detail = a; &#125; public String toString() &#123; return &quot;MyException[&quot; + detail + &quot;]&quot;; &#125;&#125;class ExceptionDemo &#123; static void compute(int a) throws MyException &#123; System.out.println(&quot;Called compute(&quot; + a + &quot;)&quot;); if(a &gt; 10) throw new MyException(a); System.out.println(&quot;Normal exit&quot;); &#125; public static void main(String args[]) &#123; try &#123; compute(1); compute(20); &#125; catch (MyException e) &#123; System.out.println(&quot;Caught &quot; + e); &#125; &#125;&#125; 该例题定义了Exception的一个子类MyException。该子类非常简单：它只含有一个构造函数和一个重载的显示异常值的toString( )方法。ExceptionDemo类定义了一个compute( )方法。该方法抛出一个MyException对象。当compute( )的整型参数比10大时该异常被引发。 main( )方法为MyException设立了一个异常处理程序，然后用一个合法的值和不合法的值调用compute( )来显示执行经过代码的不同路径。下面是结果： 1234Called compute(1)Normal exitCalled compute(20)Caught MyException[20] 十二、断言断言用于证明和测试程序的假设，比如“这里的值大于 5”。断言可以在运行时从代码中完全删除，所以对代码的运行速度没有影响。 断言有两种方法： 一种是 assert&lt;&lt;布尔表达式&gt;&gt; ； 另一种是 assert&lt;&lt;布尔表达式&gt;&gt; ：&lt;&lt;细节描述&gt;&gt;。 如果布尔表达式的值为false ， 将抛出AssertionError 异常； 细节描述是AssertionError异常的描述文本使用 javac –source 1.4 MyClass.java 的方式进行编译示例如下： 123456789101112131415public class Demo &#123; public static void main(String[] args) &#123; int x = 10; if (args.length &gt; 0) &#123; try &#123; x = Integer.parseInt(args[0]); &#125; catch (NumberFormatException nfe) &#123; /* Ignore */ &#125; &#125; System.out.println("Testing assertion that x == 10"); assert x == 10 : "Our assertion failed"; System.out.println("Test passed"); &#125;&#125; 由于引入了一个新的关键字，所以在编译的时候就需要增加额外的参数，要编译成功，必须使用 JDK1.4 的 javac 并加上参数’-source 1.4′,例如可以使用以下的命令编译上面的代码： 1javac -source 1.4 AssertExample.java 以上程序运行使用断言功能也需要使用额外的参数（并且需要一个数字的命令行参数），例如： 1java -ea AssertExample 1 程序的输出为： 123Testing assertion that x == 10Exception in thread “main” java.lang.AssertionError:Our assertion failedat AssertExample.main(AssertExample.java:20) 由于输入的参数不等于 10，因此断言功能使得程序运行时抛出断言错误，注意是错误， 这意味着程序发生严重错误并且将强制退出。断言使用 boolean 值，如果其值不为 true 则 抛出 AssertionError 并终止程序的运行。 断言推荐使用方法 用于验证方法中的内部逻辑，包括： 内在不变式 控制流程不变式 后置条件和类不变式 注意：不推荐用于公有方法内的前置条件的检查。 运行时要屏蔽断言，可以用如下方法： 1java –disableassertions 或 java –da 类名 运行时要允许断言，可以用如下方法： 1java –enableassertions 或 java –ea类名]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>异常处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（5）：static、final关键字和Object类]]></title>
    <url>%2F2017%2F08%2F24%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89%EF%BC%9Astatic%E3%80%81final%E5%85%B3%E9%94%AE%E5%AD%97%E5%92%8CObject%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[一、static关键字static 修饰符能够与变量、方法一起使用，表示是“静态”的。静态变量和静态方法能够通过类名来访问，不需要创建一个类的对象来访问该类的静态成员，所以static修饰的成员又称作类变量和类方法。静态变量与实例变量不同，实例变量总是通过对象来访问，因为它们的值在对象和对象之间有所不同。请看下面的例子： 1234567891011121314public class Demo &#123; static int i = 10; int j; Demo() &#123; this.j = 20; &#125; public static void main(String[] args) &#123; System.out.println(&quot;类变量 i=&quot; + Demo.i); Demo obj = new Demo(); System.out.println(&quot;实例变量 j=&quot; + obj.j); &#125;&#125; 运行结果： 12类变量 i=10实例变量 j=20 1.1 static的内存分配静态变量属于类，不属于任何独立的对象，所以无需创建类的实例就可以访问静态变量。之所以会产生这样的结果，是因为编译器只为整个类创建了一个静态变量的副本，也就是只分配一个内存空间，虽然有多个实例，但这些实例共享该内存。实例变量则不同，每创建一个对象，都会分配一次内存空间，不同变量的内存相互独立，互不影响，改变 a 对象的实例变量不会影响 b 对象。 123456789101112131415public class Demo &#123; static int i; int j; public static void main(String[] args) &#123; Demo obj1 = new Demo(); obj1.i = 10; obj1.j = 20; Demo obj2 = new Demo(); System.out.println("obj1.i=" + obj1.i + ", obj1.j=" + obj1.j); System.out.println("obj2.i=" + obj2.i + ", obj2.j=" + obj2.j); &#125;&#125; 运行结果： 12obj1.i=10, obj1.j=20obj2.i=10, obj2.j=0 注意：静态变量虽然也可以通过对象来访问，但是不被提倡，编译器也会产生警告。 上面的代码中，i 是静态变量，通过 obj1 改变 i 的值，会影响到 obj2；j 是实例变量，通过 obj1 改变 j 的值，不会影响到 obj2。这是因为 obj1.i 和 obj2.i 指向同一个内存空间，而 obj1.j 和 obj2.j 指向不同的内存空间，请看下图： 注意：static 的变量是在类装载的时候就会被初始化。也就是说，只要类被装载，不管你是否使用了这个static 变量，它都会被初始化。 小结：类变量(class variables)用关键字 static 修饰，在类加载的时候，分配类变量的内存，以后再生成类的实例对象时，将共享这块内存（类变量），任何一个对象对类变量的修改，都会影响其它对象。外部有两种访问方式：通过对象来访问或通过类名来访问。 1.2 静态方法静态方法是一种不能向对象实施操作的方法。例如，Math 类的 pow() 方法就是一个静态方法，语法为 Math.pow(x, a)，用来计算 x 的 a 次幂，在使用时无需创建任何 Math 对象。 因为静态方法不能操作对象，所以不能在静态方法中访问实例变量，只能访问自身类的静态变量。 以下情形可以使用静态方法： 一个方法不需要访问对象状态，其所需参数都是通过显式参数提供（例如 Math.pow()）。 一个方法只需要访问类的静态变量。 读者肯定注意到，main() 也是一个静态方法，不对任何对象进行操作。实际上，在程序启动时还没有任何对象，main() 方法是程序的入口，将被执行并创建程序所需的对象。 关于静态变量和静态方法的总结： 一个类的静态方法只能访问静态变量； 一个类的静态方法不能够直接调用非静态方法； 如访问控制权限允许，静态变量和静态方法也可以通过对象来访问，但是不被推荐； 静态方法中不存在当前对象，因而不能使用 this，当然也不能使用 super； 静态方法不能被非静态方法覆盖； 构造方法不允许声明为 static 的； 局部变量不能使用static修饰。 静态方法举例： 12345678910public class Demo &#123; static int sum(int x, int y)&#123; return x + y; &#125; public static void main(String[] args) &#123; int sum = Demo.sum(10, 10); System.out.println(&quot;10+10=&quot; + sum); &#125;&#125; 运行结果： 110+10=20 static 方法不需它所属的类的任何实例就会被调用，因此没有 this 值，不能访问实例变量，否则会引起编译错误。 注意：实例变量只能通过对象来访问，不能通过类访问。 1.3 静态初始器（静态块）块是由大括号包围的一段代码。静态初始器(Static Initializer)是一个存在于类中、方法外面的静态块。静态初始器仅仅在类装载的时候（第一次使用类的时候）执行一次，往往用来初始化静态变量。 示例代码： 123456789101112131415public class Demo &#123; public static int i; static&#123; i = 10; System.out.println(&quot;Now in static block.&quot;); &#125; public void test() &#123; System.out.println(&quot;test method: i=&quot; + i); &#125; public static void main(String[] args) &#123; System.out.println(&quot;Demo.i=&quot; + Demo.i); new Demo().test(); &#125;&#125; 运行结果是： 123Now in static block.Demo.i=10test method: i=10 1.4 静态导入静态导入是 Java 5 的新增特性，用来导入类的静态变量和静态方法。 一般我们导入类都这样写： 1import packageName.className; // 导入某个特定的类 或者 1import packageName.*; // 导入包中的所有类 而静态导入可以这样写： 1import static packageName.className.methonName; // 导入某个特定的静态方法 或者 1import static packageName.className.*; // 导入类中的所有静态成员 导入后，可以在当前类中直接用方法名调用静态方法，不必再用 className.methodName 来访问。 对于使用频繁的静态变量和静态方法，可以将其静态导入。静态导入的好处是可以简化一些操作，例如输出语句 System.out.println(); 中的 out 就是 System 类的静态变量，可以通过 import static java.lang.System.*; 将其导入，下次直接调用 out.println() 就可以了。 1234567import static java.lang.System.*;import static java.lang.Math.random;public class Demo &#123; public static void main(String[] args) &#123; out.println(&quot;产生的一个随机数：&quot; + random()); &#125;&#125; 运行结果： 1产生的一个随机数：0.05800891549018705 二、final关键字在 Java 中，声明类、变量和方法时，可使用关键字 final 来修饰。final 所修饰的数据具有“终态”的特征，表示“最终的”意思。具体规定如下： final 修饰的类不能被继承。 final 修饰的方法不能被子类重写。 final 修饰的变量（成员变量或局部变量）即成为常量，只能赋值一次。 final 修饰的成员变量必须在声明的同时赋值，如果在声明的时候没有赋值，那么只有 一次赋值的机会，而且只能在构造方法中显式赋值，然后才能使用。 final 修饰的局部变量可以只声明不赋值，然后再进行一次性的赋值。 final 一般用于修饰那些通用性的功能、实现方式或取值不能随意被改变的数据，以避免被误用，例如实现数学三角方法、幂运算等功能的方法，以及数学常量π=3.141593、e=2.71828 等。 事实上，为确保终态性，提供了上述方法和常量的 java.lang.Math 类也已被定义为final 的。 需要注意的是，如果将引用类型（任何类的类型）的变量标记为 final，那么该变量不能指向任何其它对象。但可以改变对象的内容，因为只有引用本身是 final 的。 如果变量被标记为 final，其结果是使它成为常数。想改变 final 变量的值会导致一个编译错误。下面是一个正确定义 final 变量的例子： 1public final int MAX_ARRAY_SIZE = 25; // 常量名一般大写 常量因为有 final 修饰，所以不能被继承。请看下面的代码： 12345678910111213141516public final class Demo&#123; public static final int TOTAL_NUMBER = 5; public int id; public Demo() &#123; // 非法，对final变量TOTAL_NUMBER进行二次赋值了 // 因为++TOTAL_NUMBER相当于 TOTAL_NUMBER=TOTAL_NUMBER+1 id = ++TOTAL_NUMBER; &#125; public static void main(String[] args) &#123; final Demo t = new Demo(); final int i = 10; final int j; j = 20; j = 30; // 非法，对final变量进行二次赋值 &#125;&#125; final 也可以用来修饰类（放在 class 关键字前面），阻止该类再派生出子类，例如 Java.lang.String 就是一个 final 类。这样做是出于安全原因，因为要保证一旦有字符串的引用，就必须是类 String 的字符串，而不是某个其它类的字符串（String 类可能被恶意继承并篡改）。 方法也可以被 final 修饰，被 final 修饰的方法不能被覆盖；变量也可以被 final 修饰，被 final 修饰的变量在创建对象以后就不允许改变它们的值了。一旦将一个类声明为 final，那么该类包含的方法也将被隐式地声明为 final，但是变量不是。 被 final 修饰的方法为静态绑定，不会产生多态（动态绑定），程序在运行时不需要再检索方法表，能够提高代码的执行效率。在Java中，被 static 或 private 修饰的方法会被隐式的声明为 final，因为动态绑定没有意义。 由于动态绑定会消耗资源并且很多时候没有必要，所以有一些程序员认为：除非有足够的理由使用多态性，否则应该将所有的方法都用 final 修饰。 这样的认识未免有些偏激，因为 JVM 中的即时编译器能够实时监控程序的运行信息，可以准确的知道类之间的继承关系。如果一个方法没有被覆盖并且很短，编译器就能够对它进行优化处理，这个过程为称为内联(inlining)。例如，内联调用 e.getName() 将被替换为访问 e.name 变量。这是一项很有意义的改进，这是由于CPU在处理调用方法的指令时，使用的分支转移会扰乱预取指令的策略，所以，这被视为不受欢迎的。然而，如果 getName() 在另外一个类中被覆盖，那么编译器就无法知道覆盖的代码将会做什么操作，因此也就不能对它进行内联处理了。 三、Object类Object 类位于 java.lang 包中，是所有 Java 类的祖先，Java 中的每个类都由它扩展而来。定义Java类时如果没有显示的指明父类，那么就默认继承了 Object 类。例如： 123public class Demo&#123; // ...&#125; 实际上是下面代码的简写形式： 123public class Demo extends Object&#123; // ...&#125; 在Java中，只有基本类型不是对象，例如数值、字符和布尔型的值都不是对象，所有的数组类型，不管是对象数组还是基本类型数组都是继承自 Object 类。 Object 类定义了一些有用的方法，由于是根类，这些方法在其他类中都存在，一般是进行了重载或覆盖，实现了各自的具体功能。 3.1 equals() 方法Object 类中的 equals() 方法用来检测一个对象是否等价于另外一个对象，语法为： 1public boolean equals(Object obj) 例如： 1obj1.equals(obj2); 在Java中，数据等价的基本含义是指两个数据的值相等。在通过 equals() 和“==”进行比较的时候，引用类型数据比较的是引用，即内存地址，基本数据类型比较的是值。 在Java中，数据等价的基本含义是指两个数据的值相等。在通过 equals() 和“==”进行比较的时候，引用类型数据比较的是引用，即内存地址，基本数据类型比较的是值。 注意： equals()方法只能比较引用类型，“==”可以比较引用类型及基本类型。 当用 equals() 方法进行比较时，对类 File、String、Date 及包装类来说，是比较类型及内容而不考虑引用的是否是同一个实例。 用“==”进行比较时，符号两边的数据类型必须一致（可自动转换的数据类型除外），否则编译出错，而用 equals 方法比较的两个数据只要都是引用类型即可。 3.2 hashCode()方法散列码(hashCode)是按照一定的算法由对象得到的一个数值，散列码没有规律。如果 x 和 y 是不同的对象，x.hashCode() 与 y.hashCode() 基本上不会相同。 hashCode() 方法主要用来在集合中实现快速查找等操作，也可以用于对象的比较。 在 Java 中，对 hashCode 的规定如下： 在同一个应用程序执行期间，对同一个对象调用 hashCode()，必须返回相同的整数结果——前提是 equals() 所比较的信息都不曾被改动过。至于同一个应用程序在不同执行期所得的调用结果，无需一致。 如果两个对象被 equals() 方法视为相等，那么对这两个对象调用 hashCode() 必须获得相同的整数结果。 如果两个对象被 equals() 方法视为不相等，那么对这两个对象调用 hashCode() 不必产生不同的整数结果。然而程序员应该意识到，对不同对象产生不同的整数结果，有可能提升hashTable的效率。 简单地说：如果两个对象相同，那么它们的 hashCode 值一定要相同；如果两个对象的 hashCode 值相同，它们并不一定相同。在 Java 规范里面规定，一般是覆盖 equals() 方法应该连带覆盖 hashCode() 方法。 3.3 toString()方法toString() 方法是 Object 类中定义的另一个重要方法，是对象的字符串表现形式，语法为： 1public String toString() 返回值是 String 类型，用于描述当前对象的有关信息。Object 类中实现的 toString() 方法是返回当前对象的类型和内存地址信息，但在一些子类（如 String、Date 等）中进行了 重写，也可以根据需要在用户自定义类型中重写 toString() 方法，以返回更适用的信息。 除显式调用对象的 toString() 方法外，在进行 String 与其它类型数据的连接操作时，会自动调用 toString() 方法。 以上几种方法，在Java中是经常用到的，这里仅作简单介绍，让大家对Object类和其他类有所了解，详细说明请参考 Java API 文档。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>static</tag>
        <tag>final</tag>
        <tag>Object</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（4）：多态]]></title>
    <url>%2F2017%2F08%2F23%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89%EF%BC%9A%E5%A4%9A%E6%80%81%2F</url>
    <content type="text"><![CDATA[一、多态在Java中，父类的变量可以引用父类的实例，也可以引用子类的实例。请读者先看一段代码： 12345678910111213141516171819202122232425262728293031323334public class Demo &#123; public static void main(String[] args)&#123; Animal obj = new Animal(); obj.cry(); obj = new Cat(); obj.cry(); obj = new Dog(); obj.cry(); &#125;&#125;class Animal&#123; // 动物的叫声 public void cry()&#123; System.out.println(&quot;不知道怎么叫&quot;); &#125; &#125;class Cat extends Animal&#123; // 猫的叫声 public void cry()&#123; System.out.println(&quot;喵喵~&quot;); &#125;&#125;class Dog extends Animal&#123; // 狗的叫声 public void cry()&#123; System.out.println(&quot;汪汪~&quot;); &#125;&#125; 运行结果： 123不知道怎么叫喵喵~汪汪~ 上面的代码，定义了三个类，分别是 Animal、Cat 和 Dog，Cat 和 Dog 类都继承自 Animal 类。obj 变量的类型为 Animal，它既可以指向 Animal 类的实例，也可以指向 Cat 和 Dog 类的实例，这是正确的。也就是说，父类的变量可以引用父类的实例，也可以引用子类的实例。注意反过来是错误的，因为所有的猫都是动物，但不是所有的动物都是猫。 可以看出，obj 既可以是人类，也可以是猫、狗，它有不同的表现形式，这就被称为多态。多态是指一个事物有不同的表现形式或形态。 再比如“人类”，也有很多不同的表达或实现，TA 可以是司机、教师、医生等，你憎恨自己的时候会说“下辈子重新做人”，那么你下辈子成为司机、教师、医生都可以，我们就说“人类”具备了多态性。 多态存在的三个必要条件：要有继承、要有重写、父类变量引用子类对象。 1234567891011121314151617181920212223242526272829303132333435363738394041public class Demo &#123; public static void main(String[] args)&#123; Animal obj = new Animal(); obj.cry(); obj = new Cat(); obj.bark(); obj = new Dog(); obj.cry(); &#125;&#125;class Animal&#123; // 动物的叫声 public void cry()&#123; System.out.println(&quot;不知道怎么叫&quot;); &#125; public void bark() &#123; System.out.println(&quot;哈哈！！&quot;); &#125; &#125;class Cat extends Animal&#123; // 猫的叫声 public void cry()&#123; System.out.println(&quot;喵喵~&quot;); &#125; public void bark()&#123; System.out.println(&quot;丫丫！！&quot;); &#125; &#125;class Dog extends Animal&#123; // 狗的叫声 public void cry()&#123; System.out.println(&quot;汪汪~&quot;); &#125;&#125; 从上面的例子可以看出，多态的一个好处是：当子类比较多时，也不需要定义多个变量，可以只定义一个父类类型的变量来引用不同子类的实例。请再看下面的一个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class Demo &#123; public static void main(String[] args)&#123; // 借助多态，主人可以给很多动物喂食 Master ma = new Master(); ma.feed(new Animal(), new Food()); ma.feed(new Cat(), new Fish()); ma.feed(new Dog(), new Bone()); &#125;&#125;// Animal类及其子类class Animal&#123; public void eat(Food f)&#123; System.out.println(&quot;我是一个小动物，正在吃&quot; + f.getFood()); &#125;&#125;class Cat extends Animal&#123; public void eat(Food f)&#123; System.out.println(&quot;我是一只小猫咪，正在吃&quot; + f.getFood()); &#125;&#125;class Dog extends Animal&#123; public void eat(Food f)&#123; System.out.println(&quot;我是一只狗狗，正在吃&quot; + f.getFood()); &#125;&#125;// Food及其子类class Food&#123; public String getFood()&#123; return &quot;食物&quot;; &#125;&#125;class Fish extends Food&#123; public String getFood()&#123; return &quot;鱼&quot;; &#125;&#125;class Bone extends Food&#123; public String getFood()&#123; return &quot;骨头&quot;; &#125;&#125;// Master类class Master&#123; public void feed(Animal an, Food f)&#123; an.eat(f); &#125;&#125; 运行结果： 123我是一个小动物，正在吃事物我是一只小猫咪，正在吃鱼我是一只狗狗，正在吃骨头 Master 类的 feed 方法有两个参数，分别是 Animal 类型和 Food 类型，因为是父类，所以可以将子类的实例传递给它，这样 Master 类就不需要多个方法来给不同的动物喂食。 二、动态绑定为了理解多态的本质，下面讲一下Java调用方法的详细流程。 1) 编译器查看对象的声明类型和方法名。 假设调用 obj.func(param)，obj 为 Cat 类的对象。需要注意的是，有可能存在多个名字为func但参数签名不一样的方法。例如，可能存在方法 func(int) 和 func(String)。编译器将会一一列举所有 Cat 类中名为func的方法和其父类 Animal 中访问属性为 public 且名为func的方法。 这样，编译器就获得了所有可能被调用的候选方法列表。 2) 接下来，编泽器将检查调用方法时提供的参数签名。 如果在所有名为func的方法中存在一个与提供的参数签名完全匹配的方法，那么就选择这个方法。这个过程被称为重载解析(overloading resolution)。例如，如果调用 func(“hello”)，编译器会选择 func(String)，而不是 func(int)。由于自动类型转换的存在，例如 int 可以转换为 double，如果没有找到与调用方法参数签名相同的方法，就进行类型转换后再继续查找，如果最终没有匹配的类型或者有多个方法与之匹配，那么编译错误。 这样，编译器就获得了需要调用的方法名字和参数签名。 3) 如果方法的修饰符是private、static、final（static和final将在后续讲解），或者是构造方法，那么编译器将可以准确地知道应该调用哪个方法，我们将这种调用方式 称为静态绑定(static binding)。 与此对应的是，调用的方法依赖于对象的实际类型， 并在运行时实现动态绑。例如调用 func(“hello”)，编泽器将采用动态绑定的方式生成一条调用 func(String) 的指令。 4)当程序运行，并且釆用动态绑定调用方法时，JVM一定会调用与 obj 所引用对象的实际类型最合适的那个类的方法。我们已经假设 obj 的实际类型是 Cat，它是 Animal 的子类，如果 Cat 中定义了 func(String)，就调用它，否则将在 Animal 类及其父类中寻找。 每次调用方法都要进行搜索，时间开销相当大，因此，JVM预先为每个类创建了一个方法表(method lable)，其中列出了所有方法的名称、参数签名和所属的类。这样一来，在真正调用方法的时候，虚拟机仅查找这个表就行了。在上面的例子中，JVM 搜索 Cat 类的方法表，以便寻找与调用 func(“hello”) 相匹配的方法。这个方法既有可能是 Cat.func(String)，也有可能是 Animal.func(String)。注意，如果调用super.func(“hello”)，编译器将对父类的方法表迸行搜索。 假设 Animal 类包含cry()、getName()、getAge() 三个方法，那么它的方法表如下： cry() -&gt; Animal.cry() getName() -&gt; Animal.getName() getAge() -&gt; Animal.getAge() 实际上，Animal 也有默认的父类 Object（后续会讲解），会继承 Object 的方法，所以上面列举的方法并不完整。 假设 Cat 类覆盖了 Animal 类中的 cry() 方法，并且新增了一个方法 climbTree()，那么它的参数列表为： cry() -&gt; Cat.cry() getName() -&gt; Animal.getName() getAge() -&gt; Animal.getAge() climbTree() -&gt; Cat.climbTree() 在运行的时候，调用 obj.cry() 方法的过程如下： JVM 首先访问 obj 的实际类型的方法表，可能是 Animal 类的方法表，也可能是 Cat 类及其子类的方法表。 JVM 在方法表中搜索与 cry() 匹配的方法，找到后，就知道它属于哪个类了。 JVM 调用该方法。 三、instanceof运算符多态性带来了一个问题，就是如何判断一个变量所实际引用的对象的类型 。 C++使用runtime-type information(RTTI)，Java 使用 instanceof 操作符。instanceof 运算符用来判断一个变量所引用的对象的实际类型，注意是它引用的对象的类型，不是变量的类型。请看下面的代码： 123456789101112131415161718192021222324252627282930313233343536373839public final class Demo&#123; public static void main(String[] args) &#123; // 引用 People 类的实例 People obj = new People(); if(obj instanceof Object)&#123; System.out.println(&quot;我是一个对象&quot;); &#125; if(obj instanceof People)&#123; System.out.println(&quot;我是人类&quot;); &#125; if(obj instanceof Teacher)&#123; System.out.println(&quot;我是一名教师&quot;); &#125; if(obj instanceof President)&#123; System.out.println(&quot;我是校长&quot;); &#125; System.out.println(&quot;-----------&quot;); // 分界线 // 引用 Teacher 类的实例 obj = new Teacher(); if(obj instanceof Object)&#123; System.out.println(&quot;我是一个对象&quot;); &#125; if(obj instanceof People)&#123; System.out.println(&quot;我是人类&quot;); &#125; if(obj instanceof Teacher)&#123; System.out.println(&quot;我是一名教师&quot;); &#125; if(obj instanceof President)&#123; System.out.println(&quot;我是校长&quot;); &#125; &#125;&#125;class People&#123; &#125;class Teacher extends People&#123; &#125;class President extends Teacher&#123; &#125; 运行结果： 123456我是一个对象我是人类———–我是一个对象我是人类我是一名教师 可以看出，如果变量引用的是当前类或它的子类的实例，instanceof 返回 true，否则返回 false。 四、多态对象的类型转换这里所说的对象类型转换，是指存在继承关系的对象，不是任意类型的对象。当对不存在继承关系的对象进行强制类型转换时，java 运行时将抛出 java.lang.ClassCastException 异常。在继承链中，我们将子类向父类转换称为“向上转型”，将父类向子类转换称为“向下转型”。很多时候，我们会将变量定义为父类的类型，却引用子类的对象，这个过程就是向上转型。程序运行时通过动态绑定来实现对子类方法的调用，也就是多态性。 然而有些时候为了完成某些父类没有的功能，我们需要将向上转型后的子类对象再转成子类，调用子类的方法，这就是向下转型。 注意：不能直接将父类的对象强制转换为子类类型，只能将向上转型后的子类对象再次转换为子类类型。也就是说，子类对象必须向上转型后，才能再向下转型。请看下面的代码： 12345678910111213141516class Demo &#123; public static void main(String args[]) &#123; SuperClass superObj = new SuperClass(); SonClass sonObj = new SonClass(); // 下面的代码运行时会抛出异常，不能将父类对象直接转换为子类类型 // SonClass sonObj2 = (SonClass)superObj; // 先向上转型，再向下转型 superObj = sonObj; SonClass sonObj1 = (SonClass)superObj; &#125;&#125;class SuperClass&#123; &#125;class SonClass extends SuperClass&#123; &#125; 将第7行的注释去掉，运行时会抛出异常，但是编译可以通过。 因为向下转型存在风险，所以在接收到父类的一个引用时，请务必使用 instanceof 运算符来判断该对象是否是你所要的子类，请看下面的代码： 123456789101112131415161718192021222324public class Demo &#123; public static void main(String args[]) &#123; SuperClass superObj = new SuperClass(); SonClass sonObj = new SonClass(); // superObj 不是 SonClass 类的实例 if(superObj instanceof SonClass)&#123; SonClass sonObj1 = (SonClass)superObj; &#125;else&#123; System.out.println(&quot;①不能转换&quot;); &#125; superObj = sonObj; // superObj 是 SonClass 类的实例 if(superObj instanceof SonClass)&#123; SonClass sonObj2 = (SonClass)superObj; &#125;else&#123; System.out.println(&quot;②不能转换&quot;); &#125; &#125;&#125;class SuperClass&#123; &#125;class SonClass extends SuperClass&#123; &#125; 运行结果：①不能转换 总结：对象的类型转换在程序运行时检查，向上转型会自动进行，向下转型的对象必须是当前引用类型的子类。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多态</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（3）：继承、覆盖、重载]]></title>
    <url>%2F2017%2F08%2F22%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%883%EF%BC%89%EF%BC%9A%E7%BB%A7%E6%89%BF%2F</url>
    <content type="text"><![CDATA[一、继承的概念与实现继承是类与类之间的关系，是一个很简单很直观的概念，与现实世界中的继承（例如儿子继承父亲财产）类似。继承可以理解为一个类从另一个类获取方法和属性的过程。如果类B继承于类A，那么B就拥有A的方法和属性。 继承使用 extends 关键字。例如我们已经定义了一个类 People： 123456789class People&#123; String name; int age; int height; void say()&#123; System.out.println(&quot;我的名字是 &quot; + name + &quot;，年龄是 &quot; + age + &quot;，身高是 &quot; + height); &#125;&#125; 如果现在需要定义一个类 Teacher，它也有 name、age、height 属性和 say() 方法，另外还需要增加 school、seniority、subject 属性和 lecturing() 方法，怎么办呢？我们要重新定义一个类吗？ 完全没必要，可以先继承 People 类的成员，再增加自己的成员即可，例如： 1234567891011121314class Teacher extends People&#123; String school; // 所在学校 String subject; // 学科 int seniority; // 教龄 // 覆盖 People 类中的 say() 方法 void say()&#123; System.out.println(&quot;我叫&quot; + name + &quot;，在&quot; + school + &quot;教&quot; + subject + &quot;，有&quot; + seniority + &quot;年教龄&quot;); &#125; void lecturing()&#123; System.out.println(&quot;我已经&quot; + age + &quot;岁了，依然站在讲台上讲课&quot;); &#125;&#125; 对程序的说明 name 和 age 变量虽然没有在 Teacher 中定义，但是已在 People 中定义，可以直接拿来用。 Teacher 是 People 的子类，People 是Teacher 类的父类。 子类可以覆盖父类的方法。 子类可以继承父类除private以为的所有的成员。 构造方法不能被继承。 继承是在维护和可靠性方面的一个伟大进步。如果在 People 类中进行修改，那么 Teacher 类就会自动修改，而不需要程序员做任何工作，除了对它进行编译。 单继承性：Java 允许一个类仅能继承一个其它类，即一个类只能有一个父类，这个限制被称做单继承性。后面将会学到接口(interface)的概念，接口允许多继承。 最后对上面的代码进行整理： 12345678910111213141516171819202122232425262728293031323334353637public class Demo &#123; public static void main(String[] args) &#123; Teacher t = new Teacher(); t.name = &quot;小布&quot;; t.age = 70; t.school = &quot;清华大学&quot;; t.subject = &quot;Java&quot;; t.seniority = 12; t.say(); t.lecturing(); &#125;&#125;class People&#123; String name; int age; int height; void say()&#123; System.out.println(&quot;我的名字是 &quot; + name + &quot;，年龄是 &quot; + age + &quot;，身高是 &quot; + height); &#125;&#125;class Teacher extends People&#123; String school; // 所在学校 String subject; // 学科 int seniority; // 教龄 // 覆盖 People 类中的 say() 方法 void say()&#123; System.out.println(&quot;我叫&quot; + name + &quot;，在&quot; + school + &quot;教&quot; + subject + &quot;，有&quot; + seniority + &quot;年教龄&quot;); &#125; void lecturing()&#123; System.out.println(&quot;我已经&quot; + age + &quot;岁了，依然站在讲台上讲课&quot;); &#125;&#125; 运行结果： 12我叫小布，在清华大学教Java，有12年教龄我已经70岁了，依然站在讲台上讲课 注意：构造方法不能被继承，掌握这一点很重要。 一个类能得到构造方法，只有两个办法：编写构造方法，或者根本没有构造方法，类有一个默认的构造方法。 二、super关键字super 关键字与 this 类似，this 用来表示当前类的实例，super 用来表示父类。super 可以用在子类中，通过点号(.)来获取父类的成员变量和方法。super 也可以用在子类的子类中，Java 能自动向上层类追溯。父类行为被调用，就好象该行为是本类的行为一样，而且调用行为不必发生在父类中，它能自动向上层类追溯。 super 关键字的功能： 调用父类中声明为 private 的变量。 点取已经覆盖了的方法。 作为方法名表示父类构造方法。 2.1 调用隐藏变量和被覆盖的方法1234567891011121314151617181920212223public class Demo&#123; public static void main(String[] args) &#123; Dog obj = new Dog(); obj.move(); &#125;&#125;class Animal&#123; private String desc = &quot;Animals are human&apos;s good friends&quot;; // 必须要声明一个 getter 方法 public String getDesc() &#123; return desc; &#125; public void move()&#123; System.out.println(&quot;Animals can move&quot;); &#125;&#125;class Dog extends Animal&#123; public void move()&#123; super.move(); // 调用父类的方法 System.out.println(&quot;Dogs can walk and run&quot;); // 通过 getter 方法调用父类隐藏变量 System.out.println(&quot;Please remember: &quot; + super.getDesc()); &#125;&#125; 运行结果： 123Animals can moveDogs can walk and runPlease remember: Animals are human’s good friends move() 方法也可以定义在某些祖先类中，比如父类的父类，Java 具有追溯性，会一直向上找，直到找到该方法为止。 通过 super 调用父类的隐藏变量，必须要在父类中声明 getter 方法，因为声明为 private 的数据成员对子类是不可见的。 2.2 调用父类的构造方法在许多情况下，使用默认构造方法来对父类对象进行初始化。当然也可以使用 super 来显示调用父类的构造方法。 1234567891011121314151617181920212223public class Demo&#123; public static void main(String[] args) &#123; Dog obj = new Dog(&quot;花花&quot;, 3); obj.say(); &#125;&#125;class Animal&#123; String name; public Animal(String name)&#123; this.name = name; &#125;&#125;class Dog extends Animal&#123; int age; public Dog(String name, int age)&#123; super(name); this.age = age; &#125; public void say()&#123; System.out.println(&quot;我是一只可爱的小狗，我的名字叫&quot; + name + &quot;，我&quot; + age + &quot;岁了&quot;); &#125;&#125; 运行结果： 1我是一只可爱的小狗，我的名字叫花花，我3岁了 注意：无论是 super() 还是 this()，都必须放在构造方法的第一行。 值得注意的是： 在构造方法中调用另一个构造方法，调用动作必须置于最起始的位置。 不能在构造方法以外的任何方法内调用构造方法。 在一个构造方法内只能调用一个构造方法。 如果编写一个构造方法，既没有调用 super() 也没有调用 this()，编译器会自动插入一个调用到父类构造方法中，而且不带参数。 最后注意 super 与 this 的区别：super 不是一个对象的引用，不能将 super 赋值给另一个对象变量，它只是一个指示编译器调用父类方法的特殊关键字。 三、继承中的方法的覆盖和重载3.1 覆盖在类继承中，子类可以修改从父类继承来的方法，也就是说子类能创建一个与父类方法有不同功能的方法，但具有相同的名称、返回值类型、参数列表。如果在新类中定义一个方法，其名称、返回值类型和参数列表正好与父类中的相同，那么，新方法被称做覆盖旧方法。参数列表又叫参数签名，包括参数的类型、参数的个数和参数的顺序，只要有一个不同就叫做参数列表不同。被覆盖的方法在子类中只能通过super调用。 注意：覆盖不会删除父类中的方法，而是对子类的实例隐藏，暂时不使用。 123456789101112131415161718192021222324252627282930public class Demo&#123; public static void main(String[] args) &#123; Dog myDog = new Dog("花花"); myDog.say(); // 子类的实例调用子类中的方法 Animal myAnmial = new Animal("贝贝"); myAnmial.say(); // 父类的实例调用父类中的方法 &#125;&#125;class Animal&#123; String name; public Animal(String name)&#123; this.name = name; &#125; public void say()&#123; System.out.println("我是一只小动物，我的名字叫" + name + "，我会发出叫声"); &#125;&#125;class Dog extends Animal&#123; // 构造方法不能被继承，通过super()调用 public Dog(String name)&#123; super(name); &#125; // 覆盖say() 方法 public void say()&#123; System.out.println("我是一只小狗，我的名字叫" + name + "，我会发出汪汪的叫声"); &#125;&#125; 运行结果： 12我是一只小狗，我的名字叫花花，我会发出汪汪的叫声我是一只小动物，我的名字叫贝贝，我会发出叫声 方法覆盖的原则： 覆盖方法的返回类型、方法名称、参数列表必须与原方法的相同。 覆盖方法不能比原方法访问性差（即访问权限不允许缩小）。 覆盖方法不能比原方法抛出更多的异常。 被覆盖的方法不能是final类型，因为final修饰的方法是无法覆盖的。 被覆盖的方法不能为private，否则在其子类中只是新定义了一个方法，并没有对其进行覆盖。 被覆盖的方法不能为static。如果父类中的方法为静态的，而子类中的方法不是静态的，但是两个方法除了这一点外其他都满足覆盖条件，那么会发生编译错误；反之亦然。即使父类和子类中的方法都是静态的，并且满足覆盖条件，但是仍然不会发生覆盖，因为静态方法是在编译的时候把静态方法和类的引用类型进行匹配。 3.2 重载前面已经对Java方法重载进行了说明，这里再强调一下，Java父类和子类中的方法都会参与重载，例如，父类中有一个方法是 func(){ … }，子类中有一个方法是 func(int i){ … }，就构成了方法的重载。 覆盖和重载的不同： 方法覆盖要求参数列表必须一致，而方法重载要求参数列表必须不一致。 方法覆盖要求返回类型必须一致，方法重载对此没有要求。 方法覆盖只能用于子类覆盖父类的方法，方法重载用于同一个类中的所有方法（包括从父类中继承而来的方法）。 方法覆盖对方法的访问权限和抛出的异常有特殊的要求，而方法重载在这方面没有任何限制。 父类的一个方法只能被子类覆盖一次，而一个方法可以在所有的类中可以被重载多次]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>继承</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（2）：类与对象]]></title>
    <url>%2F2017%2F08%2F21%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89%EF%BC%9A%E7%B1%BB%E4%B8%8E%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[一、类的定义及其实例化类必须先定义才能使用。类是创建对象的模板，创建对象也叫类的实例化。 下面通过一个简单的例子来理解Java中类的定义： 123456789101112public class Dog&#123; String name; int age; void bark()&#123; // 汪汪叫 System.out.println("汪汪，不要过来"); &#125; void hungry()&#123; // 饥饿 System.out.println("主人，我饿了"); &#125;&#125; 对示例的说明： public 是类的修饰符，表明该类是公共类，可以被其他类访问。修饰符将在下节讲解。 class 是定义类的关键字。 Dog 是类名称。 name、age是类的成员变量，也叫属性；bark()、hungry() 是类中的函数，也叫方法。 一个类可以包含以下类型变量： 局部变量：在方法或者语句块中定义的变量被称为局部变量。变量声明和初始化都是在方法中，方法结束后，变量就会自动销毁。 成员变量：成员变量是定义在类中、方法体之外的变量。这种变量在创建对象的时候实例化（分配内存）。成员变量可以被类中的方法和特定类的语句访问。 类变量：类变量也声明在类中，方法体之外，但必须声明为static类型。static 也是修饰符的一种，将在下节讲解。 1.1 构造方法在类实例化的过程中自动执行的方法叫做构造方法，它不需要你手动调用。构造方法可以在类实例化的过程中做一些初始化的工作。 构造方法的名称必须与类的名称相同，并且没有返回值。 每个类都有构造方法。如果没有显式地为类定义构造方法，Java编译器将会为该类提供一个默认的构造方法。 下面是一个构造方法示例： 12345678910111213141516171819202122232425public class Dog&#123; String name; int age; // 构造方法，没有返回值 Dog(String name1, int age1)&#123; name = name1; age = age1; System.out.println("感谢主人领养了我"); &#125; // 普通方法，必须有返回值 void bark()&#123; System.out.println("汪汪，不要过来"); &#125; void hungry()&#123; System.out.println("主人，我饿了"); &#125; public static void main(String arg[])&#123; // 创建对象时传递的参数要与构造方法参数列表对应 Dog myDog = new Dog("花花", 3); &#125;&#125; 运行结果： 1感谢主人领养了我 说明： 构造方法不能被显示调用。 构造方法不能有返回值，因为没有变量来接收返回值。1.2 创建对象对象是类的一个实例，创建对象的过程也叫类的实例化。对象是以类为模板来创建的。 在Java中，使用new关键字来创建对象，一般有以下三个步骤： 声明：声明一个对象，包括对象名称和对象类型。 实例化：使用关键字new来创建一个对象。 初始化：使用new创建对象时，会调用构造方法初始化对象。 例如： 12Dog myDog; // 声明一个对象myDog = new Dog(&quot;花花&quot;, 3); // 实例化 也可以在声明的同时进行初始化： 1Dog myDog = new Dog(&quot;花花&quot;, 3); 1.3 访问成员变量和方法通过已创建的对象来访问成员变量和成员方法，例如： 123456//实例化Dog myDog = new Dog(&quot;花花&quot;, 3);// 通过点号访问成员变量myDog.name;// 通过点号访问成员方法myDog.bark(); 下面的例子演示了如何访问成员变量和方法： 1234567891011121314151617181920212223242526272829public class Dog&#123; String name; int age; Dog(String name1, int age1)&#123; name = name1; age = age1; System.out.println("感谢主人领养了我"); &#125; void bark()&#123; System.out.println("汪汪，不要过来"); &#125; void hungry()&#123; System.out.println("主人，我饿了"); &#125; public static void main(String arg[])&#123; Dog myDog = new Dog("花花", 3); // 访问成员变量 String name = myDog.name; int age = myDog.age; System.out.println("我是一只小狗，我名字叫" + name + "，我" + age + "岁了"); // 访问方法 myDog.bark(); myDog.hungry(); &#125;&#125; 运行结果： 1234感谢主人领养了我我是一只小狗，我名字叫花花，我3岁了汪汪，不要过来主人，我饿了 二、Java访问修饰符Java 通过修饰符来控制类、属性和方法的访问权限和其他功能，通常放在语句的最前端。例如： 123456789public class className &#123; // body of class&#125;private boolean myFlag;static final double weeks = 9.5;protected static final int BOXWIDTH = 42;public static void main(String[] arguments) &#123; // body of method&#125; Java 的修饰符很多，分为访问修饰符和非访问修饰符。本节仅介绍访问修饰符，非访问修饰符会在后续介绍。 访问修饰符也叫访问控制符，是指能够控制类、成员变量、方法的使用权限的关键字。 在面向对象编程中，访问控制符是一个很重要的概念，可以使用它来保护对类、变量、方法和构造方法的访问。 Java支持四种不同的访问权限： 修饰符 说明 public 公有的，对所有类可见 protected 受保护的，对同一包内的类和所有子类可见 private 私有的，在同一类内可见 默认的 在同一包内可见。默认不使用任何修饰符 2.1 public：公有的被声明为public的类、方法、构造方法和接口能够被任何其他类访问。 如果几个相互访问的public类分布在不同的包中，则需要导入相应public类所在的包。由于类的继承性，类所有的公有方法和变量都能被其子类继承。 下面的方法使用了公有访问控制： 123public static void main(String[] arguments) &#123; // body of method&#125; Java程序的main() 方法必须设置成公有的，否则，Java解释器将不能运行该类。 2.2 protected：受保护的被声明为protected的变量、方法和构造方法不能被同一个包中的任何其他类访问，也不能够被不同包中的子类访问。 protected访问修饰符不能修饰类和接口，方法和成员变量能够声明为protected，但是接口的成员变量和成员方法不能声明为protected。 子类能访问protected修饰符声明的方法和变量，这样就能保护不相关的类使用这些方法和变量。 下面的父类使用了protected访问修饰符，子类重载了父类的bark()方法。 1234567891011public class Dog&#123; protected void bark() &#123; System.out.println("汪汪，不要过来"); &#125;&#125;class Teddy extends Dog&#123; // 泰迪 void bark() &#123; System.out.println("汪汪，我好怕，不要跟着我"); &#125;&#125; 如果把bark()方法声明为private，那么除了Dog之外的类将不能访问该方法。如果把bark()声明为public，那么所有的类都能够访问该方法。如果我们只想让该方法对其所在类的子类可见，则将该方法声明为protected。 2.3 private：私有的私有访问修饰符是最严格的访问级别，所以被声明为private的方法、变量和构造方法只能被所属类访问，并且类和接口不能声明为private。 声明为私有访问类型的变量只能通过类中公共的Getter/Setter方法被外部类访问。 private访问修饰符的使用主要用来隐藏类的实现细节和保护类的数据。 下面的类使用了私有访问修饰符： 12345678910111213141516public class Dog&#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 例子中，Dog类中的name、age变量为私有变量，所以其他类不能直接得到和设置该变量的值。为了使其他类能够操作该变量，定义了两对public方法，getName()/setName() 和 getAge()/setAge()，它们用来获取和设置私有变量的值。 this 是Java中的一个关键字，接下来会讲到。 在类中定义访问私有变量的方法，习惯上是这样命名的：在变量名称前面加“get”或“set”，并将变量的首字母大写。例如，获取私有变量 name 的方法为 getName()，设置 name 的方法为 setName()。这些方法经常使用，也有了特定的称呼，称为 Getter 和 Setter 方法。 2.4 默认的：不使用任何关键字不使用任何修饰符声明的属性和方法，对同一个包内的类是可见的。接口里的变量都隐式声明为public static final，而接口里的方法默认情况下访问权限为public。 如下例所示，类、变量和方法的定义没有使用任何修饰符： 123456789101112class Dog&#123; String name; int age; void bark()&#123; // 汪汪叫 System.out.println(&quot;汪汪，不要过来&quot;); &#125; void hungry()&#123; // 饥饿 System.out.println(&quot;主人，我饿了&quot;); &#125;&#125; 2.5 访问控制和继承请注意以下方法继承的规则： 父类中声明为public的方法在子类中也必须为public。 父类中声明为protected的方法在子类中要么声明为protected，要么声明为public。不能声明为private。 父类中默认修饰符声明的方法，能够在子类中声明为private。 父类中声明为private的方法，不能够被继承。 2.6 如何使用访问控制符访问控制符可以让我们很方便的控制代码的权限： 当需要让自己编写的类被所有的其他类访问时，就可以将类的访问控制符声明为 public。 当需要让自己的类只能被自己的包中的类访问时，就可以省略访问控制符。 当需要控制一个类中的成员数据时，可以将这个类中的成员数据访问控制符设置为 public、protected，或者省略。 三、Java变量的作用域在Java中，变量的作用域分为四个级别：类级、对象实例级、方法级、块级。 类级变量：又称全局级变量或静态变量，需要使用static关键字修饰，你可以与 C/C++ 中的 static 变量对比学习。类级变量在类定义后就已经存在，占用内存空间，可以通过类名来访问，不需要实例化。 对象实例级变量：就是成员变量，实例化后才会分配内存空间，才能访问。 方法级变量：就是在方法内部定义的变量，就是局部变量。 块级变量：就是定义在一个块内部的变量，变量的生存周期就是这个块，出了这个块就消失了，比如 if、for 语句的块。块是指由大括号包围的代码，例如： 12345678&#123;int age = 3; String name = &quot;www.yq1012.com&quot;; // 正确，在块内部可以访问 age 和 name 变量 System.out.println( name + &quot;已经&quot; + age + &quot;岁了&quot;);&#125;// 错误，在块外部无法访问 age 和 name 变量System.out.println( name + &quot;已经&quot; + age + &quot;岁了&quot;); 说明： 方法内部除了能访问方法级的变量，还可以访问类级和实例级的变量。 块内部能够访问类级、实例级变量，如果块被包含在方法内部，它还可以访问方法级的变量。 方法级和块级的变量必须被显示地初始化，否则不能访问。演示代码： 123456789101112131415161718192021222324252627lic class Demo&#123; public static String name = &quot;程序员&quot;; // 类级变量 public int i; // 对象实例级变量 // 属性块，在类初始化属性时候运行 &#123; int j = 2;// 块级变量 &#125; public void test1() &#123; int j = 3; // 方法级变量 if(j == 3) &#123; int k = 5; // 块级变量 &#125; // 这里不能访问块级变量，块级变量只能在块内部访问 System.out.println(&quot;name=&quot; + name + &quot;, i=&quot; + i + &quot;, j=&quot; + j); &#125; public static void main(String[] args) &#123; // 不创建对象，直接通过类名访问类级变量 System.out.println(Demo.name); // 创建对象并访问它的方法 Demo t = new Demo(); t.test1(); &#125;&#125; 运行结果： 12程序员name=程序员, i=0, j=3 四、this关键字this 关键字用来表示当前对象本身，或当前类的一个实例，通过 this 可以调用本对象的所有方法和属性。例如： 123456789101112131415public class Demo&#123; public int x = 10; public int y = 15; public void sum()&#123; // 通过 this 点取成员变量 int z = this.x + this.y; System.out.println("x + y = " + z); &#125; public static void main(String[] args) &#123; Demo obj = new Demo(); obj.sum(); &#125;&#125; 运行结果： 1x + y = 25 上面的程序中，obj 是 Demo 类的一个实例，this 与 obj 等价，执行 int z = this.x + this.y;，就相当于执行 int z = obj.x + obj.y;。 注意：this 只有在类实例化后才有意义。 4.1 使用this区分同名变量成员变量与方法内部的变量重名时，希望在方法内部调用成员变量，怎么办呢？这时候只能使用this，例如： 123456789101112131415161718public class Demo &#123; public String name; public int age; //构造方法 Demo(String name, int age)&#123; this.name = name; this.age = age; &#125; //普通方法 public void say()&#123; System.out.println("网站的名字是" + name + "，已经成立了" + age + "年"); &#125; //main方法 public static void main(String[] args) &#123; Demo obj = new Demo("程序员", 3); obj.say(); &#125;&#125; 运行结果： 1网站的名字是程序员，已经成立了3年 形参的作用域是整个方法体，是局部变量。在Demo()中，形参和成员变量重名，如果不使用this，访问到的就是局部变量name和age，而不是成员变量。在 say() 中，我们没有使用 this，因为成员变量的作用域是整个实例，当然也可以加上 this： Java 默认将所有成员变量和成员方法与 this 关联在一起，因此使用 this 在某些情况下是多余的。 4.2 作为方法名来初始化对象也就是相当于调用本类的其它构造方法，它必须作为构造方法的第一句。示例如下： 12345678910111213141516171819202122public class Demo&#123; public String name; public int age; public Demo()&#123; this(&quot;程序员&quot;, 3); &#125; public Demo(String name, int age)&#123; this.name = name; this.age = age; &#125; public void say()&#123; System.out.println(&quot;网站的名字是&quot; + name + &quot;，已经成立了&quot; + age + &quot;年&quot;); &#125; public static void main(String[] args) &#123; Demo obj = new Demo(); obj.say(); &#125;&#125; 运行结果： 1网站的名字是程序员，已经成立了3年 值得注意的是： 在构造方法中调用另一个构造方法，调用动作必须置于最起始的位置。 不能在构造方法以外的任何方法内调用构造方法。 在一个构造方法内只能调用一个构造方法。 上述代码涉及到方法重载，即Java允许出现多个同名方法，只要参数不同就可以。后续章节会讲解。 4.3 作为参数传递需要在某些完全分离的类中调用一个方法，并将当前对象的一个引用作为参数传递时。例如： 12345678910111213141516171819202122232425public class Demo&#123; public static void main(String[] args)&#123; B b = new B(new A()); &#125;&#125;class A&#123; public A()&#123; new B(this).print(); // 匿名对象 &#125; public void print()&#123; System.out.println(&quot;Hello from A!&quot;); &#125;&#125;class B&#123; A a; public B(A a)&#123; this.a = a; &#125; public void print() &#123; a.print(); System.out.println(&quot;Hello from B!&quot;); &#125;&#125; 运行结果： 12Hello from A!Hello from B! 匿名对象就是没有名字的对象。如果对象只使用一次，就可以作为匿名对象，代码中 new B(this).print(); 等价于 ( new B(this) ).print();，先通过 new B(this) 创建一个没有名字的对象，再调用它的方法。 五、方法重载在Java中，同一个类中的多个方法可以有相同的名字，只要它们的参数列表不同就可以，这被称为方法重载(method overloading)。参数列表又叫参数签名，包括参数的类型、参数的个数和参数的顺序，只要有一个不同就叫做参数列表不同。 重载是面向对象的一个基本特性。 下面看一个详细的实例。 123456789101112131415161718192021222324252627public class Demo&#123; // 一个普通的方法，不带参数 void test()&#123; System.out.println(&quot;No parameters&quot;); &#125; // 重载上面的方法，并且带了一个整型参数 void test(int a)&#123; System.out.println(&quot;a: &quot; + a); &#125; // 重载上面的方法，并且带了两个参数 void test(int a,int b)&#123; System.out.println(&quot;a and b: &quot; + a + &quot; &quot; + b); &#125; // 重载上面的方法，并且带了一个双精度参数 double test(double a)&#123; System.out.println(&quot;double a: &quot; + a); return a*a; &#125; public static void main(String args[])&#123; Demo obj= new Demo(); obj.test(); obj.test(2); obj.test(2,3); obj.test(2.0); &#125;&#125; 运行结果： 1234No parametersa: 2a and b: 2 3double a: 2.0 通过上面的实例，读者可以看出，重载就是在一个类中，有相同的函数名称，但形参不同的函数。重载的结果，可以让一个程序段尽量减少代码和方法的种类。说明： 参数列表不同包括：个数不同、类型不同和顺序不同。 仅仅参数变量名称不同是不可以的。 跟成员方法一样，构造方法也可以重载。 声明为final的方法不能被重载。 声明为static的方法不能被重载，但是能够被再次声明。 方法的重载的规则： 方法名称必须相同。 参数列表必须不同（个数不同、或类型不同、参数排列顺序不同等）。 方法的返回类型可以相同也可以不相同。 仅仅返回类型不同不足以成为方法的重载。 方法重载的实现： 方法名称相同时，编译器会根据调用方法的参数个数、参数类型等去逐个匹配，以选择对应的方法，如果匹配失败，则编译器报错，这叫做重载分辨。 六、基本运行顺序我们以下面的类来说明一个基本的 Java 类的运行顺序： 12345678910111213public class Demo&#123; private String name; private int age; public Demo()&#123; name = "java学习"; age = 3; &#125; public static void main(String[] args)&#123; Demo obj = new Demo(); System.out.println(obj.name + "的年龄是" + obj.age); &#125;&#125; 基本运行顺序是： 先运行到第 9 行，这是程序的入口。 然后运行到第 10 行，这里要 new 一个Demo，就要调用 Demo 的构造方法。 就运行到第 5 行，注意：可能很多人觉得接下来就应该运行第 6 行了，错！初始化一个类，必须先初始化它的属性。 因此运行到第 2 行，然后是第 3 行。 属性初始化完过后，才回到构造方法，执行里面的代码，也就是第 6 行、第 7 行。 然后是第8行，表示 new 一个Demo实例完成。 然后回到 main 方法中执行第 11 行。 然后是第 12 行，main方法执行完毕。 作为程序员，应该清楚程序的基本运行过程，否则糊里糊涂的，不利于编写代码，也不利于技术上的发展。 七、包装类、拆箱和装箱虽然 Java 语言是典型的面向对象编程语言，但其中的八种基本数据类型并不支持面向对象编程，基本类型的数据不具备“对象”的特性——不携带属性、没有方法可调用。 沿用它们只是为了迎合人类根深蒂固的习惯，并的确能简单、有效地进行常规数据处理。这种借助于非面向对象技术的做法有时也会带来不便，比如引用类型数据均继承了 Object 类的特性，要转换为 String 类型（经常有这种需要）时只要简单调用 Object 类中定义的toString()即可，而基本数据类型转换为 String 类型则要麻烦得多。为解决此类问题 ，Java为每种基本数据类型分别设计了对应的类，称之为包装类(Wrapper Classes)，也有教材称为外覆类或数据类型类。 基本数据类型 对应的包装类 byte Byte short Short int Integer long Long char Character float Float double Double boolean Boolean 每个包装类的对象可以封装一个相应的基本类型的数据，并提供了其它一些有用的方法。包装类对象一经创建，其内容（所封装的基本类型数据值）不可改变。 基本类型和对应的包装类可以相互装换： 由基本类型向对应的包装类转换称为装箱，例如把 int 包装成 Integer 类的对象； 包装类向对应的基本类型转换称为拆箱，例如把 Integer 类的对象重新简化为 int。 7.1 包装类的应用八个包装类的使用比较相似，下面是常见的应用场景。 1）实现 int 和 Integer 的相互转换 可以通过 Integer 类的构造方法将 int 装箱，通过 Integer 类的 intValue 方法将 Integer 拆箱。例如： 1234567891011public class Demo &#123; public static void main(String[] args) &#123; int m = 500; Integer obj = new Integer(m); // 手动装箱 int n = obj.intValue(); // 手动拆箱 System.out.println(&quot;n = &quot; + n); Integer obj1 = new Integer(500); System.out.println(&quot;obj 等价于 obj1？&quot; + obj.equals(obj1)); &#125;&#125; 运行结果： 12n = 500obj 等价于 obj1？true 2）将字符串转换为整数 Integer 类有一个静态的 paseInt() 方法，可以将字符串转换为整数，语法为： 1parseInt(String s, int radix); s 为要转换的字符串，radix 为进制，可选，默认为十进制。 下面的代码将会告诉你什么样的字符串可以转换为整数： 1234567891011121314public class Demo &#123; public static void main(String[] args) &#123; String str[] = &#123;&quot;123&quot;, &quot;123abc&quot;, &quot;abc123&quot;, &quot;abcxyz&quot;&#125;; for(String str1 : str)&#123; try&#123; int m = Integer.parseInt(str1, 10); System.out.println(str1 + &quot; 可以转换为整数 &quot; + m); &#125;catch(Exception e)&#123; System.out.println(str1 + &quot; 无法转换为整数&quot;); &#125; &#125; &#125;&#125; 1234123 可以转换为整数 123123abc 无法转换为整数abc123 无法转换为整数abcxyz 无法转换为整数 3）将整数转换为字符串 Integer 类有一个静态的 toString() 方法，可以将整数转换为字符串。例如： 1234567public class Demo &#123; public static void main(String[] args) &#123; int m = 500; String s = Integer.toString(m); System.out.println(&quot;s = &quot; + s); &#125;&#125; 运行结果： 1s = 500 7.2 自动拆箱和装箱上面的例子都需要手动实例化一个包装类，称为手动拆箱装箱。Java 1.5(5.0) 之前必须手动拆箱装箱。 Java 1.5 之后可以自动拆箱装箱，也就是在进行基本数据类型和对应的包装类转换时，系统将自动进行，这将大大方便程序员的代码书写。例如： 1234567891011public class Demo &#123; public static void main(String[] args) &#123; int m = 500; Integer obj = m; // 自动装箱 int n = obj; // 自动拆箱 System.out.println("n = " + n); Integer obj1 = 500; System.out.println("obj 等价于 obj1？" + obj.equals(obj1)); &#125;&#125; 运行结果： 12n = 500obj 等价于 obj1？true 自动拆箱装箱是常用的一个功能，需要重点掌握。 八、源文件的声明规则当在一个源文件中定义多个类，并且还有import语句和package语句时，要特别注意这些规则： 一个源文件中只能有一个public类。 一个源文件可以有多个非public类。 源文件的名称应该和public类的类名保持一致。例如：源文件中public类的类名是Employee，那么源文件应该命名为Employee.java。 如果一个类定义在某个包中，那么package语句应该在源文件的首行。 如果源文件包含import语句，那么应该放在package语句和类定义之间。如果没有package语句，那么import语句应该在源文件中最前面。 import语句和package语句对源文件中定义的所有类都有效。在同一源文件中，不能给不同的类不同的包声明。 类有若干种访问级别，并且类也分不同的类型：抽象类和final类等。这些将在后续章节介绍。 除了上面提到的几种类型，Java还有一些特殊的类，如内部类、匿名类。 8.1 一个简单的例子在该例子中，我们创建两个类 Employee 和 EmployeeTest，分别放在包 p1 和 p2 中。 Employee类有四个成员变量，分别是 name、age、designation和salary。该类显式声明了一个构造方法，该方法只有一个参数。 在Eclipse中，创建一个包，命名为 p1，在该包中创建一个类，命名为 Employee，将下面的代码复制到源文件中： 12345678910111213141516171819202122232425262728293031package p1;public class Employee&#123; String name; int age; String designation; double salary; // Employee 类的构造方法 public Employee(String name)&#123; this.name = name; &#125; // 设置age的值 public void empAge(int empAge)&#123; age = empAge; &#125; // 设置designation的值 public void empDesignation(String empDesig)&#123; designation = empDesig; &#125; // 设置salary的值 public void empSalary(double empSalary)&#123; salary = empSalary; &#125; // 输出信息 public void printEmployee()&#123; System.out.println("Name:"+ name ); System.out.println("Age:" + age ); System.out.println("Designation:" + designation ); System.out.println("Salary:" + salary); &#125;&#125; 程序都是从main方法开始执行。为了能运行这个程序，必须包含main方法并且创建一个对象。 下面给出EmployeeTest类，该类创建两个Employee对象，并调用方法设置变量的值。 在Eclipse中再创建一个包，命名为 p2，在该包中创建一个类，命名为 EmployeeTest，将下面的代码复制到源文件中： 123456789101112131415161718192021package p2;import p1.*;public class EmployeeTest&#123; public static void main(String args[])&#123; // 创建两个对象 Employee empOne = new Employee("James Smith"); Employee empTwo = new Employee("Mary Anne"); // 调用这两个对象的成员方法 empOne.empAge(26); empOne.empDesignation("Senior Software Engineer"); empOne.empSalary(1000); empOne.printEmployee(); empTwo.empAge(21); empTwo.empDesignation("Software Engineer"); empTwo.empSalary(500); empTwo.printEmployee(); &#125;&#125; 编译并运行 EmployeeTest 类，可以看到如下的输出结果： 12345678Name:James SmithAge:26Designation:Senior Software EngineerSalary:1000.0Name:Mary AnneAge:21Designation:Software EngineerSalary:500.0]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（1）：语法基础]]></title>
    <url>%2F2017%2F08%2F20%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89%EF%BC%9A%E8%AF%AD%E6%B3%95%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[Java是完全面向对象的语言。通过虚拟机的运行机制，实现“跨平台”的理念。一次编译，永久使用。 先来看一个 HelloWorld.java 程序。这个程序在屏幕上打印出一串字符”Hello World!”: 1234567public class HelloWorld&#123; public static void main(String[] args) &#123; System.out.println("Hello World!"); &#125;&#125; 程序中包括Java的一些基本特征： 类(class)：上面程序定义了一个 类 HelloWorld，该类的名字与.java文件的名字相同。 方法(method)：类的内部定义了该类的一个 方法 main。 语句(statement)：真正的“打印”功能由一个语句实现，即: System.out.println(“Hello World!”); 下面两点有关Java的书写方式： Java中的语句要以 ; 结尾 (与C/C++相同)。 用花括号 {} 来整合语句，形成程序块。通过程序块，我们可以知道程序的不同部分的范围，比如类从哪里开始，到哪里结束。 一、编译与运行Java程序要经过编译器编译才能执行。在Linux或Mac下，可以下载安装 Java JDK 使用 javac 来编译。在命令行中输入下面语句 编译 : 1$javac HelloWorld.java 当前路径下，将有一个名为HelloWorld.class的文件生成。使用 java 命令来 运行 。 1$java HelloWorld Java会搜寻该类中的main方法，并执行。 二、变量计算机语言通常需要在内存中存放数据，比如C语言中的变量，Java也有类似的变量。Java和C语言都是静态类型的语言。在使用变量之前，要声明变量的类型。 变量(variable) 占据一定的内存空间。不同类型的变量占据不同的大小。Java中的变量类型如下： 变量类型 存储大小 例值 注释 byte 1byte 3 字节 int 4bytes 3 整数 short 2bytes 3 短整数 long 8bytes 3 长整数 float 4bytes 1.2 单精度浮点数 double 8bytes 1.2 双精度浮点数 char 2bytes ‘a’ 字符 boolean 1bit true 布尔值 在Java中，变量需要先 声明(declare)才能使用。在声明中，我说明变量的类型，赋予变量以特别名字，以便在后面的程序中调用它。你可以在程序中的任意位置声明变量。 比如: 12345678910public class Test&#123; public static void main(String[] args) &#123; System.out.println("Declare in the middle:"); int a; a = 5; System.out.println(a); // print an integer &#125;&#125; 上面a是变量名。可以在声明变量的同时，给变量赋值，比如 int a = 5; “变量”的概念实际上来自于面向过程的编程语言。在Java中，所谓的变量实际上是 “基本类型” (premitive type) 。我们将在类的讲解中更多深入。 上面的程序还可以看到，Java中，可用 // 引领注释。 二、数组Java中有 数组(array) 。数组包含相同类型的多个数据。我用下面方法来声明一个整数数组: 1int[] a; 在声明数组时，数组所需的空间并没有真正分配给数组。我可以在声明的同时，用new来创建数组所需空间: 1int[] a = new int[100]; 这里创建了可以容纳100个整数的数组。相应的内存分配也完成了。 我还可以在声明的同时，给数组赋值。数组的大小也同时确定。 1int[] a = new int[] &#123;1, 3, 5, 7, 9&#125;; 2.1 数组初始化你可以在声明数组的同时进行初始化（静态初始化），也可以在声明以后进行初始化（动态初始化）。例如： 12345678910// 静态初始化// 静态初始化的同时就为数组元素分配空间并赋值int intArray[] = &#123;1,2,3,4&#125;;String stringArray[] = &#123;&quot;程序员&quot;, &quot;http://www.baidu.com&quot;, &quot;一切编程语言都是纸老虎&quot;&#125;;// 动态初始化float floatArray[] = new float[3];floatArray[0] = 1.0f;floatArray[1] = 132.63f;floatArray[2] = 100F; 2.2 数组引用可以通过下标来引用数组： 1arrayName[index]; index从0开始。与C、C++不同，Java对数组元素要进行越界检查以保证安全性。 每个数组都有一个length属性来指明它的长度，例如 intArray.length 指明数组 intArray 的长度。 其他类型的数组与整数数组相似。 2.3 数组的遍历实际开发中，经常需要遍历数组以获取数组中的每一个元素。最容易想到的方法是for循环，例如： 1234int arrayDemo[] = &#123;1, 2, 4, 7, 9, 192, 100&#125;;for(int i=0,len=arrayDemo.length; i&lt;len; i++)&#123; System.out.println(arrayDemo[i] + &quot;, &quot;);&#125; 不过，Java提供了”增强版“的for循环，专门用来遍历数组，语法为： 123for( arrayType varName: arrayName )&#123; // Some Code&#125; arrayType 为数组类型（也是数组元素的类型）；varName 是用来保存当前元素的变量，每次循环它的值都会改变；arrayName 为数组名称。 每循环一次，就会获取数组中下一个元素的值，保存到 varName 变量，直到数组结束。即，第一次循环 varName 的值为第0个元素，第二次循环为第1个元素……例如： 1234int arrayDemo[] = &#123;1, 2, 4, 7, 9, 192, 100&#125;;for(int x: arrayDemo)&#123; System.out.println(x + &quot;, &quot;);&#125; 这种增强版的for循环也被称为”foreach循环“，它是普通for循环语句的特殊简化版。所有的foreach循环都可以被改写成for循环。 但是，如果你希望使用数组的索引，那么增强版的 for 循环无法做到。 2.4 二维数组二维数组的声明、初始化和引用与一维数组相似： 123456int intArray[ ][ ] = &#123; &#123;1,2&#125;, &#123;2,3&#125;, &#123;4,5&#125; &#125;;int a[ ][ ] = new int[2][3];a[0][0] = 12;a[0][1] = 34;// ......a[1][2] = 93; Java语言中，由于把二维数组看作是数组的数组，数组空间不是连续分配的，所以不要求二维数组每一维的大小相同。例如： 1234int intArray[ ][ ] = &#123; &#123;1,2&#125;, &#123;2,3&#125;, &#123;3,4,5&#125; &#125;;int a[ ][ ] = new int[2][ ];a[0] = new int[3];a[1] = new int[5]; 【示例】通过二维数组计算两个矩阵的乘积。 12345678910111213141516171819202122232425262728293031public class Demo &#123; public static void main(String[] args)&#123; // 第一个矩阵（动态初始化一个二维数组） int a[][] = new int[2][3]; // 第二个矩阵（静态初始化一个二维数组） int b[][] = &#123; &#123;1,5,2,8&#125;, &#123;5,9,10,-3&#125;, &#123;2,7,-5,-18&#125; &#125;; // 结果矩阵 int c[][] = new int[2][4]; // 初始化第一个矩阵 for(int i=0; i&lt;2; i++) for(int j=0; j&lt;3 ;j++) a[i][j] = (i+1) * (j+2); // 计算矩阵乘积 for (int i=0; i&lt;2; i++)&#123; for (int j=0; j&lt;4; j++)&#123; c[i][j]=0; for(int k=0; k&lt;3; k++) c[i][j] += a[i][k] * b[k][j]; &#125; &#125; // 输出结算结果 for(int i=0; i&lt;2; i++)&#123; for (int j=0; j&lt;4; j++) System.out.printf("%-5d", c[i][j]); System.out.println(); &#125; &#125;&#125; 123运行结果：25 65 14 -6550 130 28 -130 几点说明 上面讲的是静态数组。静态数组一旦被声明，它的容量就固定了，不容改变。所以在声明数组时，一定要考虑数组的最大容量，防止容量不够的现象。 如果想在运行程序时改变容量，就需要用到数组列表(ArrayList，也称动态数组)或向量(Vector)。 正是由于静态数组容量固定的缺点，实际开发中使用频率不高，被 ArrayList 或 Vector 代替，因为实际开发中经常需要向数组中添加或删除元素，而它的容量不好预估。 三、表达式表达式 是变量、常量和运算符的组合，它表示一个数据。 1 + 1 是常见的表达式。再比如: 12345678910public class Test&#123; public static void main(String[] args) &#123; System.out.println("Declare in the middle:"); int a; a = 5 + 1; System.out.println(a); // print an integer &#125;&#125; 上面的5 + 1也是一个表达式，等于6。 3.1 数学表达式数学运算，结果为一个数值 1 + 2 加法 4 - 3.4 减法 7 * 1.5 乘法 3.5 / 7 除法 7 % 2 求余数 3.2 关系表达式判断表达式是否成立。即一个boolean值，真假 a &gt; 4.2 大于 3.4 &gt;= b 大于等于 1.5 &lt; 9 小于 6 &lt;= 1 小于等于 2 == 2 等于 2 != 2 不等于 3.3 布林表达式两个boolean值的与、或、非的逻辑关系 true &amp;&amp; false and (3 &gt; 1) (2 == 1) or ! true not 3.4 位运算对整数的二进制形式逐位进行逻辑运算，得到一个整数 &amp; and or ^ xor ~ not 5 &lt;&lt; 3 0b101 left shift 3 bits 6 &gt;&gt; 1 0b110 right shift 1 bit 还有下列在C中常见的运算符，我会在用到的时候进一步解释: m ++ 变量m加1 n — 变量n减1 condition ? x1 : x2 condition为一个boolean值。根据condition，取x1或x2的值 四、控制结构Java中控制结构(control flow)的语法与C类似。它们都使用{}来表达隶属关系。 4.1 选择（if）condition是一个表示真假值的表达式。statements;是语句。 123456789101112if (conditon1) &#123; statements; ...&#125;else if (condition2) &#123; statements; ...&#125;else &#123; statements; ...&#125; 4.2 循环（while）12345while (condition) &#123;statements;&#125; 4.2 循环（do…while）123do &#123;statements;&#125;while(condition); // 注意结尾的; 4.3 循环（for）123for (initial; condition; update) &#123;statements;&#125; 4.4 跳出或跳出循环在循环中，可以使用 12break; // 跳出循环continue; // 直接进入下一循环 4.5 选择（switch）123456789101112switch(expression) &#123;case 1: statements; break; case 2: statements; break; ...default: statements; break; &#125; 五、字符串从表面上看，字符串就是双引号之间的数据，例如“java”等。在Java中，可以使用下面的方法定义字符串：String stringName = “string content”; 1String webName = &quot;java学习&quot;; 字符串可以通过“+”连接，基本数据类型与字符串进行“+”操作一般也会自动转换为字符串，例如： 12345678910public class Demo &#123; public static void main(String[] args)&#123; String stuName = "小明"; int stuAge = 17; float stuScore = 92.5f; String info = stuName + "的年龄是 " + stuAge + "，成绩是 " + stuScore; System.out.println(info); &#125;&#125; String字符串与数组有一个共同点，就是它们被初始化后，长度是不变的，并且内容也不变。如果要改变它的值，就会产生一个新的字符串，如下所示： 12String str = &quot;Hello &quot;;str += &quot;World!&quot;; 这个赋值表达式看起来有点像简单的接龙，在str后面直接加上一个“World!”字符串，形成最后的字符串“Hello World!”。其运行原理是这样的：程序首先产生了str1字符串，并在内存中申请了一段空间。此时要追加新的字符串是不可能的，因为字符串被初始化后，长度是固定的。如果要改变它，只有放弃原来的空间，重新申请能够容纳“Hello World!”字符串的内存空间，然后将“Hello World!”字符串放到内存中。 实际上，String 是java.lang包下的一个类，按照标准的面向对象的语法，其格式应该为： 1String stringName = new String(&quot;string content&quot;); 例如 1String url = new String(&quot;http://www.baidu.com&quot;); 但是由于String特别常用，所以Java提供了一种简化的语法。 使用简化语法的另外一个原因是，按照标准的面向对象的语法，在内存使用上存在比较大的浪费。例如String str = new String(“abc”);实际上创建了两个String对象，一个是”abc”对象，存储在常量空间中，一个是使用new关键字为对象str申请的空间。 5.1 常用的String对象方法5.1.2 length()方法length() 返回字符串的长度，例如： 1234String str1 = &quot;微学苑&quot;;String str2 = &quot;weixueyuan&quot;;System.out.println(&quot;The lenght of str1 is &quot; + str1.length());System.out.println(&quot;The lenght of str2 is &quot; + str2.length()); 输出结果为： 12The lenght of str1 is 3The lenght of str2 is 10 5.1.2 charAt()方法charAt() 方法的作用是按照索引值获得字符串中的指定字符。Java规定，字符串中第一个字符的索引值是0，第二个字符的索引值是1，依次类推。例如： 12String str = &quot;123456789&quot;;System.out.println(str.charAt(0) + &quot; &quot; + str.charAt(5) + &quot; &quot; + str.charAt(8)) 输出结果为： 11 6 9 5.1.3 contain()方法contains() 方法用来检测字符串是否包含某个子串，例如： 12String str = &quot;baidu&quot;;System.out.println(str.contains(&quot;bai&quot;)); 输出结果： 1true 5.1.4 replace()方法字符串替换，用来替换字符串中所有指定的子串，例如： 1234ring str1 = &quot;The url of baidu is www.google.com!&quot;;String str2 = str1.replace(&quot;baidu&quot;, &quot;google&quot;);System.out.println(str1);System.out.println(str2); 输出结果： 12The url of baidu is www.google.com!The url of google is www.google.com! 注意：replace() 方法不会改变原来的字符串，而是生成一个新的字符串。 5.1.5 split()方法以指定字符串作为分隔符，对当前字符串进行分割，分割的结果是一个数组，例如： 12345678import java.util.*;public class Demo &#123; public static void main(String[] args)&#123; String str = &quot;wei_xue_yuan_is_good&quot;; String strArr[] = str.split(&quot;_&quot;); System.out.println(Arrays.toString(strArr)); &#125;&#125; 运行结果： 1[wei, xue, yuan, is, good] 以上仅仅列举了几个常用的String对象的方法，更多方法和详细解释请参考API文档。 5.2 StringBuffer与StringBuiderString 的值是不可变的，每次对String的操作都会生成新的String对象，不仅效率低，而且耗费大量内存空间。 StringBuffer类和String类一样，也用来表示字符串，但是StringBuffer的内部实现方式和String不同，在进行字符串处理时，不生成新的对象，在内存使用上要优于String。 StringBuffer 默认分配16字节长度的缓冲区，当字符串超过该大小时，会自动增加缓冲区长度，而不是生成新的对象。 StringBuffer不像String，只能通过 new 来创建对象，不支持简写方式，例如： 1234StringBuffer str1 = new StringBuffer(); // 分配16个字节长度的缓冲区StringBuffer str2 = =new StringBuffer(512); // 分配512个字节长度的缓冲区// 在缓冲区中存放了字符串，并在后面预留了16个字节长度的空缓冲区StringBuffer str3 = new StringBuffer(&quot;www.baidu.com&quot;); 5.2.1 StringBuffer类的主要方法StringBuffer类中的方法主要偏重于对于字符串的操作，例如追加、插入和删除等，这个也是StringBuffer类和String类的主要区别。实际开发中，如果需要对一个字符串进行频繁的修改，建议使用 StringBuffer。 1) append() 方法 append() 方法用于向当前字符串的末尾追加内容，类似于字符串的连接。调用该方法以后，StringBuffer对象的内容也发生改变，例如： 12StringBuffer str = new StringBuffer(“biancheng100”);str.append(true); 则对象str的值将变成”biancheng100true”。注意是str指向的内容变了，不是str的指向变了。 字符串的”+“操作实际上也是先创建一个StringBuffer对象，然后调用append()方法将字符串片段拼接起来，最后调用toString()方法转换为字符串。 这样看来，String的连接操作就比StringBuffer多出了一些附加操作，效率上必然会打折扣。 但是，对于长度较小的字符串，”+“操作更加直观，更具可读性，有些时候可以稍微牺牲一下效率。 2) deleteCharAt() deleteCharAt() 方法用来删除指定位置的字符，并将剩余的字符形成新的字符串。例如： 12StringBuffer str = new StringBuffer(&quot;abcdef&quot;);str. deleteCharAt(3); 该代码将会删除索引值为3的字符，即”d“字符。 你也可以通过delete()方法一次性删除多个字符，例如： 12StingBuffer str = new StringBuffer(&quot;abcdef&quot;);str.delete(1, 4); 该代码会删除索引值为1~4之间的字符，包括索引值1，但不包括4。 3) insert()方法 insert() 用来在指定位置插入字符串，可以认为是append()的升级版。例如： 12StringBuffer str = new StringBuffer(&quot;abcdef&quot;);str.insert(3, &quot;xyz&quot;); 最后str所指向的字符串为 abcdxyzef。 4) setCharAt() 方法 setCharAt() 方法用来修改指定位置的字符。例如： 12StringBuffer str = new StringBuffer(&quot;abcdef&quot;);str.setCharAt(3, &apos;z&apos;); 该代码将把索引值为3的字符修改为 z，最后str所指向的字符串为 abczef。 以上仅仅是部分常用方法的简单说明，更多方法和解释请查阅API文档。 5.2.2 String和StringBuffer的效率对比为了更加明显地看出它们的执行效率，下面的代码，将26个英文字母加了10000次。 123456789101112131415161718192021222324public class Demo &#123; public static void main(String[] args)&#123; String fragment = "abcdefghijklmnopqrstuvwxyz"; int times = 10000; // 通过String对象 long timeStart1 = System.currentTimeMillis(); String str1 = ""; for (int i=0; i&lt;times; i++) &#123; str1 += fragment; &#125; long timeEnd1 = System.currentTimeMillis(); System.out.println("String: " + (timeEnd1 - timeStart1) + "ms"); // 通过StringBuffer long timeStart2 = System.currentTimeMillis(); StringBuffer str2 = new StringBuffer(); for (int i=0; i&lt;times; i++) &#123; str2.append(fragment); &#125; long timeEnd2 = System.currentTimeMillis(); System.out.println("StringBuffer: " + (timeEnd2 - timeStart2) + "ms"); &#125;&#125; 运行结果： 12String: 5287msStringBuffer: 3ms 结论很明显，StringBuffer的执行效率比String快上千倍，这个差异随着叠加次数的增加越来越明显，当叠加次数达到30000次的时候，运行结果为： 12String: 35923msStringBuffer: 8ms 所以，强烈建议在涉及大量字符串操作时使用StringBuffer。 5.2.3 StringBuilder类StringBuilder类和StringBuffer类功能基本相似，方法也差不多，主要区别在于StringBuffer类的方法是多线程安全的，而StringBuilder不是线程安全的，相比而言，StringBuilder类会略微快一点。 StringBuffer、StringBuilder、String中都实现了CharSequence接口。CharSequence是一个定义字符串操作的接口，它只包括length()、charAt(int index)、subSequence(int start, int end) 这几个API。 StringBuffer、StringBuilder、String对CharSequence接口的实现过程不一样。String直接实现了CharSequence接口；StringBuilder 和 StringBuffer都是可变的字符序列，它们都继承于AbstractStringBuilder，实现了CharSequence接口。 总结一下： 线程安全： StringBuffer：线程安全 StringBuilder：线程不安全 速度： 一般情况下，速度从快到慢为 StringBuilder &gt; StringBuffer &gt; String，当然这是相对的，不是绝对的。 使用环境： 操作少量的数据使用 String； 单线程操作大量数据使用 StringBuilder； 多线程操作大量数据使用 StringBuffer。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（11）：Java HashMap源码全剖析]]></title>
    <url>%2F2017%2F08%2F19%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%8811%EF%BC%89%EF%BC%9AJava%20HashMap%E6%BA%90%E7%A0%81%E5%85%A8%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[HashMap简介 HashMap是基于哈希表实现的，每一个元素都是一个key-value对，其内部通过单链表解决冲突问题，容量不足（超过了阈值）时，同样会自动增长。 HashMap是非线程安全的，只是用于单线程环境下，多线程环境下可以采用concurrent并发包下的concurrentHashMap。 HashMap实现了Serializable接口，因此它支持序列化，实现了Cloneable接口，能被克隆。 一、HashMap源码剖析HashMap的源码如下（加入了比较详细的注释）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754package java.util; import java.io.*; public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; // 默认的初始容量（容量为HashMap中槽的数目）是16，且实际容量必须是2的整数次幂。 static final int DEFAULT_INITIAL_CAPACITY = 16; // 最大容量（必须是2的幂且小于2的30次方，传入容量过大将被这个值替换） static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 默认加载因子为0.75 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 存储数据的Entry数组，长度是2的幂。 // HashMap采用链表法解决冲突，每一个Entry本质上是一个单向链表 transient Entry[] table; // HashMap的底层数组中已用槽的数量 transient int size; // HashMap的阈值，用于判断是否需要调整HashMap的容量（threshold = 容量*加载因子） int threshold; // 加载因子实际大小 final float loadFactor; // HashMap被改变的次数 transient volatile int modCount; // 指定“容量大小”和“加载因子”的构造函数 public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); // HashMap的最大容量只能是MAXIMUM_CAPACITY if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //加载因此不能小于0 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); // 找出“大于initialCapacity”的最小的2的幂 int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; // 设置“加载因子” this.loadFactor = loadFactor; // 设置“HashMap阈值”，当HashMap中存储数据的数量达到threshold时，就需要将HashMap的容量加倍。 threshold = (int)(capacity * loadFactor); // 创建Entry数组，用来保存数据 table = new Entry[capacity]; init(); &#125; // 指定“容量大小”的构造函数 public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; // 默认构造函数。 public HashMap() &#123; // 设置“加载因子”为默认加载因子0.75 this.loadFactor = DEFAULT_LOAD_FACTOR; // 设置“HashMap阈值”，当HashMap中存储数据的数量达到threshold时，就需要将HashMap的容量加倍。 threshold = (int)(DEFAULT_INITIAL_CAPACITY * DEFAULT_LOAD_FACTOR); // 创建Entry数组，用来保存数据 table = new Entry[DEFAULT_INITIAL_CAPACITY]; init(); &#125; // 包含“子Map”的构造函数 public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR); // 将m中的全部元素逐个添加到HashMap中 putAllForCreate(m); &#125; //求hash值的方法，重新计算hash值 static int hash(int h) &#123; h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); &#125; // 返回h在数组中的索引值，这里用&amp;代替取模，旨在提升效率 // h &amp; (length-1)保证返回值的小于length static int indexFor(int h, int length) &#123; return h &amp; (length-1); &#125; public int size() &#123; return size; &#125; public boolean isEmpty() &#123; return size == 0; &#125; // 获取key对应的value public V get(Object key) &#123; if (key == null) return getForNullKey(); // 获取key的hash值 int hash = hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; //判断key是否相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125; //没找到则返回null return null; &#125; // 获取“key为null”的元素的值 // HashMap将“key为null”的元素存储在table[0]位置，但不一定是该链表的第一个位置！ private V getForNullKey() &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) return e.value; &#125; return null; &#125; // HashMap是否包含key public boolean containsKey(Object key) &#123; return getEntry(key) != null; &#125; // 返回“键为key”的键值对 final Entry&lt;K,V&gt; getEntry(Object key) &#123; // 获取哈希值 // HashMap将“key为null”的元素存储在table[0]位置，“key不为null”的则调用hash()计算哈希值 int hash = (key == null) ? 0 : hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; return null; &#125; // 将“key-value”添加到HashMap中 public V put(K key, V value) &#123; // 若“key为null”，则将该键值对添加到table[0]中。 if (key == null) return putForNullKey(value); // 若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中。 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; // 若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出！ if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 若“该key”对应的键值对不存在，则将“key-value”添加到table中 modCount++; //将key-value添加到table[i]处 addEntry(hash, key, value, i); return null; &#125; // putForNullKey()的作用是将“key为null”键值对添加到table[0]位置 private V putForNullKey(V value) &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 如果没有存在key为null的键值对，则直接题阿见到table[0]处! modCount++; addEntry(0, null, value, 0); return null; &#125; // 创建HashMap对应的“添加方法”， // 它和put()不同。putForCreate()是内部方法，它被构造函数等调用，用来创建HashMap // 而put()是对外提供的往HashMap中添加元素的方法。 private void putForCreate(K key, V value) &#123; int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); // 若该HashMap表中存在“键值等于key”的元素，则替换该元素的value值 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; e.value = value; return; &#125; &#125; // 若该HashMap表中不存在“键值等于key”的元素，则将该key-value添加到HashMap中 createEntry(hash, key, value, i); &#125; // 将“m”中的全部元素都添加到HashMap中。 // 该方法被内部的构造HashMap的方法所调用。 private void putAllForCreate(Map&lt;? extends K, ? extends V&gt; m) &#123; // 利用迭代器将元素逐个添加到HashMap中 for (Iterator&lt;? extends Map.Entry&lt;? extends K, ? extends V&gt;&gt; i = m.entrySet().iterator(); i.hasNext(); ) &#123; Map.Entry&lt;? extends K, ? extends V&gt; e = i.next(); putForCreate(e.getKey(), e.getValue()); &#125; &#125; // 重新调整HashMap的大小，newCapacity是调整后的容量 void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; //如果就容量已经达到了最大值，则不能再扩容，直接返回 if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 新建一个HashMap，将“旧HashMap”的全部元素添加到“新HashMap”中， // 然后，将“新HashMap”赋值给“旧HashMap”。 Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); &#125; // 将HashMap中的全部元素都添加到newTable中 void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125; &#125; // 将&quot;m&quot;的全部元素都添加到HashMap中 public void putAll(Map&lt;? extends K, ? extends V&gt; m) &#123; // 有效性判断 int numKeysToBeAdded = m.size(); if (numKeysToBeAdded == 0) return; // 计算容量是否足够， // 若“当前阀值容量 &lt; 需要的容量”，则将容量x2。 if (numKeysToBeAdded &gt; threshold) &#123; int targetCapacity = (int)(numKeysToBeAdded / loadFactor + 1); if (targetCapacity &gt; MAXIMUM_CAPACITY) targetCapacity = MAXIMUM_CAPACITY; int newCapacity = table.length; while (newCapacity &lt; targetCapacity) newCapacity &lt;&lt;= 1; if (newCapacity &gt; table.length) resize(newCapacity); &#125; // 通过迭代器，将“m”中的元素逐个添加到HashMap中。 for (Iterator&lt;? extends Map.Entry&lt;? extends K, ? extends V&gt;&gt; i = m.entrySet().iterator(); i.hasNext(); ) &#123; Map.Entry&lt;? extends K, ? extends V&gt; e = i.next(); put(e.getKey(), e.getValue()); &#125; &#125; // 删除“键为key”元素 public V remove(Object key) &#123; Entry&lt;K,V&gt; e = removeEntryForKey(key); return (e == null ? null : e.value); &#125; // 删除“键为key”的元素 final Entry&lt;K,V&gt; removeEntryForKey(Object key) &#123; // 获取哈希值。若key为null，则哈希值为0；否则调用hash()进行计算 int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; e = prev; // 删除链表中“键为key”的元素 // 本质是“删除单向链表中的节点” while (e != null) &#123; Entry&lt;K,V&gt; next = e.next; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; modCount++; size--; if (prev == e) table[i] = next; else prev.next = next; e.recordRemoval(this); return e; &#125; prev = e; e = next; &#125; return e; &#125; // 删除“键值对” final Entry&lt;K,V&gt; removeMapping(Object o) &#123; if (!(o instanceof Map.Entry)) return null; Map.Entry&lt;K,V&gt; entry = (Map.Entry&lt;K,V&gt;) o; Object key = entry.getKey(); int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; e = prev; // 删除链表中的“键值对e” // 本质是“删除单向链表中的节点” while (e != null) &#123; Entry&lt;K,V&gt; next = e.next; if (e.hash == hash &amp;&amp; e.equals(entry)) &#123; modCount++; size--; if (prev == e) table[i] = next; else prev.next = next; e.recordRemoval(this); return e; &#125; prev = e; e = next; &#125; return e; &#125; // 清空HashMap，将所有的元素设为null public void clear() &#123; modCount++; Entry[] tab = table; for (int i = 0; i &lt; tab.length; i++) tab[i] = null; size = 0; &#125; // 是否包含“值为value”的元素 public boolean containsValue(Object value) &#123; // 若“value为null”，则调用containsNullValue()查找 if (value == null) return containsNullValue(); // 若“value不为null”，则查找HashMap中是否有值为value的节点。 Entry[] tab = table; for (int i = 0; i &lt; tab.length ; i++) for (Entry e = tab[i] ; e != null ; e = e.next) if (value.equals(e.value)) return true; return false; &#125; // 是否包含null值 private boolean containsNullValue() &#123; Entry[] tab = table; for (int i = 0; i &lt; tab.length ; i++) for (Entry e = tab[i] ; e != null ; e = e.next) if (e.value == null) return true; return false; &#125; // 克隆一个HashMap，并返回Object对象 public Object clone() &#123; HashMap&lt;K,V&gt; result = null; try &#123; result = (HashMap&lt;K,V&gt;)super.clone(); &#125; catch (CloneNotSupportedException e) &#123; // assert false; &#125; result.table = new Entry[table.length]; result.entrySet = null; result.modCount = 0; result.size = 0; result.init(); // 调用putAllForCreate()将全部元素添加到HashMap中 result.putAllForCreate(this); return result; &#125; // Entry是单向链表。 // 它是 “HashMap链式存储法”对应的链表。 // 它实现了Map.Entry 接口，即实现getKey(), getValue(), setValue(V value), equals(Object o), hashCode()这些函数 static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; // 指向下一个节点 Entry&lt;K,V&gt; next; final int hash; // 构造函数。 // 输入参数包括&quot;哈希值(h)&quot;, &quot;键(k)&quot;, &quot;值(v)&quot;, &quot;下一节点(n)&quot; Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; // 判断两个Entry是否相等 // 若两个Entry的“key”和“value”都相等，则返回true。 // 否则，返回false public final boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) &#123; Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; &#125; return false; &#125; // 实现hashCode() public final int hashCode() &#123; return (key==null ? 0 : key.hashCode()) ^ (value==null ? 0 : value.hashCode()); &#125; public final String toString() &#123; return getKey() + &quot;=&quot; + getValue(); &#125; // 当向HashMap中添加元素时，绘调用recordAccess()。 // 这里不做任何处理 void recordAccess(HashMap&lt;K,V&gt; m) &#123; &#125; // 当从HashMap中删除元素时，绘调用recordRemoval()。 // 这里不做任何处理 void recordRemoval(HashMap&lt;K,V&gt; m) &#123; &#125; &#125; // 新增Entry。将“key-value”插入指定位置，bucketIndex是位置索引。 void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); // 若HashMap的实际大小 不小于 “阈值”，则调整HashMap的大小 if (size++ &gt;= threshold) resize(2 * table.length); &#125; // 创建Entry。将“key-value”插入指定位置。 void createEntry(int hash, K key, V value, int bucketIndex) &#123; // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); size++; &#125; // HashIterator是HashMap迭代器的抽象出来的父类，实现了公共了函数。 // 它包含“key迭代器(KeyIterator)”、“Value迭代器(ValueIterator)”和“Entry迭代器(EntryIterator)”3个子类。 private abstract class HashIterator&lt;E&gt; implements Iterator&lt;E&gt; &#123; // 下一个元素 Entry&lt;K,V&gt; next; // expectedModCount用于实现fast-fail机制。 int expectedModCount; // 当前索引 int index; // 当前元素 Entry&lt;K,V&gt; current; HashIterator() &#123; expectedModCount = modCount; if (size &gt; 0) &#123; // advance to first entry Entry[] t = table; // 将next指向table中第一个不为null的元素。 // 这里利用了index的初始值为0，从0开始依次向后遍历，直到找到不为null的元素就退出循环。 while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) ; &#125; &#125; public final boolean hasNext() &#123; return next != null; &#125; // 获取下一个元素 final Entry&lt;K,V&gt; nextEntry() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); Entry&lt;K,V&gt; e = next; if (e == null) throw new NoSuchElementException(); // 注意！！！ // 一个Entry就是一个单向链表 // 若该Entry的下一个节点不为空，就将next指向下一个节点; // 否则，将next指向下一个链表(也是下一个Entry)的不为null的节点。 if ((next = e.next) == null) &#123; Entry[] t = table; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) ; &#125; current = e; return e; &#125; // 删除当前元素 public void remove() &#123; if (current == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); Object k = current.key; current = null; HashMap.this.removeEntryForKey(k); expectedModCount = modCount; &#125; &#125; // value的迭代器 private final class ValueIterator extends HashIterator&lt;V&gt; &#123; public V next() &#123; return nextEntry().value; &#125; &#125; // key的迭代器 private final class KeyIterator extends HashIterator&lt;K&gt; &#123; public K next() &#123; return nextEntry().getKey(); &#125; &#125; // Entry的迭代器 private final class EntryIterator extends HashIterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public Map.Entry&lt;K,V&gt; next() &#123; return nextEntry(); &#125; &#125; // 返回一个“key迭代器” Iterator&lt;K&gt; newKeyIterator() &#123; return new KeyIterator(); &#125; // 返回一个“value迭代器” Iterator&lt;V&gt; newValueIterator() &#123; return new ValueIterator(); &#125; // 返回一个“entry迭代器” Iterator&lt;Map.Entry&lt;K,V&gt;&gt; newEntryIterator() &#123; return new EntryIterator(); &#125; // HashMap的Entry对应的集合 private transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet = null; // 返回“key的集合”，实际上返回一个“KeySet对象” public Set&lt;K&gt; keySet() &#123; Set&lt;K&gt; ks = keySet; return (ks != null ? ks : (keySet = new KeySet())); &#125; // Key对应的集合 // KeySet继承于AbstractSet，说明该集合中没有重复的Key。 private final class KeySet extends AbstractSet&lt;K&gt; &#123; public Iterator&lt;K&gt; iterator() &#123; return newKeyIterator(); &#125; public int size() &#123; return size; &#125; public boolean contains(Object o) &#123; return containsKey(o); &#125; public boolean remove(Object o) &#123; return HashMap.this.removeEntryForKey(o) != null; &#125; public void clear() &#123; HashMap.this.clear(); &#125; &#125; // 返回“value集合”，实际上返回的是一个Values对象 public Collection&lt;V&gt; values() &#123; Collection&lt;V&gt; vs = values; return (vs != null ? vs : (values = new Values())); &#125; // “value集合” // Values继承于AbstractCollection，不同于“KeySet继承于AbstractSet”， // Values中的元素能够重复。因为不同的key可以指向相同的value。 private final class Values extends AbstractCollection&lt;V&gt; &#123; public Iterator&lt;V&gt; iterator() &#123; return newValueIterator(); &#125; public int size() &#123; return size; &#125; public boolean contains(Object o) &#123; return containsValue(o); &#125; public void clear() &#123; HashMap.this.clear(); &#125; &#125; // 返回“HashMap的Entry集合” public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() &#123; return entrySet0(); &#125; // 返回“HashMap的Entry集合”，它实际是返回一个EntrySet对象 private Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet0() &#123; Set&lt;Map.Entry&lt;K,V&gt;&gt; es = entrySet; return es != null ? es : (entrySet = new EntrySet()); &#125; // EntrySet对应的集合 // EntrySet继承于AbstractSet，说明该集合中没有重复的EntrySet。 private final class EntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() &#123; return newEntryIterator(); &#125; public boolean contains(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;K,V&gt; e = (Map.Entry&lt;K,V&gt;) o; Entry&lt;K,V&gt; candidate = getEntry(e.getKey()); return candidate != null &amp;&amp; candidate.equals(e); &#125; public boolean remove(Object o) &#123; return removeMapping(o) != null; &#125; public int size() &#123; return size; &#125; public void clear() &#123; HashMap.this.clear(); &#125; &#125; // java.io.Serializable的写入函数 // 将HashMap的“总的容量，实际容量，所有的Entry”都写入到输出流中 private void writeObject(java.io.ObjectOutputStream s) throws IOException &#123; Iterator&lt;Map.Entry&lt;K,V&gt;&gt; i = (size &gt; 0) ? entrySet0().iterator() : null; // Write out the threshold, loadfactor, and any hidden stuff s.defaultWriteObject(); // Write out number of buckets s.writeInt(table.length); // Write out size (number of Mappings) s.writeInt(size); // Write out keys and values (alternating) if (i != null) &#123; while (i.hasNext()) &#123; Map.Entry&lt;K,V&gt; e = i.next(); s.writeObject(e.getKey()); s.writeObject(e.getValue()); &#125; &#125; &#125; private static final long serialVersionUID = 362498820763181265L; // java.io.Serializable的读取函数：根据写入方式读出 // 将HashMap的“总的容量，实际容量，所有的Entry”依次读出 private void readObject(java.io.ObjectInputStream s) throws IOException, ClassNotFoundException &#123; // Read in the threshold, loadfactor, and any hidden stuff s.defaultReadObject(); // Read in number of buckets and allocate the bucket array; int numBuckets = s.readInt(); table = new Entry[numBuckets]; init(); // Give subclass a chance to do its thing. // Read in size (number of Mappings) int size = s.readInt(); // Read the keys and values, and put the mappings in the HashMap for (int i=0; i&lt;size; i++) &#123; K key = (K) s.readObject(); V value = (V) s.readObject(); putForCreate(key, value); &#125; &#125; // 返回“HashMap总的容量” int capacity() &#123; return table.length; &#125; // 返回“HashMap的加载因子” float loadFactor() &#123; return loadFactor; &#125; &#125; 二、HashMap细节剖析2.1 存储结构 首先要清楚HashMap的存储结构，如下图所示： 图中，紫色部分即代表哈希表，也称为哈希数组，数组的每个元素都是一个单链表的头节点，链表是用来解决冲突的，如果不同的key映射到了数组的同一位置处，就将其放入单链表中。 2.2 链表节点的数据结构1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// Entry是单向链表。 // 它是 “HashMap链式存储法”对应的链表。 // 它实现了Map.Entry 接口，即实现getKey(), getValue(), setValue(V value), equals(Object o), hashCode()这些函数 static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; // 指向下一个节点 Entry&lt;K,V&gt; next; final int hash; // 构造函数。 // 输入参数包括&quot;哈希值(h)&quot;, &quot;键(k)&quot;, &quot;值(v)&quot;, &quot;下一节点(n)&quot; Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; // 判断两个Entry是否相等 // 若两个Entry的“key”和“value”都相等，则返回true。 // 否则，返回false public final boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) &#123; Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; &#125; return false; &#125; // 实现hashCode() public final int hashCode() &#123; return (key==null ? 0 : key.hashCode()) ^ (value==null ? 0 : value.hashCode()); &#125; public final String toString() &#123; return getKey() + &quot;=&quot; + getValue(); &#125; // 当向HashMap中添加元素时，绘调用recordAccess()。 // 这里不做任何处理 void recordAccess(HashMap&lt;K,V&gt; m) &#123; &#125; // 当从HashMap中删除元素时，会调用recordRemoval()。 // 这里不做任何处理 void recordRemoval(HashMap&lt;K,V&gt; m) &#123; &#125; &#125; 它的结构元素除了key、value、hash外，还有next，next指向下一个节点。另外，这里覆写了equals和hashCode方法来保证键值对的独一无二。 2.3 构造方法HashMap共有四个构造方法。构造方法中提到了两个很重要的参数：初始容量和加载因子。这两个参数是影响HashMap性能的重要参数，其中容量表示哈希表中槽的数量（即哈希数组的长度），初始容量是创建哈希表时的容量（从构造函数中可以看出，如果不指明，则默认为16），加载因子是哈希表在其容量自动增加之前可以达到多满的一种尺度，当哈希表中的条目数超出了加载因子与当前容量的乘积时，则要对该哈希表进行 resize 操作（即扩容）。 下面说下加载因子，如果加载因子越大，对空间的利用更充分，但是查找效率会降低（链表长度会越来越长）；如果加载因子太小，那么表中的数据将过于稀疏（很多空间还没用，就开始扩容了），对空间造成严重浪费。如果我们在构造方法中不指定，则系统默认加载因子为0.75，这是一个比较理想的值，一般情况下我们是无需修改的。 另外，无论我们指定的容量为多少，构造方法都会将实际容量设为不小于指定容量的2的次方的一个数，且最大值不能超过2的30次方 2.4 HashMap中key和value都允许为null。2.5 重点分析put和get要重点分析下HashMap中用的最多的两个方法put和get。先从比较简单的get方法着手，源码如下： 12345678910111213141516171819202122232425262728// 获取key对应的value public V get(Object key) &#123; if (key == null) return getForNullKey(); // 获取key的hash值 int hash = hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; /判断key是否相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125; 没找到则返回null return null; &#125; // 获取“key为null”的元素的值 // HashMap将“key为null”的元素存储在table[0]位置，但不一定是该链表的第一个位置！ private V getForNullKey() &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) return e.value; &#125; return null; &#125; 首先，如果key为null，则直接从哈希表的第一个位置table[0]对应的链表上查找。记住，key为null的键值对永远都放在以table[0]为头结点的链表中，当然不一定是存放在头结点table[0]中。 如果key不为null，则先求的key的hash值，根据hash值找到在table中的索引，在该索引对应的单链表中查找是否有键值对的key与目标key相等，有就返回对应的value，没有则返回null。 put方法稍微复杂些，代码如下： 12345678910111213141516171819202122232425 // 将“key-value”添加到HashMap中 public V put(K key, V value) &#123; // 若“key为null”，则将该键值对添加到table[0]中。 if (key == null) return putForNullKey(value); // 若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中。 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; // 若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出！ if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 若“该key”对应的键值对不存在，则将“key-value”添加到table中 modCount++; //将key-value添加到table[i]处 addEntry(hash, key, value, i); return null; &#125; 如果key为null，则将其添加到table[0]对应的链表中，putForNullKey的源码如下： 123456789101112131415// putForNullKey()的作用是将“key为null”键值对添加到table[0]位置 private V putForNullKey(V value) &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 如果没有存在key为null的键值对，则直接题阿见到table[0]处! modCount++; addEntry(0, null, value, 0); return null; &#125; 如果key不为null，则同样先求出key的hash值，根据hash值得出在table中的索引，而后遍历对应的单链表，如果单链表中存在与目标key相等的键值对，则将新的value覆盖旧的value，比将旧的value返回，如果找不到与目标key相等的键值对，或者该单链表为空，则将该键值对插入到改单链表的头结点位置（每次新插入的节点都是放在头结点的位置），该操作是有addEntry方法实现的，它的源码如下： 1234567891011// 新增Entry。将“key-value”插入指定位置，bucketIndex是位置索引。 void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); // 若HashMap的实际大小 不小于 “阈值”，则调整HashMap的大小 if (size++ &gt;= threshold) resize(2 * table.length); &#125; 注意这里倒数第三行的构造方法，将key-value键值对赋给table[bucketIndex]，并将其next指向元素e，这便将key-value放到了头结点中，并将之前的头结点接在了它的后面。该方法也说明，每次put键值对的时候，总是将新的该键值对放在table[bucketIndex]处（即头结点处）。 两外注意最后两行代码，每次加入键值对时，都要判断当前已用的槽的数目是否大于等于阀值（容量*加载因子），如果大于等于，则进行扩容，将容量扩为原来容量的2倍。 2.6 关于扩容上面我们看到了扩容的方法，resize方法，它的源码如下： 123456789101112131415161718// 重新调整HashMap的大小，newCapacity是调整后的单位 void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 新建一个HashMap，将“旧HashMap”的全部元素添加到“新HashMap”中， // 然后，将“新HashMap”赋值给“旧HashMap”。 Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); &#125; ``` 很明显，是新建了一个HashMap的底层数组，而后调用transfer方法，将就HashMap的全部元素添加到新的HashMap中（要重新计算元素在新的数组中的索引位置）。transfer方法的源码如下： // 将HashMap中的全部元素都添加到newTable中void transfer(Entry[] newTable) { Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) { Entry e = src[j]; if (e != null) { src[j] = null; do { Entry next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } while (e != null); } }}123456789101112很明显，扩容是一个相当耗时的操作，因为它需要重新计算这些元素在新的数组中的位置并进行复制处理。因此，我们在用HashMap的时，最好能提前预估下HashMap中元素的个数，这样有助于提高HashMap的性能。### 2..7 containsKey方法和containsValue方法。注意containsKey方法和containsValue方法。前者直接可以通过key的哈希值将搜索范围定位到指定索引对应的链表，而后者要对哈希数组的每个链表进行搜索。### 2.8 求hash值和索引值的方法我们重点来分析下求hash值和索引值的方法，这两个方法便是HashMap设计的最为核心的部分，二者结合能保证哈希表中的元素尽可能均匀地散列。计算哈希值的方法如下： static int hash(int h) { h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); } 123它只是一个数学公式，IDK这样设计对hash值的计算，自然有它的好处，至于为什么这样设计，我们这里不去追究，只要明白一点，用的位的操作使hash值的计算效率很高。由hash值找到对应索引的方法如下： static int indexFor(int h, int length) { return h &amp; (length-1); }```这个我们要重点说下，我们一般对哈希表的散列很自然地会想到用hash值对length取模（即除法散列法），Hashtable中也是这样实现的，这种方法基本能保证元素在哈希表中散列的比较均匀，但取模会用到除法运算，效率很低，HashMap中则通过h&amp;(length-1)的方法来代替取模，同样实现了均匀的散列，但效率要高很多，这也是HashMap对Hashtable的一个改进。 接下来，我们分析下为什么哈希表的容量一定要是2的整数次幂。首先，length为2的整数次幂的话，h&amp;(length-1)就相当于对length取模，这样便保证了散列的均匀，同时也提升了效率；其次，length为2的整数次幂的话，为偶数，这样length-1为奇数，奇数的最后一位是1，这样便保证了h&amp;(length-1)的最后一位可能为0，也可能为1（这取决于h的值），即与后的结果可能为偶数，也可能为奇数，这样便可以保证散列的均匀性，而如果length为奇数的话，很明显length-1为偶数，它的最后一位是0，这样h&amp;(length-1)的最后一位肯定为0，即只能为偶数，这样任何hash值都只会被散列到数组的偶数下标位置上，这便浪费了近一半的空间，因此，length取2的整数次幂，是为了使不同hash值发生碰撞的概率较小，这样就能使元素在哈希表中均匀地散列。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>数据结构</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（10）：hashCode方法与equal方法]]></title>
    <url>%2F2017%2F08%2F18%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%8810%EF%BC%89%EF%BC%9AhashCode%E6%96%B9%E6%B3%95%E4%B8%8Eequal%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[哈希表这个数据结构想必大多数人都不陌生，而且在很多地方都会利用到hash表来提高查找效率。在Java的Object类中有一个方法: 1public native int hashCode(); 根据这个方法的声明可知，该方法返回一个int类型的数值，并且是本地方法，因此在Object类中并没有给出具体的实现。 为何Object类需要这样一个方法？它有什么作用呢？今天我们就来具体探讨一下hashCode方法。 一、hashCode()方法的作用对于包含容器类型的程序设计语言来说，基本上都会涉及到hashCode。在Java中也一样，hashCode方法的主要作用是为了配合基于散列的集合一起正常运行，这样的散列集合包括HashSet、HashMap以及HashTable。 为什么这么说呢？考虑一种情况，当向集合中插入对象时，如何判别在集合中是否已经存在该对象了？（注意：集合中不允许重复的元素存在） 也许大多数人都会想到调用equals方法来逐个进行比较，这个方法确实可行。但是如果集合中已经存在一万条数据或者更多的数据，如果采用equals方法去逐一比较，效率必然是一个问题。此时hashCode方法的作用就体现出来了，当集合要添加新的对象时，先调用这个对象的hashCode方法，得到对应的hashcode值，实际上在HashMap的具体实现中会用一个table保存已经存进去的对象的hashcode值，如果table中没有该hashcode值，它就可以直接存进去，不用再进行任何比较了；如果存在该hashcode值， 就调用它的equals方法与新元素进行比较，相同的话就不存了，不相同就散列其它的地址，所以这里存在一个冲突解决的问题，这样一来实际调用equals方法的次数就大大降低了，说通俗一点：Java中的hashCode方法就是根据一定的规则将与对象相关的信息（比如对象的存储地址，对象的字段等）映射成一个数值，这个数值称作为散列值。下面这段代码是java.util.HashMap的中put方法的具体实现： 12345678910111213141516171819public V put(K key, V value) &#123; if (key == null) return putForNullKey(value); int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); return null; &#125; put方法是用来向HashMap中添加新的元素，从put方法的具体实现可知，会先调用hashCode方法得到该元素的hashCode值，然后查看table中是否存在该hashCode值，如果存在则调用equals方法重新确定是否存在该元素，如果存在，则更新value值，否则将新的元素添加到HashMap中。从这里可以看出，hashCode方法的存在是为了减少equals方法的调用次数，从而提高程序效率。 有些朋友误以为默认情况下，hashCode返回的就是对象的存储地址，事实上这种看法是不全面的，确实有些JVM在实现时是直接返回对象的存储地址，但是大多时候并不是这样，只能说可能存储地址有一定关联。下面是HotSpot JVM中生成hash散列值的实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445static inline intptr_t get_next_hash(Thread * Self, oop obj) &#123; intptr_t value = 0 ; if (hashCode == 0) &#123; // This form uses an unguarded global Park-Miller RNG, // so it&apos;s possible for two threads to race and generate the same RNG. // On MP system we&apos;ll have lots of RW access to a global, so the // mechanism induces lots of coherency traffic. value = os::random() ; &#125; else if (hashCode == 1) &#123; // This variation has the property of being stable (idempotent) // between STW operations. This can be useful in some of the 1-0 // synchronization schemes. intptr_t addrBits = intptr_t(obj) &gt;&gt; 3 ; value = addrBits ^ (addrBits &gt;&gt; 5) ^ GVars.stwRandom ; &#125; else if (hashCode == 2) &#123; value = 1 ; // for sensitivity testing &#125; else if (hashCode == 3) &#123; value = ++GVars.hcSequence ; &#125; else if (hashCode == 4) &#123; value = intptr_t(obj) ; &#125; else &#123; // Marsaglia&apos;s xor-shift scheme with thread-specific state // This is probably the best overall implementation -- we&apos;ll // likely make this the default in future releases. unsigned t = Self-&gt;_hashStateX ; t ^= (t &lt;&lt; 11) ; Self-&gt;_hashStateX = Self-&gt;_hashStateY ; Self-&gt;_hashStateY = Self-&gt;_hashStateZ ; Self-&gt;_hashStateZ = Self-&gt;_hashStateW ; unsigned v = Self-&gt;_hashStateW ; v = (v ^ (v &gt;&gt; 19)) ^ (t ^ (t &gt;&gt; 8)) ; Self-&gt;_hashStateW = v ; value = v ; &#125; value &amp;= markOopDesc::hash_mask; if (value == 0) value = 0xBAD ; assert (value != markOopDesc::no_hash, &quot;invariant&quot;) ; TEVENT (hashCode: GENERATE) ; return value;&#125; 因此有人会说，可以直接根据hashcode值判断两个对象是否相等吗？肯定是不可以的，因为不同的对象可能会生成相同的hashcode值。虽然不能根据hashcode值判断两个对象是否相等，但是可以直接根据hashcode值判断两个对象不等，如果两个对象的hashcode值不等，则必定是两个不同的对象。如果要判断两个对象是否真正相等，必须通过equals方法。也就是说对于两个对象: 如果调用equals方法得到的结果为true，则两个对象的hashcode值必定相等； 如果equals方法得到的结果为false，则两个对象的hashcode值不一定不同； 如果两个对象的hashcode值不等，则equals方法得到的结果必定为false； 如果两个对象的hashcode值相等，则equals方法得到的结果未知。 二、equal方法和hashCode方法在有些情况下，程序设计者在设计一个类的时候为需要重写equals方法，比如String类，但是千万要注意，在重写equals方法的同时，必须重写hashCode方法。为什么这么说呢？ 下面看一个例子： 12345678910111213141516171819202122232425262728293031323334353637import java.util.HashMap;import java.util.HashSet;import java.util.Set; class People&#123; private String name; private int age; public People(String name,int age) &#123; this.name = name; this.age = age; &#125; public void setAge(int age)&#123; this.age = age; &#125; @Override public boolean equals(Object obj) &#123; // TODO Auto-generated method stub return this.name.equals(((People)obj).name) &amp;&amp; this.age== ((People)obj).age; &#125;&#125; public class Demo &#123; public static void main(String[] args) &#123; People p1 = new People(&quot;Jack&quot;, 12); System.out.println(p1.hashCode()); HashMap&lt;People, Integer&gt; hashMap = new HashMap&lt;People, Integer&gt;(); hashMap.put(p1, 1); System.out.println(hashMap.get(new People(&quot;Jack&quot;, 12))); &#125;&#125; 在这里我只重写了equals方法，也就说如果两个People对象，如果它的姓名和年龄相等，则认为是同一个人。 这段代码本来的意愿是想这段代码输出结果为“1”，但是事实上它输出的是“null”。为什么呢？原因就在于重写equals方法的同时忘记重写hashCode方法。 虽然通过重写equals方法使得逻辑上姓名和年龄相同的两个对象被判定为相等的对象（跟String类类似），但是要知道默认情况下，hashCode方法是将对象的存储地址进行映射。那么上述代码的输出结果为“null”就不足为奇了。原因很简单，p1指向的对象和System.out.println(hashMap.get(new People(“Jack”, 12)));这句中的new People(“Jack”, 12)生成的是两个对象，它们的存储地址肯定不同。下面是HashMap的get方法的具体实现： 12345678910111213public V get(Object key) &#123; if (key == null) return getForNullKey(); int hash = hash(key.hashCode()); for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125; return null; &#125; 所以在hashmap进行get操作时，因为得到的hashCode值不同（注意，上述代码也许在某些情况下会得到相同的hashcode值，不过这种概率比较小，因为虽然两个对象的存储地址不同也有可能得到相同的hashcode值），所以导致在get方法中for循环不会执行，直接返回null。 因此如果想上述代码输出结果为“1”，很简单，只需要重写hashCode方法，让equals方法和hashCode方法始终在逻辑上保持一致性。 1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.HashMap;import java.util.HashSet;import java.util.Set; class People&#123; private String name; private int age; public People(String name,int age) &#123; this.name = name; this.age = age; &#125; public void setAge(int age)&#123; this.age = age; &#125; @Override public int hashCode() &#123; // TODO Auto-generated method stub return name.hashCode()+age; &#125; @Override public boolean equals(Object obj) &#123; // TODO Auto-generated method stub return this.name.equals(((People)obj).name) &amp;&amp; this.age== ((People)obj).age; &#125;&#125; public class Demo &#123; public static void main(String[] args) &#123; People p1 = new People(&quot;Jack&quot;, 12); System.out.println(p1.hashCode()); HashMap&lt;People, Integer&gt; hashMap = new HashMap&lt;People, Integer&gt;(); hashMap.put(p1, 1); System.out.println(hashMap.get(new People(&quot;Jack&quot;, 12))); &#125;&#125; 这样一来的话，输出结果就为“1”了。 下面这段话摘自Effective Java一书： 在程序执行期间，只要equals方法的比较操作用到的信息没有被修改，那么对这同一个对象调用多次，hashCode方法必须始终如一地返回同一个整数。 如果两个对象根据equals方法比较是相等的，那么调用两个对象的hashCode方法必须返回相同的整数结果。 如果两个对象根据equals方法比较是不等的，则hashCode方法不一定得返回不同的整数。 对于第二条和第三条很好理解，但是第一条，很多时候就会忽略。在《Java编程思想》一书中的P495页也有同第一条类似的一段话： “设计hashCode()时最重要的因素就是：无论何时，对同一个对象调用hashCode()都应该产生同样的值。如果在将一个对象用put()添加进HashMap时产生一个hashCdoe值，而用get()取出时却产生了另一个hashCode值，那么就无法获取该对象了。所以如果你的hashCode方法依赖于对象中易变的数据，用户就要当心了，因为此数据发生变化时，hashCode()方法就会生成一个不同的散列码”。 下面举个例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.util.HashMap;import java.util.HashSet;import java.util.Set; class People&#123; private String name; private int age; public People(String name,int age) &#123; this.name = name; this.age = age; &#125; public void setAge(int age)&#123; this.age = age; &#125; @Override public int hashCode() &#123; // TODO Auto-generated method stub return name.hashCode()*37+age; &#125; @Override public boolean equals(Object obj) &#123; // TODO Auto-generated method stub return this.name.equals(((People)obj).name) &amp;&amp; this.age== ((People)obj).age; &#125;&#125; public class Demo &#123; public static void main(String[] args) &#123; People p1 = new People(&quot;Jack&quot;, 12); System.out.println(p1.hashCode()); HashMap&lt;People, Integer&gt; hashMap = new HashMap&lt;People, Integer&gt;(); hashMap.put(p1, 2); p1.setAge(13); System.out.println(hashMap.get(p1)); &#125;&#125; 这段代码输出的结果为“null”，想必其中的原因大家应该都清楚了。 因此，在设计hashCode方法和equals方法的时候，如果对象中的数据易变，则最好在equals方法和hashCode方法中不要依赖于该字段。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java集合</tag>
        <tag>HashCode()</tag>
        <tag>equal()</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（9）：Java 集合对比]]></title>
    <url>%2F2017%2F08%2F17%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%889%EF%BC%89%EF%BC%9AJava%20%E9%9B%86%E5%90%88%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[一、HashMap与HashTable的区别HashMap和Hashtable的比较是Java面试中的常见问题，用来考验程序员是否能够正确使用集合类以及是否可以随机应变使用多种思路解决问题。HashMap的工作原理、ArrayList与Vector的比较以及这个问题是有关Java 集合框架的最经典的问题。 Hashtable是个过时的集合类，存在于Java API中很久了。在Java 4中被重写了，实现了Map接口，所以自此以后也成了Java集合框架中的一部分。Hashtable和HashMap在Java面试中相当容易被问到，甚至成为了集合框架面试题中最常被考的问题，所以在参加任何Java面试之前，都不要忘了准备这一题。 HashMap和Hashtable都实现了Map接口，但决定用哪一个之前先要弄清楚它们之间的分别。主要的区别有： HashMap HashTable 非线程安全（非线程同步） 线程安全（线程同步） 更适合于单线程 更适合于多线程 允许null值 不允许null值 迭代器Iterator是fail-fast迭代器 迭代器enumerator不是fail-fast的 初始容量为16 初始容量为11 两者最主要的区别在于Hashtable是线程安全，而HashMap则非线程安全 HashMap是非synchronized，而Hashtable是synchronized，这意味着Hashtable是线程安全的，多个线程可以共享一个Hashtable；而如果没有正确的同步的话，多个线程是不能共享HashMap的 由于Hashtable的实现方法里面都添加了synchronized关键字来确保线程同步，所以在单线程环境下它比HashMap要慢。如果你不需要同步，只需要单一线程，那么使用HashMap性能要好过Hashtable。仅在你需要完全的线程安全的时候使用Hashtable，而如果你使用Java 5或以上的话，请使用ConcurrentHashMap吧。 线程安全的实现原理：jvm有一个main memory，而每个线程有自己的working memory，一个线程对一个变量进行操作时，都要在自己的working memory里面建立一个copy，操作完之后再写入main memory。多个线程同时操作同一个变量，就可能会出现不可预知的结果。用synchronized的关键是建立一个镜像，这个镜像可以是要修改的变量也可以其他你认为合适的对象比如方法和类，然后通过给这个镜像加锁来实现线程安全，每个线程在获得这个锁之后，要执行完才会释放它得到的锁。这样就实现了所谓的线程安全。sychronized意味着在一次仅有一个线程能够更改Hashtable。就是说任何线程要更新Hashtable时要首先获得同步锁，其它线程要等到同步锁被释放之后才能再次获得同步锁更新Hashtable。 我们平时使用时若无特殊需求建议使用HashMap，在多线程环境下若使用HashMap需要使用Collections.synchronizedMap()方法来获取一个线程安全的集合（Collections.synchronizedMap()实现原理是Collections定义了一个SynchronizedMap的内部类，这个类实现了Map接口，在调用方法时使用synchronized来保证线程同步,当然了实际上操作的还是我们传入的HashMap实例，简单的说就是Collections.synchronizedMap()方法帮我们在操作HashMap时自动添加了synchronized来实现线程同步，类似的其它Collections.synchronizedXX方法也是类似原理） HashMap的迭代器Iterator是fail-fast迭代器，而Hashtable的enumerator迭代器不是fail-fast的。 当有其它线程改变了HashMap的结构（增加或者移除元素），将会抛出ConcurrentModificationException，但迭代器本身的remove()方法移除元素则不会抛出ConcurrentModificationException异常。但这并不是一个一定发生的行为，要看JVM。这条同样也是Enumeration和Iterator的区别。 Fail-safe和iterator迭代器相关。如果某个集合对象创建了Iterator或者ListIterator，然后其它的线程试图“结构上”更改集合对象，将会抛出ConcurrentModificationException异常。但其它线程可以通过set()方法更改集合对象是允许的，因为这并没有从“结构上”更改集合。但是假如已经从结构上进行了更改，再调用set()方法，将会抛出IllegalArgumentException异常。 结构上的更改指的是删除或者插入一个元素，这样会影响到map的结构。 HashMap可以使用null作为key，而Hashtable则不允许null作为key 虽说HashMap支持null值作为key，不过建议还是尽量避免这样使用，因为一旦不小心使用了，若因此引发一些问题，排查起来很是费事。HashMap以null作为key时，总是存储在table数组的第一个节点上 HashMap是对Map接口的实现，HashTable实现了Map接口和Dictionary抽象类 HashMap的初始容量为16，Hashtable初始容量为11，两者的填充因子默认都是0.75 HashMap扩容时是当前容量翻倍即:$capacity2$，Hashtable扩容时是容量翻倍+1即:$capacity2+1$ 两者计算hash的方法不同 Hashtable计算hash是直接使用key的hashcode对table数组的长度直接进行取模 12int hash = key.hashCode();int index = (hash &amp; 0x7FFFFFFF) % tab.length; HashMap计算hash对key的hashcode进行了二次hash，以获得更好的散列值，然后对table数组长度取摸 12345678static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;static int indexFor(int h, int n) &#123; return h &amp; (n-1); &#125; 二、HashSet与HashMap的区别HashMap和HashSet的区别是Java面试中最常被问到的问题。如果没有涉及到Collection框架以及多线程的面试，可以说是不完整。而Collection框架的问题不涉及到HashSet和HashMap，也可以说是不完整。HashMap和HashSet都是collection框架的一部分，它们让我们能够使用对象的集合。collection框架有自己的接口和实现，主要分为Set接口，List接口和Queue接口。它们有各自的特点，Set的集合里不允许对象有重复的值，List允许有重复，它对集合中的对象进行索引，Queue的工作原理是FCFS算法(First Come, First Serve)。 首先让我们来看看什么是HashMap和HashSet，然后再来比较它们之间的分别。 2.1 什么是HashSetHashSet实现了Set接口，它不允许集合中有重复的值，当我们提到HashSet时，第一件事情就是在将对象存储在HashSet之前，要先确保对象重写equals()和hashCode()方法，这样才能比较对象的值是否相等，以确保set中没有储存相等的对象。如果我们没有重写这两个方法，将会使用这个方法的默认实现。 public boolean add(Object o)方法用来在Set中添加元素，当元素值重复时则会立即返回false，如果成功添加的话会返回true。 HashSet不是key value结构，仅仅是存储不重复的元素，相当于简化版的HashMap，只是仅仅包含HashMap中的key而已。通过查看源码也证实了这一点，HashSet内部就是使用HashMap实现，只不过HashSet里面的HashMap所有的value都是同一个Object而已，因此HashSet也是非线程安全的，至于HashSet和Hashtable的区别，HashSet就是个简化的HashMap的。 下面是HashSet几个主要方法的实现，更具体的可以参考【Java学习手册：Java HashSet】 1234567891011121314151617181920212223private transient HashMap&lt;E,Object&gt; map; private static final Object PRESENT = new Object(); public HashSet() &#123; map = new HashMap&lt;E,Object&gt;(); &#125;public boolean contains(Object o) &#123; return map.containsKey(o); &#125;public boolean add(E e) &#123; return map.put(e, PRESENT)==null; &#125;public boolean add(E e) &#123; return map.put(e, PRESENT)==null; &#125;public boolean remove(Object o) &#123; return map.remove(o)==PRESENT; &#125;public void clear() &#123; map.clear(); &#125; 2.2 什么是HashMapHashMap实现了Map接口，Map接口对键值对进行映射。Map中不允许重复的键。Map接口有两个基本的实现，HashMap和TreeMap。TreeMap保存了对象的排列次序，而HashMap则不能。HashMap允许键和值为null。HashMap是非synchronized的，但collection框架提供方法能保证HashMap synchronized，这样多个线程同时访问HashMap时，能保证只有一个线程更改Map。 public Object put(Object Key,Object value)方法用来将元素添加到map中。 2.3 HashSet和HashMap的区别 HashMap HashSet HashMap实现了Map接口 HashSet实现了Set接口 HashMap储存键值对 HashSet仅仅存储对象 使用put()方法将元素放入map中 使用add()方法将元素放入set中 HashMap中使用键对象来计算hashcode值 HashSet使用成员对象来计算hashcode值，equals()方法判断对象相等性，不同返回false HashMap比较快，因为是使用唯一的键来获取对象 HashSet较HashMap来说比较慢 三、HashSet和TreeSet的区别 Hashset 的底层是由hashTable实现的，add()，remove()，contains()方法的时间复杂度是O(1).可以放入null，但只能放入一个null。Treeset 底层是由红黑树实现的,add()，remove()，contains()方法的时间复杂度是O(logn)。不允许放入null值 如果需要在Treeset 中插入对象，需要实现Comparable 接口，为其指定比较策略： 123456public class TreeSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements SortedSet&lt;E&gt;, Cloneable, java.io.Serializablepublic class HashSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable 其中SortedSet中组合了一个：Comparator&lt;? super E&gt; comparator(); HashSet是基于Hash算法实现的,其性能通常优于TreeSet,我们通常都应该使用HashSet,在我们需要排序的功能时,我门才使用TreeSet 四、ArrayList、LinkedList、Vector的底层实现和区别4.1 ArrayListArrayList是一个可以处理变长数组的类型，这里不局限于“数”组，ArrayList是一个泛型类，可以存放任意类型的对象。顾名思义，ArrayList是一个数组列表，因此其内部是使用一个数组来存放对象的，因为Object是一切类型的父类，因而ArrayList内部是有一个Object类型的数组类存放对象。ArrayList类常用的方法有add()、clear()、get()、indexOf()、remove()、sort()、toArray()、toString()等等，同时ArrayList内部有一个私有类实现Iterator接口，因此可以使用iterator()方法得到ArrayList的迭代器，同时，还有一个私有类实现了ListIterator接口，因此ArrayList也可以调用listIterator()方法得到ListIterator迭代器。 由于ArrayList是依靠数组来存放对象的，只不过封装起来了而已，因此其一些查找方法的效率都是O(n)，跟普通的数组效率差不多，只不过这个ArrayList是一个可变”数组“，并且可以存放一切指定的对象。 另外，由于ArrayList的所有方法都是默认在单一线程下进行的，因此ArrayList不具有线程安全性。若想在多线程下使用，应该使用Colletions类中的静态方法synchronizedList()对ArrayList进行调用即可。 4.2 LinkedListLinkedList可以看做为一个双向链表，所有的操作都可以认为是一个双向链表的操作，因为它实现了Deque接口和List接口。同样，LinkedList也是线程不安全的，如果在并发环境下使用它，同样用Colletions类中的静态方法synchronizedList()对LinkedList进行调用即可。 在LinkedList的内部实现中，并不是用普通的数组来存放数据的，而是使用结点来存放数据的，有一个指向链表头的结点first和一个指向链表尾的结点last。不同于ArrayList只能在数组末尾添加数据，LinkList可以很方便在链表头或者链表尾插入数据，或者在指定结点前后插入数据，还提供了取走链表头或链表尾的结点，或取走中间某个结点，还可以查询某个结点是否存在。add()方法默认在链表尾部插入数据。总之，LinkedList提供了大量方便的操作方法，并且它的插入或增加等方法的效率明显高于ArrayList类型，但是查询的效率要低一点，因为它是一个双向链表。 因此，LinkedList与ArrayList最大的区别是LinkedList更加灵活，并且部分方法的效率比ArrayList对应方法的效率要高很多，对于数据频繁出入的情况下，并且要求操作要足够灵活，建议使用LinkedList；对于数组变动不大，主要是用来查询的情况下，可以使用ArrayList。 4.3 VectorVector也是一个类似于ArrayList的可变长度的数组类型，它的内部也是使用数组来存放数据对象的。值得注意的是Vector与ArrayList唯一的区别是，Vector是线程安全的，即它的大部分方法都包含有关键字synchronized，因此，若对于单一线程的应用来说，最好使用ArrayList代替Vector，因为这样效率会快很多（类似的情况有StringBuffer与StringBuilder）；而在多线程程序中，为了保证数据的同步和一致性，可以使用Vector代替ArrayList实现同样的功能。 五、数组(Array)和列表(ArrayList)的区别 Array可以包含基本类型和对象类型，ArrayList只能包含对象类型。 Array大小是固定的，ArrayList的大小是动态变化的。 ArrayList提供了更多的方法和特性，比如：addAll()，removeAll()，iterator()等等。 ArrayList可以存任何Object，如String等。 ArrayList与数组的区别主要就是由于动态增容的效率问题了。对于基本类型数据，集合使用自动装箱来减少编码工作量。但是，当处理固定大小的基本数据类型的时候，这种方式相对比较慢。因此基本类型用Array，动态变化用ArrayList。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（8）：Java 集合框架]]></title>
    <url>%2F2017%2F08%2F16%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%888%EF%BC%89%EF%BC%9AJava%20%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[一、概念1.1 什么是集合？Java官方的入门文档是这样描述集合的： Collection(有时候也叫container)是一个简单的对象，它把多个元素组织成一个单元。集合可以用来存储、检索、操作、通信。通常情况下，集合代表了一个自然数据项，比如一组手牌(牌的集合)、邮件文件夹(邮件的集合)、电话目录(姓名到电话的映射)。如果你使用过Java或者其他语言，你应该很熟悉集合。 1.2 什么是集合框架？Collections Framework是一个用来表示和操作集合的统一的架构。集合的框架包括了： Interfaces:这些是表示集合的抽象数据类型，接口允许集合完成操作，独立与其详细的实现。在面向对象的语言中，接口构成了体系架构； Implementations:这些是接口的具体实现。本质上，是一些可复用的数据结构； Algorithms:这些方法可以对接口实现的对象进行有用的计算，比如搜索、排序。这些算法是具有多态性的：也就是说，同样的方法可以用在合适的接口的不同实现。本质上，是一些可复用的函数。 除了Java的集合框架，还有一些著名的集合框架的例子：比如C++的STL和Smalltalk的集合架构。从历史上来看，集合框架可能比较复杂，也可能有一些很陡峭的学习曲线。不过我们相信Java的集合框架会突破这样的传统，在这章你就可以自己学会。 1.3 使用集合框架有什么好处？Java的集合框架提供了一下优点： 减少编程的工作量：通过提供有用的数据结构和算法，集合框架能让你更专注的实现程序的核心功能，而不是去做一个底层的“管道工”。Java框架通过促进无关API的互操作性，使得你不用自己去实现不同API的适配 提高程序的速度与质量：集合框架提供了一些有用数据结构和算法的高性能、高质量的实现。每个接口的不同的实现也是可以互换的，所以程序可以通过切换集合来做一些调整。正因为你从实现数据结构的那些苦差事中脱离出来，你才可以有更多的实现去改善你自己程序的性能和质量 允许无关APIs的互操作：集合接口是API之间传递集合的一个“方言”，比如我的网络管理API有一个节点名的集合，而GUI工具需要一个列标题的集合，即使是分开实现它们，我们的APIs也可以无缝的接合。 省力地学习和使用新API：这是另一个领先的优势，设计者和实现者没必要在每次都重新设计API的时候都“推倒重来”地实现集合，而是直接使用标准的集合接口就好了。 促进软件的复用：符合标准集合接口的新数据结构本质上是可以复用的。对于操作这些新数据结构算法也是一样可以复用的。 二、集合框架Java集合工具包位于Java.util包下，包含了很多常用的数据结构，如数组、链表、栈、队列、集合、哈希表等。学习Java集合框架下大致可以分为如下五个部分：List列表、Set集合、Map映射、迭代器（Iterator、Enumeration）、工具类（Arrays、Collections）。 Java集合类的整体框架如下： 从上图中可以看出，集合类主要分为两大类：Collection和Map。 Collection是List、Set等集合高度抽象出来的接口，它包含了这些集合的基本操作，它主要又分为两大部分：List和Set。 List接口通常表示一个列表（数组、队列、链表、栈等），其中的元素可以重复，常用实现类为ArrayList和LinkedList，另外还有不常用的Vector。 另外，LinkedList还是实现了Queue接口，因此也可以作为队列使用。 Set接口通常表示一个集合，其中的元素不允许重复（通过hashcode和equals函数保证），常用实现类有HashSet和TreeSet，HashSet是通过Map中的HashMap实现的，而TreeSet是通过Map中的TreeMap实现的。另外，TreeSet还实现了SortedSet接口，因此是有序的集合（集合中的元素要实现Comparable接口，并覆写Compartor函数才行）。我们看到，抽象类AbstractCollection、AbstractList和AbstractSet分别实现了Collection、List和Set接口，这就是在Java集合框架中用的很多的适配器设计模式，用这些抽象类去实现接口，在抽象类中实现接口中的若干或全部方法，这样下面的一些类只需直接继承该抽象类，并实现自己需要的方法即可，而不用实现接口中的全部抽象方法。 Map是一个映射接口，其中的每个元素都是一个key-value键值对，同样抽象类AbstractMap通过适配器模式实现了Map接口中的大部分函数，TreeMap、HashMap、WeakHashMap等实现类都通过继承AbstractMap来实现，另外，不常用的HashTable直接实现了Map接口，它和Vector都是JDK1.0就引入的集合类。 Iterator是遍历集合的迭代器（不能遍历Map，只用来遍历Collection），Collection的实现类都实现了iterator()函数，它返回一个Iterator对象，用来遍历集合，ListIterator则专门用来遍历List。而Enumeration则是JDK1.0时引入的，作用与Iterator相同，但它的功能比Iterator要少，它只能再Hashtable、Vector和Stack中使用。 Arrays和Collections是用来操作数组、集合的两个工具类，例如在ArrayList和Vector中大量调用了Arrays.Copyof()方法，而Collections中有很多静态方法可以返回各集合类的synchronized版本，即线程安全的版本，当然了，如果要用线程安全的结合类，首选Concurrent并发包下的对应的集合类。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>LinkedList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（7）：Java LinkedList]]></title>
    <url>%2F2017%2F08%2F15%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%887%EF%BC%89%EF%BC%9AJava%20LinkedList%2F</url>
    <content type="text"><![CDATA[一、概述LinkedList和ArrayList一样，都实现了List接口，但其内部的数据结构有本质的不同。LinkedList是基于链表实现的（通过名字也能区分开来），所以它的插入和删除操作比ArrayList更加高效。但也是由于其为基于链表的，所以随机访问的效率要比ArrayList差。 看一下LinkedList的类的定义： 1234public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable&#123;&#125; LinkedList继承自AbstractSequenceList，实现了List、Deque、Cloneable、java.io.Serializable接口。AbstractSequenceList提供了List接口骨干性的实现以减少实现List接口的复杂度，Deque接口定义了双端队列的操作。 在LinkedList中除了本身自己的方法外，还提供了一些可以使其作为栈、队列或者双端队列的方法。这些方法可能彼此之间只是名字不同，以使得这些名字在特定的环境中显得更加合适。 1234LinkedList&lt;String&gt; list = new LinkedList&lt;String&gt;();list.add(&quot;语文: 1&quot;);list.add(&quot;数学: 2&quot;);list.add(&quot;英语: 3&quot;); 结构也相对简单一些，如下图所示： 二、数据结构LinkedList是基于链表结构实现，所以在类中包含了first和last两个指针(Node)。Node中包含了上一个节点和下一个节点的引用，这样就构成了双向的链表。每个Node只能知道自己的前一个节点和后一个节点，但对于链表来说，这已经足够了。 123456789101112131415transient int size = 0;transient Node&lt;E&gt; first; //链表的头指针transient Node&lt;E&gt; last; //尾指针//存储对象的结构 Node, LinkedList的内部类private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; // 指向下一个节点 Node&lt;E&gt; prev; //指向上一个节点 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; 三、存储3.1 add(E e)该方法是在链表的end添加元素，其调用了自己的方法linkLast(E e)。 该方法首先将last的Node引用指向了一个新的Node(l)，然后根据l新建了一个newNode，其中的元素就为要添加的e；而后，我们让last指向了newNode。接下来是自身进行维护该链表。 1234567891011121314151617181920212223242526/** * Appends the specified element to the end of this list. * * &lt;p&gt;This method is equivalent to &#123;@link #addLast&#125;. * * @param e element to be appended to this list * @return &#123;@code true&#125; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; linkLast(e); return true;&#125;/*** Links e as last element.*/void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; 3.2 add(int index, E element)该方法是在指定index位置插入元素。如果index位置正好等于size，则调用linkLast(element)将其插入末尾；否则调用 linkBefore(element, node(index))方法进行插入。该方法的实现在下面，大家可以自己仔细的分析一下。（分析链表的时候最好能够边画图边分析） 1234567891011121314151617181920212223242526272829303132/** * Inserts the specified element at the specified position in this list. * Shifts the element currently at that position (if any) and any * subsequent elements to the right (adds one to their indices). * * @param index index at which the specified element is to be inserted * @param element element to be inserted * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ public void add(int index, E element) &#123; checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index)); &#125; /** * Inserts element e before non-null Node succ. */ void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++; &#125; LinkedList的方法实在是太多，在这没法一一举例分析。但很多方法其实都只是在调用别的方法而已，所以建议大家将其几个最核心的添加的方法搞懂就可以了，比如linkBefore、linkLast。其本质也就是链表之间的删除添加等。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>LinkedList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（6）：Java ArrayList]]></title>
    <url>%2F2017%2F08%2F14%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%886%EF%BC%89%EF%BC%9AJava%20ArrayList%2F</url>
    <content type="text"><![CDATA[一、概述ArrayList可以理解为动态数组，就是Array的复杂版本。与Java中的数组相比，它的容量能动态增长。ArrayList是List接口的可变数组的实现。实现了所有可选列表操作，并允许包括 null 在内的所有元素。除了实现 List 接口外，此类还提供一些方法来操作内部用来存储列表的数组的大小。（此类大致上等同于 Vector 类，除了此类是不同步的。） 每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。 注意，此实现不是同步的。如果多个线程同时访问一个ArrayList实例，而其中至少一个线程从结构上修改了列表，那么它必须保持外部同步。（结构上的修改是指任何添加或删除一个或多个元素的操作，或者显式调整底层数组的大小；仅仅设置元素的值不是结构上的修改。） 我们先学习了解其内部的实现原理，才能更好的理解其应用。 二、ArrayList的实现对于ArrayList而言，它实现List接口、底层使用数组保存所有元素。其操作基本上是对数组的操作。下面我们来分析ArrayList的源代码： 2.1 List接口1234public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123;&#125; ArrayList继承了AbstractList，实现了List。它是一个数组队列，提供了相关的添加、删除、修改、遍历等功能。 ArrayList实现了RandmoAccess接口，即提供了随机访问功能。RandmoAccess是java中用来被List实现，为List提供快速访问功能的。在ArrayList中，我们即可以通过元素的序号快速获取元素对象；这就是快速随机访问。 ArrayList实现了Cloneable接口，即覆盖了函数clone()，能被克隆。 ArrayList实现java.io.Serializable接口，这意味着ArrayList支持序列化，能通过序列化去传输。 2.2 底层使用数组实现12345/*** The array buffer into which the elements of the ArrayList are stored.* The capacity of the ArrayList is the length of this array buffer.*/private transient Object[] elementData; 2.3 构造方法123456789101112131415161718192021222324252627282930313233343536/** * Constructs an empty list with an initial capacity of ten. */public ArrayList() &#123; this(10);&#125;/** * Constructs an empty list with the specified initial capacity. * * @param initialCapacity the initial capacity of the list * @throws IllegalArgumentException if the specified initial capacity * is negative */public ArrayList(int initialCapacity) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); this.elementData = new Object[initialCapacity];&#125;/** * Constructs a list containing the elements of the specified * collection, in the order they are returned by the collection's * iterator. * * @param c the collection whose elements are to be placed into this list * @throws NullPointerException if the specified collection is null */public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); size = elementData.length; // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class);&#125; ArrayList提供了三种方式的构造器： public ArrayList()：可以构造一个默认初始容量为10的空列表； public ArrayList(int initialCapacity)：构造一个指定初始容量的空列表； public ArrayList(Collection&lt;? extends E&gt; c)：构造一个包含指定collection的元素的列表，这些元素按照该collection的迭代器返回它们的顺序排列的。 2.4 存储ArrayList提供了set(int index, E element)、add(E e)、add(int index, E element)、addAll(Collection&lt;? extends E&gt; c)、addAll(int index, Collection&lt;? extends E&gt; c)这些添加元素的方法。下面我们一一讲解： set(int index, E element)：该方法首先调用rangeCheck(index)来校验index变量是否超出数组范围，超出则抛出异常。而后，取出原index位置的值，并且将新的element放入Index位置，返回oldValue。 123456789101112131415161718192021222324252627/** * Replaces the element at the specified position in this list with * the specified element. * * @param index index of the element to replace * @param element element to be stored at the specified position * @return the element previously at the specified position * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ // 用指定的元素替代此列表中指定位置上的元素，并返回以前位于该位置上的元素。public E set(int index, E element) &#123; rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125;/** * Checks if the given index is in range. If not, throws an appropriate * runtime exception. This method does *not* check if the index is * negative: It is always used immediately prior to an array access, * which throws an ArrayIndexOutOfBoundsException if index is negative. */ private void rangeCheck(int index) &#123; if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); &#125; add(E e)：该方法是将指定的元素添加到列表的尾部。当容量不足时，会调用grow方法增长容量。 1234567891011121314151617181920212223242526272829/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &lt;tt&gt;true&lt;/tt&gt; (as specified by &#123;@link Collection#add&#125;) */ // 将指定的元素添加到此列表的尾部。public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; add(int index, E element)：在index位置插入element。 1234567891011121314151617181920212223/** * Inserts the specified element at the specified position in this * list. Shifts the element currently at that position (if any) and * any subsequent elements to the right (adds one to their indices). * * @param index index at which the specified element is to be inserted * @param element element to be inserted * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ // 将指定的元素插入此列表中的指定位置。 // 如果当前位置有元素，则向右移动当前位于该位置的元素以及所有后续元素（将其索引加1）。public void add(int index, E element) &#123; rangeCheckForAdd(index); // 如果数组长度不足，将进行扩容。 ensureCapacityInternal(size + 1); // Increments modCount!! // 将 elementData中从Index位置开始、长度为size-index的元素， // 拷贝到从下标为index+1位置开始的新的elementData数组中。 // 即将当前位于该位置的元素以及所有后续元素右移一个位置。 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; addAll(Collection&lt;? extends E&gt; c)和addAll(int index, Collection&lt;? extends E&gt; c)：将特定Collection中的元素添加到Arraylist末尾。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Appends all of the elements in the specified collection to the end of * this list, in the order that they are returned by the * specified collection&apos;s Iterator. The behavior of this operation is * undefined if the specified collection is modified while the operation * is in progress. (This implies that the behavior of this call is * undefined if the specified collection is this list, and this * list is nonempty.) * * @param c collection containing elements to be added to this list * @return &lt;tt&gt;true&lt;/tt&gt; if this list changed as a result of the call * @throws NullPointerException if the specified collection is null */ // 按照指定collection的迭代器所返回的元素顺序，将该collection中的所有元素添加到此列表的尾部。 public boolean addAll(Collection&lt;? extends E&gt; c) &#123; Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0; &#125; /** * Inserts all of the elements in the specified collection into this * list, starting at the specified position. Shifts the element * currently at that position (if any) and any subsequent elements to * the right (increases their indices). The new elements will appear * in the list in the order that they are returned by the * specified collection&apos;s iterator. * * @param index index at which to insert the first element from the * specified collection * @param c collection containing elements to be added to this list * @return &lt;tt&gt;true&lt;/tt&gt; if this list changed as a result of the call * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; * @throws NullPointerException if the specified collection is null */ // 从指定的位置开始，将指定collection中的所有元素插入到此列表中。 public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; rangeCheckForAdd(index); Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount int numMoved = size - index; if (numMoved &gt; 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0; &#125; 在ArrayList的存储方法，其核心本质是在数组的某个位置将元素添加进入。但其中又会涉及到关于数组容量不够而增长等因素。 2.5 读取这个方法就比较简单了，ArrayList能够支持随机访问的原因也是很显然的，因为它内部的数据结构是数组，而数组本身就是支持随机访问。该方法首先会判断输入的index值是否越界，然后将数组的index位置的元素返回即可。 12345678910111213141516/*** Returns the element at the specified position in this list.** @param index index of the element to return* @return the element at the specified position in this list* @throws IndexOutOfBoundsException &#123;@inheritDoc&#125;*/// 返回此列表中指定位置上的元素。public E get(int index) &#123; rangeCheck(index); return (E) elementData[index];&#125;private void rangeCheck(int index) &#123; if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125; 2.6 删除ArrayList提供了根据下标或者指定对象两种方式的删除功能。需要注意的是该方法的返回值并不相同，如下： 根据下标删除： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// 移除此列表中指定位置上的元素。 /** * Removes the element at the specified position in this list. * Shifts any subsequent elements to the left (subtracts one from their * indices). * * @param index the index of the element to be removed * @return the element that was removed from the list * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // Let gc do its work return oldValue; &#125;``` - 指定对象删除：```java/** * Removes the first occurrence of the specified element from this list, * if it is present. If the list does not contain the element, it is * unchanged. More formally, removes the element with the lowest index * &lt;tt&gt;i&lt;/tt&gt; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;get(i)==null&amp;nbsp;:&amp;nbsp;o.equals(get(i)))&lt;/tt&gt; * (if such an element exists). Returns &lt;tt&gt;true&lt;/tt&gt; if this list * contained the specified element (or equivalently, if this list * changed as a result of the call). * * @param o element to be removed from this list, if present * @return &lt;tt&gt;true&lt;/tt&gt; if this list contained the specified element */ // 移除此列表中首次出现的指定元素（如果存在）。这是应为ArrayList中允许存放重复的元素。 public boolean remove(Object o) &#123; // 由于ArrayList中允许存放null，因此下面通过两种情况来分别处理。 if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; // 类似remove(int index)，移除列表中指定位置上的元素。 fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false; &#125; 注意：从数组中移除元素的操作，也会导致被移除的元素以后的所有元素的向左移动一个位置。 2.7 调整数组容量从上面介绍的向ArrayList中存储元素的代码中，我们看到，每当向数组中添加元素时，都要去检查添加后元素的个数是否会超出当前数组的长度，如果超出，数组将会进行扩容，以满足添加数据的需求。数组扩容有两个方法，其中开发者可以通过一个public的方法ensureCapacity(int minCapacity)来增加ArrayList的容量，而在存储元素等操作过程中，如果遇到容量不足，会调用priavte方法private void ensureCapacityInternal(int minCapacity)实现。 12345678910111213141516171819202122232425262728public void ensureCapacity(int minCapacity) &#123; if (minCapacity &gt; 0) ensureCapacityInternal(minCapacity);&#125;private void ensureCapacityInternal(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;/** * Increases the capacity to ensure that it can hold at least the * number of elements specified by the minimum capacity argument. * * @param minCapacity the desired minimum capacity */private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 从上述代码中可以看出，数组进行扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量的增长大约是其原容量的1.5倍（从int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1)这行代码得出）。这种操作的代价是很高的，因此在实际使用时，我们应该尽量避免数组容量的扩张。当我们可预知要保存的元素的多少时，要在构造ArrayList实例时，就指定其容量，以避免数组扩容的发生。或者根据实际需求，通过调用ensureCapacity方法来手动增加ArrayList实例的容量。 ArrayList还给我们提供了将底层数组的容量调整为当前列表保存的实际元素的大小的功能。它可以通过trimToSize方法来实现。代码如下： 1234567public void trimToSize() &#123; modCount++; int oldCapacity = elementData.length; if (size &lt; oldCapacity) &#123; elementData = Arrays.copyOf(elementData, size); &#125; &#125; 2.8 Fail-Fast机制ArrayList也采用了快速失败的机制，通过记录modCount参数来实现。在面对并发的修改时，迭代器很快就会完全失败，而不是冒着在将来某个不确定时间发生任意不确定行为的风险。关于Fail-Fast的更详细的介绍，在之前HashMap中已经提到。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>ArrayList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（5）：Java LinkedHashSet]]></title>
    <url>%2F2017%2F08%2F13%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%885%EF%BC%89%EF%BC%9AJava%20LinkedHashSet%2F</url>
    <content type="text"><![CDATA[一、概述首先我们需要知道的是它是一个Set的实现，所以它其中存的肯定不是键值对，而是值。此实现与HashSet的不同之处在于，LinkedHashSet维护着一个运行于所有条目的双重链接列表。此链接列表定义了迭代顺序，该迭代顺序可为插入顺序或是访问顺序。 看到上面的介绍，是不是感觉其与HashMap和LinkedHashMap的关系很像？ 注意，此实现不是同步的。如果多个线程同时访问链接的哈希Set，而其中至少一个线程修改了该Set，则它必须保持外部同步。 在【Java学习手册：LinkedHashMap】中，通过例子演示了HashMap和LinkedHashMap的区别。举一反三，我们现在学习的LinkedHashSet与之前的很相同，只不过之前存的是键值对，而现在存的只有值。 LinkedHashSet是可以按照插入顺序或者访问顺序进行迭代。 二、LinkedHashSet的实现对于LinkedHashSet而言，它继承与HashSet、又基于LinkedHashMap来实现的。LinkedHashSet底层使用LinkedHashMap来保存所有元素，它继承与HashSet，其所有的方法操作上又与HashSet相同，因此LinkedHashSet的实现上非常简单，只提供了四个构造方法，并通过传递一个标识参数，调用父类的构造器，底层构造一个LinkedHashMap来实现，在相关操作上与父类HashSet的操作相同，直接调用父类HashSet的方法即可。LinkedHashSet的源代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable &#123; private static final long serialVersionUID = -2851667679971038690L; /** * 构造一个带有指定初始容量和加载因子的新空链接哈希set。 * * 底层会调用父类的构造方法，构造一个有指定初始容量和加载因子的LinkedHashMap实例。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 */ public LinkedHashSet(int initialCapacity, float loadFactor) &#123; super(initialCapacity, loadFactor, true); &#125; /** * 构造一个带指定初始容量和默认加载因子0.75的新空链接哈希set。 * * 底层会调用父类的构造方法，构造一个带指定初始容量和默认加载因子0.75的LinkedHashMap实例。 * @param initialCapacity 初始容量。 */ public LinkedHashSet(int initialCapacity) &#123; super(initialCapacity, .75f, true); &#125; /** * 构造一个带默认初始容量16和加载因子0.75的新空链接哈希set。 * * 底层会调用父类的构造方法，构造一个带默认初始容量16和加载因子0.75的LinkedHashMap实例。 */ public LinkedHashSet() &#123; super(16, .75f, true); &#125; /** * 构造一个与指定collection中的元素相同的新链接哈希set。 * * 底层会调用父类的构造方法，构造一个足以包含指定collection * 中所有元素的初始容量和加载因子为0.75的LinkedHashMap实例。 * @param c 其中的元素将存放在此set中的collection。 */ public LinkedHashSet(Collection&lt;? extends E&gt; c) &#123; super(Math.max(2*c.size(), 11), .75f, true); addAll(c); &#125;&#125; 以上几乎就是LinkedHashSet的全部代码了，那么读者可能就会怀疑了，不是说LinkedHashSet是基于LinkedHashMap实现的吗？那我为什么在源码中甚至都没有看到出现过LinkedHashMap。不要着急，我们可以看到在LinkedHashSet的构造方法中，其调用了父类的构造方法。我们可以进去看一下： 123456789101112/** * 以指定的initialCapacity和loadFactor构造一个新的空链接哈希集合。 * 此构造函数为包访问权限，不对外公开，实际只是是对LinkedHashSet的支持。 * * 实际底层会以指定的参数构造一个空LinkedHashMap实例来实现。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 * @param dummy 标记。 */HashSet(int initialCapacity, float loadFactor, boolean dummy) &#123; map = new LinkedHashMap&lt;E,Object&gt;(initialCapacity, loadFactor);&#125; 在父类HashSet中，专为LinkedHashSet提供的构造方法如下，该方法为包访问权限，并未对外公开。由上述源代码可见，LinkedHashSet通过继承HashSet，底层使用LinkedHashMap，以很简单明了的方式来实现了其自身的所有功能。 三、总结以上就是关于LinkedHashSet的内容，我们只是从概述上以及构造方法这几个方面介绍了，并不是我们不想去深入其读取或者写入方法，而是其本身没有实现，只是继承于父类HashSet的方法。 所以我们需要注意的点是： LinkedHashSet是Set的一个具体实现，其维护着一个运行于所有条目的双重链接列表。此链接列表定义了迭代顺序，该迭代顺序可为插入顺序或是访问顺序。 LinkedHashSet继承与HashSet，并且其内部是通过LinkedHashMap来实现的。有点类似于我们之前说的LinkedHashMap其内部是基于Hashmap实现一样，不过还是有一点点区别的（具体的区别大家可以自己去思考一下）。 如果我们需要迭代的顺序为插入顺序或者访问顺序，那么LinkedHashSet是需要你首先考虑的。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>LinkedHashSet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（4）：Java LinkedHashMap]]></title>
    <url>%2F2017%2F08%2F12%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%884%EF%BC%89%EF%BC%9AJava%20LinkedHashMap%2F</url>
    <content type="text"><![CDATA[一、概述HashMap是无序的，HashMap在put的时候是根据key的hashcode进行hash然后放入对应的地方。所以在按照一定顺序put进HashMap中，然后遍历出HashMap的顺序跟put的顺序不同（除非在put的时候key已经按照hashcode排序好了，这种几率非常小） JAVA在JDK1.4以后提供了LinkedHashMap来帮助我们实现了有序的HashMap。 LinkedHashMap是HashMap的一个子类，它保留插入的顺序， 如果需要输出的顺序和输入时的相同，那么就选用LinkedHashMap。 LinkedHashMap是Map接口的哈希表和链接列表实现，具有可预知的迭代顺序。此实现提供所有可选的映射操作，并允许使用null值和null键。此类不保证映射的顺序，特别是它不保证该顺序恒久不变。 LinkedHashMap实现与HashMap的不同之处在于，LinkedHashMap维护着一个运行于所有条目的双重链接列表。此链接列表定义了迭代顺序，该迭代顺序可以是插入顺序或者是访问顺序。 注意，此实现不是同步的。如果多个线程同时访问链接的哈希映射，而其中至少一个线程从结构上修改了该映射，则它必须保持外部同步。 根据链表中元素的顺序可以分为：按插入顺序的链表和按访问顺序(调用get方法)的链表。默认是按插入顺序排序，如果指定按访问顺序排序，那么调用get方法后，会将这次访问的元素移至链表尾部，不断访问可以形成按访问顺序排序的链表。 我们写一个简单的LinkedHashMap的程序： 123456789101112LinkedHashMap&lt;String, Integer&gt; lmap = new LinkedHashMap&lt;String, Integer&gt;();lmap.put("语文", 1);lmap.put("数学", 2);lmap.put("英语", 3);lmap.put("历史", 4);lmap.put("政治", 5);lmap.put("地理", 6);lmap.put("生物", 7);lmap.put("化学", 8);for(Entry&lt;String, Integer&gt; entry : lmap.entrySet()) &#123; System.out.println(entry.getKey() + ": " + entry.getValue());&#125; 运行结果是： 12345678语文: 1数学: 2英语: 3历史: 4政治: 5地理: 6生物: 7化学: 8 我们可以观察到，和HashMap的运行结果不同，LinkedHashMap的迭代输出的结果保持了插入顺序。是什么样的结构使得LinkedHashMap具有如此特性呢？我们还是一样的看看LinkedHashMap的内部结构，对它有一个感性的认识： Hash table and linked list implementation of the Map interface, with predictable iteration order. This implementation differs from HashMap in that it maintains a doubly-linked list running through all of its entries. This linked list defines the iteration ordering, which is normally the order in which keys were inserted into the map (insertion-order). 没错，正如官方文档所说：LinkedHashMap是Hash表和链表的实现，并且依靠着双向链表保证了迭代顺序是插入的顺序。 二、插入顺序、访问顺序的演示先做几个demo来演示一下LinkedHashMap的使用。看懂了其效果，然后再来研究其原理。 2.1 HashMap看下面这个代码： 12345678910111213public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put(&quot;apple&quot;, &quot;苹果&quot;); map.put(&quot;watermelon&quot;, &quot;西瓜&quot;); map.put(&quot;banana&quot;, &quot;香蕉&quot;); map.put(&quot;peach&quot;, &quot;桃子&quot;); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); System.out.println(entry.getKey() + &quot;=&quot; + entry.getValue()); &#125;&#125; 一个比较简单的测试HashMap的代码，通过控制台的输出，我们可以看到HashMap是没有顺序的。 1234banana=香蕉apple=苹果peach=桃子watermelon=西瓜 2.2 LinkedHashMap我们现在将map的实现换成LinkedHashMap，其他代码不变：Map&lt;String, String&gt; map = new LinkedHashMap&lt;String, String&gt;();看一下控制台的输出： 1234apple=苹果watermelon=西瓜banana=香蕉peach=桃子 我们可以看到，其输出顺序是完成按照插入顺序的！也就是我们上面所说的保留了插入的顺序。我们不是在上面还提到过其可以按照访问顺序进行排序么？好的，我们还是通过一个例子来验证一下： 12345678910111213141516public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new LinkedHashMap&lt;String, String&gt;(16,0.75f,true); map.put(&quot;apple&quot;, &quot;苹果&quot;); map.put(&quot;watermelon&quot;, &quot;西瓜&quot;); map.put(&quot;banana&quot;, &quot;香蕉&quot;); map.put(&quot;peach&quot;, &quot;桃子&quot;); map.get(&quot;banana&quot;); map.get(&quot;apple&quot;); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); System.out.println(entry.getKey() + &quot;=&quot; + entry.getValue()); &#125;&#125; 代码与之前的都差不多，但我们多了两行代码，并且初始化LinkedHashMap的时候，用的构造函数也不相同，看一下控制台的输出结果： 1234watermelon=西瓜peach=桃子banana=香蕉apple=苹果 这也就是我们之前提到过的，LinkedHashMap可以选择按照访问顺序进行排序。 三、LinkedHashMap的实现对于LinkedHashMap而言，它继承于HashMap、底层使用哈希表与双向链表来保存所有元素。其基本操作与父类HashMap相似，它通过重写父类相关的方法，来实现自己的链接列表特性。下面我们来分析LinkedHashMap的源代码： 3.1 成员变量LinkedHashMap采用的hash算法和HashMap相同，但是它重新定义了数组中保存的元素Entry，该Entry除了保存当前对象的引用外，还保存了其上一个元素before和下一个元素after的引用，从而在哈希表的基础上又构成了双向链接列表。看源代码： 12345678910111213141516171819/*** The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt;* for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order.* 如果为true，则按照访问顺序；如果为false，则按照插入顺序。*/private final boolean accessOrder;/*** 双向链表的表头元素。 */private transient Entry&lt;K,V&gt; header;/*** LinkedHashMap的Entry元素。* 继承HashMap的Entry元素，又保存了其上一个元素before和下一个元素after的引用。 */private static class Entry&lt;K,V&gt; extends HashMap.Entry&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; ……&#125; LinkedHashMap中的Entry集成与HashMap的Entry，但是其增加了before和after的引用，指的是上一个元素和下一个元素的引用。 3.2 初始化通过源代码可以看出，在LinkedHashMap的构造方法中，实际调用了父类HashMap的相关构造方法来构造一个底层存放的table数组，但额外可以增加accessOrder这个参数，如果不设置，默认为false，代表按照插入顺序进行迭代；当然可以显式设置为true，代表以访问顺序进行迭代。如： 1234public LinkedHashMap(int initialCapacity, float loadFactor,boolean accessOrder) &#123; super(initialCapacity, loadFactor); this.accessOrder = accessOrder;&#125; HashMap中的相关构造方法： 1234567891011121314151617181920public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); // Find a power of 2 &gt;= initialCapacity int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; this.loadFactor = loadFactor; threshold = (int)(capacity * loadFactor); table = new Entry[capacity]; init(); &#125; 我们已经知道LinkedHashMap的Entry元素继承HashMap的Entry，提供了双向链表的功能。在上述HashMap的构造器中，最后会调用init()方法，进行相关的初始化，这个方法在HashMap的实现中并无意义，只是提供给子类实现相关的初始化调用。 LinkedHashMap重写了init()方法，在调用父类的构造方法完成构造后，进一步实现了对其元素Entry的初始化操作。 12345678910/*** Called by superclass constructors and pseudoconstructors (clone,* readObject) before any entries are inserted into the map. Initializes* the chain.*/@Overridevoid init() &#123; header = new Entry&lt;&gt;(-1, null, null, null); header.before = header.after = header;&#125; 3.3 存储LinkedHashMap并未重写父类HashMap的put方法，而是重写了父类HashMap的put方法调用的子方法void recordAccess(HashMap m) ，void addEntry(int hash, K key, V value, int bucketIndex) 和void createEntry(int hash, K key, V value, int bucketIndex)，提供了自己特有的双向链接列表的实现。我们在之前的文章中已经讲解了HashMap的put方法，我们在这里重新贴一下HashMap的put方法的源代码： 12345678910111213141516171819public V put(K key, V value) &#123; if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); return null;&#125; 重写方法： 1234567891011121314151617181920212223242526272829303132333435363738void recordAccess(HashMap&lt;K,V&gt; m) &#123; LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m; if (lm.accessOrder) &#123; lm.modCount++; remove(); addBefore(lm.header); &#125;&#125;void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 调用create方法，将新元素以双向链表的的形式加入到映射中。 createEntry(hash, key, value, bucketIndex); // 删除最近最少使用元素的策略定义 Entry&lt;K,V&gt; eldest = header.after; if (removeEldestEntry(eldest)) &#123; removeEntryForKey(eldest.key); &#125; else &#123; if (size &gt;= threshold) resize(2 * table.length); &#125;&#125;void createEntry(int hash, K key, V value, int bucketIndex) &#123; HashMap.Entry&lt;K,V&gt; old = table[bucketIndex]; Entry&lt;K,V&gt; e = new Entry&lt;K,V&gt;(hash, key, value, old); table[bucketIndex] = e; // 调用元素的addBrefore方法，将元素加入到哈希、双向链接列表。 e.addBefore(header); size++;&#125;private void addBefore(Entry&lt;K,V&gt; existingEntry) &#123; after = existingEntry; before = existingEntry.before; before.after = this; after.before = this;&#125; 3.4 读取LinkedHashMap重写了父类HashMap的get方法，实际在调用父类getEntry()方法取得查找的元素后，再判断当排序模式accessOrder为true时，记录访问顺序，将最新访问的元素添加到双向链表的表头，并从原来的位置删除。由于的链表的增加、删除操作是常量级的，故并不会带来性能的损失。 12345678910111213141516171819202122232425262728293031323334public V get(Object key) &#123; // 调用父类HashMap的getEntry()方法，取得要查找的元素。 Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)getEntry(key); if (e == null) return null; // 记录访问顺序。 e.recordAccess(this); return e.value;&#125;void recordAccess(HashMap&lt;K,V&gt; m) &#123; LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m; // 如果定义了LinkedHashMap的迭代顺序为访问顺序， // 则删除以前位置上的元素，并将最新访问的元素添加到链表表头。 if (lm.accessOrder) &#123; lm.modCount++; remove(); addBefore(lm.header); &#125;&#125;/*** Removes this entry from the linked list.*/private void remove() &#123; before.after = after; after.before = before;&#125;/**clear链表，设置header为初始状态*/public void clear() &#123; super.clear(); header.before = header.after = header;&#125; LinkedHashMap的这些额外操作基本上都是为了维护好那个具有访问顺序的双向链表，目的就是保持双向链表中节点的顺序要从eldest到youngest。 3.4 排序模式LinkedHashMap定义了排序模式accessOrder，该属性为boolean型变量，对于访问顺序，为true；对于插入顺序，则为false。 1private final boolean accessOrder; 一般情况下，不必指定排序模式，其迭代顺序即为默认为插入顺序。看LinkedHashMap的构造方法，如： 1234public LinkedHashMap(int initialCapacity, float loadFactor) &#123; super(initialCapacity, loadFactor); accessOrder = false; &#125; 这些构造方法都会默认指定排序模式为插入顺序。如果你想构造一个LinkedHashMap，并打算按从近期访问最少到近期访问最多的顺序（即访问顺序）来保存元素，那么请使用下面的构造方法构造LinkedHashMap： 123456public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) &#123; super(initialCapacity, loadFactor); this.accessOrder = accessOrder; &#125; 该哈希映射的迭代顺序就是最后访问其条目的顺序，这种映射很适合构建LRU缓存。LinkedHashMap提供了removeEldestEntry(Map.Entry eldest)方法，在将新条目插入到映射后，put和 putAll将调用此方法。该方法可以提供在每次添加新条目时移除最旧条目的实现程序，默认返回false，这样，此映射的行为将类似于正常映射，即永远不能移除最旧的元素。 123protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; return false; &#125; 此方法通常不以任何方式修改映射，相反允许映射在其返回值的指引下进行自我修改。如果用此映射构建LRU缓存，则非常方便，它允许映射通过删除旧条目来减少内存损耗。 例如：重写此方法，维持此映射只保存100个条目的稳定状态，在每次添加新条目时删除最旧的条目。 1234private static final int MAX_ENTRIES = 100; protected boolean removeEldestEntry(Map.Entry eldest) &#123; return size() &gt; MAX_ENTRIES; &#125; 四、总结其实LinkedHashMap几乎和HashMap一样：从技术上来说，不同的是它定义了一个Entry header，这个header不是放在Table里，它是额外独立出来的。LinkedHashMap通过继承hashMap中的Entry,并添加两个属性Entry before,after,和header结合起来组成一个双向链表，来实现按插入顺序或访问顺序排序。 在写关于LinkedHashMap的过程中，记起来之前面试的过程中遇到的一个问题，也是问我Map的哪种实现可以做到按照插入顺序进行迭代？当时脑子是突然短路的，但现在想想，也只能怪自己对这个知识点还是掌握的不够扎实，所以又从头认真的把代码看了一遍。 不过，我的建议是，大家首先首先需要记住的是：LinkedHashMap能够做到按照插入顺序或者访问顺序进行迭代，这样在我们以后的开发中遇到相似的问题，才能想到用LinkedHashMap来解决，否则就算对其内部结构非常了解，不去使用也是没有什么用的。我们学习的目的是为了更好的应用。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>LinkedHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（3）：Java HashTable]]></title>
    <url>%2F2017%2F08%2F11%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%883%EF%BC%89%EF%BC%9AJava%20HashTable%2F</url>
    <content type="text"><![CDATA[一、概述和HashMap一样，Hashtable也是一个散列表，它存储的内容是键值对。 Hashtable在Java中的定义为： 123public class Hashtable&lt;K,V&gt; extends Dictionary&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, java.io.Serializable&#123;&#125; 从源码中，我们可以看出，Hashtable继承于Dictionary类，实现了Map, Cloneable, java.io.Serializable接口。其中Dictionary类是任何可将键映射到相应值的类（如 Hashtable）的抽象父类，每个键和值都是对象（源码注释为：The Dictionary class is the abstract parent of any class, such as Hashtable, which maps keys to values. Every key and every value is an object.）。但其Dictionary源码注释是这样的：NOTE: This class is obsolete. New implementations should implement the Map interface, rather than extending this class. 该话指出Dictionary这个类过时了，新的实现类应该实现Map接口。 二、成员变量Hashtable是通过”拉链法”实现的哈希表。它包括几个重要的成员变量：table, count, threshold, loadFactor, modCount。 table是一个Entry[]数组类型，而Entry（在HashMap中有讲解过）实际上就是一个单向链表。哈希表的”key-value键值对”都是存储在Entry数组中的。 count是Hashtable的大小，它是Hashtable保存的键值对的数量。 threshold是Hashtable的阈值，用于判断是否需要调整Hashtable的容量。threshold的值=”容量*加载因子”。 loadFactor就是加载因子。 modCount是用来实现fail-fast机制的。 变量的解释在源码注释中如下： 123456789101112131415161718192021222324252627282930313233/** * The hash table data. */ private transient Entry&lt;K,V&gt;[] table; /** * The total number of entries in the hash table. */ private transient int count; /** * The table is rehashed when its size exceeds this threshold. (The * value of this field is (int)(capacity * loadFactor).) * * @serial */ private int threshold; /** * The load factor for the hashtable. * * @serial */ private float loadFactor; /** * The number of times this Hashtable has been structurally modified * Structural modifications are those that change the number of entries in * the Hashtable or otherwise modify its internal structure (e.g., * rehash). This field is used to make iterators on Collection-views of * the Hashtable fail-fast. (See ConcurrentModificationException). */ private transient int modCount = 0; 三、构造方法Hashtable一共提供了4个构造方法： public Hashtable(int initialCapacity, float loadFactor)： 用指定初始容量和指定加载因子构造一个新的空哈希表。useAltHashing为boolean，其如果为真，则执行另一散列的字符串键，以减少由于弱哈希计算导致的哈希冲突的发生。 public Hashtable(int initialCapacity)：用指定初始容量和默认的加载因子 (0.75) 构造一个新的空哈希表。 public Hashtable()：默认构造函数，容量为11，加载因子为0.75。 public Hashtable(Map&lt;? extends K, ? extends V&gt; t)：构造一个与给定的 Map 具有相同映射关系的新哈希表。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * Constructs a new, empty hashtable with the specified initial * capacity and the specified load factor. * * @param initialCapacity the initial capacity of the hashtable. * @param loadFactor the load factor of the hashtable. * @exception IllegalArgumentException if the initial capacity is less * than zero, or if the load factor is nonpositive. */ public Hashtable(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal Load: &quot;+loadFactor); if (initialCapacity==0) initialCapacity = 1; this.loadFactor = loadFactor; table = new Entry[initialCapacity]; threshold = (int)Math.min(initialCapacity * loadFactor, MAX_ARRAY_SIZE + 1); useAltHashing = sun.misc.VM.isBooted() &amp;&amp; (initialCapacity &gt;= Holder.ALTERNATIVE_HASHING_THRESHOLD); &#125; /** * Constructs a new, empty hashtable with the specified initial capacity * and default load factor (0.75). * * @param initialCapacity the initial capacity of the hashtable. * @exception IllegalArgumentException if the initial capacity is less * than zero. */ public Hashtable(int initialCapacity) &#123; this(initialCapacity, 0.75f); &#125; /** * Constructs a new, empty hashtable with a default initial capacity (11) * and load factor (0.75). */ public Hashtable() &#123; this(11, 0.75f); &#125; /** * Constructs a new hashtable with the same mappings as the given * Map. The hashtable is created with an initial capacity sufficient to * hold the mappings in the given Map and a default load factor (0.75). * * @param t the map whose mappings are to be placed in this map. * @throws NullPointerException if the specified map is null. * @since 1.2 */ public Hashtable(Map&lt;? extends K, ? extends V&gt; t) &#123; this(Math.max(2*t.size(), 11), 0.75f); putAll(t); &#125; 四、put方法put方法的整个流程为： 判断value是否为空，为空则抛出异常； 计算key的hash值，并根据hash值获得key在table数组中的位置index，如果table[index]元素不为空，则进行迭代，如果遇到相同的key，则直接替换，并返回旧value； 否则，我们可以将其插入到table[index]位置。 123456789101112131415161718192021222324252627282930313233343536373839public synchronized V put(K key, V value) &#123; // Make sure the value is not null确保value不为null if (value == null) &#123; throw new NullPointerException(); &#125; // Makes sure the key is not already in the hashtable. //确保key不在hashtable中 //首先，通过hash方法计算key的哈希值，并计算得出index值，确定其在table[]中的位置 //其次，迭代index索引位置的链表，如果该位置处的链表存在相同的key，则替换value，返回旧的value Entry tab[] = table; int hash = hash(key); int index = (hash &amp; 0x7FFFFFFF) % tab.length; for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; V old = e.value; e.value = value; return old; &#125; &#125; modCount++; if (count &gt;= threshold) &#123; // Rehash the table if the threshold is exceeded //如果超过阀值，就进行rehash操作 rehash(); tab = table; hash = hash(key); index = (hash &amp; 0x7FFFFFFF) % tab.length; &#125; // Creates the new entry. //将值插入，返回的为null Entry&lt;K,V&gt; e = tab[index]; // 创建新的Entry节点，并将新的Entry插入Hashtable的index位置，并设置e为新的Entry的下一个元素 tab[index] = new Entry&lt;&gt;(hash, key, value, e); count++; return null; &#125; 通过一个实际的例子来演示一下这个过程：假设我们现在Hashtable的容量为5，已经存在了(5,5)，(13,13)，(16,16)，(17,17)，(21,21)这5个键值对，目前他们在Hashtable中的位置如下： 现在，我们插入一个新的键值对，put(16,22)，假设key=16的索引为1.但现在索引1的位置有两个Entry了，所以程序会对链表进行迭代。迭代的过程中，发现其中有一个Entry的key和我们要插入的键值对的key相同，所以现在会做的工作就是将newValue=22替换oldValue=16，然后返回oldValue=16. 然后我们现在再插入一个，put(33,33)，key=33的索引为3，并且在链表中也不存在key=33的Entry，所以将该节点插入链表的第一个位置。 五、get方法相比较于put方法，get方法则简单很多。其过程就是首先通过hash()方法求得key的哈希值，然后根据hash值得到index索引（上述两步所用的算法与put方法都相同）。然后迭代链表，返回匹配的key的对应的value；找不到则返回null。 1234567891011public synchronized V get(Object key) &#123; Entry tab[] = table; int hash = hash(key); int index = (hash &amp; 0x7FFFFFFF) % tab.length; for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; return e.value; &#125; &#125; return null; &#125; 六、遍历方式Hashtable有多种遍历方式： 1234567891011121314151617181920212223//1、使用keys()Enumeration&lt;String&gt; en1 = table.keys(); while(en1.hasMoreElements()) &#123; en1.nextElement();&#125;//2、使用elements()Enumeration&lt;String&gt; en2 = table.elements(); while(en2.hasMoreElements()) &#123; en2.nextElement();&#125;//3、使用keySet()Iterator&lt;String&gt; it1 = table.keySet().iterator(); while(it1.hasNext()) &#123; it1.next();&#125;//4、使用entrySet()Iterator&lt;Entry&lt;String, String&gt;&gt; it2 = table.entrySet().iterator(); while(it2.hasNext()) &#123; it2.next();&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>HashTable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（2）：Java HashSet]]></title>
    <url>%2F2017%2F08%2F10%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%882%EF%BC%89%EF%BC%9AJava%20HashSet%2F</url>
    <content type="text"><![CDATA[一、概述 This class implements the Set interface, backed by a hash table (actually a HashMap instance). It makes no guarantees as to the iteration order of the set; in particular, it does not guarantee that the order will remain constant over time. This class permits the null element. HashSet实现Set接口，由哈希表（实际上是一个HashMap实例）支持。它不保证set 的迭代顺序；特别是它不保证该顺序恒久不变。此类允许使用null元素。 HashSet是基于HashMap来实现的，操作很简单，更像是对HashMap做了一次“封装”，而且只使用了HashMap的key来实现各种特性，我们先来感性的认识一下这个结构： 123456789HashSet&lt;String&gt; set = new HashSet&lt;String&gt;();set.add(&quot;语文&quot;);set.add(&quot;数学&quot;);set.add(&quot;英语&quot;);set.add(&quot;历史&quot;);set.add(&quot;政治&quot;);set.add(&quot;地理&quot;);set.add(&quot;生物&quot;);set.add(&quot;化学&quot;); 通过HashSet最简单的构造函数和几个成员变量来看一下，证明咱们上边说的，其底层是HashMap： 123456789101112private transient HashMap&lt;E,Object&gt; map;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object();/** * Constructs a new, empty set; the backing &lt;tt&gt;HashMap&lt;/tt&gt; instance has * default initial capacity (16) and load factor (0.75). */public HashSet() &#123; map = new HashMap&lt;&gt;();&#125; 其实在英文注释中已经说的比较明确了。首先有一个HashMap的成员变量，我们在HashSet的构造函数中将其初始化，默认情况下采用的是initial capacity为16，load factor为0.75。 二、HashSet的实现 对于HashSet而言，它是基于HashMap实现的，HashSet底层使用HashMap来保存所有元素，因此HashSet 的实现比较简单，相关HashSet的操作，基本上都是直接调用底层HashMap的相关方法来完成，只不过HashSet里面的HashMap所有的value都是同一个Object而已。HashSet的源代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169public class HashSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable &#123; static final long serialVersionUID = -5024744406713321676L; // 底层使用HashMap来保存HashSet中所有元素。 private transient HashMap&lt;E,Object&gt; map; // 定义一个虚拟的Object对象作为HashMap的value，将此对象定义为static final。 private static final Object PRESENT = new Object(); /** * 默认的无参构造器，构造一个空的HashSet。 * * 实际底层会初始化一个空的HashMap，并使用默认初始容量为16和加载因子0.75。 */ public HashSet() &#123; map = new HashMap&lt;E,Object&gt;(); &#125; /** * 构造一个包含指定collection中的元素的新set。 * * 实际底层使用默认的加载因子0.75和足以包含指定 * collection中所有元素的初始容量来创建一个HashMap。 * @param c 其中的元素将存放在此set中的collection。 */ public HashSet(Collection&lt;? extends E&gt; c) &#123; map = new HashMap&lt;E,Object&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); addAll(c); &#125; /** * 以指定的initialCapacity和loadFactor构造一个空的HashSet。 * * 实际底层以相应的参数构造一个空的HashMap。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 */ public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;E,Object&gt;(initialCapacity, loadFactor); &#125; /** * 以指定的initialCapacity构造一个空的HashSet。 * * 实际底层以相应的参数及加载因子loadFactor为0.75构造一个空的HashMap。 * @param initialCapacity 初始容量。 */ public HashSet(int initialCapacity) &#123; map = new HashMap&lt;E,Object&gt;(initialCapacity); &#125; /** * 以指定的initialCapacity和loadFactor构造一个新的空链接哈希集合。 * 此构造函数为包访问权限，不对外公开，实际只是是对LinkedHashSet的支持。 * * 实际底层会以指定的参数构造一个空LinkedHashMap实例来实现。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 * @param dummy 标记。 */ HashSet(int initialCapacity, float loadFactor, boolean dummy) &#123; map = new LinkedHashMap&lt;E,Object&gt;(initialCapacity, loadFactor); &#125; /** * 返回对此set中元素进行迭代的迭代器。返回元素的顺序并不是特定的。 * * 底层实际调用底层HashMap的keySet来返回所有的key。 * 可见HashSet中的元素，只是存放在了底层HashMap的key上， * value使用一个static final的Object对象标识。 * @return 对此set中元素进行迭代的Iterator。 */ public Iterator&lt;E&gt; iterator() &#123; return map.keySet().iterator(); &#125; /** * 返回此set中的元素的数量（set的容量）。 * * 底层实际调用HashMap的size()方法返回Entry的数量，就得到该Set中元素的个数。 * @return 此set中的元素的数量（set的容量）。 */ public int size() &#123; return map.size(); &#125; /** * 如果此set不包含任何元素，则返回true。 * * 底层实际调用HashMap的isEmpty()判断该HashSet是否为空。 * @return 如果此set不包含任何元素，则返回true。 */ public boolean isEmpty() &#123; return map.isEmpty(); &#125; /** * 如果此set包含指定元素，则返回true。 * 更确切地讲，当且仅当此set包含一个满足(o==null ? e==null : o.equals(e)) * 的e元素时，返回true。 * * 底层实际调用HashMap的containsKey判断是否包含指定key。 * @param o 在此set中的存在已得到测试的元素。 * @return 如果此set包含指定元素，则返回true。 */ public boolean contains(Object o) &#123; return map.containsKey(o); &#125; /** * 如果此set中尚未包含指定元素，则添加指定元素。 * 更确切地讲，如果此 set 没有包含满足(e==null ? e2==null : e.equals(e2)) * 的元素e2，则向此set 添加指定的元素e。 * 如果此set已包含该元素，则该调用不更改set并返回false。 * * 底层实际将将该元素作为key放入HashMap。 * 由于HashMap的put()方法添加key-value对时，当新放入HashMap的Entry中key * 与集合中原有Entry的key相同（hashCode()返回值相等，通过equals比较也返回true）， * 新添加的Entry的value会将覆盖原来Entry的value，但key不会有任何改变， * 因此如果向HashSet中添加一个已经存在的元素时，新添加的集合元素将不会被放入HashMap中， * 原来的元素也不会有任何改变，这也就满足了Set中元素不重复的特性。 * @param e 将添加到此set中的元素。 * @return 如果此set尚未包含指定元素，则返回true。 */ public boolean add(E e) &#123; return map.put(e, PRESENT)==null; &#125; /** * 如果指定元素存在于此set中，则将其移除。 * 更确切地讲，如果此set包含一个满足(o==null ? e==null : o.equals(e))的元素e， * 则将其移除。如果此set已包含该元素，则返回true * （或者：如果此set因调用而发生更改，则返回true）。（一旦调用返回，则此set不再包含该元素）。 * * 底层实际调用HashMap的remove方法删除指定Entry。 * @param o 如果存在于此set中则需要将其移除的对象。 * @return 如果set包含指定元素，则返回true。 */ public boolean remove(Object o) &#123; return map.remove(o)==PRESENT; &#125; /** * 从此set中移除所有元素。此调用返回后，该set将为空。 * * 底层实际调用HashMap的clear方法清空Entry中所有元素。 */ public void clear() &#123; map.clear(); &#125; /** * 返回此HashSet实例的浅表副本：并没有复制这些元素本身。 * * 底层实际调用HashMap的clone()方法，获取HashMap的浅表副本，并设置到HashSet中。 */ public Object clone() &#123; try &#123; HashSet&lt;E&gt; newSet = (HashSet&lt;E&gt;) super.clone(); newSet.map = (HashMap&lt;E, Object&gt;) map.clone(); return newSet; &#125; catch (CloneNotSupportedException e) &#123; throw new InternalError(); &#125; &#125; &#125; 对于HashSet中保存的对象，请注意正确重写其equals和hashCode方法，以保证放入的对象的唯一性。这两个方法是比较重要的，希望大家在以后的开发过程中需要注意一下。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>HashSet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（1）：Java HashMap]]></title>
    <url>%2F2017%2F08%2F09%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%881%EF%BC%89%EF%BC%9AJava%20HashMap%2F</url>
    <content type="text"><![CDATA[一、概述从本文你可以学习到： 什么时候会使用HashMap？他有什么特点？ 你知道HashMap的工作原理吗？ 你知道get和put的原理吗？equals()和hashCode()的都有什么作用？ 你知道hash的实现吗？为什么要这样实现？ 如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？ 当我们执行下面的操作时： 123456789101112HashMap&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;();map.put(&quot;语文&quot;, 1);map.put(&quot;数学&quot;, 2);map.put(&quot;英语&quot;, 3);map.put(&quot;历史&quot;, 4);map.put(&quot;政治&quot;, 5);map.put(&quot;地理&quot;, 6);map.put(&quot;生物&quot;, 7);map.put(&quot;化学&quot;, 8);for(Entry&lt;String, Integer&gt; entry : map.entrySet()) &#123; System.out.println(entry.getKey() + &quot;: &quot; + entry.getValue());&#125; 运行结果是： 12345678政治: 5生物: 7历史: 4数学: 2化学: 8语文: 1英语: 3地理: 6 在官方文档中是这样描述HashMap的：&gt;Hash table based implementation of the Map interface. This implementation provides all of the optional map operations, and permits null values and the null key. (The HashMap class is roughly equivalent to Hashtable, except that it is unsynchronized and permits nulls.) This class makes no guarantees as to the order of the map; in particular, it does not guarantee that the order will remain constant over time. 几个关键的信息：基于Map接口实现、允许null键/值、非同步、不保证有序(比如插入的顺序)、也不保证序不随时间变化。 在HashMap中有两个很重要的参数，容量(Capacity)和负载因子(Load factor) Initial capacity： The capacity is the number of buckets in the hash table, The initial capacity is simply the capacity at the time the hash table is created. Load factor： The load factor is a measure of how full the hash table is allowed to get before its capacity is automatically increased. 简单的说，Capacity就是bucket的大小，Load factor就是bucket填满程度的最大比例。如果对迭代性能要求很高的话不要把capacity设置过大，也不要把load factor设置过小。当bucket中的entries的数目大于capacity*load factor时就需要调整bucket的大小为当前的2倍。 二、构造函数HashMap底层维护一个数组，当新建一个HashMap的时候，就会初始化一个数组。我们看一下JDK源码中的HashMap构造函数： 12345678910111213141516171819202122public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); // Find a power of 2 &gt;= initialCapacity int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; this.loadFactor = loadFactor; threshold = (int)Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); table = new Entry[capacity]; useAltHashing = sun.misc.VM.isBooted() &amp;&amp; (capacity &gt;= Holder.ALTERNATIVE_HASHING_THRESHOLD); init();&#125; 可以看到其中一行为table = new Entry[capacity];。在构造函数中，其创建了一个Entry的数组，其大小为capacity，那么Entry又是什么结构呢？看一下源码： 12345678transient Entry&lt;K,V&gt;[] table;static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; Entry&lt;K,V&gt; next; final int hash; ……&#125; HashMap中的是通过transient Entry[] table来存储数据，该变量是通过transient进行修饰的。 Entry是一个static class，其中包含了key和value，也就是键值对，另外还包含了一个next的Entry指针。我们可以总结出：Entry就是数组中的元素，每个Entry其实就是一个key-value对，它持有一个指向下一个元素的引用，这就构成了链表。 三、put()方法和get()方法3.1 put()方法put函数大致的思路为： 对key的hashCode()做hash，然后再计算index; 如果没碰撞直接放到bucket里； 如果碰撞了，以链表的形式存在buckets后； 如果碰撞导致链表过长(大于等于TREEIFY_THRESHOLD)，就把链表转换成红黑树； 如果节点已经存在就替换old value(保证key的唯一性) 如果bucket满了(超过load factor*current capacity)，就要resize。 具体代码的实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //如果当前map中无数据，执行resize方法。并且返回n if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; ///如果要插入的键值对要存放的这个位置刚好没有元素，那么把他封装成Node对象，放在这个位置上就完事了 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); //否则的话，说明这上面有元素 else &#123; Node&lt;K,V&gt; e; K k; //如果这个元素的key与要插入的一样，那么就替换一下，也完事。 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //1.如果当前节点是TreeNode类型的数据，执行putTreeVal方法 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); //还是遍历这条链子上的数据，跟jdk7没什么区别 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); //2.完成了操作后多做了一件事情，判断，并且可能执行treeifyBin方法，treeifyBin()就是将链表转换成红黑树。 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 写入 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; //判断阈值，决定是否扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 一直到JDK7为止，HashMap的结构基于一个数组以及多个链表的实现，hash值冲突的时候，就将对应节点以链表的形式存储。 这样子的HashMap性能上就抱有一定疑问，如果说成百上千个节点在hash时发生碰撞，存储一个链表中，那么如果要查找其中一个节点，那就不可避免的花费O(N)的查找时间，这将是多么大的性能损失。这个问题终于在JDK8中得到了解决。再最坏的情况下，链表查找的时间复杂度为O(n),而红黑树一直是O(logn),这样会提高HashMap的效率。JDK7中HashMap采用的是位桶+链表的方式，即我们常说的散列链表的方式，而JDK8中采用的是位桶+链表/红黑树（有关红黑树请查看红黑树）的方式，也是非线程安全的。当某个位桶的链表的长度达到某个阀值的时候，这个链表就将转换成红黑树。 JDK8中，当同一个hash值的节点数不小于8时，将不再以单链表的形式存储了，会被调整成一颗红黑树。这就是JDK7与JDK8中HashMap实现的最大区别。JDK中Entry的名字变成了Node，原因是和红黑树的实现TreeNode相关联。 1transient Node&lt;K,V&gt;[] table; 当冲突节点数不小于8-1时，转换成红黑树。 1static final int TREEIFY_THRESHOLD = 8; 总结下put的过程： 当程序试图将一个key-value对放入HashMap中时，程序首先根据该 key 的 hashCode() 返回值决定该 Entry 的存储位置， 该位置就是此对象准备往数组中存放的位置。 如果该位置没有对象存在，就将此对象直接放进数组当中；如果该位置已经有对象存在了，则顺着此存在的对象的链开始寻找(为了判断是否是否值相同，map不允许键值对重复)， 使用 equals方法进行比较，如果对此链上的 key 通过 equals 比较有一个返回 true，新添加 Entry 的 value 将覆盖集合中原有 Entry 的 value，但key不会覆盖；如果对此链上的每个对象的 equals 方法比较都为 false，则将该对象放到数组当中，然后将数组中该位置以前存在的那个对象链接到此对象的后面，即新值存放在数组中，旧值在新值的链表上。 3.2 get()方法在理解了put之后，get就很简单了。大致思路如下： bucket里的第一个节点，直接命中； 如果有冲突，则通过key.equals(k)去查找对应的entry 若为树，则在树中通过key.equals(k)查找，O(logn)； 若为链表，则在链表中通过key.equals(k)查找，O(n)。 具体代码的实现如下： 123456789101112131415161718192021222324252627public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 直接命中 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 未命中 if ((e = first.next) != null) &#123; // 在树中get if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 在链表中get do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 3.3 null key存取对于put方法来说，HashMap会对null值key进行特殊处理，总是放到table[0]位置对于get方法来说，同样当key为null时会进行特殊处理，在table[0]的链表上查找key为null的元素 12345678910111213private V putForNullKey(V value) &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(0, null, value, 0); return null; &#125; 四、hash()与indexFor()4.1 hash()方法在get和put的过程中，计算下标时，先对hashCode进行hash操作，然后再通过hash值进一步计算下标。 在对hashCode()计算hash时具体实现是这样的： 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 按位取并，作用上相当于取模mod或者取余%。这意味着数组下标相同，并不表示hashCode相同。 可以看到这个函数大概的作用就是：高16bit不变，低16bit和高16bit做了一个异或。其中代码注释是这样写的： &gt;Computes key.hashCode() and spreads (XORs) higher bits of hash to lower. Because the table uses power-of-two masking, sets of hashes that vary only in bits above the current mask will always collide. (Among known examples are sets of Float keys holding consecutive whole numbers in small tables.) So we apply a transform that spreads the impact of higher bits downward. There is a tradeoff between speed, utility, and quality of bit-spreading. Because many common sets of hashes are already reasonably distributed (so don’t benefit from spreading), and because we use trees to handle large sets of collisions in bins, we just XOR some shifted bits in the cheapest possible way to reduce systematic lossage, as well as to incorporate impact of the highest bits that would otherwise never be used in index calculations because of table bounds. 4.2 indexFor()方法在设计hash函数时，因为目前的table长度length n为2的幂，而计算下标的时候，是这样实现的(使用&amp;位操作，而非%求余)： 123static int indexFor(int h, int n) &#123; return h &amp; (n-1); &#125; 设计者认为这方法很容易发生碰撞。为什么这么说呢？不妨思考一下，在n - 1为15(0x1111)时，其实散列真正生效的只是低4bit的有效位，当然容易碰撞了。 因此，设计者想了一个顾全大局的方法(综合考虑了速度、作用、质量)，就是把高16bit和低16bit异或了一下。设计者还解释到因为现在大多数的hashCode的分布已经很不错了，就算是发生了碰撞也用O(logn)的tree去做了。仅仅异或一下，既减少了系统的开销，也不会造成的因为高位没有参与下标的计算(table长度比较小时)，从而引起的碰撞。 如果还是产生了频繁的碰撞，会发生什么问题呢？作者注释说，他们使用树来处理频繁的碰撞(we use trees to handle large sets of collisions in bins)，在JEP-180中，描述了这个问题：&gt;Improve the performance of java.util.HashMap under high hash-collision conditions by using balanced trees rather than linked lists to store map entries. Implement the same improvement in the LinkedHashMap class. 之前已经提过，在获取HashMap的元素时，基本分两步： 首先根据hashCode()做hash，然后确定bucket的index； 如果bucket的节点的key不是我们需要的，则通过keys.equals()在链中找。 在Java 8之前的实现中是用链表解决冲突的，在产生碰撞的情况下，进行get时，两步的时间复杂度是O(1)+O(n)。因此，当碰撞很厉害的时候n很大，O(n)的速度显然是影响速度的。 因此在Java 8中，利用红黑树替换链表，这样复杂度就变成了O(1)+O(logn)了，这样在n很大的时候，能够比较理想的解决这个问题，在Java 8：HashMap的性能提升一文中有性能测试的结果。 五、resize()方法当put时，如果发现目前的bucket占用程度已经超过了Load Factor所希望的比例，那么就会发生resize。在resize的过程，简单的说就是把bucket扩充为2倍，之后重新计算index，把节点再放到新的bucket中。resize的注释是这样描述的： &gt;Initializes or doubles table size. If null, allocates in accord with initial capacity target held in field threshold. Otherwise, because we are using power-of-two expansion, the elements from each bin must either stay at same index, or move with a power of two offset in the new table. 大致意思就是说，当超过限制的时候会resize，然而又因为我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。 怎么理解呢？例如我们从16扩展为32时，元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。 下面是代码的具体实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 六、remove()、clear()、containsKey()和containsValue()6.1 remove()方法remove方法和put get类似，计算hash，计算index，然后遍历查找，将找到的元素从table[index]链表移除123456789101112131415161718192021222324252627282930public V remove(Object key) &#123; Entry&lt;K,V&gt; e = removeEntryForKey(key); return (e == null ? null : e.value); &#125; final Entry&lt;K,V&gt; removeEntryForKey(Object key) &#123; int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; e = prev; while (e != null) &#123; Entry&lt;K,V&gt; next = e.next; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; modCount++; size--; if (prev == e) table[i] = next; else prev.next = next; e.recordRemoval(this); return e; &#125; prev = e; e = next; &#125; return e; &#125; 6.2 clear()方法clear方法非常简单，就是遍历table然后把每个位置置为null，同时修改元素个数为0需要注意的是clear方法只会清楚里面的元素，并不会重置capactiy 1234567public void clear() &#123; modCount++; Entry[] tab = table; for (int i = 0; i &lt; tab.length; i++) tab[i] = null; size = 0; &#125; 6.3 containsKey()方法containsKey方法是先计算hash然后使用hash和table.length取摸得到index值，遍历table[index]元素查找是否包含key相同的值 123456789101112131415public boolean containsKey(Object key) &#123; return getEntry(key) != null; &#125;final Entry&lt;K,V&gt; getEntry(Object key) &#123; int hash = (key == null) ? 0 : hash(key.hashCode()); for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; return null; &#125; 6.4 containsValue()方法containsValue方法就比较粗暴了，就是直接遍历所有元素直到找到value，由此可见HashMap的containsValue方法本质上和普通数组和list的contains方法没什么区别，你别指望它会像containsKey那么高效 1234567891011public boolean containsValue(Object value) &#123; if (value == null) return containsNullValue(); Entry[] tab = table; for (int i = 0; i &lt; tab.length ; i++) for (Entry e = tab[i] ; e != null ; e = e.next) if (value.equals(e.value)) return true; return false; &#125; 6.5 Fail-Fast机制我们知道java.util.HashMap不是线程安全的，因此如果在使用迭代器的过程中有其他线程修改了map，那么将抛出ConcurrentModificationException，这就是所谓fail-fast策略。 fail-fast 机制是java集合(Collection)中的一种错误机制。 当多个线程对同一个集合的内容进行操作时，就可能会产生 fail-fast 事件。 例如：当某一个线程A通过 iterator去遍历某集合的过程中，若该集合的内容被其他线程所改变了；那么线程A访问集合时，就会抛出 ConcurrentModificationException异常，产生 fail-fast 事件。 这一策略在源码中的实现是通过modCount域，modCount顾名思义就是修改次数，对HashMap内容（当然不仅仅是HashMap才会有，其他例如ArrayList也会）的修改都将增加这个值（大家可以再回头看一下其源码，在很多操作中都有modCount++这句），那么在迭代器初始化过程中会将这个值赋给迭代器的expectedModCount。 12345678HashIterator() &#123; expectedModCount = modCount; if (size &gt; 0) &#123; // advance to first entry Entry[] t = table; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) ; &#125;&#125; 在迭代过程中，判断modCount跟expectedModCount是否相等，如果不相等就表示已经有其他线程修改了Map： 注意到modCount声明为volatile，保证线程之间修改的可见性。 123final Entry&lt;K,V&gt; nextEntry() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); 在HashMap的API中指出： 由所有HashMap类的“collection 视图方法”所返回的迭代器都是快速失败的：在迭代器创建之后，如果从结构上对映射进行修改，除非通过迭代器本身的 remove 方法，其他任何时间任何方式的修改，迭代器都将抛出 ConcurrentModificationException。因此，面对并发的修改，迭代器很快就会完全失败，而不冒在将来不确定的时间发生任意不确定行为的风险。 注意，迭代器的快速失败行为不能得到保证，一般来说，存在非同步的并发修改时，不可能作出任何坚决的保证。快速失败迭代器尽最大努力抛出ConcurrentModificationException。因此，编写依赖于此异常的程序的做法是错误的，正确做法是：迭代器的快速失败行为应该仅用于检测程序错误。 在上文中也提到，fail-fast机制，是一种错误检测机制。它只能被用来检测错误，因为JDK并不保证fail-fast机制一定会发生。若在多线程环境下使用 fail-fast机制的集合，建议使用“java.util.concurrent包下的类”去取代“java.util包下的类”。 6.6 两种遍历方式第一种效率高,以后一定要使用此种方式！ 1234567 Map map = new HashMap(); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); Object key = entry.getKey(); Object val = entry.getValue(); &#125; 第二种效率低,以后尽量少使用！ 123456 Map map = new HashMap(); Iterator iter = map.keySet().iterator(); while (iter.hasNext()) &#123; Object key = iter.next(); Object val = map.get(key); &#125; 七、一些问题我们现在可以回答开始的几个问题，加深对HashMap的理解： 什么时候会使用HashMap？他有什么特点？是基于Map接口的实现，存储键值对时，它可以接收null的键值，是非同步的，HashMap存储着Entry(hash, key, value, next)对象。 你知道HashMap的工作原理吗？通过hash的方法，通过put和get存储和获取对象。存储对象时，我们将K/V传给put方法时，它调用hashCode计算hash从而得到bucket位置，进一步存储，HashMap会根据当前bucket的占用情况自动调整容量(超过Load Facotr则resize为原来的2倍)。获取对象时，我们将K传给get，它调用hashCode计算hash从而得到bucket位置，并进一步调用equals()方法确定键值对。如果发生碰撞的时候，Hashmap通过链表将产生碰撞冲突的元素组织起来，在Java 8中，如果一个bucket中碰撞冲突的元素超过某个限制(默认是8)，则使用红黑树来替换链表，从而提高速度。 你知道get和put的原理吗？equals()和hashCode()的都有什么作用？通过对key的hashCode()进行hashing，并计算下标( n-1 &amp; hash)，从而获得buckets的位置。如果产生碰撞，则利用key.equals()方法去链表或树中去查找对应的节点。 你知道hash的实现吗？为什么要这样实现？在Java 1.8的实现中，是通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在bucket的n比较小的时候，也能保证考虑到高低bit都参与到hash的计算中，同时不会有太大的开销。 如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？如果超过了负载因子(默认0.75)，则会重新resize一个原来长度两倍的HashMap，并且重新调用hash方法。 Java集合小抄以Entry[]数组实现的哈希桶数组，用Key的哈希值取模桶数组的大小可得到数组下标。插入元素时，如果两条Key落在同一个桶(比如哈希值1和17取模16后都属于第一个哈希桶)，Entry用一个next属性实现多个Entry以单向链表存放，后入桶的Entry将next指向桶当前的Entry。查找哈希值为17的key时，先定位到第一个哈希桶，然后以链表遍历桶里所有元素，逐个比较其key值。当Entry数量达到桶数量的75%时(很多文章说使用的桶数量达到了75%，但看代码不是)，会成倍扩容桶数组，并重新分配所有原来的Entry，所以这里也最好有个预估值。取模用位运算(hash &amp; (arrayLength-1))会比较快，所以数组的大小永远是2的N次方， 你随便给一个初始值比如17会转为32。默认第一次放入元素时的初始值是16。iterator()时顺着哈希桶数组来遍历，看起来是个乱序。在JDK8里，新增默认为8的閥值，当一个桶里的Entry超过閥值，就不以单向链表而以红黑树来存放以加快Key的查找速度。 一个很棒的面试题 HashMap的工作原理是近年来常见的Java面试题。几乎每个Java程序员都知道HashMap，都知道哪里要用HashMap，知道Hashtable和HashMap之间的区别，那么为何这道面试题如此特殊呢？是因为这道题考察的深度很深。这题经常出现在高级或中高级面试中。投资银行更喜欢问这个问题，甚至会要求你实现HashMap来考察你的编程能力。ConcurrentHashMap和其它同步集合的引入让这道题变得更加复杂。让我们开始探索的旅程吧！ 先来些简单的问题 “你用过HashMap吗？” “什么是HashMap？你为什么用到它？” 几乎每个人都会回答“是的”，然后回答HashMap的一些特性，譬如HashMap可以接受null键值和值，而Hashtable则不能；HashMap是非synchronized;HashMap很快；以及HashMap储存的是键值对等等。这显示出你已经用过HashMap，而且对它相当的熟悉。但是面试官来个急转直下，从此刻开始问出一些刁钻的问题，关于HashMap的更多基础的细节。面试官可能会问出下面的问题： “你知道HashMap的工作原理吗？” “你知道HashMap的get()方法的工作原理吗？” 你也许会回答“我没有详查标准的Java API，你可以看看Java源代码或者Open JDK。”“我可以用Google找到答案。” 但一些面试者可能可以给出答案，“HashMap是基于hashing的原理，我们使用put(key, value)存储对象到HashMap中，使用get(key)从HashMap中获取对象。当我们给put()方法传递键和值时，我们先对键调用hashCode()方法，返回的hashCode用于找到bucket位置来储存Entry对象。”这里关键点在于指出，HashMap是在bucket中储存键对象和值对象，作为Map.Entry。这一点有助于理解获取对象的逻辑。如果你没有意识到这一点，或者错误的认为仅仅只在bucket中存储值的话，你将不会回答如何从HashMap中获取对象的逻辑。这个答案相当的正确，也显示出面试者确实知道hashing以及HashMap的工作原理。但是这仅仅是故事的开始，当面试官加入一些Java程序员每天要碰到的实际场景的时候，错误的答案频现。下个问题可能是关于HashMap中的碰撞探测(collision detection)以及碰撞的解决方法： “当两个对象的hashcode相同会发生什么？” 从这里开始，真正的困惑开始了，一些面试者会回答因为hashcode相同，所以两个对象是相等的，HashMap将会抛出异常，或者不会存储它们。然后面试官可能会提醒他们有equals()和hashCode()两个方法，并告诉他们两个对象就算hashcode相同，但是它们可能并不相等。一些面试者可能就此放弃，而另外一些还能继续挺进，他们回答“因为hashcode相同，所以它们的bucket位置相同，‘碰撞’会发生。因为HashMap使用链表存储对象，这个Entry(包含有键值对的Map.Entry对象)会存储在链表中。”这个答案非常的合理，虽然有很多种处理碰撞的方法，这种方法是最简单的，也正是HashMap的处理方法。但故事还没有完结，面试官会继续问： “如果两个键的hashcode相同，你如何获取值对象？” 面试者会回答：当我们调用get()方法，HashMap会使用键对象的hashcode找到bucket位置，然后获取值对象。面试官提醒他如果有两个值对象储存在同一个bucket，他给出答案:将会遍历链表直到找到值对象。面试官会问因为你并没有值对象去比较，你是如何确定确定找到值对象的？除非面试者直到HashMap在链表中存储的是键值对，否则他们不可能回答出这一题。 其中一些记得这个重要知识点的面试者会说，找到bucket位置之后，会调用keys.equals()方法去找到链表中正确的节点，最终找到要找的值对象。完美的答案！ 许多情况下，面试者会在这个环节中出错，因为他们混淆了hashCode()和equals()方法。因为在此之前hashCode()屡屡出现，而equals()方法仅仅在获取值对象的时候才出现。一些优秀的开发者会指出使用不可变的、声明作final的对象，并且采用合适的equals()和hashCode()方法的话，将会减少碰撞的发生，提高效率。不可变性使得能够缓存不同键的hashcode，这将提高整个获取对象的速度，使用String，Interger这样的wrapper类作为键是非常好的选择。 如果你认为到这里已经完结了，那么听到下面这个问题的时候，你会大吃一惊。“如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？”除非你真正知道HashMap的工作原理，否则你将回答不出这道题。默认的负载因子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其它集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程叫作rehashing，因为它调用hash方法找到新的bucket位置。 如果你能够回答这道问题，下面的问题来了：“你了解重新调整HashMap大小存在什么问题吗？”你可能回答不上来，这时面试官会提醒你当多线程的情况下，可能产生条件竞争(race condition)。 当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。这个时候，你可以质问面试官，为什么这么奇怪，要在多线程的环境下使用HashMap呢？：） 热心的读者贡献了更多的关于HashMap的问题： 为什么String, Interger这样的wrapper类适合作为键？ String, Interger这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的，也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点。不可变性是必要的，因为为了要计算hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性还有其他的优点如线程安全。如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法是非常重要的。如果两个不相等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。 我们可以使用自定义的对象作为键吗？ 这是前一个问题的延伸。当然你可能使用任何对象作为键，只要它遵守了equals()和hashCode()方法的定义规则，并且当对象插入到Map中之后将不会再改变了。如果这个自定义对象时不可变的，那么它已经满足了作为键的条件，因为当它创建之后就已经不能改变了。 我们可以使用CocurrentHashMap来代替Hashtable吗？这是另外一个很热门的面试题，因为ConcurrentHashMap越来越多人用了。我们知道Hashtable是synchronized的，但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。看看这篇博客查看Hashtable和ConcurrentHashMap的区别。 我个人很喜欢这个问题，因为这个问题的深度和广度，也不直接的涉及到不同的概念。让我们再来看看这些问题设计哪些知识点： hashing的概念 HashMap中解决碰撞的方法 equals()和hashCode()的应用，以及它们在HashMap中的重要性 不可变对象的好处 HashMap多线程的条件竞争 重新调整HashMap的大小 总结 HashMap的工作原理HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，让后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap在每个链表节点中储存键值对对象。 当两个不同的键对象的hashcode相同时会发生什么？ 它们会储存在同一个bucket位置的链表中。键对象的equals()方法用来找到键值对。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（10）：0-1背包问题与部分背包问题]]></title>
    <url>%2F2017%2F08%2F09%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%8810%EF%BC%89%EF%BC%9A0-1%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%E4%B8%8E%E9%83%A8%E5%88%86%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[假设我们有n件物品，分别编号为1, 2…n。其中编号为i的物品价值为$v_i$，它的重量为$w_i$。为了简化问题，假定价值和重量都是整数值。现在，假设我们有一个背包，它能够承载的重量是W。现在，我们希望往包里装这些物品，使得包里装的物品价值最大化，那么我们该如何来选择装的东西呢？问题结构如下图所示： 这个问题其实根据不同的情况可以归结为不同的解决方法。假定我们这里选取的物品每个都是独立的，不能选取部分。也就是说我们要么选取某个物品，要么不能选取，不能只选取一个物品的一部分。这种情况，我们称之为0-1背包问题。而如果我们可以使用部分的物品的话，这个问题则成为部分背包(fractional knapsack)问题。下面我们针对每种情况具体分析一下。 一、0-1背包1.1 初步分析对于这个问题，一开始确实有点不太好入手。一堆的物品，每一个都有一定的质量和价值，我们能够装入的总重量有限制，该怎么来装使得价值最大呢？对于这n个物品，每个物品我们可能会选，也可能不选，那么我们总共就可能有2^n种组合选择方式。如果我们采用这种办法来硬算的话，则整体的时间复杂度就达到指数级别的，肯定不可行。 现在我们换一种思路。既然每一种物品都有价格和重量，我们优先挑选那些单位价格最高的是否可行呢？比如在下图中，我们有3种物品，他们的重量和价格分别是10, 20, 30 kg和60, 100, 120。 那么按照单位价格来算的话，我们最先应该挑选的是价格为60的元素，选择它之后，背包还剩下50 - 10 = 40kg。再继续前面的选择，我们应该挑选价格为100的元素，这样背包里的总价值为60 + 100 = 160。所占用的重量为30, 剩下20kg。因为后面需要挑选的物品为30kg已经超出背包的容量了。我们按照这种思路能选择到的最多就是前面两个物品。如下图： 按照我们前面的期望，这样选择得到的价值应该是最大的。可是由于有一个背包重量的限制，这里只用了30kg，还有剩下20kg浪费了。这会是最优的选择吗？我们看看所有的选择情况： 很遗憾，在这几种选择情况中，我们前面的选择反而是带来价值最低的。而选择重量分别为20kg和30kg的物品带来了最大的价值。看来，我们刚才这种选择最佳单位价格的方式也行不通。 1.2 动态规划既然前面两种办法都不可行，我们再来看看有没有别的方法。我们再来看这个问题。我们需要选择n个元素中的若干个来形成最优解，假定为k个。那么对于这k个元素a1, a2, …ak来说，它们组成的物品组合必然满足总重量&lt;=背包重量限制，而且它们的价值必然是最大的。因为它们是我们假定的最优选择嘛，肯定价值应该是最大的。假定$a_k$是我们按照前面顺序放入的最后一个物品。它的重量为$w_k$，它的价值为$v_k$。既然我们前面选择的这k个元素构成了最优选择，如果我们把这个$a_k$物品拿走，对应于$k-1$个物品来说，它们所涵盖的重量范围为$0-(W-w_k)$。假定W为背包允许承重的量。假定最终的价值是V，剩下的物品所构成的价值为$V-v_k$。这剩下的$k-1$个元素是不是构成了一个这种$W-w_k$的最优解呢？ 我们可以用反证法来推导。假定拿走$a_k$这个物品后，剩下的这些物品没有构成$W-w_k$重量范围的最佳价值选择。那么我们肯定有另外$k-1$个元素，他们在$W-w_k$重量范围内构成的价值更大。如果这样的话，我们用这$k-1$个物品再加上第k个，他们构成的最终W重量范围内的价值就是最优的。这岂不是和我们前面假设的k个元素构成最佳矛盾了吗？所以我们可以肯定，在这k个元素里拿掉最后那个元素，前面剩下的元素依然构成一个最佳解。 现在我们经过前面的推理已经得到了一个基本的递推关系，就是一个最优解的子解集也是最优的。可是，我们该怎么来求得这个最优解呢？我们这样来看。假定我们定义一个函数$c[i, w]$表示到第i个元素为止，在限制总重量为w的情况下我们所能选择到的最优解。那么这个最优解要么包含有i这个物品，要么不包含，肯定是这两种情况中的一种。如果我们选择了第i个物品，那么实际上这个最优解是c[i - 1, w-wi] + vi。而如果我们没有选择第i个物品，这个最优解是c[i-1, w]。这样，实际上对于到底要不要取第i个物品，我们只要比较这两种情况，哪个的结果值更大不就是最优的么？ 在前面讨论的关系里，还有一个情况我们需要考虑的就是，我们这个最优解是基于选择物品i时总重量还是在w范围内的，如果超出了呢？我们肯定不能选择它，这就和c[i-1, w]一样。 另外，对于初始的情况呢？很明显c[0, w]里不管w是多少，肯定为0。因为它表示我们一个物品都不选择的情况。c[i, 0]也一样，当我们总重量限制为0时，肯定价值为0。 这样，基于我们前面讨论的这3个部分，我们可以得到一个如下的递推公式： 有了这个关系，我们可以更进一步的来考虑代码实现了。我们有这么一个递归的关系，其中，后面的函数结果其实是依赖于前面的结果的。我们只要按照前面求出来最基础的最优条件，然后往后面一步步递推，就可以找到结果了。 我们再来考虑一下具体实现的细节。这一组物品分别有价值和重量，我们可以定义两个数组int[] v, int[] w。v[i]表示第i个物品的价值，w[i]表示第i个物品的重量。为了表示c[i, w]，我们可以使用一个int[i][w]的矩阵。其中i的最大值为物品的数量，而w表示最大的重量限制。按照前面的递推关系，c[i][0]和c[0][w]都是0。而我们所要求的最终结果是c[n][w]。所以我们实际中创建的矩阵是(n + 1) x (w + 1)的规格。下面是该过程的一个代码参考实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class DynamicKnapSack &#123; private int[] v; private int[] w; private int[][] c; private int weight; public DynamicKnapSack(int length, int weight, int[] vin, int[] win) &#123; v = new int[length + 1]; w = new int[length + 1]; c = new int[length + 1][weight + 1]; this.weight = weight; for(int i = 0; i &lt; length + 1; i++) &#123; v[i] = vin[i]; w[i] = win[i]; &#125; &#125; public void solve() &#123; for(int i = 1; i &lt; v.length; i++) &#123; for(int k = 1; k &lt;= weight; k++) &#123; if(w[i] &lt;= k) &#123; if(v[i] + c[i - 1][k - w[i]] &gt; c[i - 1][k]) c[i][k] = v[i] + c[i - 1][k - w[i]]; else c[i][k] = c[i - 1][k]; &#125; else c[i][k] = c[i - 1][k]; &#125; &#125; &#125; public void printResult() &#123; for(int i = 0; i &lt; v. length; i++) &#123; for(int j = 0; j &lt;= weight; j++) System.out.print(c[i][j] + &quot; &quot;); System.out.println(); &#125; &#125; public static void main(String[] args) &#123; int[] v = &#123;0, 60, 100, 120&#125;; int[] w = &#123;0, 10, 20, 30&#125;; int weight = 50; DynamicKnapSack knapsack = new DynamicKnapSack(3, weight, v, w); knapsack.solve(); knapsack.printResult(); &#125;&#125; 这部分代码里关键的就是solve方法。里面两个遍历循环，i表示从1到n的范围，对应于我们递归方法里描述的c(i, w)中到第i位。而k表示的是当前的重量限制。下面是程序运行的输出结果： 12340 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 0 0 0 0 0 0 0 0 0 0 60 60 60 60 60 60 60 60 60 60 100 100 100 100 100 100 100 100 100 100 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 0 0 0 0 0 0 0 0 0 0 60 60 60 60 60 60 60 60 60 60 100 100 100 100 100 100 100 100 100 100 160 160 160 160 160 160 160 160 160 160 180 180 180 180 180 180 180 180 180 180 220 最右下角的数值220就是c[3, 50]的解。 至此，我们对于这种问题的解决方法已经分析出来了。它的总体时间复杂度为O(nw) ，其中w是设定的一个重量范围，因此也可以说它的时间复杂度为O(n)。 二、部分背包问题和前面使用动态规划方法解决问题不一样。因为这里是部分背包问题，我们可以采用前面讨论过的一个思路。就是每次选择最优单位价格的物品，直到达到背包重量限制要求。 以前面的示例来看，我们按照这种方式选择的物品结果应该如下图： 现在，我们从实现的角度再来考虑一下。我们这里的最优解是每次挑选性价比最高的物品。对于这一组物品来说，我们需要将他们按照性价比从最高到最低的顺序来取。我们可能需要将他们进行排序。然后再依次取出来放入背包中。假定我们已经有数组v，w，他们已经按照性价比排好序了。一个参考代码的实现如下： 12345678910111213141516public double selectMax() &#123; double maxValue = 0.0; int sum = 0; int i; for(i = 0; i &lt; v.length; i++) &#123; if(sum + w[i] &lt; weight) &#123; sum += w[i]; maxValue += v[i]; &#125; else break; &#125; if(i &lt; v.length &amp;&amp; sum &lt; weight) &#123; maxValue += (double)(weight - sum) / w[i] * v[i]; &#125; return maxValue; &#125; 这里省略了对数组v, w的定义。关键点在于我们选择了若干了物品后要判断是否装满了背包重量。如果没有，还要从后面的里面挑选一部分。所以有一个if(i &lt; v.length &amp;&amp; sum &lt; weight)的判断。 在实现后我们来看该问题这种解法的时间复杂度，因为需要将数组排序，我们的时间复杂度为O(nlgn)。 在前面我们挑选按照性价比排好序的物品时，排序消耗了主要的时间。在这里，我们是否真的需要去把这些物品排序呢？在某些情况下，我们只要选择一堆物品，保证他们物品重量在指定范围内。如果我们一次挑出来一批这样的物品，而且他们满足这样的条件是不是更好呢？这一种思路是借鉴快速排序里对元素进行划分的思路。主要过程如下： 求每个元素的单位价值，pi = vi /wi。然后数组按照pi进行划分，这样会被分成3个部分，L, M, N。其中L &lt; M &lt; N。这里L表示单位价值小于某个指定值的集合，M是等于这个值的集合，而N是大于这个值的集合。 我们可以首先看N的集合，因为这里都是单位价值高的集合。我们将他们的重量累加，如果WN的重量等于我们期望的值W，则N中间的结果就是我们找到的结果。 如果WN的重量大于W，我们需要在N集合里做进一步划分。 如果WN的重量小于W，我们需要在N的基础上再去L的集合里划分，找里面大的一部分。 这样重复步骤1到4. 这里和快速排序的思路基本上差不多，只是需要将一个分割的集合给记录下来。其时间复杂度也更好一点，为O(N)。这里就简单的描述下思路，等后续再将具体的实现代码给补上。 三、总结我们这里讨论的两种背包问题因为问题的不同其本质解决方法也不同。对于0-1背包来说，他们构成了一个最优解问题的基础。我们可以通过从最小的结果集递推出最终最优结果。他们之间构成了一个递归的关系。而对于部分背包问题来说，我们可以考虑用贪婪算法，每次选择当前看来最优的结果。最终也构成了一个最优的结果。一个小小的前提变化，问题解决的思路却大不同。里面的思想值得反复体会。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>0-1背包</tag>
        <tag>部分背包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（11）：最长回文子串]]></title>
    <url>%2F2017%2F08%2F09%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%8811%EF%BC%89%EF%BC%9A%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[给定字符串S，求它的最长回文子串。假设S的最长长度为1000，并且仅有唯一的最长回文子串。 一、暴力法（ Brute Force）最简便，但同时时间复杂度也是最高的肯定是暴力解法，就是遍历字符串的“所有子串”，并判断每个子串是否为对称回文。因为字符串所有子串的复杂度为$O(n^2)$，在判断回文，总体的复杂度达到$O(n^3)$。 复杂度：时间 $O(n^3) $空间 $O(1)$可以做一些简单的优化： 从最长的子串开始遍历，一旦找到一个回文，就终止迭代。 判断回文采用收缩法，从最外一对字符往中心推进。 12345678910111213141516171819202122public class Solution &#123; public String longestPalindrome(String s) &#123; for (int size = s.length(); size &gt; 0; size--) &#123; for (int low = 0, high = low+size-1; high &lt; s.length(); low++, high++) &#123; if (shrinkCheckPalindrome(s,low,high)) &#123; return s.substring(low,high+1); &#125; &#125; &#125; return s.substring(0,1); &#125; public boolean shrinkCheckPalindrome(String s, int low, int high) &#123; while (low &lt;= high) &#123; if (s.charAt(low) == s.charAt(high)) &#123; low++; high--; &#125; else &#123; return false; &#125; &#125; return true; &#125;&#125; 二、动态规划根据回文的特性，一个大回文按比例缩小后的字符串也必定是回文，比如ABCCBA，那BCCB肯定也是回文。所以我们可以根据动态规划的两个特点：第一大问题拆解为小问题，第二重复利用之前的计算结果，来解答这道题。那如何划分小问题呢，我们可以先把所有长度最短为1的子字符串计算出来，根据起始位置从左向右，这些必定是回文。然后计算所有长度为2的子字符串，再根据起始位置从左向右。到长度为3的时候，我们就可以利用上次的计算结果：如果中心对称的短字符串不是回文，那长字符串也不是，如果短字符串是回文，那就要看长字符串两头是否一样。这样，一直到长度最大的子字符串，我们就把整个字符串集穷举完了，但是由于使用动态规划，使计算时间从$O(N^3)$减少到$O(n^2)$。 复杂度：时间 $O(n^2) $空间 $O(n^2)$ 12345678910111213141516class Solution &#123; public String longestPalindrome(String s) &#123; int n = s.length(); String res = null; boolean[][] dp = new boolean[n][n]; for (int i = n - 1; i &gt;= 0; i--) &#123; for (int j = i; j &lt; n; j++) &#123; dp[i][j] = s.charAt(i) == s.charAt(j) &amp;&amp; (j - i &lt; 3 || dp[i + 1][j - 1]); if (dp[i][j] &amp;&amp; (res == null || j - i + 1 &gt; res.length())) &#123; res = s.substring(i, j + 1); &#125; &#125; &#125; return res; &#125;&#125; 三、中心扩散法动态规划虽然优化了时间，但也浪费了空间。实际上我们并不需要一直存储所有子字符串的回文情况，我们需要知道的只是中心对称的较小一层是否是回文。所以如果我们从小到大连续以某点为个中心的所有子字符串进行计算，就能省略这个空间。 这种解法中，外层循环遍历的是子字符串的中心点，内层循环则是从中心扩散，一旦不是回文就不再计算其他以此为中心的较大的字符串。由于中心对称有两种情况，一是奇数个字母以某个字母对称，而是偶数个字母以两个字母中间为对称，所以我们要分别计算这两种对称情况。 复杂度：时间 O(n^2) 空间 O(1) 12345678910111213141516171819202122232425public class Solution &#123; private int max = 0; private String res = &quot;&quot;; public String longestPalindrome(String s) &#123; if (s.length() == 1) &#123; return s; &#125; for (int i = 0; i &lt; s.length()-1; i++) &#123; checkPalindromeExpand(s,i,i); checkPalindromeExpand(s,i,i+1); &#125; return res; &#125; public void checkPalindromeExpand(String s, int low, int high) &#123; while (low &gt;= 0 &amp;&amp; high &lt; s.length()) &#123; if (s.charAt(low) == s.charAt(high)) &#123; if (high - low + 1 &gt; max) &#123; max = high - low + 1; res = s.substring(low,high+1); &#125; low--; high++; &#125; else &#123; return; &#125; &#125; &#125;&#125; 四、马拉车算法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Solution &#123; public String longestPalindrome(String s) &#123; if(s.length()&lt;=1)&#123; return s; &#125; // 预处理字符串，避免奇偶问题 String str = preProcess(s); // idx是当前能够向右延伸的最远的回文串中心点，随着迭代而更新 // max是当前最长回文串在总字符串中所能延伸到的最右端的位置 // maxIdx是当前已知的最长回文串中心点 // maxSpan是当前已知的最长回文串向左或向右能延伸的长度 int idx = 0, max = 0; int maxIdx = 0; int maxSpan = 0; int[] p = new int[str.length()]; for(int curr = 1; curr &lt; str.length(); curr++)&#123; // 找出当前下标相对于idx的对称点 int symmetryOfCurr = 2 * idx - curr; // 如果当前已知延伸的最右端大于当前下标，我们可以用对称点的P值，否则记为1等待检查 p[curr] = max &gt; curr? Math.min(p[symmetryOfCurr], max - curr):1; // 检查并更新当前下标为中心的回文串最远延伸的长度 while((curr+p[curr])&lt;str.length() &amp;&amp; str.charAt(curr+p[curr])==str.charAt(curr-p[curr]))&#123; p[curr]++; &#125; // 检查并更新当前已知能够延伸最远的回文串信息 if(curr+p[curr]&gt;max)&#123; max = p[curr] + curr; idx = curr; &#125; // 检查并更新当前已知的最长回文串信息 if(p[curr]&gt;maxSpan)&#123; maxSpan = p[curr]; maxIdx = curr; &#125; &#125; //去除占位符 return s.substring((maxIdx-maxSpan)/2,(maxSpan+maxIdx)/2-1); &#125; // 预处理，如ABC,变为$#A#B#C# private String preProcess(String s)&#123; StringBuilder sb = new StringBuilder(); sb.append("$"); for(int i = 0; i &lt; s.length(); i++)&#123; sb.append("#"); sb.append(s.charAt(i)); &#125; sb.append("#"); return sb.toString(); &#125;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>最长回文子串</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（9）：最长公共子序列和最长公共子串]]></title>
    <url>%2F2017%2F08%2F08%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%889%EF%BC%89%EF%BC%9A%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97%E5%92%8C%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[一、最长公共子序列（LCS）求最长公共子序列的数目，注意这里的子序列可以不是连续序列，务必问清楚题意。求『最长』类的题目往往与动态规划有点关系，这里是两个字符串，故应为双序列动态规划。 这道题的状态很容易找，不妨先试试以$f[i][j]$表示字符串 A 的前 i 位和字符串 B 的前 j 位的最长公共子序列数目，那么接下来试试寻找其状态转移方程。 从实际例子ABCD和EDCA出发，首先初始化f的长度为字符串长度加1，那么有$f[0][0] = 0, f[0][] = 0, f[][0] = 0$,最后应该返回$f[lenA][lenB]$. 即 f 中索引与字符串索引对应(字符串索引从1开始算起)，那么在A 的第一个字符与 B 的第一个字符相等时，$f[1][1] = 1 + f[0][0]$, 否则$f[1][1] = max(f[0][1], f[1][0])$。 推而广之，也就意味着若$A[i] == B[j]$, 则分别去掉这两个字符后，原 LCS 数目减一，那为什么一定是1而不是0或者2呢？因为不管公共子序列是以哪个字符结尾，在$A[i] == B[j]$时 LCS 最多只能增加1. 而在$A[i] != B[j]$时，由于A[i] 或者 B[j] 不可能同时出现在最终的 LCS 中，故这个问题可进一步缩小，$f[i][j] = max(f[i - 1][j], f[i][j - 1])$. 需要注意的是这种状态转移方程只依赖最终的 LCS 数目，而不依赖于公共子序列到底是以第几个索引结束。 123456789101112131415161718192021222324252627public class Demo &#123; public static int longestCommonSubsequence(String A, String B) &#123; if (A == null || A.length() == 0) return 0; if (B == null || B.length() == 0) return 0; int lenA = A.length(); int lenB = B.length(); int[][] lcs = new int[1 + lenA][1 + lenB]; for (int i = 1; i &lt; 1 + lenA; i++) &#123; for (int j = 1; j &lt; 1 + lenB; j++) &#123; if (A.charAt(i - 1) == B.charAt(j - 1)) &#123; lcs[i][j] = 1 + lcs[i - 1][j - 1]; &#125; else &#123; lcs[i][j] = Math.max(lcs[i - 1][j], lcs[i][j - 1]); &#125; &#125; &#125; return lcs[lenA][lenB]; &#125; public static void main(String[] args) &#123; String source = "zhanghua"; String target = "zhanghau"; System.out.println("longestCommonSubsequence=" + longestCommonSubsequence(source, target)); &#125;&#125; 二、最长公共子串2.1 简单考虑可以使用两根指针索引分别指向两个字符串的当前遍历位置，若遇到相等的字符时则同时向后移动一位。 1234567891011121314151617181920212223242526272829public class Demo &#123; public static int longestCommonSubstring(String A, String B) &#123; if (A == null || A.length() == 0) return 0; if (B == null || B.length() == 0) return 0; int lenA = A.length(); int lenB = B.length(); int lcs = 0, lcs_temp = 0; for (int i = 0; i &lt; lenA; ++i) &#123; for (int j = 0; j &lt; lenB; ++j) &#123; lcs_temp = 0; while ((i + lcs_temp &lt; lenA) &amp;&amp; (j + lcs_temp &lt; lenB) &amp;&amp; (A.charAt(i + lcs_temp) == B.charAt(j + lcs_temp))) &#123; ++lcs_temp; &#125; if (lcs_temp &gt; lcs) &#123; lcs = lcs_temp; &#125; &#125; &#125; return lcs; &#125; public static void main(String[] args) &#123; String source = "zhanghua"; String target = "zhanghau"; System.out.println("longestCommonString=" + longestCommonSubstring(source, target)); &#125;&#125; 2.2 动态规划把$D[i][j] $定义为：两个string的前i个和前j个字符串，尾部连到最后的最长子串。 然后$D[i][j] = $ $i = 0 || j = 0 : 0$ $s1.char[i - 1] = s2.char[j - 1] ? D[i-1][j-1] + 1 : 0;$ 另外，创建一个max的缓存，不段更新即可。 1234567891011121314151617181920212223242526272829303132public class Demo &#123; public static int longestCommonSubstring(String A, String B) &#123; if (A == null || A.length() == 0) return 0; if (B == null || B.length() == 0) return 0; int lenA = A.length(); int lenB = B.length(); int[][] D = new int[lenA + 1][lenB + 1]; int max = 0; for (int i = 0; i &lt;= lenA; i++) &#123; for (int j = 0; j &lt;= lenB; j++) &#123; if (i == 0 || j == 0) &#123; D[i][j] = 0; &#125; else &#123; if (A.charAt(i - 1) == B.charAt(j - 1)) &#123; D[i][j] = D[i - 1][j - 1] + 1; &#125; else &#123; D[i][j] = 0; &#125; &#125; max = Math.max(max, D[i][j]); &#125; &#125; return max; &#125; public static void main(String[] args) &#123; String source = "zhanghua"; String target = "zhanghau"; System.out.println("longestCommonString=" + longestCommonSubstring(source, target)); &#125;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>最长公共子序列</tag>
        <tag>最长公共子串</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（8）：KMP算法]]></title>
    <url>%2F2017%2F08%2F08%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%888%EF%BC%89%EF%BC%9AKMP%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[KMP算法是三位大牛：D.E.Knuth、J.H.Morris和V.R.Pratt同时发现的。其中第一位就是《计算机程序设计艺术》的作者！ KMP算法要解决的问题就是在字符串（也叫主串）中的模式（pattern）定位问题。说简单点就是我们平时常说的关键字搜索。模式串就是关键字（接下来称它为P），如果它在一个主串（接下来称为T）中出现，就返回它的具体位置，否则返回-1（常用手段）。 首先，对于这个问题有一个很单纯的想法：从左到右一个个匹配，如果这个过程中有某个字符不匹配，就跳回去，将模式串向右移动一位。这有什么难的？ 我们可以这样初始化： 之后我们只需要比较i指针指向的字符和j指针指向的字符是否一致。如果一致就都向后移动，如果不一致，如下图： A和E不相等，那就把i指针移回第1位（假设下标从0开始），j移动到模式串的第0位，然后又重新开始这个步骤： 12345678910111213141516171819202122public class Demo&#123; public static int bf(String ts,String ps) &#123; char[] t = ts.toCharArray(); char[] p = ps.toCharArray(); int i = 0; // 主串的位置 int j = 0; // 模式串的位置 while (i &lt; t.length &amp;&amp; j &lt; p.length) &#123; if (t[i] == p[j]) &#123; // 当两个字符相同，就比较下一个 i++; j++; &#125;else &#123; i = i - j + 1; // 一旦不匹配，i后退 j = 0; // j归0 &#125; &#125; if (j == p.length) &#123; return i - j; &#125;else &#123; return -1; &#125; &#125;&#125; 上面的程序是没有问题的，但不够好！如果是人为来寻找的话，肯定不会再把i移动回第1位，因为主串匹配失败的位置前面除了第一个A之外再也没有A了，我们为什么能知道主串前面只有一个A？因为我们已经知道前面三个字符都是匹配的！（这很重要）。移动过去肯定也是不匹配的！有一个想法，i可以不动，我们只需要移动j即可，如下图： 上面的这种情况还是比较理想的情况，我们最多也就多比较了再次。但假如是在主串“SSSSSSSSSSSSSA”中查找“SSSSB”，比较到最后一个才知道不匹配，然后i回溯，这个的效率是显然是最低的。 大牛们是无法忍受“暴力破解”这种低效的手段的，于是他们三个研究出了KMP算法。其思想就如同我们上边所看到的一样：“利用已经部分匹配这个有效信息，保持i指针不回溯，通过修改j指针，让模式串尽量地移动到有效的位置。” 所以，整个KMP的重点就在于当某一个字符与主串不匹配时，我们应该知道j指针要移动到哪？ 接下来我们自己来发现j的移动规律： 如图：C和D不匹配了，我们要把j移动到哪？显然是第1位。为什么？因为前面有一个A相同啊： 如下图也是一样的情况： 可以把j指针移动到第2位，因为前面有两个字母是一样的： 至此我们可以大概看出一点端倪，当匹配失败时，j要移动的下一个位置k。存在着这样的性质：最前面的k个字符和j之前的最后k个字符是一样的。 如果用数学公式来表示是这样的 P[0，k-1] == P[j-k， j-1]弄明白了这个就应该可能明白为什么可以直接将j移动到k位置了。 因为: 1234567当T[i] != P[j]时有T[i-j ~ i-1] == P[0 ~ j-1]由P[0 ~ k-1] == P[j-k ~ j-1]必然：T[i-k ~ i-1] == P[0 ~ k-1] 公式很无聊，能看明白就行了，不需要记住。 这一段只是为了证明我们为什么可以直接将j移动到k而无须再比较前面的k个字符。 好，接下来就是重点了，怎么求这个（这些）k呢？因为在P的每一个位置都可能发生不匹配，也就是说我们要计算每一个位置j对应的k，所以用一个数组next来保存，next[j] = k，表示当T[i] != P[j]时，j指针的下一个位置。 很多教材或博文在这个地方都是讲得比较含糊或是根本就一笔带过，甚至就是贴一段代码上来，为什么是这样求？怎么可以这样求？根本就没有说清楚。而这里恰恰是整个算法最关键的地方。 1234567891011121314151617public class Demo&#123; public static int[] getNext(String ps) &#123; char[] p = ps.toCharArray(); int[] next = new int[p.length]; next[0] = -1; int j = 0; int k = -1; while (j &lt; p.length - 1) &#123; if (k == -1 || p[j] == p[k]) &#123; next[++j] = ++k; &#125; else &#123; k = next[k]; &#125; &#125; return next; &#125;&#125; 这个版本的求next数组的算法应该是流传最广泛的，代码是很简洁。可是真的很让人摸不到头脑，它这样计算的依据到底是什么？好，先把这个放一边，我们自己来推导思路，现在要始终记住一点，next[j]的值（也就是k）表示，当P[j] != T[i]时，j指针的下一步移动位置。先来看第一个：当j为0时，如果这时候不匹配，怎么办？像上图这种情况，j已经在最左边了，不可能再移动了，这时候要应该是i指针后移。所以在代码中才会有next[0] = -1;这个初始化。如果是当j为1的时候呢？ 显然，j指针一定是后移到0位置的。因为它前面也就只有这一个位置了~~~下面这个是最重要的，请看如下图： 请仔细对比这两个图。我们发现一个规律： 12当P[k] == P[j]时，有next[j+1] == next[j] + 1 其实这个是可以证明的： 123因为在P[j]之前已经有P[0 ~ k-1] == p[j-k ~ j-1]。（next[j] == k）这时候现有P[k] == P[j]，我们是不是可以得到P[0 ~ k-1] + P[k] == p[j-k ~ j-1] + P[j]。即：P[0 ~ k] == P[j-k ~ j]，即next[j+1] == k + 1 == next[j] + 1。 这里的公式不是很好懂，还是看图会容易理解些。那如果P[k] != P[j]呢？比如下图所示：像这种情况，如果你从代码上看应该是这一句：k = next[k];为什么是这样子？你看下面应该就明白了。 现在你应该知道为什么要k = next[k]了吧！像上边的例子，我们已经不可能找到[ A，B，A，B ]这个最长的后缀串了，但我们还是可能找到[ A，B ]、[ B ]这样的前缀串的。所以这个过程像不像在定位[ A，B，A，C ]这个串，当C和主串不一样了（也就是k位置不一样了），那当然是把指针移动到next[k]啦。 在P[K]!=P[j]时，我们已经知道（0，k-1）串和（j-k,j-1）串是相等的，所以可以把（0，k-1）串当做一个新的模式串，发现在新模式串中（0，next[k]-1）串与（k-next[k],k-1）串相等，所以（j-k,j-1）中存在与（0，next[k]-1）相等的串，所以可以把j移动到next[k],继续比较和移动。 有了next数组之后就一切好办了，我们可以动手写KMP算法了： 12345678910111213141516171819202122public static int KMP(String ts, String ps) &#123; char[] t = ts.toCharArray(); char[] p = ps.toCharArray(); int i = 0; // 主串的位置 int j = 0; // 模式串的位置 int[] next = getNext(ps); while (i &lt; t.length &amp;&amp; j &lt; p.length) &#123; if (j == -1 || t[i] == p[j]) &#123; // 当j为-1时，要移动的是i，当然j也要归0 i++; j++; &#125; else &#123; // i不需要回溯了 // i = i - j + 1; j = next[j]; // j回到指定位置 &#125; &#125; if (j == p.length) &#123; return i - j; &#125; else &#123; return -1; &#125;&#125; 和暴力破解相比，就改动了4个地方。其中最主要的一点就是，i不需要回溯了。 最后，来看一下上边的算法存在的缺陷。来看第一个例子： 显然，当我们上边的算法得到的next数组应该是[ -1，0，0，1 ] 所以下一步我们应该是把j移动到第1个元素：不难发现，这一步是完全没有意义的。因为后面的B已经不匹配了，那前面的B也一定是不匹配的，同样的情况其实还发生在第2个元素A上。显然，发生问题的原因在于P[j] == P[next[j]]。所以我们也只需要添加一个判断条件即可： 12345678910111213141516171819public static int[] getNext(String ps) &#123; char[] p = ps.toCharArray(); int[] next = new int[p.length]; next[0] = -1; int j = 0; int k = -1; while (j &lt; p.length - 1) &#123; if (k == -1 || p[j] == p[k]) &#123; if (p[++j] == p[++k]) &#123; // 当两个字符相等时要跳过 next[++j] = ++k; &#125; else &#123; next[j] = k;; &#125; &#125;else &#123; k = next[k]; &#125; &#125; return next;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>KMP算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（7）：最短编辑距离]]></title>
    <url>%2F2017%2F08%2F07%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%887%EF%BC%89%EF%BC%9A%E6%9C%80%E7%9F%AD%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[现代搜索技术的发展很多以提供优质、高效的服务作为目标。比如说：baidu、google、sousou等知名全文搜索系统。当我们输入一个错误的query=”Jave” 的时候，返回中有大量包含正确的拼写 “Java”的网页。是怎么做到的呢？这其中，字符串的相似度计算是做到这一点的方法之一。 一、字符串编辑距离是一种字符串之间相似度计算的方法。给定两个字符串S、T，将S转换成T所需要的删除，插入，替换操作的数量就叫做S到T的编辑路径。而最短的编辑路径就叫做字符串S和T的编辑距离。 举个例子：S=“eeba” T=”abac” 我们可以按照这样的步骤转变：(1) 将S中的第一个e变成a;(2) 删除S中的第二个e;(3)在S中最后添加一个c; 那么S到T的编辑路径就等于3。当然，这种变换并不是唯一的，但如果3是所有变换中最小值的话。那么我们就可以说S和T的编辑距离等于3了。 二、动态规划解决编辑距离动态规划(dynamic programming)是一种解决复杂问题最优解的策略。它的基本思路就是：将一个复杂的最优解问题分解成一系列较为简单的最优解问题，再将较为简单的的最优解问题进一步分解，直到可以一眼看出最优解为止。 动态规划算法是解决复杂问题最优解的重要算法。其算法的难度并不在于算法本身的递归难以实现，而主要是编程者对问题本身的认识是否符合动态规划的思想。现在我们就来看看动态规划是如何解决编辑距离的。 假设$dp[i-1][j-1]$表示一个长为$i-1$的字符串$str1$变为长为$j-1$的字符串str2的最短距离，如果我们此时想要把$str1a$这个字符串变成$str2b$这个字符串，我们有如下几种选择： 替换： 在str1变成str2的步骤后，我们将str1a中的a替换为b，就得到str2b (如果a和b相等，就不用操作) 增加： 在str1a变成str2的步骤后，我们再在末尾添加一个b，就得到str2b (str1a先根据已知距离变成str2，再加个b) 删除： 在str1变成str2b的步骤后，对于str1a，我们将末尾的a删去，就得到str2b (str1a将a删去得到str1，而str1到str2b的编辑距离已知) 根据这三种操作，我们可以得到递推式 若a和b相等： 1dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]) 若a和b不相等： 1dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+1) 因为将一个非空字符串变成空字符串的最小操作数是字母个数（全删），反之亦然，所以： 1dp[0][j]=j, dp[i][0]=i 最后我们只要返回dp[m][n]即可，其中m是word1的长度，n是word2的长度 三、代码12345678910111213141516171819202122232425262728293031public class Demo &#123; public static int minDistance(String word1, String word2) &#123; int m = word1.length(), n = word2.length(); int[][] dp = new int[m + 1][n + 1]; // 初始化空字符串的情况 for(int i = 1; i &lt;= m; i++)&#123; dp[i][0] = i; &#125; for(int i = 1; i &lt;= n; i++)&#123; dp[0][i] = i; &#125; for(int i = 1; i &lt;= m; i++)&#123; for(int j = 1; j &lt;= n; j++)&#123; // 增加操作：str1a变成str2后再加上b，得到str2b int insertion = dp[i][j-1] + 1; // 删除操作：str1a删除a后，再由str1变为str2b int deletion = dp[i-1][j] + 1; // 替换操作：先由str1变为str2，然后str1a的a替换为b，得到str2b int replace = dp[i-1][j-1] + (word1.charAt(i - 1) == word2.charAt(j - 1) ? 0 : 1); // 三者取最小 dp[i][j] = Math.min(replace, Math.min(insertion, deletion)); &#125; &#125; return dp[m][n]; &#125; public static void main(String[] args) &#123; String source = &quot;zhanghua&quot;; String target = &quot;zhanghau&quot;; System.out.println(&quot;minDistance=&quot; + minDistance(source, target)); &#125;&#125; 测试结果为： 1minDistance=2]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>最短编辑距离</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（6）：重点掌握]]></title>
    <url>%2F2017%2F08%2F06%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%886%EF%BC%89%EF%BC%9A%E9%87%8D%E7%82%B9%E6%8E%8C%E6%8F%A1%2F</url>
    <content type="text"><![CDATA[最基础的数据结构与算法java实现。3 一、排序排序面试题： 实现快速排序以及时空复杂度分析 实现归并排序以及时空复杂度分析 实现堆排序以及时空复杂度分析 1.1 归并排序归并排序是典型的二路合并排序，将原始数据集分成两部分(不一定能够均分)，分别对它们进行排序，然后将排序后的子数据集进行合并，典型的分治法策略。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class MergeSortTest &#123; public static void main(String[] args) &#123; int[] data = new int[] &#123; 5, 3, 6, 2, 1, 9, 4, 8, 7 &#125;; print(data); mergesort(data); System.out.println("排序后的数组："); print(data); &#125; public static void mergesort(int[] arr)&#123; sort(arr, 0, arr.length-1); &#125; private static void sort(int[] a, int left, int right)&#123; //当left==right的时，已经不需要再划分了 if (left&lt;right)&#123; int middle = (left+right)/2; sort(a, left, middle); //左子数组 sort(a, middle+1, right); //右子数组 merge(a, left, middle, right); //合并两个子数组 &#125; &#125; // 合并两个有序子序列 arr[left, ..., middle] 和 arr[middle+1, ..., right]。temp是辅助数组。 private static void merge(int arr[], int left, int middle, int right)&#123; int[] temp = new int[right - left + 1]; int i=left; int j=middle+1; int k=0; //将记录由小到大地放进temp数组 while ( i&lt;=middle &amp;&amp; j&lt;=right)&#123; if (arr[i] &lt;=arr[j])&#123; temp[k++] = arr[i++]; &#125; else&#123; temp[k++] = arr[j++]; &#125; &#125; while (i &lt;=middle)&#123; temp[k++] = arr[i++]; &#125; while ( j&lt;=right)&#123; temp[k++] = arr[j++]; &#125; //把数据复制回原数组 for (i=0; i&lt;k; ++i)&#123; arr[left+i] = temp[i]; &#125; &#125; public static void print(int[] data) &#123; for (int i = 0; i &lt; data.length; i++) &#123; System.out.print(data[i] + "\t"); &#125; System.out.println(); &#125;&#125; 在合并数组的时候需要一个temp数组。我们当然有足够的理由在每次调用的时候重新new一个数组（例如，减少一个参数），但是，注意到多次的创建数组对象会造成额外的开销，我们可以在开始就创建一个足够大的数组（等于原数组长度就行），以后都使用这个数组。实际上，上面的代码就是这么写的。 时间复杂度：在归并排序中，进行一趟归并需要的关键字比较次数和数据元素移动次数最多为$n$，需要归并的趟数$log n$，故归并排序的时间复杂度为$O(nlog n)$。并且由于归并算法是固定的，不受输入数据影响，所以它在最好、最坏、平均情况下表现几乎相同，均为$O(log n)$。 空间复杂度：归并排序需要长度等于序列长度为$n$的辅助存储单元，故归并排序的空间复杂度为$O(n)$。归并排序最大的缺陷在于其空间复杂度。可不可以省略这个数组呢？不行!如果取消辅助数组而又要保证原来的数组中数据不被覆盖，那就必须要在数组中花费大量时间来移动数据。不仅容易出错，还降低了效率。因此这个辅助空间是少不掉的。 稳定性：因为我们在遇到相等的数据的时候必然是按顺序“抄写”到辅助数组上的，所以，归并排序是稳定的排序算法。 1.2 快速排序快速排序是图灵奖得主C.R.A Hoare于1960年提出的一种划分交换排序。它采用了一种分治的策略，通常称其为分治法（Divide-and-Conquer Method） 分治法的基本思想是：将原问题分解为若干个规模更小但结构与原问题相似的子问题。递归地解这些子问题，然后将这些子问题组合为原问题的解。 利用分治法可将快速排序分为三步： 从数列中挑出一个元素作为“基准”（pivot）。 分区过程，将比基准数大的放到右边，小于或等于它的数都放到左边。这个操作称为“分区操作”，分区操作结束后，基准元素所处的位置就是最终排序后它的位置 再对“基准”左右两边的子集不断重复第一步和第二步，直到所有子集只剩下一个元素为止。 1234567891011121314151617181920212223242526272829303132333435363738public class quickSortTest &#123; public static void main(String[] args) &#123; int[] data = new int[] &#123; 5, 3, 6, 2, 1, 9, 4, 8, 7 &#125;; print(data); quickSort(data); System.out.println("排序后的数组："); print(data); &#125; public static void quickSort(int[] arr)&#123; qsort(arr, 0, arr.length-1); &#125; private static void qsort(int[] arr, int left, int right)&#123; if (left &lt; right)&#123; int pivot=partition(arr, left, right); //将数组分为两部分 qsort(arr, left, pivot-1); //递归排序左子数组 qsort(arr, pivot+1, right); //递归排序右子数组 &#125; &#125; private static int partition(int[] arr, int left, int right)&#123; int pivot = arr[left]; //基准记录 while (left&lt;right)&#123; while (left&lt;right &amp;&amp; arr[right]&gt;=pivot) --right; arr[left]=arr[right]; //交换比基准小的记录到左端 while (left&lt;right &amp;&amp; arr[left]&lt;=pivot) ++left; arr[right] = arr[left]; //交换比基准大的记录到右端 &#125; //扫描完成，基准到位 arr[left] = pivot; //返回的是基准的位置 return left; &#125; public static void print(int[] data) &#123; for (int i = 0; i &lt; data.length; i++) &#123; System.out.print(data[i] + "\t"); &#125; System.out.println(); &#125;&#125; 二、查找2.1 二分查找 1234567891011121314151617181920int binary_search(int array[],int n,int value) &#123; int left=0; int right=n-1; while (left&lt;=right) &#123; int middle=left + ((right-left)&gt;&gt;1); if (array[middle]&gt;value) &#123; right =middle-1; //right赋值，适时而变 &#125; else if(array[middle]&lt;value) &#123; left=middle+1; &#125; else return middle; &#125; return -1; &#125; 三、二叉树这块内容讨论二叉树的常见遍历方式的代码（java）实现，包括前序（preorder）、中序（inorder）、后序（postorder）、层序（levelorder），进一步考虑递归和非递归的实现方式。 递归的实现方法相对简单，但由于递归的执行方式每次都会产生一个新的方法调用栈，如果递归层级较深，会造成较大的内存开销，相比之下，非递归的方式则可以避免这个问题。递归遍历容易实现，非递归则没那么简单，非递归调用本质上是通过维护一个栈，模拟递归调用的方法调用栈的行为。 在此之前，先简单定义节点的数据结构： 二叉树节点最多只有两个儿子，并保存一个节点的值，为了实验的方便，假定它为 int。同时，我们直接使用 Java 的 System.out.print 方法来输出节点值，以显示遍历结果。 1234567891011class Node&#123; public int value; public Node left; public Node right; public Node(int v)&#123; this.value=v; this.left=null; this.right=null; &#125; &#125; 3.1 前序遍历3.1.1 递归实现递归实现很简单，在每次访问到某个节点时，先输出节点值，然后再依次递归的对左儿子、右儿子调用遍历的方法。代码如下 java 1234567public void preOrder(Node root)&#123; if(root!=null)&#123; System.out.print(root.value); preOrder(root.left); preOrder(root.right); &#125;&#125; 3.1.2 非递归实现利用栈实现循环先序遍历二叉树，维护一个栈，将根节点入栈，只要栈不为空，出栈并访问，接着依次将访问节点的右节点、左节点入栈。这种方式是对先序遍历的一种特殊实现，简洁明了，但是不具备很好地扩展性，在中序和后序方式中不适用。 1234567891011public void preOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; stack = new Stack&lt;Node&gt;(); stack.push(root); while(!stack.isEmpty)&#123; Node temp = stack.pop(); System.out.print(temp.value); if(temp.right!=null)stack.push(temp.right); if(temp.left!=null)stack.push(temp.left); &#125;&#125; 还有一种方式就是利用栈模拟递归过程实现循环先序遍历二叉树。这种方式具备扩展性，它模拟了递归的过程，将左子树不断的压入栈，直到null，然后处理栈顶节点的右子树。 java 12345678910111213public void preOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; s = new Stack&lt;Node&gt;(); while(root!=null||!s.isEmtpy())&#123; while(root!=null)&#123; System.out.print(root.value);、//先访问 s.push(root);//再入栈 root = root.left; &#125; root = s.pop(); root = root.right;//如果是null，出栈并处理右子树 &#125;&#125; 3.2 中序遍历3.2.1 递归实现1234567public void inOrder(Node root)&#123; if(root!=null)&#123; preOrder(root.left); System.out.print(root.value); preOrder(root.right); &#125;&#125; 3.2.2 非递归实现利用栈模拟递归过程实现循环中序遍历二叉树。跟前序遍历的非递归实现方法二很类似。唯一的不同是访问当前节点的时机：前序遍历在入栈前访问，而中序遍历在出栈后访问。 java 12345678910111213public void inOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; s = Stack&lt;Node&gt;(); while(root!=null||s.isEmpty())&#123; while(root!=null)&#123; s.push(root); root=root.left; &#125; root = s.pop(root); System.out.print(root.value); root = root.right; &#125;&#125; 3.3 后序遍历3.3.1 递归实现1234567public void inOrder(Node root)&#123; if(root!=null)&#123; preOrder(root.left); preOrder(root.right); System.out.print(root.value); &#125;&#125; 3.3.2 非递归实现1234567891011121314151617public void postOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; s1 = new Stack&lt;Node&gt;(); Stack&lt;Node&gt; s2 = new Stack&lt;Node&gt;(); Node node = root; s1.push(node); while(s1!=null)&#123;//这个while循环的功能是找出后序遍历的逆序，存在s2里面 node = s1.pop(); if(node.left!=null) s1.push(node.left); if(node.right!=null)s1.push(node.right); s2.push(node); &#125; while(s2!=null)&#123;//将s2中的元素出栈，即为后序遍历次序 node = s2.pop(); System.out.print(node.value); &#125;&#125; 3.4 层序遍历1234567891011public static void levelTravel(Node root)&#123; if(root==null)return; Queue&lt;Node&gt; q=new LinkedList&lt;Node&gt;(); q.add(root); while(!q.isEmpty())&#123; Node temp = q.poll(); System.out.println(temp.value); if(temp.left!=null)q.add(temp.left); if(temp.right!=null)q.add(temp.right); &#125; &#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（5）：剑指offer解题报告]]></title>
    <url>%2F2017%2F08%2F05%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%885%EF%BC%89%EF%BC%9A%E5%89%91%E6%8C%87offer%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[剑指offer编程题java实现整理。 3. 二维数组中的查找（数组）在一个二维数组中，每一行都按照从左到右的递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一维数组和一个整数，判断数组中是否含有该整数。 首先选取数组中右上角的数字，如果该数字等于我们要查找的数组，查找过程结束；如果该数字大于要查找的数组，剔除这个数字所在的列；如果该数字小于要查找的数组，剔除这个数字所在的行。也就是说如果要查找的数字不再数组的右上角，则每一次都在数组的查找范围中剔除一行或者一列，这样每一步都可以缩小查找的范围，直到找到要查找的数字，或者查找范围为空。 java 12345678910111213141516171819public class Solution &#123; public boolean Find(int target, int [][] array) &#123; int row = 0; int col = array[0].length - 1; while(row&lt;=array.length-1&amp;&amp;col&gt;=0)&#123; if(target == array[row][col])&#123; return true; &#125; else if(target&gt;array[row][col])&#123; row++; &#125; else&#123; col--; &#125; &#125; return false; &#125;&#125; python 123456789101112131415# -*- coding:utf-8 -*-class Solution: # array 二维列表 def Find(self, target, array): # write code here row = 0 col = len(array[0])-1 while row&lt;=len(array)-1 and col&gt;=0: if target==array[row][col]: return True elif target&gt;array[row][col]: row+=1 else: col-=1 return False 也可以把每一行看做是一个递增的序列，利用二分查找。 java 12345678910111213141516171819public class Solution &#123; public boolean Find(int target, int [][] array) &#123; for(int i=0;i&lt;array.length;i++)&#123; int low =0; int high = array[i].length-1; while(low&lt;=high)&#123; int mid = (low+high)/2; if(array[i][mid]==target) return true; else if(array[i][mid]&gt;target) high =mid-1; else low=mid+1; &#125; &#125; return false; &#125;&#125; 4. 替换空格（字符串）请实现一个函数，把字符串中的每个空格替换成”%20”。例如输入“We are happy.”，则输出“We%20are20happy” 网络编程中，要把特殊符号转换成服务器可识别的字符。转换的规则是在“%”后面跟上ASCII码的两位十六进制的表示。比如空格的ASCII码是32，即十六进制的0X20，因此空格被替换成“%20”。 问题1：替换字符串，是在原来的字符串上做替换，还是新开辟一个字符串做替换！问题2：在当前字符串替换，怎么替换才更有效率（不考虑java里现有的replace方法）。从前往后替换，后面的字符要不断往后移动，要多次移动，所以效率低下；从后往前，先计算需要多少空间，然后从后往前移动，则每个字符只为移动一次，这样效率更高一点。 123456789101112131415161718192021222324public class Solution &#123; public String replaceSpace(StringBuffer str) &#123; int spacenum = 0;//spacenum为计算空格数 for(int i=0;i&lt;str.length();i++)&#123; if(str.charAt(i)==' ') spacenum++; &#125; int indexold = str.length()-1;//indexold为为替换前的str下标 int newlength = str.length()+2*spacenum;//计算空格转换成%20之后的str长度 int indexnew = newlength-1;//indexold为为把空格替换为%20后的str下标 str.setLength(newlength);//使str的长度扩大到转换成%20之后的长度,防止下标越界,setLength方法 for(;indexold&gt;=0&amp;&amp;indexold&lt;newlength;--indexold)&#123; if(str.charAt(indexold)==' ')&#123;//charAt方法 str.setCharAt(indexnew--,'0'); str.setCharAt(indexnew--,'2'); str.setCharAt(indexnew--,'%'); &#125; else&#123; str.setCharAt(indexnew--,str.charAt(indexold)); &#125; &#125; return str.toString(); &#125;&#125; 5. 从尾到头打印链表（链表）输入一个链表的头结点，从尾到头反过来打印每个结点的值（注意不能改变链表的结构）。 解决这个问题肯定要遍历链表。遍历的顺序是从头到尾的顺序，可输出的顺序却是从尾到头。也就是说第一个遍历到的结点最后一个输出，而最后一个遍历到的结点第一个输出。这就是典型的“后进先出”，我们可以用栈实现这种顺序。没经过一个节点的时候，把该结点放到一个栈中。当遍历完整个链表后，再从栈顶开始逐个输出结点的值，此时输出的结点的顺序就翻转过来了。实现代码如下： 12345678910111213141516import java.util.Stack;import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); while (listNode!=null)&#123; stack.push(listNode.val); listNode = listNode.next; &#125; ArrayList&lt;Integer&gt; List = new ArrayList&lt;&gt;(); while(!stack.isEmpty())&#123; List.add(stack.pop()); &#125; return List; &#125;&#125; 既然想到了用栈来实现这个函数，而递归在本质上就是一个栈结构，因此可用递归来实现。要实现反过来输出链表，我们每访问到一个节点的时候，先递归输出它后面的结点，再输出该结点自身，这样链表的输出结果就反过来了。实现代码如下： 12345678910111213141516import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; ArrayList&lt;Integer&gt; list=new ArrayList&lt;Integer&gt;(); ListNode pNode=listNode; if(pNode!=null)&#123; if(pNode.next!=null)&#123; list=printListFromTailToHead(pNode.next); &#125; list.add(pNode.val); &#125; return list; &#125;&#125; 6. 重建二叉树（二叉树）输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。 例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 根据前序遍历的特点，我们知道根结点为1 观察中序遍历。其中root节点G左侧的472必然是root的左子树，G右侧的5386必然是root的右子树。 观察左子树472，左子树的中的根节点必然是大树的root的leftchild。在前序遍历中，大树的root的leftchild位于root之后，所以左子树的根节点为2。 同样的道理，root的右子树节点5386中的根节点也可以通过前序遍历求得。在前序遍历中，一定是先把root和root的所有左子树节点遍历完之后才会遍历右子树，并且遍历的左子树的第一个节点就是左子树的根节点。同理，遍历的右子树的第一个节点就是右子树的根节点。 观察发现，上面的过程是递归的。先找到当前树的根节点，然后划分为左子树，右子树，然后进入左子树重复上面的过程，然后进入右子树重复上面的过程。最后就可以还原一棵树了。 该步递归的过程可以简洁表达如下： 确定根,确定左子树，确定右子树。 在左子树中递归。 在右子树中递归。 打印当前根。 递归代码如下： 123456789101112131415161718192021public class Solution &#123; public TreeNode reConstructBinaryTree(int [] pre,int [] in) &#123; return reConBTree(pre,0,pre.length-1,in,0,in.length-1); &#125; public TreeNode reConBTree(int [] pre,int preleft,int preright,int [] in,int inleft,int inright)&#123; if(preleft &gt; preright || inleft&gt; inright)//当到达边界条件时候返回null return null; //新建一个TreeNode TreeNode root = new TreeNode(pre[preleft]); //对中序数组进行输入边界的遍历 for(int i = inleft; i&lt;= inright; i++)&#123; if(pre[preleft] == in[i])&#123; //重构左子树，注意边界条件 root.left = reConBTree(pre,preleft+1,preleft+i-inleft,in,inleft,i-1); //重构右子树，注意边界条件 root.right = reConBTree(pre,preleft+i+1-inleft,preright,in,i+1,inright); &#125; &#125; return root; &#125;&#125; 7. 用两个栈实现队列（栈与队列）栈是一个非常常见的数据结构，它在计算机领域中被广泛应用，比如操作系统会给每个线程创建一个栈来存储函数调用时各个函数的参数、返回地址及临时变量等。栈的特点是后进先出，即最后被压入（push）栈的元素会第一个被弹出（pop）。 队列是另外一种很重要的数据结构。和栈不同的是，队列的特点是先进先出，即第一个进入队列的元素将会第一个出来。 栈和队列虽然是针锋相对的两个数据结构，但有意思的是他们却相互联系。 通过一个具体的例子来分析往队列插入和删除元素的过程。首先插入一个元素a，不妨先把它插入到stack1，此时stack1中的元素有{a}，stack2为空，再向stack1压入b和c，此时stack1中的元素有{a,b,c}，其中c处于栈顶，而stack2仍然是空的。 因为a是最先进的，最先被删除的元素应该是a，但a位于栈低。我们可以把stack1中的元素逐个弹出并压入stack2，元素在stack2的顺序正好和原来在stack1的顺序相反因此经过三次弹出stack1和压入stack2操作之后，stack1为空，而stack2的元素是{c,b,a}，这时就可以弹出stack2的栈顶a了，随后弹出stack2中的b和c，而这个过程中stack1始终为空. 从上面的分析我们可以总结出删除一个元素的步骤：当stack2中不为空时，在stack2的栈顶元素是最先进入队列的元素，可以弹出。如果stack2为空时，我们把stack1中的元素逐个弹出并压入stack2。由于先进入队列的元素被压到stack1的底端，经过弹出和压入之后就处于stack2的顶端了，又可以直接弹出。 1234567891011121314151617181920import java.util.Stack;public class Solution &#123; Stack&lt;Integer&gt; stack1 = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; stack2 = new Stack&lt;Integer&gt;(); public void push(int node) &#123; stack1.push(node); &#125; public int pop() &#123; while(!stack2.isEmpty())&#123; return stack2.pop(); &#125; while(!stack1.isEmpty())&#123; stack2.push(stack1.pop()); &#125; return stack2.pop(); &#125;&#125; 8. 旋转数组的最小数字(数组)在准备面试的时候，我们应该重点掌握二分查找、归并排序和快速排序，做到能随时正确、完整地写出它们的代码。 若面试题是要求在排序的数组（或部分排序的数组）中查找一个数字或者统计某个数字出现的次数，我们都可以尝试用二分查找算法。 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 可以采用二分法解答这个问题， mid = low + (high - low)/2 ，需要考虑三种情况： array[mid] &gt; array[high]:出现这种情况的array类似[3,4,5,6,0,1,2]，此时最小数字一定在mid的右边。low = mid + 1 array[mid] == array[high]: 出现这种情况的array类似 [1,0,1,1,1] 或者[1,1,1,0,1]，此时最小数字不好判断在mid左边，还是右边,这时只好一个一个试，low = low + 1 或者 high = high - 1 array[mid] &lt; array[high]: 出现这种情况的array类似[2,2,3,4,5,6,6],此时最小数字一定就是array[mid]或者在mid的左边。因为右边必然都是递增的。 high = mid。注意这里有个坑：如果待查询的范围最后只剩两个数，那么mid一定会指向下标靠前的数字，比如 array = [4,6]，array[low] = 4 ;array[mid] = 4 ; array[high] = 6 ; 如果high = mid - 1，就会产生错误， 因此high = mid，但情形(1)中low = mid + 1就不会错误。 代码如下： java 12345678910111213141516171819202122import java.util.ArrayList;public class Solution &#123; public int minNumberInRotateArray(int [] array) &#123; int low = 0; int high = array.length-1; while(low&lt;high)&#123; int mid = low+(high-low)/2; if(array[mid]&gt;array[high])&#123; low=mid+1; &#125; else if(array[mid]==array[high])&#123; high=high-1; &#125; else&#123; high = mid; &#125; &#125; return array[low]; &#125;&#125; 9. 斐波那契数列(数组)9.1 斐波那契数列大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。n&lt;=39。这个题可以说是迭代（Iteration） VS 递归（Recursion），f(n) = f(n-1) + f(n-2)，第一眼看就是递归啊，简直完美的递归环境，递归肯定很爽，这样想着关键代码两三行就搞定了，注意这题的n是从0开始的： 12if(n&lt;=1) return n;else return Fibonacci(n-1)+Fibonacci(n-2); 然而并没有什么用，测试用例里肯定准备着一个超大的n来让Stack Overflow，为什么会溢出？因为重复计算，而且重复的情况还很严重，举个小点的例子，n=4，看看程序怎么跑的： 123Fibonacci(4) = Fibonacci(3) + Fibonacci(2); = Fibonacci(2) + Fibonacci(1) + Fibonacci(1) + Fibonacci(0); = Fibonacci(1) + Fibonacci(0) + Fibonacci(1) + Fibonacci(1) + Fibonacci(0); 由于我们的代码并没有记录Fibonacci(1)和Fibonacci(0)的结果，对于程序来说它每次递归都是未知的，因此光是n=4时f(1)就重复计算了3次之多。 更简单的办法是从下往上计算，首先根据f(0)和f(1)算出f(2)，再根据f(1)和f(2)算出f(3)……依此类推就可以算出第n项了。很容易理解，这种思路的时间复杂度是O(n)。实现代码如下： java 1234567891011121314151617public class Solution &#123; public int Fibonacci(int n) &#123; if(n==0) return 0; if(n==1) return 1; int num1 = 0; int num2 = 1; int fibN=0; for(int i=2;i&lt;=n;++i)&#123; fibN=num1+num2; num1=num2; num2=fibN; &#125; return fibN; &#125;&#125; 9.2 跳台阶一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级台阶总共有多少种跳法。 我们把n级台阶的跳法看成是n的函数，记为f(n)。当n&gt;2时，第一次跳的时候就有两种不同的选择：一是第一次只跳1级，此时跳法数目等于后面剩下的n-1级台阶的跳法数目，即为f(n-1)；另外一种选择是第一次跳2级，此时跳法数目等于后面剩下的n-2级台阶的跳法数目，即为f(n-2)，因此n级台阶的不同跳法的总数f(n)=f(n-1)+f(n-2)。分析到这里，我们不难看出这实际上是斐波那契数列了。代码如下： java 1234567891011121314151617181920public class Solution &#123; public int JumpFloor(int target) &#123; if(target == 0) return 0; if(target == 1) return 1; if(target == 2) return 2; int num1 = 0; int num2 = 1; int jump = 0; for(int i=0;i&lt;target;i++)&#123; jump = num1+num2; num1=num2; num2=jump; &#125; return jump; &#125;&#125; 9.3 变态跳台阶一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 因为n级台阶，第一步有n种跳法：跳1级、跳2级、到跳n级。跳1级，剩下n-1级，则剩下跳法是f(n-1)，跳2级，剩下n-2级，则剩下跳法是f(n-2)。所以f(n)=f(n-1)+f(n-2)+…+f(1)，因为f(n-1)=f(n-2)+f(n-3)+…+f(1)，所以f(n)=2*f(n-1) java 1234567891011public class Solution &#123; public int JumpFloorII(int target) &#123; if(target==0) return 0; if(target==1) return 1; else&#123; return 2*JumpFloorII(target-1); &#125; &#125;&#125; 9.4 矩形覆盖我们可以用$21$的小矩形横着或者竖着去覆盖更大的矩形。请问用n个$21$的小矩形无重叠地覆盖一个$2*n$的大矩形，总共有多少种方法？ 把$28$的覆盖方法记为f(8)。用一个$12$小矩形去覆盖大矩形的最左边有两个选择。竖着放或者横着放。当竖着放时，右边剩下$27$的区域，记为f(7)。横着放时，当$12$的小矩阵横着放在左上角的时候，左下角必须横着放一个$12$的小矩阵，剩下$26$，记为f(6)，因此f(8)=f(7)+f(6)。此时可以看出，仍然是斐波那契数列。 代码如下： java 1234567891011121314151617public class Solution &#123; public int RectCover(int target) &#123; if(target==0) return 0; if(target==1) return 1; int num1=0; int num2=1; int cover =0; for(int i=0;i&lt;target;i++)&#123; cover = num1+num2; num1=num2; num2=cover; &#125; return cover; &#125;&#125; 10. 二进制中1的个数(位运算)输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 如果一个整数不为0，那么这个整数至少有一位是1。如果我们把这个整数减1，那么原来处在整数最右边的1就会变为0，原来在1后面的所有的0都会变成1(如果最右边的1后面还有0的话)。其余所有位将不会受到影响。举个例子：一个二进制数1100，从右边数起第三位是处于最右边的一个1。减去1后，第三位变成0，它后面的两位0变成了1，而前面的1保持不变，因此得到的结果是1011.我们发现减1的结果是把最右边的一个1开始的所有位都取反了。这个时候如果我们再把原来的整数和减去1之后的结果做与运算，从原来整数最右边一个1那一位开始所有位都会变成0。如1100&amp;1011=1000.也就是说，把一个整数减去1，再和原整数做与运算，会把该整数最右边一个1变成0.那么一个整数的二进制有多少个1，就可以进行多少次这样的操作。 java 12345678910public class Solution &#123; public int NumberOf1(int n) &#123; int count =0; while(n!=0)&#123; count++; n=(n-1)&amp;n; &#125; return count; &#125;&#125; 11. 数值的整数次方（位运算）12345678910111213141516171819202122public class Solution &#123; public double Power(double base, int n) &#123; double res = 1,curr = base; int exponent; if(n&gt;0)&#123; exponent = n; &#125;else if(n&lt;0)&#123; if(base==0) throw new RuntimeException(&quot;分母不能为0&quot;); exponent = -n; &#125;else&#123;// n==0 return 1;// 0的0次方 &#125; while(exponent!=0)&#123; if((exponent&amp;1)==1) res*=curr; curr*=curr;// 翻倍 exponent&gt;&gt;=1;// 右移一位 &#125; return n&gt;=0?res:(1/res); &#125;&#125; 12. 打印1到最大的n位数（null）13. 在O(1)时间删除链表结点（链表）给定单向链表的头指针和一个结点指针，定义一个函数在O(1)时间删除该结点。 我们要删除结点i，先把i的下一个结点i.next的内容复制到i，然后在把i的指针指向i.next结点的下一个结点即i.next.next，它的效果刚好是把结点i给删除了。 此外还要考虑删除的结点是头尾结点、链表中只有一个结点、链表为空这几种情况。 java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class DeleteNode &#123; /** * 链表结点 */ public static class ListNode &#123; int value; // 保存链表的值 ListNode next; // 下一个结点 &#125; /** * 给定单向链表的头指针和一个结点指针，定义一个函数在0(1)时间删除该结点, * 【注意1：这个方法和文本上的不一样，书上的没有返回值，这个因为JAVA引用传递的原因， * 如果删除的结点是头结点，如果不采用返回值的方式，那么头结点永远删除不了】 * 【注意2：输入的待删除结点必须是待链表中的结点，否则会引起错误，这个条件由用户进行保证】 * * @param head 链表表的头 * @param toBeDeleted 待删除的结点 * @return 删除后的头结点 */ public static ListNode deleteNode(ListNode head, ListNode toBeDeleted) &#123; // 如果输入参数有空值就返回表头结点 if (head == null || toBeDeleted == null) &#123; return head; &#125; // 如果删除的是头结点，直接返回头结点的下一个结点 if (head == toBeDeleted) &#123; return head.next; &#125; // 下面的情况链表至少有两个结点 // 在多个节点的情况下，如果删除的是最后一个元素 if (toBeDeleted.next == null) &#123; // 找待删除元素的前驱 ListNode tmp = head; while (tmp.next != toBeDeleted) &#123; tmp = tmp.next; &#125; // 删除待结点 tmp.next = null; &#125; // 在多个节点的情况下，如果删除的是某个中间结点 else &#123; // 将下一个结点的值输入当前待删除的结点 toBeDeleted.value = toBeDeleted.next.value; // 待删除的结点的下一个指向原先待删除引号的下下个结点，即将待删除的下一个结点删除 toBeDeleted.next = toBeDeleted.next.next; &#125; // 返回删除节点后的链表头结点 return head; &#125; 14. 调整数组顺序使奇数位于偶数前面（排序） 书上的方法类似于快排，但快排是不稳定的，即其相对位置会发生变化。 java 1234567891011121314151617181920public class Solution &#123; public void reOrderArray(int [] array) &#123; int length = array.length; if(array==null||length==0) return; int left = 0; int right = length-1; while(left&lt;right)&#123; while(left&lt;right&amp;&amp;array[left]%2==1)&#123; left++; &#125; while(left&lt;right&amp;&amp;array[right]%2==0)&#123; right--; &#125; int temp =array[right]; array[right]=array[left]; array[left]=temp; &#125; &#125;&#125; 这里要保证奇数和奇数，偶数和偶数之间的相对位置不变。可以使用插入排序的思想 java 123456789101112131415161718public class Solution &#123; public void reOrderArray(int [] array) &#123; int length = array.length; if(array==null||length==0) return; for(int i=1;i&lt;length;i++)&#123; if(array[i]%2==1)&#123; int curr = array[i]; int j=i-1; while(j&gt;=0&amp;&amp;array[j]%2==0)&#123; array[j+1]=array[j]; j--; &#125; array[j+1]=curr; &#125; &#125; &#125;&#125; 15. 链表中倒数第K个结点（链表）输入一个链表，输出该链表中倒数第k个结点。为了符合大多数人的习惯，本题从1 开始计数，即链表的尾结点是倒数第1个结点。例如一个链表有6个结点，从头结点开始它们的值依次是1、2、3、4、5、6。这个链表的倒数第3个结点的值为4的结点。 很自然的想法是先走到链表尾端，再从尾端回溯k步。可是我们从链表结点的定义可以看出本题中的链表是单向链表，单向链表的结点只有从前向后的指针而没有从后往前的指针，这种思路行不通。 既然不能从尾结点开始遍历链表，我们还是把思路回到头结点上来。假设整个链表有n个结点，那么倒数第k个结点就是从头结点开始往后走n-k+1步就可以了。如何得到结点树n？只需要从头开始遍历链表，每经过一个结点，计数器加1就行了。 也就是说我们需要遍历链表两次，第一次统计出链表中的结点的个数，第二次就能找到倒数第k个结点。但是面试官期待的解法是只需要遍历链表一次。 为了实现只遍历链表一次就能找到倒数第k个结点，我们可以定义两个指针。第一个指针从链表的头指针开始遍历向前走k-1步，第二个指针保持不动；从第k步开始，第二个指针也开始从链表的头指针开始遍历。由于两个指针的距离保持在k-1，当第一个（走在前面的）指针到达链表的尾结点时，第二个指针（走在后边的）指针正好是倒数第k个结点。 但是这样写出来的代码不够鲁棒，面试官可以找出三种办法让这段代码崩溃： 输入的ListHead为空指针。由于代码会试图访问空指针指向的内存，程序崩溃。 输入的以ListHead为头结点的链表的结点总数少于k。由于在for循环中会在链表上向前走k-1步，仍然会由于空指针造成的程序奔溃。 输入的参数k为0.由于k是一个无符号整数，那么在for循环中k-1得到的将不是-1，而是4294967295（无符号的0xFFFFFFFFF），因此for循环执行的次数远远超过我们的预计，同样也会造成程序崩溃。 面试过程中写代码特别要注意鲁棒性，若写出的代码存在多处崩溃的风险，那我们很可能和offer失之交臂。针对前面三个问题，分别处理。若输入的链表头指针为null，那么整个链表为空，此时查找倒数第k个结点自然应该返回null。若输入的k为0，也就是试图查找倒数第0个结点，由于我们计数是从1开始的，因此输入0是没有实际意义，也可以返回null。若链表的结点数少于k，在for循环中遍历链表可能会出现指向null的next，因此我们在for循环中应该加一个if循环。 代码如下： java版本 12345678910111213141516171819202122232425262728293031/*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode FindKthToTail(ListNode head,int k) &#123; if(head==null||k &lt;=0)&#123;return null;&#125; ListNode pAhead = head; ListNode pBehind = head; for(int i=1;i&lt;k;i++)&#123; if(pAhead.next != null) &#123;pAhead = pAhead.next;&#125; else &#123;return null;&#125; &#125; while(pAhead.next!=null) &#123; pAhead = pAhead.next; pBehind = pBehind.next; &#125; return pBehind; &#125;&#125; python版本 12345678910111213141516171819202122# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def FindKthToTail(self, head, k): # write code here if not head or k == 0: return None pAhead = head pBehind = None for i in xrange(0,k-1): if pAhead.next != None: pAhead = pAhead.next else: return None pBehind = head while pAhead.next != None: pAhead = pAhead.next pBehind = pBehind.next return pBehind 16. 反转链表（链表）定义一个函数，输入一个链表的头结点，反转该链表并输出反转后的头结点。链表结点定义如下： 12345678public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125; 解决与链表相关的问题总是有大量的指针操作，而指针操作的代码总是容易出错的。 为了正确地反转一个链表，需要调整链表中指针的方向。为了将调整指针这个复杂的过程分析清楚，可以借助图形来直观分析。在下图所示的链表中，h、i、j是3个相邻的结点。假设经过若干操作，我们已经把结点h之前的指针调整完毕，这些结点的next指向h，此时链表的结果如下所示： 其中（a）为一个链表，（b）把i之前的所有结点的next都指向前一个结点，导致链表在结点i、j之间断裂。 不难注意到，由于结点i的next指向了它的前一个结点，导致我们无法再链表中遍历到结点j。为了避免链表在结点i处断开，我们需要在调整结点i的next之前把结点j保存下来。 也就是说我们在调整结点i的next指针时，除了需要知道结点i本身之外，还需要前一个结点h，因为我们需要把结点i的next指向结点h。同时，我们还事先需要保存i的一个结点j，以防止链表断开。因此相应地我们需要定义3个指针，分别指向当前遍历到的结点、它的前一个结点及后一个结点。 最后我们试着找到反转后链表的头结点。不难分析出反转后链表的头结点是原始链表的尾结点。什么结点是尾结点？自然是next为null的结点。 pre\rightarrow head \rightarrow next先保存next，即$next = head.next$再反转head的指针$head.next=pre $，链表结构变成 pre\leftarrow head \ \ \ next接着向后移动结点$pre=head,head=next$ 实现代码如下： java版本 1234567891011121314151617181920212223242526272829303132public class Solution &#123; public ListNode ReverseList(ListNode head) &#123; if(head==null) return null; //head为当前节点，如果当前节点为空的话，那就什么也不做，直接返回null； ListNode pre = null; ListNode next = null; //当前节点是head，pre为当前节点的前一节点，next为当前节点的下一节点 //需要pre和next的目的是让当前节点从pre-&gt;head-&gt;next1-&gt;next2变成pre&lt;-head next1-&gt;next2 //即pre让节点可以反转所指方向，但反转之后如果不用next节点保存next1节点的话，此单链表就此断开了 //所以需要用到pre和next两个节点 //1-&gt;2-&gt;3-&gt;4-&gt;5 //1&lt;-2&lt;-3 4-&gt;5 while(head!=null)&#123; //做循环，如果当前节点不为空的话，始终执行此循环，此循环的目的就是让当前节点从指向next到指向pre //如此就可以做到反转链表的效果 //先用next保存head的下一个节点的信息，保证单链表不会因为失去head节点的原next节点而就此断裂 next = head.next; //保存完next，就可以让head从指向next变成指向pre了，代码如下 head.next = pre; //head指向pre后，就继续依次反转下一个节点 //让pre，head，next依次向后移动一个节点，继续下一次的指针反转 pre = head; head = next; &#125; //如果head为null的时候，pre就为最后一个节点了，但是链表已经反转完毕，pre就是反转后链表的第一个节点 //直接输出pre就是我们想要得到的反转后的链表 return pre; &#125;&#125; python版本 123456789101112131415161718# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回ListNode def ReverseList(self, pHead): # write code here if not pHead or not pHead.next: return pHead pre = None while pHead: next1 = pHead.next pHead.next = pre pre = pHead pHead = next1 return pre 17. 合并两个排序的链表（链表）输入两个递增排序的链表，合并这两个链表并使新链表中的结点仍然是按照递增排序的。例如下图中的链表1和链表2，则合并之后的升序链表3如下所示： 这是一个经常被各公司采用的面试题。在面试过程中，最容易犯两种错误：一是在写代码之前没有对合并的过程想清楚，最终合并出来的链表要么中间断开了，要么并没有做到递增排序；二是代码在鲁棒性方面存在问题，程序一旦有特殊的输入（如空链表）就会奔溃。首先分析合并两个链表的过程。从合并两个链表的头结点开始。链表1的头结点的值小于链表2的头结点的值，因此链表1的头结点将是合并后链表的头结点。 继续合并剩余的结点。在两个链表中剩下的结点依然是排序的，因此合并这两个链表的步骤和前面的步骤是一样的。依旧比较两个头结点的值。此时链表2的头结点值小于链表1的头结点的值，因此链表2的头结点的值将是合并剩余结点得到的链表的头结点。把这个结点和前面合并链表时得到的链表的尾结点链接起来。 当我们得到两个链表中值较小的头结点并把它链接到已经合并的链表之后，两个链表剩余的结点依然是排序的，因此合并的步骤和之前的步骤是一样的。这是典型的递归过程，我们可以定义递归函数完成这一合并过程。（解决这个问题需要大量的指针操作，如没有透彻地分析问题形成清晰的思路，很难写出正确的代码） 接下来解决鲁棒性问题，每当代码试图访问空指针指向的内存时程序就会奔溃，从而导致鲁棒性问题。本题中一旦输入空的链表就会引入空的指针，因此我们要对空链表单独处理。当第一个链表是空链表，也就是它的头结点是一个空指针时，和第二个链表合并的结果就是第二个链表。同样，当输入的第二个链表的头结点是空指针的时候，和第一个链表合并得到的结果就是第一个链表。如果两个链表都为空，合并得到的是一个空链表。（由于有大量的指针操作，如果稍有不慎就会在代码中遗留很多与鲁棒性相关的隐患。建议应聘者在写代码之前全面分析哪些情况会引入空指针，并考虑清楚怎么处理这些空指针。） 代码如下： java版本 123456789101112131415161718192021222324252627/*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode Merge(ListNode list1,ListNode list2) &#123; if (list1==null) return list2; else if (list2==null) return list1; ListNode MergeHead = null; if (listval&lt;=list2.val)&#123; MergeHead = list1; MergeHead.next = Merge(listnext,list2); &#125; else &#123;MergeHead = list2; MergeHead.next = Merge(list1,list2.next); &#125; return MergeHead; &#125;&#125; python版本 123456789101112131415161718192021# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回合并后列表 def Merge(self, pHead1, pHead2): # write code here if pHead1== None: return pHead2 if pHead2== None: return pHead1 MergeHead = None if pHeadval &lt; pHead2.val: MergeHead = pHead1 MergeHead.next = self.Merge(pHeadnext,pHead2) else: MergeHead = pHead2 MergeHead.next = self.Merge(pHead1,pHead2.next) return MergeHead 18. 树的子结构（二叉树）输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） 要查找树A中是否存在和树B结构一样的子树，我们可以分成两步：第一步在树A中找到和B的根结点的值一样的结点R，第二步再判断树A以R为根结点的子树是不是包含和树B一样的结构。 第一步在树A中查找与根结点的值一样的结点，实际上就是树的遍历。对二叉树这种数据结构熟悉的读者自然知道可以用递归的方法去遍历，也可以用循环的方法去遍历。由于递归的代码实现比较简洁，面试时如果没有特别要求，通常会采用递归的方式。参考代码如下： java第一步 123456789101112131415161718192021public boolean HasSubtree(TreeNode root1,TreeNode root2) &#123; boolean result = false; //一定要注意边界条件的检查，即检查空指针。否则程序容易奔溃，面试时尤其要注意。这里当Tree1和Tree2都不为零的时候，才进行比较。否则直接返回false if(root1!=null&amp;&amp;root2!=null)&#123; ////如果找到了对应Tree2的根节点的点 if(root1.val==root2.val)&#123; //以这个根节点为为起点判断是否包含Tree2 result = DoesTree1HaveTree2(root1,root2); &#125; //如果找不到，那么就再去root的左儿子当作起点，去判断是否包含Tree2 if(!result)&#123; result=HasSubtree(root1.left,root2); &#125; //如果还找不到，那么就再去root的右儿子当作起点，去判断是否包含Tree2 if(!result)&#123; result=HasSubtree(root1.right,root2); &#125; &#125; return result; &#125; 第二步是判断树A中以R为根结点的子树是不是和树B具有相同的结构。同样，我们也可以用递归的思路来考虑：如果结点R的值和树B的根结点不同，则以R为根结点的子树和树B一定不具有相同的结点；如果他们的值相同，则递归地判断它们各自的左右结点的值是不是相同。递归的终止条件是我们达到了树A或者树B的叶结点。 代码如下： java 12345678910111213141516public boolean DoesTree1HaveTree2(TreeNode root1,TreeNode root2)&#123; //如果Tree2已经遍历完了都能对应的上，返回true if(root2==null)&#123; return true; &#125; //如果Tree2还没有遍历完，Tree1却遍历完了。返回false if(root1==null)&#123; return false; &#125; //如果其中有一个点没有对应上，返回false if(root1.val!=root2.val)&#123; return false; &#125; //如果根节点对应的上，那么就分别去左右子节点里面匹配 return DoesTree1HaveTree2(root1.left,root2.left)&amp;&amp;DoesTree1HaveTree2(root1.right,root2.right); &#125; 二叉树相关的代码有大量的指针操作，每一次使用指针的时候，我们都要问自己这个指针有没有可能是NULL，如果是NULL该怎么处理。 19. 二叉树的镜像（二叉树）操作给定的二叉树，将其变换为源二叉树的镜像。 123456789101112131415161718192021public class Solution &#123; public void Mirror(TreeNode root) &#123; //边界 if(root==null) return; if(root.left==null&amp;&amp;root.right==null) return; //交换左右子树 TreeNode temp = root.left; root.left=root.right; root.right=temp; //递归 if(root.left!=null)&#123; Mirror(root.left); &#125; if(root.right!=null)&#123; Mirror(root.right); &#125; &#125;&#125; 20. 顺时针打印矩阵（数组）输入一个矩阵，按照从外向里以顺时针依次打印出每一个数字。例如，输入如下矩阵： java 12345678910111213141516171819202122232425262728import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printMatrix(int [][] matrix) &#123; int row = matrix.length; int col = matrix[0].length; ArrayList&lt;Integer&gt; result = new ArrayList&lt;Integer&gt; (); // 输入的二维数组非法，返回空的数组 if(row==0&amp;&amp;col==0)return result; // 定义四个关键变量，表示左上和右下的打印范围 int left =0,top=0,right=col-1,bottom=row-1; while(left&lt;=right&amp;&amp;top&lt;=bottom)&#123; // left to right for(int i=left;i&lt;=right;i++)&#123;result.add(matrix[top][i]);&#125; // top to bottom for(int i=top+1;i&lt;=bottom;i++)&#123;result.add(matrix[i][right]);&#125; // right to left if(top!=bottom)&#123; for(int i=right-1;i&gt;=left;i--)&#123;result.add(matrix[bottom][i]);&#125;&#125; // bottom to top if(left!=right)&#123; for(int i=bottom-1;i&gt;=top+1;i--)&#123;result.add(matrix[i][left]);&#125;&#125; left++;right--;top++;bottom--; &#125; return result; &#125; &#125; 21.包含min函数的栈（栈）定义栈的数据结构，请在该类型中实现一个能够得到栈最小元素的min函数。在该栈中，调用min、push及pop的时间复杂度都是O(1)。 可以利用一个辅助栈来存放最小值 每入栈一次，就与辅助栈顶比较大小，如果小就入栈，如果大就入栈当前的辅助栈顶 。 当出栈时，辅助栈也要出栈 这种做法可以保证辅助栈顶一定都是最小元素。 12345678910111213141516171819202122232425import java.util.Stack;public class Solution &#123; Stack&lt;Integer&gt; data = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; min = new Stack&lt;Integer&gt;(); public void push(int node) &#123; data.push(node); if(min.empty())&#123;min.push(data.peek());&#125; else if(data.peek()&lt;min.peek())&#123;min.push(data.peek());&#125; else min.push(min.peek()); &#125; public void pop() &#123; data.pop(); min.pop(); &#125; public int top() &#123; return data.peek(); &#125; public int min() &#123; return min.peek(); &#125;&#125; 22. 栈的压入、弹出序列（栈）输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4，5，3，2，1是该压栈序列对应的一个弹出序列，但4，3，5，1，2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 借用一个辅助的栈，遍历压栈顺序，先讲第一个放入栈中，这里是1，然后判断栈顶元素是不是出栈顺序的第一个元素，这里是4，很显然1≠4，所以我们继续压栈，直到相等以后开始出栈，出栈一个元素，则将出栈顺序向后移动一位，直到不相等，这样循环等压栈顺序遍历完成，如果辅助栈还不为空，说明弹出序列不是该栈的弹出顺序。举例：入栈1,2,3,4,5出栈4,5,3,2,1首先1入辅助栈，此时栈顶1≠4，继续入栈2此时栈顶2≠4，继续入栈3此时栈顶3≠4，继续入栈4此时栈顶4＝4，出栈4，弹出序列向后一位，此时为5，,辅助栈里面是1,2,3此时栈顶3≠5，继续入栈5此时栈顶5=5，出栈5,弹出序列向后一位，此时为3，,辅助栈里面是1,2,3….依次执行，最后辅助栈为空。如果不为空说明弹出序列不是该栈的弹出顺序。 java 1234567891011121314151617import java.util.ArrayList;import java.util.Stack;public class Solution &#123; public boolean IsPopOrder(int [] pushA,int [] popA) &#123; if(pushA.length==0||popA.length==0)return false; Stack&lt;Integer&gt; S=new Stack&lt;Integer&gt;(); int popIndex = 0; for(int i=0;i&lt;pushA.length;i++)&#123; S.push(pushA[i]); while(!S.empty()&amp;&amp;popA[popIndex]==S.peek())&#123; S.pop(); popIndex++; &#125; &#125; return S.empty(); &#125;&#125; 23. 从上往下打印二叉树（二叉树）从上往下打印出二叉树的每个节点，同层节点从左至右打印。 每次打印一个结点时，如果该结点有子结点，则把该结点的子结点放到队列的末尾。接下来到队列的头部取出最早进入队列的结点，重复前面的打印操作。 java 12345678910111213141516171819import java.util.ArrayList;import java.util.LinkedList;public class Solution &#123; public ArrayList&lt;Integer&gt; PrintFromTopToBottom(TreeNode root) &#123; ArrayList&lt;Integer&gt; List=new ArrayList&lt;Integer&gt;(); if(root==null)&#123;return List;&#125; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.add(root);//先把根结点加入队列q while(!queue.isEmpty())&#123;//队列非空时 TreeNode treenode=queue.remove();//取出队列头结点 if(treenode.left!=null)&#123;queue.add(treenode.left);&#125;//向队列加入左孩子（若有） if(treenode.right!=null)&#123;queue.add(treenode.right);&#125;//向队列加入右孩子（若有） List.add(treenode.val);//加到打印列表中 &#125; return List; &#125;&#125; 24. 二叉搜索树的后序遍历序列（二叉树）输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。在后序遍历得到的序列中，最后一个数字是树的根结点的值。数组中前面的数字可以分成两部分：第一部分是左子树结点的值，它们都比根结点小；第二部分是右子树结点的值，它们都比根结点大。 java 1234567891011121314151617181920212223242526272829303132import java.util.Arrays;public class Solution &#123; public boolean VerifySquenceOfBST(int [] sequence) &#123; int length = sequence.length; if(sequence==null||length==0)&#123;return false;&#125; int root = sequence[length-1];//根结点 int i=0; //外部初始化 //找到左子树的最后一个结点位置 for(;i&lt;length-1;i++)&#123; if(sequence[i]&gt;root)&#123; break; &#125; &#125; //如果右子树的结点值小于根结点的值，则返回false for(int j=i;j&lt;length-1;j++)&#123; if(sequence[j]&lt;root)&#123; return false; &#125; &#125; //初始化 boolean left=true; boolean right=true; //递归左右子树 if(i&gt;0)&#123; left = VerifySquenceOfBST(Arrays.copyOfRange(sequence,0,i));//Arrays的copyOfRange方法 &#125; if(i&lt;length-1)&#123; right = VerifySquenceOfBST(Arrays.copyOfRange(sequence,i,length-1)); &#125; return left&amp;&amp;right; &#125;&#125; 25. 二叉树中和为某一值的路径（二叉树）输入一颗二叉树和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。 java 12345678910111213141516171819public class Solution &#123; private ArrayList&lt;ArrayList&lt;Integer&gt;&gt; listAll = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); private ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindPath(TreeNode root,int target) &#123; if(root == null) return listAll; list.add(root.val); target -= root.val;//每次减去结点的值 //如果target等于0，则说明这条路径和为target，添加到listAll中 if(target == 0 &amp;&amp; root.left == null &amp;&amp; root.right == null) listAll.add(new ArrayList&lt;Integer&gt;(list));//因为add添加的是引用，如果不new一个的话，后面的操作会更改listAll中list的值 //向左孩子递归 if(root.left!=null)FindPath(root.left, target); //向右孩子递归 if(root.right!=null)FindPath(root.right, target); //如果不满足条件，则回到父节点； list.remove(list.size()-1); return listAll; &#125;&#125; 26. 复杂链表的复制（链表）输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） java 12345678910111213141516171819202122232425262728293031323334public class Solution &#123; public RandomListNode Clone(RandomListNode pHead) &#123; if (pHead == null) return null; //复制next 如原来是A-&gt;B-&gt;C 变成A-&gt;A'-&gt;B-&gt;B'-&gt;C-&gt;C' RandomListNode pCur = pHead; while (pCur != null) &#123; RandomListNode node = new RandomListNode(pCur.label); node.next = pCur.next; pCur.next = node; pCur = node.next; &#125; //复制random pCur是原来链表的结点 pCur.next是复制pCur的结点 pCur = pHead; while (pCur!=null) &#123; if (pCur.random!=null) pCur.next.random = pCur.random.next; pCur = pCur.next.next; &#125; //拆分链表 RandomListNode head = pHead.next; RandomListNode tmp = head; pCur = pHead; while(pCur.next!=null) &#123; tmp = pCur.next; pCur.next = tmp.next; pCur = tmp; &#125; return head; &#125;&#125; 27. 二叉搜素树与双向链表（二叉树）输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 java 12345678910111213141516171819202122232425public class Solution &#123; TreeNode head = null; TreeNode realHead = null; public TreeNode Convert(TreeNode pRootOfTree) &#123; ConvertSub(pRootOfTree); return realHead//realHead是每个子树排序后的第一个结点，head是排序后的最后一个结点; &#125; private void ConvertSub(TreeNode pRootOfTree) &#123; //递归中序遍历 if(pRootOfTree==null) return; ConvertSub(pRootOfTree.left); if (head == null) &#123; //初始处 head = pRootOfTree; realHead = pRootOfTree; &#125; else &#123; //前两句实现双向，第三句跳到下一个节点。 head.right = pRootOfTree; pRootOfTree.left = head; head = pRootOfTree; &#125; ConvertSub(pRootOfTree.right); &#125;&#125; 28. 字符串的排列（字符串）输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。 首先我要打印abc的全排列，就是第一步把a 和bc交换（得到bac,cab），这需要一个for循环，循环里面有一个swap，交换之后就相当于不管第一步了，进入下一步递归，所以跟一个递归函数， 完成递归之后把交换的换回来，变成原来的字串 12345678abc 为例子：1. 固定a, 求后面bc的全排列： abc, acb。 求完后，a 和 b交换； 得到bac,开始第二轮2. 固定b, 求后面ac的全排列： bac, bca。 求完后，b 和 c交换； 得到cab,开始第三轮3. 固定c, 求后面ba的全排列： cab, cba 即递归树： str: a b c ab ac ba bc ca cb result: abc acb bac bca cab cba java 12345678910111213141516171819202122232425262728293031import java.util.ArrayList;import java.util.*;public class Solution &#123; public ArrayList&lt;String&gt; Permutation(String str) &#123; ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); if(str.length()==0) return list; char[] array = str.toCharArray(); permutation(array,0,list); Collections.sort(list); return list; &#125; public void permutation(char[] array,int begin,ArrayList&lt;String&gt; list) &#123; if(begin == array.length-1) &#123; list.add(String.valueOf(array)); &#125;else &#123; for(int i=begin;i&lt;array.length;++i) &#123; if(i==begin || array[i]!=array[begin]) &#123; swap(array,begin,i); permutation(array,begin+1,list); swap(array,begin,i); &#125; &#125; &#125; &#125; public void swap(char[] array,int i,int j) &#123; char temp = array[i]; array[i] = array[j]; array[j] = temp; &#125;&#125; 29. 数组中出现次数超过一半的数字（数组）数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 数组中有一个数字出现的次数超过数组长度的一半，也就是说它出现的次数比其他所有数字出现的次数的和还要多。因此我们可以考虑在遍历数组的时候保存两个值：一个是数组的一个数字，一个是次数。当我们遍历到下一个数字的时候，如果下一个数字和我们之前保存的数字相同，则次数加1；如果不同，则次数减1；如果次数为0，则保存下一个数字，并把次数设为1。 还要判断这个数字是否超过数组长度的一半，如果不存在输出0。 123456789101112131415161718192021222324252627282930313233public class Solution &#123; public int MoreThanHalfNum_Solution(int [] array) &#123; if(array==null||array.length==0)&#123; return 0; &#125; int result=array[0]; int count=1; for(int i=1;i&lt;array.length;i++)&#123; if(result==array[i])&#123; count++; &#125; else if(result!=array[i])&#123; count--; &#125; if(count==0)&#123; result=array[i]; count=1; &#125; &#125; int times=0; for(int i=0;i&lt;array.length;i++)&#123; if(array[i]==result)&#123; times++; &#125; &#125; if(times*2&lt;=array.length)&#123; System.out.println(times); return 0; &#125; else return result; &#125;&#125; 30. 最小的K个数（数组）输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 第一种方法，借用partition函数 java 1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.ArrayList; public class Solution &#123; public ArrayList&lt;Integer&gt; GetLeastNumbers_Solution(int[] input, int k) &#123; ArrayList&lt;Integer&gt; output = new ArrayList&lt;Integer&gt;(); int length = input.length; if (input == null || length &lt;= 0 || length &lt; k || k&lt;= 0) &#123; return output; &#125; int left = 0; int right = length - 1; int index = partition(input,left,right); while(index != k -1) &#123; if(index &lt; k - 1) &#123; left = index + 1; //不够的话往右边走走 index = partition(input,left,right); &#125; else &#123; right = index - 1; //太多的话往左边走走 index = partition(input,left,right); &#125; &#125; for (int i = 0;i &lt; k;i++) &#123; output.add(input[i]); &#125; return (ArrayList&lt;Integer&gt;) output; &#125; //基准左右分区 private int partition(int[] input,int left,int right) &#123; int pivot = input[left]; while(left &lt; right) &#123; while(input[right] &gt;= pivot &amp;&amp; left &lt; right) &#123; right--; &#125; input[left] = input[right]; while(input[left] &lt;= pivot &amp;&amp; left &lt;right) &#123; left++; &#125; input[right] = input[left]; &#125; input[left] = pivot; return left; &#125; &#125; 第二种方法 用最大堆保存这k个数，每次只和堆顶比，如果比堆顶小，删除堆顶，新数入堆。 java 123456789101112131415161718192021222324252627282930313233343536import java.util.ArrayList;import java.util.PriorityQueue;import java.util.Comparator;public class Solution &#123; public ArrayList&lt;Integer&gt; GetLeastNumbers_Solution(int[] input, int k) &#123; ArrayList&lt;Integer&gt; result = new ArrayList&lt;Integer&gt;(); int length = input.length; if(k &gt; length || k == 0)&#123; return result; &#125; PriorityQueue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;Integer&gt;(k, new Comparator&lt;Integer&gt;() &#123; @Override//PriorityQueue默认是小顶堆，实现大顶堆，需要反转默认排序器 public int compare(Integer o1, Integer o2) &#123; return o2.compareTo(o1); &#125; &#125;); for (int i = 0; i &lt; length; i++) &#123; //如果最大堆中已有的数字少于k个，直接读入 if (maxHeap.size() != k) &#123; maxHeap.offer(input[i]); &#125; //如果最大堆中已有k个数字了，即容器已满，且大顶堆顶大于待插入数字，将待插入数字替换进大顶堆 else if (maxHeap.peek() &gt; input[i]) &#123; Integer temp = maxHeap.poll(); temp = null; maxHeap.offer(input[i]); &#125; &#125; //输出大顶堆中的数 for (Integer integer : maxHeap) &#123; result.add(integer); &#125; return result; &#125;&#125; 31. 连续子数组的最大和（数组）输入一个整型数组，数组中有正数也有负数。数组中一个或连续的多个整数组成一个子数组。求所有子数组的和的最大值。要求时间复杂度为O(n) 第一种方法 java 12345678910111213141516171819public class Solution &#123; public int FindGreatestSumOfSubArray(int[] array) &#123; if(array.length==0||array==null) return 0; int cSum = 0; int result = array[0];// result存储最大和，不能初始为0，存在负数 for(int i=0;i&lt;array.length;i++)&#123; if(cSum&lt;0)&#123; cSum=array[i];// 当前和&lt;0，抛弃不要 &#125;else&#123; cSum += array[i];//否则累加上去 &#125; if(cSum&gt;result)&#123; result = cSum;// 存储当前的最大结果 &#125; &#125; return result; &#125;&#125; 第二种方法：动态规划 1234567891011121314151617181920212223F（i）：以array[i]为末尾元素的子数组的和的最大值，子数组的元素的相对位置不变 F（i）=max（F（i-1）+array[i] ， array[i]） res：所有子数组的和的最大值 res=max（res，F（i）） 如数组[6, -3, -2, 7, -15, 1, 2, 2] 初始状态： F（0）=6 res=6 i=1： F（1）=max（F（0）-3，-3）=max（6-3，3）=3 res=max（F（1），res）=max（3，6）=6 i=2： F（2）=max（F（1）-2，-2）=max（3-2，-2）=1 res=max（F（2），res）=max（1，6）=6 i=3： F（3）=max（F（2）+7，7）=max（1+7，7）=8 res=max（F（2），res）=max（8，6）=8 i=4： F（4）=max（F（3）-15，-15）=max（8-15，-15）=-7 res=max（F（4），res）=max（-7，8）=8 以此类推 最终res的值为8 java 1234567891011public class Solution &#123; public int FindGreatestSumOfSubArray(int[] array) &#123; int res = array[0]; int max = array[0]; for(int i=1;i&lt;array.length;i++)&#123; max=Math.max(max+array[i],array[i]); res = Math.max(max,res); &#125; return res; &#125;&#125; 32.从1到n整数中1出现的次数（数组）输入一个整数n，求1到n这n个整数的十进制表示中1出现的次数。例如输入12，从1到12这些整数中包含1的数字有1、10、11、12，1一共出现了5次。 一、1的数目 编程之美上给出的规律： 如果第i位（自右至左，从1开始标号）上的数字为0，则第i位可能出现1的次数由更高位决定（若没有高位，视高位为0），等于更高位数字X当前位数的权重$10^{i-1}$。 如果第i位上的数字为1，则第i位上可能出现1的次数不仅受更高位影响，还受低位影响（若没有低位，视低位为0），等于更高位数字X当前位数的权重$10^{i-1}+$（低位数字+1）。 如果第i位上的数字大于1，则第i位上可能出现1的次数仅由更高位决定（若没有高位，视高位为0），等于（更高位数字+1）X当前位数的权重$10^{i-1}$。 二、X的数目这里的 X∈[1,9] ，因为 X=0 不符合下列规律，需要单独计算。首先要知道以下的规律： 从 1 至 10，在它们的个位数中，任意的 X 都出现了 1 次。 从 1 至 100，在它们的十位数中，任意的 X 都出现了 10 次。 从 1 至 1000，在它们的百位数中，任意的 X 都出现了 100 次。 依此类推，从 1 至 $10^i$ ，在它们的左数第二位（右数第 i 位）中，任意的 X 都出现了 $10^{i−1}$ 次。 这个规律很容易验证，这里不再多做说明。 接下来以 n=2593,X=5 为例来解释如何得到数学公式。从 1 至 2593 中，数字 5 总计出现了 813 次，其中有 259 次出现在个位，260 次出现在十位，294 次出现在百位，0 次出现在千位。 现在依次分析这些数据，首先是个位。从 1 至 2590 中，包含了 259 个 10，因此任意的 X 都出现了 259 次。最后剩余的三个数 2591, 2592 和 2593，因为它们最大的个位数字 3 &lt; X，因此不会包含任何 5。（也可以这么看，3&lt;X，则个位上可能出现的X的次数仅由更高位决定，等于更高位数字$（259）\times 10^{1-1}=259$）。 然后是十位。从 1 至 2500 中，包含了 25 个 100，因此任意的 X 都出现了 25×10=250 次。剩下的数字是从 2501 至 2593，它们最大的十位数字9&gt;X，因此会包含全部10个5。最后总计250 + 10 = 260。（也可以这么看，9&gt;X，则十位上可能出现的X的次数仅由更高位决定，等于更高位数字$（25+1）\times 10^{2-1}=260$）。 接下来是百位。从 1 至 2000 中，包含了 2 个 1000，因此任意的 X 都出现了 2×100=200 次。剩下的数字是从 2001 至 2593，它们最大的百位数字 5 == X，这时情况就略微复杂，它们的百位肯定是包含 5 的，但不会包含全部 100 个。如果把百位是 5 的数字列出来，是从 2500 至 2593，数字的个数与百位和十位数字相关，是 93+1 = 94。最后总计 200 + 94 = 294。（也可以这么看，5==X，则百位上可能出现X的次数不仅受更高位影响，还受低位影响，等于更高位数字$（2）\times 10^{3-1}+（93+1）=294$）。 最后是千位。现在已经没有更高位，因此直接看最大的千位数字 2 &lt; X，所以不会包含任何 5。（也可以这么看，2&lt;X，则千位上可能出现的X的次数仅由更高位决定，等于更高位数字$（0）\times 10^{4-1}=0$）。 到此为止，已经计算出全部数字 5 的出现次数。总结一下以上的算法，可以看到，当计算右数第 i 位包含的 X 的个数时： 取第 i 位左边（高位）的数字，乘以$10^{i−1}$ ，得到基础值a 。 取第 i 位数字，计算修正值： 如果大于 X，则结果为 $a+ 10^{i−1}$ 。 如果小于 X，则结果为 a 。 如果等 X，则取第 i 位右边（低位）数字，设为 b ，最后结果为 a+b+1 。 相应的代码非常简单，效率也非常高，时间复杂度只有 $ O( log_ {10} n) $。 代码如下： 12345678910111213141516171819202122public int NumberOfXBetween1AndN_Solution(int n,int x) &#123; if(n&lt;0||x&lt;1||x&gt;9) return 0; int high,low,curr,tmp,i = 1; high = n; int total = 0; while(high!=0)&#123; high = n/(int)Math.pow(10, i);// 获取第i位的高位 tmp = n%(int)Math.pow(10, i); curr = tmp/(int)Math.pow(10, i-1);// 获取第i位 low = tmp%(int)Math.pow(10, i-1);// 获取第i位的低位 if(curr==x)&#123; total+= high*(int)Math.pow(10, i-1)+low+1; &#125;else if(curr&lt;x)&#123; total+=high*(int)Math.pow(10, i-1); &#125;else&#123; total+=(high+1)*(int)Math.pow(10, i-1); &#125; i++; &#125; return total; &#125; 33. 把数组排成最小的数(数组)输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 java 123456789101112131415161718192021222324252627282930import java.util.ArrayList;import java.util.Collections;import java.util.Comparator;public class Solution &#123; public String PrintMinNumber(int [] numbers) &#123; int n; String s=""; ArrayList&lt;Integer&gt; list=new ArrayList&lt;Integer&gt;(); n=numbers.length; for(int i=0;i&lt;n;i++)&#123; list.add(numbers[i]);//将数组放入arrayList中 &#125; //实现了Comparator接口的compare方法，将集合元素按照compare方法的规则进行排序 Collections.sort(list,new Comparator&lt;Integer&gt;()&#123; @Override public int compare(Integer str1, Integer str2) &#123; // TODO Auto-generated method stub String s1=str1+""+str2; String s2=str2+""+str1; return s1.compareTo(s2); &#125; &#125;); for(int j:list)&#123; s+=j; &#125; return s; &#125;&#125; 34. 丑数（数组）把只包含因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 java 12345678910111213141516171819import java.util.*;public class Solution &#123; public int GetUglyNumber_Solution(int index) &#123; if(index&lt;7)return index; int[] res = new int[index]; res[0] = 1; int t2 = 0, t3 = 0, t5 = 0, i; for(i=1;i&lt;index;i++)&#123; res[i] = min(res[t2]*2,min(res[t3]*3,res[t5]*5)); if(res[i] == res[t2]*2)t2++; if(res[i] == res[t3]*3)t3++; if(res[i] == res[t5]*5)t5++; &#125; return res[index-1]; &#125; private int min(int a,int b)&#123; return (a&gt;b)? b:a; &#125;&#125; 35. 第一次只出现一次的字符（字符串）在一个字符串(1&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置. 我们可以使用一个容器来存放每个字符的出现次数。在这个数据容器中可以根据字符来查找出现的次数，也就是这个容器的作用是把一个字符映射成一个数字。在常用的数据容器中，哈希表正是这个用途。 为了解决这个问题，我们可以定义哈希表的键值（Key）是字符，而值（Value）是该字符出现的次数。同时我们还需要从头开始扫描字符串两次。第一次扫面字符串时，每扫到一个字符就在哈希表的对应项把次数加1.接下来第二次扫描时，每扫描到一个字符就能在哈希表中得到该字符出现的次数，这样第一个只出现一次的字符就是符合要求的输出。 需要涉及到Java中HashMap工作原理及实现，资料链接 java 1234567891011121314151617181920212223import java.util.HashMap;public class Solution &#123; public int FirstNotRepeatingChar(String str) &#123; HashMap&lt;Character,Integer&gt; map = new HashMap&lt;Character,Integer&gt;(); for(int i=0;i&lt;str.length();i++)&#123; char c = str.charAt(i);//charAt方法，获得位置i的串 if(map.containsKey(c))&#123;//HashMap的containKey方法； int time = map.get(c);//HashMap的get方法，得到Key c的Value； time++; map.put(c,time);//HashMap的put方法，将Key c的Value置为time； &#125;else&#123; map.put(c,1); &#125; &#125; for(int i=0;i&lt;str.length();i++)&#123; char c = str.charAt(i); if(map.get(c)==1)&#123; return i; &#125; &#125; return -1; &#125;&#125; 36. 数组中的逆序对（数组）在数组中的两个数字如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组，求出这个数组中的逆序对的总数。例如在数组{7，5，6，4}中，一共存在5个逆序对，分别是（7，6）、（7，5）、（7，4）、（5，4）和（6，4）。 可以按照归并排序的思路，先把数组分隔成子数组，先统计出子数组内部的逆序对的数目，然后再统计出两个相邻子数组之间的逆序对的数目。在统计逆序对的过程中，还需要对数组进行排序。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Solution &#123; int cnt; public int InversePairs(int[] array) &#123; cnt = 0; if (array != null) mergeSortUp2Down(array, 0, array.length - 1); return cnt; &#125; /* * 归并排序(从上往下) */ public void mergeSortUp2Down(int[] a, int start, int end) &#123; if (start &gt;= end) return; int mid = (start + end) &gt;&gt; 1; mergeSortUp2Down(a, start, mid); mergeSortUp2Down(a, mid + 1, end); merge(a, start, mid, end); &#125; /* * 将一个数组中的两个相邻有序区间合并成一个 */ public void merge(int[] a, int start, int mid, int end) &#123; int[] tmp = new int[end - start + 1]; int i = start, j = mid + 1, k = 0; while (i &lt;= mid &amp;&amp; j &lt;= end) &#123; if (a[i] &lt;= a[j]) tmp[k++] = a[i++]; else &#123; tmp[k++] = a[j++]; cnt += mid - i + 1; //关键的一步，统计逆序对.......... cnt%=1000000007; &#125; &#125; while (i &lt;= mid) tmp[k++] = a[i++]; while (j &lt;= end) tmp[k++] = a[j++]; for (k = 0; k &lt; tmp.length; k++) a[start + k] = tmp[k]; &#125;&#125; 37. 两个链表的第一个公共结点（链表）输入两个链表找出他们的第一个公共结点。 面试的时候碰到这道题，很多应聘者的第一个想法就是蛮力法：在第一个链表上顺序遍历每个结点，每遍历到一个结点的时候，在第二个链表上顺序遍历每个结点。若第二个链表上有一个结点和第一个链表上的结点一样，说明两个链表在这个结点上重合，于是就找到了它们的公共结点。如果第一个链表的长度为m，第二个链表的长度为n，显然该方法的时间复杂度是O(mn)。 通常蛮力法不会是最好的办法，我们接下来试着分析有公共结点的两个链表有哪些特点。从链表结构的定义看出，这两个链表是单向链表。如果他们有公共的结点，那么这两个链表从某一结点开始，他们的next指向同一个结点。但由于是单向链表的结点，每个结点只有一个next，因此从第一个公共结点开始，之后的结点都是重合的，不可能再出现分叉。所以两个有公共结点而部分重合的链表，拓扑形状看起来像一个Y，而不是X。 经过我们的分析发现，若两个链表有公共结点，那么公共结点出现在两个链表的尾部。如果我们从两个链表的尾部开始往前比较，最后一个相同的结点就是我们要找的结点。我们想到用栈的特点来解决这个问题：分别把两个链表的结点放入两个栈中，这样两个链表的尾结点就位于两个栈的栈顶，接下来比较两个栈顶的结点是否相同。若果相同，则把栈顶弹出接着比较下一个栈顶，直到找到最后一个相同的结点。 上面需要用到两个辅助栈。若链表的长度分别为m和n，那么空间复杂度是O(m+n)。这种思路的时间复杂度也是O(m+n)。和最开始的蛮力法相比，时间效率得到了提升，相当于是用空间换取时间效率。 之所以需要用到栈，是因为我们想同时遍历到达两个栈的尾结点。当两个链表的长度不相同时，如果我们从头开始遍历到达尾结点的时间就不一致。其实解决这个问题还有一个更简单的办法：首先遍历两个链表得到他们的长度，就能知道哪个链表比较长，以及长的链表比短的链表多几个结点。在第二次遍历的时候，在较长的链表上先走若干步，接着再同时在两个链表上遍历，找到的第一个相同的结点就是他们的第一个公共结点。 第三种思路和第二种思路相比，时间复杂度都是O(m+n)，但我们不再需要辅助的栈，因此提高了空间效率。实现代码如下： java版本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode FindFirstCommonNode(ListNode pHead1, ListNode pHead2) &#123; ListNode current1 = pHead1;//链表1 ListNode current2 = pHead2;//链表2 if(pHead1 ==null||pHead2==null)&#123;return null;&#125;// int len1 = getlistlength(pHead1);//链表1的长度 int len2 = getlistlength(pHead2);//链表2的长度 //若链表1长度大于链表2 if(len1&gt;=len2)&#123; int len=len1-len2; //遍历链表1，遍历长度为两链表长度差 while (len&gt;0)&#123; current1 = currentnext; len--; &#125; &#125; //若链表2长度大于链表1 else if(len1&lt;len2)&#123; int len=len2-len1; //遍历链表2，遍历长度为两链表长度差 while (len&gt;0)&#123; current2=current2.next; len--; &#125; &#125; //开始齐头并进，直到找到第一个公共结点 while(current1!=current2)&#123; current1 = currentnext; current2 = current2.next; &#125; return current1; &#125; //求指定链表的长度 public static int getlistlength(ListNode pHead)&#123; int length = 0; ListNode current = pHead; while(current!=null)&#123; length++; current = current.next; &#125; return length; &#125;&#125; python版本 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def FindFirstCommonNode(self, pHead1, pHead2): # write code here current1=pHead1 current2=pHead2 len1 = self.getlistlength(current1) len2 = self.getlistlength(current2) if len1&gt;=len2: length = len1-len2 while length&gt;0: current1 = currentnext length=length-1 elif len1&lt;len2: length = len2-len1 while length&gt;0: current2 = current2.next length=length-1 while current1!=current2: current1=currentnext current2=current2.next return current1 def getlistlength(self,pHead): length =0 current =pHead while current!=None: length=length+1 current = current.next return length 38. 数字在排序数组中出现的次数（数组）统计一个数字在排序数组中出现的次数。 利用二分查找直接找到第一个K和最后一个K。以下代码使用递归方法找到第一个K，使用循环方法最后一个K。 java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class Solution &#123; public int GetNumberOfK(int [] array , int k) &#123; int length = array.length; if(length == 0)&#123; return 0; &#125; int firstK = getFirstK(array, k, 0, length-1); int lastK = getLastK(array, k, 0, length-1); if(firstK != -1 &amp;&amp; lastK != -1)&#123; return lastK - firstK + 1; &#125; return 0; &#125; //递归写法 private int getFirstK(int [] array , int k, int start, int end)&#123; if(start &gt; end)&#123; return -1; &#125; int mid = (start + end) &gt;&gt; 1; if(array[mid] &gt; k)&#123; return getFirstK(array, k, start, mid-1); &#125; else if (array[mid] &lt; k)&#123; return getFirstK(array, k, mid+1, end); &#125; else if(mid-1 &gt;=0 &amp;&amp; array[mid-1] == k)&#123; return getFirstK(array, k, start, mid-1); &#125; else&#123; return mid; &#125; &#125; //循环写法 private int getLastK(int [] array , int k, int start, int end)&#123; int length = array.length; int mid = (start + end) &gt;&gt; 1; while(start &lt;= end)&#123; if(array[mid] &gt; k)&#123; end = mid-1; &#125; else if(array[mid] &lt; k)&#123; start = mid+1; &#125; else if(mid+1 &lt;= length-1 &amp;&amp; array[mid+1] == k)&#123; start = mid+1; &#125; else&#123; return mid; &#125; mid = (start + end) &gt;&gt; 1; &#125; return -1; &#125;&#125; 39. 二叉树的深度（二叉树）39.1 二叉树的深度输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 Java 经典的求二叉树深度 递归写法 java 12345678public class Solution &#123; public int TreeDepth(TreeNode root) &#123; if(root==null)return 0; int nleft = TreeDepth(root.left); int nright = TreeDepth(root.right); return nleft&gt;nright?(nleft+1):(nright+1); &#125;&#125; 39.2 平衡二叉树输入一棵二叉树，判断该二叉树是否是平衡二叉树。 有了求二叉树的深度的经验之后，我们就很容易想到一个思路：在遍历树的每个结点的时候，调用函数TreeDepth得到它的左右子树的深度。如果每个结点的左右子树的深度相差都不超过1，按照定义它就是一颗平衡的二叉树。 java 123456789101112131415161718public class Solution &#123; public boolean IsBalanced_Solution(TreeNode root) &#123; if(root==null)return true; int left = TreeDepth(root.left); int right = TreeDepth(root.right); int diff = left-right; if(diff&gt;1||diff&lt;-1) return false; return IsBalanced_Solution(root.left)&amp;&amp;IsBalanced_Solution(root.right); &#125; public int TreeDepth(TreeNode root) &#123; if(root==null)return 0; int nleft = TreeDepth(root.left); int nright = TreeDepth(root.right); return nleft&gt;nright?(nleft+1):(nright+1); &#125;&#125; 40. 数组中只出现一次的数字（数组）一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。要求时间复杂度是$O(n)$，空间复杂度是$O(1)$。 首先我们考虑这个问题的一个简单版本：一个数组里除了一个数字之外，其他的数字都出现了两次。请写程序找出这个只出现一次的数字。 这个题目的突破口在哪里？题目为什么要强调有一个数字出现一次，其他的出现两次？我们想到了异或运算的性质：任何一个数字异或它自己都等于0 。也就是说，如果我们从头到尾依次异或数组中的每一个数字，那么最终的结果刚好是那个只出现一次的数字，因为那些出现两次的数字全部在异或中抵消掉了。 有了上面简单问题的解决方案之后，我们回到原始的问题。如果能够把原数组分为两个子数组。在每个子数组中，包含一个只出现一次的数字，而其它数字都出现两次。如果能够这样拆分原数组，按照前面的办法就是分别求出这两个只出现一次的数字了。 我们还是从头到尾依次异或数组中的每一个数字，那么最终得到的结果就是两个只出现一次的数字的异或结果。因为其它数字都出现了两次，在异或中全部抵消掉了。由于这两个数字肯定不一样，那么这个异或结果肯定不为0 ，也就是说在这个结果数字的二进制表示中至少就有一位为1 。我们在结果数字中找到第一个为1 的位的位置，记为第N 位。现在我们以第N 位是不是1 为标准把原数组中的数字分成两个子数组，第一个子数组中每个数字的第N 位都为1 ，而第二个子数组的每个数字的第N 位都为0 。 现在我们已经把原数组分成了两个子数组，每个子数组都包含一个只出现一次的数字，而其它数字都出现了两次。因此到此为止，所有的问题我们都已经解决。 java 123456789101112131415161718192021222324252627282930313233//num1,num2分别为长度为1的数组。传出参数//将num1[0],num2[0]设置为返回结果public class Solution &#123; public void FindNumsAppearOnce(int [] array,int num1[] , int num2[]) &#123; if(array==null ||array.length&lt;2) return ; int temp = 0; for(int i=0;i&lt;array.length;i++) temp ^= array[i]; int indexOf1 = findFirstBitIs(temp); for(int i=0;i&lt;array.length;i++)&#123; if(isBit(array[i], indexOf1)) num1[0]^=array[i]; else num2[0]^=array[i]; &#125; &#125; //在正数num的二进制表示中找到最右边是1的位 public int findFirstBitIs(int num)&#123; int indexBit = 0; while(((num &amp; 1)==0) &amp;&amp; (indexBit)&lt;8*4)&#123; num = num &gt;&gt; 1; ++indexBit; &#125; return indexBit; &#125; //判断在num的二进制表示中从右边数起的indexBit位是不是1. public boolean isBit(int num,int indexBit)&#123; num = num &gt;&gt; indexBit; return (num &amp; 1) == 1; &#125;&#125; 41.和为S的两个数字VS和为s的连续正数序列（数组）41.1 和为s的两个数字一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。 数列满足递增，设两个头尾两个指针i和j， 若ai + aj == sum，就是答案（相差越远乘积越小） 若ai + aj &gt; sum，aj肯定不是答案之一（前面已得出 i 前面的数已是不可能），j -= 1 若ai + aj &lt; sum，ai肯定不是答案之一（前面已得出 j 后面的数已是不可能），i += 1 时间复杂度为O(n)。 1234567891011121314151617181920212223import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; FindNumbersWithSum(int [] array,int sum) &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); if(array==null||array.length&lt;2)&#123; return list; &#125; int i=0,j=array.length-1; while(i&lt;j)&#123; if(array[i]+array[j]==sum)&#123; list.add(array[i]); list.add(array[j]); break; &#125; else if(array[i]+array[j]&gt;sum)&#123; j--; &#125; else i++; &#125; return list; &#125;&#125; 41.2 和为s的连续正数序列输入一个正数s，打印出所有和为s的连续正数序列（至少含有两个数）。例如输入15，由于1+2+3+4+5=4+5+6=7+8=15，所以结果打印出三个连续序列1~5、4~6和7~8。 考虑用两个数small和big分别表示序列的最小值和最大值。首先把small初始化为1，big初始化为2，如果从small到big的序列和大于s，我们可以从序列中去掉较小的值，也就是增大small的值。如果从small到big的序列和小于s，我们可以增大big，让这个序列包含更多的数字。因为这个序列至少要有两个数字，我们一直增加small到（1+s）/2为止。 java 12345678910111213141516171819202122232425262728293031323334353637import java.util.ArrayList;/**初始化small=1，big=2;*small到big序列和小于sum，big++;大于sum，small++;*当small增加到(1+sum)/2是停止*/public class Solution &#123; public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindContinuousSequence(int sum) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; lists=new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if(sum&lt;=1)&#123;return lists;&#125; int small=1; int big=2; while(small!=(1+sum)/2)&#123; //当small==(1+sum)/2的时候停止 int curSum=sumOfList(small,big); if(curSum==sum)&#123; ArrayList&lt;Integer&gt; list=new ArrayList&lt;Integer&gt;(); for(int i=small;i&lt;=big;i++)&#123; list.add(i); &#125; lists.add(list); small++;big++; &#125;else if(curSum&lt;sum)&#123; big++; &#125;else&#123; small++; &#125; &#125; return lists; &#125; public int sumOfList(int head,int leap)&#123; //计算当前序列的和 int sum=head; for(int i=head+1;i&lt;=leap;i++)&#123; sum+=i; &#125; return sum; &#125;&#125; 42. 翻转单词顺序VS左旋转字符串（字符串）42.1 翻转单词顺序输入一个英文句子，翻转句子中单词的顺序，但单词内字符的顺序不变。为简单起见，标点符号和普通字母一样处理。例如输入字符串“I am a student”，则输出“student. a am I”。可以先翻转整个句子，然后，依次翻转每个单词。依据空格来确定单词的起始和终止位置 java 1234567891011121314151617181920212223242526public class Solution &#123; public String ReverseSentence(String str) &#123; char[] chars = str.toCharArray(); reverse(chars,0,chars.length-1); int blank = -1; for(int i =0;i&lt;chars.length-1;i++)&#123; if(chars[i]==' ')&#123; int nextblank = i; reverse(chars,blank+1,nextblank-1); blank = nextblank; &#125; &#125; reverse(chars,blank+1,chars.length-1);//单独翻转最后一个单词 return new String(chars); &#125; public void reverse(char[] chars,int low,int high)&#123; while(low&lt;high)&#123; char temp = chars[low]; chars[low]=chars[high]; chars[high]=temp; low++; high--; &#125; &#125;&#125; 42.2 左旋转字符串汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。 以“abcdefg”为例，我们可以把它分为两部分。由于想把它的前两个字符移到后面，我们就把钱两个字符分到第一部分，把后面的所有字符都分到第二部分。然后先翻转这两部分，于是就得到“bagfedc”。接下来在翻转整个字符串，得到的”cdefgab”刚好就是把原始字符串左旋转2位的结果。 123456789101112131415161718192021public class Solution &#123; public String LeftRotateString(String str,int n) &#123; char[] chars = str.toCharArray(); if(chars.length &lt; n) return &quot;&quot;; reverse(chars, 0, n-1); reverse(chars, n, chars.length-1); reverse(chars, 0, chars.length-1); return new String(chars); &#125; public void reverse(char[] chars,int low,int high)&#123; char temp; while(low&lt;high)&#123; temp = chars[low]; chars[low] = chars[high]; chars[high] = temp; low++; high--; &#125; &#125;&#125; 43. N个骰子的点数（null）44. 扑克牌的顺子（数组）从扑克牌中随机抽5张牌，判断是不是顺子，即这5张牌是不是连续的。2~10为数字本身，A为1，J为11，Q为12，K为13，而大小王可以看做是任意数字，这里定为0. java 12345678910111213141516171819202122232425262728293031import java.util.*;public class Solution &#123; public boolean isContinuous(int [] numbers) &#123; int length = numbers.length; if(numbers==null||length==0)return false;//特殊情况 Arrays.sort(numbers);//排序 //统计数组中0的个数 int numberOfZero = 0; for(int i =0;i&lt;length&amp;&amp;numbers[i]==0;i++)&#123; ++numberOfZero; &#125; int numberOfGap = 0; int small = numberOfZero; int big = small+1; while(big&lt;length)&#123; //含有对子，不可能是顺子 if(numbers[small]==numbers[big])&#123; return false; &#125; //统计数组中的间隔数目 numberOfGap += numbers[big]-numbers[small]-1; small=big; big++; &#125; //如果间隔数小于等于零的数量则可以组成顺子，否则不行。 if(numberOfGap&lt;=numberOfZero)&#123; return true; &#125;else&#123;return false;&#125; &#125;&#125; 45. 圆圈中最后剩下的数字（链表）0、…..，n-1这n个数字排成一个圆圈，从数字0开始每次从这个圆圈里删除第m个数字。求出这个圆圈里剩下的最后一个数字。约瑟夫环问题，用环形链表模拟圆圈的经典解法， java 12345678910111213141516171819202122232425262728public class Solution&#123; public int LastRemaining_Solution(int n, int m)&#123; if(m&lt;=0||n&lt;=0)return -1; //先构造循环链表 ListNode head= new ListNode(0);//头结点, 值为0 ListNode pre = head; ListNode temp = null; for(int i=1;i&lt;n;i++)&#123; temp = new ListNode(i); pre.next = temp; pre = temp; &#125; temp.next = head;//将第n-1个结点(也就是尾结点)指向头结点 ListNode temp2 = null; while(n&gt;=1)&#123; //每次都当前头结点找到第m个结点的前驱 temp2=head; for(int i =1;i&lt;m-1;i++)&#123; temp2 = temp2.next; &#125; temp2.next = temp2.next.next; head = temp2.next;//设置当前头结点 n--; &#125; return head.val; &#125;&#125; java 12345678910111213public class Solution&#123; public int LastRemaining_Solution(int n, int m) &#123; if(n==0||m==0)return -1; int last=0; for(int i=2;i&lt;=n;i++) &#123; last=(last+m)%i; &#125; return last ; &#125;&#125; 46. 求1+2+…..+n（逻辑）求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 需利用逻辑与的短路特性实现递归终止。 当n==0时，(n&gt;0)&amp;&amp;((sum+=Sum_Solution(n-1))&gt;0)只执行前面的判断，为false，然后直接返回0； 当n&gt;0时，执行sum+=Sum_Solution(n-1)，实现递归计算Sum_Solution(n)。 java 12345678public class Solution &#123; public int Sum_Solution(int n) &#123; int sum=n; boolean ans = (n&gt;0)&amp;&amp;((sum+=Sum_Solution(n-1))&gt;0); return sum; &#125;&#125; 47. 不用加减乘除做加法（位运算）首先看十进制是如何做的： 5+7=12，三步走 第一步：相加各位的值，不算进位，得到2。 第二步：计算进位值，得到10. 如果这一步的进位值为0，那么第一步得到的值就是最终结果。 第三步：重复上述两步，只是相加的值变成上述两步的得到的结果2和10，得到12。 同样我们可以用三步走的方式计算二进制值相加： 5-101，7-111 第一步：相加各位的值，不算进位，得到010，二进制每位相加就相当于各位做异或操作，101^111。 第二步：计算进位值，得到1010，相当于各位做与操作得到101，再向左移一位得到1010，(101&amp;111)&lt;&lt;1。 第三步重复上述两步， 各位相加 010^1010=1000，进位值为100=(010&amp;1010)&lt;&lt;1。继续重复上述两步：1000^100 = 1100，进位值为0，跳出循环，1100为最终结果。 12345678910public class Solution &#123; public int Add(int num1,int num2) &#123; while (num2!=0) &#123; int temp = num1^num2; num2 = (num1&amp;num2)&lt;&lt;1; num1 = temp; &#125; return num1; &#125;&#125; 49. 把字符串转换成整数（字符串）将一个字符串转换成一个整数，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。问题不难，但是要把很多特殊情况都考虑进去，却并不容易。需要考虑的特殊情况有以下几个： 空指针null 字符串为空 正负号 上下溢出 Integer.MAX_VALUE (2^31-1) Integer.MIN_VALUE(-2^31) java 1234567891011121314151617181920212223242526272829303132333435363738394041public class Solution &#123; public int StrToInt(String str) &#123; if(str==null||str.length()==0)&#123;return 0;&#125;//空指针或空字符串 char[] c = str.toCharArray(); boolean minus=false; int i=0; //正负号 if(c[i]=='+')&#123; i++; &#125;else if(c[i]=='-')&#123; i++; minus=true; &#125; int num=0; if(i&lt;c.length)&#123; num = StrToIntCore(c,minus,i); &#125;else&#123; return num; &#125; return num; &#125; int StrToIntCore(char[] str,boolean minus,int i)&#123; int num=0; for(int j=i;j&lt;str.length;j++)&#123; if(str[j]&gt;='0'&amp;&amp;str[j]&lt;='9')&#123; int flag = minus?-1:1; num = num*10+flag*(str[j]-'0'); if((!minus&amp;&amp;num&gt;Integer.MAX_VALUE)||minus&amp;&amp;num&lt;Integer.MIN_VALUE)&#123;//上下溢出 num=0; break; &#125; &#125;else&#123;//非法数值 num=0; break; &#125; &#125; return num; &#125;&#125; 50.树中两个结点的最低公共祖先（二叉树）50.1 二叉搜索树的最低公共祖先二叉搜索树是经过排序的，位于左子树的节点都比父节点小，位于右子树的节点都比父节点大。既然要找最低的公共祖先节点，我们可以从根节点开始进行比较。若当前节点的值比两个节点的值都大，那么最低的祖先节点一定在当前节点的左子树中，则遍历当前节点的左子节点；反之，若当前节点的值比两个节点的值都小，那么最低的祖先节点一定在当前节点的右子树中，则遍历当前节点的右子节点；这样，直到找到一个节点，位于两个节点值的中间，则找到了最低的公共祖先节点。 java 1234567891011public class Solution &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if(root==null||root==q||root==p)return root; if(root.val&gt;p.val&amp;&amp;root.val&gt;q.val)&#123; return lowestCommonAncestor(root.left,p,q); &#125;else if(root.val&lt;p.val&amp;&amp;root.val&lt;q.val)&#123; return lowestCommonAncestor(root.right,p,q); &#125;else return root; &#125;&#125; 50.2 普通二叉树的最低公共祖先一种简单的方法是DFS分别寻找到两个节点p和q的路径，然后对比路径，查看他们的第一个分岔口，则为LCA。这个思路比较简单，代码写起来不如下面这种方法优雅： 我们仍然可以用递归来解决，递归寻找两个带查询LCA的节点p和q，当找到后，返回给它们的父亲。如果某个节点的左右子树分别包括这两个节点，那么这个节点必然是所求的解，返回该节点。否则，返回左或者右子树（哪个包含p或者q的就返回哪个）。复杂度O(n) java 12345678910public class Solution &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if(root==null||root==p||root==q)&#123;return root;&#125; TreeNode left = lowestCommonAncestor(root.left,p,q); TreeNode right = lowestCommonAncestor(root.right,p,q); if(left!=null&amp;&amp;right!=null)return root; return left!=null? left:right; &#125;&#125; 51. 数组中重复的数字（数组）在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是重复的数字2或者3。 java 12345678910111213141516171819202122232425public class Solution &#123; public boolean duplicate(int numbers[],int length,int [] duplication) &#123; if(numbers==null||length==0)&#123;return false;&#125;//空指针或空数组 // 判断数组是否合法,即每个数都在0~n-1之间 for(int i=0;i&lt;length;i++)&#123; if(numbers[i]&gt;length-1||numbers[i]&lt;0)&#123; return false; &#125; &#125; //若数值与下标不同，则调换位置； //比较位置下标为数值(numbers[i])的数值(numbers[numbers[i]])与该数值(numbers[i])是否一致，若一致，则说明有重复数字 for(int i=0;i&lt;length;i++)&#123; while(numbers[i]!=i)&#123; if(numbers[i]==numbers[numbers[i]])&#123; duplication[0] = numbers[i]; return true; &#125; int temp=numbers[i]; numbers[i]=numbers[temp]; numbers[temp]=temp; &#125; &#125; return false; &#125;&#125; 52. 构建乘积数组（数组）给定一个数组A[0,1,…,n-1],请构建一个数组$B[0,1,…,n-1]$,其中B中的元素$B[i]=A[0]A[1]…A[i-1]A[i+1]…A[n-1]$。不能使用除法。 java 12345678910111213141516171819import java.util.ArrayList;public class Solution &#123; public int[] multiply(int[] A) &#123; int length = A.length; int[] B = new int[length]; if(length!=0)&#123; B[0]=1; for(int i=1;i&lt;length;i++)&#123; B[i]=B[i-1]*A[i-1]; &#125; int temp=1; for(int j=length-2;j&gt;=0;j--)&#123; temp = temp*A[j+1]; B[j]=temp*B[j]; &#125; &#125; return B; &#125;&#125; 53. 正则表达式匹配（字符串）当模式中的第二个字符不是“*”时： 如果字符串第一个字符和模式中的第一个字符相匹配，那么字符串和模式都后移一个字符，然后匹配剩余的。 如果字符串第一个字符和模式中的第一个字符相不匹配，直接返回false。 而当模式中的第二个字符是“*”时： 如果字符串第一个字符跟模式第一个字符不匹配，则模式后移2个字符，继续匹配。如果字符串第一个字符跟模式第一个字符匹配，可以有3种匹配方式： 模式后移2字符，相当于$x*$被忽略； 字符串后移1字符，模式后移2字符； 字符串后移1字符，模式不变，即继续匹配字符下一位，因为*可以匹配多位； java 123456789101112131415161718192021222324252627282930313233343536public class Solution &#123; public boolean match(char[] str, char[] pattern) &#123; if (str == null || pattern == null) &#123; return false; &#125; int strIndex = 0; int patternIndex = 0; return matchCore(str, strIndex, pattern, patternIndex);&#125; public boolean matchCore(char[] str, int strIndex, char[] pattern, int patternIndex) &#123; //有效性检验：str到尾，pattern到尾，匹配成功 if (strIndex == str.length &amp;&amp; patternIndex == pattern.length) &#123; return true; &#125; //pattern先到尾，匹配失败 if (strIndex != str.length &amp;&amp; patternIndex == pattern.length) &#123; return false; &#125; //模式第2个是*，且字符串第1个跟模式第1个匹配,分3种匹配模式；如不匹配，模式后移2位 if (patternIndex + 1 &lt; pattern.length &amp;&amp; pattern[patternIndex + 1] == '*') &#123; if ((strIndex != str.length &amp;&amp; pattern[patternIndex] == str[strIndex]) || (pattern[patternIndex] == '.' &amp;&amp; strIndex != str.length)) &#123; return matchCore(str, strIndex, pattern, patternIndex + 2)//模式后移2，视为x*匹配0个字符 || matchCore(str, strIndex + 1, pattern, patternIndex + 2)//视为模式匹配1个字符 || matchCore(str, strIndex + 1, pattern, patternIndex);//*匹配1个，再匹配str中的下一个 &#125; else &#123; return matchCore(str, strIndex, pattern, patternIndex + 2); &#125; &#125; //模式第2个不是*，且字符串第1个跟模式第1个匹配，则都后移1位，否则直接返回false if ((strIndex != str.length &amp;&amp; pattern[patternIndex] == str[strIndex]) || (pattern[patternIndex] == '.' &amp;&amp; strIndex != str.length)) &#123; return matchCore(str, strIndex + 1, pattern, patternIndex + 1); &#125; return false; &#125;&#125; 54. 表示数值的字符串（字符串）请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。 java 12345678910111213141516171819202122232425262728public class Solution &#123; boolean isNumeric(char[] s) &#123; if(s.length==0) return false; if((s.length==1)&amp;&amp;(s[0]&lt;'0'||s[0]&gt;'9')) return false; if(s[0]=='+'||s[0]=='-')&#123; if(s.length==2&amp;&amp;(s[1]=='.')) return false; &#125;else if((s[0]&lt;'0'||s[0]&gt;'9')&amp;&amp;s[0]!='.') return false;//首位既不是符号也不是数字还不是小数点，当然是false int i = 1; while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; if(i&lt;s.length&amp;&amp;s[i]=='.')&#123; i++; //if(i&gt;=s.length) return false; while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; &#125; if(i&lt;s.length&amp;&amp;(s[i]=='e'||s[i]=='E'))&#123; i++; if((i&lt;s.length)&amp;&amp;(s[i]=='+'||s[i]=='-'))&#123; i++; if(i&lt;s.length) while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; else return false; &#125;else if(i&lt;s.length)&#123; while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; &#125;else return false; &#125; if(i&lt;s.length) return false; return true; &#125;&#125; 55. 字符流中第一个不重复的数组（字符串）使用一个HashMap来统计字符出现的次数，同时用一个ArrayList来记录输入流，每次返回第一个出现一次的字符都是在这个ArrayList（输入流）中的字符作为key去map中查找。 java 123456789101112131415161718192021222324252627282930313233import java.util.*;public class Solution &#123; //HashMap来统计字符出现的次数 HashMap&lt;Character, Integer&gt; map=new HashMap(); //ArrayList来记录输入流 ArrayList&lt;Character&gt; list=new ArrayList&lt;Character&gt;(); //Insert one char from stringstream public void Insert(char ch) &#123; if(map.containsKey(ch))&#123; int time = map.get(ch); time++; map.put(ch,time); &#125;else&#123; map.put(ch,1); &#125; list.add(ch); &#125; //return the first appearence once char in current stringstream public char FirstAppearingOnce() &#123; char ch='#'; for(char k : list)&#123;//list迭代 if(map.get(k)==1)&#123; ch=k; break;//得到第一个结果即可break &#125; &#125; return ch; &#125;&#125; 56. 链表中环的入口结点（链表）一个链表中包含环，如何找到环的入口结点？例如在下图的链表中，环的入口结点是结点3。 以3为例分析两个指针的移动规律。指针$P_1$和$P_2$在初始化时都指向链表的头结点。由于环中有4个结点，指针$P_1$先在链表上向前移动4步。接下来两个指针以相同的速度在链表上向前移动，直到它们相遇。它们相遇的结点正好是还的入口结点。 剩下的问题就是如何得到环中结点的数目。我们可以使用一快一慢两个指针。若两个指针相遇，说明链表中有环。两个指针相遇的结点一定是在环中的。可以从这个结点出发，一边继续向前移动一边计数，当再次回到这个结点时，就可以得到环中结点数了实现代码如下： java版本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; //找到一快一满指针相遇处的节点，相遇的节点一定是在环中 public static ListNode meetingNode(ListNode pHead) &#123; if(pHead == null)&#123;return null;&#125;//空链表处理 ListNode pslow = pHead.next; if(pslow == null)&#123;return null;&#125;//无环链表处理 ListNode pfast = pslow.next; while(pfast!=null &amp;&amp; pslow!=null)&#123; if(pslow==pfast)&#123;return pfast;&#125; pslow = pslow.next;//慢指针 pfast = pfast.next; if(pfast!=null)&#123; pfast = pfast.next; &#125;//块指针 &#125; return null; &#125; public ListNode EntryNodeOfLoop(ListNode pHead)&#123; ListNode meetingNode=meetingNode(pHead);//相遇结点 //环的结点个数 if(meetingNode==null)&#123;return null;&#125;//是否有环 int nodesInLoop = 1; ListNode p1=meetingNode; while(pnext!=meetingNode)&#123; p1=pnext; ++nodesInLoop; &#125; //p1慢指针,先往前走 p1=pHead; for(int i=0;i&lt;nodesInLoop;i++)&#123; p1=pnext; &#125; //p1,p2同步走，相遇的地方即为环入口 ListNode p2=pHead; while(p1!=p2)&#123; p1=pnext; p2=p2.next; &#125; return p1; &#125; &#125; python 版本 123456789101112131415161718192021222324252627282930313233343536373839# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def meetingNode(self,pHead): if not pHead: return None pslow =pHead.next if not pslow: return None pfast = pslow.next while pfast and pslow: if pslow==pfast: return pfast pslow = pslow.next pfast = pfast.next if pfast: pfast=pfast.next return None def EntryNodeOfLoop(self, pHead): meetingNode = self.meetingNode(pHead) if not meetingNode: return None nodesInLoop = 1 p1 = meetingNode while pnext!=meetingNode: p1=pnext nodesInLoop +=1 p1 = pHead for i in xrange(0,nodesInLoop): p1=pnext p2=pHead while p1!=p2: p1=pnext p2=p2.next return p1 57. 删除链表中重复的结点（链表）在一个排序的链表中，如何删除重复的结点？如在下图中重复结点被删除之后，链表如下图所示： 从头遍历整个链表。如果当前结点的值与下一个节点的值相同，那么它们就是重复的结点，都可以被删除。为了保证删除之后的链表仍然是相连的而没有中间断开，我们要把当前结点的前一个结点preNode和后面值比当前结点的值要大的结点相连。要确保preNode要始终与下一个没有重复的结点连接在一起。 实现代码如下： java递归版 12345678910111213141516171819202122232425262728/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode deleteDuplication(ListNode pHead) &#123; if (pHead == null || pHead.next == null) &#123; // 只有0个或1个结点，则返回 return pHead; &#125; if (pHead.val == pHead.next.val) &#123; // 当前结点是重复结点 ListNode pNode = pHead.next; while (pNode != null &amp;&amp; pNode.val == pHead.val) &#123; // 跳过值与当前结点相同的全部结点,找到第一个与当前结点不同的结点 pNode = pNode.next; &#125; return deleteDuplication(pNode); // 从第一个与当前结点不同的结点开始递归 &#125; else &#123; // 当前结点不是重复结点 pHead.next = deleteDuplication(pHead.next); // 保留当前结点，从下一个结点开始递归 return pHead; &#125; &#125;&#125; python版本 1234567891011121314151617# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def deleteDuplication(self, pHead): if not pHead or not pHead.next: return pHead if pHead.val==pHead.next.val: pNode = pHead.next while pNode and pNode.val == pHead.val: pNode = pNode.next return self.deleteDuplication(pNode) else: pHead.next = self.deleteDuplication(pHead.next) return pHead java非递归 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode deleteDuplication(ListNode pHead) &#123; if(pHead==null)return null; ListNode preNode = null; ListNode node = pHead; while(node!=null)&#123; ListNode nextNode = node.next; boolean needDelete = false; //需要删除重复节点的情况 if(nextNode!=null&amp;&amp;nextNode.val==node.val)&#123; needDelete = true; &#125; //不重复结点不删除 if(!needDelete)&#123; preNode = node; node = node.next; &#125; //重复节点删除 else&#123; int value = node.val; ListNode toBeDel = node; //连续重复结点 while(toBeDel != null &amp;&amp; toBeDel.val == value)&#123; nextNode = toBeDel.next; toBeDel = nextNode; if(preNode==null) pHead = nextNode; else preNode.next = nextNode; node = nextNode; &#125; &#125; &#125; return pHead; &#125;&#125; 58. 二叉树的下一个结点（二叉树）给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。我们可发现分成两大类： 有右子树的，那么下个结点就是右子树最左边的点；（eg：D，B，E，A，F，C，G） 没有右子树的，也可以分成两类，a)是父节点左孩子（eg：N，I，L） ，那么父节点就是下一个节点 ； b)是父节点的右孩子（eg：H，J，K，M）找他的父节点的父节点的父节点…直到当前结点是其父节点的左孩子位置。如果没有eg：M，那么他就是尾节点。 java 123456789101112131415161718public class Solution &#123; public TreeLinkNode GetNext(TreeLinkNode pNode) &#123; if(pNode==null)&#123;return null;&#125; if(pNode.right!=null)&#123; pNode = pNode.right; while(pNode.left!=null)&#123; pNode = pNode.left; &#125; return pNode; &#125; while(pNode.next!=null)&#123; if(pNode.next.left==pNode)return pNode.next; pNode = pNode.next; &#125; return null; &#125;&#125; 59. 对称的二叉树（二叉树）请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 如果先序遍历的顺序分为两种先左后右和先右后左两种顺序遍历，如果两者相等说明二叉树是对称的二叉树 java 12345678910111213public class Solution &#123; boolean isSymmetrical(TreeNode pRoot)&#123; return isSymmetrical(pRoot,pRoot); &#125; boolean isSymmetrical(TreeNode pRoot1,TreeNode pRoot2)&#123; if(pRoot1==null&amp;&amp;pRoot2==null)return true; if(pRoot1==null||pRoot2==null)return false; if(pRoot1.val==pRoot2.val)&#123;return isSymmetrical(pRoot1.left,pRoot2.right)&amp;&amp;isSymmetrical(pRoot1.right,pRoot2.left); &#125;else return false; &#125;&#125; 60. 把二叉树打印成多行（二叉树）从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。用end记录每层结点数目，start记录每层已经打印的数目，当start=end，重新建立list，开始下一层打印。 java 123456789101112131415161718192021222324public class Solution &#123; ArrayList&lt;ArrayList&lt;Integer&gt; &gt; Print(TreeNode pRoot) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if(pRoot==null)&#123;return result;&#125; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); queue.add(pRoot); int start = 0,end = 1; while(!queue.isEmpty())&#123; TreeNode treenode = queue.remove(); list.add(treenode.val); start++; if(treenode.left!=null)&#123;queue.add(treenode.left);&#125; if(treenode.right!=null)&#123;queue.add(treenode.right);&#125; if(start==end)&#123; end = queue.size(); start = 0; result.add(list); list = new ArrayList&lt;Integer&gt;(); &#125; &#125; return result; &#125; &#125; 61. 按S型打印二叉树（二叉树）请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。 java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.util.ArrayList;import java.util.Stack;public class Solution &#123; public ArrayList&lt;ArrayList&lt;Integer&gt; &gt; Print(TreeNode pRoot) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; alist =new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if(pRoot==null)return alist; Stack&lt;TreeNode&gt; stack1 = new Stack&lt;TreeNode&gt;(); stack1.add(pRoot); Stack&lt;TreeNode&gt; stack2 = new Stack&lt;TreeNode&gt;(); while(!stack1.isEmpty()||!stack2.isEmpty())&#123; if(!stack1.isEmpty())&#123; ArrayList&lt;Integer&gt; alist2 = new ArrayList&lt;Integer&gt;(); while(!stack1.isEmpty())&#123; TreeNode treenode=stack1.pop(); alist2.add(treenode.val); if(treenode.left!=null)&#123; stack2.add(treenode.left); &#125; if(treenode.right!=null)&#123; stack2.add(treenode.right); &#125; &#125; alist.add(alist2); &#125; else&#123; ArrayList&lt;Integer&gt; alist2 = new ArrayList&lt;Integer&gt;(); while(!stack2.isEmpty())&#123; TreeNode treenode = stack2.pop(); alist2.add(treenode.val); if(treenode.right!=null)&#123; stack1.add(treenode.right); &#125; if(treenode.left!=null)&#123; stack1.add(treenode.left); &#125; &#125; alist.add(alist2); &#125; &#125; return alist; &#125;&#125; 62. 序列化与反序列化二叉树（二叉树）请实现两个函数，分别用来序列化和反序列化二叉树算法思想：根据前序遍历规则完成序列化与反序列化。所谓序列化指的是遍历二叉树为字符串；所谓反序列化指的是依据字符串重新构造成二叉树。 依据前序遍历序列来序列化二叉树，因为前序遍历序列是从根结点开始的。当在遍历二叉树时碰到Null指针时，这些Null指针被序列化为一个特殊的字符“#”。另外，结点之间的数值用逗号隔开。 java 12345678910111213141516171819202122232425262728293031public class Solution &#123; int index = -1; //计数变量 String Serialize(TreeNode root) &#123; StringBuilder sb = new StringBuilder();//新建字符串 if(root == null)&#123; sb.append("#,"); return sb.toString(); &#125; //递归 sb.append(root.val + ","); sb.append(Serialize(root.left)); sb.append(Serialize(root.right)); return sb.toString(); &#125; TreeNode Deserialize(String str) &#123; index++; //int len = str.length(); //if(index &gt;= len)&#123; // return null; // &#125; String[] strr = str.split(","); TreeNode node = null; if(!strr[index].equals("#"))&#123; node = new TreeNode(Integer.valueOf(strr[index])); node.left = Deserialize(str); node.right = Deserialize(str); &#125; return node; &#125;&#125; 63. 二叉搜索树的第K个结点（二叉树）给定一颗二叉搜索树，请找出其中的第k大的结点。例如在下图二叉搜索树里，按结点数值大小顺序第三个结点的值是4.如果按照中序遍历的顺序遍历一颗二叉搜索树，遍历序列的数值是递增排序的，只需要用中序遍历算法遍历一颗二叉搜索树，就很容易找出它的第K大的结点。 java 123456789101112131415161718public class Solution &#123; int index = 0; //计数器 TreeNode KthNode(TreeNode root, int k) &#123; if(root != null)&#123; //中序遍历寻找第k个 TreeNode node = KthNode(root.left,k); if(node != null) return node; index ++; if(index == k) return root; node = KthNode(root.right,k); if(node != null) return node; &#125; return null; &#125;&#125; 64. 数据流中的中位数（二叉树）Java的PriorityQueue是从JDK1.5开始提供的新的数据结构接口，默认内部是自然排序，结果为小顶堆，也可以自定义排序器，比如下面反转比较，完成大顶堆。 为了保证插入新数据和取中位数的时间效率都高效，这里使用大顶堆+小顶堆的容器，并且满足： 两个堆中的数据数目差不能超过1，这样可以使中位数只会出现在两个堆的交接处； 大顶堆的所有数据都小于小顶堆，这样就满足了排序要求。 java 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.Comparator;import java.util.PriorityQueue; public class Solution &#123; int count=0; PriorityQueue&lt;Integer&gt; minHeap = new PriorityQueue&lt;Integer&gt;(); PriorityQueue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;Integer&gt;(11, new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; //PriorityQueue默认是小顶堆，实现大顶堆，需要反转默认排序器 return o2.compareTo(o1); &#125; &#125;); public void Insert(Integer num) &#123; if (count %2 == 0) &#123;//当数据总数为偶数时，新加入的元素，应当进入小根堆 //（注意不是直接进入小根堆，而是经大根堆筛选后取大根堆中最大元素进入小根堆） //1.新加入的元素先入到大根堆，由大根堆筛选出堆中最大的元素 maxHeap.offer(num); int filteredMaxNum = maxHeap.poll(); //2.筛选后的【大根堆中的最大元素】进入小根堆 minHeap.offer(filteredMaxNum); &#125; else &#123;//当数据总数为奇数时，新加入的元素，应当进入大根堆 //（注意不是直接进入大根堆，而是经小根堆筛选后取小根堆中最大元素进入大根堆） //1.新加入的元素先入到小根堆，由小根堆筛选出堆中最小的元素 minHeap.offer(num); int filteredMinNum = minHeap.poll(); //2.筛选后的【小根堆中的最小元素】进入大根堆 maxHeap.offer(filteredMinNum); &#125; count++;&#125; public Double GetMedian() &#123; if (count %2 == 0) &#123; return new Double((minHeap.peek() + maxHeap.peek())) / 2; &#125; else &#123; return new Double(minHeap.peek()); &#125;&#125;&#125; 65. 滑动窗口的最大值（数组）给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 java 123456789101112131415161718192021222324252627282930313233343536373839import java.util.ArrayList;import java.util.LinkedList;public class Solution &#123; public ArrayList&lt;Integer&gt; maxInWindows(int [] num, int size) &#123; ArrayList&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); if (num == null) &#123; return ret; &#125; if (num.length &lt; size || size &lt; 1) &#123; return ret; &#125; LinkedList&lt;Integer&gt; indexDeque = new LinkedList&lt;&gt;(); //前size-1个中，前面比num[i]小的，对应下标从下标队列移除； for (int i = 0; i &lt; size - 1; i++) &#123; if (!indexDeque.isEmpty() &amp;&amp; num[i] &gt; num[indexDeque.getLast()]) &#123; indexDeque.removeLast(); &#125; indexDeque.addLast(i); &#125; //从第size-1个开始；前面比num[i]小的，对应下标从下标队列移除； for (int i = size - 1; i &lt; num.length; i++) &#123; while(!indexDeque.isEmpty() &amp;&amp; num[i] &gt; num[indexDeque.getLast()]) &#123; indexDeque.removeLast(); &#125; //把下一个下标加入队列中 indexDeque.addLast(i); //当第一个数字的下标与当前处理的数字的下标之差大于或者等于滑动窗口的大小时，这个数字已经从窗口划出，可以移除了； if (i - indexDeque.getFirst() + 1 &gt; size) &#123; indexDeque.removeFirst(); &#125; //下标队列的第一个是滑动窗口最大值对应的下标； ret.add(num[indexDeque.getFirst()]); &#125; return ret; &#125;&#125; 66. 矩阵中的路径（数组）请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则该路径不能再进入该格子。 例如下面的矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 java 12345678910111213141516171819202122232425262728293031public class Solution &#123; public int movingCount(int threshold, int rows, int cols) &#123; boolean[] visited=new boolean[rows*cols]; return movingCountCore(threshold, rows, cols, 0,0,visited); &#125; private int movingCountCore(int threshold, int rows, int cols, int row,int col,boolean[] visited) &#123; if(row&lt;0||row&gt;=rows||col&lt;0||col&gt;=cols) return 0; int i=row*cols+col; if(visited[i]||!checkSum(threshold,row,col)) return 0; visited[i]=true; return 1+movingCountCore(threshold, rows, cols,row,col+1,visited) +movingCountCore(threshold, rows, cols,row,col-1,visited) +movingCountCore(threshold, rows, cols,row+1,col,visited) +movingCountCore(threshold, rows, cols,row-1,col,visited); &#125; private boolean checkSum(int threshold, int row, int col) &#123; int sum=0; while(row!=0)&#123; sum+=row%10; row=row/10; &#125; while(col!=0)&#123; sum+=col%10; col=col/10; &#125; if(sum&gt;threshold) return false; return true; &#125;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（4）：二叉树题解]]></title>
    <url>%2F2017%2F08%2F04%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%884%EF%BC%89%EF%BC%9A%E4%BA%8C%E5%8F%89%E6%A0%91%E9%A2%98%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[二叉树相关题解java实现。 一、重建二叉树（剑6）输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。 例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 根据前序遍历的特点，我们知道根结点为1 观察中序遍历。其中root节点G左侧的472必然是root的左子树，G右侧的5386必然是root的右子树。 观察左子树472，左子树的中的根节点必然是大树的root的leftchild。在前序遍历中，大树的root的leftchild位于root之后，所以左子树的根节点为2。 同样的道理，root的右子树节点5386中的根节点也可以通过前序遍历求得。在前序遍历中，一定是先把root和root的所有左子树节点遍历完之后才会遍历右子树，并且遍历的左子树的第一个节点就是左子树的根节点。同理，遍历的右子树的第一个节点就是右子树的根节点。 观察发现，上面的过程是递归的。先找到当前树的根节点，然后划分为左子树，右子树，然后进入左子树重复上面的过程，然后进入右子树重复上面的过程。最后就可以还原一棵树了。 该步递归的过程可以简洁表达如下： 确定根,确定左子树，确定右子树。 在左子树中递归。 在右子树中递归。 打印当前根。 递归代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class Solution &#123; public TreeNode reConstructBinaryTree(int [] pre,int [] in) &#123; return reConBTree(pre,0,pre.length-1,in,0,in.length-1); &#125; public TreeNode reConBTree(int [] pre,int preleft,int preright,int [] in,int inleft,int inright)&#123; if(preleft &gt; preright || inleft&gt; inright)//当到达边界条件时候返回null return null; //新建一个TreeNode TreeNode root = new TreeNode(pre[preleft]); //对中序数组进行输入边界的遍历 for(int i = inleft; i&lt;= inright; i++)&#123; if(pre[preleft] == in[i])&#123; //重构左子树，注意边界条件 root.left = reConBTree(pre,preleft+1,preleft+i-inleft,in,inleft,i-1); //重构右子树，注意边界条件 root.right = reConBTree(pre,preleft+i+1-inleft,preright,in,i+1,inright); &#125; &#125; return root; &#125;&#125;``` ## 二、树的子结构（剑18）输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构）要查找树A中是否存在和树B结构一样的子树，我们可以分成两步：第一步在树A中找到和B的根结点的值一样的结点R，第二步再判断树A以R为根结点的子树是不是包含和树B一样的结构。第一步在树A中查找与根结点的值一样的结点，实际上就是树的遍历。对二叉树这种数据结构熟悉的读者自然知道可以用递归的方法去遍历，也可以用循环的方法去遍历。由于递归的代码实现比较简洁，面试时如果没有特别要求，通常会采用递归的方式。参考代码如下：&gt;java第一步```javapublic boolean HasSubtree(TreeNode root1,TreeNode root2) &#123; boolean result = false; //一定要注意边界条件的检查，即检查空指针。否则程序容易奔溃，面试时尤其要注意。这里当Tree1和Tree2都不为零的时候，才进行比较。否则直接返回false if(root1!=null&amp;&amp;root2!=null)&#123; ////如果找到了对应Tree2的根节点的点 if(root1.val==root2.val)&#123; //以这个根节点为为起点判断是否包含Tree2 result = DoesTree1HaveTree2(root1,root2); &#125; //如果找不到，那么就再去root的左儿子当作起点，去判断是否包含Tree2 if(!result)&#123; result=HasSubtree(root1.left,root2); &#125; //如果还找不到，那么就再去root的右儿子当作起点，去判断是否包含Tree2 if(!result)&#123; result=HasSubtree(root1.right,root2); &#125; &#125; return result; &#125; 第二步是判断树A中以R为根结点的子树是不是和树B具有相同的结构。同样，我们也可以用递归的思路来考虑：如果结点R的值和树B的根结点不同，则以R为根结点的子树和树B一定不具有相同的结点；如果他们的值相同，则递归地判断它们各自的左右结点的值是不是相同。递归的终止条件是我们达到了树A或者树B的叶结点。 代码如下： java 12345678910111213141516public boolean DoesTree1HaveTree2(TreeNode root1,TreeNode root2)&#123; //如果Tree2已经遍历完了都能对应的上，返回true if(root2==null)&#123; return true; &#125; //如果Tree2还没有遍历完，Tree1却遍历完了。返回false if(root1==null)&#123; return false; &#125; //如果其中有一个点没有对应上，返回false if(root1.val!=root2.val)&#123; return false; &#125; //如果根节点对应的上，那么就分别去左右子节点里面匹配 return DoesTree1HaveTree2(root1.left,root2.left)&amp;&amp;DoesTree1HaveTree2(root1.right,root2.right); &#125; 二叉树相关的代码有大量的指针操作，每一次使用指针的时候，我们都要问自己这个指针有没有可能是NULL，如果是NULL该怎么处理。 三、二叉树的镜像（剑19）操作给定的二叉树，将其变换为源二叉树的镜像。 123456789101112131415161718192021public class Solution &#123; public void Mirror(TreeNode root) &#123; //边界 if(root==null) return; if(root.left==null&amp;&amp;root.right==null) return; //交换左右子树 TreeNode temp = root.left; root.left=root.right; root.right=temp; //递归 if(root.left!=null)&#123; Mirror(root.left); &#125; if(root.right!=null)&#123; Mirror(root.right); &#125; &#125;&#125; 四、从上往下打印二叉树（剑23）从上往下打印出二叉树的每个节点，同层节点从左至右打印。 每次打印一个结点时，如果该结点有子结点，则把该结点的子结点放到队列的末尾。接下来到队列的头部取出最早进入队列的结点，重复前面的打印操作。 java 12345678910111213141516171819import java.util.ArrayList;import java.util.LinkedList;public class Solution &#123; public ArrayList&lt;Integer&gt; PrintFromTopToBottom(TreeNode root) &#123; ArrayList&lt;Integer&gt; List=new ArrayList&lt;Integer&gt;(); if(root==null)&#123;return List;&#125; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.add(root);//先把根结点加入队列q while(!queue.isEmpty())&#123;//队列非空时 TreeNode treenode=queue.remove();//取出队列头结点 if(treenode.left!=null)&#123;queue.add(treenode.left);&#125;//向队列加入左孩子（若有） if(treenode.right!=null)&#123;queue.add(treenode.right);&#125;//向队列加入右孩子（若有） List.add(treenode.val);//加到打印列表中 &#125; return List; &#125;&#125; 五、二叉搜索树的后序遍历序列（剑24）输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 在后序遍历得到的序列中，最后一个数字是树的根结点的值。数组中前面的数字可以分成两部分：第一部分是左子树结点的值，它们都比根结点小；第二部分是右子树结点的值，它们都比根结点大。 java 1234567891011121314151617181920212223242526272829303132import java.util.Arrays;public class Solution &#123; public boolean VerifySquenceOfBST(int [] sequence) &#123; int length = sequence.length; if(sequence==null||length==0)&#123;return false;&#125; int root = sequence[length-1];//根结点 int i=0; //外部初始化 //找到左子树的最后一个结点位置 for(;i&lt;length-1;i++)&#123; if(sequence[i]&gt;root)&#123; break; &#125; &#125; //如果右子树的结点值小于根结点的值，则返回false for(int j=i;j&lt;length-1;j++)&#123; if(sequence[j]&lt;root)&#123; return false; &#125; &#125; //初始化 boolean left=true; boolean right=true; //递归左右子树 if(i&gt;0)&#123; left = VerifySquenceOfBST(Arrays.copyOfRange(sequence,0,i));//Arrays的copyOfRange方法 &#125; if(i&lt;length-1)&#123; right = VerifySquenceOfBST(Arrays.copyOfRange(sequence,i,length-1)); &#125; return left&amp;&amp;right; &#125;&#125; 六、二叉树中和为某一值的路径（剑25）输入一颗二叉树和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。 java 12345678910111213141516171819public class Solution &#123; private ArrayList&lt;ArrayList&lt;Integer&gt;&gt; listAll = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); private ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindPath(TreeNode root,int target) &#123; if(root == null) return listAll; list.add(root.val); target -= root.val;//每次减去结点的值 //如果target等于0，则说明这条路径和为target，添加到listAll中 if(target == 0 &amp;&amp; root.left == null &amp;&amp; root.right == null) listAll.add(new ArrayList&lt;Integer&gt;(list));//因为add添加的是引用，如果不new一个的话，后面的操作会更改listAll中list的值 //向左孩子递归 if(root.left!=null)FindPath(root.left, target); //向右孩子递归 if(root.right!=null)FindPath(root.right, target); //如果不满足条件，则回到父节点； list.remove(list.size()-1); return listAll; &#125;&#125; 七、二叉搜索树与双向链表（剑27）输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 java 12345678910111213141516171819202122232425public class Solution &#123; TreeNode head = null; TreeNode realHead = null; public TreeNode Convert(TreeNode pRootOfTree) &#123; ConvertSub(pRootOfTree); return realHead//realHead是每个子树排序后的第一个结点，head是排序后的最后一个结点; &#125; private void ConvertSub(TreeNode pRootOfTree) &#123; //递归中序遍历 if(pRootOfTree==null) return; ConvertSub(pRootOfTree.left); if (head == null) &#123; //初始处 head = pRootOfTree; realHead = pRootOfTree; &#125; else &#123; //前两句实现双向，第三句跳到下一个节点。 head.right = pRootOfTree; pRootOfTree.left = head; head = pRootOfTree; &#125; ConvertSub(pRootOfTree.right); &#125;&#125; 八、二叉树的深度（剑39.1）输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 Java 经典的求二叉树深度 递归写法 java 12345678public class Solution &#123; public int TreeDepth(TreeNode root) &#123; if(root==null)return 0; int nleft = TreeDepth(root.left); int nright = TreeDepth(root.right); return nleft&gt;nright?(nleft+1):(nright+1); &#125;&#125; 九、平衡二叉树（剑39.2）输入一棵二叉树，判断该二叉树是否是平衡二叉树。 有了求二叉树的深度的经验之后，我们就很容易想到一个思路：在遍历树的每个结点的时候，调用函数TreeDepth得到它的左右子树的深度。如果每个结点的左右子树的深度相差都不超过1，按照定义它就是一颗平衡的二叉树。 java 123456789101112131415161718public class Solution &#123; public boolean IsBalanced_Solution(TreeNode root) &#123; if(root==null)return true; int left = TreeDepth(root.left); int right = TreeDepth(root.right); int diff = left-right; if(diff&gt;1||diff&lt;-1) return false; return IsBalanced_Solution(root.left)&amp;&amp;IsBalanced_Solution(root.right); &#125; public int TreeDepth(TreeNode root) &#123; if(root==null)return 0; int nleft = TreeDepth(root.left); int nright = TreeDepth(root.right); return nleft&gt;nright?(nleft+1):(nright+1); &#125;&#125; 十、二叉搜索树的最低公共祖先（剑50.1）二叉搜索树是经过排序的，位于左子树的节点都比父节点小，位于右子树的节点都比父节点大。既然要找最低的公共祖先节点，我们可以从根节点开始进行比较。若当前节点的值比两个节点的值都大，那么最低的祖先节点一定在当前节点的左子树中，则遍历当前节点的左子节点；反之，若当前节点的值比两个节点的值都小，那么最低的祖先节点一定在当前节点的右子树中，则遍历当前节点的右子节点；这样，直到找到一个节点，位于两个节点值的中间，则找到了最低的公共祖先节点。 java 1234567891011public class Solution &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if(root==null||root==q||root==p)return root; if(root.val&gt;p.val&amp;&amp;root.val&gt;q.val)&#123; return lowestCommonAncestor(root.left,p,q); &#125;else if(root.val&lt;p.val&amp;&amp;root.val&lt;q.val)&#123; return lowestCommonAncestor(root.right,p,q); &#125;else return root; &#125;&#125; 十一、普通二叉树的最低公共祖先（剑50.2）一种简单的方法是DFS分别寻找到两个节点p和q的路径，然后对比路径，查看他们的第一个分岔口，则为LCA。这个思路比较简单，代码写起来不如下面这种方法优雅： 我们仍然可以用递归来解决，递归寻找两个带查询LCA的节点p和q，当找到后，返回给它们的父亲。如果某个节点的左右子树分别包括这两个节点，那么这个节点必然是所求的解，返回该节点。否则，返回左或者右子树（哪个包含p或者q的就返回哪个）。复杂度O(n) java 12345678910public class Solution &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if(root==null||root==p||root==q)&#123;return root;&#125; TreeNode left = lowestCommonAncestor(root.left,p,q); TreeNode right = lowestCommonAncestor(root.right,p,q); if(left!=null&amp;&amp;right!=null)return root; return left!=null? left:right; &#125;&#125; 十二、二叉树的下一个结点（剑58）给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 我们可发现分成两大类： 有右子树的，那么下个结点就是右子树最左边的点；（eg：D，B，E，A，F，C，G） 没有右子树的，也可以分成两类，a)是父节点左孩子（eg：N，I，L） ，那么父节点就是下一个节点 ； b)是父节点的右孩子（eg：H，J，K，M）找他的父节点的父节点的父节点…直到当前结点是其父节点的左孩子位置。如果没有eg：M，那么他就是尾节点。 java 123456789101112131415161718public class Solution &#123; public TreeLinkNode GetNext(TreeLinkNode pNode) &#123; if(pNode==null)&#123;return null;&#125; if(pNode.right!=null)&#123; pNode = pNode.right; while(pNode.left!=null)&#123; pNode = pNode.left; &#125; return pNode; &#125; while(pNode.next!=null)&#123; if(pNode.next.left==pNode)return pNode.next; pNode = pNode.next; &#125; return null; &#125;&#125; 十三、对称的二叉树（剑59）请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 如果先序遍历的顺序分为两种先左后右和先右后左两种顺序遍历，如果两者相等说明二叉树是对称的二叉树 java 12345678910111213public class Solution &#123; boolean isSymmetrical(TreeNode pRoot)&#123; return isSymmetrical(pRoot,pRoot); &#125; boolean isSymmetrical(TreeNode pRoot1,TreeNode pRoot2)&#123; if(pRoot1==null&amp;&amp;pRoot2==null)return true; if(pRoot1==null||pRoot2==null)return false; if(pRoot1.val==pRoot2.val)&#123;return isSymmetrical(pRoot1.left,pRoot2.right)&amp;&amp;isSymmetrical(pRoot1.right,pRoot2.left); &#125;else return false; &#125;&#125; 十四、把二叉树打印成多行（剑60）从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。用end记录每层结点数目，start记录每层已经打印的数目，当start=end，重新建立list，开始下一层打印。 java 123456789101112131415161718192021222324public class Solution &#123; ArrayList&lt;ArrayList&lt;Integer&gt; &gt; Print(TreeNode pRoot) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if(pRoot==null)&#123;return result;&#125; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); queue.add(pRoot); int start = 0,end = 1; while(!queue.isEmpty())&#123; TreeNode treenode = queue.remove(); list.add(treenode.val); start++; if(treenode.left!=null)&#123;queue.add(treenode.left);&#125; if(treenode.right!=null)&#123;queue.add(treenode.right);&#125; if(start==end)&#123; end = queue.size(); start = 0; result.add(list); list = new ArrayList&lt;Integer&gt;(); &#125; &#125; return result; &#125; &#125; 十五、按S型打印二叉树（剑61）请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。 java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.util.ArrayList;import java.util.Stack;public class Solution &#123; public ArrayList&lt;ArrayList&lt;Integer&gt; &gt; Print(TreeNode pRoot) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; alist =new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if(pRoot==null)return alist; Stack&lt;TreeNode&gt; stack1 = new Stack&lt;TreeNode&gt;(); stack1.add(pRoot); Stack&lt;TreeNode&gt; stack2 = new Stack&lt;TreeNode&gt;(); while(!stack1.isEmpty()||!stack2.isEmpty())&#123; if(!stack1.isEmpty())&#123; ArrayList&lt;Integer&gt; alist2 = new ArrayList&lt;Integer&gt;(); while(!stack1.isEmpty())&#123; TreeNode treenode=stack1.pop(); alist2.add(treenode.val); if(treenode.left!=null)&#123; stack2.add(treenode.left); &#125; if(treenode.right!=null)&#123; stack2.add(treenode.right); &#125; &#125; alist.add(alist2); &#125; else&#123; ArrayList&lt;Integer&gt; alist2 = new ArrayList&lt;Integer&gt;(); while(!stack2.isEmpty())&#123; TreeNode treenode = stack2.pop(); alist2.add(treenode.val); if(treenode.right!=null)&#123; stack1.add(treenode.right); &#125; if(treenode.left!=null)&#123; stack1.add(treenode.left); &#125; &#125; alist.add(alist2); &#125; &#125; return alist; &#125;&#125; 十六、序列化与反序列化二叉树（剑62）请实现两个函数，分别用来序列化和反序列化二叉树算法思想：根据前序遍历规则完成序列化与反序列化。所谓序列化指的是遍历二叉树为字符串；所谓反序列化指的是依据字符串重新构造成二叉树。 依据前序遍历序列来序列化二叉树，因为前序遍历序列是从根结点开始的。当在遍历二叉树时碰到Null指针时，这些Null指针被序列化为一个特殊的字符“#”。另外，结点之间的数值用逗号隔开。 java 12345678910111213141516171819202122232425262728293031public class Solution &#123; int index = -1; //计数变量 String Serialize(TreeNode root) &#123; StringBuilder sb = new StringBuilder();//新建字符串 if(root == null)&#123; sb.append("#,"); return sb.toString(); &#125; //递归 sb.append(root.val + ","); sb.append(Serialize(root.left)); sb.append(Serialize(root.right)); return sb.toString(); &#125; TreeNode Deserialize(String str) &#123; index++; //int len = str.length(); //if(index &gt;= len)&#123; // return null; // &#125; String[] strr = str.split(","); TreeNode node = null; if(!strr[index].equals("#"))&#123; node = new TreeNode(Integer.valueOf(strr[index])); node.left = Deserialize(str); node.right = Deserialize(str); &#125; return node; &#125;&#125; 十七、二叉搜索树的第K个结点（剑63）给定一颗二叉搜索树，请找出其中的第k大的结点。例如在下图二叉搜索树里，按结点数值大小顺序第三个结点的值是4.如果按照中序遍历的顺序遍历一颗二叉搜索树，遍历序列的数值是递增排序的，只需要用中序遍历算法遍历一颗二叉搜索树，就很容易找出它的第K大的结点。 java 123456789101112131415161718public class Solution &#123; int index = 0; //计数器 TreeNode KthNode(TreeNode root, int k) &#123; if(root != null)&#123; //中序遍历寻找第k个 TreeNode node = KthNode(root.left,k); if(node != null) return node; index ++; if(index == k) return root; node = KthNode(root.right,k); if(node != null) return node; &#125; return null; &#125;&#125; 十八、数据流中的中位数（剑64）Java的PriorityQueue是从JDK1.5开始提供的新的数据结构接口，默认内部是自然排序，结果为小顶堆，也可以自定义排序器，比如下面反转比较，完成大顶堆。 为了保证插入新数据和取中位数的时间效率都高效，这里使用大顶堆+小顶堆的容器，并且满足： 两个堆中的数据数目差不能超过1，这样可以使中位数只会出现在两个堆的交接处； 大顶堆的所有数据都小于小顶堆，这样就满足了排序要求。 java 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.Comparator;import java.util.PriorityQueue; public class Solution &#123; int count=0; PriorityQueue&lt;Integer&gt; minHeap = new PriorityQueue&lt;Integer&gt;(); PriorityQueue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;Integer&gt;(11, new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; //PriorityQueue默认是小顶堆，实现大顶堆，需要反转默认排序器 return o2.compareTo(o1); &#125; &#125;); public void Insert(Integer num) &#123; if (count %2 == 0) &#123;//当数据总数为偶数时，新加入的元素，应当进入小根堆 //（注意不是直接进入小根堆，而是经大根堆筛选后取大根堆中最大元素进入小根堆） //1.新加入的元素先入到大根堆，由大根堆筛选出堆中最大的元素 maxHeap.offer(num); int filteredMaxNum = maxHeap.poll(); //2.筛选后的【大根堆中的最大元素】进入小根堆 minHeap.offer(filteredMaxNum); &#125; else &#123;//当数据总数为奇数时，新加入的元素，应当进入大根堆 //（注意不是直接进入大根堆，而是经小根堆筛选后取小根堆中最大元素进入大根堆） //1.新加入的元素先入到小根堆，由小根堆筛选出堆中最小的元素 minHeap.offer(num); int filteredMinNum = minHeap.poll(); //2.筛选后的【小根堆中的最小元素】进入大根堆 maxHeap.offer(filteredMinNum); &#125; count++;&#125; public Double GetMedian() &#123; if (count %2 == 0) &#123; return new Double((minHeap.peek() + maxHeap.peek())) / 2; &#125; else &#123; return new Double(minHeap.peek()); &#125;&#125;&#125; 十九、 二叉树最大路径和（leetcode 124）一个很有意思的问题，一个社区，所有的房子构成一棵二叉树，每个房子里有一定价值的财物，这棵二叉树有一个根节点root。如果相邻的两座房子同时被进入，就会触发警报。一个小偷，最初只能访问root节点，并可以通过二叉树的边访问房子（注：访问不意味着进入），请问不触发警报的前提下他能偷到的财物的最大价值是多少？ 123456以下面这棵二叉树为例，最多能偷走3+3+1=7的财物 3 / \ 2 3 \ \ 3 1 分析：这个问题乍一看上去可能没什么思路，但是如果是用递归，可以很优雅的解决这个问题，这需要读者对递归有比较深刻的理解。下面给出解决这个问题的java代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.util.HashMap;import java.util.Map;import common.TreeNode;public class Solution &#123; //使用一个cache 缓存以每个节点为根节点的rob方法返回值，减少计算量 Map&lt;TreeNode, Integer&gt; cache = new HashMap&lt;TreeNode, Integer&gt;(); public int rob(TreeNode root) &#123; //如果当前节点为空 直接返回0 if(null == root)&#123; return 0; &#125; //首先查看缓存中有没有这个节点的rob方法返回值 if(null != cache.get(root))&#123; return cache.get(root); &#125; //计算当前节点左孩子的rob方法返回值 int maxLeft = rob(root.left); //计算当前节点右孩子的rob方法返回值 int maxRight = rob(root.right); int maxLeftLeft = 0; int maxLeftRight = 0; //如果当前节点有左孩子 if(null != root.left)&#123; //计算其左孩子的左孩子的rob值 maxLeftLeft = rob(root.left.left); //计算其左孩子的右孩子的rob值 maxLeftRight = rob(root.left.right); &#125; int maxRightLeft = 0; int maxRightRight = 0; //如果当前节点有右孩子 if(null != root.right)&#123; //计算其右孩子的左孩子的rob值 maxRightLeft = rob(root.right.left); //计算其右孩子的右孩子的rob值 maxRightRight = rob(root.right.right); &#125; //不偷当前节点能偷到的财物的最大值 int notIncludeCurrentNodeMax = maxLeft + maxRight; //偷当前节点能偷到的财物的最大值 int includeCurrentNodeMax = maxLeftLeft + maxLeftRight + maxRightLeft + maxRightRight + root.val; //以其中的较大值作为当前节点的rob方法返回值 int res = notIncludeCurrentNodeMax &gt; includeCurrentNodeMax ? notIncludeCurrentNodeMax : includeCurrentNodeMax; //缓存当前节点的rob方法返回值 cache.put(root, res); return res; &#125;&#125; 面经中出现过的二叉树题：已整理 一、给定二叉树的先序跟后序遍历，能不能将二叉树重建（不能，因为先序：父节点-左节点-右节点，后序：左节点-右节点-父节点，两者的拓扑序列是一样的，所以无法建立），如果给出一个二叉搜索树的后续能不能建立（可以，因为只要将遍历结果排序就可以得到中序结果）。 二、判断一棵树是否是另一棵的子树。 三、 翻转二叉树 四、二叉树打印路径 六、输入一颗二叉树和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。二叉树路径和等于某个值的所有路径输出。 七、排序二叉树转双向链表 十、输入根节点和两个子节点，找到最小公共父节点，2叉树只有孩子节点；查找二叉树某两个节点的最近公共祖先 十二、输入二叉树节点 P, 找到二叉树中序遍历 P 的下一个节点。 十四、把二叉树打印成多行；中序遍历二叉树，利用O(1)空间统计遍历的每个节点的层次 十五、手写S型遍历二叉树，如何优化，最后说了个空间优化的方法 十九、leetcode 124 二叉树最大路径和 未整理求完全二叉树的节点个数，要求最优解法，我写的是递归，复杂度O(logn*logn)]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>二叉树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（3）：字符串题解]]></title>
    <url>%2F2017%2F08%2F04%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%883%EF%BC%89%EF%BC%9A%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%A2%98%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[字符串相关题解java实现 一、替换空格（剑4）请实现一个函数，把字符串中的每个空格替换成”%20”。例如输入“We are happy.”，则输出“We%20are%20happy” 网络编程中，要把特殊符号转换成服务器可识别的字符。转换的规则是在“%”后面跟上ASCII码的两位十六进制的表示。比如空格的ASCII码是32，即十六进制的0X20，因此空格被替换成“%20”。 问题1：替换字符串，是在原来的字符串上做替换，还是新开辟一个字符串做替换！问题2：在当前字符串替换，怎么替换才更有效率（不考虑java里现有的replace方法）。从前往后替换，后面的字符要不断往后移动，要多次移动，所以效率低下；从后往前，先计算需要多少空间，然后从后往前移动，则每个字符只为移动一次，这样效率更高一点。 123456789101112131415161718192021222324public class Solution &#123; public String replaceSpace(StringBuffer str) &#123; int spacenum = 0;//spacenum为计算空格数 for(int i=0;i&lt;str.length();i++)&#123; if(str.charAt(i)==' ') spacenum++; &#125; int indexold = str.length()-1;//indexold为为替换前的str下标 int newlength = str.length()+2*spacenum;//计算空格转换成%20之后的str长度 int indexnew = newlength-1;//indexold为为把空格替换为%20后的str下标 str.setLength(newlength);//使str的长度扩大到转换成%20之后的长度,防止下标越界,setLength方法 for(;indexold&gt;=0&amp;&amp;indexold&lt;newlength;--indexold)&#123; if(str.charAt(indexold)==' ')&#123;//charAt方法 str.setCharAt(indexnew--,'0'); str.setCharAt(indexnew--,'2'); str.setCharAt(indexnew--,'%'); &#125; else&#123; str.setCharAt(indexnew--,str.charAt(indexold)); &#125; &#125; return str.toString(); &#125;&#125; 二、字符串的排列（剑28）输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。 首先我要打印abc的全排列，就是第一步把a 和bc交换（得到bac,cab），这需要一个for循环，循环里面有一个swap，交换之后就相当于不管第一步了，进入下一步递归，所以跟一个递归函数， 完成递归之后把交换的换回来，变成原来的字串 12345678abc 为例子：1. 固定a, 求后面bc的全排列： abc, acb。 求完后，a 和 b交换； 得到bac,开始第二轮2. 固定b, 求后面ac的全排列： bac, bca。 求完后，b 和 c交换； 得到cab,开始第三轮3. 固定c, 求后面ba的全排列： cab, cba 即递归树： str: a b c ab ac ba bc ca cb result: abc acb bac bca cab cba java 12345678910111213141516171819202122232425262728293031import java.util.ArrayList;import java.util.*;public class Solution &#123; public ArrayList&lt;String&gt; Permutation(String str) &#123; ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); if(str.length()==0) return list; char[] array = str.toCharArray(); permutation(array,0,list); Collections.sort(list); return list; &#125; public void permutation(char[] array,int begin,ArrayList&lt;String&gt; list) &#123; if(begin == array.length-1) &#123; list.add(String.valueOf(array)); &#125;else &#123; for(int i=begin;i&lt;array.length;++i) &#123; if(i==begin || array[i]!=array[begin]) &#123; swap(array,begin,i); permutation(array,begin+1,list); swap(array,begin,i); &#125; &#125; &#125; &#125; public void swap(char[] array,int i,int j) &#123; char temp = array[i]; array[i] = array[j]; array[j] = temp; &#125;&#125; 三、第一次只出现一次的字符（字符串）在一个字符串(1&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置. 我们可以使用一个容器来存放每个字符的出现次数。在这个数据容器中可以根据字符来查找出现的次数，也就是这个容器的作用是把一个字符映射成一个数字。在常用的数据容器中，哈希表正是这个用途。 为了解决这个问题，我们可以定义哈希表的键值（Key）是字符，而值（Value）是该字符出现的次数。同时我们还需要从头开始扫描字符串两次。第一次扫面字符串时，每扫到一个字符就在哈希表的对应项把次数加1.接下来第二次扫描时，每扫描到一个字符就能在哈希表中得到该字符出现的次数，这样第一个只出现一次的字符就是符合要求的输出。 需要涉及到Java中HashMap工作原理及实现，资料链接 java 1234567891011121314151617181920212223import java.util.HashMap;public class Solution &#123; public int FirstNotRepeatingChar(String str) &#123; HashMap&lt;Character,Integer&gt; map = new HashMap&lt;Character,Integer&gt;(); for(int i=0;i&lt;str.length();i++)&#123; char c = str.charAt(i);//charAt方法，获得位置i的串 if(map.containsKey(c))&#123;//HashMap的containKey方法； int time = map.get(c);//HashMap的get方法，得到Key c的Value； time++; map.put(c,time);//HashMap的put方法，将Key c的Value置为time； &#125;else&#123; map.put(c,1); &#125; &#125; for(int i=0;i&lt;str.length();i++)&#123; char c = str.charAt(i); if(map.get(c)==1)&#123; return i; &#125; &#125; return -1; &#125;&#125; 四、翻转单词顺序（剑42.1）输入一个英文句子，翻转句子中单词的顺序，但单词内字符的顺序不变。为简单起见，标点符号和普通字母一样处理。例如输入字符串“I am a student.”，则输出“student. a am I”。可以先翻转整个句子，然后，依次翻转每个单词。依据空格来确定单词的起始和终止位置 java 1234567891011121314151617181920212223242526public class Solution &#123; public String ReverseSentence(String str) &#123; char[] chars = str.toCharArray(); reverse(chars,0,chars.length-1); int blank = -1; for(int i =0;i&lt;chars.length-1;i++)&#123; if(chars[i]==' ')&#123; int nextblank = i; reverse(chars,blank+1,nextblank-1); blank = nextblank; &#125; &#125; reverse(chars,blank+1,chars.length-1);//单独翻转最后一个单词 return new String(chars); &#125; public void reverse(char[] chars,int low,int high)&#123; while(low&lt;high)&#123; char temp = chars[low]; chars[low]=chars[high]; chars[high]=temp; low++; high--; &#125; &#125;&#125; 五、左旋转字符串（剑42.2）汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。 以“abcdefg”为例，我们可以把它分为两部分。由于想把它的前两个字符移到后面，我们就把钱两个字符分到第一部分，把后面的所有字符都分到第二部分。然后先翻转这两部分，于是就得到“bagfedc”。接下来在翻转整个字符串，得到的”cdefgab”刚好就是把原始字符串左旋转2位的结果。123456789101112131415161718192021public class Solution &#123; public String LeftRotateString(String str,int n) &#123; char[] chars = str.toCharArray(); if(chars.length &lt; n) return &quot;&quot;; reverse(chars, 0, n-1); reverse(chars, n, chars.length-1); reverse(chars, 0, chars.length-1); return new String(chars); &#125; public void reverse(char[] chars,int low,int high)&#123; char temp; while(low&lt;high)&#123; temp = chars[low]; chars[low] = chars[high]; chars[high] = temp; low++; high--; &#125; &#125;&#125; 六、 把字符串转换成整数（剑49）将一个字符串转换成一个整数，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。问题不难，但是要把很多特殊情况都考虑进去，却并不容易。需要考虑的特殊情况有以下几个： 空指针null 字符串为空 正负号 上下溢出 Integer.MAX_VALUE (2^31-1) Integer.MIN_VALUE(-2^31) java 1234567891011121314151617181920212223242526272829303132333435363738394041public class Solution &#123; public int StrToInt(String str) &#123; if(str==null||str.length()==0)&#123;return 0;&#125;//空指针或空字符串 char[] c = str.toCharArray(); boolean minus=false; int i=0; //正负号 if(c[i]=='+')&#123; i++; &#125;else if(c[i]=='-')&#123; i++; minus=true; &#125; int num=0; if(i&lt;c.length)&#123; num = StrToIntCore(c,minus,i); &#125;else&#123; return num; &#125; return num; &#125; int StrToIntCore(char[] str,boolean minus,int i)&#123; int num=0; for(int j=i;j&lt;str.length;j++)&#123; if(str[j]&gt;='0'&amp;&amp;str[j]&lt;='9')&#123; int flag = minus?-1:1; num = num*10+flag*(str[j]-'0'); if((!minus&amp;&amp;num&gt;Integer.MAX_VALUE)||minus&amp;&amp;num&lt;Integer.MIN_VALUE)&#123;//上下溢出 num=0; break; &#125; &#125;else&#123;//非法数值 num=0; break; &#125; &#125; return num; &#125;&#125; 七、正则表达式匹配（剑53）当模式中的第二个字符不是“*”时： 如果字符串第一个字符和模式中的第一个字符相匹配，那么字符串和模式都后移一个字符，然后匹配剩余的。 如果字符串第一个字符和模式中的第一个字符相不匹配，直接返回false。 而当模式中的第二个字符是“*”时： 如果字符串第一个字符跟模式第一个字符不匹配，则模式后移2个字符，继续匹配。如果字符串第一个字符跟模式第一个字符匹配，可以有3种匹配方式： 模式后移2字符，相当于$x*$被忽略； 字符串后移1字符，模式后移2字符； 字符串后移1字符，模式不变，即继续匹配字符下一位，因为*可以匹配多位； java 123456789101112131415161718192021222324252627282930313233343536public class Solution &#123; public boolean match(char[] str, char[] pattern) &#123; if (str == null || pattern == null) &#123; return false; &#125; int strIndex = 0; int patternIndex = 0; return matchCore(str, strIndex, pattern, patternIndex);&#125; public boolean matchCore(char[] str, int strIndex, char[] pattern, int patternIndex) &#123; //有效性检验：str到尾，pattern到尾，匹配成功 if (strIndex == str.length &amp;&amp; patternIndex == pattern.length) &#123; return true; &#125; //pattern先到尾，匹配失败 if (strIndex != str.length &amp;&amp; patternIndex == pattern.length) &#123; return false; &#125; //模式第2个是*，且字符串第1个跟模式第1个匹配,分3种匹配模式；如不匹配，模式后移2位 if (patternIndex + 1 &lt; pattern.length &amp;&amp; pattern[patternIndex + 1] == '*') &#123; if ((strIndex != str.length &amp;&amp; pattern[patternIndex] == str[strIndex]) || (pattern[patternIndex] == '.' &amp;&amp; strIndex != str.length)) &#123; return matchCore(str, strIndex, pattern, patternIndex + 2)//模式后移2，视为x*匹配0个字符 || matchCore(str, strIndex + 1, pattern, patternIndex + 2)//视为模式匹配1个字符 || matchCore(str, strIndex + 1, pattern, patternIndex);//*匹配1个，再匹配str中的下一个 &#125; else &#123; return matchCore(str, strIndex, pattern, patternIndex + 2); &#125; &#125; //模式第2个不是*，且字符串第1个跟模式第1个匹配，则都后移1位，否则直接返回false if ((strIndex != str.length &amp;&amp; pattern[patternIndex] == str[strIndex]) || (pattern[patternIndex] == '.' &amp;&amp; strIndex != str.length)) &#123; return matchCore(str, strIndex + 1, pattern, patternIndex + 1); &#125; return false; &#125;&#125; 八、表示数值的字符串（剑54）请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。 java 12345678910111213141516171819202122232425262728public class Solution &#123; boolean isNumeric(char[] s) &#123; if(s.length==0) return false; if((s.length==1)&amp;&amp;(s[0]&lt;'0'||s[0]&gt;'9')) return false; if(s[0]=='+'||s[0]=='-')&#123; if(s.length==2&amp;&amp;(s[1]=='.')) return false; &#125;else if((s[0]&lt;'0'||s[0]&gt;'9')&amp;&amp;s[0]!='.') return false;//首位既不是符号也不是数字还不是小数点，当然是false int i = 1; while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; if(i&lt;s.length&amp;&amp;s[i]=='.')&#123; i++; //if(i&gt;=s.length) return false; while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; &#125; if(i&lt;s.length&amp;&amp;(s[i]=='e'||s[i]=='E'))&#123; i++; if((i&lt;s.length)&amp;&amp;(s[i]=='+'||s[i]=='-'))&#123; i++; if(i&lt;s.length) while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; else return false; &#125;else if(i&lt;s.length)&#123; while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; &#125;else return false; &#125; if(i&lt;s.length) return false; return true; &#125;&#125; 九、字符流中第一个不重复的数组（剑55）使用一个HashMap来统计字符出现的次数，同时用一个ArrayList来记录输入流，每次返回第一个出现一次的字符都是在这个ArrayList（输入流）中的字符作为key去map中查找。 java 123456789101112131415161718192021222324252627282930313233import java.util.*;public class Solution &#123; //HashMap来统计字符出现的次数 HashMap&lt;Character, Integer&gt; map=new HashMap(); //ArrayList来记录输入流 ArrayList&lt;Character&gt; list=new ArrayList&lt;Character&gt;(); //Insert one char from stringstream public void Insert(char ch) &#123; if(map.containsKey(ch))&#123; int time = map.get(ch); time++; map.put(ch,time); &#125;else&#123; map.put(ch,1); &#125; list.add(ch); &#125; //return the first appearence once char in current stringstream public char FirstAppearingOnce() &#123; char ch='#'; for(char k : list)&#123;//list迭代 if(map.get(k)==1)&#123; ch=k; break;//得到第一个结果即可break &#125; &#125; return ch; &#125;&#125; 十、最长无重复字符子串给定一个字符串，找字符中的最大非重复子串 。 基本思路是维护一个窗口，每次关注窗口中的字符串，在每次判断中，左窗口和右窗口选择其一向前移动。同样是维护一个HashSet, 正常情况下移动右窗口，如果没有出现重复则继续移动右窗口，如果发现重复字符，则说明当前窗口中的串已经不满足要求，继续移动有窗口不可能得到更好的结果，此时移动左窗口，直到不再有重复字符为止，中间跳过的这些串中不会有更好的结果，因为他们不是重复就是更短。因为左窗口和右窗口都只向前，所以两个窗口都对每个元素访问不超过一遍，因此时间复杂度为O(2*n)=O(n),是线性算法。空间复杂度为HashSet的size,也是O(n). 用start记录当前处理的开始位置历遍字符串，当当前字符从开始位置start开始已经出现过的时候，子串开始位置+1，否则更新map中的hash值为当前位置 。代码如下： java 123456789101112131415161718import java.util.HashMap;public class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; if (s.length()==0) return 0; HashMap&lt;Character, Integer&gt; map = new HashMap&lt;Character, Integer&gt;(); int max=0; int lens=s.length(); for (int i=0, start=0; i&lt;lens; ++i)&#123; char ch = s.charAt(i); if (map.containsKey(ch))&#123; start = Math.max(start,map.get(ch)+1); &#125; map.put(ch,i); max = Math.max(max,i-start+1); &#125; return max; &#125;&#125; 十一、最长回文字符串已整理 十二、KMP算法已整理 面试出现的字符串题已整理 三、寻找字符串中第一个只出现一次的字符；寻找一个字符串中第一个只出现一次的字符 五、左旋转字符串；手写算法:字符串反转；翻转一个英文字符串中的单词位置，单词间以空格分隔，但不改变每个单词本身的顺序。如输入“Ha Mo”，输出“Mo Ha”；字符串反转；请实现一个函数将“I am a student”转为“student a am I”。 六、实现atoi函数，即字符串转整型；就是那个字符串转换成整数，题目不难，但考虑的细节特别多。。。没写出来；写程序 str2Int 七、写 find 函数，在目标串中匹配模式串（要考虑中文字符的情况）。 十一、最长回文子串：判断一个数字是否为回文数（此处需注意，面试官一直问我有没有更优的方法，我当时已经说出了2-3个方法，囧）， 十二、KMP算法 未整理字符串由大小写字母组成，要求去重，只允许使用几个int临时变量，要求时间复杂度尽可能少。左右括号组成的字符串，去除最少使得剩余的字符串是合法的统计一个字符串中英文字母、空格、数字的个数，考察代码风格是否规范然后要求手写纯C字符串拼接，当时笔者想到了三个细节（1：const char* str 2: 空串判断 3：返回新串还是原有串），写完代码之后面试就结束了。在出门的那一刹那，我想起了代码中一个问题，空间申请啊，内心是崩溃的。。。字符串分割字符串排序字符串中字符替换两个字符串的复制（除了字符串地址重叠的情况，也要注意判断字符串本身的空间足够不足够，对于异常情况要考虑全面）写code去除字符串S1中的字符使得最终的字符串S2不包含’ab’和’c’]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>字符串</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（2）：数组题解]]></title>
    <url>%2F2017%2F08%2F03%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%882%EF%BC%89%EF%BC%9A%E6%95%B0%E7%BB%84%E9%A2%98%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[数组相关题解java实现。 一、二维数组中的查找（剑3）在一个二维数组中，每一行都按照从左到右的递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一维数组和一个整数，判断数组中是否含有该整数。 首先选取数组中右上角的数字，如果该数字等于我们要查找的数组，查找过程结束；如果该数字大于要查找的数组，剔除这个数字所在的列；如果该数字小于要查找的数组，剔除这个数字所在的行。也就是说如果要查找的数字不再数组的右上角，则每一次都在数组的查找范围中剔除一行或者一列，这样每一步都可以缩小查找的范围，直到找到要查找的数字，或者查找范围为空。 java 12345678910111213141516171819public class Solution &#123; public boolean Find(int target, int [][] array) &#123; int row = 0; int col = array[0].length - 1; while(row&lt;=array.length-1&amp;&amp;col&gt;=0)&#123; if(target == array[row][col])&#123; return true; &#125; else if(target&gt;array[row][col])&#123; row++; &#125; else&#123; col--; &#125; &#125; return false; &#125;&#125; python 123456789101112131415# -*- coding:utf-8 -*-class Solution: # array 二维列表 def Find(self, target, array): # write code here row = 0 col = len(array[0])-1 while row&lt;=len(array)-1 and col&gt;=0: if target==array[row][col]: return True elif target&gt;array[row][col]: row+=1 else: col-=1 return False 也可以把每一行看做是一个递增的序列，利用二分查找。 java 12345678910111213141516171819public class Solution &#123; public boolean Find(int target, int [][] array) &#123; for(int i=0;i&lt;array.length;i++)&#123; int low =0; int high = array[i].length-1; while(low&lt;=high)&#123; int mid = (low+high)/2; if(array[i][mid]==target) return true; else if(array[i][mid]&gt;target) high =mid-1; else low=mid+1; &#125; &#125; return false; &#125;&#125; 二、用两个栈实现队列（剑7）栈是一个非常常见的数据结构，它在计算机领域中被广泛应用，比如操作系统会给每个线程创建一个栈来存储函数调用时各个函数的参数、返回地址及临时变量等。栈的特点是后进先出，即最后被压入（push）栈的元素会第一个被弹出（pop）。 队列是另外一种很重要的数据结构。和栈不同的是，队列的特点是先进先出，即第一个进入队列的元素将会第一个出来。 栈和队列虽然是针锋相对的两个数据结构，但有意思的是他们却相互联系。 通过一个具体的例子来分析往队列插入和删除元素的过程。首先插入一个元素a，不妨先把它插入到stack1，此时stack1中的元素有{a}，stack2为空，再向stack1压入b和c，此时stack1中的元素有{a,b,c}，其中c处于栈顶，而stack2仍然是空的。 因为a是最先进的，最先被删除的元素应该是a，但a位于栈低。我们可以把stack1中的元素逐个弹出并压入stack2，元素在stack2的顺序正好和原来在stack1的顺序相反因此经过三次弹出stack1和压入stack2操作之后，stack1为空，而stack2的元素是{c,b,a}，这时就可以弹出stack2的栈顶a了，随后弹出stack2中的b和c，而这个过程中stack1始终为空. 从上面的分析我们可以总结出删除一个元素的步骤：当stack2中不为空时，在stack2的栈顶元素是最先进入队列的元素，可以弹出。如果stack2为空时，我们把stack1中的元素逐个弹出并压入stack2。由于先进入队列的元素被压到stack1的底端，经过弹出和压入之后就处于stack2的顶端了，又可以直接弹出。 1234567891011121314151617181920import java.util.Stack;public class Solution &#123; Stack&lt;Integer&gt; stack1 = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; stack2 = new Stack&lt;Integer&gt;(); public void push(int node) &#123; stack1.push(node); &#125; public int pop() &#123; while(!stack2.isEmpty())&#123; return stack2.pop(); &#125; while(!stack1.isEmpty())&#123; stack2.push(stack1.pop()); &#125; return stack2.pop(); &#125;&#125; 三、旋转数组的最小数字（剑8）在准备面试的时候，我们应该重点掌握二分查找、归并排序和快速排序，做到能随时正确、完整地写出它们的代码。 若面试题是要求在排序的数组（或部分排序的数组）中查找一个数字或者统计某个数字出现的次数，我们都可以尝试用二分查找算法。 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 可以采用二分法解答这个问题， mid = low + (high - low)/2 ，需要考虑三种情况： array[mid] &gt; array[high]:出现这种情况的array类似[3,4,5,6,0,1,2]，此时最小数字一定在mid的右边。low = mid + 1 array[mid] == array[high]: 出现这种情况的array类似 [1,0,1,1,1] 或者[1,1,1,0,1]，此时最小数字不好判断在mid左边，还是右边,这时只好一个一个试，low = low + 1 或者 high = high - 1 array[mid] &lt; array[high]: 出现这种情况的array类似[2,2,3,4,5,6,6],此时最小数字一定就是array[mid]或者在mid的左边。因为右边必然都是递增的。 high = mid。注意这里有个坑：如果待查询的范围最后只剩两个数，那么mid一定会指向下标靠前的数字，比如 array = [4,6]，array[low] = 4 ;array[mid] = 4 ; array[high] = 6 ; 如果high = mid - 1，就会产生错误， 因此high = mid，但情形(1)中low = mid + 1就不会错误。 代码如下： java 12345678910111213141516171819202122import java.util.ArrayList;public class Solution &#123; public int minNumberInRotateArray(int [] array) &#123; int low = 0; int high = array.length-1; while(low&lt;high)&#123; int mid = low+(high-low)/2; if(array[mid]&gt;array[high])&#123; low=mid+1; &#125; else if(array[mid]==array[high])&#123; high=high-1; &#125; else&#123; high = mid; &#125; &#125; return array[low]; &#125;&#125; 四、斐波那契数列（剑9）4.1 斐波那契数列大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。n&lt;=39。这个题可以说是迭代（Iteration） VS 递归（Recursion），f(n) = f(n-1) + f(n-2)，第一眼看就是递归啊，简直完美的递归环境，递归肯定很爽，这样想着关键代码两三行就搞定了，注意这题的n是从0开始的： 12if(n&lt;=1) return n;else return Fibonacci(n-1)+Fibonacci(n-2); 然而并没有什么用，测试用例里肯定准备着一个超大的n来让Stack Overflow，为什么会溢出？因为重复计算，而且重复的情况还很严重，举个小点的例子，n=4，看看程序怎么跑的： 123Fibonacci(4) = Fibonacci(3) + Fibonacci(2); = Fibonacci(2) + Fibonacci(1) + Fibonacci(1) + Fibonacci(0); = Fibonacci(1) + Fibonacci(0) + Fibonacci(1) + Fibonacci(1) + Fibonacci(0); 由于我们的代码并没有记录Fibonacci(1)和Fibonacci(0)的结果，对于程序来说它每次递归都是未知的，因此光是n=4时f(1)就重复计算了3次之多。 更简单的办法是从下往上计算，首先根据f(0)和f(1)算出f(2)，再根据f(1)和f(2)算出f(3)……依此类推就可以算出第n项了。很容易理解，这种思路的时间复杂度是O(n)。实现代码如下： java 1234567891011121314151617public class Solution &#123; public int Fibonacci(int n) &#123; if(n==0) return 0; if(n==1) return 1; int num1 = 0; int num2 = 1; int fibN=0; for(int i=2;i&lt;=n;++i)&#123; fibN=num1+num2; num1=num2; num2=fibN; &#125; return fibN; &#125;&#125; 4.2 跳台阶一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级台阶总共有多少种跳法。 我们把n级台阶的跳法看成是n的函数，记为f(n)。当n&gt;2时，第一次跳的时候就有两种不同的选择：一是第一次只跳1级，此时跳法数目等于后面剩下的n-1级台阶的跳法数目，即为f(n-1)；另外一种选择是第一次跳2级，此时跳法数目等于后面剩下的n-2级台阶的跳法数目，即为f(n-2)，因此n级台阶的不同跳法的总数f(n)=f(n-1)+f(n-2)。分析到这里，我们不难看出这实际上是斐波那契数列了。代码如下： java 1234567891011121314151617181920public class Solution &#123; public int JumpFloor(int target) &#123; if(target == 0) return 0; if(target == 1) return 1; if(target == 2) return 2; int num1 = 0; int num2 = 1; int jump = 0; for(int i=0;i&lt;target;i++)&#123; jump = num1+num2; num1=num2; num2=jump; &#125; return jump; &#125;&#125; 4.3 变态跳台阶一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 因为n级台阶，第一步有n种跳法：跳1级、跳2级、到跳n级。跳1级，剩下n-1级，则剩下跳法是f(n-1)，跳2级，剩下n-2级，则剩下跳法是f(n-2)。所以f(n)=f(n-1)+f(n-2)+…+f(1)，因为f(n-1)=f(n-2)+f(n-3)+…+f(1)，所以f(n)=2*f(n-1) java 1234567891011public class Solution &#123; public int JumpFloorII(int target) &#123; if(target==0) return 0; if(target==1) return 1; else&#123; return 2*JumpFloorII(target-1); &#125; &#125;&#125; 4.4 矩形覆盖我们可以用$21$的小矩形横着或者竖着去覆盖更大的矩形。请问用n个$21$的小矩形无重叠地覆盖一个$2*n$的大矩形，总共有多少种方法？ 把$28$的覆盖方法记为f(8)。用一个$12$小矩形去覆盖大矩形的最左边有两个选择。竖着放或者横着放。当竖着放时，右边剩下$27$的区域，记为f(7)。横着放时，当$12$的小矩阵横着放在左上角的时候，左下角必须横着放一个$12$的小矩阵，剩下$26$，记为f(6)，因此f(8)=f(7)+f(6)。此时可以看出，仍然是斐波那契数列。 代码如下： java 1234567891011121314151617public class Solution &#123; public int RectCover(int target) &#123; if(target==0) return 0; if(target==1) return 1; int num1=0; int num2=1; int cover =0; for(int i=0;i&lt;target;i++)&#123; cover = num1+num2; num1=num2; num2=cover; &#125; return cover; &#125;&#125; 五、调整数组顺序使奇数位于偶数前面（剑14）书上的方法类似于快排，但快排是不稳定的，即其相对位置会发生变化。 java 1234567891011121314151617181920public class Solution &#123; public void reOrderArray(int [] array) &#123; int length = array.length; if(array==null||length==0) return; int left = 0; int right = length-1; while(left&lt;right)&#123; while(left&lt;right&amp;&amp;array[left]%2==1)&#123; left++; &#125; while(left&lt;right&amp;&amp;array[right]%2==0)&#123; right--; &#125; int temp =array[right]; array[right]=array[left]; array[left]=temp; &#125; &#125;&#125; 这里要保证奇数和奇数，偶数和偶数之间的相对位置不变。可以使用插入排序的思想 java 123456789101112131415161718public class Solution &#123; public void reOrderArray(int [] array) &#123; int length = array.length; if(array==null||length==0) return; for(int i=1;i&lt;length;i++)&#123; if(array[i]%2==1)&#123; int curr = array[i]; int j=i-1; while(j&gt;=0&amp;&amp;array[j]%2==0)&#123; array[j+1]=array[j]; j--; &#125; array[j+1]=curr; &#125; &#125; &#125;&#125; 六、顺时针打印矩阵（剑20）输入一个矩阵，按照从外向里以顺时针依次打印出每一个数字。 java 12345678910111213141516171819202122232425262728import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printMatrix(int [][] matrix) &#123; int row = matrix.length; int col = matrix[0].length; ArrayList&lt;Integer&gt; result = new ArrayList&lt;Integer&gt; (); // 输入的二维数组非法，返回空的数组 if(row==0&amp;&amp;col==0)return result; // 定义四个关键变量，表示左上和右下的打印范围 int left =0,top=0,right=col-1,bottom=row-1; while(left&lt;=right&amp;&amp;top&lt;=bottom)&#123; // left to right for(int i=left;i&lt;=right;i++)&#123;result.add(matrix[top][i]);&#125; // top to bottom for(int i=top+1;i&lt;=bottom;i++)&#123;result.add(matrix[i][right]);&#125; // right to left if(top!=bottom)&#123; for(int i=right-1;i&gt;=left;i--)&#123;result.add(matrix[bottom][i]);&#125;&#125; // bottom to top if(left!=right)&#123; for(int i=bottom-1;i&gt;=top+1;i--)&#123;result.add(matrix[i][left]);&#125;&#125; left++;right--;top++;bottom--; &#125; return result; &#125; &#125; 七、包含min函数的栈（剑21）定义栈的数据结构，请在该类型中实现一个能够得到栈最小元素的min函数。在该栈中，调用min、push及pop的时间复杂度都是O(1)。 可以利用一个辅助栈来存放最小值 每入栈一次，就与辅助栈顶比较大小，如果小就入栈，如果大就入栈当前的辅助栈顶 。 当出栈时，辅助栈也要出栈 这种做法可以保证辅助栈顶一定都是最小元素。 12345678910111213141516171819202122232425import java.util.Stack;public class Solution &#123; Stack&lt;Integer&gt; data = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; min = new Stack&lt;Integer&gt;(); public void push(int node) &#123; data.push(node); if(min.empty())&#123;min.push(data.peek());&#125; else if(data.peek()&lt;min.peek())&#123;min.push(data.peek());&#125; else min.push(min.peek()); &#125; public void pop() &#123; data.pop(); min.pop(); &#125; public int top() &#123; return data.peek(); &#125; public int min() &#123; return min.peek(); &#125;&#125; 八、栈的压入、弹出序列（剑22）输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4，5，3，2，1是该压栈序列对应的一个弹出序列，但4，3，5，1，2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 借用一个辅助的栈，遍历压栈顺序，先讲第一个放入栈中，这里是1，然后判断栈顶元素是不是出栈顺序的第一个元素，这里是4，很显然1≠4，所以我们继续压栈，直到相等以后开始出栈，出栈一个元素，则将出栈顺序向后移动一位，直到不相等，这样循环等压栈顺序遍历完成，如果辅助栈还不为空，说明弹出序列不是该栈的弹出顺序。举例：入栈1,2,3,4,5出栈4,5,3,2,1首先1入辅助栈，此时栈顶1≠4，继续入栈2此时栈顶2≠4，继续入栈3此时栈顶3≠4，继续入栈4此时栈顶4＝4，出栈4，弹出序列向后一位，此时为5，,辅助栈里面是1,2,3此时栈顶3≠5，继续入栈5此时栈顶5=5，出栈5,弹出序列向后一位，此时为3，,辅助栈里面是1,2,3….依次执行，最后辅助栈为空。如果不为空说明弹出序列不是该栈的弹出顺序。 java 1234567891011121314151617import java.util.ArrayList;import java.util.Stack;public class Solution &#123; public boolean IsPopOrder(int [] pushA,int [] popA) &#123; if(pushA.length==0||popA.length==0)return false; Stack&lt;Integer&gt; S=new Stack&lt;Integer&gt;(); int popIndex = 0; for(int i=0;i&lt;pushA.length;i++)&#123; S.push(pushA[i]); while(!S.empty()&amp;&amp;popA[popIndex]==S.peek())&#123; S.pop(); popIndex++; &#125; &#125; return S.empty(); &#125;&#125; 九、数组中出现次数超过一半的数字（剑29）数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 数组中有一个数字出现的次数超过数组长度的一半，也就是说它出现的次数比其他所有数字出现的次数的和还要多。因此我们可以考虑在遍历数组的时候保存两个值：一个是数组的一个数字，一个是次数。当我们遍历到下一个数字的时候，如果下一个数字和我们之前保存的数字相同，则次数加1；如果不同，则次数减1；如果次数为0，则保存下一个数字，并把次数设为1。 还要判断这个数字是否超过数组长度的一半，如果不存在输出0。 123456789101112131415161718192021222324252627282930313233public class Solution &#123; public int MoreThanHalfNum_Solution(int [] array) &#123; if(array==null||array.length==0)&#123; return 0; &#125; int result=array[0]; int count=1; for(int i=1;i&lt;array.length;i++)&#123; if(result==array[i])&#123; count++; &#125; else if(result!=array[i])&#123; count--; &#125; if(count==0)&#123; result=array[i]; count=1; &#125; &#125; int times=0; for(int i=0;i&lt;array.length;i++)&#123; if(array[i]==result)&#123; times++; &#125; &#125; if(times*2&lt;=array.length)&#123; System.out.println(times); return 0; &#125; else return result; &#125;&#125; 十、最小的K个数（剑30）输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 第一种方法，借用partition函数 java 1234567891011121314151617181920212223242526272829303132333435363738import java.util.ArrayList; public class Solution &#123; public ArrayList&lt;Integer&gt; GetLeastNumbers_Solution(int[] input, int k) &#123; ArrayList&lt;Integer&gt; output = new ArrayList&lt;Integer&gt;(); int length = input.length; if (input == null || length &lt;= 0 || length &lt; k || k&lt;= 0) &#123; return output; &#125; int low = 0; int high = length - 1; while(low&lt;high) &#123; int pivotloc = partition(input,low,high); if(pivotloc==k-1)break; else if(pivotloc &lt; k - 1) &#123; low = pivotloc + 1; //在右边 &#125; else &#123; high = pivotloc - 1; //在左边 &#125; &#125; for (int i = 0;i &lt; k;i++) &#123; output.add(input[i]); &#125; return output; &#125; //基准左右分区 private int partition(int[] input,int low,int high) &#123; int pivot = input[low]; while(low &lt; high) &#123; while(input[high] &gt;= pivot &amp;&amp; low &lt; high) high--; input[low] = input[high]; while(input[low] &lt;= pivot &amp;&amp; low &lt;high) low++; input[high] = input[low]; &#125; input[low] = pivot; return low; &#125; &#125; 第二种方法 用最大堆保存这k个数，每次只和堆顶比，如果比堆顶小，删除堆顶，新数入堆。时间 $O(NlogK)$ 空间 $O(K)$ java 1234567891011121314151617181920212223242526272829import java.util.ArrayList;import java.util.PriorityQueue;import java.util.Comparator;public class Solution &#123; public ArrayList&lt;Integer&gt; GetLeastNumbers_Solution(int[] input, int k) &#123; ArrayList&lt;Integer&gt; result = new ArrayList&lt;Integer&gt;(); int length = input.length; if(k &gt; length || k == 0)&#123; return result; &#125; PriorityQueue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;Integer&gt;(k, new Comparator&lt;Integer&gt;() &#123; @Override//PriorityQueue默认是小顶堆，实现大顶堆，需要反转默认排序器 public int compare(Integer o1, Integer o2) &#123; return o2.compareTo(o1); &#125; &#125;); //遍历数组时将数字加入优先队列（堆），一旦堆的大小大于k就将堆顶元素去除，确保堆的大小为k。遍历完后堆中的数就是最小的K个数。 for (int i:input) &#123; maxHeap.offer(i); if(maxHeap.size()&gt;k) maxHeap.poll(); &#125; //输出大顶堆中的数 for (int integer : maxHeap) &#123; result.add(integer); &#125; return result; &#125;&#125; 十一、连续子数组的最大和（剑31）输入一个整型数组，数组中有正数也有负数。数组中一个或连续的多个整数组成一个子数组。求所有子数组的和的最大值。要求时间复杂度为O(n) 第一种方法 java 12345678910111213141516171819public class Solution &#123; public int FindGreatestSumOfSubArray(int[] array) &#123; if(array.length==0||array==null) return 0; int cSum = 0; int result = array[0];// result存储最大和，不能初始为0，存在负数 for(int i=0;i&lt;array.length;i++)&#123; if(cSum&lt;0)&#123; cSum=array[i];// 当前和&lt;0，抛弃不要 &#125;else&#123; cSum += array[i];//否则累加上去 &#125; if(cSum&gt;result)&#123; result = cSum;// 存储当前的最大结果 &#125; &#125; return result; &#125;&#125; 第二种方法：动态规划 1234567891011121314151617181920212223F（i）：以array[i]为末尾元素的子数组的和的最大值，子数组的元素的相对位置不变 F（i）=max（F（i-1）+array[i] ， array[i]） res：所有子数组的和的最大值 res=max（res，F（i）） 如数组[6, -3, -2, 7, -15, 1, 2, 2] 初始状态： F（0）=6 res=6 i=1： F（1）=max（F（0）-3，-3）=max（6-3，3）=3 res=max（F（1），res）=max（3，6）=6 i=2： F（2）=max（F（1）-2，-2）=max（3-2，-2）=1 res=max（F（2），res）=max（1，6）=6 i=3： F（3）=max（F（2）+7，7）=max（1+7，7）=8 res=max（F（2），res）=max（8，6）=8 i=4： F（4）=max（F（3）-15，-15）=max（8-15，-15）=-7 res=max（F（4），res）=max（-7，8）=8 以此类推 最终res的值为8 java 1234567891011public class Solution &#123; public int FindGreatestSumOfSubArray(int[] array) &#123; int res = array[0]; int max = array[0]; for(int i=1;i&lt;array.length;i++)&#123; max=Math.max(max+array[i],array[i]); res = Math.max(max,res); &#125; return res; &#125;&#125; 十二、从1到n整数中1出现的次数（剑32）输入一个整数n，求1到n这n个整数的十进制表示中1出现的次数。例如输入12，从1到12这些整数中包含1的数字有1、10、11、12，1一共出现了5次。 一、1的数目 编程之美上给出的规律： 如果第i位（自右至左，从1开始标号）上的数字为0，则第i位可能出现1的次数由更高位决定（若没有高位，视高位为0），等于更高位数字X当前位数的权重$10^{i-1}$。 如果第i位上的数字为1，则第i位上可能出现1的次数不仅受更高位影响，还受低位影响（若没有低位，视低位为0），等于更高位数字X当前位数的权重$10^{i-1}+$（低位数字+1）。 如果第i位上的数字大于1，则第i位上可能出现1的次数仅由更高位决定（若没有高位，视高位为0），等于（更高位数字+1）X当前位数的权重$10^{i-1}$。 二、X的数目这里的 X∈[1,9] ，因为 X=0 不符合下列规律，需要单独计算。首先要知道以下的规律： 从 1 至 10，在它们的个位数中，任意的 X 都出现了 1 次。 从 1 至 100，在它们的十位数中，任意的 X 都出现了 10 次。 从 1 至 1000，在它们的百位数中，任意的 X 都出现了 100 次。 依此类推，从 1 至 $10^i$ ，在它们的左数第二位（右数第 i 位）中，任意的 X 都出现了 $10^{i−1}$ 次。 这个规律很容易验证，这里不再多做说明。 接下来以 n=2593,X=5 为例来解释如何得到数学公式。从 1 至 2593 中，数字 5 总计出现了 813 次，其中有 259 次出现在个位，260 次出现在十位，294 次出现在百位，0 次出现在千位。 现在依次分析这些数据，首先是个位。从 1 至 2590 中，包含了 259 个 10，因此任意的 X 都出现了 259 次。最后剩余的三个数 2591, 2592 和 2593，因为它们最大的个位数字 3 &lt; X，因此不会包含任何 5。（也可以这么看，3&lt;X，则个位上可能出现的X的次数仅由更高位决定，等于更高位数字$（259）\times 10^{1-1}=259$）。 然后是十位。从 1 至 2500 中，包含了 25 个 100，因此任意的 X 都出现了 25×10=250 次。剩下的数字是从 2501 至 2593，它们最大的十位数字9&gt;X，因此会包含全部10个5。最后总计250 + 10 = 260。（也可以这么看，9&gt;X，则十位上可能出现的X的次数仅由更高位决定，等于更高位数字$（25+1）\times 10^{2-1}=260$）。 接下来是百位。从 1 至 2000 中，包含了 2 个 1000，因此任意的 X 都出现了 2×100=200 次。剩下的数字是从 2001 至 2593，它们最大的百位数字 5 == X，这时情况就略微复杂，它们的百位肯定是包含 5 的，但不会包含全部 100 个。如果把百位是 5 的数字列出来，是从 2500 至 2593，数字的个数与百位和十位数字相关，是 93+1 = 94。最后总计 200 + 94 = 294。（也可以这么看，5==X，则百位上可能出现X的次数不仅受更高位影响，还受低位影响，等于更高位数字$（2）\times 10^{3-1}+（93+1）=294$）。 最后是千位。现在已经没有更高位，因此直接看最大的千位数字 2 &lt; X，所以不会包含任何 5。（也可以这么看，2&lt;X，则千位上可能出现的X的次数仅由更高位决定，等于更高位数字$（0）\times 10^{4-1}=0$）。 到此为止，已经计算出全部数字 5 的出现次数。总结一下以上的算法，可以看到，当计算右数第 i 位包含的 X 的个数时： 取第 i 位左边（高位）的数字，乘以$10^{i−1}$ ，得到基础值a 。 取第 i 位数字，计算修正值： 如果大于 X，则结果为 $a+ 10^{i−1}$ 。 如果小于 X，则结果为 a 。 如果等 X，则取第 i 位右边（低位）数字，设为 b ，最后结果为 a+b+1 。 相应的代码非常简单，效率也非常高，时间复杂度只有 $ O( log_ {10} n) $。 代码如下： 1234567891011121314151617181920212223public class Solution &#123; public int NumberOf1Between1AndN_Solution(int n) &#123; if (n &lt; 1) return 0; int len = getLenOfNum(n); if (len == 1) return 1; int tmp = (int) Math.pow(10, len - 1); int first = n / tmp; int firstOneNum = first == 1 ? n % tmp + 1 : tmp; int otherOneNUm = first * (len - 1) * (tmp / 10); return firstOneNum + otherOneNUm + NumberOf1Between1AndN_Solution(n % tmp); &#125; private int getLenOfNum(int n) &#123; int len = 0; while (n != 0) &#123; len++; n /= 10; &#125; return len; &#125;&#125; 十三、把数组排成最小的数（剑33）输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 java 123456789101112131415161718192021222324252627282930import java.util.ArrayList;import java.util.Collections;import java.util.Comparator;public class Solution &#123; public String PrintMinNumber(int [] numbers) &#123; int n; String s=""; ArrayList&lt;Integer&gt; list=new ArrayList&lt;Integer&gt;(); n=numbers.length; for(int i=0;i&lt;n;i++)&#123; list.add(numbers[i]);//将数组放入arrayList中 &#125; //实现了Comparator接口的compare方法，将集合元素按照compare方法的规则进行排序 Collections.sort(list,new Comparator&lt;Integer&gt;()&#123; @Override public int compare(Integer str1, Integer str2) &#123; // TODO Auto-generated method stub String s1=str1+""+str2; String s2=str2+""+str1; return s1.compareTo(s2); &#125; &#125;); for(int j:list)&#123; s+=j; &#125; return s; &#125;&#125; 十四、丑数（剑34）把只包含因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 java 12345678910111213141516171819import java.util.*;public class Solution &#123; public int GetUglyNumber_Solution(int index) &#123; if(index&lt;7)return index; int[] res = new int[index]; res[0] = 1; int t2 = 0, t3 = 0, t5 = 0, i; for(i=1;i&lt;index;i++)&#123; res[i] = min(res[t2]*2,min(res[t3]*3,res[t5]*5)); if(res[i] == res[t2]*2)t2++; if(res[i] == res[t3]*3)t3++; if(res[i] == res[t5]*5)t5++; &#125; return res[index-1]; &#125; private int min(int a,int b)&#123; return (a&gt;b)? b:a; &#125;&#125; 十五、数组中的逆序对（剑36）在数组中的两个数字如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组，求出这个数组中的逆序对的总数。例如在数组{7，5，6，4}中，一共存在5个逆序对，分别是（7，6）、（7，5）、（7，4）、（5，4）和（6，4）。 可以按照归并排序的思路，先把数组分隔成子数组，先统计出子数组内部的逆序对的数目，然后再统计出两个相邻子数组之间的逆序对的数目。在统计逆序对的过程中，还需要对数组进行排序。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Solution &#123; int cnt; public int InversePairs(int[] array) &#123; cnt = 0; if (array != null) mergeSortUp2Down(array, 0, array.length - 1); return cnt; &#125; /* * 归并排序(从上往下) */ public void mergeSortUp2Down(int[] a, int start, int end) &#123; if (start &gt;= end) return; int mid = (start + end) &gt;&gt; 1; mergeSortUp2Down(a, start, mid); mergeSortUp2Down(a, mid + 1, end); merge(a, start, mid, end); &#125; /* * 将一个数组中的两个相邻有序区间合并成一个 */ public void merge(int[] a, int start, int mid, int end) &#123; int[] tmp = new int[end - start + 1]; int i = start, j = mid + 1, k = 0; while (i &lt;= mid &amp;&amp; j &lt;= end) &#123; if (a[i] &lt;= a[j]) tmp[k++] = a[i++]; else &#123; tmp[k++] = a[j++]; cnt += mid - i + 1; //关键的一步，统计逆序对.......... cnt%=1000000007; &#125; &#125; while (i &lt;= mid) tmp[k++] = a[i++]; while (j &lt;= end) tmp[k++] = a[j++]; for (k = 0; k &lt; tmp.length; k++) a[start + k] = tmp[k]; &#125;&#125; 十六、数字在排序数组中出现的次数（剑38）统计一个数字在排序数组中出现的次数。 利用二分查找直接找到第一个K和最后一个K。以下代码使用递归方法找到第一个K，使用循环方法最后一个K。 java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class Solution &#123; public int GetNumberOfK(int [] array , int k) &#123; int length = array.length; if(length == 0)&#123; return 0; &#125; int firstK = getFirstK(array, k, 0, length-1); int lastK = getLastK(array, k, 0, length-1); if(firstK != -1 &amp;&amp; lastK != -1)&#123; return lastK - firstK + 1; &#125; return 0; &#125; //递归写法 private int getFirstK(int [] array , int k, int start, int end)&#123; if(start &gt; end)&#123; return -1; &#125; int mid = (start + end) &gt;&gt; 1; if(array[mid] &gt; k)&#123; return getFirstK(array, k, start, mid-1); &#125; else if (array[mid] &lt; k)&#123; return getFirstK(array, k, mid+1, end); &#125; else if(mid-1 &gt;=0 &amp;&amp; array[mid-1] == k)&#123; return getFirstK(array, k, start, mid-1); &#125; else&#123; return mid; &#125; &#125; //循环写法 private int getLastK(int [] array , int k, int start, int end)&#123; int length = array.length; int mid = (start + end) &gt;&gt; 1; while(start &lt;= end)&#123; if(array[mid] &gt; k)&#123; end = mid-1; &#125; else if(array[mid] &lt; k)&#123; start = mid+1; &#125; else if(mid+1 &lt;= length-1 &amp;&amp; array[mid+1] == k)&#123; start = mid+1; &#125; else&#123; return mid; &#125; mid = (start + end) &gt;&gt; 1; &#125; return -1; &#125;&#125; 十七、数组中只出现一次的数字（剑40）一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。要求时间复杂度是$O(n)$，空间复杂度是$O(1)$。 首先我们考虑这个问题的一个简单版本：一个数组里除了一个数字之外，其他的数字都出现了两次。请写程序找出这个只出现一次的数字。 这个题目的突破口在哪里？题目为什么要强调有一个数字出现一次，其他的出现两次？我们想到了异或运算的性质：任何一个数字异或它自己都等于0 。也就是说，如果我们从头到尾依次异或数组中的每一个数字，那么最终的结果刚好是那个只出现一次的数字，因为那些出现两次的数字全部在异或中抵消掉了。 有了上面简单问题的解决方案之后，我们回到原始的问题。如果能够把原数组分为两个子数组。在每个子数组中，包含一个只出现一次的数字，而其它数字都出现两次。如果能够这样拆分原数组，按照前面的办法就是分别求出这两个只出现一次的数字了。 我们还是从头到尾依次异或数组中的每一个数字，那么最终得到的结果就是两个只出现一次的数字的异或结果。因为其它数字都出现了两次，在异或中全部抵消掉了。由于这两个数字肯定不一样，那么这个异或结果肯定不为0 ，也就是说在这个结果数字的二进制表示中至少就有一位为1 。我们在结果数字中找到第一个为1 的位的位置，记为第N 位。现在我们以第N 位是不是1 为标准把原数组中的数字分成两个子数组，第一个子数组中每个数字的第N 位都为1 ，而第二个子数组的每个数字的第N 位都为0 。 现在我们已经把原数组分成了两个子数组，每个子数组都包含一个只出现一次的数字，而其它数字都出现了两次。因此到此为止，所有的问题我们都已经解决。 java 123456789101112131415161718192021222324252627282930313233//num1,num2分别为长度为1的数组。传出参数//将num1[0],num2[0]设置为返回结果public class Solution &#123; public void FindNumsAppearOnce(int [] array,int num1[] , int num2[]) &#123; if(array==null ||array.length&lt;2) return ; int temp = 0; for(int i=0;i&lt;array.length;i++) temp ^= array[i]; int indexOf1 = findFirstBitIs(temp); for(int i=0;i&lt;array.length;i++)&#123; if(isBit(array[i], indexOf1)) num1[0]^=array[i]; else num2[0]^=array[i]; &#125; &#125; //在正数num的二进制表示中找到最右边是1的位 public int findFirstBitIs(int num)&#123; int indexBit = 0; while(((num &amp; 1)==0) &amp;&amp; (indexBit)&lt;8*4)&#123; num = num &gt;&gt; 1; ++indexBit; &#125; return indexBit; &#125; //判断在num的二进制表示中从右边数起的indexBit位是不是1. public boolean isBit(int num,int indexBit)&#123; num = num &gt;&gt; indexBit; return (num &amp; 1) == 1; &#125;&#125; 十八、 和为s的两个数字（剑41.1）一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。 数列满足递增，设两个头尾两个指针i和j， 若ai + aj == sum，就是答案（相差越远乘积越小） 若ai + aj &gt; sum，aj肯定不是答案之一（前面已得出 i 前面的数已是不可能），j -= 1 若ai + aj &lt; sum，ai肯定不是答案之一（前面已得出 j 后面的数已是不可能），i += 1 时间复杂度为O(n)。 1234567891011121314151617181920212223import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; FindNumbersWithSum(int [] array,int sum) &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); if(array==null||array.length&lt;2)&#123; return list; &#125; int i=0,j=array.length-1; while(i&lt;j)&#123; if(array[i]+array[j]==sum)&#123; list.add(array[i]); list.add(array[j]); break; &#125; else if(array[i]+array[j]&gt;sum)&#123; j--; &#125; else i++; &#125; return list; &#125;&#125; 十九、和为s的连续正数序列（剑41.2）输入一个正数s，打印出所有和为s的连续正数序列（至少含有两个数）。例如输入15，由于1+2+3+4+5=4+5+6=7+8=15，所以结果打印出三个连续序列1~5、4~6和7~8。 考虑用两个数small和big分别表示序列的最小值和最大值。首先把small初始化为1，big初始化为2，如果从small到big的序列和大于s，我们可以从序列中去掉较小的值，也就是增大small的值。如果从small到big的序列和小于s，我们可以增大big，让这个序列包含更多的数字。因为这个序列至少要有两个数字，我们一直增加small到（1+s）/2为止。 java 12345678910111213141516171819202122232425262728293031323334353637import java.util.ArrayList;/**初始化small=1，big=2;*small到big序列和小于sum，big++;大于sum，small++;*当small增加到(1+sum)/2是停止*/public class Solution &#123; public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindContinuousSequence(int sum) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; lists=new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if(sum&lt;=1)&#123;return lists;&#125; int small=1; int big=2; while(small!=(1+sum)/2)&#123; //当small==(1+sum)/2的时候停止 int curSum=sumOfList(small,big); if(curSum==sum)&#123; ArrayList&lt;Integer&gt; list=new ArrayList&lt;Integer&gt;(); for(int i=small;i&lt;=big;i++)&#123; list.add(i); &#125; lists.add(list); small++;big++; &#125;else if(curSum&lt;sum)&#123; big++; &#125;else&#123; small++; &#125; &#125; return lists; &#125; public int sumOfList(int head,int leap)&#123; //计算当前序列的和 int sum=head; for(int i=head+1;i&lt;=leap;i++)&#123; sum+=i; &#125; return sum; &#125;&#125; 二十、扑克牌的顺子（剑44）从扑克牌中随机抽5张牌，判断是不是顺子，即这5张牌是不是连续的。2~10为数字本身，A为1，J为11，Q为12，K为13，而大小王可以看做是任意数字，这里定为0. java 12345678910111213141516171819202122232425262728293031import java.util.*;public class Solution &#123; public boolean isContinuous(int [] numbers) &#123; int length = numbers.length; if(numbers==null||length==0)return false;//特殊情况 Arrays.sort(numbers);//排序 //统计数组中0的个数 int numberOfZero = 0; for(int i =0;i&lt;length&amp;&amp;numbers[i]==0;i++)&#123; ++numberOfZero; &#125; int numberOfGap = 0; int small = numberOfZero; int big = small+1; while(big&lt;length)&#123; //含有对子，不可能是顺子 if(numbers[small]==numbers[big])&#123; return false; &#125; //统计数组中的间隔数目 numberOfGap += numbers[big]-numbers[small]-1; small=big; big++; &#125; //如果间隔数小于等于零的数量则可以组成顺子，否则不行。 if(numberOfGap&lt;=numberOfZero)&#123; return true; &#125;else&#123;return false;&#125; &#125;&#125; 二十一、求1+2+…..+n（剑46）求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 需利用逻辑与的短路特性实现递归终止。 当n==0时，(n&gt;0)&amp;&amp;((sum+=Sum_Solution(n-1))&gt;0)只执行前面的判断，为false，然后直接返回0； 当n&gt;0时，执行sum+=Sum_Solution(n-1)，实现递归计算Sum_Solution(n)。 java 12345678public class Solution &#123; public int Sum_Solution(int n) &#123; int sum=n; boolean ans = (n&gt;0)&amp;&amp;((sum+=Sum_Solution(n-1))&gt;0); return sum; &#125;&#125; 二十二、数组中重复的数字（剑51）在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是重复的数字2或者3。 java 12345678910111213141516171819202122232425public class Solution &#123; public boolean duplicate(int numbers[],int length,int [] duplication) &#123; if(numbers==null||length==0)&#123;return false;&#125;//空指针或空数组 // 判断数组是否合法,即每个数都在0~n-1之间 for(int i=0;i&lt;length;i++)&#123; if(numbers[i]&gt;length-1||numbers[i]&lt;0)&#123; return false; &#125; &#125; //若数值与下标不同，则调换位置； //比较位置下标为数值(numbers[i])的数值(numbers[numbers[i]])与该数值(numbers[i])是否一致，若一致，则说明有重复数字 for(int i=0;i&lt;length;i++)&#123; while(numbers[i]!=i)&#123; if(numbers[i]==numbers[numbers[i]])&#123; duplication[0] = numbers[i]; return true; &#125; int temp=numbers[i]; numbers[i]=numbers[temp]; numbers[temp]=temp; &#125; &#125; return false; &#125;&#125; 二十三、构建乘积数组（剑52） 给定一个数组A[0,1,…,n-1],请构建一个数组$B[0,1,…,n-1]$,其中B中的元素$B[i]=A[0]A[1]…A[i-1]A[i+1]…A[n-1]$。不能使用除法。 java 12345678910111213141516171819import java.util.ArrayList;public class Solution &#123; public int[] multiply(int[] A) &#123; int length = A.length; int[] B = new int[length]; if(length!=0)&#123; B[0]=1; for(int i=1;i&lt;length;i++)&#123; B[i]=B[i-1]*A[i-1]; &#125; int temp=1; for(int j=length-2;j&gt;=0;j--)&#123; temp = temp*A[j+1]; B[j]=temp*B[j]; &#125; &#125; return B; &#125;&#125; 二十四、 滑动窗口的最大值（剑65）给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 java 123456789101112131415161718192021222324252627282930313233343536373839import java.util.ArrayList;import java.util.LinkedList;public class Solution &#123; public ArrayList&lt;Integer&gt; maxInWindows(int [] num, int size) &#123; ArrayList&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); if (num == null) &#123; return ret; &#125; if (num.length &lt; size || size &lt; 1) &#123; return ret; &#125; LinkedList&lt;Integer&gt; indexDeque = new LinkedList&lt;&gt;(); //前size-1个中，前面比num[i]小的，对应下标从下标队列移除； for (int i = 0; i &lt; size - 1; i++) &#123; if (!indexDeque.isEmpty() &amp;&amp; num[i] &gt; num[indexDeque.getLast()]) &#123; indexDeque.removeLast(); &#125; indexDeque.addLast(i); &#125; //从第size-1个开始；前面比num[i]小的，对应下标从下标队列移除； for (int i = size - 1; i &lt; num.length; i++) &#123; while(!indexDeque.isEmpty() &amp;&amp; num[i] &gt; num[indexDeque.getLast()]) &#123; indexDeque.removeLast(); &#125; //把下一个下标加入队列中 indexDeque.addLast(i); //当第一个数字的下标与当前处理的数字的下标之差大于或者等于滑动窗口的大小时，这个数字已经从窗口划出，可以移除了； if (i - indexDeque.getFirst() + 1 &gt; size) &#123; indexDeque.removeFirst(); &#125; //下标队列的第一个是滑动窗口最大值对应的下标； ret.add(num[indexDeque.getFirst()]); &#125; return ret; &#125;&#125; 二十五、矩阵中的路径（剑66）请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则该路径不能再进入该格子。 例如下面的矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 java 12345678910111213141516171819202122232425262728293031public class Solution &#123; public int movingCount(int threshold, int rows, int cols) &#123; boolean[] visited=new boolean[rows*cols]; return movingCountCore(threshold, rows, cols, 0,0,visited); &#125; private int movingCountCore(int threshold, int rows, int cols, int row,int col,boolean[] visited) &#123; if(row&lt;0||row&gt;=rows||col&lt;0||col&gt;=cols) return 0; int i=row*cols+col; if(visited[i]||!checkSum(threshold,row,col)) return 0; visited[i]=true; return 1+movingCountCore(threshold, rows, cols,row,col+1,visited) +movingCountCore(threshold, rows, cols,row,col-1,visited) +movingCountCore(threshold, rows, cols,row+1,col,visited) +movingCountCore(threshold, rows, cols,row-1,col,visited); &#125; private boolean checkSum(int threshold, int row, int col) &#123; int sum=0; while(row!=0)&#123; sum+=row%10; row=row/10; &#125; while(col!=0)&#123; sum+=col%10; col=col/10; &#125; if(sum&gt;threshold) return false; return true; &#125;&#125; 二十六、寻找某个值的区间（leetcode 34 Search for a Range）主要考查二分查找，如果读者对二分查找不是很熟悉，这里推荐一篇博客：你真的会二分查找吗？上面讲得非常详细。 这题要求在一个排好序可能有重复元素的数组里面找到包含某个值的区间范围。要求使用O(log n)的时间，所以我们采用两次二分查找。 主要实现两个方法： searchRightIndex：查找并返回target出现在nums数组最右边的index。注意一点，如果target比数组最小值还小，那么返回-1 searchLeftIndex：查找并返回target出现在nums数组最左边的index。如果target比数组最大值还大，那么返回nums.length 1234567891011121314151617181920212223242526272829public class Solution &#123; public int[] searchRange(int[] nums, int target) &#123; int[] result = &#123;-1, -1&#125;; int index = searchRightIndex(nums, 0, nums.length - 1, target); if (index &lt; 0 || nums[index] != target) return result; result[0] = searchLeftIndex(nums, 0, index, target); result[1] = index; return result; &#125; //查找并返回target出现在nums数组最右边的index public int searchRightIndex(int[] nums, int left, int right, int target) &#123; while (left &lt;= right) &#123; int mid = (left + right) / 2; if (nums[mid] &gt; target) right = mid - 1; else left = mid + 1; &#125; return right; &#125; //查找并返回target出现在nums数组最左边的index public int searchLeftIndex(int[] nums, int left, int right, int target) &#123; while (left &lt;= right) &#123; int mid = (left + right) / 2; if (nums[mid] &lt; target) left = mid + 1; else right = mid - 1; &#125; return left; &#125; &#125; 二十七、第K个数的问题27.1 无序数组寻找第K大的数这题是一道很好的面试题目，首先题目短小，很快就能说清题意而且有很多种解法。从简单到复杂的解法都有，梯度均匀。解决它不需要预先知道特殊领域知识。 这题有很多思路： 按从大到小全排序，然后取第k个元素，时间复杂度O(nlogn)，空间复杂度O(1) 利用堆进行部分排序。维护一个大根堆，将数组元素全部压入堆，然后弹出k次，第k个就是答案。时间复杂度$O(klogn)$，空间复杂度$O(n)$ 选择排序，第k次选择后即可得到第k大的数，时间复杂度O(nk)，空间复杂度O(1) 以上三种方法时间复杂度太高。下面介绍两种更好的方法： 第一种 利用快速排序中的partition思想，从数组中随机选择一个基准pivot，把数组划分为左右两部分，左边部分元素小于pivot，右边部分元素大于或等于pivot。 1234567891011121314151617181920212223242526272829public class Solution &#123; public static int findKthLargest(int[] nums, int k) &#123; k = nums.length-k;//找前K大的 int low = 0; int high = nums.length-1; while(low &lt; high)&#123; int pivotloc = partition(nums,low,high); if(pivotloc==k) break; else if(k&lt;pivotloc)&#123;//左边 high = pivotloc-1; &#125;else if(k&gt;pivotloc)&#123; low = pivotloc+1;//在右边 &#125; &#125; return nums[k]; &#125; public static int partition(int[] nums, int low, int high)&#123; int pivot = nums[low]; while(low&lt;high)&#123; while(low&lt;high &amp;&amp; nums[high]&gt;=pivot) high--; nums[low]=nums[high]; while(low&lt;high &amp;&amp; nums[low]&lt;=pivot) low++; nums[high] =nums[low]; &#125; nums[low]=pivot; return low; &#125;&#125; 第二种方法： 遍历数组时将数字加入优先队列（堆），一旦堆的大小大于k就将堆顶元素去除，确保堆的大小为k。遍历完后堆顶就是返回值。 12345678910public class Solution &#123; public int findKthLargest(int[] nums, int k) &#123; PriorityQueue&lt;Integer&gt; p = new PriorityQueue&lt;Integer&gt;(); for(int i = 0 ; i &lt; nums.length; i++)&#123; p.add(nums[i]); if(p.size()&gt;k) p.poll(); &#125; return p.poll(); &#125;&#125; 27.2 两个有序数组的第K大的数两个有序数组，寻找归并排序后数组的中位数/第 k 大数字（与二分有关）； 27.3 N个有序数组求最大的K个数27.4 求两个有序数组的中位数二十八、求根算法（ LeetCode 69）这道题要找x的平方根，x的平方根肯定小于x/2。要在[1,x/2]有序序列当中找一个数，用二分法： 1234567891011121314public int mySqrt(int x) &#123; long high = (x / 2) + 1; long low = 0; while (high &gt;= low) &#123; long mid = (high + low) / 2; if (mid * mid == x) return (int)mid; else if (mid * mid &gt; x) high = mid - 1; else low = mid + 1; &#125; return (int)high; &#125; 二分法在数值计算中非常常见，还是得熟练掌握。其实这个题目还有另一种方法，称为牛顿法。不过他更多的是一种数学方法，算法在这里没有太多体现，不过牛顿法是数值计算中非常重要的方法，所以我还是介绍一下。不太了解牛顿法基本思想的朋友，可以参考一下牛顿法-维基百科。一般牛顿法是用数值方法来解一个f(y)=0的方程（为什么变量在这里用y是因为我们要求的开方是x，避免歧义）。对于这个问题我们构造f(y)=y^2-x，其中x是我们要求平方根的数，那么当f(y)=0时，即y^2-x=0,所以y=sqrt(x),即是我们要求的平方根。f(y)的导数f’(y)=2*y，根据牛顿法的迭代公式我们可以得到y_(n+1)=y_n-f(n)/f’(n)=(y_n+x/y_n)/2。最后根据以上公式来迭代解以上方程。 1234567891011public int sqrt(int x) &#123; if (x == 0) return 0; double lastY = 0; double y = 1; while (y != lastY) &#123; lastY = y; y = (y + x / y) / 2; &#125; return (int)(y); &#125; 面经中出现过的数组题已整理 一、二维数组，每行递增，每列递增,实现查找；二维数组，每行递增，每列递增，求第 k 大的数； 三、旋转数组的查找；有序数组 从中间某点隔开，右边的放到左边，然后问在这个数组中怎么进行二分查找。讲了思路后手写代码；一维有序数组，经过循环位移后，最小的数出现在数列中间，如果原数组严格递增，如何找这个最小数；如果原数组严格递增或递减，如何找这个最小数；如果原数组非严格递增或递减，如何找这个最小数； 五、调整数组顺序使奇数位于偶数前面；让一个数组中的所有奇数在前，偶数在后。 六、顺时针打印矩阵 九、数组中超过一半的数字；找出数组中出现次数超过一半的数，现在有一个数组，已知一个数出现的次数超过了一半，请用O(n)的复杂度的算法找出这个数我说了一个最简答的，直接遍历数组，用map存储《数，出现的次数》这个键-值对，然后找出超过一半的即可。继续优化，，，，没答上来 十、最小的K个数；数列中找第 k 大的数字（与快排或堆排序有关）；编程题最少时间复杂度求数组中第k大的数， 十一、连续子数组的最大和 十二、从1到n整数中1出现的次数；给你一个数N，问1-N这N个整数里面，每个位上一共出现多少次数字1. 这个是编程之美上的原题。当时看的时候觉得好复杂，最后我也没写出来，当然，面试的技巧就是，无论这道题你会还是不会，尽量把你的思考过程说出来，一方面防止冷场，另一方面可以让他知道你的思考过程。最后面试官让我不用最优的，我就写了个最笨的。 十五、数组中逆序对计算，打印逆序对儿，如输入数组[1,4,2,5,3]，输出(4,2),(4,3),(5,3)；统计数列中的逆序对（归并排序有关）； 十八、和为S的两个数字VS和为s的连续正数序列；有序数组寻找和为某数的一对数字；寻找和为定值的多个数；寻找和为定值的两个数 二十四、滑动窗口的最大值；coding：数组滑动窗口得到最大的窗口，O（n）复杂度、扩展：这个滑动窗口中必须有m个不同才可以，目标不变，如果改，并分析时间复杂度； 二十六、寻找某个值的区间（leetcode 34 Search for a Range） 未整理数组中后面的数减前面的数的差的最大值，要求时间、空间复杂度尽可能底多个有序数组的归并多个有序数组求交集两个有序数组求差集两个数组，求差集两个集合如何求并集，交集； leetcode 89 Gray Codeleetcode 42 trapping rain water 墙里能装多少水 有一个数组，让找到两个不重复的连续子序列A,B ，求Max(Sum(A)-Sum(B)。 3分钟解出，10分钟写完代码 LeetCode原题： 有一个集合A包含了一些数，输入N，求元素个数最小的集合B，使得A并B后内的数组合相加能够组成1到N中的所有数 给定一个数组，长度已知为N（中等），求中位数，要求时间复杂度尽可能小：快排+丢弃长度小于N/2的部分 给定数组，要求以最小的时间复杂度求得最大最小值：维护一个小数组，只存放两个数，预定义为min和max，后将剩余N-2个数和它们依次比较，大于max的覆盖max，小于min的覆盖min，其余情形的不更新数组。时间复杂度o(n)。 然后考了个数据结构，给个数组如何建堆http://xfhnever.com/categories/算法与数据结构/page/2/ 一维数组，swap 其中的几对数字（每个数字只属于一次 swap 操作），实现查找（与二分有关）；一个有序数组，其中一个数字发生变异，但不知道变异后会不会影响整体序，如何实现查找；任意交换其中的两数，发现并恢复； 给定数组，寻找 next big（堆排序有关）； 数组可能是递增、递减、递减后递增、递增后递减四种情况，递增递减都是非严格的，如果有转折点，返回转折点的值，否则返回-1； 给出一个数组，返回数组中满足类似a &gt; b &lt; c ＞ｄ这种情况的连续子数组的最大长度。时间复杂度当然是Ｏ（ｎ），比较有意思的题目。 同样是一个数组，去掉其中一个数，得到与这个数相邻中比较小的那个数的值（如果是最左侧或者最右侧则返回０）。不断重复上面的操作，直到数组为空，返回累加的最大值。 已知一个数组，有n+2个不同的数，其中n个数出现了偶数次，2个数出现了奇数次，设计算法找出这2个数又只想出一个简单的，用栈，偶数的进出进出，最后在栈中没有了，奇数的进出进，最后会留在栈中。就找到了（这个空间复杂度为O（n）继续优化，，又没想出来。 写递推公式给定整数n和m，问能不能找出整数x，使得x以后的所有整数都可以由整数n和m组合而成 区间查询最大值，要求查询复杂度为O(1)，正解为st表，我敲的线段树，也过了]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>数组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（1）：链表题解]]></title>
    <url>%2F2017%2F08%2F03%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%881%EF%BC%89%EF%BC%9A%E9%93%BE%E8%A1%A8%E9%A2%98%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[链表相关题解java实现。 一、从尾到头打印链表（剑5）输入一个链表的头结点，从尾到头反过来打印每个结点的值（注意不能改变链表的结构）。 解决这个问题肯定要遍历链表。遍历的顺序是从头到尾的顺序，可输出的顺序却是从尾到头。也就是说第一个遍历到的结点最后一个输出，而最后一个遍历到的结点第一个输出。这就是典型的“后进先出”，我们可以用栈实现这种顺序。没经过一个节点的时候，把该结点放到一个栈中。当遍历完整个链表后，再从栈顶开始逐个输出结点的值，此时输出的结点的顺序就翻转过来了。实现代码如下： 12345678910111213141516import java.util.Stack;import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); while (listNode!=null)&#123; stack.push(listNode.val); listNode = listNode.next; &#125; ArrayList&lt;Integer&gt; List = new ArrayList&lt;&gt;(); while(!stack.isEmpty())&#123; List.add(stack.pop()); &#125; return List; &#125;&#125; 既然想到了用栈来实现这个函数，而递归在本质上就是一个栈结构，因此可用递归来实现。要实现反过来输出链表，我们每访问到一个节点的时候，先递归输出它后面的结点，再输出该结点自身，这样链表的输出结果就反过来了。实现代码如下： 12345678910111213141516import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; ArrayList&lt;Integer&gt; list=new ArrayList&lt;Integer&gt;(); ListNode pNode=listNode; if(pNode!=null)&#123; if(pNode.next!=null)&#123; list=printListFromTailToHead(pNode.next); &#125; list.add(pNode.val); &#125; return list; &#125;&#125; 二、在O(1)时间删除链表结点（剑13）给定单向链表的头指针和一个结点指针，定义一个函数在O(1)时间删除该结点。 我们要删除结点i，先把i的下一个结点i.next的内容复制到i，然后在把i的指针指向i.next结点的下一个结点即i.next.next，它的效果刚好是把结点i给删除了。 此外还要考虑删除的结点是头尾结点、链表中只有一个结点、链表为空这几种情况。 java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class DeleteNode &#123; /** * 链表结点 */ public static class ListNode &#123; int value; // 保存链表的值 ListNode next; // 下一个结点 &#125; /** * 给定单向链表的头指针和一个结点指针，定义一个函数在0(1)时间删除该结点, * 【注意1：这个方法和文本上的不一样，书上的没有返回值，这个因为JAVA引用传递的原因， * 如果删除的结点是头结点，如果不采用返回值的方式，那么头结点永远删除不了】 * 【注意2：输入的待删除结点必须是待链表中的结点，否则会引起错误，这个条件由用户进行保证】 * * @param head 链表表的头 * @param toBeDeleted 待删除的结点 * @return 删除后的头结点 */ public static ListNode deleteNode(ListNode head, ListNode toBeDeleted) &#123; // 如果输入参数有空值就返回表头结点 if (head == null || toBeDeleted == null) &#123; return head; &#125; // 如果删除的是头结点，直接返回头结点的下一个结点 if (head == toBeDeleted) &#123; return head.next; &#125; // 下面的情况链表至少有两个结点 // 在多个节点的情况下，如果删除的是最后一个元素 if (toBeDeleted.next == null) &#123; // 找待删除元素的前驱 ListNode tmp = head; while (tmp.next != toBeDeleted) &#123; tmp = tmp.next; &#125; // 删除待结点 tmp.next = null; &#125; // 在多个节点的情况下，如果删除的是某个中间结点 else &#123; // 将下一个结点的值输入当前待删除的结点 toBeDeleted.value = toBeDeleted.next.value; // 待删除的结点的下一个指向原先待删除引号的下下个结点，即将待删除的下一个结点删除 toBeDeleted.next = toBeDeleted.next.next; &#125; // 返回删除节点后的链表头结点 return head; &#125; 三、链表中倒数第K个结点（剑15）输入一个链表，输出该链表中倒数第k个结点。为了符合大多数人的习惯，本题从1 开始计数，即链表的尾结点是倒数第1个结点。例如一个链表有6个结点，从头结点开始它们的值依次是1、2、3、4、5、6。这个链表的倒数第3个结点的值为4的结点。 很自然的想法是先走到链表尾端，再从尾端回溯k步。可是我们从链表结点的定义可以看出本题中的链表是单向链表，单向链表的结点只有从前向后的指针而没有从后往前的指针，这种思路行不通。 既然不能从尾结点开始遍历链表，我们还是把思路回到头结点上来。假设整个链表有n个结点，那么倒数第k个结点就是从头结点开始往后走n-k+1步就可以了。如何得到结点树n？只需要从头开始遍历链表，每经过一个结点，计数器加1就行了。 也就是说我们需要遍历链表两次，第一次统计出链表中的结点的个数，第二次就能找到倒数第k个结点。但是面试官期待的解法是只需要遍历链表一次。 为了实现只遍历链表一次就能找到倒数第k个结点，我们可以定义两个指针。第一个指针从链表的头指针开始遍历向前走k-1步，第二个指针保持不动；从第k步开始，第二个指针也开始从链表的头指针开始遍历。由于两个指针的距离保持在k-1，当第一个（走在前面的）指针到达链表的尾结点时，第二个指针（走在后边的）指针正好是倒数第k个结点。 但是这样写出来的代码不够鲁棒，面试官可以找出三种办法让这段代码崩溃： 输入的ListHead为空指针。由于代码会试图访问空指针指向的内存，程序崩溃。 输入的以ListHead为头结点的链表的结点总数少于k。由于在for循环中会在链表上向前走k-1步，仍然会由于空指针造成的程序奔溃。 输入的参数k为0.由于k是一个无符号整数，那么在for循环中k-1得到的将不是-1，而是4294967295（无符号的0xFFFFFFFFF），因此for循环执行的次数远远超过我们的预计，同样也会造成程序崩溃。 面试过程中写代码特别要注意鲁棒性，若写出的代码存在多处崩溃的风险，那我们很可能和offer失之交臂。针对前面三个问题，分别处理。若输入的链表头指针为null，那么整个链表为空，此时查找倒数第k个结点自然应该返回null。若输入的k为0，也就是试图查找倒数第0个结点，由于我们计数是从1开始的，因此输入0是没有实际意义，也可以返回null。若链表的结点数少于k，在for循环中遍历链表可能会出现指向null的next，因此我们在for循环中应该加一个if循环。 代码如下： java版本 12345678910111213141516171819202122232425262728293031/*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode FindKthToTail(ListNode head,int k) &#123; if(head==null||k &lt;=0)&#123;return null;&#125; ListNode pAhead = head; ListNode pBehind = head; for(int i=1;i&lt;k;i++)&#123; if(pAhead.next != null) &#123;pAhead = pAhead.next;&#125; else &#123;return null;&#125; &#125; while(pAhead.next!=null) &#123; pAhead = pAhead.next; pBehind = pBehind.next; &#125; return pBehind; &#125;&#125; python版本 12345678910111213141516171819202122# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def FindKthToTail(self, head, k): # write code here if not head or k == 0: return None pAhead = head pBehind = None for i in xrange(0,k-1): if pAhead.next != None: pAhead = pAhead.next else: return None pBehind = head while pAhead.next != None: pAhead = pAhead.next pBehind = pBehind.next return pBehind 四、反转链表（剑16）定义一个函数，输入一个链表的头结点，反转该链表并输出反转后的头结点。链表结点定义如下： 12345678public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125; 解决与链表相关的问题总是有大量的指针操作，而指针操作的代码总是容易出错的。 为了正确地反转一个链表，需要调整链表中指针的方向。为了将调整指针这个复杂的过程分析清楚，可以借助图形来直观分析。在下图所示的链表中，h、i、j是3个相邻的结点。假设经过若干操作，我们已经把结点h之前的指针调整完毕，这些结点的next指向h，此时链表的结果如下所示： 其中（a）为一个链表，（b）把i之前的所有结点的next都指向前一个结点，导致链表在结点i、j之间断裂。 不难注意到，由于结点i的next指向了它的前一个结点，导致我们无法再链表中遍历到结点j。为了避免链表在结点i处断开，我们需要在调整结点i的next之前把结点j保存下来。 也就是说我们在调整结点i的next指针时，除了需要知道结点i本身之外，还需要前一个结点h，因为我们需要把结点i的next指向结点h。同时，我们还事先需要保存i的一个结点j，以防止链表断开。因此相应地我们需要定义3个指针，分别指向当前遍历到的结点、它的前一个结点及后一个结点。 最后我们试着找到反转后链表的头结点。不难分析出反转后链表的头结点是原始链表的尾结点。什么结点是尾结点？自然是next为null的结点。 pre\rightarrow head \rightarrow next先保存next，即$next = head.next$再反转head的指针$head.next=pre $，链表结构变成 pre\leftarrow head \ \ \ next接着向后移动结点$pre=head,head=next$ 实现代码如下： java版本 1234567891011121314151617181920212223242526272829303132public class Solution &#123; public ListNode ReverseList(ListNode head) &#123; if(head==null) return null; //head为当前节点，如果当前节点为空的话，那就什么也不做，直接返回null； ListNode pre = null; ListNode next = null; //当前节点是head，pre为当前节点的前一节点，next为当前节点的下一节点 //需要pre和next的目的是让当前节点从pre-&gt;head-&gt;next1-&gt;next2变成pre&lt;-head next1-&gt;next2 //即pre让节点可以反转所指方向，但反转之后如果不用next节点保存next1节点的话，此单链表就此断开了 //所以需要用到pre和next两个节点 //1-&gt;2-&gt;3-&gt;4-&gt;5 //1&lt;-2&lt;-3 4-&gt;5 while(head!=null)&#123; //做循环，如果当前节点不为空的话，始终执行此循环，此循环的目的就是让当前节点从指向next到指向pre //如此就可以做到反转链表的效果 //先用next保存head的下一个节点的信息，保证单链表不会因为失去head节点的原next节点而就此断裂 next = head.next; //保存完next，就可以让head从指向next变成指向pre了，代码如下 head.next = pre; //head指向pre后，就继续依次反转下一个节点 //让pre，head，next依次向后移动一个节点，继续下一次的指针反转 pre = head; head = next; &#125; //如果head为null的时候，pre就为最后一个节点了，但是链表已经反转完毕，pre就是反转后链表的第一个节点 //直接输出pre就是我们想要得到的反转后的链表 return pre; &#125;&#125; python版本 123456789101112131415161718# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回ListNode def ReverseList(self, pHead): # write code here if not pHead or not pHead.next: return pHead pre = None while pHead: next1 = pHead.next pHead.next = pre pre = pHead pHead = next1 return pre 五、合并两个排序的链表（剑17）输入两个递增排序的链表，合并这两个链表并使新链表中的结点仍然是按照递增排序的。例如下图中的链表1和链表2，则合并之后的升序链表3如下所示： 这是一个经常被各公司采用的面试题。在面试过程中，最容易犯两种错误：一是在写代码之前没有对合并的过程想清楚，最终合并出来的链表要么中间断开了，要么并没有做到递增排序；二是代码在鲁棒性方面存在问题，程序一旦有特殊的输入（如空链表）就会奔溃。首先分析合并两个链表的过程。从合并两个链表的头结点开始。链表1的头结点的值小于链表2的头结点的值，因此链表1的头结点将是合并后链表的头结点。 继续合并剩余的结点。在两个链表中剩下的结点依然是排序的，因此合并这两个链表的步骤和前面的步骤是一样的。依旧比较两个头结点的值。此时链表2的头结点值小于链表1的头结点的值，因此链表2的头结点的值将是合并剩余结点得到的链表的头结点。把这个结点和前面合并链表时得到的链表的尾结点链接起来。 当我们得到两个链表中值较小的头结点并把它链接到已经合并的链表之后，两个链表剩余的结点依然是排序的，因此合并的步骤和之前的步骤是一样的。这是典型的递归过程，我们可以定义递归函数完成这一合并过程。（解决这个问题需要大量的指针操作，如没有透彻地分析问题形成清晰的思路，很难写出正确的代码） 接下来解决鲁棒性问题，每当代码试图访问空指针指向的内存时程序就会奔溃，从而导致鲁棒性问题。本题中一旦输入空的链表就会引入空的指针，因此我们要对空链表单独处理。当第一个链表是空链表，也就是它的头结点是一个空指针时，和第二个链表合并的结果就是第二个链表。同样，当输入的第二个链表的头结点是空指针的时候，和第一个链表合并得到的结果就是第一个链表。如果两个链表都为空，合并得到的是一个空链表。（由于有大量的指针操作，如果稍有不慎就会在代码中遗留很多与鲁棒性相关的隐患。建议应聘者在写代码之前全面分析哪些情况会引入空指针，并考虑清楚怎么处理这些空指针。） 代码如下： java 1234567891011121314151617181920212223242526public class Solution &#123; public ListNode Merge(ListNode list1,ListNode list2) &#123; //新建一个头节点，用来存合并的链表。 ListNode head=new ListNode(-1); head.next=null; ListNode root=head; while(list1!=null&amp;&amp;list2!=null)&#123; if(list1.val&lt;list2.val)&#123; head.next=list1; list1=list1.next; &#125;else&#123; head.next=list2; list2=list2.next; &#125; head = head.next; &#125; //把未结束的链表连接到合并后的链表尾部 if(list1!=null)&#123; head.next=list1; &#125; if(list2!=null)&#123; head.next=list2; &#125; return root.next; &#125;&#125; java递归写法 123456789101112131415161718public class Solution &#123; public ListNode Merge(ListNode list1,ListNode list2) &#123; if (list1==null) return list2; else if (list2==null) return list1; ListNode MergeHead = null; if (list.val&lt;=list2.val)&#123; MergeHead = list1; MergeHead.next = Merge(list1.next,list2); &#125; else &#123;MergeHead = list2; MergeHead.next = Merge(list1,list2.next); &#125; return MergeHead; &#125;&#125; 六、复杂链表的复制（剑26）输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） java 12345678910111213141516171819202122232425262728293031323334public class Solution &#123; public RandomListNode Clone(RandomListNode pHead) &#123; if (pHead == null) return null; //复制next 如原来是A-&gt;B-&gt;C 变成A-&gt;A'-&gt;B-&gt;B'-&gt;C-&gt;C' RandomListNode pCur = pHead; while (pCur != null) &#123; RandomListNode node = new RandomListNode(pCur.label); node.next = pCur.next; pCur.next = node; pCur = node.next; &#125; //复制random pCur是原来链表的结点 pCur.next是复制pCur的结点 pCur = pHead; while (pCur!=null) &#123; if (pCur.random!=null) pCur.next.random = pCur.random.next; pCur = pCur.next.next; &#125; //拆分链表 RandomListNode head = pHead.next; RandomListNode tmp = head; pCur = pHead; while(pCur.next!=null) &#123; tmp = pCur.next; pCur.next = tmp.next; pCur = tmp; &#125; return head; &#125;&#125; 七、二叉搜索树与双向链表（剑27）输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 java 12345678910111213141516171819202122232425public class Solution &#123; TreeNode head = null; TreeNode realHead = null; public TreeNode Convert(TreeNode pRootOfTree) &#123; ConvertSub(pRootOfTree); return realHead//realHead是每个子树排序后的第一个结点，head是排序后的最后一个结点; &#125; private void ConvertSub(TreeNode pRootOfTree) &#123; //递归中序遍历 if(pRootOfTree==null) return; ConvertSub(pRootOfTree.left); if (head == null) &#123; //初始处 head = pRootOfTree; realHead = pRootOfTree; &#125; else &#123; //前两句实现双向，第三句跳到下一个节点。 head.right = pRootOfTree; pRootOfTree.left = head; head = pRootOfTree; &#125; ConvertSub(pRootOfTree.right); &#125;&#125; 八、两个链表的第一个公共结点（剑37）输入两个链表找出他们的第一个公共结点。 面试的时候碰到这道题，很多应聘者的第一个想法就是蛮力法：在第一个链表上顺序遍历每个结点，每遍历到一个结点的时候，在第二个链表上顺序遍历每个结点。若第二个链表上有一个结点和第一个链表上的结点一样，说明两个链表在这个结点上重合，于是就找到了它们的公共结点。如果第一个链表的长度为m，第二个链表的长度为n，显然该方法的时间复杂度是O(mn)。 通常蛮力法不会是最好的办法，我们接下来试着分析有公共结点的两个链表有哪些特点。从链表结构的定义看出，这两个链表是单向链表。如果他们有公共的结点，那么这两个链表从某一结点开始，他们的next指向同一个结点。但由于是单向链表的结点，每个结点只有一个next，因此从第一个公共结点开始，之后的结点都是重合的，不可能再出现分叉。所以两个有公共结点而部分重合的链表，拓扑形状看起来像一个Y，而不是X。 经过我们的分析发现，若两个链表有公共结点，那么公共结点出现在两个链表的尾部。如果我们从两个链表的尾部开始往前比较，最后一个相同的结点就是我们要找的结点。我们想到用栈的特点来解决这个问题：分别把两个链表的结点放入两个栈中，这样两个链表的尾结点就位于两个栈的栈顶，接下来比较两个栈顶的结点是否相同。若果相同，则把栈顶弹出接着比较下一个栈顶，直到找到最后一个相同的结点。 上面需要用到两个辅助栈。若链表的长度分别为m和n，那么空间复杂度是O(m+n)。这种思路的时间复杂度也是O(m+n)。和最开始的蛮力法相比，时间效率得到了提升，相当于是用空间换取时间效率。 之所以需要用到栈，是因为我们想同时遍历到达两个栈的尾结点。当两个链表的长度不相同时，如果我们从头开始遍历到达尾结点的时间就不一致。其实解决这个问题还有一个更简单的办法：首先遍历两个链表得到他们的长度，就能知道哪个链表比较长，以及长的链表比短的链表多几个结点。在第二次遍历的时候，在较长的链表上先走若干步，接着再同时在两个链表上遍历，找到的第一个相同的结点就是他们的第一个公共结点。 第三种思路和第二种思路相比，时间复杂度都是O(m+n)，但我们不再需要辅助的栈，因此提高了空间效率。实现代码如下： java版本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode FindFirstCommonNode(ListNode pHead1, ListNode pHead2) &#123; ListNode current1 = pHead1;//链表1 ListNode current2 = pHead2;//链表2 if(pHead1 ==null||pHead2==null)&#123;return null;&#125;// int len1 = getlistlength(pHead1);//链表1的长度 int len2 = getlistlength(pHead2);//链表2的长度 //若链表1长度大于链表2 if(len1&gt;=len2)&#123; int len=len1-len2; //遍历链表1，遍历长度为两链表长度差 while (len&gt;0)&#123; current1 = currentnext; len--; &#125; &#125; //若链表2长度大于链表1 else if(len1&lt;len2)&#123; int len=len2-len1; //遍历链表2，遍历长度为两链表长度差 while (len&gt;0)&#123; current2=current2.next; len--; &#125; &#125; //开始齐头并进，直到找到第一个公共结点 while(current1!=current2)&#123; current1 = currentnext; current2 = current2.next; &#125; return current1; &#125; //求指定链表的长度 public static int getlistlength(ListNode pHead)&#123; int length = 0; ListNode current = pHead; while(current!=null)&#123; length++; current = current.next; &#125; return length; &#125;&#125; python版本 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def FindFirstCommonNode(self, pHead1, pHead2): # write code here current1=pHead1 current2=pHead2 len1 = self.getlistlength(current1) len2 = self.getlistlength(current2) if len1&gt;=len2: length = len1-len2 while length&gt;0: current1 = currentnext length=length-1 elif len1&lt;len2: length = len2-len1 while length&gt;0: current2 = current2.next length=length-1 while current1!=current2: current1=currentnext current2=current2.next return current1 def getlistlength(self,pHead): length =0 current =pHead while current!=None: length=length+1 current = current.next return length 九、圆圈中最后剩下的的数字（剑45）0、…..，n-1这n个数字排成一个圆圈，从数字0开始每次从这个圆圈里删除第m个数字。求出这个圆圈里剩下的最后一个数字。约瑟夫环问题，用环形链表模拟圆圈的经典解法， java 12345678910111213141516171819202122232425262728public class Solution&#123; public int LastRemaining_Solution(int n, int m)&#123; if(m&lt;=0||n&lt;=0)return -1; //先构造循环链表 ListNode head= new ListNode(0);//头结点, 值为0 ListNode pre = head; ListNode temp = null; for(int i=1;i&lt;n;i++)&#123; temp = new ListNode(i); pre.next = temp; pre = temp; &#125; temp.next = head;//将第n-1个结点(也就是尾结点)指向头结点 ListNode temp2 = null; while(n&gt;=1)&#123; //每次都当前头结点找到第m个结点的前驱 temp2=head; for(int i =1;i&lt;m-1;i++)&#123; temp2 = temp2.next; &#125; temp2.next = temp2.next.next; head = temp2.next;//设置当前头结点 n--; &#125; return head.val; &#125;&#125; java 12345678910111213public class Solution&#123; public int LastRemaining_Solution(int n, int m) &#123; if(n==0||m==0)return -1; int last=0; for(int i=2;i&lt;=n;i++) &#123; last=(last+m)%i; &#125; return last ; &#125;&#125; 十、链表中环的入口结点（剑56）一个链表中包含环，如何找到环的入口结点？例如在下图的链表中，环的入口结点是结点3。 以3为例分析两个指针的移动规律。指针$P_1$和$P_2$在初始化时都指向链表的头结点。由于环中有4个结点，指针$P_1$先在链表上向前移动4步。接下来两个指针以相同的速度在链表上向前移动，直到它们相遇。它们相遇的结点正好是还的入口结点。 剩下的问题就是如何得到环中结点的数目。我们可以使用一快一慢两个指针。若两个指针相遇，说明链表中有环。两个指针相遇的结点一定是在环中的。可以从这个结点出发，一边继续向前移动一边计数，当再次回到这个结点时，就可以得到环中结点数了实现代码如下： java版本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; //找到一快一满指针相遇处的节点，相遇的节点一定是在环中 public static ListNode meetingNode(ListNode pHead) &#123; if(pHead == null)&#123;return null;&#125;//空链表处理 ListNode pslow = pHead.next; if(pslow == null)&#123;return null;&#125;//无环链表处理 ListNode pfast = pslow.next; while(pfast!=null &amp;&amp; pslow!=null)&#123; if(pslow==pfast)&#123;return pfast;&#125; pslow = pslow.next;//慢指针 pfast = pfast.next; if(pfast!=null)&#123; pfast = pfast.next; &#125;//块指针 &#125; return null; &#125; public ListNode EntryNodeOfLoop(ListNode pHead)&#123; ListNode meetingNode=meetingNode(pHead);//相遇结点 //环的结点个数 if(meetingNode==null)&#123;return null;&#125;//是否有环 int nodesInLoop = 1; ListNode p1=meetingNode; while(pnext!=meetingNode)&#123; p1=pnext; ++nodesInLoop; &#125; //p1慢指针,先往前走 p1=pHead; for(int i=0;i&lt;nodesInLoop;i++)&#123; p1=pnext; &#125; //p1,p2同步走，相遇的地方即为环入口 ListNode p2=pHead; while(p1!=p2)&#123; p1=pnext; p2=p2.next; &#125; return p1; &#125; &#125; python 版本 123456789101112131415161718192021222324252627282930313233343536373839# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def meetingNode(self,pHead): if not pHead: return None pslow =pHead.next if not pslow: return None pfast = pslow.next while pfast and pslow: if pslow==pfast: return pfast pslow = pslow.next pfast = pfast.next if pfast: pfast=pfast.next return None def EntryNodeOfLoop(self, pHead): meetingNode = self.meetingNode(pHead) if not meetingNode: return None nodesInLoop = 1 p1 = meetingNode while pnext!=meetingNode: p1=pnext nodesInLoop +=1 p1 = pHead for i in xrange(0,nodesInLoop): p1=pnext p2=pHead while p1!=p2: p1=pnext p2=p2.next return p1 十一、删除链表中重复的结点（剑57）在一个排序的链表中，如何删除重复的结点？如在下图中重复结点被删除之后，链表如下图所示： 从头遍历整个链表。如果当前结点的值与下一个节点的值相同，那么它们就是重复的结点，都可以被删除。为了保证删除之后的链表仍然是相连的而没有中间断开，我们要把当前结点的前一个结点preNode和后面值比当前结点的值要大的结点相连。要确保preNode要始终与下一个没有重复的结点连接在一起。 实现代码如下： java递归版 12345678910111213141516171819202122232425262728/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode deleteDuplication(ListNode pHead) &#123; if (pHead == null || pHead.next == null) &#123; // 只有0个或1个结点，则返回 return pHead; &#125; if (pHead.val == pHead.next.val) &#123; // 当前结点是重复结点 ListNode pNode = pHead.next; while (pNode != null &amp;&amp; pNode.val == pHead.val) &#123; // 跳过值与当前结点相同的全部结点,找到第一个与当前结点不同的结点 pNode = pNode.next; &#125; return deleteDuplication(pNode); // 从第一个与当前结点不同的结点开始递归 &#125; else &#123; // 当前结点不是重复结点 pHead.next = deleteDuplication(pHead.next); // 保留当前结点，从下一个结点开始递归 return pHead; &#125; &#125;&#125; python版本 1234567891011121314151617# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def deleteDuplication(self, pHead): if not pHead or not pHead.next: return pHead if pHead.val==pHead.next.val: pNode = pHead.next while pNode and pNode.val == pHead.val: pNode = pNode.next return self.deleteDuplication(pNode) else: pHead.next = self.deleteDuplication(pHead.next) return pHead java非递归 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode deleteDuplication(ListNode pHead) &#123; if(pHead==null)return null; ListNode preNode = null; ListNode node = pHead; while(node!=null)&#123; ListNode nextNode = node.next; boolean needDelete = false; //需要删除重复节点的情况 if(nextNode!=null&amp;&amp;nextNode.val==node.val)&#123; needDelete = true; &#125; //不重复结点不删除 if(!needDelete)&#123; preNode = node; node = node.next; &#125; //重复节点删除 else&#123; int value = node.val; ListNode toBeDel = node; //连续重复结点 while(toBeDel != null &amp;&amp; toBeDel.val == value)&#123; nextNode = toBeDel.next; toBeDel = nextNode; if(preNode==null) pHead = nextNode; else preNode.next = nextNode; node = nextNode; &#125; &#125; &#125; return pHead; &#125;&#125; 十二、翻转部分链表(leetcode 92 Reverse Linked List II)给了一个链表，第1个结点标号为1，把链表中标号在M到N区间的部分反转 （我写的很慢，面试官看不下去了，让我只说思路） 这道题是比较常见的链表反转操作，不过不是反转整个链表，而是从m到n的一部分。分为两个步骤，第一步是找到m结点所在位置，第二步就是进行反转直到n结点。反转的方法就是每读到一个结点，把它插入到m结点前面位置，然后m结点接到读到结点的下一个。总共只需要一次扫描，所以时间是O(n)，只需要几个辅助指针，空间是O(1)。代码如下： java 123456789101112131415161718192021222324252627public class Solution &#123; public ListNode reverseBetween(ListNode head, int m, int n) &#123; if(head == null) return null; ListNode dummy = new ListNode(0); dummy.next = head; ListNode preNode = dummy; int i=1; while(preNode.next!=null &amp;&amp; i&lt;m) &#123; preNode = preNode.next; i++; &#125; ListNode mNode = preNode.next; ListNode cur = mNode.next; while(cur!=null &amp;&amp; i&lt;n) &#123; ListNode next = cur.next; cur.next = preNode.next; preNode.next = cur; mNode.next = next; cur = next; i++; &#125; return dummy.next; &#125; &#125; 十三、链表插入排序(leetcode 147 Insertion Sort List)这道题跟Sort List类似，要求在链表上实现一种排序算法，这道题是指定实现插入排序。插入排序是一种$O(n^2)$复杂度的算法，基本想法相信大家都比较了解，就是每次循环找到一个元素在当前排好的结果中相对应的位置，然后插进去，经过n次迭代之后就得到排好序的结果了。了解了思路之后就是链表的基本操作了，搜索并进行相应的插入。时间复杂度是排序算法的$O(n^2)$，空间复杂度是O(1)。代码如下： java 123456789101112131415161718192021222324252627public class Solution &#123; public ListNode insertionSortList(ListNode head) &#123; if( head == null )&#123; return head; &#125; ListNode helper = new ListNode(0); //new starter of the sorted list ListNode cur = head; //the node will be inserted ListNode pre = helper; //insert node between pre and pre.next ListNode next = null; //the next node will be inserted //not the end of input list while( cur != null )&#123; next = cur.next; //find the right place to insert while( pre.next != null &amp;&amp; pre.next.val &lt; cur.val )&#123; pre = pre.next; &#125; //insert between pre and pre.next cur.next = pre.next; pre.next = cur; pre = helper; cur = next; &#125; return helper.next; &#125;&#125; 十四、链表归并排序(leetcode 148 Sort List)链表排序，不允许直接交换节点的值，敲的有点bug，指针初始化有问题 这道题跟Insertion Sort List类似，要求我们用O(nlogn)算法对链表进行排序，但是并没有要求用哪一种排序算法，我们可以使用归并排序，快速排序，堆排序等满足要求的方法来实现。对于这道题比较容易想到的是归并排序，因为我们已经做过Merge Two Sorted Lists，这是归并排序的一个subroutine。剩下我们需要做的就是每次找到中点，然后对于左右进行递归，最后用Merge Two Sorted Lists把他们合并起来。代码如下： java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class Solution &#123; public ListNode sortList(ListNode head) &#123; if (head == null || head.next == null) return head; // step 1. cut the list to two halves ListNode prev = null, slow = head, fast = head; while (fast != null &amp;&amp; fast.next != null) &#123; prev = slow; slow = slow.next; fast = fast.next.next; &#125; prev.next = null; // step 2. sort each half ListNode l1 = sortList(head); ListNode l2 = sortList(slow); // step 3. merge l1 and l2 return merge(l1, l2); &#125; ListNode merge(ListNode l1, ListNode l2) &#123; ListNode l = new ListNode(0), p = l; while (l1 != null &amp;&amp; l2 != null) &#123; if (l1.val &lt; l2.val) &#123; p.next = l1; l1 = l1.next; &#125; else &#123; p.next = l2; l2 = l2.next; &#125; p = p.next; &#125; if (l1 != null) p.next = l1; if (l2 != null) p.next = l2; return l.next; &#125;&#125; 十五、实现稀疏矩阵相乘给定两个稀疏矩阵A和B，求AB。A的列数和B的行数相等。 面经：（链表，即每行的非零值存进单链表里。从面试官表情判断，应该说对了） http://www.chongchonggou.com/g_5929864.html例如： 1234567891011121314A = [ [ 1, 0, 0], [-1, 0, 3]]B = [ [ 7, 0, 0 ], [ 0, 0, 0 ], [ 0, 0, 1 ]] | 1 0 0 | | 7 0 0 | | 7 0 0 |AB = | -1 0 3 | x | 0 0 0 | = | -7 0 3 | | 0 0 1 | 这道题让我们实现稀疏矩阵相乘，稀疏矩阵的特点是矩阵中绝大多数的元素为0，而相乘的结果是还应该是稀疏矩阵，即还是大多数元素为0，那么我们使用传统的矩阵相乘的算法肯定会处理大量的0乘0的无用功，所以我们需要适当的优化算法，使其可以顺利通过OJ，我们知道一个$ i x k$ 的矩阵A乘以一个 $k x j$ 的矩阵B会得到一个$i x j$ 大小的矩阵C，那么我们来看结果矩阵中的某个元素$C[i][j]$是怎么来的。起始是$A[i][0]B[0][j] + A[i][1]B[1][j] + … + A[i][k]B[k][j]$，那么为了不重复计算0乘0，我们首先遍历A数组，要确保$A[i][k]$不为0，才继续计算，然后我们遍历B矩阵的第k行，如果$B[K][J]$不为0，我们累加结果矩阵$res[i][j] += A[i][k] B[k][j]$; 这样我们就能高效的算出稀疏矩阵的乘法，参见代码如下： java 1234567891011121314151617public int[][] multiply(int[][] A, int[][] B) &#123; int rowA = A.length; int colA = A[0].length; int colB = B[0].length; int[][] res = new int[rowA][colB]; for (int i = 0; i &lt; rowA; i++) &#123; for (int k = 0; k &lt; colA; j++) &#123; if (A[i][k] != 0) &#123;// Check whether A[i][j]==0 to spare much time, because a sparse matrix has more than 95% zero elements for (int j = 0; j&lt; colB; j++) &#123; if (B[k][j] != 0) res[i][j] += A[i][k] * B[k][j]; &#125; &#125; &#125; &#125; return res; &#125; 再来看另一种方法，这种方法其实核心思想跟上面那种方法相同，稍有不同的是我们用一个链表来记录每一行中，各个位置中不为0的列数和其对应的值，这样就得到i个链表，然后我们遍历链表，取出每行中不为零的列数和值，然后遍历B中对应行进行累加相乘，参见代码如下： java 123456789101112131415161718192021222324252627public int[][] multiply(int[][] A, int[][] B) &#123; int rowA = A.length; colA = A[0].length; colB = B[0].length; int[][] res = new int[rowA][colB]; LinkedList&lt;Point&gt;[] rowsA = new LinkedList[rowA]; for (int i = 0; i &lt; rowA; i++) &#123;// Create non-zero array for each row in A rowsA[i] = new LinkedList(); for (int j = 0; j &lt; colA; j++) &#123; if (A[i][j] != 0) &#123; rowsA[i].add(new Point(j, A[i][j])); &#125; &#125; &#125; for (int i = 0; i &lt; rowA; i++) &#123;// Only deal with non-zero elements in the above arrays for (int j = 0; j &lt; rowsA[i].size(); j++) &#123; int col = rowsA[i].get(j).x; int val = rowsA[i].get(j).y; for (int k = 0; k &lt; colB; k++) &#123; res[i][k] += val * B[col][k]; &#125; &#125; &#125; return res; &#125; 十六、排序链表中找出和为给定值的一段链表排序链表中找出和为给定值的一段链表 面经中出现过的链表题： 二、如何在O(1)时间删除链表节点； 三、第一题是链表倒数第 k 节点；如何找链表倒数第K个结点（联系这个题目，两链表找交点的题就可以在O(1)空间解决了）；单链表如何判断有环；链表中倒数第K个结点 四、反转链表递归、非递归；链表反转；链表逆序。。正中我下怀~；写程序 翻转链表；给了个单链表逆置，写代码；用C/C++实现单链表的反转。 六、复杂链表的复制 七、二叉搜索树转换成一个排好序的双向链表；上网搜搜有怎样将二叉排序树变成双向链表 八、两个相交链表如何找交点（我说了用栈保存每个链表节点的方法，他问有没有O(1)空间解法，一时没想到）；判断两条链表是否交叉 十、确定链表中环的起始位置 十二、翻转部分链表(Reverse Linked List II) 十三、链表插入排序(Insertion Sort List) 十四、链表归并排序(Sort List) 十六、排序链表中找出和为给定值的一段链表]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（19）：海量数据处理]]></title>
    <url>%2F2017%2F08%2F02%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8819%EF%BC%89%EF%BC%9A%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[一、何谓海量数据处理？所谓海量数据处理，其实很简单，海量，海量，何谓海量，就是数据量太大，所以导致要么是无法再较短时间内迅速解决，要么是数据太大，导致无法一次性装入内存。 那解决办法呢？针对时间，我们可以采用巧妙的算法搭配合适的数据结构，如Bloom Filter、Hash、Bit-map、堆、数据库或倒排索引、trie，针对空间，无非就一个办法：大而化小、分而治之、hash映射，你不是说规模太大嘛，那简单啊，就把规模大化为规模小的，各个击破不就完了嘛。 至于所谓的单机及集群问题，通俗点来讲，单机就是处理装载数据的机器有限（只要考虑CPU、内存、硬盘的数据交互），而集群，机器有多辆，适合分布式处理，并行计算（更多考虑节点和节点间的数据交互）。 再者，通过本blog内的有关海量数据处理的文章，我们已经大致知道，处理海量数据问题无非就是： 分而治之、hash映射+hash统计+堆、快速、归并排序； 双层桶划分 Bloom Filter、bitmap trie树、数据库、倒排索引； 外排序 分布式处理之Hadoop、Mapreduce 下面，本文第一部分、从Set、Map谈到hashtable、hash_map、hash_set，简要介绍下Set、Map、Multiset、Multimap，及hash_set、hash_map、hash_multiset、hash_multimap之区别，而第二部分，则针对上述6种方法模式结合对应的海量数据处理面试题分别具体阐述。 二、从Set、Map谈到hashtable、hash_map/hash_set稍后本文第二部分将多次提到hash_map、hash_set，下面稍稍介绍下这些容器，以作为基础准备。一般来说，STL容器分为两种 序列式容器：vector、list、deque、stack、queue、heap 关联式容器：关联式容器又分为set（集合）和map（映射表）两大类，以及这两大类的衍生体（多键集合）和multimap（多键映射表），这些容器均以RB-tree完成。此外，还有第三类关联容器，如hashtable（散列表），以及以hashtable为底层机制完成的hash_set（散列集合）、hash_map（散列映射表）、hash_multiset(散列多键集合)、hash_multimap（散列多键映射表）。也就是说，set、map、multiset、multimap都内含一个RB-tree，而hash_set、hash_map、hash_multiset、hash_multimap都内含一个hashtable。 所谓关联式容器，类似关联式数据库，每笔数据或每个元素都有一个键值（key）和一个实值（value），即所谓的Key-Value（键-值对）。当元素被插入到关联式容器中时，容器内结构（RB-Tree、hashtable）便依照其键值大小，以某种特定规则将这个元素放置于适当位置。 包括在非关联式数据库中，比如，在MongoDB内，文档（document）是最基本的数据组织形式，每个文档也是以Key-Value(键-值对)的方式组织起来。一个文档可以有多个Key-Value组合，每个Value可以是不同的类型，比如String、Integer、List等等。${“name”:”July”,”Sex”:”male”,”age”:23}$ 2.1 关联式容器之集合：set/map/multimap/multimapset，同map一样，所有元素都会根据元素的键值自动被排序，因为set、map两者的所有操作，都是调用RB-tree的操作行为，不过，值得注意的是，两者都不允许两个元素有相同的键值。 不同的是，set的元素不像map那样可以同时拥有实值（value）和键值（key），set元素的键值就是实值，实值就是键值，而map的所有元素都是pair，同时拥有实值（value）和键值（key），pair的第一个元素被视为键值，第二个元素被视为实值。 至于multiset、multimap，他们的特性及用法和set、map完全相同，唯一的差别就在于它们允许键值重复，即所有的插入操作基于RB-tree的insert——equal()而非insert_unique()。 2.2 关联式容器之映射表：hash_set/hash_map/hash_multiset/hash_multimaphash_set/hash_map，两者的一切操作都是基于hashtable之上。hash_set同set一样，同时拥有实值和键值，且实质就是键值，键值就是实值，而hash_map同map一样，每一个元素同时拥有一个实值(value)和一个键值(key)，所以其使用方式，和上面的map基本相同。但由于hash_set/hash_map都是基于hashtable之上，所以不具备自动排序功能。为什么?因为hashtable没有自动排序功能。 至于hash_multiset/hash_multimap的特性与上面的multiset/multimap完全相同，唯一的差别就是它们hash_multiset/hash_multimap的底层实现机制是hashtable（而multiset/multimap，上面说了，底层实现机制是RB-tree），所以它们的元素都不会被自动排序，不过也都允许键值重复。 所以，综上，说白了，什么样的结构决定其什么样的性质，因为set/map/multiset/multimap都是基于RB-tree之上，所以有自动排序功能，而hash_set/hash_map/hash_multiset/hash_multimap都是基于hashtable之上，所以不含有自动排序功能，至于加个前缀multi_无非就是允许键值重复而已。 三、处理海量数据问题之六把密钥密钥一：分而治之/Hash映射+Hash_map统计+堆/快/归并排序这一部分介绍处理海量数据问题的第一把密钥：分而治之、哈希映射+Hash统计+堆、快速、归并排序 （1）海量日志数据，提取出某日访问 既然是海量数据处理，那么可想而知，给我们的数据那就一定是海量的。针对这个数据的海量，我们如何着手呢?对的，无非就是分而治之/hash映射 + hash统计 + 堆/快速/归并排序，说白了，就是先映射，而后统计，最后排序： 分而治之/hash映射：针对数据太大，内存受限，只能是：把大文件化成(取模映射)小文件，即16字方针：大而化小，各个击破，缩小规模，逐个解决 hash统计：当大文件转化了小文件，那么我们便可以采用常规的Hashmap(ip，value)来进行频率统计。 堆/快速排序：统计完了之后，便进行排序(可采取堆排序)，得到次数最多的IP 具体而论，“首先是这一天，并且是访问百度的日志的IP取出来，逐个写到一个大文件中。注意到IP是32位的，最多有个$2^{32}$个IP。同样可以采用映射的方法，比如%1000，把整个大文件映射为1000个小文件，再找出每个小文件中出现频率最大的IP（可以采用Hash_map对那1000个文件中的所以IP进行频率统计，然后依次找出各个文件中频率最大的那个IP）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。” 关于本题，还有几个问题： Hash取模是一种等价映射，不会存在同一个元素分散到不同小文件中的情况，即这里采用的是mod1000算法，那么相同的IP在hash取模后，只可能落在同一个文件中，不可能被分散的。因为如果两个IP相等，那么经过Hash（IP）之后的哈希值是相同的，将此哈希值取模，必定仍然相等。 那到底什么是hash映射呢？简单来说，就是为了便于计算机在有限的内存中处理大数据，从而通过一种映射散列的方式让数据均匀分布在对应的内存位置（如大数据通过取余的方式映射城小数存放在内存中，或者大文件映射成多个小文件），而这个映射散列方式便是我们通常所说的hash函数，设计的好的hash函数能让数据均匀分布而减少冲突。尽管数据映射到了另外一些不同的位置，但数据还是原来的数据，只是代替和表示这些原始数据的形式发生了变化而已。 （2）寻找热门查询，300万个查询字符串中统计最热门的10个查询 原题：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1~255字节。假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门），请你统计最热门的10个查询串，要求使用的内存不能超过1G。 由上面那题可知，数据大则划分为小的，比如一亿个IP求Top10，可先%1000将IP分到1000个小文件中去，并保证一种IP只出现在一个文件中，再对每个小文件中的IP进行Hashmap计数统计并按数量排序，最后归并或者最小堆一次处理每个小文件的top10以得到最后的结。 但如果数据规模比较小，能一次性装入内存呢？比如这第二题，虽然有一千万个Query，但是由于重复度比较高，因此事实上只有300万的Query，每个Query255Byte，因此我们可以考虑把它们都放进内存中去（300万个字符串假设没有重复，都是最大长度，那么最多占用内存3M*1K/4=0.75G。所以可以将所有字符串都存放在内存中进行处理），而现在只需要一个合适的数据结构，在这里，HashTable绝对是我们优先的选择。 所以我们放弃分而治之、Hash映射的步骤，直接上Hash统计，然后排序。所以，针对此类典型的Top K问题，采取的对策往往是：Hashmap+堆。如下所示： Hash_map统计：先对这批海量数据预处理。具体方法是：维护一个Key为Query字符串，Value为该QUery出现次数的HashTable，即Hash_map(Query,Value)，每次读取一个Query，如果该字符串不在table内，则加入该字串，并且将Value值设为1；如果该字串在Table中，那么将该字串的计数加1即可。最终我们在O(N)的时间复杂度内用Hash表完成了统计。 堆排序：借助堆这个数据结构，找出Top K，时间复杂度为NlogK。即借助堆结构，我们可以在log量级的时间内查找和调整。因此，维护一个K（该题目中是10）大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。所以，我们最终的时间复杂度是$O(N)+N·O(logK)$，（N为1000万，N’是300万） 这篇文章中所述的堆排序思路：“维护k个元素的最小堆，即用容量为K的最小堆存储最先遍历到的k个数，并假设它们即是最大的k个数，建堆费时$O(k)$，并调整堆，费时$O(logk)$，这样我们就有$k_1&gt;k_2&gt;…k_{min}$（kmin设为小顶堆中最小元素）。继续遍历数列，每次遍历一个元素x，与堆顶元素比较，若$x&gt;k_{min}$，则更新堆（用时$log_k$），否则不更新堆。这样下来，总费时$O（klogk+（n-k）logk）=O（n*logk）$。此方法得益于在堆中，查找等各项操作时间复杂度均为logk。” 当然，你也可以采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。 （3）有一个1G大小的文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小时1M。返回频数最高的100个词。 由上面那两个例题，分而治之 + hash统计 + 堆/快速排序这个套路，我们已经开始有了屡试不爽的感觉。下面，再拿几道再多多验证下。请看此第3题：又是文件很大，又是内存受限，咋办?还能怎么办呢?无非还是： 分而治之/hash映射：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。 hash统计：对每个小文件，采用trie树、hash——map等统计每个文件中出现的词以及相应的频率。 堆、归并排序：取出出现频率最大的100个词（可以用含100个结点的最小堆），并把100个词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并（类似于归并排序）的过程了。 （4）海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。 如果每个数据元素只出现一次，而且只出现在某一台机器中，那么可以采取以下步骤统计出现次数TOP10的数据元素： 堆排序：在每台电脑上求出TOP10，可以采用包含10个元素的堆完成（TOP10小，用最大堆，TOP10大，用最小堆，比如求TOP10大，我们首先取前10个元素调整成最小堆，如果发现，然后扫描后面的数据，并与堆顶元素比较，如果比堆顶元素大，那么用该元素替换堆顶，然后再调整为最小堆。最后堆中的元素就是TOP10大）。 求出每台电脑上的TOP10后，然后把这100台电脑上的TOP10组合起来，共1000个数据，再利用上面类似的方法求出TOP10就可以了。 但如果同一个元素重复出现在不同的电脑中呢，如下例子所述：就拿2台机器求top2的情况来说，第一台：a(50)、b(50)、c(49)、d(49)、e(0)、e(0)，第二台：a(0)、b(0)、c(49)、d(49)、e(50)、f(50)，这个时候，你可以有两种方法： 遍历一遍所有数据，重新hash取摸，如此使得同一个元素只出现在单独的一台电脑中，然后采用上面所说的方法，统计每台电脑中各个元素的出现次数找出TOP10，继而组合100台电脑上的TOP10，找出最终的TOP10。 暴力求解：直接统计统计每台电脑中各个元素的出现次数，然后把同一个元素在不同机器中的出现次数相加，最终从所有数据中找出TOP10。 （5）有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。 hash映射：顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件（记为）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。 hash统计：找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。注：hash_map(query,query_count)是用来统计每个query的出现次数，不是存储他们的值，出现一次，则count+1。 堆/快速/归并排序：利用快速/堆/归并排序按照出现次数进行排序。将排序好的query和对应的query_cout输出到文件中。这样得到了10个排好序的文件。对这10个文件进行归并排序（内排序与外排序相结合）。 除此之外，此题还有以下两个方法： 一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。 与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。 （6）给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？ 可以估计每个文件的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。 分而治之/hash映射：遍历文件a，对每个url求取，然后根据所取得的值将url分别存储到1000个小文件（记为$a_0,a_1…..,a_{999}$）中。这样每个小文件的大约为300M。遍历文件b，采取和a相同的方式将url分别存储到1000小文件中（记为$b_0,b_1……,b_{999}$）。这样处理后，所有可能相同的url都在对应的小文件（$a_0vsb_0,a_1vsb_1……a_{999}vsb_{999}$）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。 hash_set统计：求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。 （7）怎么在海量数据中找出重复次数最多的一个？ 先做hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求（具体参考前面的题）。 （8）上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据。 上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用hash_map/搜索二叉树/红黑树等来进行统计次数。然后利用堆取出前N个出现次数最多的数据。 （9）一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。 方案1：如果文件比较大，无法一次性读入内存，可以采用hash取模的方法，将大文件分解为多个小文件，对于单个小文件利用hash_map统计出每个小文件中10个最常出现的词，然后再进行归并处理，找出最终的10个最常出现的词。 方案2：通过hash取模将大文件分解为多个小文件后，除了可以用hash_map统计出每个小文件中10个最常出现的词，也可以用trie树统计每个词出现的次数，时间复杂度是$O(nle)$（le表示单词的平准长度），最终同样找出出现最频繁的前10个词（可用堆来实现），时间复杂度是$O(nlg10)$。 密钥二：双层桶划分双层桶划分，其实本质上还是分而治之的思想，重在“分”的技巧上！ 适用范围：第k大，中位数，不重复或重复的数字 基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。可以通过多次缩小，双层只是一个例子。 （1）2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。 有点像鸽巢原理，整数个数为$2^{32}$,也就是，我们可以将这$2^{32}$个数，划分为$2^8$个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用bitmap就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。 （2）5亿个int找它们的中位数。 这个例子比上面那个更明显。首先我们将int划分为$2^{16}$个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。 实际上，如果不是int是int64，我们可以经过3次这样的划分即可降低到可以接受的程度。即可以先将int64分成$2^{24}$个区域，然后确定区域的第几大数，在将该区域分成$2^{20}$个子区域，然后确定是子区域的第几大数，然后子区域里的数的个数只有$2^{20}$，就可以直接利用direct addr table进行统计了。 密钥三：Bloom filter/Bitmap Bloom filter 海量数据处理之Bloom Filter详解 适用范围：可以用来实现数据字典，进行数据的判重，或者集合求交集 基本原理及要点： 对于原理来说很简单，位数组+k个独立hash函数。将hash函数对应的值的位数组置1，查找时如果发现所有hash函数对应位都是1说明存在，很明显这个过程并不保证查找的结果是100%正确的。同时也不支持删除一个已经插入的关键字，因为该关键字对应的位会牵动到其他的关键字。所以一个简单的改进就是 counting Bloom filter，用一个counter数组代替位数组，就可以支持删除了。 还有一个比较重要的问题，如何根据输入元素个数n，确定位数组m的大小及hash函数个数。当hash函数个数$k=(ln2)(m/n)$时错误率最小。在错误率不大于E的情况下，m至少要等于$nlg(1/E)$才能表示任意n个元素的集合。但m还应该更大些，因为还要保证bit数组里至少一半为0，则$m&gt;=nlg(1/E)*lge$ 大概就是$nlg(1/E)1.44$倍(lg表示以2为底的对数)。 举个例子我们假设错误率为0.01，则此时m应大概是n的13倍。这样k大概是8个。 注意这里m与n的单位不同，m是bit为单位，而n则是以元素个数为单位(准确的说是不同元素的个数)。通常单个元素的长度都是有很多bit的。所以使用bloom filter内存上通常都是节省的。 扩展：Bloom filter将集合中的元素映射到位数组中，用k（k为哈希函数个数）个映射位是否全1表示元素在不在这个集合中。Counting bloom filter（CBF）将位数组中的每一位扩展为一个counter，从而支持了元素的删除操作。Spectral Bloom Filter（SBF）将其与集合元素的出现次数关联。SBF采用counter中的最小值来近似表示元素的出现频率。 问题实例： 给你A,B两个文件，各存放50亿条URL，每条URL占用64字节，内存限制是4G，让你找出A,B文件共同的URL。如果是三个乃至n个文件呢？ 根据这个问题我们来计算下内存的占用，$4G=2^{32}$大概是40亿*8大概是340亿，n=50亿，如果按出错率0.01算需要的大概是650亿个bit。现在可用的是340亿，相差并不多，这样可能会使出错率上升些。另外如果这些urlip是一一对应的，就可以转换成ip，则大大简单了。 同时，上文的第5题：给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。 Bitmap介绍 Bit-map详解 所谓的Bit-map就是用一个bit位来标记某个元素对应的Value， 而Key即是该元素。由于采用了Bit为单位来存储数据，因此在存储空间方面，可以大大节省。 如果说了这么多还没明白什么是Bit-map，那么我们来看一个具体的例子，假设我们要对0-7内的5个元素(4,7,2,5,3)排序（这里假设这些元素没有重复）。那么我们就可以采用Bit-map的方法来达到排序的目的。要表示8个数，我们就只需要8个Bit（1Bytes），首先我们开辟1Byte的空间，将这些空间的所有Bit位都置为0，如下图： 然后遍历这5个元素，首先第一个元素是4，那么就把4对应的位置为1（可以这样操作 p+(i/8)|(0×01&lt;&lt;(i%8)) 当然了这里的操作涉及到Big-ending和Little-ending的情况，这里默认为Big-ending）,因为是从零开始的，所以要把第五位置为一，如下图： 然后再处理第二个元素7，将第八位置为1,，接着再处理第三个元素，一直到最后处理完所有的元素，将相应的位置为1，这时候的内存的Bit位的状态如下： 然后我们现在遍历一遍Bit区域，将该位是一的位的编号输出（2，3，4，5，7），这样就达到了排序的目的。下面的代码给出了一个BitMap的用法：排序。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//定义每个Byte中有8个Bit位 #include ＜memory.h＞ #define BYTESIZE 8 void SetBit(char *p, int posi) &#123; for(int i=0; i ＜ (posi/BYTESIZE); i++) &#123; p++; &#125; *p = *p|(0x01＜＜(posi%BYTESIZE));//将该Bit位赋值1 return; &#125; void BitMapSortDemo() &#123; //为了简单起见，我们不考虑负数 int num[] = &#123;3,5,2,10,6,12,8,14,9&#125;; //BufferLen这个值是根据待排序的数据中最大值确定的 //待排序中的最大值是14，因此只需要2个Bytes(16个Bit) //就可以了。 const int BufferLen = 2; char *pBuffer = new char[BufferLen]; //要将所有的Bit位置为0，否则结果不可预知。 memset(pBuffer,0,BufferLen); for(int i=0;i＜9;i++) &#123; //首先将相应Bit位上置为1 SetBit(pBuffer,num[i]); &#125; //输出排序结果 for(int i=0;i＜BufferLen;i++)//每次处理一个字节(Byte) &#123; for(int j=0;j＜BYTESIZE;j++)//处理该字节中的每个Bit位 &#123; //判断该位上是否是1，进行输出，这里的判断比较笨。 //首先得到该第j位的掩码（0x01＜＜j），将内存区中的 //位和此掩码作与操作。最后判断掩码是否和处理后的 //结果相同 if((*pBuffer&amp;(0x01＜＜j)) == (0x01＜＜j)) &#123; printf(&quot;%d &quot;,i*BYTESIZE + j); &#125; &#125; pBuffer++; &#125; &#125; int _tmain(int argc, _TCHAR* argv[]) &#123; BitMapSortDemo(); return 0; &#125; 可进行数据的快速查找，判重，删除，一般来说数据范围是int的10倍以下。问题实例 在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。 方案1：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。 方案2：也可采用与第1题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。” 给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？ 方案1：用位图/Bitmap的方法，申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。 密匙四：Trie树/数据库/倒排索引 Trie树 适用范围：数据量大，重复多，但是数据种类小，可以放入内存 基本原理及要点：实现方式，节点孩子的表示方式 扩展：压缩实现。 问题实例 密钥一（2）：寻找热门查询：查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个，每个不超过255字节。 密钥一（5）：有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。要你按照query的频度排序。 密钥一（8）：一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词。其解决方法是：用trie树统计每个词出现的次数，时间复杂度是O(n*le)（le表示单词的平准长度），然后是找出出现最频繁的前10个词。 1000万字符串，其中有些是相同的(重复),需要把重复的全部去掉，保留没有重复的字符串。请问怎么设计和实现？ 更多有关Trie树的介绍，请参见此文：从Trie树（字典树）谈到后缀树 数据库索引 适用范围：大数据量的增删改查 基本原理及要点：利用数据的设计实现方法，对海量数据的增删改查进行处理。 关于数据库索引及其优化，更多可参见此文：海量数据处理专题（七）——数据库索引及优化。同时，关于MySQL索引背后的数据结构及算法原理，这里还有一篇很好的文章：MySQL索引背后的数据结构及算法原理。 倒排索引(Inverted index) 适用范围：搜索引擎，关键字查询 基本原理及要点：为何叫倒排索引？一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。 以英文为例，下面是要被索引的文本： 123T0 = “it is what it is” T1 = “what is it” T2 = “it is a banana” 我们就能得到下面的反向文件索引： 12345“a”: &#123;2&#125; “banana”: &#123;2&#125; “is”: &#123;0, 1, 2&#125; “it”: &#123;0, 1, 2&#125; “what”: &#123;0, 1&#125; 检索的条件”what”,”is”和”it”将对应集合的交集。 正向索引开发出来用来存储每个文档的单词的列表。正向索引的查询往往满足每个文档有序频繁的全文查询和每个单词在校验文档中的验证这样的查询。在正向索引中，文档占据了中心的位置，每个文档指向了一个它所包含的索引项的序列。也就是说文档指向了它包含的那些单词，而反向索引则是单词指向了包含它的文档，很容易看到这个反向的关系。 问题实例：文档检索系统，查询那些文件包含了某单词，比如常见的学术论文的关键字搜索。 密匙五：外排序适用范围：大数据的排序，去重基本原理及要点：外排序的归并方法，置换选择败者树原理，最优归并树 （1）有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16个字节，内存限制大小是1M。返回频数最高的100个词。 这个数据具有很明显的特点，词的大小为16个字节，但是内存只有1m做hash有些不够，所以可以用来排序。内存可以当输入缓冲区使用。 关于多路归并算法及外排序的具体应用场景，请参见：如何给10000000个数据量的磁盘文件排序。 密匙六：分布式处理之MapreduceMapReduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。这样做的好处是可以在任务被分解后，可以通过大量机器进行并行计算，减少整个操作的时间。但如果你要我再通俗点介绍，那么，说白了，Mapreduce的原理就是一个归并排序。 适用范围：数据量大，但是数据种类小可以放入内存 基本原理及要点：将数据交给不同的机器去处理，数据划分，结果归约。 问题实例 The canonical example application of MapReduce is a process to count the appearances of each different word in a set of documents: 海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。 一共有N个机器，每个机器上有N个数。每个机器最多存O(N)个数并对它们操作。如何找到$N^2$个数的中位数(median)？ 更多具体阐述请参见：从Hadhoop框架与MapReduce模式中谈海量数据处理 MapReduce技术的初步了解与学习 四、海量数据处理题库搜集 如何在海量数据中查找给定部分数据最相似的top200向量，向量的维度也很高，因为之前了解过其他面蚂蚁金服的朋友，也有问到这个题目的，所以反应比较快，直接就说可以用KD树，聚类，hash， 如何在海量数据中查找给定部分数据最相似的top200向量，向量的维度也很高，实现一个分布式的topN算法 最后是一个海量数据处理题，给了个滴滴打车的背景，实质就是如何在海量数据中找到最大值。我按着分治思想说了一个解法，意思到了，他也就没往深处问。 要求手写海量数据topK的问题，手写个最小堆，之前没自己写过，幸亏ＴＴＦ同学临时指导了一下，都准备好了也没面到 海量数据问题，给定１０亿个数，统计出现次数最多的１００个数，如果把数换成字符串呢 给你1000个数，怎样随机抽取10个数 之后就是大数据题目，1KW句子算相似度（还是那套分块+hash/建索引，但是因为本人不是做这个的，文本处理根本说一片空白，所以就不误导大家了），之后就是一直围绕大数据的题目不断深化。 怎么在2G内存里找100TB数据的中位数 从大数据中找出topk 对大小在1-10000的1亿个数进行排序，你会怎么做？（友情提示，不要用基于比较的排序算法哦） 怎么在2G内存里找100TB数据的中位数， 10亿个整数，1G内存，O(n)算法，统计只出现一次的数。 N个数找K大数那个题,堆解释了一遍,比较满意,问还能怎么优化O(Nlogk)的方法，并行方面想 给$10^{10}$个64位数,100M内存的空间排序,感谢队长刚好在去的前一天教过我一个求中位数的方法.用文件操作来做了,像快排一样,二分选个数统计大于那个数的数量和小于那个数的数量,如果能用100M的空间排序就把那些数排了,如果不能继续.直到能排为止. 第k大之类的套路题 如何从很多的query中找出一个query （我开始想到hash，后来经提示我想到了前缀树）若允许有错误，可以再怎么解决（不知道，面试官提示了布隆过滤器） 百度二面那个query找相似的系统设计题，我先后说了前缀树，kd树等解决方案，虽然不对，但面试至少知道我了解这些东西]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（18）：倒排索引]]></title>
    <url>%2F2017%2F08%2F01%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8818%EF%BC%89%EF%BC%9A%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[一、倒排索引倒排索引（inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或一组文档中存储位置的映射。它是文档检索系统中最常用的数据结构。 有两种不同的倒排索引形式： 一条记录的水平反向索引（或者反向档案索引）包含每个引用单词的文档的列表。 一个单词的水平反向索引（或者完全反向索引）又包含每个单词在一个文档中的位置。 后者的形式提供了更多的兼容性（比如短语搜索），但是需要更多的时间和空间来创建。 以英文为例，下面是要被索引的文本： $T_0$=”it is what it is” $T_1$=”what is it” $T_2$=”it is a banana” 我们就能得到下面的反向文件索引： “a”: {2} “banana”: {2} “is”: {0, 1, 2} “it”: {0, 1, 2} “what”: {0, 1} 检索的条件”what”, “is” 和 “it” 将对应这个集合：${\displaystyle \{0,1\}\cap \{0,1,2\}\cap \{0,1,2\}=\{0,1}$ 对相同的文字，我们得到后面这些完全反向索引，有文档数量和当前查询的单词结果组成的的成对数据。 同样，文档数量和当前查询的单词结果都从零开始。所以，”banana”: {(2, 3)} 就是说 “banana”在第三个文档里 $T_{2}$，而且在第三个文档的位置是第四个单词(地址为 3)。 “a”: {(2, 2)} “banana”: {(2, 3)} “is”: {(0, 1), (0, 4), (1, 1), (2, 1)} “it”: {(0, 0), (0, 3), (1, 2), (2, 0)} “what”: {(0, 2), (1, 0)} 如果我们执行短语搜索”what is it” 我们得到这个短语的全部单词各自的结果所在文档为文档0和文档1。但是这个短语检索的连续的条件仅仅在文档1得到。 有了这个索引系统，搜索引擎可以很方便地响应用户的查询，比如用户输入查询词“banana”，搜索系统查找倒排索引，从中可以读出包含这个单词的文档，这些文档就是提供给用户的搜索结果，而利用单词频率信息、文档频率信息即可以对这些候选搜索结果进行排序，计算文档和查询的相似性，按照相似性得分由高到低排序输出，此即为搜索系统的部分内部流程。 二、单词词典单词词典是倒排索引中非常重要的组成部分，它用来维护文档集合中出现过的所有单词的相关信息，同时用来记载某个单词对应的倒排列表在倒排文件中的位置信息。在支持搜索时，根据用户的查询词，去单词词典里查询，就能够获得相应的倒排列表，并以此作为后续排序的基础。 对于一个规模很大的文档集合来说，可能包含几十万甚至上百万的不同单词，能否快速定位某个单词，这直接影响搜索时的响应速度，所以需要高效的数据结构来对单词词典进行构建和查找，常用的数据结构包括哈希加链表结构和树形词典结构。 2.1 哈希加链表 这种词典结构主要由两个部分构成： 主体部分是哈希每个哈希表项保存一个指针，指针指向冲突链表，在冲突链表里，相同哈希值的单词形成链表结构。之所以会有冲突链表，是因为两个不同单词获得相同的哈希值，如果是这样，在哈希方法里被称做是一次冲突，可以将相同哈希值的单词存储在链表里，以供后续查找。 在建立索引的过程中，词典结构也会相应地被构建出来。比如在解析一个新文档的时候，对于某个在文档中出现的单词T，首先利用哈希函数获得其哈希值，之后根据哈希值对应的哈希表项读取其中保存的指针，就找到了对应的冲突链表。如果冲突链表里已经存在这个单词，说明单词在之前解析的文档里已经出现过。如果在冲突链表里没有发现这个单词，说明该单词是首次碰到，则将其加入冲突链表里。通过这种方式，当文档集合内所有文档解析完毕时，相应的词典结构也就建立起来了。 在响应用户查询请求时，其过程与建立词典类似，不同点在于即使词典里没出现过某个单词，也不会添加到词典内。以图1-7为例，假设用户输入的查询请求为单词3，对这个单词进行哈希，定位到哈希表内的2号槽，从其保留的指针可以获得冲突链表，依次将单词3和冲突链表内的单词比较，发现单词3在冲突链表内，于是找到这个单词，之后可以读出这个单词对应的倒排列表来进行后续的工作，如果没有找到这个单词，说明文档集合内没有任何文档包含单词，则搜索结果为空。 2.2 树形结构B树（或者B+树）是另外一种高效查找结构，图1-8是一个 B树结构示意图。B树与哈希方式查找不同，需要字典项能够按照大小排序（数字或者字符序），而哈希方式则无须数据满足此项要求。 B树形成了层级查找结构，中间节点用于指出一定顺序范围的词典项目存储在哪个子树中，起到根据词典项比较大小进行导航的作用，最底层的叶子节点存储单词的地址信息，根据这个地址就可以提取出单词字符串。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>倒排索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（17）：simhash]]></title>
    <url>%2F2017%2F08%2F01%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8817%EF%BC%89%EF%BC%9Asimhash%2F</url>
    <content type="text"><![CDATA[一、引入随着信息爆炸时代的来临，互联网上充斥着着大量的近重复信息，有效地识别它们是一个很有意义的课题。例如，对于搜索引擎的爬虫系统来说，收录重复的网页是毫无意义的，只会造成存储和计算资源的浪费；同时，展示重复的信息对于用户来说也并不是最好的体验。造成网页近重复的可能原因主要包括： 镜像网站 内容复制 嵌入广告 计数改变 少量修改 一个简化的爬虫系统架构如下图所示： 事实上，传统比较两个文本相似性的方法，大多是将文本分词之后，转化为特征向量距离的度量，比如常见的欧氏距离、海明距离或者余弦角度等等。两两比较固然能很好地适应，但这种方法的一个最大的缺点就是，无法将其扩展到海量数据。例如，试想像Google那种收录了数以几十亿互联网信息的大型搜索引擎，每天都会通过爬虫的方式为自己的索引库新增的数百万网页，如果待收录每一条数据都去和网页库里面的每条记录算一下余弦角度，其计算量是相当恐怖的。 我们考虑采用为每一个web文档通过hash的方式生成一个指纹（fingerprint）。传统的加密式hash，比如md5，其设计的目的是为了让整个分布尽可能地均匀，输入内容哪怕只有轻微变化，hash就会发生很大地变化。我们理想当中的哈希函数，需要对几乎相同的输入内容，产生相同或者相近的hashcode，换句话说，hashcode的相似程度要能直接反映输入内容的相似程度。很明显，前面所说的md5等传统hash无法满足我们的需求。 二、simhash的原理simhash是locality sensitive hash（局部敏感哈希）的一种，最早由Moses Charikar在《similarity estimation techniques from rounding algorithms》一文中提出。Google就是基于此算法实现网页文件查重的。simhash算法的主要思想是降维，将高维的特征向量映射成一个f-bit的指纹(fingerprint)，通过比较两篇文章的f-bit指纹的Hamming Distance来确定文章是否重复或者高度近似。我们假设有以下三段文本： the cat sat on the mat the cat sat on a mat we all scream for ice cream 使用传统hash可能会产生如下的结果： 123456789irb(main):006:0&gt; p1 = &apos;the cat sat on the mat&apos; irb(main):005:0&gt; p2 = &apos;the cat sat on a mat&apos; irb(main):007:0&gt; p3 = &apos;we all scream for ice cream&apos; irb(main):007:0&gt; p1.hash =&gt; 415542861 irb(main):007:0&gt; p2.hash =&gt; 668720516 irb(main):007:0&gt; p3.hash =&gt; 767429688 使用simhash会应该产生类似如下的结果: 123456789irb(main):003:0&gt; p1.simhash =&gt; 851459198 00110010110000000011110001111110 irb(main):004:0&gt; p2.simhash =&gt; 847263864 00110010100000000011100001111000 irb(main):002:0&gt; p3.simhash =&gt; 984968088 00111010101101010110101110011000 海明距离的定义，为两个二进制串中不同位的数量。上述三个文本的simhash结果，其两两之间的海明距离为(p1,p2)=4，(p1,p3)=16以及(p2,p3)=12。事实上，这正好符合文本之间的相似度，p1和p2间的相似度要远大于与p3的。 如何实现这种hash算法呢？图解如下： 算法过程大概如下： 将Doc进行关键词抽取(其中包括分词和计算权重)，抽取出n个(关键词，权重)对， 即图中的(feature, weight)们。 记为 feature_weight_pairs = [fw1, fw2 … fwn]，其中 fwn = (feature_n,weight_n)。 hash_weight_pairs = [ (hash(feature), weight) for feature, weight in feature_weight_pairs ]生成图中的(hash,weight)们, 此时假设hash生成的位数bits_count = 6（如图）; 然后对hash_weight_pairs进行位的纵向累加，如果该位是1，则+weight,如果是0，则-weight，最后生成bits_count个数字，如图所示是[13, 108, -22, -5, -32, 55], 这里产生的值和hash函数所用的算法相关。 [13,108,-22,-5,-32,55] -&gt; 110001这个就很简单啦，正1负0。 三、海明距离当我们算出所有doc的simhash值之后，需要计算doc A和doc B之间是否相似的条件是：A和B的海明距离是否小于等于n，这个n值根据经验一般取值为3, 那海明距离怎么计算呢？二进制串A 和 二进制串B 的海明距离 就是 A xor B 后二进制中1的个数。 1234举例如下：A = 100111;B = 101010;hamming_distance(A, B) = count_1(A xor B) = count_1(001101) = 3; simhash本质上是局部敏感性的hash，和md5之类的不一样。 正因为它的局部敏感性，所以我们可以使用海明距离来衡量simhash值的相似度。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>哈希</tag>
        <tag>simhash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（16）：一致性哈希]]></title>
    <url>%2F2017%2F07%2F30%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8816%EF%BC%89%EF%BC%9A%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%2F</url>
    <content type="text"><![CDATA[一致性哈希算法在1997年由麻省理工学院提出的一种分布式哈希（DHT）实现算法，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简单哈希算法带来的问题，使得分布式哈希（DHT）可以在P2P环境中真正得到应用。 一、Hash算法一致性hash算法提出了在动态变化的Cache环境中，判定哈希算法好坏的四个定义： 平衡性(Balance)：平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。 单调性(Monotonicity)：单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应能够保证原有已分配的内容可以被映射到原有的或者新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。 分散性(Spread)：在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。 负载(Load)：负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同 的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。 假设一个简单的场景：有4个cache服务器（后简称cache）组成的集群，当一个对象object传入集群时，这个对象应该存储在哪一个cache里呢？一种简单的方法是使用映射公式： 1Hash(object) % 4 这个算法就可以保证任何object都会尽可能随机落在其中一个cache中。一切运行正常。 然后考虑以下情况： 由于流量增大，需要增加一台cache，共5个cache。这时，映射公式就变成Hash(object) % 5。有一个cache服务器down掉，变成3个cache。这时，映射公式就变成Hash(object) % 3。可见，无论新增还是减少节点，都会改变映射公式，而由于映射公式改变，几乎所有的object都会被映射到新的cache中，这意味着一时间所有的缓存全部失效。 大量的数据请求落在app层甚至是db层上，这样严重的违反了单调性原则,这对服务器的影响当然是灾难性的。 接下来主要讲解一下一致性哈希算法是如何设计的： 二、一致性Hash算法2.1 环形Hash空间按照常用的hash算法来将对应的key哈希到一个具有2^32次方个桶的空间中，即$0至 (2^{32})-1$的数字空间中。现在我们可以将这些数字头尾相连，想象成一个闭合的环形。如下图 2.2 数据映射现在我们将object1、object2、object3、object4四个对象通过特定的Hash函数计算出对应的key值，然后散列到Hash环上。如下图： 1234Hash(object1) = key1；Hash(object2) = key2；Hash(object3) = key3；Hash(object4) = key4； 2.3 机器映射在采用一致性哈希算法的分布式集群中将新的机器加入，其原理是通过使用与对象存储一样的Hash算法将机器也映射到环中（一般情况下对机器的hash计算是采用机器的IP或者机器唯一的别名作为输入值），然后以顺时针的方向计算，将所有对象存储到离自己最近的机器中。 假设现在有NODE1，NODE2，NODE3三台机器，通过Hash算法得到对应的KEY值，映射到环中，其示意图如下： 123Hash(NODE1) = KEY1;Hash(NODE2) = KEY2;Hash(NODE3) = KEY3; 通过上图可以看出对象与机器处于同一哈希空间中，这样按顺时针转动object1存储到了NODE1中，object3存储到了NODE2中，object2、object4存储到了NODE3中。在这样的部署环境中，hash环是不会变更的，因此，通过算出对象的hash值就能快速的定位到对应的机器中，这样就能找到对象真正的存储位置了。 2.4 机器的删除与添加普通hash求余算法最为不妥的地方就是在有机器的添加或者删除之后会照成大量的对象存储位置失效，这样就大大的不满足单调性了。下面来分析一下一致性哈希算法是如何处理的。 2.4.1 节点（机器）的删除以上面的分布为例，如果NODE2出现故障被删除了，那么按照顺时针迁移的方法，object3将会被迁移到NODE3中，这样仅仅是object3的映射位置发生了变化，其它的对象没有任何的改动。如下图： 2.4.2 节点（机器）的添加如果往集群中添加一个新的节点NODE4，通过对应的哈希算法得到KEY4，并映射到环中，如下图： 通过按顺时针迁移的规则，那么object2被迁移到了NODE4中，其它对象还保持这原有的存储位置。通过对节点的添加和删除的分析，一致性哈希算法在保持了单调性的同时，还是数据的迁移达到了最小，这样的算法对分布式集群来说是非常合适的，避免了大量数据迁移，减小了服务器的的压力。 2.5 平衡性根据上面的图解分析，一致性哈希算法满足了单调性和负载均衡的特性以及一般hash算法的分散性，但这还并不能当做其被广泛应用的原由，因为还缺少了平衡性。下面将分析一致性哈希算法是如何满足平衡性的。hash算法是不保证平衡的，如上面只部署了NODE1和NODE3的情况（NODE2被删除的图），object1存储到了NODE1中，而object2、object3、object4都存储到了NODE3中，这样就照成了非常不平衡的状态。 2.6 虚拟节点其实，理论上，只要cache足够多，每个cache在圆环上就会足够分散。但是在真实场景里，cache服务器只会有很少，所以，在一致性哈希算法中，为了尽可能的满足平衡性，其引入了虚拟节点的概念。 “虚拟节点”（ virtual node ）是实际节点（机器）在 hash 空间的复制品（ replica ），一实际个节点（机器）对应了若干个“虚拟节点”，这个对应个数也成为“复制个数”，“虚拟节点”在 hash 空间中以hash值排列。 以上面只部署了NODE1和NODE3的情况（NODE2被删除的图）为例，之前的对象在机器上的分布很不均衡，现在我们以2个副本（复制个数）为例，这样整个hash环中就存在了4个虚拟节点，最后对象映射的关系图如下： 根据上图可知对象的映射关系： 1object1-&gt;NODE1-1，object2-&gt;NODE1-2，object3-&gt;NODE3-2，object4-&gt;NODE3-1 通过虚拟节点的引入，对象的分布就比较均衡了。那么在实际操作中，正真的对象查询是如何工作的呢？对象从hash到虚拟节点到实际节点的转换如下图： “虚拟节点”的hash计算可以采用对应节点的IP地址加数字后缀的方式。例如假设NODE1的IP地址为192.168.1.100。引入“虚拟节点”前，计算 cache A 的 hash 值： 1Hash(“192.168.1.100”); 引入“虚拟节点”后，计算“虚拟节”点NODE1-1和NODE1-2的hash值： 12Hash(“192.168.1.100#1”); // NODE1-1Hash(“192.168.1.100#2”); // NODE1-2]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>哈希</tag>
        <tag>一致性哈希</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（15）：布隆过滤器]]></title>
    <url>%2F2017%2F07%2F29%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8815%EF%BC%89%EF%BC%9A%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%2F</url>
    <content type="text"><![CDATA[一、引入什么情况下需要布隆过滤器？我们先来看几个比较常见的例子： 字处理软件中，需要检查一个英语单词是否拼写正确 在 FBI，一个嫌疑人的名字是否已经在嫌疑名单上 在网络爬虫里，一个网址是否被访问过 yahoo, gmail等邮箱垃圾邮件过滤功能 这几个例子有一个共同的特点： 如何判断一个元素是否存在一个集合中？ 二、常规思路与局限如果想判断一个元素是不是在一个集合里，一般想到的是将集合中所有元素保存起来，然后通过比较确定。链表、树、散列表（又叫哈希表，Hash table）等等数据结构都是这种思路。但是随着集合中元素的增加，我们需要的存储空间越来越大。同时检索速度也越来越慢。 数组 链表 树、平衡二叉树、Trie Map (红黑树) 哈希表 虽然上面描述的这几种数据结构配合常见的排序、二分搜索可以快速高效的处理绝大部分判断元素是否存在集合中的需求。但是当集合里面的元素数量足够大，如果有500万条记录甚至1亿条记录呢？这个时候常规的数据结构的问题就凸显出来了。 数组、链表、树等数据结构会存储元素的内容，一旦数据量过大，消耗的内存也会呈现线性增长，最终达到瓶颈。 有的同学可能会问，哈希表不是效率很高吗？查询效率可以达到O(1)。但是哈希表需要消耗的内存依然很高。使用哈希表存储一亿 个垃圾 email 地址的消耗？哈希表的做法：首先，哈希函数将一个email地址映射成8字节信息指纹；考虑到哈希表存储效率通常小于50%（哈希冲突）；因此消耗的内存：8 2 1亿 字节 = 1.6G 内存，普通计算机是无法提供如此大的内存。这个时候，布隆过滤器（Bloom Filter）就应运而生。在继续介绍布隆过滤器的原理时，先讲解下关于哈希函数的预备知识。 三、哈希函数哈希函数的概念是：将任意大小的数据转换成特定大小的数据的函数，转换后的数据称为哈希值或哈希编码。 一个应用是Hash table（散列表，也叫哈希表），是根据哈希值 (Key value) 而直接进行访问的数据结构。也就是说，它通过把哈希值映射到表中一个位置来访问记录，以加快查找的速度。下面是一个典型的 hash 函数 / 表示意图： 可以明显的看到，原始数据经过哈希函数的映射后称为了一个个的哈希编码，数据得到压缩。哈希函数是实现哈希表和布隆过滤器的基础。 哈希函数有以下两个特点： 如果两个散列值是不相同的（根据同一函数），那么这两个散列值的原始输入也是不相同的。 散列函数的输入和输出不是唯一对应关系的，如果两个散列值相同，两个输入值很可能是相同的。但也可能不同，这种情况称为 “散列碰撞”（或者 “散列冲突”）。 缺点： 引用吴军博士的《数学之美》中所言，哈希表的空间效率还是不够高。如果用哈希表存储一亿个垃圾邮件地址，每个email地址 对应 8bytes, 而哈希表的存储效率一般只有50%，因此一个email地址需要占用16bytes. 因此一亿个email地址占用1.6GB，如果存储几十亿个email address则需要上百GB的内存。除非是超级计算机，一般的服务器是无法存储的。 所以要引入下面的 Bloom Filter。 四、布隆过滤器（Bloom Filter）布隆过滤器（英语：Bloom Filter）是1970年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 4.1 原理布隆过滤器（Bloom Filter）的核心实现是一个超大的位数组和几个哈希函数。假设位数组的长度为m，哈希函数的个数为k 以上图为例，具体的操作流程：假设集合里面有3个元素{x, y, z}，哈希函数的个数为3。首先将位数组进行初始化，将里面每个位都设置位0。 对于集合里面的每一个元素，将元素依次通过3个哈希函数进行映射，每次映射都会产生一个哈希值，这个值对应位数组上面的一个点，然后将位数组对应的位置标记为1。查询W元素是否存在集合中的时候，同样的方法将W通过哈希映射到位数组上的3个点。如果3个点的其中有一个点不为1，则可以判断该元素一定不存在集合中。反之，如果3个点都为1，则该元素可能存在集合中。 注意：此处不能判断该元素是否一定存在集合中，可能存在一定的误判率。可以从图中可以看到：假设某个元素通过映射对应下标为4，5，6这3个点。虽然这3个点都为1，但是很明显这3个点是不同元素经过哈希得到的位置，因此这种情况说明元素虽然不在集合中，也可能对应的都是1，这是误判率存在的原因。 4.2 添加与查询 布隆过滤器添加元素 将要添加的元素给k个哈希函数 得到对应于位数组上的k个位置 将这k个位置设为1 布隆过滤器查询元素 将要查询的元素给k个哈希函数 得到对应于位数组上的k个位置 如果k个位置有一个为0，则肯定不在集合中 如果k个位置全部为1，则可能在集合中 4.3 优点 It tells us that the element either definitely is not in the set or may be in the set. 相比于其它的数据结构，布隆过滤器在空间和时间方面都有巨大的优势。布隆过滤器存储空间和插入/查询时间都是常数（O(k)）。另外，散列函数相互之间没有关系，方便由硬件并行实现。布隆过滤器不需要存储元素本身，在某些对保密要求非常严格的场合有优势。 布隆过滤器可以表示全集，其它任何数据结构都不能； 4.4 缺点但是布隆过滤器的缺点和优点一样明显。误算率是其中之一。随着存入的元素数量增加，误算率随之增加。但是如果元素数量太少，则使用散列表足矣。 误判补救方法是：再建立一个小的白名单，存储那些可能被误判的信息。 另外，一般情况下不能从布隆过滤器中删除元素. 我们很容易想到把位数组变成整数数组，每插入一个元素相应的计数器加 1, 这样删除元素时将计数器减掉就可以了。然而要保证安全地删除元素并非如此简单。首先我们必须保证删除的元素的确在布隆过滤器里面. 这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。 4.5 实例可以快速且空间效率高的判断一个元素是否属于一个集合；用来实现数据字典，或者集合求交集。 Google chrome 浏览器使用bloom filter识别恶意链接（能够用较少的存储空间表示较大的数据集合，简单的想就是把每一个URL都可以映射成为一个bit） 又如： 检测垃圾邮件 假定我们存储一亿个电子邮件地址，我们先建立一个十六亿二进制（比特），即两亿字节的向量，然后将这十六亿个二进制全部设置为零。对于每一个电子邮件地址 X，我们用八个不同的随机数产生器（F1,F2, …,F8） 产生八个信息指纹（f1, f2, …, f8）。再用一个随机数产生器 G 把这八个信息指纹映射到 1 到十六亿中的八个自然数 g1, g2, …,g8。现在我们把这八个位置的二进制全部设置为一。当我们对这一亿个 email 地址都进行这样的处理后。一个针对这些 email 地址的布隆过滤器就建成了。 再如: A,B 两个文件，各存放 50 亿条 URL，每条 URL 占用 64 字节，内存限制是 4G，让你找出 A,B 文件共同的 URL。如果是三个乃至 n 个文件呢？ 分析 ：如果允许有一定的错误率，可以使用 Bloom filter，4G 内存大概可以表示 340 亿 bit。将其中一个文件中的 url 使用 Bloom filter 映射为这 340 亿 bit，然后挨个读取另外一个文件的 url，检查是否与 Bloom filter，如果是，那么该 url 应该是共同的 url（注意会有一定的错误率）。” 4.6 实现下面给出python的简单实现，使用murmurhash算法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import mmh3from bitarray import bitarray# zhihu_crawler.bloom_filter# Implement a simple bloom filter with murmurhash algorithm.# Bloom filter is used to check wether an element exists in a collection, and it has a good performance in big data situation.# It may has positive rate depend on hash functions and elements count.BIT_SIZE = 5000000class BloomFilter: def __init__(self): # Initialize bloom filter, set size and all bits to 0 bit_array = bitarray(BIT_SIZE) bit_array.setall(0) self.bit_array = bit_array def add(self, url): # Add a url, and set points in bitarray to 1 (Points count is equal to hash funcs count.) # Here use 7 hash functions. point_list = self.get_postions(url) for b in point_list: self.bit_array[b] = 1 def contains(self, url): # Check if a url is in a collection point_list = self.get_postions(url) result = True for b in point_list: result = result and self.bit_array[b] return result def get_postions(self, url): # Get points positions in bit vector. point1 = mmh3.hash(url, 41) % BIT_SIZE point2 = mmh3.hash(url, 42) % BIT_SIZE point3 = mmh3.hash(url, 43) % BIT_SIZE point4 = mmh3.hash(url, 44) % BIT_SIZE point5 = mmh3.hash(url, 45) % BIT_SIZE point6 = mmh3.hash(url, 46) % BIT_SIZE point7 = mmh3.hash(url, 47) % BIT_SIZE return [point1, point2, point3, point4, point5, point6, point7]]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>哈希</tag>
        <tag>布隆过滤器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（14）：最短路算法]]></title>
    <url>%2F2017%2F07%2F28%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8814%EF%BC%89%EF%BC%9A%E6%9C%80%E7%9F%AD%E8%B7%AF%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[最短路径问题是图论研究中的一个经典算法问题，旨在寻找图（由结点和路径组成的）中两结点之间的最短路径。算法具体的形式包括： 确定起点的最短路径问题 - 即已知起始结点，求最短路径的问题。适合使用Dijkstra算法。 确定终点的最短路径问题 - 与确定起点的问题相反，该问题是已知终结结点，求最短路径的问题。在无向图中该问题与确定起点的问题完全等同，在有向图中该问题等同于把所有路径方向反转的确定起点的问题。 确定起点终点的最短路径问题 - 即已知起点和终点，求两结点之间的最短路径。 全局最短路径问题 - 求图中所有的最短路径。适合使用Floyd-Warshall算法。 一、Dijkstra算法1.1 算法思想Dijkstra(迪杰斯特拉)算法是典型的单源最短路径算法，用于计算一个节点到其他所有节点的最短路径。主要特点是以起始点为中心向外层层扩展，直到扩展到终点为止。Dijkstra算法是很有代表性的最短路径算法，在很多专业课程中都作为基本内容有详细的介绍，如数据结构，图论，运筹学等等。注意该算法要求图中不存在负权边。 问题描述： 在无向图 G=(V,E) 中，假设每条边 E[i] 的长度为 w[i]，找到由顶点 V0 到其余各点的最短路径。（单源最短路径） 算法思想： 设G=(V,E)是一个带权有向图，把图中顶点集合V分成两组，第一组为已求出最短路径的顶点集合（用S表示，初始时S中只有一个源点，以后每求得一条最短路径 , 就将加入到集合S中，直到全部顶点都加入到S中，算法就结束了），第二组为其余未确定最短路径的顶点集合（用U表示），按最短路径长度的递增次序依次把第二组的顶点加入S中。在加入的过程中，总保持从源点v到S中各顶点的最短路径长度不大于从源点v到U中任何顶点的最短路径长度。此外，每个顶点对应一个距离，S中的顶点的距离就是从v到此顶点的最短路径长度，U中的顶点的距离，是从v到此顶点只包括S中的顶点为中间顶点的当前最短路径长度。 适用条件与限制 有向图和无向图都可以使用本算法，无向图中的每条边可以看成相反的两条边。 用来求最短路的图中不能存在负权边。(可以利用拓扑排序检测) 1.2 算法步骤 初始时，S只包含源点，即S＝{v}，v的距离为0。U包含除v外的其他顶点，即:U={其余顶点}，若v与U中顶点u有边，则正常有权值，若u不是v的出边邻接点，则权值为∞。 从U中选取一个距离v最小的顶点k，把k，加入S中（该选定的距离就是v到k的最短路径长度）。 以k为新考虑的中间点，修改U中各顶点的距离；若从源点v到顶点u的距离（经过顶点k）比原来距离（不经过顶点k）短，则修改顶点u的距离值，修改后的距离值的顶点k的距离加上边上的权。 重复步骤b和c直到所有顶点都包含在S中。 步骤动画如下： 实例如下 用Dijkstra算法找出以A为起点的单源最短路径步骤如下 1.3 代码实现以”邻接矩阵”为例对迪杰斯特拉算法进行说明。 123456789public class MatrixUDG &#123; private int mEdgNum; // 边的数量 private char[] mVexs; // 顶点集合 private int[][] mMatrix; // 邻接矩阵 private static final int INF = Integer.MAX_VALUE; // 最大值 ...&#125; MatrixUDG是邻接矩阵对应的结构体。mVexs用于保存顶点，mEdgNum用于保存边数，mMatrix则是用于保存矩阵信息的二维数组。例如，mMatrix[i][j]=1，则表示”顶点i(即mVexs[i])”和”顶点j(即mVexs[j])”是邻接点；mMatrix[i][j]=0，则表示它们不是邻接点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/* * Dijkstra最短路径。 * 即，统计图中"顶点vs"到其它各个顶点的最短路径。 * * 参数说明： * vs -- 起始顶点(start vertex)。即计算"顶点vs"到其它顶点的最短路径。 * prev -- 前驱顶点数组。即，prev[i]的值是"顶点vs"到"顶点i"的最短路径所经历的全部顶点中，位于"顶点i"之前的那个顶点。 * dist -- 长度数组。即，dist[i]是"顶点vs"到"顶点i"的最短路径的长度。 */public void dijkstra(int vs, int[] prev, int[] dist) &#123; // flag[i]=true表示"顶点vs"到"顶点i"的最短路径已成功获取 boolean[] flag = new boolean[mVexs.length]; // 初始化 for (int i = 0; i &lt; mVexs.length; i++) &#123; flag[i] = false; // 顶点i的最短路径还没获取到。 prev[i] = 0; // 顶点i的前驱顶点为0。 dist[i] = mMatrix[vs][i]; // 顶点i的最短路径为"顶点vs"到"顶点i"的权。 &#125; // 对"顶点vs"自身进行初始化 flag[vs] = true; dist[vs] = 0; // 遍历mVexs.length-1次；每次找出一个顶点的最短路径。 int k=0; for (int i = 1; i &lt; mVexs.length; i++) &#123; // 寻找当前最小的路径； // 即，在未获取最短路径的顶点中，找到离vs最近的顶点(k)。 int min = INF; for (int j = 0; j &lt; mVexs.length; j++) &#123; if (flag[j]==false &amp;&amp; dist[j]&lt;min) &#123; min = dist[j]; k = j; &#125; &#125; // 标记"顶点k"为已经获取到最短路径 flag[k] = true; // 修正当前最短路径和前驱顶点 // 即，当已经"顶点k的最短路径"之后，更新"未获取最短路径的顶点的最短路径和前驱顶点"。 for (int j = 0; j &lt; mVexs.length; j++) &#123; int tmp = (mMatrix[k][j]==INF ? INF : (min + mMatrix[k][j])); if (flag[j]==false &amp;&amp; (tmp&lt;dist[j]) ) &#123; dist[j] = tmp; prev[j] = k; &#125; &#125; &#125; // 打印dijkstra最短路径的结果 System.out.printf("dijkstra(%c): \n", mVexs[vs]); for (int i=0; i &lt; mVexs.length; i++) System.out.printf(" shortest(%c, %c)=%d\n", mVexs[vs], mVexs[i], dist[i]);&#125; 1.4 时间复杂度我们可以用大O符号将该算法的运行时间表示为边数m和顶点数n的函数。 对于基于顶点集Q的实现，算法的运行时间是$O(|E|\cdot dk_{Q}+|V|\cdot em_{Q})$，其中$dk_{Q}和em_{Q}$分别表示完成键的降序排列时间和从Q中提取最小键值的时间。 Dijkstra算法最简单的实现方法是用一个链表或者数组来存储所有顶点的集合Q，所以搜索Q中最小元素的运算（Extract-Min(Q)）只需要线性搜索 Q中的所有元素。这样的话算法的运行时间是$O(n^{2})$。 对于边数少于$n^{2}$的稀疏图来说，我们可以用邻接表来更有效的实现该算法。同时需要将一个二叉堆或者斐波纳契堆用作优先队列来寻找最小的顶点（Extract-Min）。当用到二叉堆的时候，算法所需的时间为${\displaystyle O((m+n)logn)}$，斐波纳契堆能稍微提高一些性能，让算法运行时间达到${\displaystyle O(m+nlogn)}$。然而，使用斐波纳契堆进行编程，常常会由于算法常数过大而导致速度没有显著提高。 二、Floyd算法Dijkstra很优秀，但是使用Dijkstra有一个最大的限制，就是不能有负权边。而Bellman-Ford适用于权值可以为负、无权值为负的回路的图。这比Dijkstra算法的使用范围要广。 2.1 算法思想Floyd算法是一个经典的动态规划算法。用通俗的语言来描述的话，首先我们的目标是寻找从点i到点j的最短路径。从动态规划的角度看问题，我们需要为这个目标重新做一个诠释（这个诠释正是动态规划最富创造力的精华所在） 从任意节点i到任意节点j的最短路径不外乎2种可能，1是直接从i到j，2是从i经过若干个节点k到j。所以，我们假设Dis(i,j)为节点u到节点v的最短路径的距离，对于每一个节点k，我们检查Dis(i,k) + Dis(k,j) &lt; Dis(i,j)是否成立，如果成立，证明从i到k再到j的路径比i直接到j的路径短，我们便设置Dis(i,j) = Dis(i,k) + Dis(k,j)，这样一来，当我们遍历完所有节点k，Dis(i,j)中记录的便是i到j的最短路径的距离。 2.2 算法步骤初始状态：S是记录各个顶点间最短路径的矩阵。 初始化S：矩阵S中顶点a[i][j]的距离为顶点i到顶点j的权值；如果i和j不相邻，则a[i][j]=∞。实际上，就是将图的原始矩阵复制到S中。 注:a[i][j]表示矩阵S中顶点i(第i个顶点)到顶点j(第j个顶点)的距离。 以顶点A(第1个顶点)为中介点，若$a[i][j] &gt; a[i][0]+a[0][j]$，则设置$a[i][j]=a[i][0]+a[0][j]$。 以顶点$a[1]$，上一步操作之后，$a[1][6]=∞$；而将A作为中介点时，(B,A)=12，(A,G)=14，因此B和G之间的距离可以更新为26。 同理，依次将顶点B,C,D,E,F,G作为中介点，并更新a[i][j]的大小。 2.3 代码实现以”邻接矩阵”为例对弗洛伊德算法进行说明，对于”邻接表”实现的图在后面会给出相应的源码。 123456789public class MatrixUDG &#123; private int mEdgNum; // 边的数量 private char[] mVexs; // 顶点集合 private int[][] mMatrix; // 邻接矩阵 private static final int INF = Integer.MAX_VALUE; // 最大值 ...&#125; MatrixUDG是邻接矩阵对应的结构体。mVexs用于保存顶点，mEdgNum用于保存边数，mMatrix则是用于保存矩阵信息的二维数组。例如，mMatrix[i][j]=1，则表示”顶点i(即mVexs[i])”和”顶点j(即mVexs[j])”是邻接点；mMatrix[i][j]=0，则表示它们不是邻接点。 12345678910111213141516171819202122232425262728293031323334353637383940414243/* * floyd最短路径。 * 即，统计图中各个顶点间的最短路径。 * * 参数说明： * path -- 路径。path[i][j]=k表示，"顶点i"到"顶点j"的最短路径会经过顶点k。 * dist -- 长度数组。即，dist[i][j]=sum表示，"顶点i"到"顶点j"的最短路径的长度是sum。 */public void floyd(int[][] path, int[][] dist) &#123; // 初始化 for (int i = 0; i &lt; mVexs.length; i++) &#123; for (int j = 0; j &lt; mVexs.length; j++) &#123; dist[i][j] = mMatrix[i][j]; // "顶点i"到"顶点j"的路径长度为"i到j的权值"。 path[i][j] = j; // "顶点i"到"顶点j"的最短路径是经过顶点j。 &#125; &#125; // 计算最短路径 for (int k = 0; k &lt; mVexs.length; k++) &#123; for (int i = 0; i &lt; mVexs.length; i++) &#123; for (int j = 0; j &lt; mVexs.length; j++) &#123; // 如果经过下标为k顶点路径比原两点间路径更短，则更新dist[i][j]和path[i][j] int tmp = (dist[i][k]==INF || dist[k][j]==INF) ? INF : (dist[i][k] + dist[k][j]); if (dist[i][j] &gt; tmp) &#123; // "i到j最短路径"对应的值设，为更小的一个(即经过k) dist[i][j] = tmp; // "i到j最短路径"对应的路径，经过k path[i][j] = path[i][k]; &#125; &#125; &#125; &#125; // 打印floyd最短路径的结果 System.out.printf("floyd: \n"); for (int i = 0; i &lt; mVexs.length; i++) &#123; for (int j = 0; j &lt; mVexs.length; j++) System.out.printf("%2d ", dist[i][j]); System.out.printf("\n"); &#125;&#125; 2.4 时间复杂度Floyd-Warshall算法的时间复杂度为$O(N^{3})$，空间复杂度为$O(N^{2})$。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Dijkstra算法</tag>
        <tag>Floyd算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（32）：MapReduce执行流程详解]]></title>
    <url>%2F2017%2F07%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8832%EF%BC%89%EF%BC%9AMapReduce%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[一、简述MapReduce最早是由Google提出的分布式数据处理模型，随后受到了业内的广泛关注，并被大量应用到各种商业场景中。比如： 搜索：网页爬取、倒排索引、PageRank。 Web访问日志分析：分析和挖掘用户在web上的访问、购物行为特征，实现个性化推荐；分析用户访问行为。 文本统计分析：比如莫言小说的WordCount、词频TFIDF分析；学术论文、专利文献的引用分析和统计；维基百科数据分析等。 海量数据挖掘：非结构化数据、时空数据、图像数据的挖掘。 机器学习：监督学习、无监督学习、分类算法如决策树、SVM等。 自然语言处理：基于大数据的训练和预测；基于语料库构建单词同现矩阵，频繁项集数据挖掘、重复文档检测等。 广告推荐：用户点击（CTR)和购买行为（CVR）预测。 一个Map/Reduce作业（job）通常会把输入的数据（input file）且分为若干个独立的数据块（splits），然后由map任务（task）以完全并行的方式处理它们。Map/Reduce框架会对map的输出做一个Shuffle操作，Suffle操作后的结果会输入给reduce任务。整个Map/Reduce框架负责任务的调度和监控，以及重新执行已经失败的任务。 Map/Reduce计算集群由一个单独的JobTracker（master）和每个集群节点一个TaskTracker（slave）共同组成。JobTracker负责调度构成一个作业的所有任务，这些任务会被分派到不同的TaskTracker上去执行，JobTRacker会去监控它们的执行、重新执行已经失败的任务。而TaskTracker仅负责执行由JobTracker指派的任务。 本文将按照map/reduce执行流程中各个任务的时间顺序详细叙述map/reduce的各个任务模块，包括：输入分片（input split）、map阶段、combiner阶段、shuffle阶段和reduce阶段。下图是一个不错的执行流程图： 二、作业的提交与监控JobClient是用户提交的作业与JobTracker交互的主要接口。 JobClient提交作业的过程如下： map/reduce程序通过runJob()方法新建一个JobClient实例; 向JobTracker请求一个新jobID，通过JobTracker的getNewJobId()获取； 检查作业输入输出说明。如果没有指定输出目录或者输出目录已经存在，作业将不会被提交，map/reduce程序； 输入作业划分split，如果划分无法计算（如：输入路径不存在），作业将不会被提交，错误返回给map/reduce程序。 将运行作业所需要的资源（作业的jar文件、配置文件、计算所得的输入划分）复制到一个以作业ID命名的目录中； 通过调用JobTracker的submitJob()方法，告诉JobTracker作业准备提交； JobTracker将提交的作业放到一个内部队列中，交由作业调度器进行调度，并对其进行初始化。 创建Map任务、Reduce任务：一个split对应一个map，有多少split就有多少map; Reduce任务的数量由JobConf的mapred.reduce.tasks属性决定 TaskTracker执行一个简单的循环，定期发送心跳（heartbeat）给JobTracker 三、执行流程3.1 Input fileInput file是map/reduce任务的原始数据，一般存储在HDFS上。应用程序至少应该指明输入/输出的位置（路径），并通过实现合适的接口或抽象类提供map和reduce函数。再加上其他作业的参数，就构成了作业配置（job configuration）。然后，Hadoop的 job client提交作业（jar包/可执行程序等）和配置信息给JobTracker，后者负责分发这些软件和配置信息给slave、调度任务并监控它们的执行，同时提供状态和诊断信息给job-client。 3.1.1 InputFormatInputFormat为Map/Reduce作业输入的细节规范。Map/Reduce框架根据作业的InputFormat来： 检查作业输入的正确性，如格式等。 把输入文件切分成多个逻辑InputSplit实例， 一个InputSplit将会被分配给一个独立的Map任务。 提供RecordReader实现，这个RecordReader从逻辑InputSplit中获得输入记录（”K-V对”），这些记录将由Map任务处理。 InputFormat有如下几种: TextInputFormat: TextInputFormat是默认的INputFormat，输入文件中的每一行就是一个记录，Key是这一行的byte offset，而value是这一行的内容。如果一个作业的Inputformat是TextInputFormat，并且框架检测到输入文件的后缀是 .gz 和 .lzo，就会使用对应的CompressionCodec自动解压缩这些文件。但是需要注意，上述带后缀的压缩文件不会被切分，并且整个压缩文件会分给一个mapper来处理。 KeyValueTextInputFormat 输入文件中每一行就是一个记录，第一个分隔符字符切分每行。在分隔符字符之前的内容为Key，在之后的为Value。分隔符变量通过key.value.separator.in.input.line变量设置，默认为(\t)字符。 NLineInputFormat 与TextInputFormat一样，但每个数据块必须保证有且只有Ｎ行，mapred.line.input.format.linespermap属性，默认为1。 SequenceFileInputFormat 一个用来读取字符流数据的InputFormat，为用户自定义的。字符流数据是Hadoop自定义的压缩的二进制数据格式。它用来优化从一个MapReduce任务的输出到另一个MapReduce任务的输入之间的数据传输过程。 3.2 输入分片（Input files）InputSplit是一个单独的Map任务需要处理的数据块。一般的InputSplit是字节样式输入，然后由RecordReader处理并转化成记录样式。通常一个split就是一个block，这样做的好处是使得Map任务可以在存储有当前数据的节点上运行本地的任务，而不需要通过网络进行跨节点的任务调度。 可以通过设置mapred.min.split.size， mapred.max.split.size, block.size来控制拆分的大小。如果mapred.min.split.size大于block size，则会将两个block合成到一个split，这样有部分block数据需要通过网络读取；如果mapred.max.split.size小于block size，则会将一个block拆成多个split，增加了Map任务数。 在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务，输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组，输入分片（input split）往往和hdfs的block（块）关系很密切，假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别是3mb、65mb和127mb，那么mapreduce会把3mb文件分为一个输入分片（input split），65mb则是两个输入分片（input split）而127mb也是两个输入分片（input split），换句话说我们如果在map计算前做输入分片调整，例如合并小文件，那么就会有5个map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce优化计算的一个关键点。 12输入文件大小 10M 65M 127M分割后的InputSplit大小 10M 64M,1M 64M，63M 在Map任务开始前，会先获取文件在HDFS上的路径和block信息，然后根据splitSize对文件进行切分（splitSize = computeSplitSize(blockSize, minSize, maxSize) ），默认splitSize 就等于blockSize的默认值（64m）。 假设现在我们有两个文本文件，作为我们例子的输入： 1234567File 1 内容：My name is TonyMy company is pivotalFile 2 内容：My name is LisaMy company is EMC 3.2 Map阶段Map是一类将输入记录集转换为中间格式记录集的独立任务，主要是读取InputSplit的每一个Key,Value对并进行处理。 首先我们的输入就是两个文件， 默认情况下就是两个split, 对应前面图中的split 0, split 1 两个split 默认会分给两个Mapper来处理， WordCount例子相当地暴力， 这一步里面就是直接把文件内容分解为单词和 1 （注意， 不是具体数量， 就是数字1）其中的单词就是我们的主健，也称为Key, 后面的数字就是对应的值，也称为value. 那么对应两个Mapper的输出就是： split 0 12345678My 1name 1is 1Tony 1My 1company 1is 1Pivotal 1 split 1 12345678My 1name 1is 1Lisa 1My 1company 1is 1EMC 1 3.3 Shuffle阶段将map的输出作为reduce的输入的过程就是shuffle了，这个是mapreduce优化的重点地方。这里我不讲怎么优化shuffle阶段，讲讲shuffle阶段的原理，因为大部分的书籍里都没讲清楚shuffle阶段。Shuffle一开始就是map阶段做输出操作，一般mapreduce计算的都是海量数据，map输出时候不可能把所有文件都放到内存操作，因此map写入磁盘的过程十分的复杂，更何况map输出时候要对结果进行排序，内存开销是很大的，map在做输出时候会在内存里开启一个环形内存缓冲区，这个缓冲区专门用来输出的，默认大小是100mb，并且在配置文件里为这个缓冲区设定了一个阀值，默认是0.80（这个大小和阀值都是可以在配置文件里进行配置的），同时map还会为输出操作启动一个守护线程，如果缓冲区的内存达到了阀值的80%时候，这个守护线程就会把内容写到磁盘上，这个过程叫spill，另外的20%内存可以继续写入要写进磁盘的数据，写入磁盘和写入内存操作是互不干扰的，如果缓存区被撑满了，那么map就会阻塞写入内存的操作，让写入磁盘操作完成后再继续执行写入内存操作，前面我讲到写入磁盘前会有个排序操作，这个是在写入磁盘操作时候进行，不是在写入内存时候进行的，如果我们定义了combiner函数，那么排序前还会执行combiner操作。每次spill操作也就是写入磁盘操作时候就会写一个溢出文件，也就是说在做map输出有几次spill就会产生多少个溢出文件，等map输出全部做完后，map会合并这些输出文件。这个过程里还会有一个Partitioner操作，对于这个操作很多人都很迷糊，其实Partitioner操作和map阶段的输入分片（Input split）很像，一个Partitioner对应一个reduce作业，如果我们mapreduce操作只有一个reduce操作，那么Partitioner就只有一个，如果我们有多个reduce操作，那么Partitioner对应的就会有多个，Partitioner因此就是reduce的输入分片，这个程序员可以编程控制，主要是根据实际key和value的值，根据实际业务类型或者为了更好的reduce负载均衡要求进行，这是提高reduce效率的一个关键所在。到了reduce阶段就是合并map输出文件了，Partitioner会找到对应的map输出文件，然后进行复制操作，复制操作时reduce会开启几个复制线程，这些线程默认个数是5个，程序员也可以在配置文件更改复制线程的个数，这个复制过程和map写入磁盘过程类似，也有阀值和内存大小，阀值一样可以在配置文件里配置，而内存大小是直接使用reduce的tasktracker的内存大小，复制时候reduce还会进行排序操作和合并文件操作，这些操作完了就会进行reduce计算了。 3.3.1 PartitionPartition 是什么？ Partition 就是分区。 为什么要分区？ 因为有时候会有多个Reducer, Partition就是提前对输入进行处理， 根据将来的Reducer进行分区. 到时候Reducer处理的时候， 只需要处理分给自己的数据就可以了。 如何分区？ 主要的分区方法就是按照Key 的不同，把数据分开，其中很重要的一点就是要保证Key的唯一性， 因为将来做Reduce的时候有可能是在不同的节点上做的， 如果一个Key同时存在于两个节点上， Reduce的结果就会出问题， 所以很常见的Partition方法就是哈希。 结合我们的例子， 我们这里假设有两个Reducer, 前面两个split 做完Partition的结果就会如下： split 0 1234567891011Partition 1:company 1is 1is 1Partition 2:My 1My 1name 1Pivotal 1Tony 1 split 1 1234567891011Partition 1:company 1is 1is 1EMC 1Partition 2:My 1My 1name 1Lisa 1 其中Partition 1 将来是准备给Reducer 1 处理的， Partition 2 是给Reducer 2 的 这里我们可以看到， Partition 只是把所有的条目按照Key 分了一下区， 没有其他任何处理， 每个区里面的Key 都不会出现在另外一个区里面。 3.3.2 SortSort 就是排序喽， 其实这个过程在我来看并不是必须的， 完全可以交给客户自己的程序来处理。 那为什么还要排序呢？ 可能是写MapReduce的大牛们想，“大部分reduce 程序应该都希望输入的是已经按Key排序好的数据， 如果是这样， 那我们就干脆顺手帮你做掉啦， 请叫我雷锋！” ……好吧， 你是雷锋. 那么我们假设对前面的数据再进行排序， 结果如下： split 0 1234567891011Partition 1:company 1is 1is 1Partition 2:My 1My 1name 1Pivotal 1Tony 1 split 1 1234567891011Partition 1:company 1EMC 1is 1is 1Partition 2:Lisa 1My 1My 1name 1 这里可以看到， 每个partition里面的条目都按照Key的顺序做了排序 3.3.3 Combine什么是Combine呢？ Combine 其实可以理解为一个mini Reduce 过程， 它发生在前面Map的输出结果之后， 目的就是在结果送到Reducer之前先对其进行一次计算， 以减少文件的大小， 方便后面的传输。 但这步也不是必须的。 按照前面的输出， 执行Combine: split 0 123456789Partition 1:company 1is 2Partition 2:My 2name 1Pivotal 1Tony 1 split 1 123456789Partition 1:company 1EMC 1is 2Partition 2:Lisa 1My 2name 1 我们可以看到， 针对前面的输出结果， 我们已经局部地统计了is 和My的出现频率， 减少了输出文件的大小。 3.3.4 copy下面就要准备把输出结果传送给Reducer了。 这个阶段被称为Copy, 但事实上雷子认为叫他Download更为合适， 因为实现的时候， 是通过http的方式， 由Reducer节点向各个mapper节点下载属于自己分区的数据。 那么根据前面的Partition, 下载完的结果如下： Reducer 节点 1 共包含两个文件: 123Partition 1:company 1is 2 1234Partition 1:company 1EMC 1is 2 Reducer 节点 2 也是两个文件: 12345 Partition 2:My 2name 1Pivotal 1Tony 1 1234Partition 2:Lisa 1My 2name 1 这里可以看到， 通过Copy, 相同Partition 的数据落到了同一个节点上。 3.3.5 merge如上一步所示， 此时Reducer得到的文件是从不同Mapper那里下载到的， 需要对他们进行合并为一个文件， 所以下面这一步就是Merge, 结果如下： Reducer 节点 1 12345company 1company 1EMC 1is 2is 2 Reducer 节点 2 1234567Lisa 1My 2My 2name 1name 1Pivotal 1Tony 1 3.4 Redeuce阶段reduce阶段对数据进行归约处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。归约后的数据输出到本地文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。 四、实例说明下面将以WordCount为例，解释MapReduce各个阶段的概念。 假设存在一个文本a.txt，文本内每行是一个数字，我们要统计每个数字出现的次数。文本内的数字称为Word，数字出现的次数称为Count。如果MaxCompute Mapreduce完成这一功能，需要经历下图描述的几个步骤： 首先对文本进行分片，将每片内的数据作为单个Map Worker的输入； Map处理输入，每获取一个数字，将数字的Count设置为1，并将此对输出，此时以Word作为输出数据的Key；在Shuffle阶段前期，首先对每个Map Worker的输出，按照Key值，即Word值排序。排序后进行Combine操作，即将Key值(Word值)相同的Count累加，构成一个新的对。此过程被称为合并排序； 在Shuffle阶段后期，数据被发送到Reduce端。Reduce Worker收到数据后依赖Key值再次对数据排序； 每个Reduce Worker对数据进行处理时，采用与Combiner相同的逻辑，将Key值(Word值)相同的Count累加，得到输出结果； 分布式相关mr 方案解决矩阵相乘的代码；hadoop原理，shuffle如何排序，map如何切割数据，如何处理数据倾斜，join的mr代码 MR的shuffle过程？内存不够时涉及大文件排序如何处理？ 答：先hash到不同文件中，每个文件排序，然后每个文件读取行，类似归并排序的思路？ Hadoop,Spark,storm下面的产品，原理，适用场景 spark跟hadoop的区别答：spark有RDD机制，写内存，相对hadoop适合迭代运算 如何用hadoop实现k-means简单介绍 MapReduce 原理，有没有看过源码，说说 Map 阶段怎么实现的,MapReduce 实现统计出现次数最多的前 100 个访问 IP.MapReduce 实现统计不重复用户 ID,MapReduce 实现两个数据集求交集。HBase 行健怎么设计,spark 性能一般优化方法,spark streaming 和 storm 区别.给了一张笔试题， 10 道选择，一道大题。选择题是 java 基础知识，大题一个有三问：根据场景写出 Hive 建表语句； Hsql 从表中查询；用MapReduce写好友推荐，在一堆单词里面找出现次数最多的k个用分布式的方法做采样怎么保证采样结果完全符合预期？后面又问了Hadoop,Spark,storm下面的产品，原理，适用场景，写一个 Hadoop 版本的 wordcount。K-means能否分布式实现？ 答：因为本身是迭代式算法，所以只能半分布式实现，即在计算类的均值、每个样本点属于哪个类的时候还有怎么解决mapreduce数据倾斜]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（13）：深度优先搜索和广度优先搜索]]></title>
    <url>%2F2017%2F07%2F27%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8813%EF%BC%89%EF%BC%9A%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%E5%92%8C%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[BFS和DFS是两种十分重要的搜索算法，BFS适合查找最优解，DFS适合查找是否存在解(或者说能找到任意一个可行解)。用这两种算法即可以解决大部分树和图的问题。 一、深度优先搜索（DFS）1.1 介绍图的深度优先搜索（Depth First Search），和树的先序遍历比较类似。它的思想：假设初始状态是图中所有顶点均未被访问，则从某个顶点V出发，首先访问该顶点，然后依次从它的各个未被访问的邻接点出发深度优先搜索遍历图，直至图中所有和V有路径相通的顶点都被访问到。若此时尚有其他顶点未被访问到，则另选一个未被访问的顶点作起始点，重复上述过程，直至图中所有顶点都被访问到为止。 显然，深度优先搜索是一个递归的过程。 1.2 图解1.2.1 无向图的深度优先搜索下面以“无向图”为例，来对深度优先搜索进行演示。对上面的图G1进行深度优先遍历，从顶点A开始。 第1步：访问A 第2步：访问（A的邻接点）C。在第一步访问A之后，接下来应该访问的是A的邻接点，即“C/D/F”中的一个。但在本文的实现中，顶点ABCDEFG是按照顺序存储，C在“D和F的前面，因此，先访问C。” 第3步：访问（C的邻接点）B。在第2步访问C之后，接下来应该访问C的邻接点，即”B和D”中一个(A已经被访问过，就不算在内)。而由于B在D之前，先访问B。 第4步：访问(C的邻接点)D。 在第3步访问了C的邻接点B之后，B没有未被访问的邻接点；因此，返回到访问C的另一个邻接点D。 第5步：访问(A的邻接点)F。 前面已经访问了A，并且访问完了”A的邻接点B的所有邻接点(包括递归的邻接点在内)”；因此，此时返回到访问A的另一个邻接点F。 第6步：访问(F的邻接点)G。 第7步：访问(G的邻接点)E。 因此访问顺序是：$A=&gt;C=&gt;=&gt;B=&gt;D=&gt;F=&gt;G=&gt;E$ 1.2.2 有向图的深度优先搜索下面以“有向图”为例，来对深度优先搜索进行演示。对上面的图G2进行深度优先遍历，从顶点A开始。 第1步：访问A。 第2步：访问B。在访问了A之后，接下来应该访问的是A的出边的另一个顶点，即顶点B。 第3步：访问C。在访问了B之后，接下来应该访问的是B的出边的另一个顶点，即顶点C,E,F。在本文实现的图中，顶点ABCDEFG按照顺序存储，因此先访问C。 第4步：访问E。接下来访问C的出边的另一个顶点，即顶点E。 第5步：访问D。接下来访问E的出边的另一个顶点，即顶点B,D。顶点B已经被访问过，因此访问顶点D。 第6步：访问F。 接下应该回溯”访问A的出边的另一个顶点F”。 第7步：访问G。 因此访问顺序是：$A =&gt; B =&gt; C =&gt; E =&gt; D =&gt; F =&gt; G$ 二、广度优先搜索（BFS）2.1 介绍广度优先搜索算法（Breadth First Search），又称为“宽度优先搜索”或“横向优先搜索”，简称BFS。 它的思想是：从图中某顶点V出发，在访问了V之后依次访问V的各个未曾访问过的邻接点，然后分别从这些邻接点出发依次访问他们的邻接点，并使得“先被访问的顶点的邻接点先于后被访问的顶点的邻接点被访问”，直至图中所有已被访问的顶点的邻接点都被访问到。如果此时图中尚有顶点未被访问，则需要另选一个未曾被访问的顶点作为新的起始点，重复上述过程，知道图中所有顶点都被访问到为止。 换句话说，广度优先搜索遍历图的过程是以V为起点，由近至远，依次访问和V有路径相同且路径长度为1、2、3……的顶点。 2.2 图解2.2.1 无向图的广度优先搜索下面以“无向图”为例，来对广度优先搜索进行演示。还是以上面的图G1为例进行说明。 第1步：访问A。 第2步：依次访问C,D,F。在访问了A之后，接下来访问A的邻接点。前面已经说过，在本文实现中，顶点ABCDEFG按照顺序存储的，C在”D和F”的前面，因此，先访问C。再访问完C之后，再依次访问D,F。 第3步：依次访问B,G。 在第2步访问完C,D,F之后，再依次访问它们的邻接点。首先访问C的邻接点B，再访问F的邻接点G。 第4步：访问E。在第3步访问完B,G之后，再依次访问它们的邻接点。只有G有邻接点E，因此访问G的邻接点E。 因此访问顺序是：$A =&gt; C =&gt; D =&gt; F =&gt; B =&gt; G =&gt; E$ 2.2.2 有向图的广度优先搜索下面以“有向图”为例，来对广度优先搜索进行演示。还是以上面的图G2为例进行说明。 第1步：访问A。 第2步：访问B。 第3步：依次访问C,E,F。 在访问了B之后，接下来访问B的出边的另一个顶点，即C,E,F。前面已经说过，在本文实现中，顶点ABCDEFG按照顺序存储的，因此会先访问C，再依次访问E,F。 第4步：依次访问D,G。 在访问完C,E,F之后，再依次访问它们的出边的另一个顶点。还是按照C,E,F的顺序访问，C的已经全部访问过了，那么就只剩下E,F；先访问E的邻接点D，再访问F的邻接点G。 三、代码实现无向图和有向图的BFS与DFS 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227/** * Java: 邻接矩阵表示的&quot;无向图(Matrix Undirected Graph)&quot; * * @author skywang * @date 2014/04/19 */import java.io.IOException;import java.util.Scanner;public class Demo &#123; private char[] mVexs; // 顶点集合 private int[][] mMatrix; // 邻接矩阵 /* * 创建图(自己输入数据) */ public Demo() &#123; // 输入&quot;顶点数&quot;和&quot;边数&quot; System.out.printf(&quot;input vertex number: &quot;); int vlen = readInt(); System.out.printf(&quot;input edge number: &quot;); int elen = readInt(); if ( vlen &lt; 1 || elen &lt; 1 || (elen &gt; (vlen*(vlen - 1)))) &#123; System.out.printf(&quot;input error: invalid parameters!\n&quot;); return ; &#125; // 初始化&quot;顶点&quot; mVexs = new char[vlen]; for (int i = 0; i &lt; mVexs.length; i++) &#123; System.out.printf(&quot;vertex(%d): &quot;, i); mVexs[i] = readChar(); &#125; // 初始化&quot;边&quot; mMatrix = new int[vlen][vlen]; for (int i = 0; i &lt; elen; i++) &#123; // 读取边的起始顶点和结束顶点 System.out.printf(&quot;edge(%d):&quot;, i); char c1 = readChar(); char c2 = readChar(); int p1 = getPosition(c1); int p2 = getPosition(c2); if (p1==-1 || p2==-1) &#123; System.out.printf(&quot;input error: invalid edge!\n&quot;); return ; &#125; mMatrix[p1][p2] = 1; mMatrix[p2][p1] = 1; &#125; &#125; /* * 创建图(用已提供的矩阵) * * 参数说明： * vexs -- 顶点数组 * edges -- 边数组 */ public Demo(char[] vexs, char[][] edges) &#123; // 初始化&quot;顶点数&quot;和&quot;边数&quot; int vlen = vexs.length; int elen = edges.length; // 初始化&quot;顶点&quot; mVexs = new char[vlen]; for (int i = 0; i &lt; mVexs.length; i++) mVexs[i] = vexs[i]; // 初始化&quot;边&quot; mMatrix = new int[vlen][vlen]; for (int i = 0; i &lt; elen; i++) &#123; // 读取边的起始顶点和结束顶点 int p1 = getPosition(edges[i][0]); int p2 = getPosition(edges[i][1]); mMatrix[p1][p2] = 1; mMatrix[p2][p1] = 1; &#125; &#125; /* * 返回ch位置 */ private int getPosition(char ch) &#123; for(int i=0; i&lt;mVexs.length; i++) if(mVexs[i]==ch) return i; return -1; &#125; /* * 读取一个输入字符 */ private char readChar() &#123; char ch=&apos;0&apos;; do &#123; try &#123; ch = (char)System.in.read(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; while(!((ch&gt;=&apos;a&apos;&amp;&amp;ch&lt;=&apos;z&apos;) || (ch&gt;=&apos;A&apos;&amp;&amp;ch&lt;=&apos;Z&apos;))); return ch; &#125; /* * 读取一个输入字符 */ private int readInt() &#123; Scanner scanner = new Scanner(System.in); return scanner.nextInt(); &#125; /* * 返回顶点v的第一个邻接顶点的索引，失败则返回-1 */ private int firstVertex(int v) &#123; if (v&lt;0 || v&gt;(mVexs.length-1)) return -1; for (int i = 0; i &lt; mVexs.length; i++) if (mMatrix[v][i] == 1) return i; return -1; &#125; /* * 返回顶点v相对于w的下一个邻接顶点的索引，失败则返回-1 */ private int nextVertex(int v, int w) &#123; if (v&lt;0 || v&gt;(mVexs.length-1) || w&lt;0 || w&gt;(mVexs.length-1)) return -1; for (int i = w + 1; i &lt; mVexs.length; i++) if (mMatrix[v][i] == 1) return i; return -1; &#125; /* * 深度优先搜索遍历图的递归实现 */ private void DFS(int i, boolean[] visited) &#123; visited[i] = true; System.out.printf(&quot;%c &quot;, mVexs[i]); // 遍历该顶点的所有邻接顶点。若是没有访问过，那么继续往下走 for (int w = firstVertex(i); w &gt;= 0; w = nextVertex(i, w)) &#123; if (!visited[w]) DFS(w, visited); &#125; &#125; /* * 深度优先搜索遍历图 */ public void DFS() &#123; boolean[] visited = new boolean[mVexs.length]; // 顶点访问标记 // 初始化所有顶点都没有被访问 for (int i = 0; i &lt; mVexs.length; i++) visited[i] = false; System.out.printf(&quot;DFS: &quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; if (!visited[i]) DFS(i, visited); &#125; System.out.printf(&quot;\n&quot;); &#125; /* * 广度优先搜索（类似于树的层次遍历） */ public void BFS() &#123; int head = 0; int rear = 0; int[] queue = new int[mVexs.length]; // 辅组队列 boolean[] visited = new boolean[mVexs.length]; // 顶点访问标记 for (int i = 0; i &lt; mVexs.length; i++) visited[i] = false; System.out.printf(&quot;BFS: &quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; if (!visited[i]) &#123; visited[i] = true; System.out.printf(&quot;%c &quot;, mVexs[i]); queue[rear++] = i; // 入队列 &#125; while (head != rear) &#123; int j = queue[head++]; // 出队列 for (int k = firstVertex(j); k &gt;= 0; k = nextVertex(j, k)) &#123; //k是为访问的邻接顶点 if (!visited[k]) &#123; visited[k] = true; System.out.printf(&quot;%c &quot;, mVexs[k]); queue[rear++] = k; &#125; &#125; &#125; &#125; System.out.printf(&quot;\n&quot;); &#125; /* * 打印矩阵队列图 */ public void print() &#123; System.out.printf(&quot;Martix Graph:\n&quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; for (int j = 0; j &lt; mVexs.length; j++) System.out.printf(&quot;%d &quot;, mMatrix[i][j]); System.out.printf(&quot;\n&quot;); &#125; &#125; public static void main(String[] args) &#123; char[] vexs = &#123;&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;D&apos;, &apos;E&apos;, &apos;F&apos;, &apos;G&apos;&#125;; char[][] edges = new char[][]&#123; &#123;&apos;A&apos;, &apos;C&apos;&#125;, &#123;&apos;A&apos;, &apos;D&apos;&#125;, &#123;&apos;A&apos;, &apos;F&apos;&#125;, &#123;&apos;B&apos;, &apos;C&apos;&#125;, &#123;&apos;C&apos;, &apos;D&apos;&#125;, &#123;&apos;E&apos;, &apos;G&apos;&#125;, &#123;&apos;F&apos;, &apos;G&apos;&#125;&#125;; Demo pG; // 自定义&quot;图&quot;(输入矩阵队列) //pG = new MatrixUDG(); // 采用已有的&quot;图&quot; pG = new Demo(vexs, edges); pG.print(); // 打印图 pG.DFS(); // 深度优先遍历 pG.BFS(); // 广度优先遍历 &#125;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>图</tag>
        <tag>DFS</tag>
        <tag>BFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（31）：在线最优化求解（online Optimization）]]></title>
    <url>%2F2017%2F07%2F26%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8831%EF%BC%89%EF%BC%9A%E5%9C%A8%E7%BA%BF%E6%9C%80%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3%EF%BC%88online%20Optimization%EF%BC%89%2F</url>
    <content type="text"><![CDATA[转载自冯扬《在线最优化求解》 最优化求解问题可能是我们在工作中遇到的最多的一类问题了：从已有的数据中提炼出最适合的模型参数，从而对未知的数据进行预测。当我们面对高维高数据量的场景时，常见的批量处理的方式已经显得力不从心，需要有在线处理的方法来解决此类问题。本文以模型的稀疏性作为主线，逐一介绍几个在线最优化求解算法，并进行推导，力求讲清楚算法的来龙去脉，以及不同算法之间的区别和联系，达到融会贯通。在各个算法原理介绍之后，都给出该算法的工程实现伪代码，可以用于实际工作的参考。 一、动机与目的在实际工作中，无论是工程师、项目经理、产品同学都会经常讨论一类话题：“从线上对比的效果来看，某某特征或因素对xx产品的最终效果有很大的影响。”这类话题本质上说的是通过已有的数据反映出某些特定的因素对结果有很强的正（或负）相关性。而如何定量计算这种相关性？如何得到一套模型参数能够使得效果达到最好？这就是最优化计算要做的事情。 举一类典型点的例子：在推荐和广告计算中，我们经常会需要对某些值进行预测，例如在一条推荐或广告在曝光之前先预测用户是否会点击（CTR预估），或者是否会由此产生某些转换（RPM预估）。这列问题可以表示为：针对一个输入$X=[x_1,x_2,……x_N]\in R^N$，通过某个函数$h(X)$计算（预测）输出$y\in R$。根据$y$值为连续的还是离散的，预测问题被划分成回归问题（Regression）和分类问题（Classification）。而利用已有的样本数据$\{(X_j,y_j) | j=1,2,3….,M\}$训练$h(X)$的过程往往转换成一个最优化求解的过程。 无论是线性回归（Linear Regression）、逻辑回归（Logistic Regression）、支持向量机（SVM）、深度学习（Deep Learning）中，最优化求解都是基本的步骤。常见的梯度下降、牛顿法、拟牛顿法等属于批量处理的方法（Batch），每次更新都需要对已经训练过的样本重新训练一遍。当我们面对高维高数据量的时候，批量处理的方式就显得笨重和不够高效，因此需要在线处理的方法来解决相同的问题。关于在线最优化问题（Online Optimization）的论文比较多，注意查找阅读费时费力，那么本文就以高维高数据量的应用场景中比较看重的稀疏性作为主线，来介绍一些在线最优化的方法。 本文的预期读者大概有如下几类： 具有很深的机器学习经验和背景的高阶人员：就拿这篇文章当做一个关于在线最优化算法的回归材料好了，如有错误和不足欢迎指正。 具有一定机器学习经验的中级读者：可以将本文作为一个理论资料进行阅读，略过“预备知识”部分，直接进入主题，将之前对于在线最优化算法的理解串联起来，希望对将来的工作提供帮助。 对机器学习有认识但是时间经验较少的初级读者：从预备知识看起，注意理解相关概念和方法，从而达到融会贯通的目的。 仅仅对算法的工程实现感兴趣的读者：大致浏览下预备知识的2.3节，了解我们要讨论什么，然后直奔各算法的算法逻辑（伪代码），照着实现就好了。 高富帅和白富美：只需要知道本文讨论的是一堆好用的求最优解的方法，可以用于分类回归预测的一系列问题，然后吩咐工程师去实践就好了。还可以拿这篇文章嘲笑机器学习的屌丝：看你们弄些啥，累死累活的，挣那么几个钢镚。 二、预备知识2.1 凸函数如果$f(X)$是定义在N为向量空间上的实值函数，对于在$f(X)$的定义域$C$上的任意两个点$X_1$和$X_2$，以及任意$[0,1]$之间的值$t$都有： f(tX_1+(1-t)X_2)≤tf(X_1)+(1-t)f(X_2) \ \ \ ∀𝑋_1,𝑋_2∈𝐶, \ \ 0≤𝑡≤1则$f(X)$是严格凸函数（Strict Convex），如图一所示，（a）为严格凸函数，（b）为凸函数。 2.2 拉格朗日乘数法及KKT条件通常我们需要求解的最优化问题有如下三类： 1.无约束优化问题： X=arg\underset{X}{min} \ f(X)含义是求解$X$，使得目标函数$f(X)$最小；2.有等式约束的优化问题： X=arg\underset{X}{min} \ f(X) \\ s.t. \ \ h_k(X)=0; \ \ k=1,2...n含义是在n个等式约束$h_k(X)=0$的条件下，求解$X$，使得目标函数$f(X)$最小；3.有不等式约束的优化问题： X=arg\underset{X}{min} \ f(X) \\ s.t. \ \ h_k(X)=0; \ \ k=1,2...n \\ g_l(X)≤0；l=1,2.....m含义是在n个等式约束$h_k(X)$以及m各不等式约束$g_l(X)$的条件下，求解$X$使得目标函数$f(X)$最小。 针对无约束最优化问题，通常做法就是对$f(X)$求导，并令$\frac{\partial}{\partial X}f(X)=0$，求解可以得到最优值。如果$f(X)$为凸函数，则可以保证结果为全局最优解。 针对有等式约束的最优化问题，采用拉格朗日乘数法（Lagrange Multiplier）进行求解：通过拉格朗日系数$A=[a_1,a_2…a_n]^T\in R^n$把等式约束和目标函数组合成为一个式子，对该式子进行最优化求解： X=arg\underset{X}{min}\ [f(X)+A^TH(X)]其中,$H(X)=[h_1(X),h_2(X)…h_n(X)]^T\in R^n$，相当于将有等式约束的最优化问题转换成了无约束最优化求解问题，解决方法依旧是对$f(X)+A^TH(X)$的各个参数$(X,A)$求偏导，并令其为0，联立等式求解。 针对有不等式约束的最优化问题，常用的方法是KKT条件（Karush-Kuhn-Tucker Conditions）：同样地，把所有的不等式约束、等是约束和目标函数全部写为一个式子： L(X,A,B)=f(X)+A^TH(X)+B^TG(X)KKT条件是说最优值必须满足以下条件： \frac{\partial}{\partial X}L(X,A,B)=0 \\ H(X)=0 \\ B^TG(X)=0其中，$B=[b_1,b_2…b_m]^T\in R^m，G(X)=[g_1(X),g_2(X)…g_m(X)]^T \in R^m$。KKT条件是求解最优值$X^*$的必要条件，要使其成为充分必要条件，还需要$f(X)$为凸函数才行。 在KKT条件中，$B^TG(X)=0$这个条件最有趣，因为$g_l(X)≤0$,如果要满足这个等式，需要$b_l=0$或者$g_l(X)=0$。在我们后面的推导中会用到这个性质。 2.3 从Batch到Online我们面对的最优化问题都是无约束的最优化问题（有约束最优化问题可以利用拉格朗日乘数法或KKT条件转换成无约束最优化问题），因此我们通常可以将它们描述成： W=𝑎𝑟𝑔\underset{w}{𝑚𝑖𝑛}\ l(𝑊, 𝑍) \\ 𝑍 = \{(𝑋_𝑗,𝑦_𝑗)|𝑗 = 1,2,...𝑀\}\\𝑦_𝑗 =h(𝑊,𝑋_𝑗)这里$Z$为观测样本集合（训练集）；$X_j$是第$j$条阉割版的特征向量；$y_j=h(W,X_j)$为预测值；$h(W,X_j)$为特征向量到预测值的映射函数；$l (𝑊,𝑍) $为最优化求解的目标函数，也称作损失函数，损失函数通常可以分解为各样本损失函数的累加，即$l(𝑊,𝑍) =\sum_{j=1}^{M}l(𝑊,𝑍_𝑗)$;$W$为特征权重，也就是我们需要求解的参数。以线性回归和逻辑回归为例，它们的映射函数和损失函数分别为：在2.1中我们给出了无约束最优化问题解析解的求法。而在我们实际的数值计算中，通常做法是随机给定一个初始的$W^{(0)}$，通过迭代，在每次迭代中计算损失函数在当前$W^{(t)}$的下降方向，并更新$W$,直到损失函数稳定在最小的点。例如著名的梯度下降法（Gradient Descent）就是通过计算损失函数的在当前$W^{(t)}$处的梯度（Gradient），以梯度$\nabla_Wl(W^{(t)},Z)$的反方向作为下降方向更新$W$，如果损失函数是一个非平滑的凸函数（Non-Smooth Convex），在不可导处用次梯度（Subgradient）方向的反方向作为下降方向。算法如下：GD是一种批量处理的方式（Batch），每次更新$W$的时候都要扫描所有的样本以计算一个全局的梯度$\nabla_Wl(W,Z)$ 考虑另一种权重更新策略：在算法2中，每次迭代仅仅根据单个样本更新权重$W$，这种算法称作随机梯度下降（SGD，Stochastic Gradient Descent） 与 GD 每次扫所有的样本以计算一个全局的梯度相比，SGD 则每次只针对一 个观测到的样本进行更新。通常情况下，SGD 能够比 GD“更快”地令𝑊逼近最优值。当样 本数𝑀特别大的时候，SGD 的优势更加明显，并且由于 SGD 针对观测到的“一条”样本更新 𝑊，很适合进行增量计算，实现梯度下降的 Online 模式(OGD, Online Gradient Descent)。 2.4 正则化正则化(Regularization)的意义本质上是为了避免训练得到的模型过度拟合(overfitting) 训练数据。我们用图 2 来说明什么是过拟合。图 2 是一个局部加权线性回归(Locally weighted linear regression)的训练结果，当学习度为 1 时，相当于进行线性回归，这时候模型与训练样本以及实际曲线拟合得都不够好，模型处于 欠拟合(underfitting)状态;当学习度逐渐增加到 4 的过程中，模型逐渐与实际曲线吻合; 随着学习度继续增加，越来越多的样本直接落到模型曲线上(模型拟合训练数据)，但是模 型却与实际曲线相差越来越大，出现了过拟合。过拟合体现出来的现象就是特征权重𝑊的各个维度的绝对值非常大:一些大正数，一些大负数。这种模型虽然能够很好匹配样本(如图 2 中 Degree = 20 的情况)，但是对新样本做 预测的时候会使得预测值与真实值相差很远。 为了避免过拟合的情况，我们通常在损失函数的基础上加一个关于特征权重𝑊的限制， 限制它的模不要太大，如果用𝜓(𝑊)表示特征权重𝑊的一种求模计算，那么(2-3-1)转换成: 为了避免过拟合的情况，我们通常在损失函数的基础上加一个关于特征权重$𝑊$的限制， 限制它的模不要太大，如果用$𝜓(𝑊)$表示特征权重$𝑊$的一种求模计算，那么(2-3-1)转换成: W=arg \underset {W}{min }l(W,Z) \\ s.t. 𝜓(𝑊)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>online learning</tag>
        <tag>FTRL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（12）：排序]]></title>
    <url>%2F2017%2F07%2F26%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8812%EF%BC%89%EF%BC%9A%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[经典排序算法在面试中占有很大的比重，也是基础，在这里整理并用Java实现了几大经典排序算法，包括冒泡排序、插入排序、选择排序、希尔排序、归并排序、快速排序、堆排序、同排序。我们默认将一个无序数列排序成由小到大。 一、冒泡排序（Bubble Sort）1.1 思想冒泡排序(bubble sort)：每个回合都从第一个元素开始和它后面的元素比较，如果比它后面的元素更大的话就交换，一直重复，直到这个元素到了它能到达的位置。每次遍历都将剩下的元素中最大的那个放到了序列的“最后”(除去了前面已经排好的那些元素)。注意检测是否已经完成了排序，如果已完成就可以退出了。 1.2 代码123456789101112131415161718192021public class Demo&#123; public static void BubbleSort(int[] arr)&#123; int temp = 0; for (int i = arr.length - 1; i &gt; 0; --i) &#123; // 每次需要排序的长度 for (int j = 0; j &lt; i; ++j) &#123; // 从第一个元素到第i个元素 if (arr[j] &gt; arr[j + 1]) &#123; temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125; &#125;//loop j &#125;//loop i &#125;// method bubbleSort public static void main(String[] args)&#123; int[] arr = &#123;10,30,20,60,40,50&#125;; Demo.BubbleSort(arr); for(int i:arr)&#123; System.out.print(i+","); &#125; &#125;&#125; 1.3 时空复杂度冒泡排序的关键字比较次数与数据元素的初始状态无关。第一趟的比较次数为n-1，第i趟的比较次数为n-i，第n-1趟（最后一趟）的比较次数为1，因此冒泡排序总的比较次数为$n(n-1)/2$ 冒泡排序的数据元素移动次数与序列的初始状态有关。在最好的情况下，移动次数为0次；在最坏的情况下，移动次数为$n(n-1)/2$ 冒泡排序的时间复杂度为$O(n^2)$。冒泡排序不需要辅助存储单元，其空间复杂度为$O(1)$。如果关键字相等，则冒泡排序不交换数据元素，他是一种稳定的排序方法。 时间复杂度：最好$O(n)$；最坏$O(n^2)$；平均$O(n^2)$空间复杂度：$O(1)$稳定性：稳定 二、选择排序（Selection Sort）2.1 思想每个回合都选择出剩下的元素中最大的那个，选择的方法是首先默认第一元素是最大的，如果后面的元素比它大的话，那就更新剩下的最大的元素值，找到剩下元素中最大的之后将它放入到合适的位置就行了。和冒泡排序类似，只是找剩下的元素中最大的方式不同而已。 2.2 代码1234567891011121314151617181920212223242526public class Demo&#123; public static void selectionSort(int[] arr) &#123; int temp, min = 0; for (int index = 0; index &lt; arr.length - 1; ++index) &#123; min = index; // 循环查找最小值 for (int j = index + 1; j &lt; arr.length; ++j) &#123; if (arr[min] &gt; arr[j]) &#123; min = j; &#125; &#125; if (min != index) &#123; temp = arr[index]; arr[index] = arr[min]; arr[min] = temp; &#125; &#125; &#125; public static void main(String[] args)&#123; int[] arr = &#123;10,30,20,60,40,50&#125;; Demo.selectionSort(arr); for(int i:arr)&#123; System.out.print(i+","); &#125; &#125;&#125; 2.3 时空复杂度对具有$n$个数据元素的序列进行排序时，选择排序需要进行$n-1$趟选择。进行第$i$趟选择时，后面已经有$i-1$个数据元素排好序，第$i$趟从剩下的$n-i+1$个数据元素中选择一个关键字最大的数据元素，并将它与第$i$个数据元素交换，这样即可使后面的$i$个数据元素排好序。 选择排序的关键字比较次数与序列的初始状态无关。对n个数据元素进行排序时，第一趟的比较次数为$n-1$，第$i$趟的比较次数是$n-1$次，第$n-1$趟（最后一趟）的比较次数是1次。因此，总的比较次数为$n(n-1)/2$ 选择排序每一趟都可能移动一次数据元素，其总的移动次数与序列的初始状态有关。当序列已经排好序时，元素的移动次数为0。当每一趟都需要移动数据元素时，总的移动次数为$n-1$ 选择排序的时间复杂度为$O(n^2)$。选择排序不需要辅助的存储单元，其空间复杂度为$O(1)$。选择排序在排序过程中需要在不相邻的数据元素之间进行交换，它是一种不稳定的排序方法。 时间复杂度：$O(n^2)$空间复杂度：$O(1)$稳定性：不稳定 三、插入排序（Insertion Sort）3.1 思想对具有$n$个数据元素的序列进行排序时，插入排序需要进行$n-1$趟插入。进行第$j（1≤j≤n-1）$趟插入时，前面已经有$j$个元素排好序了，第$j$趟将$a_{j+1}$插入到已经排好序的序列中，这样即可使前面的$j+1$个数据排好序。 3.2 代码1234567891011121314151617181920public class Demo&#123; public static void insertionSort(int[] arr)&#123; for (int i=1; i&lt;arr.length; ++i)&#123; int value = arr[i]; int position=i; while (position&gt;0 &amp;&amp; arr[position-1]&gt;value)&#123; arr[position] = arr[position-1]; position--; &#125; arr[position] = value; &#125;//loop i &#125; public static void main(String[] args)&#123; int[] arr = &#123;10,30,20,60,40,50&#125;; Demo.insertionSort(arr); for(int i:arr)&#123; System.out.print(i+","); &#125; &#125;&#125; 3.3 时空复杂度直接插入排序关键字比较次数和数据元素移动次数与数据元素的初始状态有关。在最好的情况下，待排序的序列是已经排好序的，每一趟插入，只需要比较一次就可以确定待插入的数据元素的位置，需要移动两次数据元素。因此总的关键字比较次数为$n-1$,总的数据元素移动次数为$2(n-1)$ 在最坏的情况下，待排序的序列是反序的，每一趟中，待插入的数据元素需要与前面已排序序列的每一个数据元素进行比较，移动次数等于比较次数。因此，总的比较次数和移动次数都是$n(n-1)/2$ 直接插入排序的时间复杂度为$O(n^2)$。直接插入排序需要一个单元的辅助存储单元，空间复杂度为$O(1)$。直接插入排序只在相邻的数据元素之间进行交换，它是一种稳定的排序方法。 最好情况$O(n)$；最坏情况$O(n^2)$；平均时间复杂度为：$O(n^2)$空间复杂度：$O(1)$稳定性：稳定 四、希尔排序（Shell Sort）4.1 思想希尔排序，也称递减增量排序算法，实质是分组插入排序。由 Donald Shell 于1959年提出。希尔排序是非稳定排序算法。 希尔排序的基本思想是：将数组列在一个表中并对列分别进行插入排序，重复这过程，不过每次用更长的列（步长更长了，列数更少了）来进行。最后整个表就只有一列了。将数组转换至表是为了更好地理解这算法，算法本身还是使用数组进行排序。 例如，假设有这样一组数[ 13 14 94 33 82 25 59 94 65 23 45 27 73 25 39 10 ]，如果我们以步长为5开始进行排序，我们可以通过将这列表放在有5列的表中来更好地描述算法，这样他们就应该看起来是这样： 123413 14 94 33 8225 59 94 65 2345 27 73 25 3910 然后我们对每列进行排序： 123410 14 73 25 2313 27 94 33 3925 59 94 65 8245 将上述四行数字，依序接在一起时我们得到：[ 10 14 73 25 23 13 27 94 33 39 25 59 94 65 82 45 ]。这时10已经移至正确位置了，然后再以3为步长进行排序： 12345610 14 7325 23 1327 94 3339 25 5994 65 8245 排序之后变为： 12345610 14 1325 23 3327 25 5939 65 7345 94 8294 最后以1步长进行排序（此时就是简单的插入排序了）。 4.2 代码1234567891011121314151617181920public class Demo &#123; public static void main(String[] args) &#123; int[] data = new int[] &#123;10,30,20,60,40,50&#125;; shellSort(data); for(int i:data) &#123; System.out.println(i); &#125; &#125; public static void shellSort(int[] arr)&#123; int temp; for (int delta = arr.length/2; delta&gt;=1; delta/=2)&#123; //对每个增量进行一次排序 for (int i=delta; i&lt;arr.length; i++)&#123; for (int j=i; j&gt;=delta &amp;&amp; arr[j]&lt;arr[j-delta]; j-=delta)&#123; //注意每个地方增量和差值都是delta temp = arr[j-delta]; arr[j-delta] = arr[j]; arr[j] = temp; &#125; &#125;//loop i &#125;//loop delta &#125;&#125; 上面源码的步长的选择是从n/2开始，每次再减半，直至为0。步长的选择直接决定了希尔排序的复杂度。在维基百科上有对于步长串行的详细介绍。 五、归并排序（Merge Sort）5.1 思想典型的是二路合并排序，将原始数据集分成两部分(不一定能够均分)，分别对它们进行排序，然后将排序后的子数据集进行合并，这是典型的分治法策略。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Demo &#123; public static void main(String[] args) &#123; int[] data = new int[] &#123;10,30,20,60,40,50&#125;; mergesort(data); for(int i:data) &#123; System.out.println(i); &#125; &#125; public static void mergesort(int[] arr)&#123; sort(arr, 0, arr.length-1); &#125; private static void sort(int[] a, int left, int right)&#123; //当left==right的时，已经不需要再划分了 if (left&lt;right)&#123; int middle = (left+right)/2; sort(a, left, middle); //左子数组 sort(a, middle+1, right); //右子数组 merge(a, left, middle, right); //合并两个子数组 &#125; &#125; // 合并两个有序子序列 arr[left, ..., middle] 和 arr[middle+1, ..., right]。temp是辅助数组。 private static void merge(int arr[], int left, int middle, int right)&#123; int[] temp = new int[right - left + 1]; int i=left; int j=middle+1; int k=0; //将记录由小到大地放进temp数组 while ( i&lt;=middle &amp;&amp; j&lt;=right)&#123; if (arr[i] &lt;=arr[j])&#123; temp[k++] = arr[i++]; &#125; else&#123; temp[k++] = arr[j++]; &#125; &#125; while (i &lt;=middle)&#123; temp[k++] = arr[i++]; &#125; while ( j&lt;=right)&#123; temp[k++] = arr[j++]; &#125; //把数据复制回原数组 for (i=0; i&lt;k; ++i)&#123; arr[left+i] = temp[i]; &#125; &#125; &#125; 5.3 时空复杂度在归并排序中，进行一趟归并需要的关键字比较次数和数据元素移动次数最多为$n$，需要归并的趟数$log_{2}n$，故归并排序的时间复杂度为$O(nlog_{2}n)$。归并排序需要长度等于序列长度为$n$的辅助存储单元，故归并排序的空间复杂度为$O(n)$。归并排序是稳定的排序算法。 时间复杂度：$O(nlog_{2}n)$空间复杂度：$O(n)$稳定性：稳定 六、快速排序（Quick Sort）6.1 思想快速排序是图灵奖得主C.R.A Hoare于1960年提出的一种划分交换排序。它采用了一种分治的策略，通常称其为分治法（Divide-and-Conquer Method） 分治法的基本思想是：将原问题分解为若干个规模更小但结构与原问题相似的子问题。递归地解这些子问题，然后将这些子问题组合为原问题的解。 利用分治法可将快速排序分为三步： 从数列中挑出一个元素作为“基准”（pivot）。 分区过程，将比基准数大的放到右边，小于或等于它的数都放到左边。这个操作称为“分区操作”，分区操作结束后，基准元素所处的位置就是最终排序后它的位置 再对“基准”左右两边的子集不断重复第一步和第二步，直到所有子集只剩下一个元素为止。 6.2 代码12345678910111213141516171819202122232425262728293031public class Demo &#123; public static void main(String[] args) &#123; int[] data = new int[] &#123;10,30,20,60,40,50&#125;; quickSort(data); for(int i:data) &#123; System.out.println(i); &#125; &#125; public static void quickSort(int[] arr)&#123; qsort(arr, 0, arr.length-1); &#125; private static void qsort(int[] arr, int left, int right)&#123; if (left &lt; right)&#123; int pivot=partition(arr, left, right); //将数组分为两部分 qsort(arr, left, pivot-1); //递归排序左子数组 qsort(arr, pivot+1, right); //递归排序右子数组 &#125; &#125; private static int partition(int[] arr, int left, int right)&#123; int pivot = arr[left]; //基准记录 while (left&lt;right)&#123; while (left&lt;right &amp;&amp; arr[right]&gt;=pivot) --right; arr[left]=arr[right]; //交换比基准小的记录到左端 while (left&lt;right &amp;&amp; arr[left]&lt;=pivot) ++left; arr[right] = arr[left]; //交换比基准大的记录到右端 &#125; //扫描完成，基准到位 arr[left] = pivot; //返回的是基准的位置 return left; &#125;&#125; 6.3 时空复杂度 时间复杂度：最好$O(nlog_2n)$；平均$O(nlog_2n)$，最坏：$O(n^2)$空间复杂度：$O(log_2n)$稳定性：不稳定 七、堆排序（Heap Sort）7.1 思想先上一张堆排序动画演示图： 堆排序在 top K 问题中使用比较频繁。堆排序是采用二叉堆的数据结构来实现的，虽然实质上还是一维数组。堆（二叉堆）可以视为一棵完全的二叉树，完全二叉树的一个“优秀”的性质是，除了最底层之外，每一层都是满的，这使得堆可以利用数组来表示（普通的一般的二叉树通常用链表作为基本容器表示），每一个结点对应数组中的一个元素。 如下图，是一个堆和数组的相互关系 对于给定的某个结点的下标 i，可以很容易的计算出这个结点的父结点、孩子结点的下标： Parent(i) = floor(i/2)，i 的父节点下标 Left(i) = 2i，i 的左子节点下标 Right(i) = 2i + 1，i 的右子节点下标 二叉堆具有以下性质： 父节点的键值总是大于或等于（小于或等于）任何一个子节点的键值。 每个节点的左右子树都是一个二叉堆（都是最大堆或最小堆）。 二叉堆一般分为两种：最大堆和最小堆。 最大堆的定义如下： 最大堆中的最大元素值出现在根结点（堆顶） 堆中每个父节点的元素值都大于等于其孩子结点（如果存在） 最小堆的定义如下： 最小堆中的最小元素值出现在根结点（堆顶） 堆中每个父节点的元素值都小于等于其孩子结点（如果存在） 【提问】堆是怎么调整的，介绍大顶堆和小顶堆； 堆排序的原理如下： 步骤： 构造最大堆（Build_Max_Heap）：若数组下标范围为0~n，考虑到单独一个元素是大根堆，则从下标n/2开始的元素均为大根堆。于是只要从n/2-1开始，向前依次构造大根堆，这样就能保证，构造到某个节点时，它的左右子树都已经是大根堆。 堆排序（HeapSort）：由于堆是用数组模拟的。得到一个大根堆后，数组内部并不是有序的。因此需要将堆化数组有序化。思想是移除根节点，并做最大堆调整的递归运算。第一次将heap[0]与heap[n-1]交换，再对heap[0…n-2]做最大堆调整。第二次将heap[0]与heap[n-2]交换，再对heap[0…n-3]做最大堆调整。重复该操作直至heap[0]和heap[1]交换。由于每次都是将最大的数并入到后面的有序区间，故操作完后整个数组就是有序的了。 最大堆调整（Max_Heapify）：该方法是提供给上述两个过程调用的。目的是将堆的末端子节点作调整，使得子节点永远小于父节点 。 7.2 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class Demo &#123; public static void main(String[] args) &#123; int[] arr = &#123; 50, 10, 90, 30, 70, 40, 80, 60, 20 &#125;; // 堆排序 heapSort(arr); for (int i = 0; i &lt; arr.length; i++) &#123; System.out.print(arr[i] + " "); &#125; &#125; /** * 堆排序 */ private static void heapSort(int[] arr) &#123; // 将待排序的序列构建成一个大顶堆 for (int i = arr.length / 2; i &gt;= 0; i--)&#123; heapAdjust(arr, i, arr.length); &#125; // 逐步将每个最大值的根节点与末尾元素交换，并且再调整二叉树，使其成为大顶堆 for (int i = arr.length - 1; i &gt; 0; i--) &#123; swap(arr, 0, i); // 将堆顶记录和当前未经排序子序列的最后一个记录交换 heapAdjust(arr, 0, i); // 交换之后，需要重新检查堆是否符合大顶堆，不符合则要调整 &#125; &#125; /** * 构建堆的过程 * @param arr 需要排序的数组 * @param i 需要构建堆的根节点的序号 * @param n 数组的长度 */ private static void heapAdjust(int[] arr, int i, int n) &#123; int child; int father; for (father = arr[i]; leftChild(i) &lt; n; i = child) &#123; child = leftChild(i); // 如果左子树小于右子树，则需要比较右子树和父节点 if (child != n - 1 &amp;&amp; arr[child] &lt; arr[child + 1]) &#123; child++; // 序号增1，指向右子树 &#125; // 如果父节点小于孩子结点，则需要交换 if (father &lt; arr[child]) &#123; arr[i] = arr[child]; &#125; else &#123; break; // 大顶堆结构未被破坏，不需要调整 &#125; &#125; arr[i] = father; &#125; // 获取到左孩子结点 private static int leftChild(int i) &#123; return 2 * i + 1; &#125; // 交换元素位置 private static void swap(int[] arr, int index1, int index2) &#123; int tmp = arr[index1]; arr[index1] = arr[index2]; arr[index2] = tmp; &#125; &#125; 7.3 时空复杂度堆排序在建立堆和调整堆的过程中会产生比较大的开销，在元素少的时候并不适用。但是，在元素比较多的情况下，还是不错的一个选择。尤其是在解决诸如“前n大的数”一类问题时，几乎是首选算法。 八、桶排序桶排序 (Bucket sort)或所谓的箱排序的原理是将数组分到有限数量的桶子里，然后对每个桶子再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排序），最后将各个桶中的数据有序的合并起来。 排序过程： 假设待排序的一组数统一的分布在一个范围中，并将这一范围划分成几个子范围，也就是桶 将待排序的一组数，分档规入这些子桶，并将桶中的数据进行排序 将各个桶中的数据有序的合并起来 设有数组 array = [29, 25, 3, 49, 9, 37, 21, 43]，那么数组中最大数为 49，先设置 5 个桶，那么每个桶可存放数的范围为：0~9、10~19、20~29、30~39、40~49，然后分别将这些数放人自己所属的桶，如下图： 然后，分别对每个桶里面的数进行排序，或者在将数放入桶的同时用插入排序进行排序。最后，将各个桶中的数据有序的合并起来，如下图： Data Structure Visualizations 提供了一个桶排序的分步动画演示。 九、各种排序方法的时空复杂度9.1 时间复杂度 ＂快些以$O(nlog_2n)$的速度归队＂ 即快，希，归，堆都是$O(nlog_2n)$，其他都是$O(n^2)$，基数排序例外，是$O(d(n+rd))$ 9.2 空间复杂度 快排$O(log_2n)$ 归并$O(n)$ 基数$O(r_d)$ 其他$O(1)$ 9.3 稳定性 ＂心情不稳定，快些找一堆朋友聊天吧＂ 即不稳定的有：快，希，堆 9.4 其他性质 直接插入排序，初始基本有序情况下，是$O(n)$ 冒泡排序，初始基本有序情况下，是$O(n)$ 快排在初始状态越差的情况下算法效果越好． 堆排序适合记录数量比较大的时候，从n个记录中选择k个记录． 经过一趟排序，元素可以在它最终的位置的有：交换类的（冒泡，快排），选择类的（简单选择，堆） 比较次数与初始序列无关的是：简单选择与折半插入 排序趟数与原始序列有关的是：交换类的（冒泡和快排） 参考资料维基百科：希尔排序，快速排序，归并排序，堆排序排序算法可视化]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（11）：哈希表]]></title>
    <url>%2F2017%2F07%2F25%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8811%EF%BC%89%EF%BC%9A%E5%93%88%E5%B8%8C%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[一、哈希表的基本概念哈希表（Hash Table）是一种特殊的数据结构，它最大的特点就是可以快速实现查找、插入和删除。因为它独有的特点，Hash表经常被用来解决大数据问题，也因此被广大的程序员所青睐。 我们知道，数组的最大特点就是：寻址容易，插入和删除困难；而链表正好相反，寻址困难，而插入和删除操作容易。那么如果能够结合两者的优点，做出一种寻址、插入和删除操作同样快速容易的数据结构，那该有多好。这就是哈希表创建的基本思想，而实际上哈希表也实现了这样的一个“夙愿”，哈希表就是这样一个集查找、插入和删除操作于一身的数据结构。 哈希表（Hash Table）：也叫散列表，是根据关键码值（key-value）而直接进行访问的数据结构，也就是我们常用到的map。 哈希函数：也称为是散列函数，是Hash表的映射函数，它可以把任意长度的输入变换成固定长度的输出，该输出就是哈希值。哈希函数能使对一个数据序列的访问过程变得更加迅速有效，通过哈希函数，数据元素能够被很快的进行定位。 哈希表和哈希函数的标准定义：若关键字为k，则其值存放在h(k)的存储位置上。由此，不需比较便可直接取得所查记录。称这个对应关系f为哈希函数，按这个思想建立的表为哈希表。 设所有可能出现的关键字集合记为U(简称全集)。实际发生(即实际存储)的关键字集合记为K（|K|比|U|小得多）。 散列方法是使用函数h将U映射到表T[0..m-1]的下标上（m=O(|U|)）。这样以U中关键字为自变量，以h为函数的运算结果就是相应结点的存储地址。从而达到在O(1)时间内就可完成查找。其中： h：U→{0，1，2，…，m-1} ，通常称h为哈希函数(Hash Function)。哈希函数h的作用是压缩待处理的下标范围，使待处理的|U|个值减少到m个值，从而降低空间开销。 T为哈希表(Hash Table)。 $h(K_i)(K_i∈U)$是关键字为Ki结点存储地址(亦称散列值或散列地址)。 将结点按其关键字的哈希地址存储到哈希表中的过程称为散列(Hashing) 设计出一个简单、均匀、存储利用率高的散列函数是散列技术中最关键的问题。但是，一般散列函数都面临着冲突的问题。两个不同的关键字，由于散列函数值相同，因而被映射到同一表位置上。该现象称为冲突(Collision)或碰撞。发生冲突的两个关键字称为该散列函数的同义词(Synonym)。 最理想的解决冲突的方法是安全避免冲突。要做到这一点必须满足两个条件：其一是$|U|≤m$，其二是选择合适的散列函数。这只适用于|U|较小，且关键字均事先已知的情况，此时经过精心设计散列函数h有可能完全避免冲突。 但通常情况下，h是一个压缩映像。虽然$|K|≤m$，但$|U|&gt;m$，故无论怎样设计h，也不可能完全避免冲突。因此，只能在设计h时尽可能使冲突最少。同时还需要确定解决冲突的方法，使发生冲突的同义词能够存储到表中。 冲突的频繁程度除了与h相关外，还与表的填满程度相关。设m和n分别表示表长和表中填入的结点数，则将α=n/m定义为散列表的装填因子(Load Factor)。α越大，表越满，冲突的机会也越大。通常取α≤1。 二、哈希表的实现方法我们之前说了，哈希表是一个集查找、插入和删除操作于一身的数据结构。那这么完美的数据结构到底是怎么实现的呢？哈希表有很多种不同的实现方法，为了实现哈希表的创建，这些所有的方法都离不开两个问题——“定址”和“解决冲突”。 在这里，我们通过详细地介绍哈希表最常用的方法——取余法（定值）+拉链法（解决冲突），来一起窥探一下哈希表强大的优点。 取余法大家一定不会感觉陌生，就是我们经常说的取余数的操作。 拉链法是什么，“拉链”说白了就是“链表数组”。我这么一解释，大家更晕了，啥是“链表数组”啊？为了更好地解释“链表数组”，我们用下图进行解释：图中的主干部分是一个顺序存储结构数组，但是有的数组元素为空，有的对应一个值，有的对应的是一个链表，这就是“链表数组”。比如数组0的位置对应一个链表，链表有两个元素“496”和“896”，这说明元素“496”和“896”有着同样的Hash地址，这就是我们上边介绍的“冲突”或者“碰撞”。但是“链表数组”的存储方式很好地解决了Hash表中的冲突问题，发生冲突的元素会被存在一个对应Hash地址指向的链表中。实际上，“链表数组”就是一个指针数组，每一个指针指向一个链表的头结点，链表可能为空，也可能不为空。 说完这些，大家肯定已经理解了“链表数组”的概念，那我们就一起看看Hash表是如何根据“取余法+拉链法”构建的吧。 将所有关键字为同义词的结点链接在同一个链表中。若选定的散列表长度为m，则可将散列表定义为一个由m个头指针组成的指针数组$T[0..m-1]$。凡是散列地址为i的结点，均插入到以$T[i]$为头指针的单链表中。T中各分量的初值均应为空指针。在拉链法中，装填因子$α$可以大于1，但一般均取$α≤1$。 举例说明拉链法的执行过程，设有一组关键字为(26，36，41，38，44，15，68，12，6，51)，用取余法构造散列函数，初始情况如下图所示：最终结果如下图所示：理解了Hash表的创建，那根据建立的Hash表进行查找就很容易被理解了。 查找操作，如果理解了插入和删除，查找操作就比较简单了，令待查找的关键字是x，也可分为几种情况： 1）x所属的Hash地址未被占用，即不存在与x相同的Hash地址关键字，当然也不存在x了； 2）x所属的Hash地址被占用了，但它所存的关键不属于这个Hash地址，与1）相同，不存在与x相同Hash地址的关键字； 3）x所属的Hash地址被占用了，且它所存的关键属于这个Hash地址，即存在与x相同sHash地址的关键字，只是不知这个关键字是不是x，需要进一步查找。 由此可见，Hash表插入、删除和插入操作的效率都相当的高。 思考一个问题：如果关键字是字符串怎么办？我们怎么根据字符串建立Hash表？ 通常都是将元素的key转换为数字进行散列，如果key本身就是整数，那么散列函数可以采用keymod tablesize（要保证tablesize是质数）。而在实际工作中经常用字符串作为关键字，例如身姓名、职位等等。这个时候需要设计一个好的散列函数进程处理关键字为字符串的元素。参考《数据结构与算法分析》第5章，有以下几种处理方法： 方法1：将字符串的所有的字符的ASCII码值进行相加，将所得和作为元素的关键字。此方法的缺点是不能有效的分布元素，例如假设关键字是有8个字母构成的字符串，散列表的长度为10007。字母最大的ASCII码为127，按照方法1可得到关键字对应的最大数值为127×8=1016，也就是说通过散列函数映射时只能映射到散列表的槽0-1016之间，这样导致大部分槽没有用到，分布不均匀，从而效率低下。 方法2：假设关键字至少有三个字母构成，散列函数只是取前三个字母进行散列。该方法只是取字符串的前三个字符的ASCII码进行散列，最大的得到的数值是2851，如果散列的长度为10007，那么只有28%的空间被用到，大部分空间没有用到。因此如果散列表太大，就不太适用。 方法3：借助Horner’s 规则，构造一个质数（通常是37）的多项式，（非常的巧妙，不知道为何是37）。计算公式为:$key[keysize-i-1]*37^i, 0≤i&lt;keysize$求和。该方法存在的问题是如果字符串关键字比较长，散列函数的计算过程就变长，有可能导致计算的hashVal溢出。针对这种情况可以采取字符串的部分字符进行计算，例如计算偶数或者奇数位的字符。 三、哈希表定址与解决冲突3.1 哈希表“定址的方法”其实常用的“定址”的手法有“五种”： 直接定址法：很容易理解，key=Value+C；这个“C”是常量。Value+C其实就是一个简单的哈希函数。 除法取余法：key=value%C 数字分析法：这种蛮有意思，比如有一组value1=112233，value2=112633，value3=119033，针对这样的数我们分析数中间两个数比较波动，其他数不变。那么我们取key的值就可以是key1=22,key2=26,key3=90。 平方取中法 折叠法：举个例子，比如value=135790，要求key是2位数的散列值。那么我们将value变为13+57+90=160，然后去掉高位“1”,此时key=60，哈哈，这就是他们的哈希关系，这样做的目的就是key与每一位value都相关，来做到“散列地址”尽可能分散的目地。 影响哈希查找效率的一个重要因素是哈希函数本身。当两个不同的数据元素的哈希值相同时，就会发生冲突。为减少发生冲突的可能性，哈希函数应该将数据尽可能分散地映射到哈希表的每一个表项中。 3.2 哈希表“解决冲突”的方法Hash表解决冲突的方法主要有以下几种： 链接地址法：将哈希值相同的数据元素存放在一个链表中，在查找哈希表的过程中，当查找到这个链表时，必须采用线性查找方法。 开放定址法：如果两个数据元素的哈希值相同，则在哈希表中为后插入的数据元素另外选择一个表项。当程序查找哈希表时，如果没有在第一个对应的哈希表项中找到符合查找要求的数据元素，程序就会继续往后查找，直到找到一个符合查找要求的数据元素，或者遇到一个空的表项。线性探测带来的最大问题就是冲突的堆积，你把别人预定的坑占了，别人也就要像你一样去找坑。改进的办法有二次方探测法和随机数探测法。开放地址法包括线性探测、二次探测以及双重散列等方法。其中线性探测法示意图如下：散列过程如下图所示： 再散列函数法：发生冲突时就换一个散列函数计算，总会有一个可以把冲突解决掉，它能够使得关键字不产生聚集，但相应地增加了计算的时间。 公共溢出区法：其实就是为所有的冲突，额外开辟一块存储空间。如果相对基本表而言，冲突的数据很少的时候，使用这种方法比较合适。 3.3 哈希表“定址”和“解决冲突”之间的权衡虽然哈希表是在关键字和存储位置之间建立了对应关系，但是由于冲突的发生，哈希表的查找仍然是一个和关键字比较的过程，不过哈希表平均查找长度比顺序查找要小得多，比二分查找也小。 查找过程中需和给定值进行比较的关键字个数取决于下列三个因素：哈希函数、处理冲突的方法和哈希表的装填因子。 哈希函数的”好坏”首先影响出现冲突的频繁程度，但如果哈希函数是均匀的，则一般不考虑它对平均查找长度的影响。 对同一组关键字，设定相同的哈希函数，但使用不同的冲突处理方法，会得到不同的哈希表，它们的平均查找长度也不同。 一般情况下，处理冲突方法相同的哈希表，其平均查找长度依赖于哈希表的装填因子α。显然，α越小，产生冲突的机会就越大；但α过小，空间的浪费就过多。通过选择一个合适的装填因子α，可以将平均查找长度限定在一个范围内。 总而言之，哈希表“定址”和“解决冲突”之间的权衡决定了哈希表的性能。 四、实现Hashmap假设我们要设计的是一个用来保存某大学所有在校学生个人信息的数据表。因为在校学生数量也不是特别巨大(8W?)，每个学生的学号是唯一的,因此，我们可以简单的应用直接定址法，声明一个10W大小的数组，每个学生的学号作为主键。然后每次要添加或者查找学生，只需要根据需要去操作即可。 但是，显然这样做是很脑残的。这样做系统的可拓展性和复用性就非常差了，比如有一天人数超过10W了？如果是用来保存别的数据呢？或者我只需要保存20条记录呢？声明大小为10W的数组显然是太浪费了的。 如果我们是用来保存大数据量（比如银行的用户数，4大的用户数都应该有3-5亿了吧？），这时候我们计算出来的HashCode就很可能会有冲突了， 我们的系统应该有“处理冲突”的能力，此处我们通过挂链法“处理冲突”。 如果我们的数据量非常巨大，并且还持续在增加，如果我们仅仅只是通过挂链法来处理冲突，可能我们的链上挂了上万个数据后，这个时候再通过静态搜索来查找链表，显然性能也是非常低的。所以我们的系统应该还能实现自动扩容，当容量达到某比例后，即自动扩容，使装载因子保存在一个固定的水平上。 综上所述，我们对这个Hash容器的基本要求应该有如下几点： 满足Hash表的查找要求 能支持从小数据量到大数据量的自动转变（自动扩容） 使用挂链法解决冲突 代码详见hashmap源码剖析]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>哈希表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（10）：查找]]></title>
    <url>%2F2017%2F07%2F24%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8810%EF%BC%89%EF%BC%9A%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[一、基本概念查找（Search）就是根据给定的某个值，在查找表中确定一个其关键字等于给定值的数据元素（或记录）。在计算机应用中，查找是常用的基本运算，例如编译程序中符号表的查找。先说明几个概念： 查找表（Search Table）：由同一类型的数据元素（或记录）构成的集合关键字（Key）：数据元素中某个数据项的值，又称为键值。主键（Primary Key）：可唯一地标识某个数据元素或记录的关键字。 平均查找长度（Average Search Length，ASL）：需和指定key进行比较的关键字的个数的期望值，称为查找算法在查找成功时的平均查找长度。 对于含有n个数据元素的查找表，查找成功的平均查找长度为：$ASL = P_i*C_i$的和。 $P_i$：查找表中第i个数据元素的概率。 $C_i$：找到第i个数据元素时已经比较过的次数。 查找表按照操作方式可分为： 静态查找表（Static Search Table）:只做查找操作的查找表。它的主要操作是： 查询某个“特定的”数据元素是否在表中 检索某个“特定的”数据元素和各种属性 动态查找表（Dynamic Search Table）：在查找的同时进行插入或删除等操作： 查找时插入数据 查找时删除数据 按照查找表是否有序分为无序查找和有序查找： 无序查找：被查找数列有序无序均可； 有序查找：被查找数列必须为有序数列。 二、无序表查找说明：顺序查找适合于存储结构为顺序存储或链接存储的线性表。 基本思想：顺序查找也称为线形查找，属于无序查找算法。从数据结构线形表的一端开始，顺序扫描，依次将扫描到的结点关键字与给定值k相比较，若相等则表示查找成功；若扫描结束仍没有找到关键字等于k的结点，表示查找失败。 算法分析：最好情况是在第一个位置就找到了，此为O(1)；最坏情况是在最后一个位置才找到，此为O(n)；所以平均查找次数为$(n+1)/2$，最终时间复杂度为$O(n)$ 1234567891011121314# 最基础的遍历无序列表的查找算法# 时间复杂度O(n)def sequential_search(lis, key): length = len(lis) for i in range(length): if lis[i] == key: return i else: return Falseif __name__ == '__main__': LIST = [1, 5, 8, 123, 22, 54, 7, 99, 300, 222] result = sequential_search(LIST, 123) print(result) 三、有序表查找查找表中的数据必须按照某个主键进行某种排序！ 3.1 二分查找说明：元素必须是有序的，如果是无序的则要先进行排序操作。基本思想：也称为折半查找，属于有序查找算法。用给定值k先与中间结点的关键字比较，中间结点把线性表分成两个子表，若相等则查找成功；若不相等，再根据k与该中间节点关键字的比较结果确定下一步查找哪个字表，这样递归进行，知道查找到或查找结束发现表中没有这样的结点。 复杂度分析：最坏情况下,关键字比较次数为$log_2(n+1)$，且期望时间复杂度为$O(log_2n)$ 注意：折半查找的前提条件是需要有序表顺序存储，对于静态查找表，一次排序后不再发生变化，折半查找能得到不错的效率。但对于需要频繁执行插入或删除操作的数据集来说，维护有序的排序会带来不小的工作量，那就不建议使用。 12345678910111213141516171819202122232425# 针对有序查找表的二分查找算法# 时间复杂度O(log(n))def binary_search(list, key): low = 0 high = len(list) - 1 time = 0 while low &lt;= high: time += 1 mid = int((low + high) / 2) if key &lt; list[mid]: high = mid - 1 elif key &gt; list[mid]: low = mid + 1 else: # 打印折半的次数 print("times: %s" % time) return mid print("times: %s" % time) return -1if __name__ == '__main__': LIST = [1, 5, 7, 8, 22, 54, 99, 123, 200, 222, 444] result = binary_search(LIST, 99) print(result) 3.2 插值查找在介绍插值查找之前，首先考虑一个新问题，为什么上述算法一定要是折半，而不是折四分之一或者折更多呢？ 打个比方，在英文字典里面查“apple”，你下意识翻开字典是翻前面的书页还是后面的书页呢？如果再让你查“zoo”，你又怎么查？很显然，这里绝对不会是从中农间开始查起，而是有一定目的的往前或往后翻。 同样的，比如要在取值范围1~10000之间100个元素从小到大均匀分布的数组中查找5，我们自然会从数组下标较小的开始查找。 经过上面的分析，折半查找这种查找方式，不是自适应的（也就是说是傻瓜式的）。二分查找中查找点计算如下： mid=(low+high)/2即 mid=low+(high-low)/2通过类比，我们可以将查找的点改进为如下： mid=low+\frac{key -list[low]}{list[high]-list [low]}\times (high-low)也就是将上述的比例参数1/2改进为自适应的，根据关键字在整个有序表中所处的位置，让mid值的变化更靠近关键字key，这样也就间接地减少了比较次数。 基本思想：基于二分查找算法，将查找点的选择改进为自适应选择，可以提高查找效率。当然，插值查找也属于有序查找。 注意：对于表长较大，而关键字分布又比较均匀的查找表来说，插值查找算法的平均性能比折半查找要好的多。反之，数组中如果分布非常不均匀，那么插值查找未必是很合适的选择。 复杂度分析：查找成功或者失败的时间复杂度均为$O(log2(log2n))$。 123456789101112131415161718192021222324252627# 插值查找算法# 时间复杂度O(log(n))def binary_search(lis, key): low = 0 high = len(lis) - 1 time = 0 while low &lt;= high: time += 1 # 计算mid值是插值算法的核心代码 mid = low + int((high - low) * (key - lis[low])/(lis[high] - lis[low])) print("mid=%s, low=%s, high=%s" % (mid, low, high)) if key &lt; lis[mid]: high = mid - 1 elif key &gt; lis[mid]: low = mid + 1 else: # 打印查找的次数 print("times: %s" % time) return mid print("times: %s" % time) return -1if __name__ == '__main__': LIST = [1, 5, 7, 8, 22, 54, 99, 123, 200, 222, 444] result = binary_search(LIST, 444) print(result) 3.3 斐波那契查找在介绍斐波那契查找算法之前，我们先介绍一下和它很紧密相连并且大家都熟知的一个概念——黄金分割。 黄金比例又称为黄金分割，是指事物各部分间一定的数学比例关系，即将整体一分为二，较大部分与较小部分之比等于整体与较大部分之比，其比值约为1：0.618。 0.618倍公认为是最具有审美意义的比例数字，这个数值的作用不仅仅体现在诸如绘画、雕塑、音乐、建筑等艺术领域，而且在管理、工程设计等方面有着不可忽视的作用。因此被称为黄金分割。 大家记不记得斐波那契数列：1，1，2，3，5，8，13，21，34，55，89……（从第三个数开始，后面每一个数都是前两个数的和）。然后我们会发现，随着斐波那契数列的递增，前后两个数的比值会越来越接近0.618，利用这个特性，我们就可以将黄金比例运用到查找技术中。 基本思想：也是二分查找的一种提升算法，通过运用黄金比例的概念在数列中选择查找点进行查找，提高查找效率。同样地，斐波那契查找也属于一种有序查找算法。 相对于折半查找，一般将待比较的key值与第mid=（low+high）/2位置的元素进行比较，比较结果分为三种情况： 相等，mid位置的元素即为所求 ,low=mid+1 &lt;,high=mid-1 斐波那契查找和折半查找很相似，它是根据斐波那契序列的特点对有序表进行分割的。他要求开始表中记录的个数为某个斐波那契小1，即$n=F(k)-1$ 开始将k值与第F(k-1)位置的记录进行比较（即mid=low+F(k-1)-1），比较结果也分为三种 相等，mid位置的元素即为所求 ,low=mid+1,k-=2：说明，low=high+1说明待查找的元素在[mid+1,high]范围内，k-=2说明范围[mid,high]内的元素个数为$n-(F(k-1))=Fk-1-F(k-1)=Fk-F(k-1)-1=f(k-2)-1$个，所以可以递归地应用斐波那契查找。 &lt;,high=mid-1,k-=1:说明，low=mid+1，说明待查找的元素在[mid+1,high]范围内，k-=1说明范围[low,mid-1]内的元素个数为F(k-1)-1个，所以可以递归的应用斐波那契查找 复杂度分析：最坏情况下，时间复杂度为$O(log_2n)$，且其期望复杂度也为$O(log_2n)$。就平均性能，要优于二分查找。但是在最坏情况下，比如这里如果key为1，则始终处于左侧半区查找，此时其效率要低于二分查找。 总结：二分查找的mid运算是加法与除法，插值查找则是复杂的四则运算，而斐波那契查找只是最简单的加减运算。在海量数据的查找中，这种细微的差别可能会影响最终的查找效率。因此，三种有序表的查找方法本质上是分割点的选择不同，各有优劣，应根据实际情况进行选择。 四、线性索引查找对于海量的无序数据，为了提高查找速度，一般会为其构造索引表。索引就是把一个关键字与他相对应的记录进行关联的过程。 一个索引由若干个索引项构成，每个索引项至少包含关键字和其对应的记录在存储器中的位置等信息。 索引按照结构可以分为：线性索引、树形索引和多级索引。线性索引：将索引项的集合通过线性结构来组织，也叫索引表 线性索引可分为：稠密索引、分块索引和倒排索引。 稠密索引： 指的是在线性索引中，为数据集合中的每个记录都建立一个索引项。这其实就相当于给无序的集合，建立了一张有序的线性表，其索引项一定是按照关键码进行有序的排列。这也相当于把查找过程中需要的排序工作给提前做了。 分块索引： 分块查找又称索引顺序查找，是顺序查找的一种改进方法。 算法思想：将n个数据元素“按块有序”划分为m块（m&lt;n）。每一块中的结点不必有序，但块与块之间必须“按块有序”；即第一块中人艺元素的关键字都必须小于第2块中任一元素的关键字；而第二块中任一元素又都必须小于第三块中的任一元素，…… 算法流程：首先选取各块中的最大关键字构成一个索引表；查找分为两个部分：先对索引表进行二分查找或顺序查找，以确定待查记录在哪一块中；然后，在已确定的块中用顺序法进行查找。 这其实是有序查找和无序查找的一种中间状态或者说妥协状态。因为数据量过大，建立完整的稠密索引耗时耗力，占用资源过多；但如果不做任何排序或索引，那么遍历的查找也无法接受，只能这种，做一定程度的排序或索引。分块索引的效率比遍历查找的$O(n)$要高一些，但与二分查找的$O(logn)$还是要差不少。 倒排索引：不是由记录来确定属性值，而是由属性值来确定记录的位置，这种被称为倒排索引。其中记录号表存储具有相同关键字的所有记录的地址或引用（可以是指向记录的指针或该记录的主关键字）。倒排索引是最基础的搜索引擎技术。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>查找算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（30）：Scikit-Learn总结]]></title>
    <url>%2F2017%2F07%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8830%EF%BC%89%EF%BC%9AScikit-Learn%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[http://ff120.github.io/2017/05/14/机器学习专题/机器学习_Scikit-Learn使用技巧/Scikit-learn是一个很受欢迎的机器学习方面的python工具包，它定义的一些范式和处理流程影响深远，所以，了解这个工具包对于机器学习算法的整个流程会有一个整体的了解。它已经实现了很多方法帮助我们便捷的处理数据，例如，划分数据集为训练集和验证集，交叉验证，数据预处理，归一化等等。 一、性能评价指标12345678910111213141516# 计算均方误差from sklearn import metricsimport numpy as nprmse = np.sqrt(metrics.mean_squared_error(y_test,y_pred))# 计算准确率acc = metrics.accuracy_score(y_test,y_pred)#混淆矩阵cm = metrics.confusion_matrix(y_test,y_pred)# classification_reportcr = metrics.classification_report(y_true,y_pred)# ROC AUC 曲线from sklearn.metrics import roc_curve,auc 二、数据集划分123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from sklearn import cross_validationX_train,X_test,y_train,y_test = cross_validation.train_test_split(X,y,test_size=0.3,random_state=0)# K折from sklearn.cross_validation import KFoldkf = KFold(n_samples, n_folds=2)for train, test in kf: print(&quot;%s %s&quot; % (train, test))# 保证不同的类别之间的均衡，这里需要用到标签labelsfrom sklearn.cross_validation import StratifiedKFoldlabels = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]skf = StratifiedKFold(labels, 3)for train, test in skf: print(&quot;%s %s&quot; % (train, test)) # 留一交叉验证from sklearn.cross_validation import LeaveOneOutloo = LeaveOneOut(n_samples)for train, test in loo: print(&quot;%s %s&quot; % (train, test))# 留P交叉验证from sklearn.cross_validation import LeavePOutlpo = LeavePOut(n_samples, p=2)for train, test in lpo: print(&quot;%s %s&quot; % (train, test)) # 按照额外提供的标签留一交叉验证,常用的情况是按照时间序列from sklearn.cross_validation import LeaveOneLabelOutlabels = [1, 1,1, 2, 2]lolo = LeaveOneLabelOut(labels)for train, test in lolo: print(&quot;%s %s&quot; % (train, test)) # 按照额外提供的标签留P交叉验证from sklearn.cross_validation import LeavePLabelOutlabels = [1, 1, 2, 2, 3, 3,3]lplo = LeavePLabelOut(labels, p=2)for train, test in lplo: print(&quot;%s %s&quot; % (train, test))# 随机分组from sklearn.cross_validation import ShuffleSplitss = ShuffleSplit(16, n_iter=3, test_size=0.25,random_state=0)for train_index, test_index in ss: print(&quot;%s %s&quot; % (train_index, test_index))# 考虑类别均衡的随机分组from sklearn.cross_validation import StratifiedShuffleSplitimport numpy as npX = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])y = np.array([0, 0, 1, 1])sss = StratifiedShuffleSplit(y, 3, test_size=0.5, random_state=0)for train, test in sss: print(&quot;%s %s&quot; % (train, test)) 三、特征选择12345678910111213141516171819202122232425262728293031323334353637383940# 基于方差的特征选择from sklearn import feature_selectionvt = feature_selection.VarianceThreshold(threshold=&apos;&apos;)vt.fit(X_train)X_train_transformed = vt.transform(X_train)X_test_transformed = vt.transform(X_test)# 按照某种排序规则 选择前K个特征# 除了使用系统定义好的函数f_classif，还可以自己定义函数sk = SelectKBest(feature_selection.f_classif,k=100)sk.fit(X_train,y_train)X_train_transformed = sk.transform(X_train)X_test_transformed = sk.transform(X_test)# 递归特征消除rfecv = RFECV(estimator=svc, step=step, cv=StratifiedKFold(y, n_folds = n_folds),scoring=&apos;accuracy&apos;)rfecv.fit(X_train, y_train)X_train_transformed = rfecv.transform(X_train)X_test_transformed = rfecv.transform(y_train)# 使用L1做特征选择from sklearn.svm import LinearSVClsvc = LinearSVC(C=1, penalty=&quot;l1&quot;, dual=False)lsvc.fit(X_train,y_train)X_train_transformed = lsvc.transform(X_train)X_test_transformed = lsvc.transform(y_train)# 基于树的特征选择from sklearn.ensemble import ExtraTreesClassifieretc = ExtraTreesClassifier()etc.fit(X_train, y_train)X_train_transformed = etc.transform(X_train)X_test_transformed = etc.transform(X_test)# 基于线性判别分析做特征选择from sklearn.discriminant_analysis import LinearDiscriminantAnalysislda = LinearDiscriminantAnalysis(solver=&apos;lsqr&apos;,shrinkage=&apos;auto&apos;)lda.fit(X_train, y_train)X_train_transformed = lda.transform(X_train)X_test_transformed = lda.transform(X_test) 基于方差的特征选择 123456789101112from sklearn.feature_selection import VarianceThresholdX = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]sel = VarianceThreshold(threshold=(.8 * (1 - .8)))X2 = sel.fit_transform(X)X2Out[5]: array([[0, 1], [1, 0], [0, 0], [1, 1], [1, 0], [1, 1]]) 可以看到，方差小于0.16的只有第一维特征，所以X2保留下来的是原来的第二维和第三维特征。这应该是最简单的特征选择方法了：假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用，而且实际当中，一般不太会有95%以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。 Univariate feature selection （单变量特征选择） 主要使用统计的方法计算各个统计值，再根据一定的阈值筛选出符合要求的特征，去掉不符合要求的特征。 主要的统计方法有 F值分类： f_classif 值回归：f_regression 卡方统计：chi2 (适用于非负特征值和稀疏特征值) 主要的选择策略 选择排名前K的特征：SelectKbest 选择前百分之几的特征：SelectPercentile SelectFpr：Select features based on a false positive rate test. SelectFdr：Select features based on an estimated false discovery rate. SelectFwe：Select features based on family-wise error rate. GenericUnivariateSelect：Univariate feature selector with configurable mode. 其中 false positive rate：FP / (FP + TP) 假设类别为0，1；记0为negative,1为positive, FPR就是实际的类别是0，但是分类器错误的预测为1的个数 与 分类器预测的类别为1的样本的总数（包括正确的预测为1和错误的预测为1） 的比值。 estimated false discovery rate: 错误的拒绝原假设的概率； family-wise error rate: 至少有一个检验犯第一类错误的概率；假设检验的两类错误： &gt; - 第一类错误：原假设是正确的，但是却被拒绝了。(用α表示） &gt; - 第二类错误：原假设是错误的，但是却被接受了。(用β表示)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>机器学习</tag>
        <tag>Scikit-Learn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（9）：Trie树]]></title>
    <url>%2F2017%2F07%2F23%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%889%EF%BC%89%EF%BC%9ATrie%E6%A0%91%2F</url>
    <content type="text"><![CDATA[Trie树是一种非常重要的数据结构，它在信息检索，字符串匹配等领域有广泛的应用，同时，它也是很多算法和复杂数据结构的基础，如后缀树，AC自动机等，因此，掌握Trie树这种数据结构，对于一名IT人员，显得非常基础且必要！ 一、什么是Trie树Trie树，又叫字典树、前缀树（Prefix Tree）、单词查找树或键树，是一种多叉树结构。 字典树（Trie）可以保存一些字符串-&gt;值的对应关系。基本上，它跟 Java 的 HashMap 功能相同，都是 key-value 映射，只不过 Trie 的 key 只能是字符串。是一种哈希树的变种 如下图： 上图是一棵Trie树，表示了关键字集合{“a”,”to”,”tea”,”ted”,”ten”,”i”,”in”,”inn”}。从上图可以归纳出Trie树的基本性质： 根节点不包含字符，除节结点外的每一个节点都包含一个字符。 从根节点到某一个节点，路径上经过的字符连接起来，为该节点对应的字符串。 每个节点的所有子节点包含的字符互不相同。 通常在实现的时候，会在节点结构中设置一个标志，用来标记该结点处是否构成一个单词（关键字）。 可以看出，Trie树的关键字一般都是字符串，而且Trie树把每个关键字保存在一条路径上，而不是一个结点中。另外，两个有公共前缀的关键字，在Trie树中前缀部分的路径相同，所以Trie树又叫做前缀树（Prefix Tree）。 二、Trie的优缺点Trie的核心思想是空间换时间。利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。 2.1 优点Trie的强大之处就在于它的时间复杂度，插入和查询的效率很高，都为$O(K)$，其中 $K$ 是待插入/查询的字符串的长度，而与Trie中保存了多少个元素无关。 关于查询，会有人说 hash 表时间复杂度是$O(1)$不是更快？但是，哈希搜索的效率通常取决于 hash 函数的好坏，若一个坏的 hash 函数导致很多的冲突，效率并不一定比Trie树高。 而Trie树中不同的关键字就不会产生冲突。它只有在允许一个关键字关联多个值的情况下才有类似hash碰撞发生。 此外，Trie树不用求 hash 值，对短字符串有更快的速度。因为通常，求hash值也是需要遍历字符串的。 Trie树可以对关键字按字典序排序。 2.2 缺点当然，当 hash 函数很好时，Trie树的查找效率会低于哈希搜索。 其次因为Trie的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。所以它的空间消耗比较大。 三、Trie树的应用典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：最大限度地减少无谓的字符串比较，查询效率比哈希表高。 3.1 字符串检索给出N个单词组成的熟词表，以及一篇全用小写英文书写的文章，请你按最早出现的顺序写出所有不在熟词表中的生词。 检索/查询功能是Trie树最原始的功能。给定一组字符串，查找某个字符串是否出现过，思路就是从根节点开始一个一个字符进行比较： 如果沿路比较，发现不同的字符，则表示该字符串在集合中不存在。 如果所有的字符全部比较完并且全部相同，还需判断最后一个节点的标志位（标记该节点是否代表一个关键字）。 12345struct trie_node&#123; bool isKey; // 标记该节点是否代表一个关键字 trie_node *children[26]; // 各个子节点 &#125;; 3.2 词频统计Trie树常被搜索引擎系统用于文本词频统计 。 12345struct trie_node&#123; int count; // 记录该节点代表的单词的个数 trie_node *children[26]; // 各个子节点 &#125;; 思路：为了实现词频统计，我们修改了节点结构，用一个整型变量count来计数。对每一个关键字执行插入操作，若已存在，计数加1，若不存在，插入后count置1。 3.3 排序Trie树可以对大量字符串按字典序进行排序，思路也很简单：给定N个互不相同的仅由一个单词构成的英文名，让你将他们按字典序从小到大输出。用字典树进行排序，采用数组的方式创建字典树，这棵树的每个结点的所有儿子很显然地按照其字母大小排序。对这棵树进行先序遍历即可。 3.4 前缀匹配例如：找出一个字符串集合中所有以ab开头的字符串。我们只需要用所有字符串构造一个trie树，然后输出以$a-&gt;b-&gt;$开头的路径上的关键字即可。 trie树前缀匹配常用于搜索提示。如当输入一个网址，可以自动搜索出可能的选择。当没有完全匹配的搜索结果，可以返回前缀最相似的可能 3.5 最长公共前缀查找一组字符串的最长公共前缀，只需要将这组字符串构建成Trie树，然后从跟节点开始遍历，直到出现多个节点为止（即出现分叉）。 举例说明：给出N 个小写英文字母串，以及Q 个询问，即询问某两个串的最长公共前缀的长度是多少？ 解决方案：首先对所有的串建立其对应的字母树。此时发现，对于两个串的最长公共前缀的长度即它们所在结点的公共祖先个数，于是，问题就转化为了离线（Offline）的最近公共祖先（Least Common Ancestor，简称LCA）问题。而最近公共祖先问题同样是一个经典问题，可以用下面几种方法： 利用并查集（Disjoint Set），可以采用采用经典的Tarjan 算法； 求出字母树的欧拉序列（Euler Sequence ）后，就可以转为经典的最小值查询（Range Minimum Query，简称RMQ）问题了； 关于并查集，Tarjan算法，RMQ问题，网上有很多资料。 3.6 作为辅助结构如后缀树，AC自动机等。 3.7 应用实例 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。 之前在此文：海量数据处理面试题集锦与Bit-map详解中给出的参考答案：用trie树统计每个词出现的次数，时间复杂度是$O(nle)$（le表示单词的平均长度），然后是找出出现最频繁的前10个词。也可以用堆来实现（具体的操作可参考第三章、寻找最小的k个数），时间复杂度是$O(nlg10)$。所以总的时间复杂度，是$O(nle)$与$O(nlg10)$中较大的哪一个。 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。 1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？ 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。 寻找热门查询：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录，这些查询串的重复读比较高，虽然总数是1千万，但是如果去除重复和，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就越热门。请你统计最热门的10个查询串，要求使用的内存不能超过1G。 (1) 请描述你解决这个问题的思路； (2) 请给出主要的处理流程，算法，以及算法的复杂度。 四、Trie树的实现Trie树的插入、删除、查找的操作都是一样的，只需要简单的对树进行一遍遍历即可，时间复杂度：O（n）（n是字符串的长度）。 trie树每一层的节点数是$26^i$级别的。所以为了节省空间，对于Tried树的实现可以使用数组和链表两种方式。空间的花费，不会超过单词数×单词长度。 数组：由于我们知道一个Tried树节点的子节点的数量是固定26个（针对不同情况会不同，比如兼容数字，则是36等），所以可以使用固定长度的数组来保存节点的子节点 优点：在对子节点进行查找时速度快 缺点：浪费空间，不管子节点有多少个，总是需要分配26个空间 链表：使用链表的话我们需要在每个子节点中保存其兄弟节点的链接，当我们在一个节点的子节点中查找是否存在一个字符时，需要先找到其子节点，然后顺着子节点的链表从左往右进行遍历 优点：节省空间，有多少个子节点就占用多少空间，不会造成空间浪费 缺点：对子节点进行查找相对较慢，需要进行链表遍历，同时实现也较数组麻烦 java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169import java.util.ArrayList;import java.util.List;/** * 单词查找树 */class Trie &#123; /** 单词查找树根节点，根节点为一个空的节点 */ private Vertex root = new Vertex(); /** 单词查找树的节点(内部类) */ private class Vertex &#123; /** 单词出现次数统计 */ int wordCount; /** 以某个前缀开头的单词，它的出现次数 */ int prefixCount; /** 子节点用数组表示 */ Vertex[] vertexs = new Vertex[26]; /** * 树节点的构造函数 */ public Vertex() &#123; wordCount = 0; prefixCount = 0; &#125; &#125; /** * 单词查找树构造函数 */ public Trie() &#123; &#125; /** * 向单词查找树添加一个新单词 * * @param word * 单词 */ public void addWord(String word) &#123; addWord(root, word.toLowerCase()); &#125; /** * 向单词查找树添加一个新单词 * * @param root * 单词查找树节点 * @param word * 单词 */ private void addWord(Vertex vertex, String word) &#123; if (word.length() == 0) &#123; vertex.wordCount++; &#125; else if (word.length() &gt; 0) &#123; vertex.prefixCount++; char c = word.charAt(0); int index = c - 'a'; if (null == vertex.vertexs[index]) &#123; vertex.vertexs[index] = new Vertex(); &#125; addWord(vertex.vertexs[index], word.substring(1)); &#125; &#125; /** * 统计某个单词出现次数 * * @param word * 单词 * @return 出现次数 */ public int countWord(String word) &#123; return countWord(root, word); &#125; /** * 统计某个单词出现次数 * * @param root * 单词查找树节点 * @param word * 单词 * @return 出现次数 */ private int countWord(Vertex vertex, String word) &#123; if (word.length() == 0) &#123; return vertex.wordCount; &#125; else &#123; char c = word.charAt(0); int index = c - 'a'; if (null == vertex.vertexs[index]) &#123; return 0; &#125; else &#123; return countWord(vertex.vertexs[index], word.substring(1)); &#125; &#125; &#125; /** * 统计以某个前缀开始的单词，它的出现次数 * * @param word * 前缀 * @return 出现次数 */ public int countPrefix(String word) &#123; return countPrefix(root, word); &#125; /** * 统计以某个前缀开始的单词，它的出现次数(前缀本身不算在内) * * @param root * 单词查找树节点 * @param word * 前缀 * @return 出现次数 */ private int countPrefix(Vertex vertex, String prefixSegment) &#123; if (prefixSegment.length() == 0) &#123; return vertex.prefixCount; &#125; else &#123; char c = prefixSegment.charAt(0); int index = c - 'a'; if (null == vertex.vertexs[index]) &#123; return 0; &#125; else &#123; return countPrefix(vertex.vertexs[index], prefixSegment.substring(1)); &#125; &#125; &#125; /** * 调用深度递归算法得到所有单词 * @return 单词集合 */ public List&lt;String&gt; listAllWords() &#123; List&lt;String&gt; allWords = new ArrayList&lt;String&gt;(); return depthSearchWords(allWords, root, ""); &#125; /** * 递归生成所有单词 * @param allWords 单词集合 * @param vertex 单词查找树的节点 * @param wordSegment 单词片段 * @return 单词集合 */ private List&lt;String&gt; depthSearchWords(List&lt;String&gt; allWords, Vertex vertex, String wordSegment) &#123; Vertex[] vertexs = vertex.vertexs; for (int i = 0; i &lt; vertexs.length; i++) &#123; if (null != vertexs[i]) &#123; if (vertexs[i].wordCount &gt; 0) &#123; allWords.add(wordSegment + (char)(i + 'a')); if(vertexs[i].prefixCount &gt; 0)&#123; depthSearchWords(allWords, vertexs[i], wordSegment + (char)(i + 'a')); &#125; &#125; else &#123; depthSearchWords(allWords, vertexs[i], wordSegment + (char)(i + 'a')); &#125; &#125; &#125; return allWords; &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; Trie trie = new Trie(); trie.addWord("abc"); trie.addWord("abcd"); trie.addWord("abcde"); trie.addWord("abcdef"); System.out.println(trie.countPrefix("abc")); System.out.println(trie.countWord("abc")); System.out.println(trie.listAllWords()); &#125;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>Trie树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（29）：Sparsity and Some Basics of L1 Regularization]]></title>
    <url>%2F2017%2F07%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8829%EF%BC%89%EF%BC%9ASparsity%20and%20Some%20Basics%20of%20L1%20Regularization%2F</url>
    <content type="text"><![CDATA[转载自pluskid的个人博客Sparsity 是当今机器学习领域中的一个重要话题。John Lafferty 和 Larry Wasserman 在 2006 年的一篇评论中提到： Some current challenges … are high dimensional data, sparsity, semi-supervised learning, the relation between computation and risk, and structured prediction. —John Lafferty and Larry Wasserman. Challenges in statistical machine learning. Statistica Sinica. Volume 16, Number 2, pp. 307-323, 2006. Sparsity 的最重要的“客户”大概要属 high dimensional data 了吧。现在的机器学习问题中，具有非常高维度的数据随处可见。例如，在文档或图片分类中常用的 bag of words 模型里，如果词典的大小是一百万，那么每个文档将由一百万维的向量来表示。高维度带来的的一个问题就是计算量：在一百万维的空间中，即使计算向量的内积这样的基本操作也会是非常费力的。不过，如果向量是稀疏的的话（事实上在 bag of words 模型中文档向量通常都是非常稀疏的），例如两个向量分别只有$L_1$和$L_2$个非零元素，那么计算内积可以只使用$min(L_1,L_2 )$次乘法完成。因此稀疏性对于解决高维度数据的计算量问题是非常有效的。 当然高维度带来的问题不止是在计算量上。例如在许多生物相关的问题中，数据的维度非常高，但是由于收集数据需要昂贵的实验，因此可用的训练数据却相当少，这样的问题通常称为“small , large problem”—我们一般用 表示数据点的个数，用 表示变量的个数，即数据维度。当$p&gt;&gt;n$的时候，不做任何其他假设或者限制的话，学习问题基本上是没法进行的。因为如果用上所有变量的话，$p$越大，通常会导致模型越复杂，但是反过来$n$又很小，于是就会出现很严重的 overfitting 问题。例如，最简单的线性回归模型： f(X)=\sum_{j=1}^p使用square loss来学习的话，就变成最小化如下的问题： J(w)=\frac{1}{n}\sum_{i=1}^n(y_i-f(x_i))^2=\frac{1}{n}||y-Xw||^2这里$X=(x_1,······,x_n)^T \in R^{n \times p}$是数据矩阵，而$y=(y_1,······,y_n)^T$是由标签组成的列向量。该问题具有解析解$\hat{w}=(X^TX)^{-1}X^Ty$然而，如果$p&gt;n$的话，矩阵$X^TX$将会不是满秩的，而这个解也没法算出来。捉着更确切地说，将会有无穷多个解。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解随机选一个的话，很可能并不是很好地解，总而言之，我们过拟合了。 解决 overfitting 最常用的办法就是 regularization ，例如著名的 ridge regression 就是添加一个 $\ell_2 $regularizer ： J_R(w)=\frac{1}{n}||y-Xw||^2+\lambda ||w||^2直观地看，添加这个regularizer会使得模型的解偏向于norm较小的w。从凸优化的角度来说，最小化上面这个$J(w)$等价于如下问题： \underset{w}{min}\frac{1}{n}||y-Xw||^2其中$C$和$\lambda$对应的是个常数。也就是说，也就是说，我们通过限制$w$的norm的大小实现了对模型空间的限制，从而在一定程度上（取决于$\lambda$的大小）避免了overfitting。不过ridge regression并不具有产生稀疏解的能力，得到的系数$w$仍然需要数据中的所有特征才能计算预测结果，从计算量上来说并没有得到改观。 不过，特别是在像生物或者医学等通常需要和人交互的领域，稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。比如说，一个病如果依赖于 5 个变量的话，将会更易于医生理解、描述和总结规律，但是如果依赖于 5000 个变量的话，基本上就超出人肉可处理的范围了。 在这里引入稀疏性的方法是用$L_1$regularization 代替 $L_2$regularization，得到如下的目标函数： J_L(w)=\frac{1}{n}||y-Xw||^2+\lambda ||w||_1该问题通常被称为LASSO（least absolute shrinkage and selection operator）。LASSO 仍然是一个 convex optimization 问题，不过不再具有解析解。它的优良性质是能产生稀疏性，导致$w$中许多项变成零。 可是，为什么它能产生稀疏性呢？这也是一直让我挺感兴趣的一个问题，事实上在之前申请学校的时候一次电话面试中我也被问到了这个问题。我当时的回答是背后的理论我并不是很清楚，但是我知道一个直观上的理解。下面我们就先来看一下这个直观上的理解。 首先，和 ridge regression 类似，上面形式的 LASSO 问题也等价于如下形式： \underset {w}{min }\frac{1}{n}||y-Xw||^2, \ \ \ s.t. ||w||_1≤C也就是说，我们将模型空间限制在$w$的一个 $\ell_1$-ball中。为了便于可视化，我们考虑两维的情况，在 $(w^1,w^2)$平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为$C$的一个 norm ball 。等高线与 norm ball 首次相交的地方就是最优解。如图所示： 可以看到，$\ell_1-ball$与$\ell_2-ball$的不同就在于他和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置为产生稀疏性，例如图中的相交点就有$w^1=0$ ，而更高维的时候（想象一下三维的 $\ell_1 $-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。 相比之下，$\ell_2 $-ball 就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么$\ell_1$ regularization 能产生稀疏性，而$\ell_2$regularization 不行的原因了。 不过，如果只限于 intuitive 的解释的话，就不那么好玩了，但是背后完整的理论又不是那么容易能够搞清楚的，既然这次的标题是 Basics ，我们就先来看一个简单的特殊情况好了。 接下来我们考虑 orthonormal design 的情况：$\frac{1}{n}X^TX=I$，然后看看LASSO的解具体是什么样子。注意orthonormal design 实际上是要求特征之间相互正交。这可以通过对数据进行 PCA以及模长 normalize 来实现。 注意到LASSO 的目标函数是 convex 的，根据 KKT 条件，在最优解的地方要求 gradient 。不过这里有一点小问题：$\ell_1$ -norm 不是光滑的，不存在 gradient ，所以我们需要用一点 subgradient 的东西。 定义：（subgradient，subdifferential）.对于在$p$维欧式空间中的凸开子集$U$上定义的实值函数$f:U \rightarrow R$，一个向量$p$维向量$v$称为$f$在一点$x_0 \in U$处的subgradient，如果对于任意$x \in U$，满足 f(x)-f(x_0)≥v·(x-x_0)由在点$x_0$处的所有subgradient所组成的集合称为$x_0$处的subdifferential，记为$\partial f(x_0)$ 注意 subgradient 和 subdifferential 只是对凸函数定义的。例如一维的情况，$f(x)=|x|$，在$x=0$处的subdifferential 就是$[-1,+1]$这个区间（集合）。注意在$f$的 gradient 存在的点，subdifferential 将是由 gradient 构成的一个单点集合。这样就将 gradient 的概念加以推广了。这个推广有一个很好的性质。 性质（CONDITION GLOBAL MINIMIZER）.点$x_0$是凸函数$f$的一个全局最小值点，当且仅当$0\in \partial f(x_0)$ 证明很简单，将$0\in \partial f(x_0)$带入定义的那个式子就可以得到。有了这个工具之后，就可以对 LASSO 的最优解进行分析了。在此之前，我们先看一下原始的 least square 问题的最优解现在变成了什么样子，由于 orthonormal design ，我们有 \hat{w}=\frac{1}{n}X^Ty然后我们再来看LASSO，假设$\bar{w}=(\bar{w}^1,······，\bar{w}^p)^T$是$J_L(w)$的全局最优值点。考虑第$j$个变量$\bar{w}^j$，有两种情况。 第一种情况：gradient存在，此时$\bar{w}^j≠0$ 由于gradient在最小值点必须等于零，我们有 \frac{\partial J_L(w)}{\partial w_j}|_{\bar{w}_j}=0亦即 -\frac{2}{n}(X^Ty-X^TX\bar{w})_j+\lambda sign(\bar{w}^j)=0根据orthonormal design性质以及least square问题在orthonormal design时的解$\hat{w}^j$化简得到 \bar{w}^j=\hat{w}^j-\frac{\lambda}{2}sign(\bar{w}^j)从这个式子也可以明显地看出$\bar{w}^j$和$\hat{w}^j$是同号的，于是$sign(\bar{w}^j)=sign(\hat{w}^j)$所以上面的式子变为 \bar{w}^j=\hat{w}^j-\frac{\lambda}{2}sign(\bar{w}^j)=sign(\hat{w}^j)(|\hat{w}^j|-\frac{\lambda}{2})再用一次$sign(\bar{w}^j)=sign(\hat{w}^j)$，两边同时乘以$sign(\bar{w}^j)$，可以得到 |\hat{w}^j|-\frac{\lambda}{2}=|\bar{w}^j|≥0于是刚才的式子可以进一步写成 \bar{w}^j=sign(\hat{w}^j)(|\hat{w}^j|-\frac{\lambda}{2})_+这里$(x)_+=max \{x,0\}$表示$x$ 的正部。 第二种情况：gradient不存在，此时$\bar{w}^j=0$根据subgradient在最小值点出的性质，此时有： 0=\bar{w}^j\in \partial {J_L(\bar{w})}=\{-\frac{2}{n}(X^Ty-X^TX\bar{w})_j+\lambda e:e \in [-1,1]\}亦即存在$e_0 \in [-1,1]$使得 0=2\bar{w}^j-2\hat{w}^j+\lambda e_0于是 |\hat{w}^j|=\frac{\lambda}{2}|e_0|≤\frac{\lambda}{2}又因为$\bar{w}^j=0$，所以这个时候式子也可以统一为 \bar{w}^j=sign(\hat{w}^j)(|\hat{w}^j|-\frac{\lambda}{2})_+的形式。 如此一来，在 orthonormal design 的情况下，LASSO 的最优解就可以写为 \bar{w}^j=sign(\hat{w}^j)(|\hat{w}^j|-\frac{\lambda}{2})_+可以用图形象地表达出来。 图上画了原始的 least square 解，LASSO 的解以及 ridge regression 的解，用上面同样的方法（不过由于 ridge regularizer 是 smooth 的，所以过程却简单得多）可以得知 ridge regression 的解是如下形式 \frac{n}{1+n\lambda}\hat{w}^j可以认为ridge regression 只是做了一个全局缩放，而 LASSO 则是做了一个 soft thresholding ：将绝对值小于$\frac{\lambda}{2}$的那些系数直接变成零了，这也就更加令人信服地解释了 LASSO 为何能够产生稀疏解了。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>L1正则</tag>
        <tag>稀疏性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（8）：红黑树]]></title>
    <url>%2F2017%2F07%2F22%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%888%EF%BC%89%EF%BC%9A%E7%BA%A2%E9%BB%91%E6%A0%91%2F</url>
    <content type="text"><![CDATA[红黑树，即R-B Tree，本文的主要内容包括：红黑树的特性、红黑树的时间复杂度和它的证明，红黑树的时间复杂度和它的证明，红黑树的左旋、右旋、插入、删除等操作 一、红黑树的定义1.1 红黑树的定义R-B Tree，全称是Red-black Tree，又称为“红黑树”，它是一种特殊的二叉查找树。红黑树的每个节点上都有存储位表示节点的颜色，可以是红（Red）或黑（Black） 红黑树是一种自平衡二叉查找树，是在计算机科学在用到的一种数据结构，典型的用途是实现关联数组。它是在1972年由鲁道夫.贝尔发明的，称之为“对称二叉B树”，它的现代名字是在 Leo J. Guibas 和 Robert Sedgewick 于1978年写的一篇论文中获得的。它是复杂的，但它的操作有着良好的最坏情况运行时间，并且在实践中是高效的：他可以在$O(logn)$的时间内做查找、插入和删除，这里的n是树中元素的数目。 红黑树是每个节点都带有颜色属性的二叉查找树，颜色为红色或黑色。在二叉查找树强制的一般要求以外，对于任何有效的红黑树我们增加了如下的额外要求: 性质1：节点是红色或黑色 性质2：根节点是黑色 性质3：所有叶子节点都是黑色（叶子是NIL节点） 性质4：每个红色节点必须有两个黑色的子节点。(从每个叶子到根的所有路径上不能有两个连续的红色节点。) 性质5.：从任一节点到其每个叶子的所有简单路径都包含相同数目的黑色节点。 注意： 性质3中的叶子节点，是只为空(NIL或null)的节点。 性质5确保从根到叶子的最长的可能路径不多于最短的可能路径的两倍长，因而，红黑树是相对是接近平衡的二叉树。因为操作比如插入、删除和查找某个值的最坏情况时间都要求与树的高度成比例，这个在高度上的理论上限允许红黑树在最坏情况下都是高效的，而不同于普通的二叉查找树。 要知道为什么这些性质确保了这个结果，注意到性质4导致了路径不能有两个毗连的红色节点就足够了。最短的可能路径都是黑色节点，最长的可能路径有交替的红色和黑色节点。因为根据性质5所有最长的路径都有相同数目的黑色节点，这就表明了没有路径能多于任何其他路径的两倍长。 【提问】请解释红黑性，以及为什么每次操作都是稳定O(logn)的复杂度 下面是一个具体的红黑树的图例： 1.2 红黑树的应用红黑树的应用比较广泛，主要是用它来存储有序的数据，它的时间复杂度是$O(lgn)$，效率非常之高。 例如，Java集合中的$TreeSet$和$TreeMap$，C++的STL中的Set、Map，以及Linux虚拟内存的管理，都是通过红黑树去实现的。 二、红黑树的基本操作：左旋和右旋红黑树的基本操作是添加、删除。在对红黑树进行添加或删除之后，都会用到旋转方法。为什么呢？道理很简单，添加或删除红黑树中的节点之后，红黑树就发生了变化，可能不满足红黑树的5条性质，也就不再是一颗红黑树了，而是一颗普通的树。而通过旋转，可以使这颗树重新成为红黑树。简单点说，旋转的目的是让树保持红黑树的特性。 恢复红黑树的性质需要少量(O(logn))的颜色变更(实际是非常快速的)和不超过三次树旋转(对于插入操作是两次)。虽然插入和删除很复杂，但操作时间仍可以保持为O(logn) 次。 旋转包括两种：左旋 和 右旋。下面分别对它们进行介绍。 2.1 左旋对X进行左旋，意味着“将X变成一个左节点”。 左旋的伪代码《算法导论》：参考上面的示意图和下面的伪代码，理解“红黑树T的节点x进行左旋”是如何进行的。 123456789101112LEFT-ROTATE(T, x) 01 y ← right[x] // 前提：这里假设x的右孩子为y。下面开始正式操作02 right[x] ← left[y] // 将 “y的左孩子” 设为 “x的右孩子”，即 将β设为x的右孩子03 p[left[y]] ← x // 将 “x” 设为 “y的左孩子的父亲”，即 将β的父亲设为x04 p[y] ← p[x] // 将 “x的父亲” 设为 “y的父亲”05 if p[x] = nil[T] 06 then root[T] ← y // 情况1：如果 “x的父亲” 是空节点，则将y设为根节点07 else if x = left[p[x]] 08 then left[p[x]] ← y // 情况2：如果 x是它父节点的左孩子，则将y设为“x的父节点的左孩子”09 else right[p[x]] ← y // 情况3：(x是它父节点的右孩子) 将y设为“x的父节点的右孩子”10 left[y] ← x // 将 “x” 设为 “y的左孩子”11 p[x] ← y // 将 “x的父节点” 设为 “y” 理解左旋之后，看看下面更鲜明的例子。 2.2 右旋对Y进行右旋，意味着“将Y变成一个右节点” 右旋的伪代码《算法导论》：参考上面的示意图和下面的伪代码，理解“红黑树T的节点y进行右旋”是如何进行的。 123456789101112RIGHT-ROTATE(T, y) 01 x ← left[y] // 前提：这里假设y的左孩子为x。下面开始正式操作02 left[y] ← right[x] // 将 “x的右孩子” 设为 “y的左孩子”，即 将β设为y的左孩子03 p[right[x]] ← y // 将 “y” 设为 “x的右孩子的父亲”，即 将β的父亲设为y04 p[x] ← p[y] // 将 “y的父亲” 设为 “x的父亲”05 if p[y] = nil[T] 06 then root[T] ← x // 情况1：如果 “y的父亲” 是空节点，则将x设为根节点07 else if y = right[p[y]] 08 then right[p[y]] ← x // 情况2：如果 y是它父节点的右孩子，则将x设为“y的父节点的左孩子”09 else left[p[y]] ← x // 情况3：(y是它父节点的左孩子) 将x设为“y的父节点的左孩子”10 right[x] ← y // 将 “y” 设为 “x的右孩子”11 p[y] ← x // 将 “y的父节点” 设为 “x” 理解右旋之后，看看下面一个更鲜明的例子。 综上，左旋和右旋是相对的两个概念，原理类似。理解一个也就理解了另一个。 在实际应用中，若没有彻底理解左旋和右旋，可能会将它们混淆。下面谈谈我对如何区分左旋 和右旋 的理解。 2.3 区分左旋和右旋仔细观察上面”左旋”和”右旋”的示意图。我们能清晰的发现，它们是对称的。无论是左旋还是右旋，被旋转的树，在旋转前是二叉查找树，并且旋转之后仍然是一颗二叉查找树。 左旋示例图(以x为节点进行左旋)： 12345 z x / / \ --(左旋)--&gt; xy z / y 对x进行左旋，意味着，将“x的右孩子”设为“x的父亲节点”；即，将 x变成了一个左节点(x成了为z的左孩子)！。 因此，左旋中的“左”，意味着“被旋转的节点将变成一个左节点”。 右旋示例图(以x为节点进行右旋)： 12345 y x \ / \ --(右旋)--&gt; xy z \ z 对x进行右旋，意味着，将“x的左孩子”设为“x的父亲节点”；即，将 x变成了一个右节点(x成了为y的右孩子)！ 因此，右旋中的“右”，意味着“被旋转的节点将变成一个右节点”。 三、红黑树的基本操作：添加将一个节点插入到红黑树中，需要执行哪些步骤呢？首先，将红黑树当作一颗二叉查找树，将节点插入；然后，将节点着色为红色；最后，通过旋转和重新着色等方法来修正该树，使之重新成为一颗红黑树。详细描述如下： 第一步: 将红黑树当作一颗二叉查找树，将节点插入。 红黑树本身就是一颗二叉查找树，将节点插入后，该树仍然是一颗二叉查找树。也就意味着，树的键值仍然是有序的。此外，无论是左旋还是右旋，若旋转之前这棵树是二叉查找树，旋转之后它一定还是二叉查找树。这也就意味着，任何的旋转和重新着色操作，都不会改变它仍然是一颗二叉查找树的事实。 好吧？那接下来，我们就来想方设法的旋转以及重新着色，使这颗树重新成为红黑树！ 第二步：将插入的节点着色为”红色”。 为什么着色成红色，而不是黑色呢？为什么呢？在回答之前，我们需要重新温习一下红黑树的特性： 每个节点或者是黑色，或者是红色。 根节点是黑色。 每个叶子节点是黑色。 [注意：这里叶子节点，是指为空的叶子节点！] 如果一个节点是红色的，则它的子节点必须是黑色的。 从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。 将插入的节点着色为红色，不会违背”特性(5)”！少违背一条特性，就意味着我们需要处理的情况越少。接下来，就要努力的让这棵树满足其它性质即可；满足了的话，它就又是一颗红黑树了。o(∩∩)o…哈哈 第三步: 通过一系列的旋转或着色等操作，使之重新成为一颗红黑树。 第二步中，将插入节点着色为”红色”之后，不会违背”特性(5)”。那它到底会违背哪些特性呢？ 对于”特性(1)”，显然不会违背了。因为我们已经将它涂成红色了。 对于”特性(2)”，显然也不会违背。在第一步中，我们是将红黑树当作二叉查找树，然后执行的插入操作。而根据二叉查找数的特点，插入操作不会改变根节点。所以，根节点仍然是黑色。 对于”特性(3)”，显然不会违背了。这里的叶子节点是指的空叶子节点，插入非空节点并不会对它们造成影响。 对于”特性(4)”，是有可能违背的！ 那接下来，想办法使之”满足特性(4)”，就可以将树重新构造成红黑树了。 下面看看代码到底是怎样实现这三步的。 添加操作的伪代码《算法导论》 123456789101112131415161718RB-INSERT(T, z) y ← nil[T] // 新建节点“y”，将y设为空节点。 x ← root[T] // 设“红黑树T”的根节点为“x” while x ≠ nil[T] // 找出要插入的节点“z”在二叉树T中的位置“y” do y ← x if key[z] &lt; key[x] then x ← left[x] else x ← right[x] p[z] ← y // 设置 “z的父亲” 为 “y” if y = nil[T] then root[T] ← z // 情况1：若y是空节点，则将z设为根 else if key[z] &lt; key[y] then left[y] ← z // 情况2：若“z所包含的值” &lt; “y所包含的值”，则将z设为“y的左孩子” else right[y] ← z // 情况3：(“z所包含的值” &gt;= “y所包含的值”)将z设为“y的右孩子” left[z] ← nil[T] // z的左孩子设为空 right[z] ← nil[T] // z的右孩子设为空。至此，已经完成将“节点z插入到二叉树”中了。 color[z] ← RED // 将z着色为“红色” RB-INSERT-FIXUP(T, z) // 通过RB-INSERT-FIXUP对红黑树的节点进行颜色修改以及旋转，让树T仍然是一颗红黑树 结合伪代码以及为代码上面的说明，先理解RB-INSERT。理解了RB-INSERT之后，我们接着对 RB-INSERT-FIXUP的伪代码进行说明。 添加修正操作的伪代码《算法导论》 1234567891011121314151617RB-INSERT-FIXUP(T, z)while color[p[z]] = RED // 若“当前节点(z)的父节点是红色”，则进行以下处理。 do if p[z] = left[p[p[z]]] // 若“z的父节点”是“z的祖父节点的左孩子”，则进行以下处理。 then y ← right[p[p[z]]] // 将y设置为“z的叔叔节点(z的祖父节点的右孩子)” if color[y] = RED // Case 1条件：叔叔是红色 then color[p[z]] ← BLACK ▹ Case 1 // (01) 将“父节点”设为黑色。 color[y] ← BLACK ▹ Case 1 // (02) 将“叔叔节点”设为黑色。 color[p[p[z]]] ← RED ▹ Case 1 // (03) 将“祖父节点”设为“红色”。 z ← p[p[z]] ▹ Case 1 // (04) 将“祖父节点”设为“当前节点”(红色节点) else if z = right[p[z]] // Case 2条件：叔叔是黑色，且当前节点是右孩子 then z ← p[z] ▹ Case 2 // (01) 将“父节点”作为“新的当前节点”。 LEFT-ROTATE(T, z) ▹ Case 2 // (02) 以“新的当前节点”为支点进行左旋。 color[p[z]] ← BLACK ▹ Case 3 // Case 3条件：叔叔是黑色，且当前节点是左孩子。(01) 将“父节点”设为“黑色”。 color[p[p[z]]] ← RED ▹ Case 3 // (02) 将“祖父节点”设为“红色”。 RIGHT-ROTATE(T, p[p[z]]) ▹ Case 3 // (03) 以“祖父节点”为支点进行右旋。 else (same as then clause with &quot;right&quot; and &quot;left&quot; exchanged) // 若“z的父节点”是“z的祖父节点的右孩子”，将上面的操作中“right”和“left”交换位置，然后依次执行。color[root[T]] ← BLACK 根据被插入节点的父节点的情况，可以将”当节点z被着色为红色节点，并插入二叉树”划分为三种情况来处理。 第一种情况说明：被插入的节点是根节点。处理方法：直接把此节点涂为黑色。 第二种情况说明：被插入的节点的父节点是黑色。处理方法：什么也不需要做。节点被插入后，仍然是红黑树。 第三种情况说明：被插入的节点的父节点是红色。处理方法：那么，该情况与红黑树的“特性(5)”相冲突。这种情况下，被插入节点是一定存在非空祖父节点的；进一步的讲，被插入节点也一定存在叔叔节点(即使叔叔节点为空，我们也视之为存在，空节点本身就是黑色节点)。理解这点之后，我们依据”叔叔节点的情况”，将这种情况进一步划分为3种情况(Case)。 上面三种情况(Case)处理问题的核心思路都是：将红色的节点移到根节点；然后，将根节点设为黑色。下面对它们详细进行介绍。 Case 1：叔叔是红色 现象说明：当前节点(即，被插入节点)的父节点是红色，且当前节点的祖父节点的另一个子节点（叔叔节点）也是红色。 处理策略 将“父节点”设为黑色。 将“叔叔节点”设为黑色。 将“祖父节点”设为“红色”。 将“祖父节点”设为“当前节点”(红色节点)；即，之后继续对“当前节点”进行操作。 下面谈谈为什么要这样处理。(建议理解的时候，通过下面的图进行对比) “当前节点”和“父节点”都是红色，违背“特性(4)”。所以，将“父节点”设置“黑色”以解决这个问题。 但是，将“父节点”由“红色”变成“黑色”之后，违背了“特性(5)”：因为，包含“父节点”的分支的黑色节点的总数增加了1。 解决这个问题的办法是：将“祖父节点”由“黑色”变成红色，同时，将“叔叔节点”由“红色”变成“黑色”。关于这里，说明几点： 第一，为什么“祖父节点”之前是黑色？这个应该很容易想明白，因为在变换操作之前，该树是红黑树，“父节点”是红色，那么“祖父节点”一定是黑色。 第二，为什么将“祖父节点”由“黑色”变成红色，同时，将“叔叔节点”由“红色”变成“黑色”；能解决“包含‘父节点’的分支的黑色节点的总数增加了1”的问题。这个道理也很简单。“包含‘父节点’的分支的黑色节点的总数增加了1” 同时也意味着 “包含‘祖父节点’的分支的黑色节点的总数增加了1”，既然这样，我们通过将“祖父节点”由“黑色”变成“红色”以解决“包含‘祖父节点’的分支的黑色节点的总数增加了1”的问题； 但是，这样处理之后又会引起另一个问题“包含‘叔叔’节点的分支的黑色节点的总数减少了1”，现在我们已知“叔叔节点”是“红色”，将“叔叔节点”设为“黑色”就能解决这个问题。 所以，将“祖父节点”由“黑色”变成红色，同时，将“叔叔节点”由“红色”变成“黑色”；就解决了该问题。 按照上面的步骤处理之后：当前节点、父节点、叔叔节点之间都不会违背红黑树特性，但祖父节点却不一定。若此时，祖父节点是根节点，直接将祖父节点设为“黑色”，那就完全解决这个问题了；若祖父节点不是根节点，那我们需要将“祖父节点”设为“新的当前节点”，接着对“新的当前节点”进行分析。 Case 2：叔叔是黑色，且当前节点是右孩子 现象说明：当前节点(即，被插入节点)的父节点是红色，叔叔节点是黑色，且当前节点是其父节点的右孩子 处理策略 将“父节点”作为“新的当前节点”。 以“新的当前节点”为支点进行左旋。 下面谈谈为什么要这样处理。(建议理解的时候，通过下面的图进行对比) 首先，将“父节点”作为“新的当前节点”；接着，以“新的当前节点”为支点进行左旋。 为了便于理解，我们先说明第(02)步，再说明第(01)步；为了便于说明，我们设置“父节点”的代号为F(Father)，“当前节点”的代号为S(Son)。 为什么要“以F为支点进行左旋”呢？根据已知条件可知：S是F的右孩子。而之前我们说过，我们处理红黑树的核心思想：将红色的节点移到根节点；然后，将根节点设为黑色。既然是“将红色的节点移到根节点”，那就是说要不断的将破坏红黑树特性的红色节点上移(即向根方向移动)。 而S又是一个右孩子，因此，我们可以通过“左旋”来将S上移！ 按照上面的步骤(以F为支点进行左旋)处理之后：若S变成了根节点，那么直接将其设为“黑色”，就完全解决问题了；若S不是根节点，那我们需要执行步骤(01)，即“将F设为‘新的当前节点’”。那为什么不继续以S为新的当前节点继续处理，而需要以F为新的当前节点来进行处理呢？这是因为“左旋”之后，F变成了S的“子节点”，即S变成了F的父节点；而我们处理问题的时候，需要从下至上(由叶到根)方向进行处理；也就是说，必须先解决“孩子”的问题，再解决“父亲”的问题；所以，我们执行步骤(01)：将“父节点”作为“新的当前节点”。 Case 3：叔叔是黑色，且当前节点是左孩子 现象说明：当前节点(即，被插入节点)的父节点是红色，叔叔节点是黑色，且当前节点是其父节点的左孩子 处理策略 将“父节点”设为“黑色”。 将“祖父节点”设为“红色”。 以“祖父节点”为支点进行右旋。 下面谈谈为什么要这样处理。(建议理解的时候，通过下面的图进行对比) 为了便于说明，我们设置“当前节点”为S(Original Son)，“兄弟节点”为B(Brother)，“叔叔节点”为U(Uncle)，“父节点”为F(Father)，祖父节点为G(Grand-Father)。 S和F都是红色，违背了红黑树的“特性(4)”，我们可以将F由“红色”变为“黑色”，就解决了“违背‘特性(4)’”的问题；但却引起了其它问题：违背特性(5)，因为将F由红色改为黑色之后，所有经过F的分支的黑色节点的个数增加了1。那我们如何解决“所有经过F的分支的黑色节点的个数增加了1”的问题呢？ 我们可以通过“将G由黑色变成红色”，同时“以G为支点进行右旋”来解决。 提示：上面的进行Case 3处理之后，再将节点”120”当作当前节点，就变成了Case 2的情况。 四、红黑树的基本操作：删除将红黑树内的某一个节点删除。需要执行的操作依次是：首先，将红黑树当作一颗二叉查找树，将该节点从二叉查找树中删除；然后，通过”旋转和重新着色”等一系列来修正该树，使之重新成为一棵红黑树。详细描述如下： 第一步：将红黑树当作一颗二叉查找树，将节点删除。 这和”删除常规二叉查找树中删除节点的方法是一样的”。分3种情况： 被删除节点没有儿子，即为叶节点。那么，直接将该节点删除就OK了。 被删除节点只有一个儿子。那么，直接删除该节点，并用该节点的唯一子节点顶替它的位置。 被删除节点有两个儿子。那么，先找出它的后继节点；然后把“它的后继节点的内容”复制给“该节点的内容”；之后，删除“它的后继节点”。在这里，后继节点相当于替身，在将后继节点的内容复制给”被删除节点”之后，再将后继节点删除。这样就巧妙的将问题转换为”删除后继节点”的情况了，下面就考虑后继节点。 在”被删除节点”有两个非空子节点的情况下，它的后继节点不可能是双子非空。既然”的后继节点”不可能双子都非空，就意味着”该节点的后继节点”要么没有儿子，要么只有一个儿子。若没有儿子，则按”情况① “进行处理；若只有一个儿子，则按”情况2”进行处理。 第二步：通过”旋转和重新着色”等一系列来修正该树，使之重新成为一棵红黑树。 因为”第一步”中删除节点之后，可能会违背红黑树的特性。所以需要通过”旋转和重新着色”来修正该树，使之重新成为一棵红黑树。 删除操作的伪代码《算法导论》 12345678910111213141516171819RB-DELETE(T, z)if left[z] = nil[T] or right[z] = nil[T] then y ← z // 若“z的左孩子” 或 “z的右孩子”为空，则将“z”赋值给 “y”； else y ← TREE-SUCCESSOR(z) // 否则，将“z的后继节点”赋值给 “y”。if left[y] ≠ nil[T] then x ← left[y] // 若“y的左孩子” 不为空，则将“y的左孩子” 赋值给 “x”； else x ← right[y] // 否则，“y的右孩子” 赋值给 “x”。p[x] ← p[y] // 将“y的父节点” 设置为 “x的父节点”if p[y] = nil[T] then root[T] ← x // 情况1：若“y的父节点” 为空，则设置“x” 为 “根节点”。 else if y = left[p[y]] then left[p[y]] ← x // 情况2：若“y是它父节点的左孩子”，则设置“x” 为 “y的父节点的左孩子” else right[p[y]] ← x // 情况3：若“y是它父节点的右孩子”，则设置“x” 为 “y的父节点的右孩子”if y ≠ z then key[z] ← key[y] // 若“y的值” 赋值给 “z”。注意：这里只拷贝z的值给y，而没有拷贝z的颜色！！！ copy y&apos;s satellite data into z if color[y] = BLACK then RB-DELETE-FIXUP(T, x) // 若“y为黑节点”，则调用return y 结合伪代码以及为代码上面的说明，先理解RB-DELETE。理解了RB-DELETE之后，接着对 RB-DELETE-FIXUP的伪代码进行说明 123456789101112131415161718192021222324RB-DELETE-FIXUP(T, x)while x ≠ root[T] and color[x] = BLACK do if x = left[p[x]] then w ← right[p[x]] // 若 “x”是“它父节点的左孩子”，则设置 “w”为“x的叔叔”(即x为它父节点的右孩子) if color[w] = RED // Case 1: x是“黑+黑”节点，x的兄弟节点是红色。(此时x的父节点和x的兄弟节点的子节点都是黑节点)。 then color[w] ← BLACK ▹ Case 1 // (01) 将x的兄弟节点设为“黑色”。 color[p[x]] ← RED ▹ Case 1 // (02) 将x的父节点设为“红色”。 LEFT-ROTATE(T, p[x]) ▹ Case 1 // (03) 对x的父节点进行左旋。 w ← right[p[x]] ▹ Case 1 // (04) 左旋后，重新设置x的兄弟节点。 if color[left[w]] = BLACK and color[right[w]] = BLACK // Case 2: x是“黑+黑”节点，x的兄弟节点是黑色，x的兄弟节点的两个孩子都是黑色。 then color[w] ← RED ▹ Case 2 // (01) 将x的兄弟节点设为“红色”。 x ← p[x] ▹ Case 2 // (02) 设置“x的父节点”为“新的x节点”。 else if color[right[w]] = BLACK // Case 3: x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的左孩子是红色，右孩子是黑色的。 then color[left[w]] ← BLACK ▹ Case 3 // (01) 将x兄弟节点的左孩子设为“黑色”。 color[w] ← RED ▹ Case 3 // (02) 将x兄弟节点设为“红色”。 RIGHT-ROTATE(T, w) ▹ Case 3 // (03) 对x的兄弟节点进行右旋。 w ← right[p[x]] ▹ Case 3 // (04) 右旋后，重新设置x的兄弟节点。 color[w] ← color[p[x]] ▹ Case 4 // Case 4: x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的右孩子是红色的。(01) 将x父节点颜色 赋值给 x的兄弟节点。 color[p[x]] ← BLACK ▹ Case 4 // (02) 将x父节点设为“黑色”。 color[right[w]] ← BLACK ▹ Case 4 // (03) 将x兄弟节点的右子节设为“黑色”。 LEFT-ROTATE(T, p[x]) ▹ Case 4 // (04) 对x的父节点进行左旋。 x ← root[T] ▹ Case 4 // (05) 设置“x”为“根节点”。 else (same as then clause with &quot;right&quot; and &quot;left&quot; exchanged) // 若 “x”是“它父节点的右孩子”，将上面的操作中“right”和“left”交换位置，然后依次执行。color[x] ← BLACK 下面对删除函数进行分析。在分析之前，我们再次温习一下红黑树的几个特性： 每个节点或者是黑色，或者是红色。 根节点是黑色。 每个叶子节点是黑色。 [注意：这里叶子节点，是指为空的叶子节点！] 如果一个节点是红色的，则它的子节点必须是黑色的。 从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。 前面我们将”删除红黑树中的节点”大致分为两步，在第一步中”将红黑树当作一颗二叉查找树，将节点删除”后，可能违反”特性(2)、(4)、(5)”三个特性。第二步需要解决上面的三个问题，进而保持红黑树的全部特性。 为了便于分析，我们假设”x包含一个额外的黑色”(x原本的颜色还存在)，这样就不会违反”特性(5)”。为什么呢？ 通过RB-DELETE算法，我们知道：删除节点y之后，x占据了原来节点y的位置。 既然删除y(y是黑色)，意味着减少一个黑色节点；那么，再在该位置上增加一个黑色即可。这样，当我们假设”x包含一个额外的黑色”，就正好弥补了”删除y所丢失的黑色节点”，也就不会违反”特性(5)”。 因此，假设”x包含一个额外的黑色”(x原本的颜色还存在)，这样就不会违反”特性(5)”。 现在，x不仅包含它原本的颜色属性，x还包含一个额外的黑色。即x的颜色属性是”红+黑”或”黑+黑”，它违反了”特性(1)”。 现在，我们面临的问题，由解决”违反了特性(2)、(4)、(5)三个特性”转换成了”解决违反特性(1)、(2)、(4)三个特性”。RB-DELETE-FIXUP需要做的就是通过算法恢复红黑树的特性(1)、(2)、(4)。RB-DELETE-FIXUP的思想是：将x所包含的额外的黑色不断沿树上移(向根方向移动)，直到出现下面的姿态： x指向一个”红+黑”节点。此时，将x设为一个”黑”节点即可。 x指向根。此时，将x设为一个”黑”节点即可。 非前面两种姿态。 将上面的姿态，可以概括为3种情况。 第一种情况说明：x是“红+黑”节点。处理方法：直接把x设为黑色，结束。此时红黑树性质全部恢复。 第二种情况说明：x是“黑+黑”节点，且x是根。处理方法：什么都不做，结束。此时红黑树性质全部恢复。 第三种情况说明：x是“黑+黑”节点，且x不是根。处理方法：这种情况又可以划分为4种子情况。这4种子情况如下表所示： Case 1：x是”黑+黑”节点，x的兄弟节点是红色 现象说明：x是”黑+黑”节点，x的兄弟节点是红色。(此时x的父节点和x的兄弟节点的子节点都是黑节点)。 处理策略 将x的兄弟节点设为“黑色”。 将x的父节点设为“红色”。 对x的父节点进行左旋。 左旋后，重新设置x的兄弟节点。 下面谈谈为什么要这样处理。(建议理解的时候，通过下面的图进行对比) 这样做的目的是将“Case 1”转换为“Case 2”、“Case 3”或“Case 4”，从而进行进一步的处理。对x的父节点进行左旋；左旋后，为了保持红黑树特性，就需要在左旋前“将x的兄弟节点设为黑色”，同时“将x的父节点设为红色”；左旋后，由于x的兄弟节点发生了变化，需要更新x的兄弟节点，从而进行后续处理。 Case 3：x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的左孩子是红色，右孩子是黑色的 现象说明：x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的左孩子是红色，右孩子是黑色的。 处理策略 将x兄弟节点的左孩子设为“黑色”。 将x兄弟节点设为“红色”。 对x的兄弟节点进行右旋。 右旋后，重新设置x的兄弟节点。 下面谈谈为什么要这样处理。(建议理解的时候，通过下面的图进行对比) 我们处理“Case 3”的目的是为了将“Case 3”进行转换，转换成“Case 4”,从而进行进一步的处理。转换的方式是对x的兄弟节点进行右旋；为了保证右旋后，它仍然是红黑树，就需要在右旋前“将x的兄弟节点的左孩子设为黑色”，同时“将x的兄弟节点设为红色”；右旋后，由于x的兄弟节点发生了变化，需要更新x的兄弟节点，从而进行后续处理。 Case 4：x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的右孩子是红色的，x的兄弟节点的左孩子任意颜色 现象说明：x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的右孩子是红色的，x的兄弟节点的左孩子任意颜色。 处理策略 将x父节点颜色 赋值给 x的兄弟节点。 将x父节点设为“黑色”。 将x兄弟节点的右子节设为“黑色”。 对x的父节点进行左旋。 设置“x”为“根节点”。 下面谈谈为什么要这样处理。(建议理解的时候，通过下面的图进行对比) 我们处理“Case 4”的目的是：去掉x中额外的黑色，将x变成单独的黑色。处理的方式是“：进行颜色修改，然后对x的父节点进行左旋。下面，我们来分析是如何实现的。 为了便于说明，我们设置“当前节点”为S(Original Son)，“兄弟节点”为B(Brother)，“兄弟节点的左孩子”为BLS(Brother’s Left Son)，“兄弟节点的右孩子”为BRS(Brother’s Right Son)，“父节点”为F(Father)。 我们要对F进行左旋。但在左旋前，我们需要调换F和B的颜色，并设置BRS为黑色。为什么需要这里处理呢？因为左旋后，F和BLS是父子关系，而我们已知BL是红色，如果F是红色，则违背了“特性(4)”；为了解决这一问题，我们将“F设置为黑色”。 但是，F设置为黑色之后，为了保证满足“特性(5)”，即为了保证左旋之后： 第一，“同时经过根节点和S的分支的黑色节点个数不变”。若满足“第一”，只需要S丢弃它多余的颜色即可。因为S的颜色是“黑+黑”，而左旋后“同时经过根节点和S的分支的黑色节点个数”增加了1；现在，只需将S由“黑+黑”变成单独的“黑”节点，即可满足“第一”。 第二，“同时经过根节点和BLS的分支的黑色节点数不变”。若满足“第二”，只需要将“F的原始颜色”赋值给B即可。之前，我们已经将“F设置为黑色”(即，将B的颜色”黑色”，赋值给了F)。至此，我们算是调换了F和B的颜色。 第三，“同时经过根节点和BRS的分支的黑色节点数不变”。在“第二”已经满足的情况下，若要满足“第三”，只需要将BRS设置为“黑色”即可。 经过，上面的处理之后。红黑树的特性全部得到的满足！接着，我们将x设为根节点，就可以跳出while循环(参考伪代码)；即完成了全部处理。 至此，我们就完成了Case 4的处理。理解Case 4的核心，是了解如何“去掉当前节点额外的黑色”。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>树</tag>
        <tag>红黑树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（28）：L1、L2正则化]]></title>
    <url>%2F2017%2F07%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8828%EF%BC%89%EF%BC%9AL1%E3%80%81L2%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[之前讨论了机器学习中的偏差-方差权衡。机器学习里的损失函数（代价函数）可以用来描述模型与真模型（ground truth）之间的差距，因此可以解决“偏差”的问题。但是仅有损失函数，我们无法解决方差的问题，因而会有过拟合风险。 这次我们讨论损失函数的反面——正则项，看看L1正则项和L2正则项是如何使机器学习模型避免过拟合的。 我们希望选择或学习一个合适的模型。若在空间中存在“真模型”，那我们所选择的模型要与真模型的参数个数相同，所选择的模型的参数向量与真模型的参数向量相近。 过拟合指的是我们以为追求提高模型对训练数据的预测能力，所选模型的复杂度往往会比真模型更高。即学习时选择的模型所包含的参数过多，以致于出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。 一、从经验风险最小化到结构经验最小化经验风险最小化（empirical risk minimization）认为经验风险最小的模型是最优的模型，即求解最优化问题： \underset{f\in\mathscr{F}}{\min}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}当样本容量足够大的时候，经验风险最小化学习效果良好。比如极大似然估计，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。 但是当样本容量很小时，经验风险最小化学习会产生过拟合（over-fitting）的现象。这就引出了结构风险最小化，它等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term），它的定义为： R_{srm}\left(f\right)=\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)其中$J(f)$为模型的复杂度，模型$f$越复杂，复杂度$J(f)$就越大；反之，模型越简单，复杂度$J(f)$就越小，即复杂度表示了对复杂模型的惩罚。$\lambda≥0$是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险和模型复杂度同时小。结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。比如贝叶斯估计中的最大后验概率估计就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。结构风险最小化的策略认为结构风险最小的模型是最优的模型，求解最优模型即求解最优化问题： \min_{f\in\mathscr{F}}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)这样，监督学习问题变成了经验风险或结构风险函数的最优化问题。 其中正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如，正则化项可以是模型参数向量的范数。它的一般形式如下： \min_{f\in\mathscr{F}}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)第一项是经验风险，第二项是正则化项，$\lambda≥0$为调整两者之间关系的系数。 二、范数与正则项在线性代数、函数分析等数学分支中，范数（Norm）是一个函数，其赋予某个向量空间（或矩阵）中的每个向量以长度或大小。对于零向量，另其长度为零。直观的说，向量或矩阵的范数越大，则我们可以说这个向量或矩阵也就越大。有时范数有很多更为常见的叫法，如绝对值其实便是一维向量空间中实数或复数的范数，而Euclidean距离也是一种范数。 范数满足通常意义上长度的三个基本性质： 非负性：$||\vec{x}||≥0$ 齐次性：$||c·\vec{x}||=|c|||\vec{x}||$ 三角不等式：$||\vec{x}+\vec{y}||≤||\vec{x}||+||\vec{y}||$ 在这里，我们需要关注的最主要是范数的「非负性」。我们刚才讲，损失函数通常是一个有下确界的函数。而这个性质保证了我们可以对损失函数做最优化求解。如果我们要保证目标函数依然可以做最优化求解，那么我们就必须让正则项也有一个下界。非负性无疑提供了这样的下界，而且它是一个下确界——由齐次性保证（当 c=0 时）。 因此，我们说，范数的性质使得它天然地适合作为机器学习的正则项。而范数需要的向量，则是机器学习的学习目标——参数向量。 范数的一般化定义：设$p≥1$的实数，$p-norm$定义为： ||x||_p:=(\sum_{i=1}^n|x_i|^p)^{\frac{1}{p}} \ \ \ \ （1）机器学习中有几个常用的范数，分别是： $L_1-$范数：$||x⃗ ||=∑^d_{i=1}| x_i|$; $L_2-$范数：$||x⃗ ||_2=(∑^d_{i=1}x^2_i)^{1/2}$； $L_p-$范数：$||x||_p=(\sum_{i=1}^n|x_i|^p)^{\frac{1}{p}}$ $L_∞-$范数：$||x⃗ ||_∞=lim_{p→+∞}(∑^d_{i=1}x^p_i)^{1/p}$。 当p=1时，我们称之为taxicab Norm，也叫Manhattan Norm。其来源是曼哈顿的出租车司机在四四方方的曼哈顿街道中从一点到另一点所需要走过的距离。也即我们所要讨论的l1范数。其表示某个向量中所有元素绝对值的和。 而当p=2时，则是我们最为常见的Euclidean norm。也称为Euclidean distance。也即我们要讨论的l2范数。 而当p=0时，因其不再满足三角不等性，严格的说此时p已不算是范数了，但很多人仍然称之为l0范数。 这三个范数有很多非常有意思的特征，尤其是在机器学习中的正则化（Regularization）以及稀疏编码（Sparse Coding）有非常有趣的应用。 下图给出了一个Lp球的形状随着P的减少的可视化图。 在机器学习中，如果使用了$||\vec{w}||_p作为正则项$，则我们说，该机器学习任务引入了$L_p-$正则项。 2.1 $L_0$与$L_1-$正则项（LASSO regularizer）在机器学习里，最简单的学习算法可能是所谓的线性回归模型 F(x⃗ ;w⃗ ,b)=∑_{i=1}^nw_i⋅x_i+b我们考虑这样一种普遍的情况，即：预测目标背后的真是规律，可能只和某几个维度的特征有关；而其它维度的特征，要不然作用非常小，要不然纯粹是噪声。在这种情况下，除了这几个维度的特征对应的参数之外，其它维度的参数应该为零。若不然，则当其它维度的特征存在噪音时，模型的行为会发生预期之外的变化，导致过拟合。 于是，我们得到了避免过拟合的第一个思路：使尽可能多的参数为零。为此，最直观地我们可以引入$L_0$-范数。令 Ω(F(x⃗ ;w⃗ ))\overset{def}=ℓ_0\frac{∥w⃗ ∥_0}{n},ℓ_0>0这意味着，我们希望绝大多数$\vec{w}$的分量为零。 通过引入 $L_0-$正则项，我们实际上引入了一种「惩罚」机制，即：若要增加模型复杂度以加强模型的表达能力降低损失函数，则每次使得一个参数非零，则引入 $ℓ_0$ 的惩罚系数。也就是说，如果使得一个参数非零得到的收益（损失函数上的收益）不足 $ℓ_0$；那么增加这样的复杂度是得不偿失的。 通过引入$L_0-$正则项，我们可以使模型稀疏化且易于解释，并且在某种意义上实现了「特征选择」。这看起来很美好，但是 $L_0-$正则项也有绕不过去坎： 非连续 非凸 不可求导 因此，$L_0$正则项虽好，但是求解这样的最优化问题，难以在多项式时间内找到有效解（NP-Hard 问题）。于是，我们转而考虑 L0L0-范数最紧的凸放松（tightest convex relaxation）：L1-范数。令 Ω(F(x⃗ ;w⃗ ))\overset{def}{=}ℓ_1\frac{∥w⃗ ∥_1}{n},ℓ_1>0我们来看一下参数更新的过程，有哪些变化。考虑目标函数 Obj(F)=L(F)+γ⋅ℓ_1\frac{∥w⃗ ∥_1}{n}对参数$w_i$求偏导数 \frac{∂Obj}{∂w_i}=\frac{∂L}{∂w_i}+\frac{γℓ_1}{n}sgn(w_i)因此参数更新的过程为 w_i→w′_i\overset{def}{=}w_i−η\frac{∂L}{∂w_i}−η\frac{γℓ_1}{n}sgn(w_i)因为$η\frac{γℓ_1}{n}&gt;0$所以多出的项$η\frac{γℓ_1}{n}sgn(w_i)$使得$w_i→0$，实现稀疏化。 2.2 $L_2$正则项（Ridge Regularizer）让我们回过头，考虑多项式模型，它的一般形式为： F=∑_{i=1}^nw_i⋅x^i+b我们注意到，当多项式模型过拟合时，函数曲线倾向于靠近噪声点。这意味着，函数曲线会在噪声点之间来回扭曲跳跃。这也就是说，在某些局部，函数曲线的切线斜率会非常高（函数导数的绝对值非常大）。对于多项式模型来说，函数导数的绝对值，实际上就是多项式系数的一个线性加和。这也就是说，过拟合的多项式模型，它的参数的绝对值会非常大（至少某几个参数分量的绝对值非常大）。因此，如果我们有办法使得这些参数的值，比较稠密均匀地集中在0附近，就能有效地避免过拟合。 于是我们引入$L_2-$正则项，令 Ω(F(x⃗ ;w⃗ ))\overset{def}=ℓ_2\frac{∥w⃗ ∥_2 }{2n},ℓ_2>0因此有目标函数 Obj(F)=L(F)+γ⋅ℓ_2\frac{∥w⃗ ∥_2 }{2n}对参数$w_i$求偏导数，有 \frac{∂Obj}{∂w_i}=\frac{∂L}{∂w_i}+\frac{γℓ_2}{n}w_i再有参数更新 w_i→w′_i\overset{def}{=}w_i−η\frac{∂L}{∂w_i}−η\frac{γℓ_2}{n}w_i=(1−η\frac{γℓ_2}n)w_i−η\frac{∂L}{∂w_i}考虑到$η\frac{γℓ_2}{n}&gt;0$，因此，引入$L_2-$正则项之后，相当于衰减了（decay）参数的权重，使参数趋近于0。 2.3 $L_1-$正则项与$L_2-$正则项的区别现在，我们考虑这样一个问题：为什么使用$L-1-$正则项，会倾向于使得参数稀疏化；而使用$L_2-$正则项，会倾向于使得参数稠密地接近于0？ 这里引用一张来自周志华老师的著作，《机器学习》（西瓜书）里的插图，尝试解释这个问题。 为了简便起见，我们只考虑模型有两个参数$w_1$和$w_2$的情形。 在图中，我们有三组等值线，位于同一条等值线上的$w_1$与$w_2$映射到相同的平方损失项、$L_1-$范数和$L_2-$范数。并且，对于三组等值线来说，当$(w_1,w_2)$沿着等值线法线方向，向外扩张，则对应的值增大；反之，若沿着法线向内收缩，则对应的值减小。 因此，对于目标函数$Obj(F)$来说，实际上是要在正则项的等值线与损失函数的等值线中寻找一个交点，使得二者的和最小。 对于$L_1-$正则项来说，因为$L_1-$正则项是一组菱形，这些交点容易落在坐标轴上。因此，另一个参数的值在这个交点上就是0，从而实现了稀疏化。 对于 $L_2-$正则项来说，因为 $L_2-$正则项的等值线是一组圆形。所以，这些交点可能落在整个平面的任意位置。所以它不能实现「稀疏化」。但是，另一方面，由于 $(w_1,w_2)$ 落在圆上，所以它们的值会比较接近。这就是为什么 $L_2-$正则项可以使得参数在零附近稠密而平滑。 三、贝叶斯先验从贝叶斯的角度来看，正则化等价于对模型参数引入先验分布。 3.1 Linear Regression我们先看下最原始的线性回归： \begin{align*} & p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(\epsilon^{(i)})^2}{2\delta^2} \right)\\ \Rightarrow & p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})^2}{2\delta^2} \right) \end{align*}由最大似然估计（MLE）： \begin{align*} L(w) & = p(\vec{y}|X;w)\\ & = \prod_{i=1}^{m} p(y^{(i)}|x^{(i)};\theta)\\ & = \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})^2}{2\delta^2} \right) \end{align*}取对数： \begin{align*} l(w) & = \log L(w)\\ & =\log \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})}{2\delta^2} \right)\\ & = \sum_{i=1}^{m} \log \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})^2}{2\delta^2} \right)\\ & = m \log \frac{1}{\sqrt{2\pi}\delta} - \frac{1}{\delta^2}\cdot \frac{1}{2} \sum_{i=1}^{m} (y^{(i)} - w^Tx^{(i)})^2 \end{align*}即 w_{MLE} = \arg \underset{w}{\min} \sum_{i=1}^{m} (y^{(i)} - w^Tx^{(i)})^2这就导出了我们最原始的$least-squares$损失函数，但这是在我们对参数$w$没有加入任何先验分布的情况下。在数据维度很高的情况下，我们的模型参数很多，模型复杂度高，容易发生过拟合。 比如我们常说的 “small n, large p problem”。（我们一般用 n 表示数据点的个数，用 p 表示变量的个数 ，即数据维度。当 的时候，不做任何其他假设或者限制的话，学习问题基本上是没法进行的。因为如果用上所有变量的话， p 越大，通常会导致模型越复杂，但是反过来 n 又很小，于是就会出现很严重的 overfitting 问题。Linear regression一般只对low dimension适用，比如n=50, p=5，而且这五个变量还不存在multicolinearity. 这个时候，我们可以对参数$w$引入先验分布，降低模型复杂度。 3.2 Ridge RegressionRidge Regression的提出就是为了解决multicolinearity的，加一个L2 penalty term也是因为算起来方便。然而它并不能shrink parameters to 0.所以没法做variable selection。 我们对参数 w 引入协方差为 $\alpha$ 的零均值高斯先验。 取对数： 等价于： 这不就是Ridge Regression吗？看我们得到的参数，在零附近是不是很密集，老实说 ridge regression 并不具有产生稀疏解的能力，也就是说参数并不会真出现很多零。假设我们的预测结果与两个特征相关，L2正则倾向于综合两者的影响，给影响大的特征赋予高的权重；而L1正则倾向于选择影响较大的参数，而舍弃掉影响较小的那个。实际应用中 L2正则表现往往会优于 L1正则，但 L1正则会大大降低我们的计算量。 Typically ridge or ℓ2 penalties are much better for minimizing prediction error rather than ℓ1 penalties. The reason for this is that when two predictors are highly correlated, ℓ1 regularizer will simply pick one of the two predictors. In contrast, the ℓ2 regularizer will keep both of them and jointly shrink the corresponding coefficients a little bit. Thus, while the ℓ1 penalty can certainly reduce overfitting, you may also experience a loss in predictive power. 那现在我们知道了，对参数引入 高斯先验 等价于L2正则化。 3.3 LASSOLASSO是针对Ridge Regression的没法做variable selection的问题提出来的，L1 penalty虽然算起来麻烦，没有解析解，但是可以把某些系数shrink到0。 在Ridge Regression中，我们对 w 引入了高斯分布，那么拉普拉斯分布(Laplace distribution)呢？ 注：LASSO - least absolute shrinkage and selection operator. 我们看下拉普拉斯分布长啥样： 关于拉普拉斯和正态分布的渊源，大家可以参见 正态分布的前世今生。重复之前的推导过程我们很容易得到： 该问题通常被称为 LASSO (least absolute shrinkage and selection operator) 。LASSO 仍然是一个 convex optimization 问题，不具有解析解。它的优良性质是能产生稀疏性，导致 w 中许多项变成零。 再次总结下，对参数引入 拉普拉斯先验 等价于 L1正则化。 3.4 Elastic Net然而LASSO虽然可以做variable selection，但是不consistent啊，而且当n很小时至多只能选出n个变量；而且不能做group selection。 可能有同学会想，既然 L1和 L2正则各自都有自己的优势，那我们能不能将他们 combine 起来？ 于是有了在L1和L2 penalty之间做个权重就是elastic net 因为lasso在解决之前提到的“small n, large p problem”存在一定缺陷。 得到结果： 此外针对不consistent有了adaptive lasso，针对不能做group selection有了group lasso, 在graphical models里有了graphical lasso。然后有人说unbiasedness, sparsity and continuity这三条都满足多好，于是有了MCP和SCAD同时满足这三条性质。还有很多penalized regression的方法。 3.5 总结 正则化参数等价于对参数引入 先验分布，使得 模型复杂度 变小（缩小解空间），对于噪声以及 outliers 的鲁棒性增强（泛化能力）。整个最优化问题从贝叶斯观点来看是一种贝叶斯最大后验估计，其中 正则化项对应后验估计中的先验信息，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计的形式。 这篇文章从理论推导讲到算法实现。除了高斯先验、拉普拉斯先验，还讲了其他先验：Lazy Sparse Stochastic Gradient Descent for Regularized Mutlinomial Logistic Regression]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>L1正则</tag>
        <tag>L2正则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（7）：数据库索引原理及优化]]></title>
    <url>%2F2017%2F07%2F21%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%887%EF%BC%89%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[本文以MySQL数据库为研究对象，讨论与数据库索引相关的一些话题。特别需要说明的是，MySQL支持诸多存储引擎，而各种存储引擎对索引的支持也各不相同，因此MySQL数据库支持多种索引类型，如BTree索引，哈希索引，全文索引等等。为了避免混乱，本文将只关注于BTree索引，因为这是平常使用MySQL时主要打交道的索引，至于哈希索引和全文索引本文暂不讨论。 文章主要内容分为三个部分。 第一部分主要从数据结构及算法理论层面讨论MySQL数据库索引的数理基础。 第二部分结合MySQL数据库中MyISAM和InnoDB数据存储引擎中索引的架构实现讨论聚集索引、非聚集索引及覆盖索引等话题。 第三部分根据上面的理论基础，讨论MySQL中高性能使用索引的策略。 一、数据结构及算法基础为什么这里要讲查询算法和数据结构呢？因为之所以要建立索引，其实就是为了构建一种数据结构，可以在上面应用一种高效的查询算法，最终提高数据的查询速度。 1.1 索引的本质MySQL官方对索引的定义为：索引（Index）是帮助MySQL高效获取数据的数据结构。提取句子主干，就可以得到索引的本质：索引是数据结构。 我们知道，数据库查询是数据库的最主要功能之一。我们都希望查询数据的速度能尽可能的快，因此数据库系统的设计者会从查询算法的角度进行优化。最基本的查询算法当然是顺序查找（linear search），这种复杂度为O(n)的算法在数据量很大时显然是糟糕的，好在计算机科学的发展提供了很多更优秀的查找算法，例如二分查找（binary search）、二叉树查找（binary tree search）等。 如果稍微分析一下会发现，每种查找算法都只能应用于特定的数据结构之上，例如二分查找要求被检索数据有序，而二叉树查找只能应用于二叉查找树上，但是数据本身的组织结构不可能完全满足各种数据结构（例如，理论上不可能同时将两列都按顺序进行组织），所以，在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。 看一个例子： 图1展示了一种可能的索引方式。左边是数据表，一共有两列七条记录，最左边的是数据记录的物理地址（注意逻辑上相邻的记录在磁盘上也并不是一定物理相邻的）。为了加快Col2的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找在O(log2n)O(log2n)的复杂度内获取到相应数据。 虽然这是一个货真价实的索引，但是实际的数据库系统几乎没有使用二叉查找树或其进化品种红黑树（red-black tree）实现的，原因会在下文介绍。 1.2 B树和B+树目前大部分数据库系统及文件系统都采用B-Tree或其变种B+Tree作为索引结构，在本文的下一节会结合存储器原理及计算机存取原理讨论为什么B-Tree和B+Tree在被如此广泛用于索引，这一节先单纯从数据结构角度描述它们。 要理解B树，必须从二叉查找树（Binary search tree）讲起。 二叉查找树是一种查找效率非常高的数据结构，它有三个特点。 123（1）每个节点最多只有两个子树。（2）左子树都为小于父节点的值，右子树都为大于父节点的值。（3）在n个节点中找到目标值，一般只需要log(n)次比较。 二叉查找树的结构不适合数据库，因为它的查找效率与层数相关。越处在下层的数据，就需要越多次比较。它的搜索时间复杂度为$O(log_2N)$，所以它的搜索效率和树的深度有关极端情况下，n个数据需要n次比较才能找到目标值。对于数据库来说，每进入一层，就要从硬盘读取一次数据，这非常致命，因为硬盘的读取时间远远大于数据处理时间，数据库读取硬盘的次数越少越好，这一点也会在后面深入剖析。 如果要提高查询速度，那么就要降低树的深度。要降低树的深度，很自然的方法就是采用多叉树，再结合平衡二叉树的思想，我们可以构建一个平衡多叉树结构，然后就可以在上面构建平衡多路查找算法，提高大数据量下的搜索效率。 1.2.1 B树B树（B-tree）是一种树状数据结构，能够用来存储排序后的数据。这种数据结构能够让查找数据、循序存取、插入数据及删除的动作，都在对数时间内完成。B树，概括来说是一个一般化的二叉查找树，可以拥有多于2个子节点。与自平衡二叉查找树不同，B-树为系统最优化大块数据的读和写操作。B-tree算法减少定位记录时所经历的中间过程，从而加快存取速度。这种数据结构常被应用在数据库和文件系统的实作上。 在B树中查找给定关键字的方法是，首先把根结点取来，在根结点所包含的关键字K1,…,Kn查找给定的关键字（可用顺序查找或二分查找法），若找到等于给定值的关键字，则查找成功；否则，一定可以确定要查找的关键字在Ki与Ki+1之间，Pi为指向子树根节点的指针，此时取指针Pi所指的结点继续查找，直至找到，或指针Pi为空时查找失败。 B树作为一种多路搜索树（并不是二叉的）： 定义任意非叶子结点最多只有M个儿子；且M&gt;2； 根结点的儿子数为[2, M]； 除根结点以外的非叶子结点的儿子数为[M/2, M]； 每个结点存放至少M/2-1（取上整）和至多M-1个关键字；(至少2个关键字） 非叶子结点的关键字个数=指向儿子的指针个数-1； 非叶子结点的关键字：K[1], K[2], …, K[M-1]；且K[i] &lt; K[i+1]； 非叶子结点的指针：P[1], P[2], …, P[M]；其中P[1]指向关键字小于K[1]的子树，P[M]指向关键字大于K[M-1]的子树，其它P[i]指向关键字属于(K[i-1], K[i])的子树； 所有叶子结点位于同一层； 此外，在B树中，除非数据已经填满，否则不会增加新的层。也就是说，B树追求层越少越好。 这样的数据结构，非常有利于减少读取硬盘的次数。假定一个节点可以容纳100个值，那么三层的B树可以容纳100万个数据，但若换成二叉查找树，则需要20层！假定操作系统一次读取一个节点，并且根节点保留在内存中，那么B树在100万个数据中查找目标值，只需要读取两次硬盘。 如下图为一个M=3的B树示例： 1.2.2 B+树B+树是B树的变体，MySQL普遍使用B+Tree实现其索引结构。也是一种多路搜索树，其定义基本与B-树相同，除了： 1）非叶子结点的子树指针与关键字个数相同； 2）非叶子结点的子树指针P[i]，指向关键字值属于[K[i], K[i+1])的子树（B-树是开区间）； 3）为所有叶子结点增加一个链指针； 4）所有关键字都在叶子结点出现； B+树的搜索与B树也基本相同，区别是B+树只有达到叶子结点才命中（B树可以在非叶子结点命中），其性能也等价于在关键字全集做一次二分查找； B+树的性质： 1.所有关键字都出现在叶子结点的链表中（稠密索引），且链表中的关键字恰好是有序的； 2.不可能在非叶子结点命中； 3.非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层； 4.更适合文件索引系统。 一般在数据库系统或文件系统中使用的B+ Tree结构都在经典B+ Tree的基础上进行了优化，增加了顺序访问指针。做这个优化的目的是为了提高区间访问的性能，例如下图中如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。 【提问】数据库索引用什么建的 ：b+树；解释一下B+树，并说出数据库索引的原理； 二、计算机组成原理上文说过，红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B-/+Tree作为索引结构，这一节将结合计算机组成原理相关知识讨论B-/+Tree作为索引的理论基础。 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。下面先介绍内存和磁盘存取原理，然后再结合这些原理分析B-/+Tree作为索引的效率。 2.1 主存存取原理目前计算机使用的主存基本都是随机读写存储器（RAM），现代RAM的结构和存取原理比较复杂，这里本文抛却具体差别，抽象出一个十分简单的存取模型来说明RAM的工作原理。 从抽象角度看，主存是一系列的存储单元组成的矩阵，每个存储单元存储固定大小的数据。每个存储单元有唯一的地址，现代主存的编址规则比较复杂，这里将其简化成一个二维地址：通过一个行地址和一个列地址可以唯一定位到一个存储单元。上图展示了一个4 x 4的主存模型。 主存的存取过程如下： 当系统需要读取主存时，则将地址信号放到地址总线上传给主存，主存读到地址信号后，解析信号并定位到指定存储单元，然后将此存储单元数据放到数据总线上，供其它部件读取。写主存的过程类似，系统将要写入单元地址和数据分别放在地址总线和数据总线上，主存读取两个总线的内容，做相应的写操作。 这里可以看出，主存存取的时间仅与存取次数呈线性关系，因为不存在机械操作，两次存取的数据的“距离”不会对时间有任何影响，例如，先取A0再取A1和先取A0再取D3的时间消耗是一样的。 2.2 磁盘存取原理上文说过，索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O操作。与主存不同，磁盘I/O存在机械运动耗费，因此磁盘I/O的时间消耗是巨大的。 磁盘读取数据靠的是机械运动，当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。 为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间，最后便是对读取数据的传输。 所以每次读取数据花费的时间可以分为寻道时间、旋转延迟、传输时间三个部分。其中： 寻道时间是磁臂移动到指定磁道所需要的时间，主流磁盘一般在5ms以下。 旋转延迟就是我们经常听说的磁盘转速，比如一个磁盘7200转，表示每分钟能转7200次，也就是说1秒钟能转120次，旋转延迟就是1/120/2 = 4.17ms。 传输时间指的是从磁盘读出或将数据写入磁盘的时间，一般在零点几毫秒，相对于前两个时间可以忽略不计。 那么访问一次磁盘的时间，即一次磁盘IO的时间约等于5+4.17 = 9ms左右，听起来还挺不错的，但要知道一台500 -MIPS的机器每秒可以执行5亿条指令，因为指令依靠的是电的性质，换句话说执行一次IO的时间可以执行40万条指令，数据库动辄十万百万乃至千万级数据，每次9毫秒的时间，显然是个灾难。 2.3 局部性原理与磁盘预读由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 三、B-/+Tree索引的性能分析到这里终于可以分析为何数据库索引采用B-/+Tree存储结构了。上文说过数据库索引是存储到磁盘的而我们又一般以使用磁盘I/O次数来评价索引结构的优劣。先从B-Tree分析，根据B-Tree的定义，可知检索一次最多需要访问h-1个节点（根节点常驻内存）。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧：每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为$O(h)=O(log_dN)$。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。 综上所述，如果我们采用B-Tree存储结构，搜索时I/O次数一般不会超过3次，所以用B-Tree作为索引结构效率是非常高的。 3.1 B+树性能分析从上面介绍我们知道，B树的搜索复杂度为O(h)=O(logdN)，所以树的出度d越大，深度h就越小，I/O的次数就越少。B+Tree恰恰可以增加出度d的宽度，因为每个节点大小为一个页大小，所以出度的上限取决于节点内key和data的大小： 1dmax=floor(pagesize/(keysize+datasize+pointsize))//floor表示向下取整 由于B+Tree内节点去掉了data域，因此可以拥有更大的出度，从而拥有更好的性能。 3.2 B+树查找过程 B-树和B+树查找过程基本一致。如上图所示，如果要查找数据项29，那么首先会把磁盘块1由磁盘加载到内存，此时发生一次IO，在内存中用二分查找确定29在17和35之间，锁定磁盘块1的P2指针，内存时间因为非常短（相比磁盘的IO）可以忽略不计，通过磁盘块1的P2指针的磁盘地址把磁盘块3由磁盘加载到内存，发生第二次IO，29在26和30之间，锁定磁盘块3的P2指针，通过指针加载磁盘块8到内存，发生第三次IO，同时内存中做二分查找找到29，结束查询，总计三次IO。真实的情况是，3层的b+树可以表示上百万的数据，如果上百万的数据查找只需要三次IO，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次IO，那么总共需要百万次的IO，显然成本非常非常高。 这一章从理论角度讨论了与索引相关的数据结构与算法问题，下一章将讨论B+Tree是如何具体实现为MySQL中索引，同时将结合MyISAM和InnDB存储引擎介绍非聚集索引和聚集索引两种不同的索引实现形式。 四、MySQL索引实现在MySQL中，索引属于存储引擎级别的概念，不同存储引擎对索引的实现方式是不同的，本文主要讨论MyISAM和InnoDB两个存储引擎的索引实现方式。 4.1 MyISAM索引实现MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。下图是MyISAM索引的原理图： 这里设表一共有三列，假设我们以Col1为主键，则上图是一个MyISAM表的主索引（Primary key）示意。可以看出MyISAM的索引文件仅仅保存数据记录的地址。在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。如果我们在Col2上建立一个辅助索引，则此索引的结构如下图所示： 同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。 4.2 InnoDB索引实现虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。 第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 上图是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。 第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。例如，下图为定义在Col3上的一个辅助索引： 这里以英文字符的ASCII码作为比较准则。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。 再例如，用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择。 五、索引使用策略及优化MySQL的优化主要分为结构优化（Scheme optimization）和查询优化（Query optimization）。本章讨论的高性能索引策略主要属于结构优化范畴。本章的内容完全基于上文的理论基础，实际上一旦理解了索引背后的机制，那么选择高性能的策略就变成了纯粹的推理，并且可以理解这些策略背后的逻辑。 5.1 联合索引及最左前缀原理 联合索引（复合索引） 首先介绍一下联合索引。联合索引其实很简单，相对于一般索引只有一个字段，联合索引可以为多个字段创建一个索引。它的原理也很简单，比如，我们在（a,b,c）字段上创建一个联合索引，则索引记录会首先按照A字段排序，然后再按照B字段排序然后再是C字段，因此，联合索引的特点就是： 第一个字段一定是有序的 当第一个字段值相等的时候，第二个字段又是有序的，比如下表中当A=2时所有B的值是有序排列的，依次类推，当同一个B值得所有C字段是有序排列的 12345678| A | B | C | | 1 | 2 | 3 | | 1 | 4 | 2 | | 1 | 1 | 4 | | 2 | 3 | 5 | | 2 | 4 | 4 | | 2 | 4 | 6 | | 2 | 5 | 5 | 其实联合索引的查找就跟查字典是一样的，先根据第一个字母查，然后再根据第二个字母查，或者只根据第一个字母查，但是不能跳过第一个字母从第二个字母开始查。这就是所谓的最左前缀原理。 最左前缀原理 我们再来详细介绍一下联合索引的查询。还是上面例子，我们在（a,b,c）字段上建了一个联合索引，所以这个索引是先按a 再按b 再按c进行排列的，所以以下的查询方式都可以用到索引 123select * from table where a=1；select * from table where a=1 and b=2；select * from table where a=1 and b=2 and c=3； 上面三个查询按照 （a ）, （a，b ）,（a，b，c ）的顺序都可以利用到索引，这就是最左前缀匹配。 1select * from table where a=1 and c=3； //那么只会用到索引a。 比如： 12select * from table where b=2 and a=1；select * from table where b=2 and a=1 and c=3； 如果用到了最左前缀而只是颠倒了顺序，也是可以用到索引的，因为mysql查询优化器会判断纠正这条sql语句该以什么样的顺序执行效率最高，最后才生成真正的执行计划。但我们还是最好按照索引顺序来查询，这样查询优化器就不用重新编译了。 前缀索引 除了联合索引之外，对mysql来说其实还有一种前缀索引。前缀索引就是用列的前缀代替整个列作为索引key，当前缀长度合适时，可以做到既使得前缀索引的选择性接近全列索引，同时因为索引key变短而减少了索引文件的大小和维护开销。 字符串列(varchar,char,text等)，需要进行全字段匹配或者前匹配。也就是=‘xxx’ 或者 like ‘xxx%’ 字符串本身可能比较长，而且前几个字符就开始不相同。比如我们对中国人的姓名使用前缀索引就没啥意义，因为中国人名字都很短，另外对收件地址使用前缀索引也不是很实用，因为一方面收件地址一般都是以XX省开头，也就是说前几个字符都是差不多的，而且收件地址进行检索一般都是like ’%xxx%’，不会用到前匹配。相反对外国人的姓名可以使用前缀索引，因为其字符较长，而且前几个字符的选择性比较高。同样电子邮件也是一个可以使用前缀索引的字段。 前一半字符的索引选择性就已经接近于全字段的索引选择性。如果整个字段的长度为20，索引选择性为0.9，而我们对前10个字符建立前缀索引其选择性也只有0.5，那么我们需要继续加大前缀字符的长度，但是这个时候前缀索引的优势已经不明显，没有太大的建前缀索引的必要了。 一些文章中也提到： MySQL 前缀索引能有效减小索引文件的大小，提高索引的速度。但是前缀索引也有它的坏处：MySQL 不能在 ORDER BY 或 GROUP BY 中使用前缀索引，也不能把它们用作覆盖索引(Covering Index)。 5.2 索引优化策略 最左前缀匹配原则，上面讲到了 主键外检一定要建索引 对 where,on,group by,order by 中出现的列使用索引 尽量选择区分度高的列作为索引,区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0 对较小的数据列使用索引,这样会使索引文件更小,同时内存中也可以装载更多的索引键 索引列不能参与计算，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’); 为较长的字符串使用前缀索引 尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可 不要过多创建索引, 权衡索引个数与DML之间关系，DML也就是插入、删除数据操作。这里需要权衡一个问题，建立索引的目的是为了提高查询效率的，但建立的索引过多，会影响插入、删除数据的速度，因为我们修改的表数据，索引也需要进行调整重建 对于like查询，”%”不要放在前面。 12SELECT * FROMhoudunwangWHEREunameLIKE&apos;后盾%&apos; -- 走索引 SELECT * FROMhoudunwangWHEREunameLIKE &quot;%后盾%&quot; -- 不走索引 查询where条件数据类型不匹配也无法使用索引 字符串与数字比较不使用索引; 123CREATE TABLEa(achar(10)); EXPLAIN SELECT * FROMaWHEREa=&quot;1&quot; – 走索引 EXPLAIN SELECT * FROM a WHERE a=1 – 不走索引 正则表达式不使用索引,这应该很好理解,所以为什么在SQL中很难看到regexp关键字的原因]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>数据库索引原理及优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（6）：B树、B+树]]></title>
    <url>%2F2017%2F07%2F20%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%886%EF%BC%89%EF%BC%9AB%E6%A0%91%E3%80%81B%2B%E6%A0%91%2F</url>
    <content type="text"><![CDATA[具体讲解之前，有一点，再次强调下：B-树，即为B树。因为B树的原英文名称为B-tree，而国内很多人喜欢把B-tree译作B-树，其实，这是个非常不好的直译，很容易让人产生误解。如人们可能会以为B-树是一种树，而B树又是一种一种树。而事实上是，B-tree就是指的B树。特此说明。 一、B树B树（B-tree）是一种树状数据结构，能够用来存储排序后的数据。这种数据结构能够让查找数据、循序存取、插入数据及删除的动作，都在对数时间内完成。B树，概括来说是一个一般化的二叉查找树，可以拥有多于2个子节点。与自平衡二叉查找树不同，B-树为系统最优化大块数据的读和写操作。B-tree算法减少定位记录时所经历的中间过程，从而加快存取速度。这种数据结构常被应用在数据库和文件系统的实作上。 在B树中查找给定关键字的方法是，首先把根结点取来，在根结点所包含的关键字K1,…,Kn查找给定的关键字（可用顺序查找或二分查找法），若找到等于给定值的关键字，则查找成功；否则，一定可以确定要查找的关键字在Ki与Ki+1之间，Pi为指向子树根节点的指针，此时取指针Pi所指的结点继续查找，直至找到，或指针Pi为空时查找失败。 B树作为一种多路搜索树（并不是二叉的）： 定义任意非叶子结点最多只有M个儿子；且M&gt;2； 根结点的儿子数为[2, M]； 除根结点以外的非叶子结点的儿子数为[M/2, M]； 每个结点存放至少M/2-1（取上整）和至多M-1个关键字；(至少2个关键字） 非叶子结点的关键字个数=指向儿子的指针个数-1； 非叶子结点的关键字：K[1], K[2], …, K[M-1]；且K[i] &lt; K[i+1]； 非叶子结点的指针：P[1], P[2], …, P[M]；其中P[1]指向关键字小于K[1]的子树，P[M]指向关键字大于K[M-1]的子树，其它P[i]指向关键字属于(K[i-1], K[i])的子树； 所有叶子结点位于同一层； 如下图为一个M=3的B树示例： 二、B+树B+树是B树的变体，也是一种多路搜索树，其定义基本与B-树相同，除了： 1）非叶子结点的子树指针与关键字个数相同； 2）非叶子结点的子树指针P[i]，指向关键字值属于[K[i], K[i+1])的子树（B-树是开区间）； 3）为所有叶子结点增加一个链指针； 4）所有关键字都在叶子结点出现； B+树的搜索与B树也基本相同，区别是B+树只有达到叶子结点才命中（B树可以在非叶子结点命中），其性能也等价于在关键字全集做一次二分查找； B+树的性质： 1.所有关键字都出现在叶子结点的链表中（稠密索引），且链表中的关键字恰好是有序的； 2.不可能在非叶子结点命中； 3.非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层； 4.更适合文件索引系统。 三、B*树$B^*$树是B+树的变体，在B+树的非根和非叶子结点再增加指向兄弟的指针，将结点的最低利用率从1/2提高到2/3。 $B^*$树定义了非叶子结点关键字个数至少为$\frac{2}{3}M$，即块的最低使用率为2/3（代替B+树的1/2）； B+树的分裂：当一个结点满时，分配一个新的结点，并将原结点中1/2的数据复制到新结点，最后在父结点中增加新结点的指针；B+树的分裂只影响原结点和父结点，而不会影响兄弟结点，所以它不需要指向兄弟的指针； $B^*$树的分裂：当一个结点满时，如果它的下一个兄弟结点未满，那么将一部分数据移到兄弟结点中，再在原结点插入关键字，最后修改父结点中兄弟结点的关键字（因为兄弟结点的关键字范围改变了）；如果兄弟也满了，则在原结点与兄弟结点之间增加新结点，并各复制1/3的数据到新结点，最后在父结点增加新结点的指针； 所以，$B^*$树分配新结点的概率比B+树要低，空间使用率更高。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>B树</tag>
        <tag>B+树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（5）：AVL树]]></title>
    <url>%2F2017%2F07%2F19%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%885%EF%BC%89%EF%BC%9AAVL%E6%A0%91%2F</url>
    <content type="text"><![CDATA[我们知道，对于一般的二叉搜索树（Binary Search Tree），其期望高度（即为一棵平衡树时）为$log_2n$，其各操作的时间复杂度$O(log_2n)$同时也由此而决定。但是，在某些极端的情况下（如在插入的序列是有序的时），二叉搜索树将退化成近似链或链，此时，其操作的时间复杂度将退化成线性的，即O(n)。我们可以通过随机化建立二叉搜索树来尽量的避免这种情况，但是在进行了多次的操作之后，由于在删除时，我们总是选择将待删除节点的后继代替它本身，这样就会造成总是右边的节点数目减少，以至于树向左偏沉。这同时也会造成树的平衡性受到破坏，提高它的操作的时间复杂度。于是就有了我们下边介绍的平衡二叉树。 一、平衡二叉树平衡二叉树定义：平衡二叉树（Balanced Binary Tree）又被称为AVL树（有别于AVL算法），且具有以下性质：它是一 棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。平衡二叉树的常用算法有红黑树、AVL树等。在平衡二叉搜索树中，我们可以看到，其高度一般都良好地维持在O(log2n)，大大降低了操作的时间复杂度。 其严格定义为：一颗空树是平衡二叉树；若T是一棵非空二叉树，其左、右子树为TL和TR，令hl和hr分别为左、右子树的深度。当且仅当 TL、TR都是平衡二叉树； 并且满足公式$|hl-hr|≤1$时，则T是平衡二叉树 相应地，定义$hl-hr$为二叉平衡树的平衡因子（balance factor）。因此，平衡二叉树上所有结点的平衡因子可能是-1，0，1。换言之，若一颗二叉树上任一结点的平衡因子的绝对值都不大于1，则该树就是平衡二叉树。 最小二叉平衡树的节点的公式如下：$F(n)=F(n-1)+F(n-2)+1$ 这个类似于一个递归的数列，可以参考Fibonacci数列，1是根节点，$F(n-1)$是左子树的节点数量，$F(n-2)$是右子树的节点数量。 二、平衡查找树（AVL树）2.1 AVL树的定义平衡二叉树（AVL树，发明者的姓名缩写）：一种高度平衡的排序二叉树，其每一个节点的左子树和右子树的高度差最多等于1。平衡二叉树首先必须是一棵二叉排序树！ 平衡因子（Balance Factor）：将二叉树上节点的左子树深度减去右子树深度的值。对于平衡二叉树所有包括分支节点和叶节点的平衡因子只可能是-1,0和1，只要有一个节点的因子不在这三个值之内，该二叉树就是不平衡的。 最小不平衡子树：距离插入结点最近的，且平衡因子的绝对值大于1的节点为根的子树。 n个结点的AVL树最大深度约1.44log2n。查找、插入和删除在平均和最坏情况下都是O(logn)。增加和删除可能需要通过一次或多次树旋转来重新平衡这个树。这个方案很好的解决了二叉查找树退化成链表的问题，把插入，查找，删除的时间复杂度最好情况和最坏情况都维持在O(logN)。但是频繁旋转会使插入和删除牺牲掉O(logN)左右的时间，不过相对二叉查找树来说，时间上稳定了很多。 可以采用动态平衡技术保持一个平衡二叉树。构造平衡二叉树的时候，也可以采用相同的方法，默认初始时，是一个空树，插入节点时，通过动态平衡技术对二叉树进行调整。 Adeleon-Velskii和Landis提出了一个动态地保持二叉排序树平衡的方法，其基本思想是：在构造二叉排序树的过程中，每当插入一个结点时，首先检查是否因插入而破坏了树的平衡性，若是因插入结点而破坏了树的平衡性，则找出其中最小不平衡树，在保持排序树特性的前提下，调整最小不平衡子树各结点之间的连接关系，以达到新的平衡。通常这样得到的平衡二叉排序树简称为AVL树。 2.2 AVL树的自平衡操作——旋转AVL树最关键的也是最难的一步操作就是旋转。旋转主要是为了实现AVL树在实施了插入和删除操作以后，树重新回到平衡的方法。下面我们重点研究一下AVL树的旋转。 2.2.1 不平衡的四种情况对于一个平衡的节点，由于任意节点最多有两个儿子，因此高度不平衡时，此节点的两颗子树的高度差2。容易看出，这种不平衡出现在下面四种情况： 1) 6节点的左子树3节点高度比右子树7节点大2，左子树3节点的左子树1节点高度大于右子树4节点，这种情况成为左左。 2) 6节点的左子树2节点高度比右子树7节点大2，左子树2节点的左子树1节点高度小于右子树4节点，这种情况成为左右。 3) 2节点的左子树1节点高度比右子树5节点小2，右子树5节点的左子树3节点高度大于右子树6节点，这种情况成为右左。 4) 2节点的左子树1节点高度比右子树4节点小2，右子树4节点的左子树3节点高度小于右子树6节点，这种情况成为右右。 从图2中可以可以看出，1和4两种情况是对称的，这两种情况的旋转算法是一致的，只需要经过一次旋转就可以达到目标，我们称之为单旋转。2和3两种情况也是对称的，这两种情况的旋转算法也是一致的，需要进行两次旋转，我们称之为双旋转。 2.2.2 单旋转单旋转是针对于左左和右右这两种情况的解决方案，这两种情况是对称的，只要解决了左左这种情况，右右就很好办了。图3是左左情况的解决方案，节点k2不满足平衡特性，因为它的左子树k1比右子树Z深2层，而且k1子树中，更深的一层的是k1的左子树X子树，所以属于左左情况。 为使树恢复平衡，我们把k2变成这棵树的根节点，因为k2大于k1，把k2置于k1的右子树上，而原本在k1右子树的Y大于k1，小于k2，就把Y置于k2的左子树上，这样既满足了二叉查找树的性质，又满足了平衡二叉树的性质。 这样的操作只需要一部分指针改变，结果我们得到另外一颗二叉查找树，它是一棵AVL树，因为X向上一移动了一层，Y还停留在原来的层面上，Z向下移动了一层。整棵树的新高度和之前没有在左子树上插入的高度相同，插入操作使得X高度长高了。因此，由于这颗子树高度没有变化，所以通往根节点的路径就不需要继续旋转了。 2.2.3 双旋转双旋转：对于左右和右左这两种情况，单旋转不能使它达到一个平衡状态，要经过两次旋转。双旋转是针对于这两种情况的解决方案，同样的，这样两种情况也是对称的，只要解决了左右这种情况，右左就很好办了。图4是左右情况的解决方案，节点k3不满足平衡特性，因为它的左子树k1比右子树Z深2层，而且k1子树中，更深的一层的是k1的右子树k2子树，所以属于左右情况。 为使树恢复平衡，我们需要进行两步，第一步，把k1作为根，进行一次右右旋转，旋转之后就变成了左左情况，所以第二步再进行一次左左旋转，最后得到了一棵以k2为根的平衡二叉树树。 三、构建过程下面是由[1,2,3,4,5,6,7,10,9]构建平衡二叉树]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>树</tag>
        <tag>平衡二叉树</tag>
        <tag>AVL树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（4）：二叉查找树]]></title>
    <url>%2F2017%2F07%2F19%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%884%EF%BC%89%EF%BC%9A%E4%BA%8C%E5%8F%89%E6%9F%A5%E6%89%BE%E6%A0%91%2F</url>
    <content type="text"><![CDATA[一、定义二叉排序树（Binary Sort Tree）又称为二叉查找树（Binary Search Tree）、二叉搜索树。它是特殊的二叉树：对于二叉树，假设x为二叉树中的任意一个结点，x节点包含关键字key，节点x的$key$值记为$key[x]$。如果y是x的左子树中的一个结点，则$key[y]&lt;= key[x]$；如果y是x的右子树的一个结点，则$key[y] &gt;= key[x]$。那么，这棵树就是二叉查找树。 二叉查找树是先对待查找的数据进行生成树，确保树的左分支的值小于右分支的值，然后再就行和每个节点的父节点比较大小，查找最合适的范围。这个算法的效率查找效率很高，但是如果使用这种查找方法要首先创建树。 它或者是一颗空树，或者是具有下列性质的二叉树： 若任意节点的左子树不为空，则左子树上的所有节点的值均小于它的根结点的值； 若任意节点的右子树不为空，则左子树上所有节点的值均小于它的根结点的值； 任意节点的左、右子树也分别为二叉查找树。 没有键值相等的节点。 二叉查找树的性质：对二叉查找树进行中序遍历，即可得到有序的数列。 构造一颗二叉排序树的目的，其实并不是为了排序，而是为了提高查找和插入删除关键字的速度。不管怎么说，在一个有序数据集上的查找，速度总是要快于无序的数据集的，而二叉排序树这样的非线性结构，也有利于插入和排序的实现。 二叉查找树的高度决定了二叉查找树的查找效率。 二、查找、插入与删除2.1 查找在二叉查找树中查找x的过程如下： 若二叉树是空树，则查找失败。 若x等于根结点的数据，则查找成功，否则。 若x小于根结点的数据，则递归查找其左子树，否则。 递归查找其右子树。 复杂度分析，它和二分查找一样，插入和查找的时间复杂度均为O(logn)，但是在最坏的情况下仍然会有O(n)的时间复杂度。原因在于插入和删除元素的时候，树没有保持平衡（比如，我们查找上图（b）中的“93”，我们需要进行n次查找操作）。我们追求的是在最坏的情况下仍然有较好的时间复杂度，这就是平衡查找树设计的初衷。 根据上述的步骤，写出其查找操作的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/**查找指定的元素,默认从 * 根结点出开始查询*/ public boolean contains(T t) &#123; return contains(t, rootTree); &#125; /**从某个结点出开始查找元素*/ public boolean contains(T t, BinaryNode&lt;T&gt; node) &#123; if(node==null) return false;//结点为空，查找失败 int result = t.compareTo(node.data); if(result&gt;0) return contains(t,node.right);//递归查询右子树 else if(result&lt;0) return contains(t, node.left);//递归查询左子树 else return true; &#125; /** 这里我提供一个对二叉树最大值 最小值的搜索*/ /**找到二叉查找树中的最小值*/ public T findMin() &#123; if(isEmpty()) &#123; System.out.println("二叉树为空"); return null; &#125;else return findMin(rootTree).data; &#125; /**找到二叉查找树中的最大值*/ public T findMax() &#123; if(isEmpty()) &#123; System.out.println("二叉树为空"); return null; &#125;else return findMax(rootTree).data; &#125; /**查询出最小元素所在的结点*/ public BinaryNode&lt;T&gt; findMin(BinaryNode&lt;T&gt; node) &#123; if(node==null) return null; else if(node.left==null) return node; return findMin(node.left);//递归查找 &#125; /**查询出最大元素所在的结点*/ public BinaryNode&lt;T&gt; findMax(BinaryNode&lt;T&gt; node) &#123; if(node!=null) &#123; while(node.right!=null) node=node.right; &#125; return node; &#125; 2.2 插入插入：从根结点开始逐个与关键字进行对比，小了去左边，大了去右边，碰到子树为空的情况就将新的节点连接。二叉查找树的插入过程如下： 1) 若当前的二叉查找树为空，则插入的元素为根节点; 2) 若插入的元素值小于根节点值，则将元素插入到左子树中; 3) 若插入的元素值不小于根节点值，则将元素插入到右子树中。 12345678910111213141516171819202122/**插入元素*/ public void insert(T t) &#123; rootTree = insert(t, rootTree); &#125; /**在某个位置开始判断插入元素*/ public BinaryNode&lt;T&gt; insert(T t,BinaryNode&lt;T&gt; node) &#123; if(node==null) &#123; //新构造一个二叉查找树 return new BinaryNode&lt;T&gt;(t, null, null); &#125; int result = t.compareTo(node.data); if(result&lt;0) node.left= insert(t,node.left); else if(result&gt;0) node.right= insert(t,node.right); else ;//doNothing return node; &#125; 2.3 删除如果要删除的节点是叶子，直接删；如果只有左子树或只有右子树，则删除节点后，将子树连接到父节点即可；如果同时有左右子树，则可以将二叉排序树进行中序遍历，取将要被删除的节点的前驱或者后继节点替代这个被删除的节点的位置。 二叉查找树的删除，分三种情况进行处理： 1) p为叶子节点，直接删除该节点，再修改其父节点的指针（注意分是根节点和不是根节点），如图a; p为单支节点（即只有左子树或右子树）。让p的子树与p的父亲节点相连，删除p即可（注意分是根节点和不是根节点），如图b; p的左子树和右子树均不空。找到p的后继y，因为y一定没有左子树，所以可以删除y，并让y的父亲节点成为y的右子树的父亲节点，并用y的值代替p的值；或者方法二是找到p的前驱x，x一定没有右子树，所以可以删除x，并让x的父亲节点成为y的左子树的父亲节点。如图c。 123456789101112131415161718192021222324/**删除元素*/ public void remove(T t) &#123; rootTree = remove(t,rootTree); &#125; /**在某个位置开始判断删除某个结点*/ public BinaryNode&lt;T&gt; remove(T t,BinaryNode&lt;T&gt; node) &#123; if(node == null) return node;//没有找到,doNothing int result = t.compareTo(node.data); if(result&gt;0) node.right = remove(t,node.right); else if(result&lt;0) node.left = remove(t,node.left); else if(node.left!=null&amp;&amp;node.right!=null) &#123; node.data = findMin(node.right).data; node.right = remove(node.data,node.right); &#125; else node = (node.left!=null)?node.left:node.right; return node; &#125; 2.4 总结二叉排序树总结： 二叉排序树以链式进行存储，保持了链式结构在插入和删除操作上的优点。 在极端情况下，查询次数为1，但最大操作次数不会超过树的深度。也就是说，二叉排序树的查找性能取决于二叉排序书的形状，也就引申除了后面的平衡二叉树。 给定一个元素集合，可以构造不同的二叉排序树，当它同时是一个完全二叉树的时候，查找的时间复杂度为$O(log(n))$，近似于二分查找。 当出现最极端的斜树时，时间复杂度为$O(n)$，等同于顺序查找，效果最差。 下图为二叉树查找和顺序查找以及二分查找性能的对比图： 基于二叉查找树进行优化，进而可以得到其他的树表查找算法，比如平衡树、红黑树等高效算法。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>树</tag>
        <tag>二叉查找树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（3）：二叉树]]></title>
    <url>%2F2017%2F07%2F19%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%883%EF%BC%89%EF%BC%9A%E4%BA%8C%E5%8F%89%E6%A0%91%2F</url>
    <content type="text"><![CDATA[数据结构中有很多树的结构，这里整理了二叉树、二叉查找树、AVL树、红黑树、B树、B+树、trie树的基本概念与操作。 一、二叉树的概念1.1 树的基本概念树是一种数据结构，它是由n（n&gt;=1）个有限节点组成一个具有层次关系的集合。它具有以下特点： 每个节点有零个或多个子节点； 没有父节点的节点称为根节点； 每一个非根节点有且只有一个父节点； 除了根节点外，每个子节点可以分为多个不相交的子树。 若一个结点有子树，那么该结点称为子树根的”双亲”，子树的根是该结点的”孩子”。有相同双亲的结点互为”兄弟”。一个结点的所有子树上的任何结点都是该结点的后裔。从根结点到某个结点的路径上的所有结点都是该结点的祖先。 结点的度：结点拥有的子树的数目。 叶子：度为零的结点。 分支结点：度不为零的结点。 树的度：树中结点的最大的度。 层次：根结点的层次为1，其余结点的层次等于该结点的双亲结点的层次加1。 树的高度：树中结点的最大层次。 无序树：如果树中结点的各子树之间的次序是不重要的，可以交换位置。 有序树：如果树中结点的各子树之间的次序是重要的, 不可以交换位置。 森林：0个或多个不相交的树组成。对森林加上一个根，森林即成为树；删去根，树即成为森林。 1.2 二叉树的定义二叉树是每个节点最多有两个子树（不存在度大于2的结点）的树结构。二叉树的子树有左右之分，次序不能颠倒。它有5种基本形态：二叉树可以是空集；根可以有空的左子树或右子树；或者左右子树皆为空。 1.3 二叉树的性质1.3.1 性质一二叉树的第i层至多有$2^{i-1}$个结点； 证明：下面用”数学归纳法”进行证明。 当$i=1$时，第$i$层的节点数目为$2^{i-1}=2^{0}=1$。因为第1层上只有一个根结点，所以命题成立。 假设当$i&gt;1$，第$i$层的节点数目为$2^{i-1}$。这个是根据(01)推断出来的！ 下面根据这个假设，推断出”第$(i+1)$层的节点数目为$2^{i}$”即可。由于二叉树的每个结点至多有两个孩子，故”第$(i+1)$层上的结点数目” 最多是 “第i层的结点数目的2倍”。即，第$(i+1)$层上的结点数目最大值$=2×2^{i-1}=2^{i}$。 故假设成立，原命题得证！ 1.3.2 性质二深度为k的二叉树至多有$2^{k-1}$个结点； 证明：在具有相同深度的二叉树中，当每一层都含有最大结点数时，其树中结点数最多。利用”性质1”可知，深度为k的二叉树的结点数至多为: $2^0+2^1+…+2^{k-1}=2^k-1$故原命题得证！ 1.3.3 性质三包含n个结点的二叉树的高度至少为$log_2 (n+1)$； 证明：根据”性质2”可知，高度为h的二叉树最多有2{h}–1个结点。反之，对于包含n个节点的二叉树的高度至少为$log_2(n+1)$。 1.3.4 性质四对任何一颗二叉树T，如果其终端结点数为$n_0$，度为2的结点数为$n_2$，则$n_0=n_2+1$ 证明：因为二叉树中所有结点的度数均不大于2，所以结点总数(记为n)=”0度结点数$(n_0)$” + “1度结点数$(n_1)$” + “2度结点数$(n_2)$”。由此，得到等式一。(等式一) $n=n_0+n_1+n_2$ 另一方面，0度结点没有孩子，1度结点有一个孩子，2度结点有两个孩子，故二叉树中孩子结点总数是：$n_1+2n_2$。 此外，只有根不是任何结点的孩子。故二叉树中的结点总数又可表示为等式二。 (等式二) $n=n1+2n_2+1$ 由(等式一)和(等式二)计算得到：n0=n2+1。原命题得证！ 1.4 满二叉树和完全二叉树1.4.1 满二叉树满二叉树的定义：除最后一层无任何子节点外，每一层上的所有结点都有两个子结点。也可以这样理解，除叶子结点外的所有结点均有两个子结点。节点数达到最大值，所有叶子结点必须在同一层上。 满二叉树的性质： 一棵树的深度为h，最大层数为k，深度与最大层数相同，k=h； 叶子数为$2^h$ 第k层的结点数是：$2^{k-1}$； 总结点数是$2^k-1$，且总节点数一定是奇数。 1.4.2 完全二叉树定义：一颗二叉树中，只有最小面两层结点的度可以小于2，并且最下一层的叶结点集中在靠左的若干位置上。这样的二叉树称为完全二叉树。 特点：叶子结点只能出现在最下层和次下层，且最小层的叶子结点集中在树的左部。显然，一颗满二叉树必定是一颗完全二叉树，而完全二叉树未必是满二叉树。 注意：完全二叉树是效率很高的数据结构，堆是一种完全二叉树或者近似完全二叉树，所以效率极高，像十分常用的排序算法、Dijkstra算法、Prim算法等都要用堆才能优化，二叉排序树的效率也要借助平衡树来提高，而平衡性基于完全二叉树。 二、二叉树的遍历【提问】 请分别写出并解释二叉树的先序、中序、后续遍历的递归与非递归版本 给定二叉树的先序跟后序遍历，能不能将二叉树重建：不能，因为先序为父节点-左节点-右节点，后序为左节点-右节点-父节点，两者的拓扑序列是一样的，所以无法建立；如果换成一棵二叉搜索树的后续能不能建立：可以，因为只要将遍历结果排序就可以得到中序结果。 这块内容讨论二叉树的常见遍历方式的代码（java）实现，包括前序（preorder）、中序（inorder）、后序（postorder）、层序（levelorder），进一步考虑递归和非递归的实现方式。递归的实现方法相对简单，但由于递归的执行方式每次都会产生一个新的方法调用栈，如果递归层级较深，会造成较大的内存开销，相比之下，非递归的方式则可以避免这个问题。递归遍历容易实现，非递归则没那么简单，非递归调用本质上是通过维护一个栈，模拟递归调用的方法调用栈的行为。 在此之前，先简单定义节点的数据结构： 二叉树节点最多只有两个儿子，并保存一个节点的值，为了实验的方便，假定它为 int。同时，我们直接使用 Java 的 System.out.print 方法来输出节点值，以显示遍历结果。 1234567891011class Node&#123; public int value; public Node left; public Node right; public Node(int v)&#123; this.value=v; this.left=null; this.right=null; &#125; &#125; 2.1 前序遍历2.1.1 递归实现递归实现很简单，在每次访问到某个节点时，先输出节点值，然后再依次递归的对左儿子、右儿子调用遍历的方法。代码如下 java 1234567public void preOrder(Node root)&#123; if(root!=null)&#123; System.out.print(root.value); preOrder(root.left); preOrder(root.right); &#125;&#125; 2.1.2 非递归实现利用栈实现循环先序遍历二叉树，维护一个栈，将根节点入栈，只要栈不为空，出栈并访问，接着依次将访问节点的右节点、左节点入栈。这种方式是对先序遍历的一种特殊实现，简洁明了，但是不具备很好地扩展性，在中序和后序方式中不适用。 1234567891011public void preOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; stack = new Stack&lt;Node&gt;(); stack.push(root); while(!stack.isEmpty)&#123; Node temp = stack.pop(); System.out.print(temp.value); if(temp.right!=null)stack.push(temp.right); if(temp.left!=null)stack.push(temp.left); &#125;&#125; 还有一种方式就是利用栈模拟递归过程实现循环先序遍历二叉树。这种方式具备扩展性，它模拟了递归的过程，将左子树不断的压入栈，直到null，然后处理栈顶节点的右子树。 java 12345678910111213public void preOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; s = new Stack&lt;Node&gt;(); while(root!=null||!s.isEmtpy())&#123; while(root!=null)&#123; System.out.print(root.value);、//先访问 s.push(root);//再入栈 root = root.left; &#125; root = s.pop(); root = root.right;//如果是null，出栈并处理右子树 &#125;&#125; 2.2 中序遍历2.2.1 递归实现1234567public void inOrder(Node root)&#123; if(root!=null)&#123; preOrder(root.left); System.out.print(root.value); preOrder(root.right); &#125;&#125; 2.2.2 非递归实现利用栈模拟递归过程实现循环中序遍历二叉树。跟前序遍历的非递归实现方法二很类似。唯一的不同是访问当前节点的时机：前序遍历在入栈前访问，而中序遍历在出栈后访问。 java 12345678910111213public void inOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; s = Stack&lt;Node&gt;(); while(root!=null||s.isEmpty())&#123; while(root!=null)&#123; s.push(root); root=root.left; &#125; root = s.pop(root); System.out.print(root.value); root = root.right; &#125;&#125; 2.3 后序遍历2.3.1 递归实现1234567public void inOrder(Node root)&#123; if(root!=null)&#123; preOrder(root.left); preOrder(root.right); System.out.print(root.value); &#125;&#125; 2.3.2 非递归实现1234567891011121314151617public void postOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; s1 = new Stack&lt;Node&gt;(); Stack&lt;Node&gt; s2 = new Stack&lt;Node&gt;(); Node node = root; s1.push(node); while(s1!=null)&#123;//这个while循环的功能是找出后序遍历的逆序，存在s2里面 node = s1.pop(); if(node.left!=null) s1.push(node.left); if(node.right!=null)s1.push(node.right); s2.push(node); &#125; while(s2!=null)&#123;//将s2中的元素出栈，即为后序遍历次序 node = s2.pop(); System.out.print(node.value); &#125;&#125; 2.4 层序遍历1234567891011public static void levelTravel(Node root)&#123; if(root==null)return; Queue&lt;Node&gt; q=new LinkedList&lt;Node&gt;(); q.add(root); while(!q.isEmpty())&#123; Node temp = q.poll(); System.out.println(temp.value); if(temp.left!=null)q.add(temp.left); if(temp.right!=null)q.add(temp.right); &#125; &#125; 总结一下：树的遍历主要有两种，一种是深度优先遍历，像前序、中序、后序；另一种是广度优先遍历，像层次遍历。在树结构中两者的区别还不是非常明显，但从树扩展到有向图，到无向图的时候，深度优先搜索和广度优先搜索的效率和作用还是有很大不同的。 深度优先一般用递归，广度优先一般用队列。一般情况下能用递归实现的算法大部分也能用堆栈来实现。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>树</tag>
        <tag>先序遍历</tag>
        <tag>中序遍历</tag>
        <tag>后序遍历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（2）：栈与队列]]></title>
    <url>%2F2017%2F07%2F18%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%882%EF%BC%89%EF%BC%9A%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[一、栈1.1 栈的定义栈（stack）是限定仅在表尾进行插入和删除操作的线性表。允许插入和删除的一端称为栈顶（top），另一端称为栈底，不含任何数据元素的栈称为空栈。它有以下几个特点： 栈中的数据是按照“后进先出（LIFO，Last In First Out）”方式进出栈的。 向栈中添加、删除数据时，只能从栈顶进行操作 栈通常包括的三种操作：push、peek、pop push：向栈中添加元素 peek：返回栈顶元素 pop：返回并删除栈顶元素的操作。 1.2 进栈与出栈下图为栈的示例图，栈中的数据依次为：$30=&gt;20=&gt;10$ 下图为出栈的示意图： 出栈前：栈顶元素是30。此时，栈中的元素依次是$30 = &gt; 20 =&gt; 10 $ 出栈后：30出栈之后，栈顶元素变成20。此时，栈中的元素依次是 $20 =&gt; 10$ 下图为入栈的示意图： 入栈前：栈顶元素是20。此时，栈中的元素依次是$20 =&gt; 10 $ 入栈后：40入栈之后，栈顶元素变成40。此时，栈中的元素依次是 $40 =&gt; 20 =&gt; 10$ 二、队列2.1 队列的定义队列（queue）是只允许在一段进行插入操作，而在另一端进行删除操作的线性表。允许插入的一段称为队尾，允许删除的一端称为队头。它有以下几个特点： 队列中数据是按照“先进先出（FIFO，First-In-First-Out）”方式进出队列的。 队列只允许在“队首”进行删除操作，而在“队尾”进行插入操作。 2.2 出队列和入队列下图为队列的示意图：队列中有10，20，30共3个数据。 下图为出队列的示意图： 出队列前：队首是10，队尾是30。 出队列后：出队列(队首)之后。队首是20，队尾是30。 下图为入队列的示意图： 入队列前：队首是20，队尾是30。 入队列后：40入队列(队尾)之后。队首是20，队尾是40。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>栈</tag>
        <tag>队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（1）：数组与链表]]></title>
    <url>%2F2017%2F07%2F18%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%881%EF%BC%89%EF%BC%9A%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[线性表是一种线性结构，它是具有相同类型的n个数据元素组成的优先序列。介绍线性表的几个基本组成部分：数组、单向链表、双向链表。 一、数组数组有上界和下界，数组的元素在上下界内是连续的。 存储10，20，30，40，50的数组的示意图如下：数组的特点是：数据是连续的；随机访问速度块。 数组中稍微复杂一点的是多维数组和动态数组。对于C语言而言，多维数组本质上也是通过一维数组实现的。至于动态数组，是指数组的容量能动态增长的数组；对于C语言而言，若要提供动态数组，需要手动实现；而对于C++而言，STL提供了Vector；对于Java而言，Collection集合中提供了ArrayList和Vector。 数组是将元素在内存中连续存放，由于每个元素占用内存相同，可以通过下标迅速访问数组中任何元素。但是如果要在数组中增加一个元素，需要移动大量元素，在内存中空出一个元素的空间，然后将要增加的元素放在其中。同样的道理，如果想删除一个元素，同样需要移动大量元素去填掉被移动的元素。如果应用需要快速访问数据，很少或不插入和删除元素，就应该用数组。 二、单向链表2.1 定义单向链表（单链表）是链表的一种，它由节点组成，每个节点都包含下一个节点的指针。 单链表的示意图如下： 表头为空，表头的后继节点是“节点10”（数据为10的节点），“节点10”的后继节点是“节点20”（数据为20的结点）…… 2.2 单链表删除节点删除“节点30”删除之前：“节点20”的后继节点为“节点30”，而“节点30”的后继节点为“节点40”删除之后：“节点20”的后继节点为“节点40” 2.3 单链表添加节点在“节点10 ”与“节点20”之间添加“节点15” 添加之前：“节点10”的后继节点为“节点20”添加之后：“节点10”的后继节点为“节点15”，而“节点15”的后继节点为“节点20”。 单链表的特点是：节点的链接方向是单向的；相对于数组来说，单链表的随机访问速度较慢，但是单链表删除、添加数据的效率很高。 三、双向链表3.1 定义双向链表（双链表）是链表的一种。和单链表一样，双链表也是由节点组成，它的每一个数据节点中都有两个指针，分别指向直接后继和直接前驱。所以，从双向链表的任意一个节点开始，都可以很方便地访问它的前驱结点和后继节点。一般我们都构造双向循环链表。 双链表的示意图如下：表头为空，表头的后继节点为“节点10”（数据为10的结点）；“节点10”的后继节点是“节点20”（数据为10的节点），“节点20”的前继节点是“节点10”；“节点20”的后继节点是“节点30”，“节点30”的前继节点是“节点20”;……；末尾节点的后继节点是表头。 3.2 双链表删除节点删除“节点30” 删除之前：“节点20”的后继节点为“节点30”，“节点30”的前继节点为“节点20”。“节点30”的后继节点为“节点40”，“节点40”的前继节点为“节点30”删除之后：“节点20”的后继节点为“节点40”，“节点40”的前继节点为“节点20” 3.3 双链表添加节点在”节点10”与”节点20”之间添加”节点15”添加之前：”节点10”的后继节点为”节点20”，”节点20” 的前继节点为”节点10”。添加之后：”节点10”的后继节点为”节点15”，”节点15” 的前继节点为”节点10”。”节点15”的后继节点为”节点20”，”节点20” 的前继节点为”节点15”。 四、数组与链表的区别数组是将元素在内存中连续存放，由于每个元素占用内存相同，可以通过下标迅速访问数组中任何元素。但是如果要在数组中增加一个元素，需要移动大量元素，在内存中空出一个元素的空间，然后将要增加的元素放在其中。同样的道理，如果想删除一个元素，同样需要移动大量元素去填掉被移动的元素。如果应用需要快速访问数据，很少或不插入和删除元素，就应该用数组。 链表恰好相反，链表中的元素在内存中不是顺序存储的，而是通过存在元素中的指针联系到一起。比如：上一个元素有个指针指到下一个元素，以此类推，直到最后一个元素。如果要访问链表中一个元素，需要从第一个元素开始，一直找到需要的元素位置。但是增加和删除一个元素对于链表数据结构就非常简单了，只要修改元素中的指针就可以了。如果应用需要经常插入和删除元素你就需要用链表数据结构了。 C++语言中可以用数组处理一组数据类型相同的数据，但不允许动态定义数组的大小，即在使用数组之前必须确定数组的大小。而在实际应用中，用户使用数组之前有时无法准确确定数组的大小，只能将数组定义成足够大小，这样数组中有些空间可能不被使用，从而造成内存空间的浪费。链表是一种常见的数据组织形式，它采用动态分配内存的形式实现。需要时可以用new分配内存空间，不需要时用delete将已分配的空间释放，不会造成内存空间的浪费。 (1) 从逻辑结构角度来看 a：数组必须事先定义固定的长度（元素个数），不能适应数据动态地增减的情况。当数据增加时，可能超出原先定义的元素个数；当数据减少时，造成内存浪费。 b：链表动态地进行存储分配，可以适应数据动态地增减的情况，且可以方便地插入、删除数据项。（数组中插入、删除数据项时，需要移动其它数据项） (2)从内存存储角度来看 a：(静态)数组从栈中分配空间, 对于程序员方便快速,但自由度小。 b：链表从堆中分配空间, 自由度大但申请管理比较麻烦. 总结 数组静态分配内存，链表动态分配内存；数组在内存中连续，链表不连续；数组元素在栈区，链表元素在堆区；数组利用下标定位，时间复杂度为O(1)，链表定位元素时间复杂度O(n)；数组插入或删除元素的时间复杂度O(n)，链表的时间复杂度O(1)。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>链表</tag>
        <tag>数组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（27）：Isolation Forest]]></title>
    <url>%2F2017%2F07%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8827%EF%BC%89%EF%BC%9AIsolation%20Forest%2F</url>
    <content type="text"><![CDATA[“An outlier is an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism.” — D. M. Hawkins, Identification of Outliers, Chapman and Hall, 1980. 异常检测 (anomaly detection)，或者又被称为“离群点检测” (outlier detection)，是机器学习研究领域中跟现实紧密联系、有广泛应用需求的一类问题。但是，什么是异常，并没有标准答案，通常因具体应用场景而异。如果要给一个比较通用的定义，很多文献通常会引用 Hawkins 在文章开头那段话。很多后来者的说法，跟这个定义大同小异。这些定义虽然笼统，但其实暗含了认定“异常”的两个标准或者说假设： 1）异常数据跟样本中大多数数据不太一样。 2）异常数据在整体数据样本中占比比较小。 为了刻画异常数据的“不一样”，最直接的做法是利用各种统计的、距离的、密度的量化指标去描述数据样本跟其他样本的疏离程度。而 Isolation Forest (Liu et al. 2011) 的想法要巧妙一些，它尝试直接去刻画数据的“疏离”(isolation)程度，而不借助其他量化指标。Isolation Forest 因为简单、高效，在学术界和工业界都有着不错的名声。 一、简介我们先用一个简单的例子来说明 Isolation Forest 的基本想法。假设现在有一组一维数据（如下图所示），我们要对这组数据进行随机切分，希望可以把点 A 和点 B 单独切分出来。具体的，我们先在最大值和最小值之间随机选择一个值 x，然后按照 =x 可以把数据分成左右两组。然后，在这两组数据中分别重复这个步骤，直到数据不可再分。显然，点 B 跟其他数据比较疏离，可能用很少的次数就可以把它切分出来；点 A 跟其他数据点聚在一起，可能需要更多的次数才能把它切分出来。 我们把数据从一维扩展到两维。同样的，我们沿着两个坐标轴进行随机切分，尝试把下图中的点A’和点B’分别切分出来。我们先随机选择一个特征维度，在这个特征的最大值和最小值之间随机选择一个值，按照跟特征值的大小关系将数据进行左右切分。然后，在左右两组数据中，我们重复上述步骤，再随机的按某个特征维度的取值把数据进行细分，直到无法细分，即：只剩下一个数据点，或者剩下的数据全部相同。跟先前的例子类似，直观上，点B’跟其他数据点比较疏离，可能只需要很少的几次操作就可以将它细分出来；点A’需要的切分次数可能会更多一些。按照先前提到的关于“异常”的两个假设，一般情况下，在上面的例子中，点B和点B’ 由于跟其他数据隔的比较远，会被认为是异常数据，而点A和点A’ 会被认为是正常数据。直观上，异常数据由于跟其他数据点较为疏离，可能需要较少几次切分就可以将它们单独划分出来，而正常数据恰恰相反。这其实正是 Isolation Forest（IF）的核心概念。IF采用二叉树去对数据进行切分，数据点在二叉树中所处的深度反应了该条数据的“疏离”程度。整个算法大致可以分为两步： 训练：抽取多个样本，构建多棵二叉树（Isolation Tree，即 iTree）； 预测：综合多棵二叉树的结果，计算每个数据点的异常分值。 训练： 构建一棵 iTree 时，先从全量数据中抽取一批样本，然后随机选择一个特征作为起始节点，并在该特征的最大值和最小值之间随机选择一个值，将样本中小于该取值的数据划到左分支，大于等于该取值的划到右分支。然后，在左右两个分支数据中，重复上述步骤，直到满足如下条件： 1）数据不可再分，即：只包含一条数据，或者全部数据相同。 2）二叉树达到限定的最大深度。 预测： 计算数据 x 的异常分值时，先要估算它在每棵 iTree 中的路径长度（也可以叫深度）。具体的，先沿着一棵 iTree，从根节点开始按不同特征的取值从上往下，直到到达某叶子节点。假设 iTree 的训练样本中同样落在 x 所在叶子节点的样本数为 T.size，则数据 x 在这棵 iTree 上的路径长度 h(x)，可以用下面这个公式计算： h(x)=e+C(T.size)公式中，$e$表示数据$x$从$iTree$的根节点到叶节点过程中经过的边的数目，$C(T.size)$ 可以认为是一个修正值，它表示在一棵用 $T.size$ 条样本数据构建的二叉树的平均路径长度。一般的，$C(n)$的计算公式如下： C(n)=2H(n-1)-\frac{2(n-1)}{n}其中，$H(n-1)$可用$ln(n-1)+0.5772156649$估算，这里的常数是欧拉常数。数据 x 最终的异常分值 Score(x) 综合了多棵 iTree 的结果： Score(x)=2^{-\frac{E[h(x)]}{C(\psi)}}公式中，$E(h(x)) $表示数据 x 在多棵 iTree 的路径长度的均值，$\psi$表示单棵 iTree 的训练样本的样本数，$C(\psi)$ 表示用$\psi$条数据构建的二叉树的平均路径长度，它在这里主要用来做归一化。 从异常分值的公式看，如果数据 x 在多棵 iTree 中的平均路径长度越短，得分越接近 1，表明数据 x 越异常；如果数据 x 在多棵 iTree 中的平均路径长度越长，得分越接近 0，表示数据 x 越正常；如果数据 x 在多棵 iTree 中的平均路径长度接近整体均值，则打分会在 0.5 附近。 二、算法特点在论文中，也比较了其它的常用异常挖掘的算法。比如常用的统计方法，基于分类的方法，和基于聚类的方法，这些传统算法通常是对正常的数据构建一个模型，然后把不符合这个模型的数据，认为是异常数据。而且，这些模型通常为正常数据作优化，而不是为异常数据作优化。而iForest可以显示地找出异常数据，而不用对正常的数据构建模型。 由于异常数据的两个特征（少且不同： few and different）：异常数据只占很少量;异常数据特征值和正常数据差别很大。 异常数据的这两个特征，确定了算法的理论基础。因此，构建二叉树型结构的时候，异常数据离根更近，而正常数据离根更远，更深。算法为了效率考虑，也限定了树的深度：ceil(log2(n))，这个值近似于树的平均深度，因为只需要关心那些低于平均高度的数据点，而不需要树进行完全生成。 算法只需要两个参数：树的多少与采样的多少。实验发现，在100颗树的时候，路径的长度就已经覆盖得比较好了，因此选100颗也就够了。采样，是为了更好的将正常数据和异常数据分离开来。有别于其它模型，采样数据越多，反面会降低iForest识别异常数据的能力。因为，通常使用256个样本，这也是scikit-learn实现时默认使用的采样数。 由于算法只需要采样数据256条样本，并且对树的深度也有限制，因此，算法对内存要求很低，且处理速度很快，其时间复杂度也是线性的。 不像其它算法，需要计算距离或者密度来寻找异常数据，iForest算法可以很好的处理高维数据和大数据，并且也可以作为在线预测。假设采样为256条，结点最大为511个，假设一个节点占b字节，共使用t颗树，那么需要的内存只有511tb字节，基本上只需要几M到几十M的内存就够了。数据还显示，预测287,748条数据只花了7.6秒。 另外，iForest既能发现群异常数据，也能发现散点异常数据。同时也能处理训练数据中不包含异常数据的情况。 三、代码示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 print(__doc__)import numpy as npimport matplotlib.pyplot as pltfrom sklearn.ensemble import IsolationForestrng = np.random.RandomState(42)# Generate train dataX = 0.3 * rng.randn(100, 2)X_train = np.r_[X + 2, X - 2]# Generate some regular novel observationsX = 0.3 * rng.randn(20, 2)X_test = np.r_[X + 2, X - 2]# Generate some abnormal novel observationsX_outliers = rng.uniform(low=-4, high=4, size=(20, 2))# fit the modelclf = IsolationForest(max_samples=100, random_state=rng)clf.fit(X_train)y_pred_train = clf.predict(X_train)y_pred_test = clf.predict(X_test)y_pred_outliers = clf.predict(X_outliers)# plot the line, the samples, and the nearest vectors to the planexx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])Z = Z.reshape(xx.shape)plt.title("IsolationForest")plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=20, edgecolor='k')b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green', s=20, edgecolor='k')c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red', s=20, edgecolor='k')plt.axis('tight')plt.xlim((-5, 5))plt.ylim((-5, 5))plt.legend([b1, b2, c], ["training observations", "new regular observations", "new abnormal observations"], loc="upper left")plt.show() 算法基本上不需要配置参数就可以直接使用，通常就以下几个(参数明显比随机森林简单)： n_estimators: 默认为100，配置iTree树的多少 max_samples: 默认为265，配置采样大小 max_features: 默认为全部特征，对高维数据，可以只选取部分特征]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>iForest</tag>
        <tag>异常检测算法</tag>
        <tag>集成算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（26）：因子分解机（FM）与场感知分解机（FFM）]]></title>
    <url>%2F2017%2F07%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8826%EF%BC%89%EF%BC%9A%E5%9B%A0%E5%AD%90%E5%88%86%E8%A7%A3%E6%9C%BA%EF%BC%88FM%EF%BC%89%E4%B8%8E%E5%9C%BA%E6%84%9F%E7%9F%A5%E5%88%86%E8%A7%A3%E6%9C%BA%EF%BC%88FFM%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本文转载自美团点评技术团队 FM和FFM模型是最近几年提出的模型，凭借其在数据量比较打并且特征稀疏的情况下，忍让能够得到优秀的性能和效果，屡次在各大公司举办的CTR预估比赛中获得不错的战绩。 在计算广告领域，点击率CTR（click-through rate）和转化率CVR（conversion rate）是衡量广告流量的两个关键指标。准确的估计CTR、CVR对于提高流量的价值，增加广告收入有重要的指导作用。预估CTR、CVR，业界常用的方法由人工特征工程+LR（Logistic Regression）、GBDT（Gradient Boosting Decision Tree）+LR、FM（Factorization Machine）和FFM（Field-aware Factorization Machine）模型。在这些模型中，FM和FFM近年来表现突出，分别在Criteo和Avazu举办的CTR预测竞赛中夺得冠军。 本文基于对FFM模型的深度调研和使用经验，从原理、实现和应用几个方面对FFM进行探讨，希望能够从原理上解释FFM模型在点击率预估上取得优秀效果的原因。因为FFM是在FM的基础上改进得来的，所以，我们首先引入FM模型。 一、FM（因子分解机）1.1 FM的原理及推导因子分解机（Factorization Machine，简称FM），又称分解机。是由德国康斯坦茨大学的Steffen Rendle（现任职于Google）于2010年最早提出的，旨在解决大规模稀疏数据下的特征组合问题。在系统介绍FM之前，先了解一下在实际场景中，稀疏数据是怎样产生的。 假设一个广告分类的问题，根据用户和广告位相关的特征，预测用户是否点击了广告。元数据如下： Clicked? Country Day Ad_type 1 USA 26/11/15 Movie 0 China 1/7/14 Game 1 China 19/2/15 Game “Clicked？”是label，Country、Day、Ad_type是特征。由于三种特征都是categorical类型的，需要经过独热编码（One-Hot Encoding）转换成数值型特征。 Clicked? Country=USA Country=China Day=26/11/15 Day=1/7/14 Day=19/2/15 Ad_type=Movie Ad_type=Game 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 由上表可以看出，经过One-Hot编码之后，大部分样本数据特征是比较稀疏的。上面的样例中，每个样本有7维特征，但平均仅有3维特征具有非零值。实际上，这种情况并不是此例独有的，在真实应用场景中这种情况普遍存在。例如，CTR/CVR预测时，用户的性别、职业、教育水平、品类偏好、商品的品类等，经过One-Hot编码转换后都会导致样本数据的稀疏性。特别是商品品类这种类型的特征，如商品的末级品类约有550个，采用One-Hot编码生成550个数值特征，但每个样本的这550个特征，有且仅有一个是有效的（非零）。由此可见，数据稀疏性是实际问题中不可避免的挑战。 One-Hot编码的另一个特点就是导致特征空间大。例如，商品品类有550维特征，一个categorical特征转换为550维数值特征，特征空间剧增。 同时通过观察大量的样本数据可以发现，某些特征经过关联之后，与label之间的相关性就会提高。如：“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征，对用户的点击有着正向的影响。换句话说，来自“China”的用户很可能会在“Chinese New Year”有大量的浏览、购买行为，而在“Thanksgiving”却不会有特别的消费行为。这种关联特征与label的正向相关性在实际问题中是普遍存在的，如“化妆品”类商品与“女”性，“球类运动配件”的商品与“男”性，“电影票”的商品与“电影”品类偏好等。因此，引入两个特征的组合是非常有意义的。 表示特征之间的关联，最直接的方法的是构造组合特征。样本中特征之间的关联信息在one-hot编码和浅层学习模型（如LR、SVM）是做不到的。目前工业界主要有两种手段得到组合特征： 1）人工特征工程（数据分析＋人工构造）； 2）通过模型做组合特征的学习（深度学习方法、FM/FFM方法） 本章主要讨论FM和FFM用来学习特征之间的关联。多项式模型是包含特征组合的最直观的模型。在多项式模型中，特征 $x_i$ 和 $x_j$ 的组合采用 $x_i$ 表示，即 $x_i$ 和 $x_j$ 都非零时，组合特征 $x_ix_j$ 才有意义。从对比的角度，本文只讨论二阶多项式模型。模型的表达式如下： y(x)=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^nw_{ij}x_ix_j其中，$n$代表样本的特征数量，$x_i$是第$i$个特征的值，$w_0、w_i、w_{ij}$是模型的参数。 从这个公式可以看出，组合特征的参数一共有$\frac{n(n-1)}{2}$个，任意两个参数都是独立的。然而，在数据稀疏性普遍存在的实际应用场景中，二次项参数的训练是很困难的。其原因是，回归模型的参数$w$的学习结果就是从训练样本中计算充分统计量（凡是符合指数族分布的模型都具有此性质），而在这里交叉项的每一个参数$w_{ij}$的学习过程需要大量的$x_i$、$x_j$同时非零的训练样本数据。由于样本数据本来就很稀疏，能够满足“$x_i$和$x_j$都非零”的样本数就会更少。训练样本不充分，学到的参数$w_{ij}$就不是充分统计量结果，导致参数$w_{ij}$不准确，而这会严重影响模型预测的效果（performance）和稳定性。 那么，如何解决二次项参数的训练问题呢？矩阵分解提供了一种解决思路。在Model-based的协同过滤中，一个rating矩阵可以分解为user矩阵和item矩阵，每个user和item都可以采用一个隐向量表示。比如在下图中的例子，我们把每个user表示成一个二维向量，同时把每个item表示成一个二维向量，两个向量点积就是矩阵中user对item的打分。 类似地，所有二次项参数 $w_{ij}$可以组成一个对称阵 $W$（为了方便说明FM的由来，对角元素可以设置为正实数），那么这个矩阵就可以分解为 $W=V^TV$，$V$ 的第$ j$列便是第 $j$ 维特征的隐向量。换句话说，每个参数 $w_{ij}=⟨v_i,v_j⟩$，这就是FM模型的核心思想。因此，FM的模型方程为（本文不讨论FM的高阶形式） y(x)=w_0+\sum _{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n⟨vi,vj⟩x_ix_j \ \ \ \ \ \ ···（2）其中，$v_i$是第i维特征的隐向量，$⟨⋅,⋅⟩$代表向量点积，计算公式为 ⟨v_i,v_j⟩=\sum_{f=1}^kv_{i,f}·v_{j,f}隐向量的长度为$k(k&lt;&lt;n)$，包含k个描述特征的因子。具体解读一下这个公式 线性模型+交叉项：直观地看FM模型表达式，前两项是线性回归模型的表达式，最后一项是二阶特征交叉项（又称组合特征项），表示模型将两个互异的特征分量之间的关联信息考虑进来。用交叉项表示组合特征，从而建立特征与结果之间的非线性关系。 交叉项系数 → 隐向量内积：由于FM模型是在线性回归基础上加入了特征交叉项，模型求解时不直接求特征交叉项的系数$w_{ij}$（因为对应的组合特征数据稀疏，参数学习不充分），故而采用隐向量的内积$⟨v_i,v_j⟩$表示$w_{ij}$。具体的，FM求解过程中的做法是：对每一个特征分量$x_i$引入隐向量$v_i＝(v_{i,1},v_{i,2},⋯,v_{i,k})$，利用$v_iv^T_j$内积结果对交叉项的系数$w_{ij}$进行估计，公式表示：$ŵ_{ij}=v_iv^T_j$ 根据上式，二次项的参数数量减少为$kn$个，远少于多项式模型的参数数量。 此外，参数因子化表示后，使得$x_hx_i$的参数与$x_ix_j$的参数不再相互独立。这样我们就可以在样本系数的情况下相对合理地估计FM模型交叉项的参数。具体地： ⟨v_h,v_i⟩=\sum_{f=1}^k v_{h,f}·v_{i,f}⟨v_i,v_j⟩=\sum_{f=1}^k v_{i,f}·v_{j,f}$x_hx_i$与$x_ix_j$的系数分别为$⟨v_h,v_i⟩$和$⟨v_i,v_j⟩$，它们之间有共同项$v_i$，也就是说，所有包含$x_i$的非零组合特征（存在某个$j≠i$,使得$x_ix_j≠0$）的样本都可以用来学习隐向量$v_i$，这在很大程度上避免了数据系数行造成参数估计不准确的影响。而在多项式模型中，$w_{hi}$和$w_{ij}$是相互独立的。 显而易见，公式(2)是一个通用的拟合方程，可以采用不同的损失函数用于解决回归、二元分类等问题，比如可以采用MSE（Mean Square Error）损失函数来求解回归问题，也可以采用Hinge、Cross-Entropy损失来求解分类问题。当然，在进行二元分类时，FM的输出需要经过Sigmoid变换，这与Logistic回归是一样的。 FM应用场景 损失函数 说明 回归 均方误差（MSE）损失 Mean Square Error，与平方误差类似 二类分类 Hinge/Cross-Entopy损失 分类时，结果需要做sigmoid变换 直观上看，FM的复杂度是$O(kn^2)$，但是，通过下面的等价转换，可以将FM的二次项化简，其复杂度可以优化到$O(kn)$，即： \sum_{i=1}^n\sum_{j=i+1}^n⟨v_i,v_j⟩x_i,x_j=\frac{1}{2}\sum_{f=1}^k[(\sum_{i=1}^nv_{i,f}x_i)^2-\sum_{i=1}^nv_{i,f}^2x_i^2]下面给出详细推导： \sum_{i=1}^n\sum_{j=i+1}^n⟨v_i,v_j⟩x_ix_j $$$$ =\frac{1}{2}\sum_{i=1}^n\sum_{f=1}^n⟨v_i,v_j⟩x_ix_j-\frac{1}{2}\sum_{i=1}^n⟨v_i,v_i⟩x_ix_i\\=\frac{1}{2}(\sum_{i=1}^n\sum_{j=1}^n\sum_{f=1}^kv_{i,f}v_{j,f}x_ix_j-\sum_{i=1}^n\sum_{f=1}^kv_{i,f}v_{i,f}x_ix_i)\\=\frac{1}{2}\sum_{f=1}^k[(\sum_{i=1}^nv_{i,f}x_i)·(\sum_{j=1}^nv_{j,f}x_j)-\sum_{i=1}^nv_{i,f}^2x_i^2]\\=\frac{1}{2}\sum_{f=1}^k[(\sum_{i=1}^nv_{i,f}x_i)^2- \sum_{i=1}^nv_{i,f}^2x_i^2]解读第一步到第二部，这里用A表示系数矩阵V的上三角元素，B表示对角线上的交叉项系数。由于系数矩阵V是一个对称阵，所以下三角和上三角相等，有下式成立： A=\frac{1}{2}(2A+B)-\frac{1}{2}B其中， A=\sum_{i=1}^n\sum_{j=i+1}^n⟨v_i,v_j⟩x_ix_j,B=\sum_{i=1}^n⟨v_i,v_j⟩x_ix_i如果用随机梯度下降（SGD）法学系模型参数。那么模型各个参数的梯度如下： \frac{\partial}{\partial\theta}y\left(x\right)=\left\{\begin{array}{l} 1,\ \ if\ \theta\ is\ w_0\left(\textrm{常数项}\right)\\ x_i,\ if\ \theta\ is\ w_i\left(\textrm{线性项}\right)\\ x_i\underset{j=1}{\overset{n}{\varSigma}}v_{j,f}x_j-v_{i,f}x_{i}^{2},\ if\ \theta\ is\ v_{i,f}\left(\textrm{交叉项}\right)\\ \end{array}\right.其中，$v_{j,f}$是隐向量$v_j$的第f个元素。 由于$\underset{j=1}{\overset{n}{\varSigma}}v_{j,f}x_j$只与f有关，在参数迭代过程中，只需要计算第一次所有f的$\underset{j=1}{\overset{n}{\varSigma}}v_{j,f}x_j$，就能够方便地得到所有$v_{i,f}$的梯度。显然，计算所有f的$\underset{j=1}{\overset{n}{\varSigma}}v_{j,f}x_j$的复杂度是$O(kn)$；已知$\underset{j=1}{\overset{n}{\varSigma}}v_{j,f}x_j$时，计算每个参数梯度的复杂度是$O(n)$；得到梯度后，更新每个参数的复杂度是$O(1)$；模型参数一共有$nk+n+1$个。因此，FM参数训练的时间复杂度为$O(kn )$ 1.2 FM的优势综上可知，FM算法可以再线性时间内完成模型训练，以及对新样本作出预测，所以说FM是一个非常高效的模型。FM模型的核心作用可以概括为以下三个： 1）FM降低了交叉项参数学习不充分的影响：one-hot编码后的样本数据非常稀疏，组合特征更是如此。为了解决交叉项参数学习不充分、导致模型有偏或不稳定的问题。作者借鉴矩阵分解的思路：每一维特征用k维的隐向量表示，交叉项的参数$w_ij$用对应特征隐向量的内积表示，即$⟨v_i,v_j⟩$。这样参数学习由之前学习交叉项参数$w_{ij}$的过程，转变为学习$n$个单特征对应k维隐向量的过程。很明显，单特征参数（k维隐向量$v_i$）的学习要比交叉项参数$w_{ij}$学习的更加充分。示例说明：假如有10w条训练样本，其中出现女性特征的样本数为3w，出现男性特征的样本数为7w，出现汽车特征的样本数为2000，出现化妆品的样本数为1000。特征共现的样本数如下： 共现交叉特征 样本数 注 &lt;女性，汽车&gt; 500 同时出现&lt;女性，汽车&gt;的样本数 &lt;女性，化妆品&gt; 1000 同时出现&lt;女性，化妆品&gt;的样本数 &lt;男性，汽车&gt; 1500 同时出现&lt;男性，汽车&gt;的样本数 &lt;男性，化妆品&gt; 0 样本中无此特征组合项 &lt;女性，汽车&gt;的含义是女性看汽车广告。可以看到，但特征对应的样本数远大于组合特征对应的样本数。训练时，但特征参数相比交叉项特征参数会学习地更充分。因此，可以说FM降低了因数据稀疏，导致交叉项参数学习不充分的影响。 2）FM提升了模型预估能力。依然看上面的示例，样本中没有没有&lt;男性，化妆品&gt;交叉特征，即没有男性看化妆品广告的数据。如果yoga多项式模型来建模，对应的交叉项参数$w_{男性，化妆品}$是学不出来的，因为数据中没有对应的共现交叉特征。那么多项式模型就不能对出现的男性看化妆品广告场景给出准确地预估。FM模型是否能得到交叉项参数$w_{男性，化妆品}$呢？答案是肯定的。由于FM模型是把交叉项参数用对应的特征隐向量内积表示，这里表示为$w_{男性，化妆品}=$，即用男性特征隐向量$v_{男性}$和化妆品特征隐向量$v_{化妆品}$的内积表示交叉项参数$w_{男性，化妆品}$由于FM学习的参数就是单特征的隐向量，那么男性看化妆品广告的预估结果可以用$$得到。这样，即便训练集中没有出现男性看化妆品广告的样本，FM模型仍然可以用来预估，提升了预估的能力。 3）FM提升了参数学习效率：这个显而易见，参数个数由$(n2+n+1)(n2+n+1)$变为$(nk+n+1)(nk+n+1)$个，模型训练复杂度也由$O(mn^2)$变为$O(mnk)$。mm为训练样本数。对于训练样本和特征数而言，都是线性复杂度。此外，就FM模型本身而言，它是在多项式模型基础上对参数的计算做了调整，因此也有人把FM模型称为多项式的广义线性模型，也是恰如其分的。从交互项的角度看，FM仅仅是一个可以表示特征之间交互关系的函数表法式，可以推广到更高阶形式，即将多个互异特征分量之间的关联信息考虑进来。例如在广告业务场景中，如果考虑User-Ad-Context三个维度特征之间的关系，在FM模型中对应的degree为3。 最后一句话总结，FM最大特点和优势：FM模型对稀疏数据有更好的学习能力，通过交互项可以学习特征之间的关联关系，并且保证了学习效率和预估能力。 与其他模型相比，它的优势如下： FM是一种比较灵活的模型，通过合适的特征变换方式，FM可以模拟二阶多项式核的SVM模型、MF模型、SVD++模型等； 相比SVM的二阶多项式核而言，FM在样本稀疏的情况下是有优势的；而且，FM的训练/预测复杂度是线性的，而二项多项式核SVM需要计算核矩阵，核矩阵复杂度就是N平方。 相比MF而言，我们把MF中每一项的rating分改写为 $r_{ui}∼β_u+γ_i+x^T_uy_i$，从公式(2)中可以看出，这相当于只有两类特征 $u$ 和$ i$ 的FM模型。对于FM而言，我们可以加任意多的特征，比如user的历史购买平均值，item的历史购买平均值等，但是MF只能局限在两类特征。SVD++与MF类似，在特征的扩展性上都不如FM，在此不再赘述。 二、FFM（场感知分解机器）2.1 FFM的原理及推导场感知分解机器（Field-aware Factorization Machine ，简称FFM）最初的概念来自Yu-Chin Juan(阮毓钦，毕业于中国台湾大学，现在美国Criteo工作)与其比赛队员，是他们借鉴了来自Michael Jahrer的论文中的field概念提出了FM的升级版模型。通过引入field的概念，FFM把相同性质的特征归于同一个field。以上面的广告分类为例，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中。同理，商品的末级品类编码生成了550个特征，这550个特征都是说明商品所属的品类，因此它们也可以放到同一个field中。简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户性别、职业、品类偏好等。在FFM中，每一维特征 $x_i$，针对其它特征的每一种field $f_j$，都会学习一个隐向量 $v_{i,f_j}$。因此，隐向量不仅与特征相关，也与field相关。也就是说，“Day=26/11/15”这个特征与“Country”特征和“Ad_type”特征进行关联的时候使用不同的隐向量，这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来。 假设样本的 nn 个特征属于 ff 个field，那么FFM的二次项有 nfnf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型。根据FFM的field敏感特性，可以导出其模型方程。 y(x)=w_0+∑_{i=1}^nw_ix_i+∑_{i=1}^n∑_{j=i+1}^n⟨v_{i,fj},v_{j,f_i}⟩x_ix_j其中，$f_j$是第j个特征所属的field。如果隐向量的长度为k，那么FFM的二次参数有nfk个，远多于FM模型的nk个。此外，由于隐向量与field相关，FFM二次项并不能够化简，其复杂度为$O(kn^2)$。 下面以一个例子简单说明FFM的特征组合方式。输入记录如下 User Movie Genre Price YuChin 3Idiots Comedy, Drama $9.99 这条记录可以编码成5个特征，其中“Genre=Comedy”和“Genre=Drama”属于同一个field，“Price”是数值型，不用One-Hot编码转换。为了方便说明FFM的样本格式，我们将所有的特征和对应的field映射成整数编号。 Field name Field index Feature name Feature index User 1 User=YuChin 1 Movie 2 Movie=3Idiots 2 Genre 3 Genre=Comedy 3 Genre=Drama 4 Price 4 Price 5 那么，FFM的组合特征有10项，如下图所示。 其中，红色表示Field编码，蓝色表示Feature编码，绿色表示样本的组合特征取值（离散化后的结果）。二阶交叉项的系数是通过与Field相关的隐向量的内积得到的。如果单特征有n个，全部做二阶特征组合的话，会有$C^2_n=\frac{n(n−1)}{2}$个。 2.2 FFM的应用在DSP的场景中，FFM主要用来预估站内的CTR和CVR，即一个用户对一个商品的潜在点击率和点击后的转化率。 CTR和CVR预估模型都是在线下训练，然后用于线上预测。两个模型采用的特征大同小异，主要有三类：用户相关的特征、商品相关的特征、以及用户-商品匹配特征。用户相关的特征包括年龄、性别、职业、兴趣、品类偏好、浏览/购买品类等基本信息，以及用户近期点击量、购买量、消费额等统计信息。商品相关的特征包括所属品类、销量、价格、评分、历史CTR/CVR等信息。用户-商品匹配特征主要有浏览/购买品类匹配、浏览/购买商家匹配、兴趣偏好匹配等几个维度。 为了使用FFM方法，所有的特征必须转换成“field_id:feat_id:value”格式，field_id代表特征所属field的编号，feat_id是特征编号，value是特征的值。数值型的特征比较容易处理，只需分配单独的field编号，如用户评论得分、商品的历史CTR/CVR等。categorical特征需要经过One-Hot编码成数值型，编码产生的所有特征同属于一个field，而特征的值只能是0或1，如用户的性别、年龄段，商品的品类id等。除此之外，还有第三类特征，如用户浏览/购买品类，有多个品类id且用一个数值衡量用户浏览或购买每个品类商品的数量。这类特征按照categorical特征处理，不同的只是特征的值不是0或1，而是代表用户浏览或购买数量的数值。按前述方法得到field_id之后，再对转换后特征顺序编号，得到feat_id，特征的值也可以按照之前的方法获得。 CTR、CVR预估样本的类别是按不同方式获取的。CTR预估的正样本是站内点击的用户-商品记录，负样本是展现但未点击的记录；CVR预估的正样本是站内支付（发生转化）的用户-商品记录，负样本是点击但未支付的记录。构建出样本数据后，采用FFM训练预估模型，并测试模型的性能。 #(field) #(feature) AUC Logloss 站内CTR 39 2456 0.77 0.38 站内CVR 67 2441 0.92 0.13 由于模型是按天训练的，每天的性能指标可能会有些波动，但变化幅度不是很大。这个表的结果说明，站内CTR/CVR预估模型是非常有效的。 在训练FFM的过程中，有许多小细节值得特别关注。 第一，样本归一化。FFM默认是进行样本数据的归一化，即 pa.normpa.norm 为真；若此参数设置为假，很容易造成数据inf溢出，进而引起梯度计算的nan错误。因此，样本层面的数据是推荐进行归一化的。 第二，特征归一化。CTR/CVR模型采用了多种类型的源特征，包括数值型和categorical类型等。但是，categorical类编码后的特征取值只有0或1，较大的数值型特征会造成样本归一化后categorical类生成特征的值非常小，没有区分性。例如，一条用户-商品记录，用户为“男”性，商品的销量是5000个（假设其它特征的值为零），那么归一化后特征“sex=male”（性别为男）的值略小于0.0002，而“volume”（销量）的值近似为1。特征“sex=male”在这个样本中的作用几乎可以忽略不计，这是相当不合理的。因此，将源数值型特征的值归一化到 [0,1][0,1] 是非常必要的。 第三，省略零值特征。从FFM模型的表达式可以看出，零值特征对模型完全没有贡献。包含零值特征的一次项和组合项均为零，对于训练模型参数或者目标值预估是没有作用的。因此，可以省去零值特征，提高FFM模型训练和预测的速度，这也是稀疏样本采用FFM的显著优势。 2.3 FFM实现Yu-Chin Juan实现了一个C++版的FFM模型，源码可从Github下载[10]。这个版本的FFM省略了常数项和一次项，模型方程如下。 ϕ(w,x)=∑_{j1,j2∈C_2}⟨w_{j_1,f_2},w_{j_2,f_1}⟩x_{j_1}x_{j_2}其中，$C_2$是非零特征的二元组合，$j_1$是特征，属于field $f_1$，$w_{j_1,f_2}$是特征 $j_1$对field $f_2$ 的隐向量。此FFM模型采用logistic loss作为损失函数，和L2惩罚项，因此只能用于二元分类问题。 \underset{w}{min}∑_{i=1}^Llog(1+exp{−y_iϕ(w,x_i)})+\frac{λ}{2}‖w‖2其中，$y_i∈{−1,1}$是第 i个样本的label，L是训练样本数量，λ 是惩罚项系数。模型采用SGD优化，优化流程如下。参考 Algorithm1, 下面简单解释一下FFM的SGD优化过程。算法的输入 tr、va、pa 分别是训练样本集、验证样本集和训练参数设置。 根据样本特征数量（tr.ntr.n）、field的个数（tr.mtr.m）和训练参数（papa），生成初始化模型，即随机生成模型的参数； 如果归一化参数 pa.normpa.norm 为真，计算训练和验证样本的归一化系数，样本i的归一化系数为R[i]=\frac{1}{||X[i]||} 对每一轮迭代，如果随机更新参数 pa.randpa.rand 为真，随机打乱训练样本的顺序； 对每一个训练样本，执行如下操作: 计算每一个样本的FFM项，即公式中的输出 $ϕ$； 计算每一个样本的训练误差，如算法所示，这里采用的是交叉熵损失函数 $log(1+eϕ)$； 利用单个样本的损失函数计算梯度 $gΦ$，再根据梯度更新模型参数； 对每一个验证样本，计算样本的FFM输出，计算验证误差； 重复步骤3~5，直到迭代结束或验证误差达到最小。 在SGD寻优时，代码采用了一些小技巧，对于提升计算效率是非常有效的。 第一，梯度分步计算。采用SGD训练FFM模型时，只采用单个样本的损失函数来计算模型参数的梯度。 L=L_{err}+L_{reg}=log(1+exp\{−y_iϕ(w,x_i)\})+\frac{λ}{2}‖w‖^2\frac{∂L}{∂w}=\frac{∂L_{err}}{∂ϕ}\frac{∂ϕ}{∂w}+\frac{∂L_{reg}}{∂w}上面的公式表明，$\frac{∂L_{err}}{∂ϕ}$与具体的模型参数无关。因此，每次更新模型时，只需计算一次，之后直接调用$\frac{∂L_{err}}{∂ϕ}$的值即可。对于更新 $nfk$个模型参数，这种方式能够极大提升运算效率。 第二，自适应学习率。此版本的FFM实现没有采用常用的指数递减的学习率更新策略，而是利用 $nfk$ 个浮点数的临时空间，自适应地更新学习率。学习率是参考AdaGrad算法计算的[11]，按如下方式更新 w_{j_1,j_2}^ {'}=w_{j_1,f_2}-\frac{η}{\sqrt{1+\sum_t(g^t_{w_{j_1,f_2}})^2}}·g_{w_{j_1,f_2}}其中，$w_{j_1,f_2}$是特征 $j_1$ 对field $f_2$ 隐向量的一个元素，元素下标未标出；$g_{w_{j_1,f_2}}$是损失函数对参数 $w_{j_1,f_2}$的梯度；$g^t_{w_{j_1,f_2}}$是第 t 次迭代的梯度；η是初始学习率。可以看出，随着迭代的进行，每个参数的历史梯度会慢慢累加，导致每个参数的学习率逐渐减小。另外，每个参数的学习率更新速度是不同的，与其历史梯度有关，根据AdaGrad的特点，对于样本比较稀疏的特征，学习率高于样本比较密集的特征，因此每个参数既可以比较快速达到最优，也不会导致验证误差出现很大的震荡。 第三，OpenMP多核并行计算。OpenMP是用于共享内存并行系统的多处理器程序设计的编译方案，便于移植和多核扩展[12]。FFM的源码采用了OpenMP的API，对参数训练过程SGD进行了多线程扩展，支持多线程编译。因此，OpenMP技术极大地提高了FFM的训练效率和多核CPU的利用率。在训练模型时，输入的训练参数ns_threads指定了线程数量，一般设定为CPU的核心数，便于完全利用CPU资源。 第四，SSE3指令并行编程。SSE3全称为数据流单指令多数据扩展指令集3，是CPU对数据层并行的关键指令，主要用于多媒体和游戏的应用程序中。SSE3指令采用128位的寄存器，同时操作4个单精度浮点数或整数。SSE3指令的功能非常类似于向量运算。例如，a 和 b 采用SSE3指令相加（a 和 b 分别包含4个数据），其功能是 a 中的4个元素与 b 中4个元素对应相加，得到4个相加后的值。采用SSE3指令后，向量运算的速度更加快捷，这对包含大量向量运算的FFM模型是非常有利的。 除了上面的技巧之外，FFM的实现中还有很多调优技巧需要探索。例如，代码是按field和特征的编号申请参数空间的，如果选取了非连续或过大的编号，就会造成大量的内存浪费；在每个样本中加入值为1的新特征，相当于引入了因子化的一次项，避免了缺少一次项带来的模型偏差等。 后记本文主要介绍了FFM的思路来源和理论原理，并结合源码说明FFM的实际应用和一些小细节。从理论上分析，FFM的参数因子化方式具有一些显著的优势，特别适合处理样本稀疏性问题，且确保了较好的性能；从应用结果来看，站内CTR/CVR预估采用FFM是非常合理的，各项指标都说明了FFM在点击率预估方面的卓越表现。当然，FFM不一定适用于所有场景且具有超越其他模型的性能，合适的应用场景才能成就FFM的“威名”。 参考文献 http://blog.csdn.net/lilyth_lilyth/article/details/48032119 http://www.cnblogs.com/Matrix_Yao/p/4773221.html http://www.herbrich.me/papers/adclicksfacebook.pdf https://www.kaggle.com/c/criteo-display-ad-challenge https://www.kaggle.com/c/avazu-ctr-prediction https://en.wikipedia.org/wiki/Demand-side_platform http://www.algo.uni-konstanz.de/members/rendle/pdf/Rendle2010FM.pdf http://www.cs.cmu.edu/~wcohen/10-605/2015-guest-lecture/FM.pdf http://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf https://github.com/guestwalk/libffm https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad http://openmp.org/wp/openmp-specifications/ http://blog.csdn.net/gengshenghong/article/details/7008704 https://kaggle2.blob.core.windows.net/competitions/kddcup2012/2748/media/Opera.pdf]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>FM</tag>
        <tag>FFM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（25）：最速下降法、牛顿法、拟牛顿法]]></title>
    <url>%2F2017%2F07%2F09%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8825%EF%BC%89%EF%BC%9A%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D%E6%B3%95%E3%80%81%E7%89%9B%E9%A1%BF%E6%B3%95%E3%80%81%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、最速下降法最速下降法，又称为梯度下降法，是无约束最优化领域中最简单的算法，单独就这种算法来看，属于早就“过时”了的一种算法。但是，它的理念是其他某些算法的组成部分，或者说是在其他算法中，也有最速下降法的影子。它是一种迭代算法，每一步需要求解目标函数的梯度向量。 假设$f(x)$是$R^n$上具有一阶连续偏导的函数。要求解的无约束最优化问题是： \underset {x\in R^n}{min} \ f(x)梯度下降法是一种迭代算法。选取适当的初值$x^{(0)}$，不断迭代，更新$x$的值，进行目标函数的极小化，直到收敛。由于负梯度方向是使函数值下降最快的方向，在迭代的每一步，以负梯度方向更新$x$的值，从而达到减少函数值的目的。 由于$f(x)$具有一阶连续偏导数，若第$k$次迭代值为$x^{(k)}$，则可将$f(x)$在$x^{(k)}$附近进行一阶泰勒展开： f(x)=f(x^{(k)})+g_k^T(x-x^{(k)})这里，$g_k=g(x^{(k)})=∇f(x^{(k)})$为$f(x)$在$x^{(k)}$的梯度。 求出第$k+1$次迭代值$x^{(k+1)}$: x^{(k+1)}\leftarrow x^{(k)}+\lambda_kp_k其中$p_k$是搜索方向，取负梯度方向$p_k=-∇f(x^{(k)})$,$\lambda _k$是步长，由一维搜索确定，即$\lambda_k$使得 f(x^{(k)}+\lambda_kp_k)=\underset {\lambda≥0}{min} \ f(x^{(k)}+\lambda p_k)算法步骤如下： 输入：目标函数$f(x)$，梯度函数$g(x)=∇f(x)$，计算精度$\xi$; 输出：$f(x)$的极小点$x^*$ 取初始值$x^{(0)}\in R^{n}$，置$k=0$ 计算 $f(x^{(k)})$ 计算梯度$g_k=g(x^{(k)})$，当$||g_k||＜\xi$时，停止迭代，令$x^*=x^{(k)}$；否则，令$p_k = -g(x^{(k)})$，求$\lambda_k$，使f(x^{(k)}+\lambda_kp_k)=\underset {\lambda≥0}{min} \ f(x^{(k)}+\lambda p_k) 置$x^{(k+1 )}=x^{(k)}+\lambda _kp_k$，计算$f(x^{(k+1)})$当$||f(x^{(k+1)})-f(x^{(k)})||＜\xi$或$||x^{(k=1)}-x^{(k)}||＜\xi$，停止迭代，令$x^*=x^{(k+1)}$ 否则，置$k=k+1$，转到步骤3。 当目标函数是凸函数时，梯度下降法的解释全局最优解。一般情况下，其解不保证是全局最优解。梯度下降法的收敛速度也未必是很快的。 二、牛顿法考虑如下无约束的极小化问题 \underset{X}{min}\ f(x)其中$X=(x_1,x_2,x_3，···，x_N)^T \in R^N$这里我们假定$f$为凸函数，且两阶连续可微。记$x^*$为目标函数的极小值。 为了简单起见，首先考虑$N=1$的简单情形（此时目标函数$f(X)$变为$f(x)$）。牛顿法的基本思想是：在现有极小值估计值的附近对$f(x)$作二阶泰勒展开，进而找极小点的下一个估计值。设$x_k$为当前的极小点估计值，则 f(x) = f(x_k)+ f^{'}(x_k)(x-x_k)+\frac{1}{2}f^{''}(x_k)(x-x_k)^2表示$f(x)$在$x_k$附近的二阶泰勒展开式（略去了关于$x-x_k$的高阶项）。由于求得是最值，由极值必要条件可知，$f(x)$应该满足f^{'}(x)=0，即 f^{'}(x_k)+f^{''}(x_k)(x-x_k)^2从而求得 x=x_k-\frac{f^{'}(x_k)}{f^{''}{(x_k)}}于是，若给定初始值$x_0$，则可以构造如下的迭代格式 x_{k+1}=x_k-\frac{f^{'}(x_k)}{f^{''}{(x_k)}}于是，若给定初始值$x_0$，则可以构造如下的迭代格式 x_{k+1}=x_k-\frac{f^{'}(x_k)}{f^{''}{(x_k)}} , \ k=0,1,···产生序列$\{x_k\}$来逼近$f(x)$的极小点。在一定条件下$\{x_k\}$可以收敛到$f(x)$的极小点。 对于$N&gt;1$的情形，二阶泰勒展开式可以做推广，此时 f(X)=f(X_k)+∇f(X_k)\ ·\ (X-X_k)+\frac{1}{2}· (X-X_k)^T·∇^2f(X_k)·(X-X_k)其中$∇f$为$f$的梯度向量，$∇^2f$为海森矩阵，其定义分别为 \nabla f=\left[\begin{array}{c} \frac{\partial f}{\partial x_1}\\ \frac{\partial f}{\partial x_2}\\ ···\\ \frac{\partial f}{\partial x_N}\\ \end{array}\right],\\nabla f^2=\left[\begin{matrix} \frac{\partial^2f}{\partial x_{1}^{2}}& \frac{\partial^2f}{\partial x_1\partial x_2}& ···& \frac{\partial^2f}{\partial x_1\partial x_N}\\ \frac{\partial^2f}{\partial x_2\partial x_1}& \frac{\partial^2f}{\partial x_{2}^{2}}& ···& \frac{\partial^2f}{\partial x_2\partial x_N}\\ ···& ···& ···& ···\\ \frac{\partial^2f}{\partial x_N\partial x_1}& \frac{\partial^2f}{\partial x_N\partial x_2}& ···& \frac{\partial^2f}{\partial x_{N}^{2}}\\ \end{matrix}\right]注意，$∇f$和$∇^2f$中的元素均为关于$X$的函数，以下分别将其简记为$g$和$H$。特别地，若$f$的混合偏导数可交换次序(即对$\forall\ i,j$，成立$\frac{\partial^2f}{\partial x_i\partial x_j}=\frac{\partial^2f}{\partial x_j\partial x_i}$)，则海森矩阵$H$为对称矩阵，而$∇f(X_k)$和$∇^2f(X_k )$则表示将$X$取为$X_k$后得到的实值向量和矩阵，以下分别将其简记为$g_k$和$H_k$（这里字母g表示gradient，H表示Hessian） 同样地，由于是求极小点，极值必要条件要求它为$f(X)$的驻点，即 ∇f(X)=0亦即对二阶泰勒展开作用一个梯度算子 g_k+H_k·(X-X_k)=0进一步，若矩阵$H_k$非奇异，则可解得 X=X_k-H_k^{-1}·g_k于是，若给定初始值$X_0$，则同样可以构造出迭代格式 X_{k+1}=X_k-H^{-1}_k·g_k这就是原始的牛顿迭代法，其迭代格式中的搜索方向$d_k=-H^{-1}_k·g_k$称为牛顿方向。下面给出牛顿法的完整算法描述： 给定初值$X_0$和精度阀值$\xi$，并令$k:=0$ 计算$g_k$和$H_k$ 若$||g_k||＜\xi$，则停止迭代；否则确定搜索方向$d_k=-H^{-1}_k·g_k$ 计算新的迭代点$X_{k+1}:=X_k+d_k$ 令k:=k+1，转至步2 当目标函数是二次函数时，由于二次泰勒展开函数与原目标函数不是近似而是完全相同的二次式，海森矩阵退化成一个常数矩阵，从任一初始点出发，秩序一步迭代即可达到$f(X)$的极小点$X^*$，因此牛顿法是一种具有二次收敛性的算法。对于非二次函数，若函数的二次形性态较强，或迭代点已进入极小点的领域，则其收敛速度也是很快的，这是牛顿法的主要优点。 但原始牛顿法由于迭代公式中没有步长因子，而是定步长迭代，对于非二次型目标函数，有时会使函数值上升，即出现$f(X_{k=1})&gt;f(X_k )$的情况，这表明原始牛顿法不能保证函数值稳定地下降，在严重的情况下甚至可能造成迭代点列$\{X_k\}$的发散而导致计算失败。 为了消除这个弊病，人们提出了“阻尼牛顿法”，阻尼牛顿法每次迭代的方向仍然采用$d_k$，但每次迭代需沿此方向作一维搜索（line search），寻求最优的步长因子$\lambda _k $，即 \lambda_k =arg \underset{\lambda \in R}{min}f(X_k+\lambda d_k)下面给出阻尼牛顿法的完整算法描述： 给定初值$X_0$和精度阀值$\xi$，并令$k:=0$ 计算$g_k$和$H_k$ 若$||g_k||＜\xi$，则停止迭代；否则确定搜索方向$d_k=-H^{-1}_k·g_k$ 利用$\lambda_k =arg \underset{\lambda \in R}{min}f(X_k+\lambda d_k)$得到步长$\lambda _k$，计算新的迭代点$X_{k+1}:=X_k+d_k$ 令k:=k+1，转至步2 至此完成了牛顿法的算法介绍，接下来对其做个小结： 牛顿法是梯度下降法的进一步发展，梯度下降法利用目标函数的一阶偏导数信息、以负梯度方向作为搜索方向，只考虑目标函数在迭代点的局部性质；而牛顿法不仅使用目标函数的一阶偏导数，还进一步利用了目标函数的二阶偏导数，这样就考虑了梯度变化的趋势，因而能更全面地确定合适的搜索方向加快收敛，它具二阶收敛速度。但牛顿法主要存在以下两个缺点： 对目标函数有较严格的要求。函数必须具有连续的一、二阶偏导数，海森矩阵必须正定。 极端相当复杂，除需要计算梯度以外，还需要计算二阶偏导数矩阵和它的逆矩阵。计算量、存储量均很大，且均以维数$N$的平方比增加，当$N$很大时这个问题更加突出。 三、拟牛顿法牛顿法虽然收敛速度快，但是计算过程中需要计算目标函数的二阶偏导数，计算复杂度较大。而且有时目标函数的海森矩阵无法保持正定，从而使牛顿法失效。为了克服这两个问题，人们提出了拟牛顿法。这个方法的基本思想是：不用二阶偏导数而构造出可以近似海森矩阵或者海森矩阵的逆的正定对称阵，在拟牛顿的条件下优化目标函数。不同的构造方法就产生了不同的拟牛顿法。 也有人把“拟牛顿法”翻译成“准牛顿法”，其实都是表示“类似于牛顿法”的意思，因此只是对算法中用来计算搜索方向的海森矩阵（或海森矩阵的逆）作了近似计算罢了。 在介绍具体的拟牛顿法之前，我们先推到一个拟牛顿条件，或者叫拟牛顿方程，还有的叫做割线条件。因为对海森矩阵（或海森矩阵的逆）做近似总不能随便近似，也需要理论指导，而拟牛顿条件则是用来提供理论指导的，它指出了用来近似的矩阵应该满足的条件。 为明确起见，下文中用$B$表示对海森矩阵$H$本身的近似，而用$D$表示对海森矩阵的逆$H^{-1}$的近似，即$B≈H,D≈H^{-1}$ 3.1 拟牛顿条件设经过$k+1$次迭代后得到$X_{k+1}$，此时将目标函数$f(X)$在$X_{k+1}$附近作泰勒展开，取二阶近似，得到 f(X)≈ f(X_{k+1})+∇f(X_{k+1})\ ·\ (X-X_{k+1})+\frac{1}{2}· (X-X_{k+1})^T·∇^2f(X_{k+1})·(X-X_{k+1})在两边同时作用一个梯度算子$∇$，可得 ∇f(X)≈∇f（X_{k+1}）+H_{k+1}·(X-X_{k+ 1})取$X=X_k$并整理，可得 g_{k+1}-g_k≈H_{k+1}·(X_{k+1}-X_k)若引入记号$s_k=X_{k+1}， y_k=g_{k+1}-g_k$则可以改写成 y_k≈H_{k+1}·s_k或者 s_k≈H^{-1}_{k+1}·y_k这就是所谓的拟牛顿条件，它对迭代过程中的海森矩阵$H_{k+1}$作约束，因此，对$H_{k+1}$做近似的$B_{k+1}$，以及对$H_{k+1}^{-1}$做近似的$D_{k+1 }$可以将 y_k≈H_{k+1}·s_k或者 s_k≈H^{-1}_{k+1}·y_k作为指导。 3.2 DFP算法DFP算法是以William C.Davidon、Roger Fletcher、Michael J.D.Powell三个人的名字的首字母命名的，它由Davidon于1959年首先提出，是最早的拟牛顿法。该算法的核心是：通过迭代的方法，对$H_{k+1}^{-1}$做近似，迭代格式为 D_{k+1}=D_k+\Delta D_k , k=0,1,2,···其中的$D_0$通常取为单位矩阵$I$。因此，关键是每一步的校正矩阵$\Delta D_k$如何构造。 注意，我们猜想$\Delta D_k$可能与$s_k,y_k$和$D_k$发生关联。这里，我们采用“待定法”，即首先将$\Delta D_k$待定城某种形式，然后结合拟牛顿条件来进行推导。 那将$\Delta D_k$待定成什么形式呢？说起来比较tricky，我们将其待定为 \Delta D_k=\alpha uu^T+\beta vv^T其中$\alpha$和$\beta$为待定向量。从形式上看，这种待定公式至少保证了矩阵$\Delta D_k$的对称性（因为$uu^T$和$vv^T$均为对称矩阵） 将其代入迭代式，并结合拟牛顿指导条件，可得 s_k=D_ky_k+\alpha uu^Ty_k+\beta vv^Ty_k将其改写一下 s_k=D_ky_k+u(\alpha u^Ty_k)+v(\beta v^Ty_k)\\=D_ky_k+(\alpha u^Ty_k)u+(\beta v^Ty_k)v括号中为两个数，既然是数，我们不妨作如下简单赋值 \alpha u^Ty_k=1 ，\ \beta v^Ty_k=-1$$即$$\alpha=\frac{1}{u^Ty_k},\beta=-\frac{1}{v^Ty_k}其中向量$u,v$仍有待确定。 我们把$s_k=D_ky_k+u-v$写作 u-v=s_k-D_ky_k要上式成立，不妨直接取 u=s_k,v=D_ky_k代入求$\alpha$和$\beta$的式子，便得到 \alpha=\frac{1}{s^T_ky_k},\beta=\frac{1}{(D_ky_k)^Ty_k}=-\frac{1}{y^T_kD_ky_k}其中第二个式子用到了$D_k$的对称性。至此，我们已经将校正矩阵$\Delta D_k$构造出来了，我们就可以得到 \Delta D_k=\frac{s_ks_k^T}{s_k^Ty_k}-\frac{D_ky_ky_k^TD_k}{y_k^TD_ky_k}综上，我们给出DFP算法的一个完整的算法描述。 给定初值$X_0$和精度阀值$\xi$，并令$k:=0$ 确定搜索方向$d_k=-D^{-1}_k·g_k$ 利用$\lambda_k =arg \underset{\lambda \in R}{min}f(X_k+\lambda d_k)$得到步长$\lambda _k$，令$s_k=\lambda_kd_k$，计算新的迭代点$X_{k+1}:=X_k+s_k$ 若$||g_{k=1}||&lt;\xi$，则算法结束 计算$y_k=g_{k+1}-g_k$ 计算D_{k+1}=D_k+\frac{s_ks_k^T}{s_k^Ty_k}-\frac{D_ky_ky_k^TD_k}{y_k^TD_ky_k} 令$k:=k+1$转至步骤2. 3.3 BFGS算法BFGS算法是以其发明者Broyden、Fletcher、Goldfarb和Shanno四个人的名字的首字母命名的。与DFP算法相比，BFGS算法性能更加。目前它已成为求解无约束非线性优化问题最常用的方法之一。BFGS算法已有较完善的局部收敛理论，对其全局收敛的研究也取得了重要成果。 BFGS算法中核心公式的推导过程和DFP完全类似，只是互换了其中$s_k$和$y_k$的位置。需要注意的是，BFGS算法是直接逼近海森矩阵，即$B_k≈H_k$,仍采用迭代方法，设迭代格式为 B_{k+1}=B_k+\Delta B_k , k=0,1,2,···其中的$B_0$也常取为单位矩阵$I$。因此，关键是每一步的校正矩阵$\Delta B_k$如何构造，同样，将其待定为 \Delta B_k=\alpha uu^T+\beta vv^T将其代入上式，并结合指导条件$y_k≈H_{k+1}·s_k$，可得 y_k=B_ks_k+(au^Ts_k)u+(\beta v^Ts_k)v通过令$au^Ts_k=1,\beta v^Ts_k=-1$,以及 u=y_k,v=B_ks_k$$可以算得 $$\alpha=\frac{1}{y^T_ks_k},\beta = -\frac{1}{s^T_kB_ks_k}综上，便得到了如下的校正矩阵$\Delta B_k$的公式 \Delta B_k=\frac{y_ky_k^T}{y_k^Ts_k}-\frac{B_ks_ks_k^TB_k}{s^T_kB_ks_k}好了，现在把矩阵$\Delta B_k$和$\Delta D_k$拿出来对比一下，除了你将$D$换成$B$外，就是把$s_k$和$y_k$互换了一下位置。 最后，给出BFGS算法的一个完整算法描述： 给定初值$X_0$和精度阀值$\xi$，并令$k:=0$ 确定搜索方向$d_k=-B^{-1}_k·g_k$ 利用$\lambda_k =arg \underset{\lambda \in R}{min}f(X_k+\lambda d_k)$得到步长$\lambda _k$，令$s_k=\lambda_kd_k$，计算新的迭代点$X_{k+1}:=X_k+s_k$ 若$||g_{k=1}||&lt;\xi$，则算法结束 计算$y_k=g_{k+1}-g_k$ 计算B_{k+1}=B_k+\frac{y_ky_k^T}{y_k^Ts_k}-\frac{B_ks_ks_k^TB_k}{s^T_kB_ks_k} 令$k:=k+1$转至步骤2. 3.4 L-BFGS算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>损失函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（24）：机器学习中的损失函数]]></title>
    <url>%2F2017%2F07%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8824%EF%BC%89%EF%BC%9A%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[损失函数（loss function）是用来估量模型的预测值f(x)与真实值$Y$不一致的程度，它是一个非负实数值函数，通常使用$L(Y,f(x))$来表示，损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数的重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下的式子： \theta^* = argmin_\theta \frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i;\theta))+\lambda Φ(θ) 前面的均值函数表示的是经验风险函数，$L$代表的是损失函数，后面的$Φ$是正则化项（regularizer）或者叫惩罚项（penalty term）,它可以是$L_1$，也可以是$L_2$等其他的正则函数。整个式子表示的意思是找到使目标函数最小时的$\theta$值。下面列出集中常见的损失函数。 一、对数损失函数（逻辑回归）有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到，而逻辑回归得到的并不是平方损失。在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数（即$max F(y, f(x)) —&gt; min -F(y, f(x))$)。从损失函数的视角来看，它就成了log损失函数了。 Log损失函数的标准形式： L(Y,P(Y|X))=-logP(Y|X)刚刚说到，取对数是为了方便计算极大似然估计，因为在MLE中，直接求导比较困难，所以通常都是先取对数再求导找极值点。损失函数$L(Y.P(Y|X))$表达的是样本在分类$Y$的情况下，使概率$P(Y|X)$达到最大值（换言之，就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者什么样的参数才能使我们观测到目前这组数据的概率最大）。因为log函数是单调递增的，所以$logP(Y|X)$也会达到最大值，因此在前面加上负号之后，最大化$P(Y|X)$就等价于最小化$L$了。 logistic回归的$P(y|x)$表达式如下（为了将类别标签y统一为1和0，下面将表达式分开表示）： P\left(Y=y^{\left(i\right)}|x^{\left(i\right)};\theta\right)=\left\{\begin{array}{l} h_{\theta}\left(x^{\left(i\right)}\right)=\frac{1}{1+e^{-\theta^Tx}},\,\,y^{\left(i\right)}=1\\ 1-h_{\theta}\left(x^{\left(i\right)}\right)=\frac{e^{-\theta^Tx}}{1+e^{-\theta^Tx}},\,\,y^{\left(i\right)}=0\\ \end{array}\right.将上面的公式合并在一起，可得到第$i$个样本正确预测的概率： P(y^{(i)}|x^{(i)};\theta)=(h_\theta(x^{(i)}))^{y(i)}·(1-h_\theta(x^{(i)}))^{1-y(i)}上式是对一个样本进行建模的数据表达。对于所有的样本，假设每条样本生成过程独立，在整个样本空间中（N个样本）的概率分布为： P\left(Y\ | \ X;\theta\right)=\prod_{i=1}^N{\left(\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)^{y^{\left(i\right)}}\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)^{1-y^{\left(i\right)}}\right)}将上式代入到对数损失函数中，得到最终的损失函数为： J(\theta) = -\frac{1}{N}\sum_{i=1}^N{y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)}之所以有人认为逻辑回归是平方损失，是因为在使用梯度下降来求最优解的时候，它的迭代式子与平方损失求导后的式子非常相似，从而给人一种直观上的错觉。 二、平方损失函数（最小二乘法，Ordinary Least Squares）最小二乘法是线性回归的一种，OLS将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理，可以参考【central limit theorem】），最后通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。换言之，OLS是基于距离的，而这个距离就是我们用的最多的欧几里得距离。为什么它会选择使用欧式距离作为误差度量呢（即Mean squared error， MSE），主要有以下几个原因： 简单，计算方便； 欧氏距离是一种很好的相似性度量标准； 在不同的表示域变换后特征性质不变。 平方损失（Square loss）的标准形式如下：L(Y,f(X))=(Y-f(x))^2当样本个数为n时，此时的损失函数变为：$$L(Y,f(X))=\sum_{i=1}^n(Y-f(X))^2$$$Y-f(X)$表示的是残差，整个式子表示的是残差的平方和，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是最小化残差的平方和（residual sum of squares，RSS）。 而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标，公式如下： MSE=\frac{1}{N}\sum_{i=1}^N(\tilde{Y_i}-Y_i)^2上面提到了线性回归，这里额外补充一句，我们通常说的线性有两种情况，一种是因变量y是自变量x的线性函数，一种是因变量y是参数α的线性函数。在机器学习中，通常指的都是后一种情况。 三、指数损失函数（Adaboost）学过Adaboost算法的人都知道，它是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。在Adaboost中，经过m此迭代之后，可以得到$f_m(x)$: f_m(x)=f_{m-1}(x)+a_mG_m(x)Adaboost每次迭代时的目的是为了找到最小化下列式子时的参数$a$和G： arg\underset{a,G}{min}=\sum_{i=1}^Nexp[-y_i(f_{m-1}(x_i)+aG(x_i))]而指数损失函数(exp-loss）的标准形式如下: L(y,f(x))=exp[-yf(x)]可以看出，Adaboost的目标式子就是指数损失，在给定N个样本的情况下，Adaboost的损失函数为： L(y,f(x))=\frac{1}{N}\sum_{i=1}^nexp[-y_if(x_i)]四、Hinge损失函数（SVM）4.1 Hinge损失函数（SVM）线性支持向量机学习除了原始最优化问题，还有另外一种解释，就是最优化以下目标函数： \sum_i^{N}[1-y_i(w·x_i+b)]_++\lambda||w||^2目标函数的第一项是经验损失或经验风险，函数 L(y·(w·x+b))=[1-y(w·x+b)]_+称为合页损失函数（hinge loss function）。下标”+”表示以下取正值的函数： \left[z\right]_+=\left\{\begin{array}{l} z\ ,\ z>0\\ 0\ ,\ z\le 0\\ \end{array}\right.这就是说，当样本点$(x_i,y_i)$被正确分类且函数间隔（确信度）$y_i(w·x_i+b)$大于1时，损失是0，否则损失是$1-y_i(w·x_i+b)$。目标函数的第二项是系数为$\lambda$的$w$的$L_2$范数，是正则化项。 接下来证明线性支持向量机原始最优化问题： \underset{w,b,\xi}{\min}\ \frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i} s.t.\ \ y_i\left( w·x_i+b \right) \geqslant 1-\xi _i\ ,\ i=1,2,···,N \xi _i\geqslant 0,\ i=1,2,···\mathrm{，}N等价于最优化问题 \underset{w,b}{min }\sum_i^{N}[1-y_i(w·x_i+b)]_++\lambda||w||^2先令$[1-y_i(w·x_i+b)]_+=\xi_i$，则$\xi_i≥0$，第二个约束条件成立；由$[1-y_i(w·x_i+b)]_+=\xi_i$，当$1-y_i(w·x_i+b)&gt;0$时，有$y_i(w·x_i+b)=1-\xi_i$;当$1-y_i(w·x_i+b)≤0$时，$\xi_i=0$，有$y_i(w·x_i+b)≥1-\xi_i$，所以第一个约束条件成立。所以两个约束条件都满足，最优化问题可以写作 \underset{w,b}{min}\sum_{i=1}^N\xi_i+\lambda||w||^2若取$\lambda =\frac{1}{2C}$则 \underset{w,b}{min} \frac{1}{C}(\frac{1}{2} ||w||^2+C\sum_{i=1}^N \xi_i)与原始最优化问题等价。 合页损失函数图像如图所示，横轴是函数间隔$y(w·x+b)$，纵轴是损失。由于函数形状像一个合页，故名合页损失函数。 图中还画出了0-1损失函数，可以认为它是一个二类分类问题的真正的损失函数，而合页损失函数是0-1损失函数的上界。由于0-1损失函数不是连续可导的，直接优化其构成的目标函数比较困难，可以认为线性支持向量机是优化由0-1损失函数的上界（合页损失函数）构成的目标函数。这时的上界损失函数又称为代理损失函数（surrogate function）。图中虚线显示的是感知机的损失函数$[-y_i(w·x_i+b)]_+$。这时当样本点$(x_i,y_i)$被正确分类时，损失是0，否则损失是$-y_i(w·x_i+b)$，相比之下，合页损失函数不仅要分类正确，而且确信度足够高时损失才是0，也就是说，合页损失函数对学习有更高的要求 4.2 逻辑斯谛回归和SVM的损失函数对比我们先来看一下带松弛变量的 SVM 和正则化的逻辑回归它们的损失函数：其中 $g(z)=(1+exp(−z))^{−1}$可以将两者统一起来: 这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重,两者的根本目的都是一样的。 svm考虑局部（支持向量），而logistic回归考虑全局，就像大学里的辅导员和教师间的区别。 辅导员关心的是挂科边缘的人，常常找他们谈话，告诫他们一定得好好学习，不要浪费大好青春，挂科了会拿不到毕业证、学位证等等，相反，对于那些相对优秀或者良好的学生，他们却很少去问，因为辅导员相信他们一定会按部就班的做好分内的事；而大学里的教师却不是这样的，他们关心的是班里的整体情况，大家是不是基本都理解了，平均分怎么样，至于某个人的分数是59还是61，他们倒不是很在意。 总结： LR采用log损失，SVM采用合页损失。 LR对异常值敏感，SVM对异常值不敏感。 在训练集较小时，SVM较适用，而LR需要较多的样本。 LR模型找到的那个超平面，是尽量让所有点都远离他，而SVM寻找的那个超平面，是只让最靠近中间分割线的那些点尽量远离，即只用到那些支持向量的样本。 对非线性问题的处理方式不同，LR主要靠特征构造，必须组合交叉特征，特征离散化。SVM也可以这样，还可以通过kernel。 svm 更多的属于非参数模型，而logistic regression 是参数模型，本质不同。其区别就可以参考参数模型和非参模型的区别 那怎么根据特征数量和样本量来选择SVM和LR模型呢？Andrew NG的课程中给出了以下建议： 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。(LR和不带核函数的SVM比较类似。)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>损失函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（23）：TF-IDF与余弦相似度]]></title>
    <url>%2F2017%2F07%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8823%EF%BC%89%EF%BC%9ATF-IDF%E4%B8%8E%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[TF-IDF(term frequency=inverse document frequency)是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常备搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。 一、原理设想现在我们正在阅读新闻，如何最快速的了解新闻的主旨？毫无疑问——关键词。TF-IDF就具有这样的能力：提取关键词。 1.1 TF假设一个词在一篇文章中出现的次数越多，那么它就越”紧扣主题”。以本文为例，我们可以统计词频(TF)，不难发现“TF-IDF”,“应用”、“原理”是出现频率很高的词，后文称keywords。这符合我们的假设，但是有些词却出现的次数更多，如：的、是、有等。这类词语没有明确意义，我们称为停顿词(Stopwords)。 如果单纯按照词频算关键词，你会发现几乎所有的文章都是stopwords的词频最高。换句话说，像这种”万金油”，是没有区分度的词语，不能很好的起到将文章分类的作用。 此外，抛开停用词，如果该文档中的几个词出现的频率一样，也不意味着，作为关键词，它们的重要性是一致的。比如这篇文档中，“TF-IDF”、“意义”、“文档”这三个词的词频出现的次数一样多，但因为“意义”是很常见的词，相对而言，“TF-IDF”、“文档”不那么常见。即使它们的词频一样，我们也有理由认为，“TF-IDF”和“文档”的重要性大于“意义”，也就是使，在关键词排序上，“TF-IDF”和“文档”也应该排在“意义”的前面。 所以，我们需要一个重要性调整系数，衡量一个词是不是常见词。如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。这时就需要祭出逆文档频率(IDF)来解决词语权重的问题。 1.2 IDF用统计学语言表达，就是在词频的基础上，要对每个词分配一个”重要性”权重。最常见的词（”的”、”是”、”在”）给予最小的权重，较常见的词（”中国”）给予较小的权重，较少见的词（”蜜蜂”、”养殖”）给予较大的权重。这个权重叫做”逆文档频率”（Inverse Document Frequency，缩写为IDF），它的大小与一个词的常见程度成反比。 知道了”词频”（TF）和”逆文档频率”（IDF）以后，将这两个值相乘，就得到了一个词的TF-IDF值。某个词对文章的重要性越高，它的TF-IDF值就越大。所以，排在最前面的几个词，就是这篇文章的关键词。 1.3 公式化表达对于在某一特定文件里的词语$t_i$来说，它的重要性可表示为： TF_{i,j}=\frac{n_{i,j}}{\sum_kn_{k,j}}以上式子中$n_{i,j}$是该词在文件$d_{j}$中的出现次数而分母则是在文件$d_j$中所有字词的出现次数之和。 逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到： IDF_i=log\frac{|D|}{|{j:t_i\in d_j}|}其中 $|D|$：语料库中的文件总数 $|{j:t_i\in d_j}|$：包含词语$t_i$的文件数目（即$n_{i,j}≠0的文件数目$）如果该词语不在语料库中，就会导致分母为零，因此一般情况下使用$1+|{j:t_i\in d_j}|$ 然后 TF-IDF = TF_{i,j}\times IDF _i某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的tf-idf。因此，tf-idf倾向于过滤掉常见的词语，保留重要的词语。 1.4 应用我们通过Google搜索结果数为例，将含有中文“的”结果数15.8亿作为整个语料库大小，计算一些关键词和停用词的TF-IDF值。为了计算简便，假设全文分词后一共500词，则结果如下： TF-IDF的优点是计算简单，利于理解，性价比极高。但是它也有缺陷，首先单纯依据文章中的TF来衡量重要性，忽略了位置信息。如段首，句首一般权重更高；其次，有的文章可能关键词只出现1-2次，但可能通篇都是围绕其进行阐述和解释，所以单纯靠TF仍然不能解决所有的情况。 二、余弦相似度余弦相似性通过测量两个向量的夹角的余弦值来度量它们之间的相似性。0度角的余弦值是1，而其他任何角度的余弦值都不大于1；并且其最小值是-1。从而两个向量之间的角度的余弦值确定两个向量是否大致指向相同的方向。两个向量有相同的指向时，余弦相似度的值为1；两个向量夹角为90°时，余弦相似度的值为0；两个向量指向完全相反的方向时，余弦相似度的值为-1。这结果是与向量的长度无关的，仅仅与向量的指向方向相关。余弦相似度通常用于正空间，因此给出的值为0到1之间。 注意这上下界对任何维度的向量空间中都适用，而且余弦相似性最常用于高维正空间。例如在信息检索中，每个词项被赋予不同的维度，而一个文档由一个向量表示，其各个维度上的值对应于该词项在文档中出现的频率。余弦相似度因此可以给出两篇文档在其主题方面的相似度。 2.1 定义两个向量间的余弦值可以通过使用欧几里得点积公式求出： a·b=|a|·|b|\ cos \theta给定两个属性向量$A$和$B$，其余相似性$\theta$由点积和向量长度给出，如下所示： similarity = cos(\theta)=\frac{A·B}{|A||B|}=\frac{\sum_{i=1}^nA_i\times B_i}{\sqrt{\sum_{i=1}^n(A_i)^2}\times \sqrt{\sum_{i=1}^n(B_i)^2}}这里的$A_i$和$B_i$分别代表向量$A$和$B$的各分量。 给出的相似性范围从-1到1：-1意味着两个向量指向的方向正好截然相反，1表示它们的指向是完全相同的，0通常表示它们之间是独立的，而在这之间的值则表示中间的相似性或相异性。 对于文本匹配，属性向量A 和B 通常是文档中的词频向量。余弦相似性，可以被看作是在比较过程中把文件长度正规化的方法。 在信息检索的情况下，由于一个词的频率（TF-IDF权）不能为负数，所以这两个文档的余弦相似性范围从0到1。并且，两个词的频率向量之间的角度不能大于90°。 由此，我们就得到了”找出相似文章”的一种算法： 1）使用TF-IDF算法，找出两篇文章的关键词； 2）每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）； 3）生成两篇文章各自的词频向量； 4）计算两个向量的余弦相似度，值越大就表示越相似。 “余弦相似度”是一种非常有用的算法，只要是计算两个向量的相似程度，都可以采用它。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>TF-IDF</tag>
        <tag>余弦相似度</tag>
        <tag>文档检索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（22）：主成分分析]]></title>
    <url>%2F2017%2F07%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8822%EF%BC%89%EF%BC%9A%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。 一、数据的向量表示及降维问题一般情况下，在数据挖掘和机器学习中，数据被表示为向量。例如某个淘宝店2012年全年的流量及交易情况可以看成一组记录的集合，其中每一天的数据是一条记录，格式如下： (日期, 浏览量, 访客数, 下单数, 成交数, 成交金额) 其中“日期”是一个记录标志而非度量值，而数据挖掘关心的大多是度量值，因此如果我们忽略日期这个字段后，我们得到一组记录，每条记录可以被表示为一个五维向量，其中一条看起来大约是这个样子： (500,240,25,13,2312.15)^𝖳注意这里用了转置，因为习惯上使用列向量表示一条记录（后面会看到原因），本文后面也会遵循这个准则。不过为了方便有时会省略转置符号，但我们说到向量默认都是指列向量。 我们当然可以对这一组五维向量进行分析和挖掘，不过我们知道，很多机器学习算法的复杂度和数据的维数有着密切关系，甚至与维数呈指数级关联。当然，这里区区五维的数据，也许还无所谓，但是实际机器学习中处理成千上万甚至几十万维的情况也并不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行降维。 降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。 举个例子，假如某学籍数据有两列M和F，其中M列的取值是如何此学生为男性取值1，为女性取值0；而F列是学生为女性取值1，男性取值0。此时如果我们统计全部学籍数据，会发现对于任何一条记录来说，当M为1时F必定为0，反之当M为0时F必定为1。在这种情况下，我们将M或F去掉实际上没有任何信息的损失，因为只要保留一列就可以完全还原另一列。 当然上面是一个极端的情况，在现实中也许不会出现，不过类似的情况还是很常见的。例如上面淘宝店铺的数据，从经验我们可以知道，“浏览量”和“访客数”往往具有较强的相关关系，而“下单数”和“成交数”也具有较强的相关关系。这里我们非正式的使用“相关关系”这个词，可以直观理解为“当某一天这个店铺的浏览量较高（或较低）时，我们应该很大程度上认为这天的访客数也较高（或较低）”。后面的章节中我们会给出相关性的严格数学定义。 这种情况表明，如果我们删除浏览量或访客数其中一个指标，我们应该期待并不会丢失太多信息。因此我们可以删除一个，以降低机器学习算法的复杂度。 上面给出的是降维的朴素思想描述，可以有助于直观理解降维的动机和可行性，但并不具有操作指导意义。例如，我们到底删除哪一列损失的信息才最小？亦或根本不是单纯删除几列，而是通过某些变换将原始数据变为更少的列但又使得丢失的信息最小？到底如何度量丢失信息的多少？如何根据原始数据决定具体的降维操作步骤？ 要回答上面的问题，就要对降维问题进行数学化和形式化的讨论。而PCA是一种具有严格数学基础并且已被广泛采用的降维方法。下面我不会直接描述PCA，而是通过逐步分析问题，让我们一起重新“发明”一遍PCA。 二、向量的表示及基变换既然我们面对的数据被抽象为一组向量，那么下面有必要研究一些向量的数学性质。而这些数学性质将成为后续导出PCA的理论基础。 2.1 内积与投影下面先来看一个高中就学过的向量运算：内积。两个维数相同的向量的内积被定义为： (a_1,a_2,⋯,a_n)^𝖳⋅(b_1,b_2,⋯,b_n)𝖳=a_1b_1+a_2b_2+⋯+a_nb_n内积运算将两个向量映射为一个实数。其计算方式非常容易理解，但是其意义并不明显。下面我们分析内积的几何意义。 假设A和B是两个n维向量，我们知道n维向量可以等价表示为n维空间中的一条从原点发射的有向线段，为了简单起见我们假设A和B均为二维向量，则$A=(x_1,y_1)$，$B=(x_2,y_2)$。则在二维平面上A和B可以用两条发自原点的有向线段表示，见下图：现在我们从A点向B所在直线引一条垂线。我们知道垂线与B的交点叫做A在B上的投影，再设A与B的夹角是a，则投影的矢量长度为$|A|cos(a)|A|cos(a)$，其中$|A|=\sqrt{x^2_1+y^2_1}$是向量A的模，也就是A线段的标量长度。 注意这里我们专门区分了矢量长度和标量长度，标量长度总是大于等于0，值就是线段的长度；而矢量长度可能为负，其绝对值是线段长度，而符号取决于其方向与标准方向相同或相反。 到这里还是看不出内积和这东西有什么关系，不过如果我们将内积表示为另一种我们熟悉的形式： A⋅B=|A||B|cos(a)A与B的内积等于A到B的投影长度乘以B的模。再进一步，如果我们假设B的模为1，即让$|B|=1$，那么就变成了： A⋅B=|A|cos(a)也就是说，设向量B的模为1，则A与B的内积值等于A向B所在直线投影的矢量长度！这就是内积的一种几何解释，也是我们得到的第一个重要结论。 2.2 基下面我们继续在二维空间内讨论向量。上文说过，一个二维向量可以对应二维笛卡尔直角坐标系中从原点出发的一个有向线段。例如下面这个向量： 在代数表示方面，我们经常用线段终点的点坐标表示向量，例如上面的向量可以表示为(3,2)，这是我们再熟悉不过的向量表示。 不过我们常常忽略，只有一个(3,2)本身是不能够精确表示一个向量的。我们仔细看一下，这里的3实际表示的是向量在x轴上的投影值是3，在y轴上的投影值是2。也就是说我们其实隐式引入了一个定义：以x轴和y轴上正方向长度为1的向量为标准。那么一个向量(3,2)实际是说在x轴投影为3而y轴的投影为2。注意投影是一个矢量，所以可以为负。 更正式的说，向量(x,y)实际上表示线性组合： x(1,0)^T+y(0,1)^T不难证明所有二维向量都可以表示为这样的线性组合。此处（1，0）和（0，1）叫做二维空间中的一组基。 所以，要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了。只不过我们经常省略第一步，而默认以(1,0)和(0,1)为基。 我们之所以默认选择$(1,0)$和$(0,1)$为基，当然是比较方便，因为它们分别是x和y轴正方向上的单位向量，因此就使得二维平面上点坐标和向量一一对应，非常方便。但实际上任何两个线性无关的二维向量都可以成为一组基，所谓线性无关在二维平面内可以直观认为是两个不在一条直线上的向量。 例如，$(1,1)$和$(-1,1)$也可以成为一组基。一般来说，我们希望基的模是1，因为从内积的意义可以看到，如果基的模是1，那么就可以方便的用向量点乘基而直接获得其在新基上的坐标了！实际上，对应任何一个向量我们总可以找到其同方向上模为1的向量，只要让两个分量分别除以模就好了。例如，上面的基可以变为$(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$和$-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}$。 现在，我们想获得$(3,2)$在新基上的坐标，即在两个方向上的投影矢量值，那么根据内积的几何意义，我们只要分别计算(3,2)和两个基的内积，不难得到新的坐标为$(\frac{5}{\sqrt{2}},−\frac{1}{\sqrt{2}}$。下图给出了新的基以及(3,2)在新基上坐标值的示意图： 另外这里要注意的是，我们列举的例子中基是正交的（即内积为0，或直观说相互垂直），但可以成为一组基的唯一要求就是线性无关，非正交的基也是可以的。不过因为正交基有较好的性质，所以一般使用的基都是正交的。 2.3 基变换的矩阵表示下面我们找一种简便的方式来表示基变换。还是拿上面的例子，想一下，将$(3,2)$变换为新基上的坐标，就是用$(3,2)$与第一个基做内积运算，作为第一个新的坐标分量，然后用(3,2)与第二个基做内积运算，作为第二个新坐标的分量。实际上，我们可以用矩阵相乘的形式简洁的表示这个变换： \left[\begin{matrix} \frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\\ -\frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\\ \end{matrix}\right]\left[\begin{array}{c} 3\\ 2\\ \end{array}\right]=\left[\begin{array}{c} \frac{5}{\sqrt{2}}\\ -\frac{1}{\sqrt{2}}\\ \end{array}\right]其中矩阵的两行分别为两个基，乘以原向量，其结果刚好为新基的坐标。可以稍微推广一下，如果我们有m个二维向量，只要将二维向量按列排成一个两行m列矩阵，然后用“基矩阵”乘以这个矩阵，就得到了所有这些向量在新基下的值。例如(1,1)，(2,2)，(3,3)，想变换到刚才那组基上，则可以这样表示： \left[\begin{matrix} \frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\\ -\frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\\ \end{matrix}\right]\left[\begin{matrix} 1& 2& 3\\ 1& 2& 3\\ \end{matrix}\right]=\left[\begin{matrix} \frac{2}{\sqrt{2}}& \frac{4}{\sqrt{2}}& \frac{6}{\sqrt{2}}\\ 0& 0& 0\\ \end{matrix}\right]于是一组向量的基变换被干净的表示为矩阵的相乘。 一般的，如果我们有M个N维向量，想将其变换为由R个N维向量表示的新空间中，那么首先将R个基按行组成矩阵A，然后将向量按列组成矩阵B，那么两矩阵的乘积AB就是变换结果，其中AB的第m列为A中第m列变换后的结果。 数学表示为： \left[\begin{array}{c} p_1\\ p_2\\ ···\\ p_r\\ \end{array}\right]\left[\begin{matrix} a_1& a_2& ···& a_M\\ \end{matrix}\right]=\left[\begin{matrix} p_1a_1& p_1a_2& ···& p_1a_M\\ p_2a_1& p_2a_2& ···& p_2a_M\\ ···& ···& ···& ···\\ p_ra_1& p_ra_2& ···& p_ra_M\\ \end{matrix}\right]其中$p_i$是一个行向量，表示第$i$个基，$a_j$是一个列向量，表示第$j$个原始数据记录。 特别要注意的是，这里R可以小于N，而R决定了变换后数据的维数。也就是说，我们可以将一N维数据变换到更低维度的空间中去，变换后的维度取决于基的数量。因此这种矩阵相乘的表示也可以表示降维变换。 最后，上述分析同时给矩阵相乘找到了一种物理解释：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，一个矩阵可以表示一种线性变换。很多同学在学线性代数时对矩阵相乘的方法感到奇怪，但是如果明白了矩阵相乘的物理意义，其合理性就一目了然了。 三、协方差矩阵及优化目标上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示，而且如果基的数量少于向量本身的维数，则可以达到降维的效果。但是我们还没有回答一个最最关键的问题：如何选择基才是最优的。或者说，如果我们有一组N维向量，现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息？ 要完全数学化这个问题非常繁杂，这里我们用一种非形式化的直观方法来看这个问题。 为了避免过于抽象的讨论，我们仍以一个具体的例子展开。假设我们的数据由五条记录组成，将它们表示成矩阵形式： \left[\begin{matrix} 1& 1& 2& 4& 2\\ 1& 3& 3& 4& 4\\ \end{matrix}\right]其中每一列为一条数据记录，而一行为一个字段。为了后续处理方便，我们首先将每个字段内所有值都减去字段均值，其结果是将每个字段都变为均值为0（这样做的道理和好处后面会看到）。 我们看上面的数据，第一个字段均值为2，第二个字段均值为3，所以变换后： \left[\begin{matrix} -1& -1& 0& 2& 0\\ -2& 0& 0& 1& 1\\ \end{matrix}\right]我们可以看下五条数据在平面直角坐标系内的样子： 现在问题来了：如果我们必须使用一维来表示这些数据，又希望尽量保留原始的信息，你要如何选择？ 通过上一节对基变换的讨论我们知道，这个问题实际上是要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。 那么如何选择这个方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散。 以上图为例，可以看出如果向x轴投影，那么最左边的两个点会重叠在一起，中间的两个点也会重叠在一起，于是本身四个各不相同的二维点投影后只剩下两个不同的值了，这是一种严重的信息丢失，同理，如果向y轴投影最上面的两个点和分布在x轴上的两个点也会重叠。所以看来x和y轴都不是最好的投影选择。我们直观目测，如果向通过第一象限和第三象限的斜线投影，则五个点在投影后还是可以区分的。 下面，我们用数学方法表述这个问题。 3.1 方差上文说到，我们希望投影后投影值尽可能分散，而这种分散程度，可以用数学上的方差来表述。此处，一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值，即： Var(a)=\frac{1}{m}\sum^m_{i=1}(a_i-\mu)^2由于上面我们已经将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示： Var(a)=\frac{1}{m}\sum^m_{i=1}a_i^2于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。 3.2 协方差对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。 如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。 数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则： Cov(a,b) = \frac{1}{m}\sum_{i=1}^ma_ib_i可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m。 当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。 至此，我们得到了降维问题的优化目标：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。 3.3 协方差矩阵上面我们导出了优化目标，但是这个目标似乎不能直接作为操作指南（或者说算法），因为它只说要什么，但根本没有说怎么做。所以我们要继续在数学上研究计算方案。 我们看到，最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们来了灵感： 假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X： X=\left[\begin{matrix} a_1& a_1& ···& a_m\\ b_1& b_2& ···& b_m\\ \end{matrix}\right]然后我们用X乘以X的转置，并乘上系数$1/m$： \frac{1}{m}XX^T = \left[\begin{matrix} \frac{1}{m}\sum_{i=1}^ma_i^2& \frac{1}{m}\sum_{i=1}^ma_ib_i\\ \frac{1}{m}\sum_{i=1}^ma_ib_i& \frac{1}{m}\sum_{i=1}^mb_i^2\\ \end{matrix}\right]奇迹出现了！这个对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。 根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况：设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设￥$C=\frac{1}{m}XX^𝖳$，则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差。 3.4 协方差矩阵对角化根据上述推导，我们发现要达到优化条件，等价于将协方差对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系： 设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设$Y=PX$，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系： D = \frac{1}{m}YY^T\\ =\frac{1}{m}(PX)(PX)^T\\ =\frac{1}{m}PXX^TP\\=PCP^T现在事情很明白了，我们要找的$P$不是别的，而是能让原始协方差矩阵对角化的$P$。换句话说，优化目标变成了寻找一个矩阵$P$，满足$PCP^T$是一个对角矩阵，并且对角元素按从小到大依次排列，那么$P$的前$K$行就是要寻找的基，用$P$的前$K$行组成的矩阵乘以$X$就使得$X$从$N$维降到了$K$维并满足上述优化条件。 至此，我们离“发明”PCA还有仅一步之遥！ 现在所有焦点都聚焦在了协方差矩阵对角化问题上，有时，我们真应该感谢数学家的先行，因为矩阵对角化在线性代数领域已经属于被玩烂了的东西，所以这在数学上根本不是问题。 由上文知道，协方差矩阵C是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质： 1）实对称矩阵不同特征值对应的特征向量必然正交。 2）设特征向量λλ重数为r，则必然存在r个线性无关的特征向量对应于λλ，因此可以将这r个特征向量单位正交化。 由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为$e_1,e_2,⋯,e_n$，我们将其按列组成矩阵： E = (e_1\ e_2 \ ··· \ e_n)则对协方差矩阵$C$有如下结论： E^TCE = \varLambda\ =\left[\begin{matrix} \lambda_1& & & \\ & \lambda_2& & \\ & & ···& \\ & & & \lambda_n\\ \end{matrix}\right]其中Λ为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。以上结论不再给出严格的数学证明，对证明感兴趣的朋友可以参考线性代数书籍关于“实对称矩阵对角化”的内容。 到这里，我们发现我们已经找到了需要的矩阵P：P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照ΛΛ中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。 至此我们完成了整个PCA的数学原理讨论。在下面的一节，我们将给出PCA的一个实例。 四、算法及实例为了巩固上面的理论，我们在这一节给出一个具体的PCA实例。 4.1 PCA算法总结一下PCA的算法步骤：设有m条n维数据。 1）将原始数据按列组成n行m列矩阵X 2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值 3）求出协方差矩阵$C=\frac{1}{m}XX^T$ 4）求出协方差矩阵的特征值及对应的特征向量 5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前K行组成矩阵P 6）$Y=PX$即为降维到K维后的数据 4.2 实例这里以上文提到的 \left[\begin{matrix} -1& -1& 0& 2& 0\\ -2& 0& 0& 1& 1\\ \end{matrix}\right]为例，我们用PCA方法将这组二维数据降到一维。 因为这个矩阵的每行已经是零均值，这里我们直接求协方差矩阵： C=\frac{1}{5} \left[\begin{matrix} -1& -1& 0& 2& 0\\ -2& 0& 0& 1& 1\\ \end{matrix}\right] \left[\begin{matrix} -1& -2\\ -1& 0\\ 0& 0\\ 2& 1\\ 0& 1\\ \end{matrix}\right]=\left[\begin{matrix} \frac{6}{5}& \frac{4}{5} \\ \frac{4}{5}& \frac{6}{5} \\ \end{matrix}\right]然后求其特征值和特征向量，具体求解方法不再详述。求解后特征值为： \lambda_1=2 , \lambda_2 =\frac{2}{5}其对应的特征向量分别是： c_1\left[\begin{matrix} -2\\ 0\\ \end{matrix}\right],c_2\left[\begin{matrix} -1\\ 1\\ \end{matrix}\right]其中对应的特征向量分别是一个通解，$c_1$和$c_2$可取任意实数。那么标准化后的特征向量为： \left[\begin{matrix} \frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{2}}\\ \end{matrix}\right],\left[\begin{matrix} -\frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{2}}\\ \end{matrix}\right]因此我们的矩阵P是： P=\left[\begin{matrix} \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\ \end{matrix}\right]可以验证协方差矩阵C的对角化： PCP^T=\left[\begin{matrix} \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\ \end{matrix}\right]\left[\begin{matrix} \frac{6}{5}& \frac{4}{5} \\ \frac{4}{5}& \frac{6}{5} \\ \end{matrix}\right]\left[\begin{matrix} \frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\ \end{matrix}\right]=\left[\begin{matrix} 2&0\\ 0&\frac{2}{5}\\ \end{matrix}\right]最后我们用P的第一行乘以数据矩阵，就得到了降维后的表示： Y =\left[\begin{matrix} \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\ \end{matrix}\right] \left[\begin{matrix} -1&-1&0&2&0\\ -2&0&0&1&1\\ \end{matrix}\right]=\left[\begin{matrix} -\frac{3}{\sqrt{2}}&\frac{1}{\sqrt{2}}&0&\frac{3}{\sqrt{2}}&-\frac{1}{2}\\ \end{matrix}\right]降维投影结果如下图： 五、理论意义PCA将n个特征降维到k个，可以用来进行数据压缩，如果100维的向量最后可以用10维来表示，那么压缩率为90%。同样图像处理领域的KL变换使用PCA做图像压缩。但PCA要保证降维后，还要保证数据的特性损失最小。再看回顾一下PCA的效果。经过PCA处理后，二维数据投影到一维上可以有以下几种情况： 我们认为左图好，一方面是投影后方差最大，一方面是点到直线的距离平方和最小，而且直线过样本点的中心点。为什么右边的投影效果比较差？直觉是因为坐标轴之间相关，以至于去掉一个坐标轴，就会使得坐标点无法被单独一个坐标轴确定。 PCA得到的k个坐标轴实际上是k个特征向量，由于协方差矩阵对称，因此k个特征向量正交。 得到的新的样例矩阵$Y=PX$就是m个样例到k个特征向量的投影，也是这k个特征向量的线性组合。P中e之间是正交的。从矩阵乘法中可以看出，PCA所做的变换是将原始样本点（n维），投影到k个正交的坐标系中去，丢弃其他维度的信息。举个例子，假设宇宙是n维的（霍金说是11维的），我们得到银河系中每个星星的坐标（相对于银河系中心的n维向量），然而我们想用二维坐标去逼近这些样本点，假设算出来的协方差矩阵的特征向量分别是图中的水平和竖直方向，那么我们建议以银河系中心为原点的x和y坐标轴，所有的星星都投影到x和y上，得到下面的图片。然而我们丢弃了每个星星离我们的远近距离等信息。 六、进一步讨论根据上面对PCA的数学原理的解释，我们可以了解到一些PCA的能力和限制。PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。 因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。 PCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。 但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。 上图中黑色点表示采样数据，排列成转盘的形状。容易想象，该数据的主元是$(P_1,P_2)$或是旋转角$\theta$。在这里PCA找出的主元将是$(P_1,P_2 )$。但是这显然不是最优和最简化的主元。$(P_1,P_2 )$之间存在着非线性的关系。根据先验的知识可知旋转角$\theta$是最优的主元（类比极坐标）。则在这种情况下，PCA就会失效。但是，如果加入先验的知识，对数据进行某种划归，就可以将数据转化为以$\theta$为线性的空间中。这类根据先验知识对数据预先进行非线性转换的方法就成为kernel-PCA，它扩展了PCA能够处理的问题的范围，又可以结合一些先验约束，是比较流行的方法。 有时数据的分布并不是满足高斯分布。如图表 5所示，在非高斯分布的情况下，PCA方法得出的主元可能并不是最优的。在寻找主元时不能将方差作为衡量重要性的标准。要根据数据的分布情况选择合适的描述完全分布的变量，然后根据概率分布式 P(y_1,y_2)=P(y_1)P(y_2 )来计算两个向量上数据分布的相关性。等价的，保持主元间的正交假设，寻找的主元同样要使$P(y_1,y_2)=0$。这一类方法被称为独立主元分解(ICA)。数据的分布并不满足高斯分布，呈明显的十字星状。这种情况下，方差最大的方向并不是最优主元方向。另外PCA还可以用于预测矩阵中缺失的元素。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>降维</tag>
        <tag>非监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（21）：SVD]]></title>
    <url>%2F2017%2F07%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8821%EF%BC%89%EF%BC%9ASVD%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%E5%8F%8A%E5%85%B6%E6%84%8F%E4%B9%89%2F</url>
    <content type="text"><![CDATA[一、简介SVD实际上是数学专业内容，但它现在已经深入到不同的领域中。SVD的过程不是很好理解，因为它不够直观，但它对矩阵分解的效果却非常好。比如，Netflix（一个提供在线电影租赁的公司）曾经就悬赏100万美金，如果谁能提高他的电影推荐系统评分预测率10%的话。令人惊讶的是，这个目标充满了挑战，来自世界各地的团队运用了各种不同的技术。最终的获胜队伍“BellKor’s Pragmatic Chaos”采用的核心算法就是基于SVD。 SVD提供了一种非常便捷的矩阵分解方式，能够发现数据中十分有意思的潜在模式，在这篇文章中，我们将会提供对SVD集合上的理解和一些简单的应用实例。 1.1 几何意义奇异值分解就是把一个线性变换分解成两个线性变换，一个线性变化代表旋转，另一个代表拉伸。 线性代数中最让人印象深刻的一点是，要将矩阵和空间中的线性变化视为同样的事物。比如对角矩阵$M$作用在任何一个向量上 \left[\begin{matrix} 3& 0\\ 0& 1\\ \end{matrix}\right]\left[\begin{array}{c} x\\ y\\ \end{array}\right]=\left[\begin{array}{c} 3x\\ y\\ \end{array}\right]其几何意义为在水平$x$方向上拉伸3倍，$y$方向保持不变的线性变换。换言之对角矩阵起到作用是将水平垂直网格作水平拉伸（或者反射后水平拉伸）的线性变换。如果$M$不是对角矩阵。而是一个对称矩阵： M=\left[\begin{matrix} 2& 1\\ 1& 2\\ \end{matrix}\right]那么我们也总能找到一组网格线，使得矩阵作用在该网格上仅仅表现为（反射）拉伸变换，而没有发生旋转变换。这个矩阵产生的变换效果如下图所示 考虑一下更一般的非对称矩阵 M=\left[\begin{matrix} 1& 1\\ 0& 1\\ \end{matrix}\right]很遗憾，此时我们再也找不到一组网格，使得矩阵作用在该网格之后只有拉伸变换（找不到背后的数学原因就是对一般非对称矩阵无法保证在实数域上可对角化）。我们退而求其次，找到一组网格，使得矩阵作用在该网格之后允许有拉伸变换和旋转变换，但要保证变换后的网格依旧互相垂直。这是可以做到的 下面我们就可以自然过渡到奇异值分解的引入。奇异值分解的几何含义为：对于任何的一个矩阵，我们要找到一组两两正交单位向量序列，使得矩阵作用在此向量序列上后得到新的向量序列保持两两正交。下面我们要说明的是，奇异值的几何含义为：这组变换后的新的向量序列的长度。当矩阵$M$作用在正交单位向量$v_1$和$v_2$上之后，得到$Mv_1$和$Mv_2$也是正交的。令$u_1$和$u_2$分别是$Mv_1$和$Mv_2$方向上的单位向量，即$Mv_1 = \sigma_1u_1$，$Mv_2 = \sigma _2 u_2$，写在一起就是$M[v_1,v_2] = [\sigma_1 u_1 \ \sigma_2 u_2]$，整理得到 M=M\left[v_1\ v_2\right]\left[\begin{array}{c} v_{1}^{T}\\ v_{2}^{T}\\ \end{array}\right]=\left[\sigma_1u_1 \ \sigma_2u_2\right]\left[\begin{array}{c} v_{1}^{T}\\ v_{2}^{T}\\ \end{array}\right]=\left[u_1\ u_2\right]\left[\begin{matrix} \sigma_1& 0\\ 0& \sigma_2\\ \end{matrix}\right]\left[\begin{array}{c} v_{1}^{T}\\ v_{2}^{T}\\ \end{array}\right]这样就得到矩阵$M$的奇异值分解。奇异值$\sigma_1$和$\sigma_2$分别是$Mv_1$和$Mv_2$的长度。很容易可以把结论推广到一般$n$维的情况 二、奇异值分解2.1 特征值分解如果方阵对某个向量只产生伸缩，而不产生旋转效果，那么这个向量就称为矩阵的特征向量，伸缩的比例就是对应的特征值。 Ax=\lambda x 所以这其实是在平面上对一个轴进行的拉伸变换（如蓝色的箭头所示），在图中，蓝色的箭头是一个最主要的变化（变化方向可能不止一个），如果我们想要描述好一个变换，那我们描述好这个变换主要的变化方向就好了。反过来看看之前特征值分解的式子，分解得到的$\Sigma$矩阵是一个对角阵，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化的方向（从主要的变化到次要的变化排列）。 当矩阵是高维的情况下，那么这个矩阵就是高维空间下的一个线性变换，这个线性变化可能没法通过图片来表示，但是可以想象，这个变换也同样有很多的变换方向，我们通过特征值分解得到的前N个特征向量，那么就对应了这个矩阵最主要的N个变化方向。我们利用这前N个变化方向，就可以近似这个矩阵变换。也就是之前受的：提取这个矩阵最重要的特征。总结一下，特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间敢很多的事情。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。 学线性代数的时候，我们应该都学过这样一个定理： 若A为n阶实对称阵（方阵），则存在由特征值组成的对角阵$\varLambda$和特征向量组成的正交阵$Q$，使得： A=Q\varLambda Q^T 这就是我们所说的特征值分解（Eigenvalue decomposition: EVD）（$R^n → R^n$），而奇异值分解其实可以看做是特征值分解在任意矩阵$m\times n$上的推广形式($R^n →R^m$)。只有对方阵才有特征值的概念，所以对于任意的矩阵，我们引入了奇异值。 2.2 奇异值分解上面的特征值分解是一个提取矩阵特征很不错的方法，当它只是对方阵而言的，在现实的世界中，我们看到的大部分都不是方阵，比如说有N个学生，每个学生有M科成绩，这样形成的一个$N\times M$的矩阵就不可能是方阵！那么现在就来分析：对于任意的$m\times n$的矩阵，能否找到一组正交基使得经过它变换后还是正交基？答案是肯定的，它就是SVD分解的精髓所在。 下面我们从特征值分解出发，导出奇异值分解。 首先我们注意到$A^T A$为$n$阶对称矩阵，我们可以对它做特征值分解。 A^T A = VDV^T这个时候我们可以得到一组正交基，$\{v_1,v_2,···v_n\}$： (A^T A)v_i=\lambda _i v_i (Av_i,Av_j) = (Av_i)^T(Av_j) = v_i^TA^TAv_j = v_i^T(\lambda_jv_j) = \lambda_jv_i^Tv_j = 0由$r(A^T A)=r(A)=r$，这个时候我们得到了一组正交基，$\{Av_1,Av_2,···,Av_r\}$，先将其标准化，令： u_i = \frac{Av_i}{|Av_i|}=\frac{1}{\sqrt{\lambda _i}}Av_i\Rightarrow Av_i = \sqrt {\lambda _i} u_i = \delta_iu_i其中 |Av_i|^2=(Av_i,Av_i)=\lambda_iv_i^Tv_i=\lambda_i \Rightarrow |Av_i| = \sqrt {\lambda _i}=\delta _i(奇异值)将向量组$\{u_1,u_2,···,u_r\}$扩充为$R^m$中的标准正交基$\{u_1,u_2,···,u_r,···,u_m$，则： AV =A(v_1v_2···v_n) =(Av_1 \ Av_2 \ ···\ Av_r\ 0 ··· \ 0)\\=(\delta _1u_1 \ \delta_2u_2 ··· \delta _r u_r \ 0 ··· \ 0=U\Sigma\\\Rightarrow A =U\Sigma V^T我们可以从下图中直观的感受奇异值分解的矩阵相乘。任意的矩阵$A$是可以分解成三个矩阵。其中$V$表示了原始域的标准正交基，$U$表示经过A变化后的$co-domain$的标准正交基，$\Sigma$表示了$V$中的向量与$U$中相对应向量之间的关系。 在很多情况下，前10%甚至1%的奇异值的和就占了全部分奇异值之和的99%以上了。也就是说，我们也可以用前$r$大的奇异值来近似描述矩阵，这里定义一下部分奇异值分解： A_{m\times n}≈ U_{m\times r} \Sigma_{r\times r}V^T_{r\times n}$r$是一个远小于$m、n$的数，这样矩阵的乘法看起来像是下面的样子：右边的三个矩阵相乘的结果将会是一个接近于A的矩阵，在这儿，$r$越接近于n，则相乘的结果越接近于A。而这三个矩阵的面积之和（早存储观点来说，矩阵面积越小，存储量就越小）要远远小于原始的矩阵A，我们如果想要压缩空间来表示原矩阵A，我们存下这里的三个矩阵:$U、\Sigma、V$就好了。 三、应用实例3.1 推荐系统我们现在有一批高尔夫球手对九个不同hole的所需挥杆次数数据，我们希望基于这些数据建立模型，来预测选手对于某个给定hole的挥杆次数。（这个例子来自于： Singular Value Decomposition (SVD) Tutorial，强烈建议大家都去看看） 最简单的一个思路，我们队每个hole设立一个难度指标HoleDifficulty，对每位选手的能力也设立一个评价指标PlayAbility，实际的得分取决于这俩者的乘积： PredictedScore = HoleDifficulty · PlayerAbility我们可以简单地把每位选手的Ability都设为1，那么： 接着我们将HoleDifficulty 和 PlayAbility这两个向量标准化，可以得到如下的关系： 好熟悉，这不就是传说中的SVD吗，这样就出来了。 这里面蕴含了一个非常有趣的思想，也是SVD这么有用的核心： 最开始高尔夫球员和Holes之间是没有直接联系的，我们通过feature把它们联系在一起：不同的Hole进洞难度是不一样的，每个球手对进度难度的把控也是不一样的，那么我们就可以通过进洞难度这个feature将它们联系在一起，将它们乘起来就得到了我们想要的挥杆次数。 这个思想很重要，对于我们理解LSI和SVD再推荐系统中的应用相当重要。 SVD分解其实就是利用隐藏的Feature建立起矩阵行与列之间的联系。 大家可能注意到，上面那个矩阵秩为1，所以我们很容易就能将其分解，但是在实际问题中我们就得依靠SVD分解，这个时候的隐藏特征往往也不止一个了。 我们将上面的数据稍作修改：进行奇异值分解，可以得到：隐藏特征的重要性是与其对应的奇异值大小成正比的，也就是奇异值越大，其所对应的隐藏特征也越重要。 我们将这个思想推广一下 在推荐系统中，用户和物品之间没有直接联系。但是我们可以通过feature把它们联系在一起。对于电影来说，这样的特征可以是：喜剧还是悲剧，是动作片还是爱情片。用户和这样的feature之间是有关系的，比如某个用户喜欢看爱情片，另外一个用户喜欢看动作片；物品和feature之间也是有关系的，比如某个电影是喜剧，某个电影是悲剧。那么通过和feature之间的联系，我们就找到了用户和物品之间的关系。 3.2 数据压缩矩阵的奇异值是一个数学意义上的概念，一般是由奇异值分解（Singular Value Decomposition，简称SVD分解）得到。如果要问奇异值表示什么物理意义，那么就必须考虑在不同的实际工程应用中奇异值所对应的含义。下面先尽量避开严格的数学符号推导，直观的从一张图片出发，让我们来看看奇异值代表什么意义。 这是女神上野树里（Ueno Juri）的一张照片，像素为$450\times 333$我们都知道，图片实际上对应着一个矩阵，矩阵的大小就是像素大小，比如这张图对应的矩阵阶数就是450*333，矩阵上每个元素的数值对应着像素值。我们记这个像素矩阵为 AA 。现在我们对矩阵 AA 进行奇异值分解。直观上，奇异值分解将矩阵分解成若干个秩一矩阵之和，用公式表示就是： A = \sigma_1u_1v_1^T+\sigma_2u_2v_2^T+···+\sigma_ru_rv_r^T其中等式右边每一项前的系数$\sigma$就是奇异值，$u$和$v$分别表示列向量，秩一矩阵的意思是秩为1的矩阵。注意到每一项$uv^T$都是秩为1的矩阵。我们假定奇异值满足 \sigma_1≥\sigma_2≥···≥\sigma_r＞0（奇异值大于0是个重要的性质，但这里先别在意），如果不满足的话重新排列顺序即可，这无非是编号顺序的问题。 既然奇异值有从大到小排列的顺序，我们自然要问，如果只保留大的奇异值，舍去较小的奇异值，这样(1)式里的等式自然不再成立，那会得到怎样的矩阵——也就是图像？ 令 $A_1=σ_1u_1v^T_1$ ，这只保留(1)中等式右边第一项，然后作图结果就是完全看不清是啥···我们试着多增加几项进来： A_5=σ_1μ_1v^T_1+σ_2μ_2v^T_2+...+σ_5μ_5v^T_5再作图隐约可以辨别这是短发伽椰子的脸……但还是很模糊，毕竟我们只取了5个奇异值而已。下面我们取20个奇异值试试，也就是(1)式等式右边取前20项构成 A20。虽然还有些马赛克般的模糊，但我们总算能辨别出这是Juri酱的脸。当我们取到(1)式等式右边前50项时：我们得到和原图差别不大的图像。也就是说当k从1不断增大时，A_k不断的逼近A。让我们回到公式 A=σ_1μ_1v^T_1+σ_2μ_2v^T_2+...+σ_rμ_rv^T_r矩阵表示一个$450\times333$的矩阵，需要保存$450\times 333=149850$个元素的值。等式右边和分别是$450\times 1$和$333\times1$的向量，每一项有个元素。如果我们要存储很多高清的图片，而又受限于存储空间的限制，在尽可能保证图像可被识别的精度的前提下，我们可以保留奇异值较大的若干项，舍去奇异值较小的项即可。例如在上面的例子中，如果我们只保留奇异值分解的前50项，则需要存储的元素为，和存储原始矩阵相比，存储量仅为后者的26%。 奇异值往往对应着矩阵中隐含的重要信息，且重要性和奇异值大小正相关。每个矩阵A都可以表示为一系列秩为1的“小矩阵”之和，而奇异值则衡量了这些“小矩阵”对于A的权重。 奇异值分解也可以高效地表示数据。例如，假设我们想传送下列图片，包含$15 \times 25 $个黑色或者白色的像素阵列因为在图像中只有三种类型的列（如下）,它可以以更紧凑的形式被表示。就保留主要样本数据来看，该过程跟PCA( principal component analysis)技术有一些联系，PCA也使用了SVD去检测数据间依赖和冗余信息.如果对M进行奇异值分解的话，我们只会得到三个非零的奇异值。 \sigma _1 =14.72 \ \sigma_2 =5.22 \sigma _3 =3.31因此，矩阵可以如下表示 M=u_1σ_1v_1^T+ u_2σ_2v_2^T + u_3σ_3 v_3^T我们有三个包含15个元素的向量$v_i$，三个包含25个元素的向量$u_i$，以及三个奇异值$\sigma _i$，这意味着我们可以只用123个数字就能表示这个矩阵而不是出现在矩阵中的375个元素。在这种方式下，我们看到在矩阵中有三个线性独立的列，也就是说矩阵的秩是3。 3.3 图像去噪在图像处理领域，奇异值不仅可以应用在数据压缩上，还可以对图像去噪。如果一副图像包含噪声，我们有理由相信那些较小的奇异值就是由于噪声引起的。当我们强行令这些较小的奇异值为0时，就可以去除图片中的噪声。如下是一张$25*15$的图像（本例来源于[1]） 但往往我们只能得到如下带有噪声的图像（和无噪声图像相比，下图的部分白格子中带有灰色）： 通过奇异值分解，我们发现矩阵的奇异值从大到小分别为：14.15，4.67，3.00，0.21，……，0.05。除了前3个奇异值较大以外，其余奇异值相比之下都很小。强行令这些小奇异值为0，然后只用前3个奇异值构造新的矩阵，得到可以明显看出噪声减少了（白格子上灰白相间的图案减少了）。 3.4 数据分析我们搜集的数据中总是存在噪声：无论采用的设备多精密，方法有多好，总是会存在一些误差的。如果你们还记得上文提到的，大的奇异值对应了矩阵中的主要信息的话，运用SVD进行数据分析，提取其中的主要部分的话，还是相当合理的。 作为例子，假如我们搜集的数据如下所示： 我们将数据用矩阵的形式表示：经过奇异值分解后，得到 \sigma_1 = 6.04 \ \sigma _2 =0.22由于第一个奇异值远比第二个要大，数据中有包含一些噪声，第二个奇异值在原始矩阵分解相对应的部分可以忽略。经过SVD分解后，保留了主要样本点如图所示就保留主要样本数据来看，该过程跟PCA( principal component analysis)技术有一些联系，PCA也使用了SVD去检测数据间依赖和冗余信息. 3.5 潜在语义索引LSI 潜在语义索引（Latent Semantic Indexing）与PCA不太一样，至少不是实现了SVD就可以直接用的，不过LSI也是一个严重依赖于SVD的算法，之前吴军老师在矩阵计算与文本处理中的分类问题中谈到： “三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。” 上面这段话可能不太容易理解，不过这就是LSI的精髓内容，我下面举一个例子来说明一下，下面的例子来自LSA tutorial： 这就是一个矩阵，不过不太一样的是，这里的一行表示一个词在哪些title中出现了（一行就是之前说的一维feature），一列表示一个title中有哪些词，（这个矩阵其实是我们之前说的那种一行是一个sample的形式的一种转置，这个会使得我们的左右奇异向量的意义产生变化，但是不会影响我们计算的过程）。比如说T1这个title中就有guide、investing、market、stock四个词，各出现了一次，我们将这个矩阵进行SVD，得到下面的矩阵：左奇异向量表示词的一些特性，右奇异向量表示文档的一些特性，中间的奇异值矩阵表示左奇异向量的一行与右奇异向量的一列的重要程序，数字越大越重要。 继续看这个矩阵还可以发现一些有意思的东西，首先，左奇异向量的第一列表示每一个词的出现频繁程度，虽然不是线性的，但是可以认为是一个大概的描述，比如book是0.15对应文档中出现的2次，investing是0.74对应了文档中出现了9次，rich是0.36对应文档中出现了3次； 其次，右奇异向量中一的第一行表示每一篇文档中的出现词的个数的近似，比如说，T6是0.49，出现了5个词，T2是0.22，出现了2个词。 然后我们反过头来看，我们可以将左奇异向量和右奇异向量都取后2维（之前是3维的矩阵），投影到一个平面上，可以得到： 在图上，每一个红色的点，都表示一个词，每一个蓝色的点，都表示一篇文档，这样我们可以对这些词和文档进行聚类，比如说stock 和 market可以放在一类，因为他们老是出现在一起，real和estate可以放在一类，dads，guide这种词就看起来有点孤立了，我们就不对他们进行合并了。按这样聚类出现的效果，可以提取文档集合中的近义词，这样当用户检索文档的时候，是用语义级别（近义词集合）去检索了，而不是之前的词的级别。这样一减少我们的检索、存储量，因为这样压缩的文档集合和PCA是异曲同工的，二可以提高我们的用户体验，用户输入一个词，我们可以在这个词的近义词的集合中去找，这是传统的索引无法做到的。 3.6 主成分分析PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。方差的大小描述的是一个变量的信息量，我们在讲一个东西的稳定性的时候，往往说要减小方差，如果一个模型的方差很大，那就说明模型不稳定了。但是对于我们用于机器学习的数据（主要是训练数据），方差大才有意义，不然输入的数据都是同一个点，那方差就为0了，这样输入的多个数据就等同于一个数据了。以下面这张图为例子： 这个假设是一个摄像机采集一个物体运动得到的图片，上面的点表示物体运动的位置，假如我们想要用一条直线去拟合这些点，那我们会选择什么方向的线呢？当然是图上标有signal的那条线。如果我们把这些点单纯的投影到x轴或者y轴上，最后在x轴与y轴上得到的方差是相似的（因为这些点的趋势是在45度左右的方向，所以投影到x轴或者y轴上都是类似的），如果我们使用原来的xy坐标系去看这些点，容易看不出来这些点真正的方向是什么。但是如果我们进行坐标系的变化，横轴变成了signal的方向，纵轴变成了noise的方向，则就很容易发现什么方向的方差大，什么方向的方差小了。 一般来说，方差大的方向是信号的方向，方差小的方向是噪声的方向，我们在数据挖掘中或者数字信号处理中，往往要提高信号与噪声的比例，也就是信噪比。对上图来说，如果我们只保留signal方向的数据，也可以对原数据进行不错的近似了。 PCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。 还是假设我们矩阵每一行表示一个样本，每一列表示一个feature，用矩阵的语言来表示，将一个m * n的矩阵A的进行坐标轴的变化，P就是一个变换的矩阵从一个N维的空间变换到另一个N维的空间，在空间中就会进行一些类似于旋转、拉伸的变化。 A_{m\times n}P_{n\times n}=\tilde{A}_{m\times n}而将一个$m\times n$的矩阵$A$变换成一个$m\times r$的矩阵，这样就会使本来有$n$个feature的，变成了有$r$个feature了（r&lt;n），这r个其实就是对n个feature的一种提炼，我们就把这个称为feature的压缩。用数学语言表示就是： A_{m\times n}P_{n\times r}=\tilde{A}_{m\times r}但是这个和SCD扯上关系的呢？之前谈到，SVD得出的奇异向量也是从奇异值由大到小排列的，按照PCA的观点来看，就是方差最大的坐标轴就是第一个奇异向量，方差次大的坐标轴就是第二个奇异向量。之前得到的SVD式子如下： A_{m\times n}≈ U_{m\times r} \Sigma_{r\times r}V^T_{r\times n}在矩阵的两边同时乘上一个矩阵$V$，由于$V$是一个正交的矩阵，所以转置乘以$V$得到单位阵$I$，所以可以化成后面的式子： A_{m\times n}V_{r\times n}≈ U_{m\times r} \Sigma_{r\times r}V^T_{r\times n}V_{r\times n}=U_{m\times r} \Sigma_{r\times r}将后面的式子与$A\times P $那个$m\times n$矩阵变换为$m\times r$的矩阵的式子对照看看，在这里，其实$V$就是$P$，也就是一个变化的向量。这里将一个$m\times n$的矩阵压缩到一个$m \ times r$的矩阵，也就是对列进行压缩。 如果我们想对行进行压缩（在PCA的观点下，对行进行业所可以理解为，讲一些相似的sample合并在一起，或者将一些没有太大价值的sample去掉），同样我们写出一个通用的行压缩例子： P_{r\times m}A_{m\times n}=\tilde{A}_{r\times n}这样从一个$m$行的矩阵压缩到一个$r$行的矩阵了，对SVD来说也是一样的，我们对SVD分解的式子两边乘以一个转置$U$得到： U^T_{r\times m}A_{m\times n}≈ U^T_{r\times m}U_{m\times r} \Sigma_{r\times r}V^T_{r\times n}= \Sigma_{r\times r}V^T_{r\times n} 这样我们就得到了对行进行压缩的式子。可以看出，其实PCA几乎可以说是对SVD的一个包装，如果我们实现了SVD，那也就实现了PCA了，而且更好的地方是，有了SVD，我们就可以得到两个方向的PCA，如果我们对A’A进行特征值的分解，只能得到一个方向的PCA。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（20）：机器学习模型优化四要素]]></title>
    <url>%2F2017%2F06%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8820%EF%BC%89%EF%BC%9A%E9%A1%B9%E7%9B%AE%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E5%9B%9B%E8%A6%81%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[本文转载自美团点评技术团队博客，该文以业界视角介绍了机器学习如何发挥其实际价值。作者胡淏，目前是美团算法工程师，毕业于哥伦比亚大学。先后在携程、支付宝、美团从事算法开发工作。了解风控、基因、旅游、即时物流相关问题的行业领先算法方案与流程。 一、机器学习工程师的知识图谱图1 机器学习工程师的知识图谱 上图列出了我认为一个成功的机器学习工程师需要关注和积累的点。机器学习实践中，我们平时都在积累自己的“弹药库”：分类、回归、无监督模型、Kaggle上特征变换的黑魔法、样本失衡的处理办法、缺失值填充……这些大概可以归类成模型和特征两个点。我们需要参考成熟的做法、论文，并自己实现，此外还需要多反思自己方法上是否还可以改进。如果模型和特征这俩个点都已经做的很好了，你就拥有了一张绿卡，能跨过在数据相关行业发挥模型技术价值的准入门槛。 在这个时候，比较关键的一步，就是搞笑的技术变现能力。 所谓高效，就是解决业务核心问题的专业能力。本文将描述这些专业能力，也就是模型优化的四个要素：模型、数据、特征、业务，还有更重要的，就是他们在模型项目中的优先级。 二、模型项目推进的四要素项目推进过程中，四个要素相互之间的优先级大致是：业务&gt;特征&gt;数据&gt;模型。图2 四要素解决问题细分+优先级 2.1 业务一个模型项目有好的技术选型、完备的特征体系、高质量的数据一定是很加分的，不过真正决定项目好与坏还有一个大前提，就是在这个项目的技术目标是否在解决当下核心业务问题。 业务问题包含两个方面：业务KPI和Deadline。举个例子，业务问题在两周之内降低目前手机丢失带来的支付宝销赃风险。这时如果你的方案是研发手机丢失的核心特征，比如改密是否合理，基本上就死的很惨，因为两周根本完不成，改密合理性也未必是模型优化好的切入点；反之，如果你的方案是和运营同学看bad case，梳理现阶段的作案通用手段，并通过分析上线一个简单模型或者业务规则的补丁，就明智很多。如果上线之后，案件量真掉下来了，就算你的方案准确率很糟糕、方法很low，但你解决了业务问题，这才是最重要的。 虽然业务目标很关键，不过一般讲，业务运营同学真的不太懂得如何和技术有效的沟通业务目标，比如： 我们想做一个线下门店风险评级的项目，希望运营通过反作弊模型角度帮我们给门店打个分，这个分数包含的问题有：风险是怎么定义的、为什么要做风险评级、更大的业务目标是什么、怎么排期的、这个风险和我们反作弊模型之间的腋窝你是怎么看的？ 做一个区域未来10min的配送时间预估模型。我们想通过运营的模型衡量在恶劣天气的时候每个区域的运力是否被击穿（业务现状和排期？运力被击穿可以扫下盲吗？运力击穿和配送时间之间是个什么业务逻辑、时间预估是刻画运力紧张度的最有效手段么？业务的关键场景是恶劣天气的话，我们仅仅训练恶劣天气场景的时间预估模型是否就好了？） 为了保证整个技术项目没有做偏，项目一开始一定要和业务聊清楚三件事情： 业务核心问题、关键场景是什么。 如何评估该项目的成功，指标是什么。 通过项目输出什么关键信息给到业务，业务如何运营这个信息从而达到业务目标。 项目过程中，也要时刻回到业务，检查项目的健康度。 2.2 数据与特征要说正确的业务理解和切入，在为技术项目保驾护航，数据、特征便是一个模型项目性能方面的天花板。garbage in， garbage out 就在说这个问题。 这两天有位听众微信问我一个很难回答的问题，大概意思是，数据是特征拼起来构成的集合嘛，所以这不是两个要素。从逻辑上面讲，数据的确是一列一列的特征，不过数据与特征在概念层面是不同的：数据是已经采集的信息，特征是以兼容模型、最优化为目标对数据进行加工。就比如通过word2vec将非结构化数据结构化，就是将数据转化为特征的过程。 所以，我更认为特征工程是基于数据的一个非常精细、刻意的加工过程。从传统的特征转换、交互，到embedding、word2vec、高维分类变量数值化，最终目的都是更好的去利用现有的数据。之前有聊到的将推荐算法引入有监督学习模型优化中的做法，就是在把两个本不可用的高维ID类变量变成可用的数值变量。 观察到自己和童鞋们在特征工程中会遇到一些普遍问题，比如，特征设计不全面，没有耐心把现有特征做得细致……也整理出来一套方法论，仅供参考： 图3 变量体系、研发流程 在特征设计的时候，有两个点可以帮助我们把特征想的更全面： 现有的基础数据 业务“二维图” 这两个方面的整合，就是一个变量的体系。变量（特征），从技术层面是加工数据，而从业务层面实际在反应RD的业务理解和数据刻画业务能力。“二维图”，实际上未必是二维的，更重要的是我们需要把业务整个流程抽象成几个核心的维度，举几个例子： 这两个方面的整合，就是一个变量的体系。变量（特征），从技术层面是加工数据，而从业务层面实际在反应RD的业务理解和数据刻画业务能力。“二维图”，实际上未必是二维的，更重要的是我们需要把业务整个流程抽象成几个核心的维度，举几个例子： 外卖配送时间业务（维度甲：配送的环节，骑手到点、商家出餐、骑手配送、交付用户；维度乙：颗粒度，订单粒度、商家粒度、区域城市粒度；维度丙：配送类型，众包、自营……）。 反作弊变量体系（维度甲：作弊环节，登录、注册、实名、转账、交易、参与营销活动、改密……；维度乙：作弊介质，账户、设备、IP、WiFi、银行卡……）。 通过这些维度，你就可以展开一个“二维图”，把现有你可以想到的特征填上去，你一定会发现很多空白，比如下图，那么哪里还是特征设计的盲点就一目了然： 图4 账户维度在转账、红包方面的特征很少；没有考虑WiFi这个媒介；客满与事件数据没考虑 数据和特征决定了模型性能的天花板。deep learning当下在图像、语音、机器翻译、自动驾驶等领域非常火，但是 deep learning在生物信息、基因学这个领域就不是热词：这背后是因为在前者，我们已经知道数据从哪里来，怎么采集，这些数据带来的信息基本满足了模型做非常准确的识别；而后者，即便有了上亿个人体碱基构成的基因编码，技术选型还是不能长驱直入——超高的数据采集成本，人后天的行为数据的获取壁垒等一系列的问题，注定当下这个阶段在生物信息领域，人工智能能发出的声音很微弱，更大的舞台留给了生物学、临床医学、统计学。 2.3 模型图5 满房开房的技术选型、特征工程roadmap 模型这件事儿，许多时候追求的不仅仅是准确率，通常还有业务这一层更大的约束。如果你在做一些需要强业务可解释的模型，比如定价和反作弊，那实在没必要上一个黑箱模型来为难业务。这时候，统计学习模型就很有用。 这种情况下，比拼性能的话，我觉得下面这个不等式通常成立：Glmnet&gt;LASSO&gt;=Ridge&gt;LR/Logistic。相比最基本的LR/Logistic，ridge通过正则化约束缓解了LR在过拟合方面的问题，lasso更是通过L1约束做类似变量选择的工作。 不过两个算法的痛点是很难决定最优的约束强度，Glmnet是Stanford给出的一套非常高效的解决方案。所以目前，我认为线性结构的模型，Glmnet的痛点是最少的，而且在R、Python、Spark上面都开源了。 如果我们开发复杂模型，通常成立第二个不等式 RF（Random Forest，随机森林）&lt;= GBDT &lt;= XGBoost 。拿数据说话，29个Kaggle公开的winner solution里面，17个使用了类似GBDT这样的Boosting框架，其次是 DNN（Deep Neural Network，深度神经网络），RF的做法在Kaggle里面非常少见。 RF和GBDT两个算法的雏形是CART（Classification And Regression Trees），由L Breiman和J Friedman两位作者在1984年合作推出。但是在90年代在发展模型集成思想the ensemble的时候，两位作者代表着两个至今也很主流的派系：stacking/ Bagging &amp; Boosting。 一种是把相互独立的CART（randomized variables，bootstrap samples）水平铺开，一种是深耕的Boosting，在拟合完整体后更有在局部长尾精细刻画的能力。同时，GBDT模型相比RF更加简单，内存占用小，这都是业界喜欢的性质。XGBoost在模型的轻量化和快速训练上又做了进一步的工作，也是目前我们比较喜欢尝试的模型。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>业务</tag>
        <tag>特征</tag>
        <tag>数据</tag>
        <tag>模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle系列（4）：Rental Listing Inquiries（三）：XGBoost调参指南]]></title>
    <url>%2F2017%2F06%2F14%2Fkaggle%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9ARental%20Listing%20Inquiries%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9AXGBoost%E8%B0%83%E5%8F%82%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[这篇文章翻译自Complete Guide to Parameter Tuning in XGBoost (with codes in Python)，它详细介绍了XGBoost中参数的含义，然后在一个实例中对参数调整进行了实验。 一、Introduction如果你的预测模型效果不怎么好，使用XGBoost吧。XGBoost已经成为许多数据科学家的终极武器了！这是一个内部实现高度复杂的算法，在处理各种不规范的数据时有足够强大的表现。 利用XGBoost建立模型很简单，但是因为它使用了很多参数，以致使用XGBoost来提升预测效果比较有难度。为提高模型的预测能力，调参是必须要做的一步。但仍然有很多现实的挑战——哪些参数是我们需要调整的？每个参数的最佳值又应该是多少呢？ 这篇文章最适合刚刚接触XGBoost的人，在这篇文章中，我们将会介绍一些XGBoost相关的知识，同时了解一些XGBoost参数调整技艺。最后使用Python对一个数据集实践XGBoost。 二、What should you know ?XGBoost(eXtreme Gradient Boosting)是Gradient Boosting算法的一个优化的版本。因为我在前一篇文章，基于Python的Gradient Boosting算法参数调整完全指南，里面已经涵盖了Gradient Boosting算法的很多细节了。我强烈建议大家在读本篇文章之前，把那篇文章好好读一遍。它会帮助你对Boosting算法有一个宏观的理解，同时也会对GBM的参数调整有更好的体会。 特别鸣谢：我个人十分感谢Mr Sudalai Rajkumar (aka SRK)大神的支持，目前他在AV Rank中位列第二。如果没有他的帮助，就没有这篇文章。在他的帮助下，我们才能给无数的数据科学家指点迷津。给他一个大大的赞！ 三、Table of Contents3.1 The XGBoost AdvantageXGBoost算法可以给预测模型带来能力的提升。当我对它的表现有更多了解的时候，当我对它的高准确率背后的原理有更多了解的时候，我发现它具有很多优势： 正则化标准GBM的实现没有像XGBoost这样的正则化步骤。正则化对减少过拟合也是有帮助的。实际上，XGBoost以“正则化提升(regularized boosting)”技术而闻名。 并行处理不过，众所周知，Boosting算法是顺序处理的，它怎么可能并行呢？每一课树的构造都依赖于前一棵树，那具体是什么让我们能用多核处理器去构造一个树呢？我希望你理解了这句话的意思。如果你希望了解更多，点击这个链接。XGBoost 也支持Hadoop实现。 高度的灵活性XGBoost 允许用户定义自定义优化目标和评价标准它对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。 缺失值处理XGBoost内置处理缺失值的规则。用户需要提供一个和其它样本不同的值，然后把它作为一个参数传进去，以此来作为缺失值的取值。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。 剪枝当分裂时遇到一个负损失时，GBM会停止分裂。因此GBM实际上是一个贪心算法。XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂。这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。GBM会在-2处停下来，因为它遇到了一个负值。但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂。 内置交叉验证XGBoost允许在每一轮boosting迭代中使用交叉验证。因此，可以方便地获得最优boosting迭代次数。而GBM使用网格搜索，只能检测有限个值。 在已有的模型基础上继续XGBoost可以在上一轮的结果上继续训练。这个特性在某些特定的应用上是一个巨大的优势。sklearn中的GBM的实现也有这个功能，两种算法在这一点上是一致的。 相信你已经对XGBoost强大的功能有了点概念。注意这是我自己总结出来的几点，你如果有更多的想法，尽管在下面评论指出，我会更新这个列表的！你的胃口被我吊起来了吗？棒棒哒！如果你想更深入了解相关信息，可以参考下面这些文章： XGBoost Guide - Introduce to Boosted Trees Words from the Auther of XGBoost 3.2 Understanding XGBoost ParametersXGBoost的作者把所有参数分成了三类： 通用参数：宏观函数控制。 Booster参数：控制每一步的Booster（tree/regression） 学习目标参数：控制训练目标的表现 在这里我会类比GBM来讲解，所以作为一种基础知识，强烈推荐先阅读这篇文章。 3.2.1 通用参数（General Parameters）这些参数用来控制XGBoost的宏观功能。 booster[默认gbtree]：选择每次迭代的模型，有两种选择： gbtree：基于树的模型 gbliner：线性模型 silent[默认0]： 当这个参数值为1时，静默模式开启，不会输出任何信息。一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。 nthread[默认值为最大可能的线程数]： 这个参数用来进行多线程控制，应当输入系统的核数。如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。 还有两个参数，XGBoost会自动设置，目前你不用管它。接下来咱们一起看booster参数。 3.2.2 Booster参数（Booster Parameters）尽管有两种booster可供选择，我这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到。 eta[默认0.3]： 和GBM中的 learning rate 参数类似。通过减少每一步的权重，可以提高模型的鲁棒性。典型值为0.01-0.2。 min_child_weight[默认1]： 决定最小叶子节点样本权重和。和GBM的min_child_leaf 参数类似，但不完全一样。XGBoost的这个参数是最小样本权重的和，而GBM参数是最小样本总数。这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。 max_depth[默认6]： 和GBM中的参数相同，这个值为树的最大深度。这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。需要使用CV函数来进行调优。典型值：3-10 max_leaf_nodes： 树上最大的节点或叶子的数量。可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成 $n^2$ 个叶子。如果定义了这个参数，GBM会忽略max_depth参数。 gamma[默认0]： 在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。 max_delta_step[默认0]： 这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。 通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。 这个参数一般用不到，但是你可以挖掘出来它更多的用处。 subsample[默认1]： 和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。典型值：0.5-1。 colsample_bytree[默认1]： 和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。典型值：0.5-1 colsample_bylevel[默认1]： 用来控制树的每一级的每一次分裂，对列数的采样的占比。我个人一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。 lambda[默认1] 权重的L2正则化项。(和Ridge regression类似)。这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。 alpha[默认1]： 权重的L1正则化项。(和Lasso regression类似)。可以应用在很高维度的情况下，使得算法的速度更快。 scale_pos_weight[默认1]： 在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。 3.2.3 学习目标参数这个参数用来控制理想的优化目标和每一步结果的度量方法。 objective[默认reg:linear]：这个参数定义需要被最小化的损失函数。最常用的值有： binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。 multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。在这种情况下，你还需要多设一个参数：num_class(类别数目)。 multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。 eval_metric[默认值取决于objective参数的取值]：对于有效数据的度量方法。对于回归问题，默认值是rmse，对于分类问题，默认值是error。典型值有： rmse 均方根误差( ∑Ni=1ϵ2N−−−−−−√ ) mae 平均绝对误差( ∑Ni=1|ϵ|N ) logloss 负对数似然函数值 error 二分类错误率(阈值为0.5) merror 多分类错误率 mlogloss 多分类logloss损失函数 auc 曲线下面积 seed(默认0)：随机数的种子。设置它可以复现随机数据的结果，也可以用于调整参数 如果你之前用的是Scikit-learn,你可能不太熟悉这些参数。但是有个好消息，python的XGBoost模块有一个sklearn包，XGBClassifier。这个包中的参数是按sklearn风格命名的。会改变的函数名是： eta -&gt;learning_rate lambda-&gt;reg_lambda alpha-&gt;reg_alpha 你肯定在疑惑为啥咱们没有介绍和GBM中的n_estimators类似的参数。XGBClassifier中确实有一个类似的参数，但是，是在标准XGBoost实现中调用拟合函数时，把它作为num_boosting_rounds参数传入。 XGBoost Guide 的一些部分是我强烈推荐大家阅读的，通过它可以对代码和参数有一个更好的了解： XGBoost Parameters (official guide) XGBoost Demo Codes (xgboost GitHub repository) Python API Reference (official guide) 3.3 Tuning Parameters (with Example) 未完待续······]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
        <tag>XGBoost</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle系列（3）：Rental Listing Inquiries（二）：XGBoost]]></title>
    <url>%2F2017%2F06%2F13%2Fkaggle%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9ARental%20Listing%20Inquiries%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9AXGBoost%2F</url>
    <content type="text"><![CDATA[上一节我们对数据集进行了初步的探索，并将其可视化，对数据有了初步的了解。这样我们有了之前数据探索的基础之后，就有了对其建模的基础feature，结合目标变量，即可进行模型训练了。我们使用交叉验证的方法来判断线下的实验结果，也就是把训练集分成两部分，一部分是训练集，用来训练分类器，另一部分是验证集，用来计算损失评估模型的好坏。 在Kaggle的希格斯子信号识别竞赛中，XGBoost因为出众的效率与较高的预测准确度在比赛论坛中引起了参赛选手的广泛关注，在1700多支队伍的激烈竞争中占有一席之地。随着它在Kaggle社区知名度的提高，最近也有队伍借助XGBoost在比赛中夺得第一。其次，因为它的效果好，计算复杂度不高，也在工业界中有大量的应用。 今天，我们就先来跑一个XGBoost版的Base Model。先回顾一下XGBoost的原理吧：机器学习算法系列（8）：XgBoost 一、 准备工作首先我们导入需要的包： 12345678910import osimport sys import operatorimport numpy as npimport pandas as pdfrom scipy import sparseimport xgboost as xgbfrom sklearn import model_selection,preprocessing,ensemblefrom sklearn.metrics import log_lossfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer 其中一些包的用途会在之后具体用到的时候进行讲解。 导入我们的数据： 12345678910data_path = '../data/'train_file = data_path + "train.json"test_file = data_path +"test.json"train_df = pd.read_json(train_file)test_df = pd.read_json(test_file)print train_df.shapeprint test_df.shape(49352, 15)(74659, 14) 查看一下前两行： 1train_df.head(2) 二、特征构建我们不需要对数值型数据进行任何的预处理，所以首先建立一个数值型特征的列表，纳入features_to_use 1features_to_use = ["bathrooms","bedrooms","latitude","longitude","price"] 现在让我们根据已有的一些特征来构建一些新的特征： 1234567891011121314151617181920212223242526272829303132# 照片数量(num_photos)train_df['num_photos']=train_df['photos'].apply(len)test_df['num_photos']=train_df['photos'].apply(len)# 特征数量train_df['num_features']=train_df['features'].apply(len)test_df['num_features']=test_df['features'].apply(len)# 描述词汇数量train_df['num_description_words'] = train_df['description'].apply(lambda x: len(x.split(" ")))test_df['num_description_words'] = test_df['description'].apply(lambda x: len(x.split(" ")))#把创建的时间分解为多个特征 train_df['created']=pd.to_datetime(train_df['created'])test_df['created']=pd.to_datetime(test_df['created']) #让我们从时间中分解出一些特征，比如年，月，日，时#年train_df['created_year'] = train_df['created'].dt.yeartest_df['created_year'] = test_df['created'].dt.year#月train_df['created_month'] = train_df['created'].dt.monthtest_df['created_month'] = test_df['created'].dt.month#日train_df['created_day'] = train_df['created'].dt.daytest_df['created_day'] = test_df['created'].dt.day#时train_df['created_hour'] = train_df['created'].dt.hourtest_df['created_hour'] = test_df['created'].dt.hour#把这些特征都放到所需特征列表中（上面已经创建，并加入了数值型特征） features_to_use.extend(["num_photos","num_features","num_description_words","created_year","created_month","created_day","created_hour","listing_id"]) 我们有四个分类型的特征： display_address manager_id building_id street_address 可以对它们分别进行特征编码： 12345678categorical = ["display_address","manager_id",'building_id',"street_address"]for f in categorical: if train_df[f].dtype == 'object': lbl = preprocessing.LabelEncoder() lbl.fit(list(train_df[f].values)+list(test_df[f].values)) train_df[f] = lbl.transform(list(train_df[f].values)) test_df[f] = lbl.transform(list(test_df[f].values)) features_to_use.append(f) 还有一些字符串类型的特征，可以先把它们合并起来 1234train_df[&quot;features&quot;] = train_df[&quot;features&quot;].apply(lambda x:&quot; &quot;.join([&quot;_&quot;.join(i.split(&quot; &quot;))for i in x]))print train_df[&apos;features&apos;].head(2)test_df[&apos;features&apos;] = test_df[&quot;features&quot;].apply(lambda x: &quot; &quot;.join([&quot;_&quot;.join(i.split(&quot; &quot;))for i in x]))print test_df[&apos;features&apos;].head(2) 得到的字符串结果如下： 10000 Doorman Elevator Fitness_Center Cats_Allowed D…100004 Laundry_In_Building Dishwasher Hardwood_Floors… 然后CountVectorizer类来计算TF-IDF权重 123tfidf = CountVectorizer(stop_words =&quot;english&quot;,max_features=200)tr_sparse = tfidf.fit_transform(train_df[&quot;features&quot;])te_sparse = tfidf.transform(test_df[&quot;features&quot;]) 这里我们需要提一点，对数据集进行特征变换时，必须同时对训练集和测试集进行操作。现在把这些处理过的特征放到一个集合中（横向合并） 12train_X = sparse.hstack([train_df[features_to_use],tr_sparse]).tocsr()test_X = sparse.hstack([test_df[features_to_use],te_sparse]).tocsr() 然后把目标变量转换为0、1、2，如下 12345target_num_map = &#123;'high':0 , 'medium':1 , 'low':2&#125;train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))print train_X.shape,test_X.shape(49352, 217) (74659, 217) 可以看到，经过上面一系列的变量构造之后，其数量已经达到了217个。 接下来就可以进行建模啦。 三、XGB建模先写一个通用的XGB模型的函数： 12345678910111213141516171819202122232425262728def runXGB(train_X,train_y,test_X,test_y=None,feature_names=None,seed_val=0,num_rounds=1000): #参数设定 param = &#123;&#125; param['objective'] = 'multi:softprob'#多分类、输出概率值 param['eta'] = 0.1#学习率 param['max_depth'] = 6#最大深度，越大越容易过拟合 param['silent'] = 1#打印提示信息 param['num_class'] = 3#三个类别 param['eval_metric']= "mlogloss"#对数损失 param['min_child_weight']=1#停止条件，这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 param['subsample'] =0.7#随机采样训练样本 param['colsample_bytree'] = 0.7# 生成树时进行的列采样 param['seed'] = seed_val#随机数种子 num_rounds = num_rounds#迭代次数 plst = list(param.items()) xgtrain = xgb.DMatrix(train_X,label=train_y) if test_y is not None: xgtest = xgb.DMatrix(test_X,label=test_y) watchlist = [(xgtrain,'train'),(xgtest,'test')] model = xgb.train(plst,xgtrain,num_rounds,watchlist,early_stopping_rounds=20) # early_stopping_rounds 当设置的迭代次数较大时，early_stopping_rounds 可在一定的迭代次数内准确率没有提升就停止训练 else: xgtest = xgb.DMatrix(test_X) model = xgb.train(plst,xgtrain,num_rounds) pred_test_y = model.predict(xgtest) return pred_test_y,model 函数返回的是预测值和模型。 5折交叉验证将训练集划分为五份，其中的一份作为验证集。 12345678910cv_scores = []kf = model_selection.KFold(n_splits=5,shuffle=True,random_state=2016)for dev_index,val_index in kf.split(range(train_X.shape[0])): dev_X,val_X = train_X[dev_index,:],train_X[val_index,:] dev_y,val_y = train_y[dev_index],train_y[val_index] pred,model = runXGB(dev_X,dev_y,val_X,val_y) cv_scores.append(log_loss(val_y,preds)) print cv_scores break 结果如下： 123456789101112131415161718192021222324252627282930[0] train-mlogloss:1.04135 test-mlogloss:1.04229Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.Will train until test-mlogloss hasn't improved in 20 rounds.[1] train-mlogloss:0.989004 test-mlogloss:0.99087[2] train-mlogloss:0.944233 test-mlogloss:0.947047[3] train-mlogloss:0.90536 test-mlogloss:0.908933[4] train-mlogloss:0.872054 test-mlogloss:0.876526[5] train-mlogloss:0.841783 test-mlogloss:0.847383[6] train-mlogloss:0.815921 test-mlogloss:0.822307[7] train-mlogloss:0.793337 test-mlogloss:0.800476[8] train-mlogloss:0.773562 test-mlogloss:0.781413[9] train-mlogloss:0.754927 test-mlogloss:0.76381[10] train-mlogloss:0.738299 test-mlogloss:0.747959············[367] train-mlogloss:0.348196 test-mlogloss:0.548011[368] train-mlogloss:0.347768 test-mlogloss:0.547992[369] train-mlogloss:0.347303 test-mlogloss:0.548021[370] train-mlogloss:0.346807 test-mlogloss:0.548065[371] train-mlogloss:0.346514 test-mlogloss:0.548079[372] train-mlogloss:0.34615 test-mlogloss:0.548097[373] train-mlogloss:0.345859 test-mlogloss:0.548111[374] train-mlogloss:0.345377 test-mlogloss:0.548081[375] train-mlogloss:0.344961 test-mlogloss:0.548068[376] train-mlogloss:0.344493 test-mlogloss:0.548024[377] train-mlogloss:0.344086 test-mlogloss:0.547975Stopping. Best iteration:[357] train-mlogloss:0.352182 test-mlogloss:0.547867 迭代357次之后，在训练集上的对数损失为0.352182，在验证集上的损失为0.5478。 然后在对测试集进行预测： 1preds,model=runXGB(train_X,train_y,test_X,num_rounds=400) 把结果按照比赛规定的格式写入csv文件： 1234out_df = pd.DataFrame(preds)out_df.columns = ["high", "medium", "low"]out_df["listing_id"] = test_df.listing_id.valuesout_df.to_csv("xgb_starter2.csv", index=False) 看一下最后的结果：提交到kaggle上，这样我们整个建模的过程就完成了。 接下来两节中，我们重点讲一讲关于XGBoost的调参经验以及使用SK-learn计算TF-IDF。]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>kaggle</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle系列（2）：Rental Listing Inquiries（一）：EDA]]></title>
    <url>%2F2017%2F06%2F13%2Fkaggle%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9ARental%20Listing%20Inquiries%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AEDA%2F</url>
    <content type="text"><![CDATA[一、比赛简介1.1 比赛目的这个kaggle比赛是由Sigma和RentHop两家公司共同推出的比赛。比赛的数据来自于RentHop的租房信息，大概的思路就是根据出租房的一系列特征，比如地理位置（经纬度、街道地址）、发布时间、房间设施（浴室、卧室数量）、描述信息、发布的图片信息、价格等来预测消费者对出租房的喜好程度。 这样可以帮助RentHop公司更好地处理欺诈事件，让房主和中介更加理解租客的需求与偏好，做出更加合理的决策。 1.2 数据集在这个比赛中，房源的数据来自于renthop网站，这些公寓都位于纽约市。其目的之前已经提到过了，就是基于一系列特征预测公寓房源的受欢迎程度，其目标变量是：interest_level，它是指从在网站上发布房源起始的时间内，房源的询问次数。 其中，比赛一共给了五个数据文件，分别是： train.json：训练集 test.json：测试集 sample_submission.csv：格式正确的提交示例 images_sample.zip：租房图片集（只抽取了100个图片集） kaggle-renthop.7z：所有的租房图片集，一共有78.5GB的压缩文件。 给出的特征的含义： bathrooms: 浴室的数量 bedrooms: 卧室的数量 building_id： created：发布时间 description：一些描述 display_address：展出地址 features: 公寓的一些特征 latitude：纬度 listing_id longitude：经度 manager_id：管理ID photos: 租房图片集 price: 美元 street_address：街道地址 interest_level: 目标变量，受欢迎程度. 有三个类: ‘high’, ‘medium’, ‘low’ 1.3 提交要求这个比赛使用的是多分类对数似然损失函数来评价模型。因为每一个房源都有一个对应的最准确的类别，对每一个房源，需要提交它属于每一类的概率值，它的计算公式如下： \log loss=-\frac{1}{N}\sum_{i=1}^N{\sum_{j=1}^M{y_{ij}\log\left(p_{ij}\right)}}其中$N$是测试集中的样本数量，$M$是类别的数量（3类：high、medium、low）,$log$是自然对数，$y_{ij}$表示样本$i$属于$j$类则为1，否则为0.$p_{ij}$表示样本$i$属于类别$j$的预测概率值。 一个样本的属于三个类别的预测可能性不需要加和为1，因为已经预先归一化了。为避免对数函数的极端情况，预测概率被替代为$max(min(p,1-10^{-15}),10^{-15})$ 最后提交的文件为csv格式，它包含对每一类的预测概率值，行的顺序没有要求，文件必须要有一个表头，看起来像下面的示例： listing_id high medium low 7065104 0.07743 0.23002 0.69254 7089035 0.0 1.0 0.0 二、Exploratory Data Analysis在进行建模之前，我们都会对原始数据进行一些可视化探索，以便更快地熟悉数据，更有效进行之后的特征工程和建模。 我们先导入一些EDA过程中所需要的包： 12345678910import numpy as np import pandas as pd import matplotlib.pyplot as pltimport seaborn as snsimport jsoncolor = sns.color_palette() # 调色板%matplotlib inlinepd.options.mode.chained_assignment = None # default = 'warn' 其中numpy和pandas是数据分析处理中最流行的包，matplotlib和seaborn两个包用来绘制可视化图像，使用%matplotlib命令可以将matplotlib的图表直接嵌入到Notebook之中（%是魔术命令）。 2.1 数据初探使用pandas打开训练集文件train.json，取前两行观测： 12train_df = pd.read_json('data/train.json')train_df.head(8) 我们可以看到给定的数据中包含各种类型的特征，按照其特征可以分为以下几个类别： 特征类型 特征 数值型 bathrooms、bedrooms、price 高势集类别 building_id、display_address、manager_id、street_address 时间型 created 地理位置型特征 longitude、latitude 文本 description 稀疏特征 features id型特征 listing_id、index 看一下训练集和测试集分别有多少 12print "Train Rows:",train_df.shape[0]print "Test Rows:",test_df.shape[0] Train Rows: 49352Test Rows: 74659 训练集有49352个样例，测试集有74659个样例。 接下来我们一一对这些特征进行探索。 2.2 目标变量在深入探索之前，我们先看看目标变量Interest level 123456int_level = train_df['interest_level'].value_counts()plt.figure(figsize=(10,5))sns.barplot(int_level.index,int_level.values,alpha=0.8,color=color[2])plt.xlabel("number of occurrences",fontsize = 12)plt.ylabel("Interest Level",fontsize=12)plt.show() 兴趣度在大多数情况下都是低的，其次是中等，只有少部分的样例为高分。 2.3 数值型特征2.3.1 浴室（bathrooms）先看看浴室的数量分布123456cnt_srs = train_df['bathrooms'].value_counts()plt.figure(figsize=(10,5))sns.barplot(cnt_srs.index,cnt_srs.values,alpha=0.8,color=color[2])plt.xlabel("number of occurrences",fontsize = 12)plt.ylabel("bathrooms",fontsize=12)plt.show() 可以看到绝大多数的样例的浴室数量为1，其次为2个浴室。 再看看不同兴趣程度的浴室数量分布，运用小提琴图来呈现： 12345678#浴室数量大于3的记为3train_df['bathrooms'].loc[train_df['bathrooms']&gt;3]=3plt.figure(figsize=(12,6))sns.violinplot(x = 'interest_level',y = 'bathrooms',data= train_df,alpha=0.8)plt.xlabel("interest level",fontsize = 12)plt.ylabel("bathrooms",fontsize=12)plt.show() 可以看到在不同的兴趣程度上，浴室数量的分布差不多。 2.3.2 卧室（bedrooms）123456cnt_bedrooms = train_df['bedrooms'].value_counts()plt.figure(figsize=(10,5))sns.barplot(cnt_bedrooms.index,cnt_bedrooms.values,alpha=0.8,color=color[3])plt.ylabel("number of occurrences",fontsize = 12)plt.xlabel("bedrooms",fontsize=12)plt.show() 卧室数量基本集中在1和2，也有不少没有卧室，3个卧室的房子也不少。 看看不同兴趣程度的卧室数量分布，同样也用小提琴图来呈现： 12345plt.figure(figsize=(12,6))sns.countplot(x='bedrooms',hue='interest_level',data=train_df)plt.ylabel("number of occurrences",fontsize = 12)plt.xlabel("bedrooms",fontsize=12)plt.show() 2.3.3 价格（price）对价格排序，看一下价格的分布： 12345plt.figure(figsize=(10,5))plt.scatter(range(train_df.shape[0]),np.sort(train_df.price.values))plt.xlabel('index',fontsize=12)plt.ylabel('price',fontsize=12)plt.show() 可以观察到有几个价格格外的高，视为异常值，我们把它们移除掉，然后再绘制分布直方图。 1234567#99%分位数ulimit = np.percentile(train_df.price.values,99)train_df['price'].loc[train_df['price']&gt;ulimit]=ulimitplt.figure(figsize=(10,5))sns.distplot(train_df.price.values,bins=50,kde=True,color=color[3])plt.xlabel('price',fontsize=12)plt.show() 可以观察到分布略微有点右偏。 2.4 地理位置型2.4.1 纬度（latitude）先看看纬度的分布情况 1234567891011#避免极端情况llimit = np.percentile(train_df.latitude.values,1)ulimit = np.percentile(train_df.latitude.values,99)train_df['latitude'].loc[train_df['latitude']&lt;llimit]=llimittrain_df['latitude'].loc[train_df['latitude']&gt;ulimit]=ulimitplt.figure(figsize=(10,5))sns.distplot(train_df.latitude.values,bins=50,kde=True,color=color[3])plt.xlabel('latitude',fontsize=12)plt.show() 由图可知，纬度基本上介于40.6到40.9之间 2.4.2 经度（longitude）1234567891011#避免极端情况llimit = np.percentile(train_df.longitude.values,1)ulimit = np.percentile(train_df.longitude.values,99)train_df['longitude'].loc[train_df['longitude']&lt;llimit]=llimittrain_df['longitude'].loc[train_df['longitude']&gt;ulimit]=ulimitplt.figure(figsize=(12,6))sns.distplot(train_df.longitude.values,bins=50)plt.xlabel('longitude',fontsize=14)plt.show() 经度介于-73.8和-74.02之间。接下来，我们尝试把经纬度对应到地图上，绘制成热图，也就是房源在地理位置上的分布密度图。 1234567891011from mpl_toolkits.basemap import Basemapfrom matplotlib import cmwest,south,east,north =-74.02,40.64,-73.85,40.86fig =plt.figure(figsize=(16,12))ax = fig.add_subplot(111)m=Basemap(projection='merc', llcrnrlat=south,urcrnrlat=north, llcrnrlon=west,urcrnrlon=east, lat_ts=south,resolution='i')x,y=m(train_df['longitude'].values,train_df['latitude'].values)m.hexbin(x,y,gridsize=200,bins='log',cmap=cm.YlOrRd_r) 基本和纽约市的城市热图相匹配。 2.5 时间型2.5.1 发布时间（Created）先看一下不同时间的分布状况。 12345678910train_df['created']=pd.to_datetime(train_df['created'])train_df['date_created']=train_df['created'].dt.datecnt_srs = train_df['date_created'].value_counts()plt.figure(figsize=(14,7))ax = plt.subplot(111)ax.bar(cnt_srs.index,cnt_srs.values,alpha=0.8)ax.xaxis_date()plt.xticks(rotation='vertical')plt.show() 从图中观察到发布时间是从2016年的4月至6月，当然这是训练集的情况，对应的，再看看测试集的发布时间状况。 12345678910test_df['created']=pd.to_datetime(test_df['created'])test_df['date_created']=test_df['created'].dt.datecnt_srs = test_df['date_created'].value_counts()plt.figure(figsize=(12,6))ax = plt.subplot(111)ax.bar(cnt_srs.index,cnt_srs.values,alpha=0.8)ax.xaxis_date()plt.xticks(rotation='vertical')plt.show() 可知，测试集的时间分布和训练集类似。 再看看不同时刻的样本分布情况： 1234567train_df['hour_created'] = train_df['created'].dt.hourcnt_srs = train_df['hour_created'].value_counts()plt.figure(figsize=(14,7))sns.barplot(cnt_srs.index,cnt_srs.values,alpha=0.8,color=color[4])plt.xticks(rotation='vertical')plt.show() 看起来像是一天中比较早的几个小时创建的比较多。可能是那个时候流量不拥挤，数据就更新了。 2.6 其他类型特征2.6.1 展示地址（Display Address）1234cnt_srs = train_df.groupby('display_address')['display_address'].count()for i in [2,10,50,100,500]: print "Display_address that appear less than &#123;&#125; \ times:&#123;&#125;%".format(i,round((cnt_srs&lt;i).mean()*100,2)) 上述代码中（cnt_srs&lt;i）返回的是布尔值True | False。再求一个得到的结果为：Display_address that appear less than 2 times:63.22%Display_address that appear less than 10 times:89.6%Display_address that appear less than 50 times:97.73%Display_address that appear less than 100 times:99.26%Display_address that appear less than 500 times:100.0% 绘制展示地址频次分布直方图： 12345plt.figure(figsize=(12,6))plt.hist(cnt_srs.values,bins=150,log=True,alpha=0.9)plt.xlabel('Number of times display_adress appeared',fontsize=12)plt.ylabel('log(Count)',fontsize=12)plt.show() 大部分的展览地址出现次数在给定的数据集中少于100次。没有超过500次的。 再看看展示地址的词云图： 12345678# wordcloud for display addressplt.figure(figsize=(12,6))wordcloud = WordCloud(background_color='white', width=600, height=300, max_font_size=50, max_words=40).generate(text_da)wordcloud.recolor(random_state=0)plt.imshow(wordcloud)plt.title("Wordcloud for Display Address", fontsize=30)plt.axis("off")plt.show() 2.6.2 照片数量（Photos）这个比赛也有巨大的照片数据。让我们先看看照片的数量: 12345678train_df["num_photos"] = train_df["photos"].apply(len)cnt_srs = train_df['num_photos'].value_counts()plt.figure(figsize=(14,7))sns.barplot(x=cnt_srs.index,y=cnt_srs.values,alpha=0.8)plt.xlabel("number of photos",fontsize=14)plt.ylabel('number of occurrences',fontsize=14)plt.show() 大多数样例的照片数量集中在3~8张。 再来看看不同兴趣程度下的照片数量分布： 123456train_df['num_photos'].loc[train_df['num_photos']&gt;12]=12plt.figure(figsize=(14,7))sns.violinplot(x='num_photos',y='interest_level',data=train_df,order=['low','medium','high'])plt.xlabel('Number of photos',fontsize=12)plt.ylabel("Interest Level",fontsize=12)plt.show() 2.6.3 描述特征的数量（features）每一个房源都对应一个features列，它描述了该样例的特征，比如位于市中心呀、能养猫呀、可以肆意遛狗，类似于这种亲民的特点。有的时候，这种利民条件越多，或许会提高消费者的兴趣，当然也不一定，可以先来看看特征数量的分布：12345678train_df['num_features'] = train_df['features'].apply(len)cnt_srs = train_df['num_features'].value_counts()plt.figure(figsize=(14,7))sns.barplot(x=cnt_srs.index,y=cnt_srs.values,alpha=0.8)plt.ylabel('Number of Occurrences',fontsize=12)plt.xlabel('Number of features',fontsize=12)plt.show() 再看看不同兴趣程度下的描述特征数量分布： 123456789#避免极端情况train_df['num_features'].loc[train_df['num_features']&gt;17]=17plt.figure(figsize=(14,7))sns.violinplot(y='num_features',x='interest_level',\ data=train_df,order=['low','medium','high'])plt.xlabel('Interest Level',fontsize=12)plt.ylabel('Number of features',fontsize=12)plt.show() 也可以看看描述特征的词云： 12345678910111213141516from wordcloud import WordCloudtext = ''text_da = ''for index,row in train_df.iterrows(): for feature in row['features']: text = ' '.join([text,"_".join(feature.strip().split(" "))]) text_da = " ".join([text_da,"_".join(row['display_address'].strip().split(" "))])text = text.strip()text_da = text_da.strip()plt.figure(figsize=(14,7))wordcloud = WordCloud(background_color='white',width=600, height=300,max_font_size=50,max_words=40).generate(text)wordcloud.recolor(random_state=0)plt.imshow(wordcloud)plt.title("Wordcloud for features",fontsize=30)plt.axis("off")plt.show() 以上这些探索性分析只是对原始数据初步的认识与了解，完了就可以建立一个base model。随着之后的特征工程对其进行更深层次的探索挖掘，不断迭代，使得我们的模型的预测效果越来越好。下一篇就开始着手建立一些base model。]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>EDA</tag>
        <tag>特征工程</tag>
        <tag>kaggle</tag>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析系列（3）：数据倾斜]]></title>
    <url>%2F2017%2F06%2F11%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%2F</url>
    <content type="text"><![CDATA[数据倾斜是大数据领域绕不开的拦路虎，当你所需处理的数据量到达了上亿甚至是千亿条的时候，数据倾斜将是横在你面前一道巨大的坎。很可能有几周甚至几月都要头疼于数据倾斜导致的各类诡异的问题。 数据倾斜是指：mapreduce程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完。Hive的执行是分阶段的，map处理数据量的差异取决于上一个stage的reduce输出，所以如何将数据均匀的分配到各个reduce中，就是解决数据倾斜的根本所在。 以下是一些常见的数据倾斜情形： 一、Group by 倾斜group by造成的倾斜相对来说比较容易解决。hive提供两个参数可以解决： 1.1 hive.map.aggr一个是hive.map.aggr，默认值已经为true，他的意思是做map aggregation，也就是在mapper里面做聚合。这个方法不同于直接写mapreduce的时候可以实现的combiner，但是却实现了类似combiner的效果。事实上各种基于mr的框架如pig，cascading等等用的都是map aggregation（或者叫partial aggregation）而非combiner的策略，也就是在mapper里面直接做聚合操作而不是输出到buffer给combiner做聚合。对于map aggregation，hive还会做检查，如果aggregation的效果不好，那么hive会自动放弃map aggregation。判断效果的依据就是经过一小批数据的处理之后，检查聚合后的数据量是否减小到一定的比例，默认是0.5，由hive.map.aggr.hash.min.reduction这个参数控制。所以如果确认数据里面确实有个别取值倾斜，但是大部分值是比较稀疏的，这个时候可以把比例强制设为1，避免极端情况下map aggr失效。hive.map.aggr还有一些相关参数，比如map aggr的内存占用等，具体可以参考这篇文章。 1.2 hive.groupby.skewindata另一个参数是hive.groupby.skewindata。这个参数的意思是做reduce操作的时候，拿到的key并不是所有相同值给同一个reduce，而是随机分发，然后reduce做聚合，做完之后再做一轮MR，拿前面聚合过的数据再算结果。所以这个参数其实跟hive.map.aggr做的是类似的事情，只是拿到reduce端来做，而且要额外启动一轮job，所以其实不怎么推荐用，效果不明显。 1.3 count distinct 改写另外需要注意的是count distinct操作往往需要改写SQL，可以按照下面这么做： 12345/*改写前*/select a, count(distinct b) as c from tbl group by a;/*改写后*/select a, count(*) as c from (select a, b from tbl group by a, b) group by a; 二、Join倾斜2.1 skew joinjoin造成的倾斜，常见情况是不能做map join的两个表(能做map join的话基本上可以避免倾斜)，其中一个是行为表，另一个应该是属性表。比如我们有三个表，一个用户属性表users，一个商品属性表items，还有一个用户对商品的操作行为表日志表logs。假设现在需要将行为表关联用户表： 1select * from logs a join users b on a.user_id = b.user_id; 其中logs表里面会有一个特殊用户user_id = 0，代表未登录用户，假如这种用户占了相当的比例，那么个别reduce会收到比其他reduce多得多的数据，因为它要接收所有user_id = 0的记录进行处理，使得其处理效果会非常差，其他reduce都跑完很久了它还在运行。 hive给出的解决方案叫skew join，其原理把这种user_id = 0的特殊值先不在reduce端计算掉，而是先写入hdfs，然后启动一轮map join专门做这个特殊值的计算，期望能提高计算这部分值的处理速度。当然你要告诉hive这个join是个skew join，即：set 1hive.optimize.skewjoin = true; 还有要告诉hive如何判断特殊值，根据hive.skewjoin.key设置的数量hive可以知道，比如默认值是100000，那么超过100000条记录的值就是特殊值。总结起来，skew join的流程可以用下图描述： 2.2 特殊值分开处理法不过，上述方法还要去考虑阈值之类的情况，其实也不够通用。所以针对join倾斜的问题，一般都是通过改写sql解决。对于上面这个问题，我们已经知道user_id = 0是一个特殊key，那么可以把特殊值隔离开来单独做join，这样特殊值肯定会转化成map join，非特殊值就是没有倾斜的普通join了： 12345678select *from (select * from logs where user_id = 0) a join (select * from users where user_id = 0) b on a.user_id = b.user_idunion allselect * from logs a join users bon a.user_id &lt;&gt; 0 and a.user_id = b.user_id; 2.3 随机数分配法上面这种个别key倾斜的情况只是一种倾斜情况。最常见的倾斜是因为数据分布本身就具有长尾性质，比如我们将日志表和商品表关联： 1select * from logs a join items b on a.item_id = b.item_id; 这个时候，分配到热门商品的reducer就会很慢，因为热门商品的行为日志肯定是最多的，而且我们也很难像上面处理特殊user那样去处理item。这个时候就会用到加随机数的方法，也就是在join的时候增加一个随机数，随机数的取值范围n相当于将item给分散到n个reducer： 12345select a.*, b.*from (select *, cast(rand() * 10 as int) as r_id from logs)ajoin (select *, r_id from items lateral view explode(range_list(1,10)) rl as r_id)bon a.item_id = b.item_id and a.r_id = b.r_id 上面的写法里，对行为表的每条记录生成一个1-10的随机整数，对于item属性表，每个item生成10条记录，随机key分别也是1-10，这样就能保证行为表关联上属性表。其中range_list(1,10)代表用udf实现的一个返回1-10整数序列的方法。这个做法是一个解决join倾斜比较根本性的通用思路，就是如何用随机数将key进行分散。当然，可以根据具体的业务场景做实现上的简化或变化。 2.4 业务设计除了上面两类情况，还有一类情况是因为业务设计导致的问题，也就是说即使行为日志里面join key的数据分布本身并不明显倾斜，但是业务设计导致其倾斜。比如对于商品item_id的编码，除了本身的id序列，还人为的把item的类型也作为编码放在最后两位，这样如果类型1（电子产品）的编码是00，类型2（家居产品）的编码是01，并且类型1是主要商品类，将会造成以00为结尾的商品整体倾斜。这时，如果reduce的数量恰好是100的整数倍，会造成partitioner把00结尾的item_id都hash到同一个reducer，引爆问题。这种特殊情况可以简单的设置合适的reduce值来解决，但是这种坑对于不了解业务的情况下就会比较隐蔽。 三、典型的业务场景3.1 空值产生的数据倾斜场景：如日志中，常会有信息丢失的问题，比如日志中的 user_id，如果取其中的 user_id 和 用户表中的user_id 关联，会碰到数据倾斜的问题。 解决方法1： user_id为空的不参与关联（红色字体为修改后） 1234567select * from log a join users b on a.user_id is not null and a.user_id = b.user_idunion allselect * from log a where a.user_id is null; 解决方法2 ：赋与空值分新的key值 1234select * from log a left outer join users b on case when a.user_id is null then concat(‘hive’,rand() ) else a.user_id end = b.user_id; 结论：方法2比方法1效率更好，不但io少了，而且作业数也少了。解决方法1中 log读取两次，jobs是2。解决方法2 job数是1 。这个优化适合无效 id (比如 -99 , ’’, null 等) 产生的倾斜问题。把空值的 key 变成一个字符串加上随机数，就能把倾斜的数据分到不同的reduce上 ,解决数据倾斜问题。 3.2 不同数据类型关联产生数据倾斜场景：用户表中user_id字段为int，log表中user_id字段既有string类型也有int类型。当按照user_id进行两个表的Join操作时，默认的Hash操作会按int型的id来进行分配，这样会导致所有string类型id的记录都分配到一个Reducer中。 解决方法：把数字类型转换成字符串类型 123select * from users a left outer join logs b on a.usr_id = cast(b.user_id as string) 3.3 小表不小不大，怎么用 map join 解决倾斜问题使用 map join 解决小表(记录数少)关联大表的数据倾斜问题，这个方法使用的频率非常高，但如果小表很大，大到map join会出现bug或异常，这时就需要特别的处理。 以下例子: 123select * from log a left outer join users b on a.user_id = b.user_id; users 表有 600w+ 的记录，把 users 分发到所有的 map 上也是个不小的开销，而且 map join 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。 12345678select /*+mapjoin(x)*/* from log a left outer join ( select /*+mapjoin(c)*/d.* from ( select distinct user_id from log ) c join users d on c.user_id = d.user_id ) x on a.user_id = b.user_id; 假如，log里user_id有上百万个，这就又回到原来map join问题。所幸，每日的会员uv不会太多，有交易的会员不会太多，有点击的会员不会太多，有佣金的会员不会太多等等。所以这个方法能解决很多场景下的数据倾斜问题。 四、总结使map的输出数据更均匀的分布到reduce中去，是我们的最终目标。由于Hash算法的局限性，按key Hash会或多或少的造成数据倾斜。大量经验表明数据倾斜的原因是人为的建表疏忽或业务逻辑可以规避的。在此给出较为通用的步骤： 1）采样log表，哪些user_id比较倾斜，得到一个结果表tmp1。由于对计算框架来说，所有的数据过来，他都是不知道数据分布情况的，所以采样是并不可少的。 2）数据的分布符合社会学统计规则，贫富不均。倾斜的key不会太多，就像一个社会的富人不多，奇特的人不多一样。所以tmp1记录数会很少。把tmp1和users做map join生成tmp2,把tmp2读到distribute file cache。这是一个map过程。 3）map读入users和log，假如记录来自log,则检查user_id是否在tmp2里，如果是，输出到本地文件a,否则生成的key,value对，假如记录来自member,生成的key,value对，进入reduce阶段。 4）最终把a文件，把Stage3 reduce阶段输出的文件合并起写到hdfs。 如果确认业务需要这样倾斜的逻辑，考虑以下的优化方案： 1）对于join，在判断小表不大于1G的情况下，使用map join 2）对于group by或distinct，设定 hive.groupby.skewindata=true 3）尽量使用上述的SQL语句调节进行优化]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据倾斜</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析系列（2）：卡方检验]]></title>
    <url>%2F2017%2F06%2F10%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9A%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[$\chi^2$检验（chi-square test）或称卡方检验，是一种用途较广的假设检验方法，，主要是比较两个及两个以上样本率( 构成比）以及两个分类变量的关联性分析。其根本思想就是在于比较理论频数和实际频数的吻合程度或拟合优度问题。 它的发明者卡尔·皮尔逊是一位历史上罕见的百科全书式的学者，研究领域涵盖了生物、历史、宗教、哲学、法律。在文本分类中可以用卡方值做特征选择（降维），也可以用卡方检验做异常用户的检测。 一、四格表资料的卡方检验两组大白鼠在不同致癌剂作用下的发癌率如下表，问两组发癌率有无差别？ 这四格资料表就专称四格表（fourfold table），或称2行2列表（2×2 contingency table）。从该资料算出的两组发癌率分别为73.24%和92.86%，两者的差别可能是抽样误差所致，亦可能是两组发癌率（总体率）确有所不同。这里可通过卡方检验来区别其差异有无统计学意义，检验的基本公式为： \chi^2=\sum \frac{(A-T)^2}{T}式中A为实际数，以上四格表的四个数据就是实际数。T为理论数，是根据检验假设推断出来的；即假设这两组的发癌率本无不同，差别仅是由抽样误差所致。这里可将两组合计发癌率作为理论上的发癌率，即91/113=80.3%，以此为依据便可推算出四格表中相应的四格的理论数。以表1资料为例检验如下。 检验步骤： 1）建立检验假设：$H_0:\ n_1=n_2 \ H_1 : n_1 ≠n_2;$ 2）计算理论数（TRC）,计算公式为：TRC = \frac{n_r\times n_c}{n}式中$TRC$是表示第R行C列格子的理论数，$n_r$是与理论数同行的合计数，$n_c$是与理论数同列的合计数，$n$为总例数。 第1行1列： 71×91/113=57.18第1行2列： 71×22/113=13.82第2行1列： 42×91/113=33.82第2行2列： 42×22/113=8.18 以推算结果，可与原四项实际数并列成下表：因为上表每行和每列合计数都是固定的，所以只要用TRC式求得其中一项理论数（例如T1.1=57.18），则其余三项理论数都可用同行或同列合计数相减，直接求出。 3）计算卡方值按公式代入\chi^2 = \frac{(52-57.18)^2}{57.18}···+ \frac{(3-8.18)^2}{8.18}=6.48 4）查卡方值表求$P$值 在查表之前应知本题自由度。按卡方检验的自由度v=（行数-1）（列数-1），则该题的自由度v=（2-1）（2-1）=1，查卡方界值表，找到$\chi^2_{0.05}(1)=3.85$，而本题卡方=6.48即卡方＞$\chi^2_{0.05}(1)$，P＜0.05，差异有显著统计学意义，按α=0.05水准，拒绝H0，可以认为两组发癌率有差别。 通过实例计算，读者对卡方的基本公式有如下理解：若各理论数与相应实际数相差越小，卡方值越小；如两者相同，则卡方值必为零，而卡方永远为正值。又因为每一对理论数和实际数都加入卡方值中，分组越多，即格子数越多，卡方值也会越大，因而每考虑卡方值大小的意义时同时要考虑到格子数。因此自由度大时，卡方的界值也相应增大。 二、四格表卡方值的校正卡方值表是数理统计根据正态分布中$\chi^2 = \sum (\frac{x_i-\mu }{\sigma})^2$的定义计算出来的。是一种近似。在自由度大于1、理论数皆大于5时，这种近似很好；当自由度为1时，尤其当1＜T＜5，而n＞40时，应用以下校正公式： \chi^2 = \frac{\sum{(|A-T|-0.5)^2}}{T}例2.某医师用甲、乙两疗法治疗小儿单纯性消化不良，结果小表试比较两种疗法效果有无差异？ 从表可见，T1.2和T2.2数值都＜5，且总例数大于40，故宜用校正公式检验。步骤如下： 1）检验假设：$H_0：π1=π2；H_1：π1≠π2；α=0.05$ 2）计算理论数：（已完成列入四格表括弧中） 3）计算卡方值：应用校正公式运算如下： \chi^2 = \frac{\sum{(|A-T|-0.5)^2}}{T}=2.746查卡方界值表$\chi^2_{0.05}(1)=3.84$，，故卡方＜$\chi^2_{0.05}(1)$，P＞0.05。按α=0.05水准，接受H0，两种疗效差异无统计学意义。 如果不采用校正公式，而用原基本公式，算得的结果卡方=4.068，则结论就不同了。 如果观察资料的T＜1或n＜40时，四格表资料用上述校正法也不行，可参考预防医学专业用的医学统计学教材中的Fisher精确检验法直接计算概率以作判断。 三、行列表的卡方检验适用于两个组以上的率或百分比差别的显著性检验。其检验步骤与上述相同，简单计算公式如下： \chi^2 = n(\sum \frac{A^2}{n_rn_c}-1 )式中n为总例数；A为各观察值；$n_r$和$n_c$为与各A值相应的行和列合计的总数。 例3.北方冬季日照短而南移，居宅设计如何适应以获得最大日照量，增强居民体质，减少小儿佝偻病，实属重要。胡氏等1986年在北京进行住宅建筑日照卫生标准的研究，对214幢楼房居民的婴幼儿712人体检，检出轻度佝偻病333例，比较了居室朝向与患病的关系。现将该资料归纳如表4作行列检验。 该表资料由2行4列组成，称2×4表，可用行×列卡方公式检验。 1）检验假设：H0：四类朝向居民婴幼儿佝偻病患病率相同；H1：四类朝向居民婴幼儿佝偻病患率不同；α=0.05 2）计算卡方值：\chi^2 = 712(\frac{180^2}{379\times 380 }+···+\frac{33^2}{333\times 98}-1)=15.079 3）确定P值和分析：本题v=（2-1)（4-3）=3，据此查卡方界值表：$\chi^2_{0.05}(3)=7.81$，本题卡方=15.08，卡方＞ $\chi^2_{0.05}(3)$，P＜0.05，拒绝$H_0$，可以认为居室朝向不同的居民，婴幼儿佝偻病患病率有差异。 一般认为行列表中不宜有1/5以上格子的理论数小于5，或有小于1的理论数。当理论数太小可采取下列方法处理：①增加样本含量以增大理论数；②删去上述理论数太小的行和列；③将太小理论数所在行或列与性质相近的邻行邻列中的实际数合并，使重新计算的理论数增大。由于后两法可能会损失信息，损害样本的随机性，不同的合并方式有可能影响推断结论，故不宜作常规方法。另外，不能把不同性质的实际数合并，如研究血型时，不能把不同的血型资料合并。 如检验结果拒绝检验假设，只能认为各总体率或总体构成比之间总的来说有差别，但不能说明它们彼此之间都有差别，或某两者间有差别。 四、应用场景卡方检验的一个典型应用场景是衡量特定条件下的分布是否与理论分布一致，比如：特定用户某项指标的分布与大盘的分布是否差异很大，这时通过临界概率可以合理又科学的筛选异常用户。 另外，x2值描述了自变量与因变量之间的相关程度：x2值越大，相关程度也越大，所以很自然的可以利用x2值来做降维，保留相关程度大的变量。再回到刚才新闻分类的场景，如果我们希望获取和娱乐类别相关性最强的100个词，以后就按照标题是否包含这100个词来确定新闻是否归属于娱乐类，怎么做？很简单，对娱乐类新闻标题所包含的每个词按上述步骤计算x2值，然后按x2值排序，取x2值最大的100个词。]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>卡方检验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析系列（1）：SQL查询执行顺序]]></title>
    <url>%2F2017%2F06%2F09%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9ASQL%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[SQL语句有一个让大部分人都感到困惑的特性，就是：SQL语句的执行顺序跟其语句的语法顺序并不一致。SQL语句的执行顺序是： FROM ON JOIN WHERE GROUP BY HAVING SELECT DISTINCT UNION ORDER BY LIMIT 一、准备工作这里的测试操作都是在MySQL数据库上完成的。 1.1 新建数据库首先我们新建一个测试数据库TestDB 1create database TestDB; 1.2 创建测试集table1和table212345678910111213CREATE TABLE table1 ( customer_id VARCHAR(10) NOT NULL, city VARCHAR(10) NOT NULL, PRIMARY KEY(customer_id) )ENGINE=INNODB DEFAULT CHARSET=UTF8; CREATE TABLE table2 ( order_id INT NOT NULL auto_increment, customer_id VARCHAR(10), PRIMARY KEY(order_id) )ENGINE=INNODB DEFAULT CHARSET=UTF8; 1.3 插入测试数据123456789101112INSERT INTO table1(customer_id,city) VALUES(&apos;163&apos;,&apos;hangzhou&apos;); INSERT INTO table1(customer_id,city) VALUES(&apos;9you&apos;,&apos;shanghai&apos;); INSERT INTO table1(customer_id,city) VALUES(&apos;tx&apos;,&apos;hangzhou&apos;); INSERT INTO table1(customer_id,city) VALUES(&apos;baidu&apos;,&apos;hangzhou&apos;); INSERT INTO table2(customer_id) VALUES(&apos;163&apos;); INSERT INTO table2(customer_id) VALUES(&apos;163&apos;); INSERT INTO table2(customer_id) VALUES(&apos;9you&apos;); INSERT INTO table2(customer_id) VALUES(&apos;9you&apos;); INSERT INTO table2(customer_id) VALUES(&apos;9you&apos;); INSERT INTO table2(customer_id) VALUES(&apos;tx&apos;); INSERT INTO table2(customer_id) VALUES(NULL); 准备工作做完以后，table1和table2看起来应该像下面这样： 123456789101112131415161718192021222324mysql&gt; select * from table1;+-------------+----------+| customer_id | city |+-------------+----------+| 163 | hangzhou || 9you | shanghai || baidu | hangzhou || tx | hangzhou |+-------------+----------+4 rows in set (0.00 sec)mysql&gt; select * from table2;+----------+-------------+| order_id | customer_id |+----------+-------------+| 1 | 163 || 2 | 163 || 3 | 9you || 4 | 9you || 5 | 9you || 6 | tx || 7 | NULL |+----------+-------------+7 rows in set (0.00 sec) 1.4 准备SQL查询语句12345678SELECT a.customer_id, COUNT(b.order_id) as total_orders FROM table1 AS a LEFT JOIN table2 AS b ON a.customer_id = b.customer_id WHERE a.city = &apos;hangzhou&apos; GROUP BY a.customer_id HAVING count(b.order_id) &lt; 2 ORDER BY total_orders DESC; 这些测试表和测试数据均来自《MySQL技术内幕：SQL编程》，接下来根据这个语句来详细地讲述SQL逻辑查询语句的执行顺序。 二、SQL查询语句执行顺序现在，我们先给出一个查询语句的执行顺序： 12345678910(7) SELECT (8) DISTINCT &lt;select_list&gt;(1) FROM &lt;left_table&gt;(3) &lt;join_type&gt; JOIN &lt;right_table&gt;(2) ON &lt;join_condition&gt;(4) WHERE &lt;where_condition&gt;(5) GROUP BY &lt;group_by_list&gt;(6) HAVING &lt;having_condition&gt;(9) ORDER BY &lt;order_by_condition&gt;(10) LIMIT &lt;limit_number&gt; 在这些SQL语句的执行过程中，都会产生一个虚拟表，用来保存SQL语句的执行结果（这是重点），现在就追踪这个虚拟表的变化，得到最终的查询结果的过程，来分析整个SQL逻辑查询的执行顺序和过程。 2.1 执行FROM语句第一步，执行FROM语句。我们首先需要知道最开始从哪个表开始的，这就是FROM告诉我们的。现在有了left table和right table两个表，我们到底从哪个表开始，还是会从两个表进行某种联系以后再开始呢？它们之间如何产生联系呢？——笛卡尔积，笛卡尔积是所有可能的有序对组成的集合，其中有序对的第一个对象是X的成员，第二个对象是Y的成员。 经过FROM语句对两个表执行笛卡尔积，会得到一个虚拟表，暂且叫VT1(vitual table 1)，内容如下： 1234567891011121314151617181920212223242526272829303132+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 9you | shanghai | 1 | 163 || baidu | hangzhou | 1 | 163 || tx | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 || 9you | shanghai | 2 | 163 || baidu | hangzhou | 2 | 163 || tx | hangzhou | 2 | 163 || 163 | hangzhou | 3 | 9you || 9you | shanghai | 3 | 9you || baidu | hangzhou | 3 | 9you || tx | hangzhou | 3 | 9you || 163 | hangzhou | 4 | 9you || 9you | shanghai | 4 | 9you || baidu | hangzhou | 4 | 9you || tx | hangzhou | 4 | 9you || 163 | hangzhou | 5 | 9you || 9you | shanghai | 5 | 9you || baidu | hangzhou | 5 | 9you || tx | hangzhou | 5 | 9you || 163 | hangzhou | 6 | tx || 9you | shanghai | 6 | tx || baidu | hangzhou | 6 | tx || tx | hangzhou | 6 | tx || 163 | hangzhou | 7 | NULL || 9you | shanghai | 7 | NULL || baidu | hangzhou | 7 | NULL || tx | hangzhou | 7 | NULL |+-------------+----------+----------+-------------+ 总共有28（table1的记录数*table2的记录总数）条记录。这就是VT1的结果，接下来的操作就在VT!的基础上进行。 2.2 执行ON过滤执行完笛卡尔积以后，接着就进行ON a.customer_id = b.customer_id条件过滤，根据ON中指定的条件，去掉那些不符合条件的数据，得到VT2表，内容如下： 12345678910+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 || 9you | shanghai | 3 | 9you || 9you | shanghai | 4 | 9you || 9you | shanghai | 5 | 9you || tx | hangzhou | 6 | tx |+-------------+----------+----------+-------------+ VT2就是经过ON条件筛选以后得到的有用数据，而接下来的操作将在VT2的基础上继续进行。 2.3 添加外部行这一步只有在连接类型为OUTER JOIN时才发生，如LEFT OUTER JOIN、RIGHT OUTER JOIN和FULL OUTER JOIN。在大多数的时候，我们都是会省略掉OUTER关键字的，但OUTER表示的就是外部行的概念。 LEFT OUTER JOIN把左表记为保留表，得到的结果为： 1234567891011+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 || 9you | shanghai | 3 | 9you || 9you | shanghai | 4 | 9you || 9you | shanghai | 5 | 9you || tx | hangzhou | 6 | tx || baidu | hangzhou | NULL | NULL |+-------------+----------+----------+-------------+ RIGHT OUTER JOIN把右表记为保留表，得到的结果为： 1234567891011+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 || 9you | shanghai | 3 | 9you || 9you | shanghai | 4 | 9you || 9you | shanghai | 5 | 9you || tx | hangzhou | 6 | tx || NULL | NULL | 7 | NULL |+-------------+----------+----------+-------------+ FULL OUTER JOIN把左右表都作为保留表，得到的结果为： 123456789101112+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 || 9you | shanghai | 3 | 9you || 9you | shanghai | 4 | 9you || 9you | shanghai | 5 | 9you || tx | hangzhou | 6 | tx || baidu | hangzhou | NULL | NULL || NULL | NULL | 7 | NULL |+-------------+----------+----------+-------------+ 添加外部行的工作就是在VT2表的基础上添加保留表中被过滤条件过滤掉的数据，非保留表中的数据被赋予了NULL值，最后生成虚拟表VT3。 由于在准备的测试SQL查询逻辑语句中使用的是LEFT JOIN，过滤掉了以下这条数据： 1| baidu | hangzhou | 现在就把这条数据添加到VT2表中，得到的VT3表如下： 1234567891011+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 || 9you | shanghai | 3 | 9you || 9you | shanghai | 4 | 9you || 9you | shanghai | 5 | 9you || tx | hangzhou | 6 | tx || baidu | hangzhou | NULL | NULL |+-------------+----------+----------+-------------+ 接下里的操作都会在该VT3表上进行。 2.4 执行WHERE过滤对添加外部行得到的VT3进行WHERE过滤，只有符合的记录才会输出到虚拟表VT4中。当我们执行WHERE a.city = &#39;hangzhou&#39;的时候，就会得到以下内容，并存在虚拟表VT4中： 12345678+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 || tx | hangzhou | 6 | tx || baidu | hangzhou | NULL | NULL |+-------------+----------+----------+-------------+ 但是在使用WHERE字句时，需要注意以下两点： 由于数据还没有分组，因此还不能在WHERE过滤器中使用where_condition =MIN(col)这类分组统计的过滤； 由于还没有进行列的选取操作，因此在WHERE中使用列的别名也是不被允许的，如：SELECT city AS c FROM t WHERE c=&#39;shanghai&#39;;是不允许出现的。 2.5 执行GROUP BY分组GROUP BY子句主要是对使用WHERE子句得到的虚拟表进行分组操作。即“根据(by)一定的规则进行分组(Group)”。它的作用是通过一定的规则将一个数据集划分成若干个小的区域，然后针对若干个小区域进行数据处理。我们执行测试语句中的GROUP BY a.customer_id，就是对VT4按照a.customer_id进行了分组，这里就得到了以下三个组别 1234567891011第一组+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 |第二组| tx | hangzhou | 6 | tx |第三组| baidu | hangzhou | NULL | NULL |+-------------+----------+----------+-------------+ 得到的内容会存入虚拟表VT5中，此时，我们就得到了一个VT5虚拟表，接下来的操作都会在该表上完成。 2.6 执行HAVING过滤HAVING字句主要和GROUP BY字句配合使用，对分组得到的VT5虚拟表进行条件过滤。当我执行测试语句中的HAVING COUNT(b.order_id)&lt;2时，将得到以下内容： 123456+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| baidu | hangzhou | NULL | NULL || tx | hangzhou | 6 | tx |+-------------+----------+----------+-------------+ 这就是虚拟表6 2.7 SELECT列表现在才会执行到SELECT子句，不要以为SELECT子句被写在第一行，就是第一个被执行的。 执行测试语句中的SELECT a.customer_id ,COUNT(b.oredr_id) as total_orders，我们从虚拟表VT6中选择我们需要的内容。我们将得到以下内容： 123456+-------------+--------------+| customer_id | total_orders |+-------------+--------------+| baidu | 0 || tx | 1 |+-------------+--------------+ 2.8 执行DISTINCT子句如果在查询中指定了DISTINCT子句，则会创建一张内存临时表（如果内存放不下，就需要存放在硬盘了）。这张临时表的表结构和上一步产生的虚拟表VT7是一样的，不同的是对进行DISTINCT操作的列增加了一个唯一索引，以此来去除重复数据。 由于测试SQL语句中并没有使用DISTINCT，所以，在该查询中，这一步不会生成一个虚拟表。 2.9 执行ORDER BY子句对虚拟表中的内容按照指定的列进行排序，然后返回一个新的虚拟表，我们执行测试SQL语句中的ORDER BY total_orders DESC，就会得到以下内容：123456+-------------+--------------+| customer_id | total_orders |+-------------+--------------+| tx | 1 || baidu | 0 |+-------------+--------------+ 可以看到这是对total_orders列进行降序排列。上述结果会存储在VT8中。 2.10执行LIMIT子句LIMIT子句从上一步得到的VT8虚拟表中选出从指定位置开始的指定行数据。对于没有营养ORDER BY的LIMIT子句，得到的结果同样是无序的，所以，很多时候，我们都会看到LIMIT子句会和ORDER BY子句一起使用。 MYSQL数据库的LIMIT支持如下形式的选择：1LIMIT n, m 表示从第n条记录开始选择m条记录。而很多开发人员喜欢使用该语句来解决分页问题。对于小数据，使用LIMIT子句没有任何问题，当数据量非常大的时候，使用LIMIT n, m是非常低效的。因为LIMIT的机制是每次都是从头开始扫描，如果需要从第60万行开始，读取3条数据，就需要先扫描定位到60万行，然后再进行读取，而扫描的过程是一个非常低效的过程。所以，对于大数据处理时，是非常有必要在应用层建立一定的缓存机制]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kaggle系列（1）：Kaggle 数据挖掘比赛经验分享]]></title>
    <url>%2F2017%2F06%2F05%2FKaggle%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9AKaggle%20%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[转载自知乎：Kaggle 数据挖掘比赛经验分享 作者是陈成龙，目前在腾讯社交与效果广告部任职数据挖掘工程师，负责 Lookalike 相似人群扩展相关工作。曾在 Kaggle 数据科学家排行榜排名全球第十，国内第一。 简介Kaggle 于 2010 年创立，专注于开展数据科学、机器学习相关的竞赛，是全球最大的数据科学社区和数据竞赛平台。笔者从 2013 年开始，陆续参加了多场 Kaggle上面举办的比赛，相继获得了 CrowdFlower 搜索相关性比赛第一名（1326支队伍）和 HomeDepot 商品搜索相关性比赛第三名（2125支队伍），曾在 Kaggle 数据科学家排行榜排名全球第十，国内第一。笔者目前在腾讯社交与效果广告部任职数据挖掘工程师，负责 Lookalike 相似人群扩展相关工作。此文分享笔者在参加数据挖掘比赛过程中的一点心得体会。 一、Kaggle基本介绍Kaggle 于 2010 年创立，专注于开展数据科学、机器学习相关的竞赛，是全球最大的数据科学社区和数据竞赛平台。在 Kaggle 上，企业或者研究机构发布商业和科研难题，悬赏吸引全球的数据科学家，通过众包的方式解决建模问题。而参赛者可以接触到丰富的真实数据，解决实际问题，角逐名次，赢取奖金。诸如 Google，Facebook，Microsoft 等知名科技公司均在 Kaggle 上面举办过数据挖掘比赛。2017年3月，Kaggle 被 Google CloudNext 收购。 1.1 参赛方式可以以个人或者组队的形式参加比赛。组队人数一般没有限制，但需要在 Merger Deadline 前完成组队。为了能参与到比赛中，需要在 Entry Deadline 前进行至少一次有效提交。最简单地，可以直接提交官方提供的 Sample Submission。关于组队，建议先单独个人进行数据探索和模型构建，以个人身份进行比赛，在比赛后期（譬如离比赛结束还有 2~3 周）再进行组队，以充分发挥组队的效果（类似于模型集成，模型差异性越大，越有可能有助于效果的提升，超越单模型的效果）。当然也可以一开始就组好队，方便分工协作，讨论问题和碰撞火花。 Kaggle 对比赛的公正性相当重视。在比赛中，每个人只允许使用一个账号进行提交。在比赛结束后 1~2 周内，Kaggle 会对使用多账号提交的 Cheater 进行剔除（一般会对 Top 100 的队伍进行 Cheater Detection）。在被剔除者的 Kaggle 个人页面上，该比赛的成绩也会被删除，相当于该选手从没参加过这个比赛。此外，队伍之间也不能私自分享代码或者数据，除非在论坛上面公开发布。 比赛一般只提交测试集的预测结果，无需提交代码。每人（或每个队伍）每天有提交次数的限制，一般为2次或者5次，在 Submission 页面会有提示。 1.2 比赛获奖Kaggle 比赛奖金丰厚，一般前三名均可以获得奖金。在最近落幕的第二届 National Data Science Bowl 中，总奖金池高达 100W 美刀，其中第一名可以获得 50W 美刀的奖励，即使是第十名也能收获 2.5W 美刀的奖金。获奖的队伍需要在比赛结束后 1~2 周内，准备好可执行的代码以及 README，算法说明文档等提交给 Kaggle 来进行获奖资格的审核。Kaggle 会邀请获奖队伍在 Kaggle Blog 中发表 Interview，来分享比赛故事和经验心得。对于某些比赛，Kaggle 或者主办方会邀请获奖队伍进行电话/视频会议，获奖队伍进行 Presentation，并与主办方团队进行交流。 1.3 比赛类型从 Kaggle 提供的官方分类来看，可以划分为以下类型（如下图1所示）： Featured：商业或科研难题，奖金一般较为丰厚； Recruitment：比赛的奖励为面试机会； Research：科研和学术性较强的比赛，也会有一定的奖金，一般需要较强的领域和专业知识； Playground：提供一些公开的数据集用于尝试模型和算法； Getting Started：提供一些简单的任务用于熟悉平台和比赛； In Class：用于课堂项目作业或者考试。 从领域归属划分：包含搜索相关性，广告点击率预估，销量预估，贷款违约判定，癌症检测等。 从任务目标划分：包含回归，分类（二分类，多分类，多标签），排序，混合体（分类+回归）等。 从数据载体划分：包含文本，语音，图像和时序序列等。 从特征形式划分：包含原始数据，明文特征，脱敏特征（特征的含义不清楚）等。 1.4 比赛流程一个数据挖掘比赛的基本流程如下图2所示，具体的模块我将在下一章进行展开陈述。 这里想特别强调的一点是，Kaggle 在计算得分的时候，有Public Leaderboard (LB)和 Private LB 之分。具体而言，参赛选手提交整个测试集的预测结果，Kaggle 使用测试集的一部分计算得分和排名，实时显示在 Public LB上，用于给选手提供及时的反馈和动态展示比赛的进行情况；测试集的剩余部分用于计算参赛选手的最终得分和排名，此即为 Private LB，在比赛结束后会揭晓。用于计算 Public LB 和 Private LB 的数据有不同的划分方式，具体视比赛和数据的类型而定，一般有随机划分，按时间划分或者按一定规则划分。 这个过程可以概括如下图3所示，其目的是避免模型过拟合，以得到泛化能力好的模型。如果不设置 Private LB（即所有的测试数据都用于计算 Public LB），选手不断地从 Public LB（即测试集）中获得反馈，进而调整或筛选模型。这种情况下，测试集实际上是作为验证集参与到模型的构建和调优中来。Public LB上面的效果并非是在真实未知数据上面的效果，不能可靠地反映模型的效果。划分 Public LB 和 Private LB 这样的设置，也在提醒参赛者，我们建模的目标是要获得一个在未知数据上表现良好的模型，而并非仅仅是在已知数据上效果好。 二、数据挖掘比赛流程从上面图2可以看到，做一个数据挖掘比赛，主要包含了数据分析，数据清洗，特征工程，模型训练和验证等四个大的模块，以下来一一对其进行介绍。 2.1 数据分析数据分析可能涉及以下方面： 分析特征变量的分布 特征变量为连续值：如果为长尾分布并且考虑使用线性模型，可以对变量进行幂变换或者对数变换。 特征变量为离散值：观察每个离散值的频率分布，对于频次较低的特征，可以考虑统一编码为“其他”类别。 分析目标变量的分布 目标变量为连续值：查看其值域范围是否较大，如果较大，可以考虑对其进行对数变换，并以变换后的值作为新的目标变量进行建模（在这种情况下，需要对预测结果进行逆变换）。一般情况下，可以对连续变量进行Box-Cox变换。通过变换可以使得模型更好的优化，通常也会带来效果上的提升。 目标变量为离散值：如果数据分布不平衡，考虑是否需要上采样/下采样；如果目标变量在某个ID上面分布不平衡，在划分本地训练集和验证集的时候，需要考虑分层采样（Stratified Sampling）。 分析变量之间两两的分布和相关度 可以用于发现高相关和共线性的特征。 通过对数据进行探索性分析（甚至有些情况下需要肉眼观察样本），还可以有助于启发数据清洗和特征抽取，譬如缺失值和异常值的处理，文本数据是否需要进行拼写纠正等。 2.2 数据清洗数据清洗是指对提供的原始数据进行一定的加工，使得其方便后续的特征抽取。其与特征抽取的界限有时也没有那么明确。常用的数据清洗一般包括： 数据的拼接 提供的数据散落在多个文件，需要根据相应的键值进行数据的拼接。 特征缺失值的处理 特征值为连续值：按不同的分布类型对缺失值进行补全：偏正态分布，使用均值代替，可以保持数据的均值；偏长尾分布，使用中值代替，避免受 outlier 的影响； 特征值为离散值：使用众数代替 文本数据的清洗 在比赛当中，如果数据包含文本，往往需要进行大量的数据清洗工作。如去除HTML 标签，分词，拼写纠正, 同义词替换，去除停词，抽词干，数字和单位格式统一等。 2.3 特征工程有一种说法是，特征决定了效果的上限，而不同模型只是以不同的方式或不同的程度来逼近这个上限。这样来看，好的特征输入对于模型的效果至关重要，正所谓”Garbage in, garbage out”。要做好特征工程，往往跟领域知识和对问题的理解程度有很大的关系，也跟一个人的经验相关。特征工程的做法也是Case by Case，以下就一些点，谈谈自己的一些看法。 2.3.1 特征变换主要针对一些长尾分布的特征，需要进行幂变换或者对数变换，使得模型（LR或者DNN）能更好的优化。需要注意的是，Random Forest 和 GBDT 等模型对单调的函数变换不敏感。其原因在于树模型在求解分裂点的时候，只考虑排序分位点。 2.3.2 特征编码对于离散的类别特征，往往需要进行必要的特征转换/编码才能将其作为特征输入到模型中。常用的编码方式有 LabelEncoder，OneHotEncoder（sklearn里面的接口）。譬如对于”性别”这个特征（取值为男性和女性），使用这两种方式可以分别编码为$\{0,1\}$和$\{[1,0], [0,1]\}$。 对于取值较多（如几十万）的类别特征（ID特征），直接进行OneHotEncoder编码会导致特征矩阵非常巨大，影响模型效果。可以使用如下的方式进行处理： 统计每个取值在样本中出现的频率，取 Top N 的取值进行 One-hot 编码，剩下的类别分到“其他“类目下，其中 N 需要根据模型效果进行调优； 统计每个 ID 特征的一些统计量（譬如历史平均点击率，历史平均浏览率）等代替该 ID 取值作为特征，具体可以参考 Avazu 点击率预估比赛第二名的获奖方案； 参考 word2vec 的方式，将每个类别特征的取值映射到一个连续的向量，对这个向量进行初始化，跟模型一起训练。训练结束后，可以同时得到每个ID的Embedding。具体的使用方式，可以参考 Rossmann 销量预估竞赛第三名的获奖方案(entron/entity-embedding-rossmann) 对于 Random Forest 和 GBDT 等模型，如果类别特征存在较多的取值，可以直接使用 LabelEncoder 后的结果作为特征。 2.4 模型训练与验证2.4.1 模型选择在处理好特征后，我们可以进行模型的训练和验证。 对于稀疏型特征（如文本特征，One-hot的ID类特征），我们一般使用线性模型，譬如 Linear Regression 或者 Logistic Regression。Random Forest 和 GBDT 等树模型不太适用于稀疏的特征，但可以先对特征进行降维（如PCA，SVD/LSA等），再使用这些特征。稀疏特征直接输入 DNN 会导致网络 weight 较多，不利于优化，也可以考虑先降维，或者对 ID 类特征使用 Embedding 的方式； 对于稠密型特征，推荐使用 XGBoost 进行建模，简单易用效果好； 数据中既有稀疏特征，又有稠密特征，可以考虑使用线性模型对稀疏特征进行建模，将其输出与稠密特征一起再输入 XGBoost/DNN 建模，具体可以参考2.5.2节 Stacking 部分。 2.4.2 调参和模型验证对于选定的特征和模型，我们往往还需要对模型进行超参数的调优，才能获得比较理想的效果。调参一般可以概括为以下三个步骤： 训练集和验证集的划分。根据比赛提供的训练集和测试集，模拟其划分方式对训练集进行划分为本地训练集和本地验证集。划分的方式视具体比赛和数据而定，常用的方式有： 随机划分：譬如随机采样 70% 作为训练集，剩余的 30% 作为测试集。在这种情况下，本地可以采用 KFold 或者 Stratified KFold 的方法来构造训练集和验证集。 按时间划分：一般对应于时序序列数据，譬如取前 7 天数据作为训练集，后 1 天数据作为测试集。这种情况下，划分本地训练集和验证集也需要按时间先后划分。常见的错误方式是随机划分，这种划分方式可能会导致模型效果被高估。 按某些规则划分：在 HomeDepot 搜索相关性比赛中，训练集和测试集中的 Query 集合并非完全重合，两者只有部分交集。而在另外一个相似的比赛中（CrowdFlower 搜索相关性比赛），训练集和测试集具有完全一致的 Query 集合。对于 HomeDepot 这个比赛中，训练集和验证集数据的划分，需要考虑 Query 集合并非完全重合这个情况，其中的一种方法可以参考第三名的获奖方案。 指定参数空间。在指定参数空间的时候，需要对模型参数以及其如何影响模型的效果有一定的了解，才能指定出合理的参数空间。譬如DNN或者XGBoost中学习率这个参数，一般就选 0.01 左右就 OK 了（太大可能会导致优化算法错过最优化点，太小导致优化收敛过慢）。再如 Random Forest，一般设定树的棵数范围为 100~200 就能有不错的效果，当然也有人固定数棵数为 500，然后只调整其他的超参数。 按照一定的方法进行参数搜索。常用的参数搜索方法有，Grid Search，Random Search以及一些自动化的方法（如 Hyperopt）。其中，Hyperopt 的方法，根据历史已经评估过的参数组合的效果，来推测本次评估使用哪个参数组合更有可能获得更好的效果。有关这些方法的介绍和对比，可以参考文献 [2]。 2.4.3 适当利用Public LB的反馈在2.4.2节中我们提到本地验证（Local Validation）结果，当将预测结果提交到 Kaggle 上时，我们还会接收到 Public LB 的反馈结果。如果这两个结果的变化趋势是一致的，如 Local Validation 有提升，Public LB 也有提升，我们可以借助 Local Validation 的变化来感知模型的演进情况，而无需靠大量的 Submission。如果两者的变化趋势不一致，需要考虑2.4.2节中提及的本地训练集和验证集的划分方式，是否跟训练集和测试集的划分方式一致。 另外，在以下一些情况下，往往 Public LB 反馈亦会提供有用信息，适当地使用这些反馈也许会给你带来优势。如图4所示，(a)和(b)表示数据与时间没有明显的关系（如图像分类），(c)和(d)表示数据随时间变化（如销量预估中的时序序列）。(a)和(b)的区别在于，训练集样本数相对于 Public LB 的量级大小，其中(a)中训练集样本数远超于 Public LB 的样本数，这种情况下基于训练集的 Local Validation 更可靠；而(b)中，训练集数目与 Public LB 相当，这种情况下，可以结合 Public LB 的反馈来指导模型的选择。一种融合的方式是根据 Local Validation 和 Public LB 的样本数目，按比例进行加权。譬如评估标准为正确率，Local Validation 的样本数为$N_l$，正确率为$A_l$；Public LB 的样本数为 $N_p$，正确率为 $A_p$。则可以使用融合后的指标：$（N_l A_l + N_p A_p）/(N_l + N_p)$，来进行模型的筛选。对于(c)和(d)，由于数据分布跟时间相关，很有必要使用 Public LB 的反馈来进行模型的选择，尤其对于(c)图所示的情况。 2.5 模型集成如果想在比赛中获得名次，几乎都要进行模型集成（组队也是一种模型集成）。关于模型集成的介绍，已经有比较好的博文了，可以参考 [3]。在这里，我简单介绍下常用的方法，以及个人的一些经验。 2.5.1 Averaging 和 Voting直接对多个模型的预测结果求平均或者投票。对于目标变量为连续值的任务，使用平均；对于目标变量为离散值的任务，使用投票的方式。 2.5.2 Stacking图5展示了使用 5-Fold 进行一次 Stacking 的过程（当然在其上可以再叠加 Stage 2, Stage 3 等）。其主要的步骤如下： 数据集划分。将训练数据按照5-Fold进行划分（如果数据跟时间有关，需要按时间划分，更一般的划分方式请参考3.4.2节，这里不再赘述）； 基础模型训练 I（如图5第一行左半部分所示）。按照交叉验证（Cross Validation）的方法，在训练集（Training Fold）上面训练模型（如图灰色部分所示），并在验证集（Validation Fold）上面做预测，得到预测结果（如图黄色部分所示）。最后综合得到整个训练集上面的预测结果（如图第一个黄色部分的CV Prediction所示）。 基础模型训练 II（如图5第二和三行左半部分所示）。在全量的训练集上训练模型（如图第二行灰色部分所示），并在测试集上面做预测，得到预测结果（如图第三行虚线后绿色部分所示）。 Stage 1 模型集成训练 I（如图5第一行右半部分所示）。将步骤 2 中得到的 CV Prediction 当作新的训练集，按照步骤 2 可以得到 Stage 1模型集成的 CV Prediction。 Stage 1 模型集成训练 II（如图5第二和三行右半部分所示）。将步骤 2 中得到的 CV Prediction 当作新的训练集和步骤 3 中得到的 Prediction 当作新的测试集，按照步骤 3 可以得到 Stage 1 模型集成的测试集 Prediction。此为 Stage 1 的输出，可以提交至 Kaggle 验证其效果。 在图5中，基础模型只展示了一个，而实际应用中，基础模型可以多种多样，如SVM，DNN，XGBoost 等。也可以相同的模型，不同的参数，或者不同的样本权重。重复4和5两个步骤，可以相继叠加 Stage 2, Stage 3 等模型。 2.5.3 BlendingBlending 与 Stacking 类似，但单独留出一部分数据（如 20%）用于训练 Stage X 模型。 2.5.4 Bagging Ensemble SelectionBagging Ensemble Selection [5] 是我在 CrowdFlower 搜索相关性比赛中使用的方法，其主要的优点在于可以以优化任意的指标来进行模型集成。这些指标可以是可导的（如 LogLoss 等）和不可导的（如正确率，AUC，Quadratic Weighted Kappa等）。它是一个前向贪婪算法，存在过拟合的可能性，作者在文献 [5] 中提出了一系列的方法（如 Bagging）来降低这种风险，稳定集成模型的性能。使用这个方法，需要有成百上千的基础模型。为此，在 CrowdFlower 的比赛中，我把在调参过程中所有的中间模型以及相应的预测结果保留下来，作为基础模型。这样做的好处是，不仅仅能够找到最优的单模型（Best Single Model），而且所有的中间模型还可以参与模型集成，进一步提升效果。 2.6 自动化框架从上面的介绍可以看到，做一个数据挖掘比赛涉及到的模块非常多，若有一个较自动化的框架会使得整个过程更加的高效。在 CrowdFlower 比赛较前期，我对整一个项目的代码架构进行了重构，抽象出来特征工程，模型调参和验证，以及模型集成等三大模块，极大的提高了尝试新特征，新模型的效率，也是我最终能斩获名次的一个有利因素。这份代码开源在 Github 上面，目前是 Github 有关 Kaggle 竞赛解决方案的 Most Stars，地址链接。 其主要包含以下部分： 模块化特征工程 接口统一，只需写少量的代码就能够生成新的特征； 自动将单独的特征拼接成特征矩阵。 自动化模型调参和验证 自定义训练集和验证集的划分方法； 使用 Grid Search / Hyperopt 等方法，对特定的模型在指定的参数空间进行调优，并记录最佳的模型参数以及相应的性能。 自动化模型集成 对于指定的基础模型，按照一定的方法（如Averaging/Stacking/Blending 等）生成集成模型。 三、Kaggle竞赛方案盘点3.1 图像分类到目前为止，Kaggle 平台上面已经举办了大大小小不同的赛事，覆盖图像分类，销量预估，搜索相关性，点击率预估等应用场景。在不少的比赛中，获胜者都会把自己的方案开源出来，并且非常乐于分享比赛经验和技巧心得。这些开源方案和经验分享对于广大的新手和老手来说，是入门和进阶非常好的参考资料。以下笔者结合自身的背景和兴趣，对不同场景的竞赛开源方案作一个简单的盘点，总结其常用的方法和工具，以期启发思路。 3.1.1 图像分类National Data Science Bowl 3.1.2 任务详情随着深度学习在视觉图像领域获得巨大成功，Kaggle 上面出现了越来越多跟视觉图像相关的比赛。这些比赛的发布吸引了众多参赛选手，探索基于深度学习的方法来解决垂直领域的图像问题。NDSB就是其中一个比较早期的图像分类相关的比赛。这个比赛的目标是利用提供的大量的海洋浮游生物的二值图像，通过构建模型，从而实现自动分类。 3.1.3 获奖方案1st place:Cyclic Pooling + Rolling Feature Maps + Unsupervised and Semi-Supervised Approaches。值得一提的是，这个队伍的主力队员也是Galaxy Zoo行星图像分类比赛的第一名，其也是Theano中基于FFT的Fast Conv的开发者。在两次比赛中，使用的都是 Theano，而且用的非常溜。方案链接：Classifying plankton with deep neural networks 2nd place：Deep CNN designing theory + VGG-like model + RReLU。这个队伍阵容也相当强大，有前MSRA 的研究员Xudong Cao，还有大神Tianqi Chen，Naiyan Wang，Bing XU等。Tianqi 等大神当时使用的是 CXXNet（MXNet 的前身），也在这个比赛中进行了推广。Tianqi 大神另外一个大名鼎鼎的作品就是 XGBoost，现在 Kaggle 上面几乎每场比赛的 Top 10 队伍都会使用。方案链接：National Data Science Bowl 17th place：Realtime data augmentation + BN + PReLU。方案链接：ChenglongChen/caffe-windows 3.1.4 常用工具 Theano: Welcome – Theano 0.9.0 documentation Keras: Keras Documentation Cuda-convnet2: akrizhevsky/cuda-convnet2 Caffe: Caffe | Deep Learning Framework CXXNET: dmlc/cxxnet MXNet: dmlc/mxnet 3.2 销量估计3.2.1 任务名称Walmart Recruiting – Store Sales Forecasting 3.2.2 任务详情Walmart 提供 2010-02-05 到 2012-11-01 期间的周销售记录作为训练数据，需要参赛选手建立模型预测 2012-11-02 到 2013-07-26 周销售量。比赛提供的特征数据包含：Store ID, Department ID, CPI，气温，汽油价格，失业率，是否节假日等。 3.2.3 获奖方案1st place：Time series forecasting method: stlf + arima + ets。主要是基于时序序列的统计方法，大量使用了 Rob J Hyndman 的 forecast R 包。方案链接：Walmart Recruiting – Store Sales Forecasting2nd place：Time series forecasting + ML: arima + RF + LR + PCR。时序序列的统计方法+传统机器学习方法的混合，方案链接：Walmart Recruiting – Store Sales Forecasting16th placeFeature engineering + GBM。方案链接：ChenglongChen/Kaggle_Walmart-Recruiting-Store-Sales-Forecasting 3.2.4 常用工具 R forecast package: https://cran.r-project.org/web/packages/forecast/index.html R GBM package: https://cran.r-project.org/web/packages/gbm/index.html 3.3 搜索相关性3.3.1 任务名称CrowdFlower Search Results Relevance 3.3.2 任务详情比赛要求选手利用约几万个 (query, title, description) 元组的数据作为训练样本，构建模型预测其相关性打分 {1, 2, 3, 4}。比赛提供了 query, title和description的原始文本数据。比赛使用 Quadratic Weighted Kappa 作为评估标准，使得该任务有别于常见的回归和分类任务。 3.3.3 获奖方案1st place：Data Cleaning + Feature Engineering + Base Model + Ensemble。对原始文本数据进行清洗后，提取了属性特征，距离特征和基于分组的统计特征等大量的特征，使用了不同的目标函数训练不同的模型（回归，分类，排序等），最后使用模型集成的方法对不同模型的预测结果进行融合。方案链接：ChenglongChen/Kaggle_CrowdFlower 3.3.4 常用工具 NLTK: Natural Language Toolkit Gensim: gensim: topic modelling for humans XGBoost: dmlc/xgboost RGF: baidu/fast_rgf 3.4 点击率预估I3.4.1 任务名称Criteo Display Advertising Challenge 3.4.2 任务详情经典的点击率预估比赛。该比赛中提供了7天的训练数据，1 天的测试数据。其中有13 个整数特征，26 个类别特征，均脱敏，因此无法知道具体特征含义。 3.4.3 获奖方案1st place：GBDT 特征编码 + FFM。台大的队伍，借鉴了Facebook的方案 [6]，使用 GBDT 对特征进行编码，然后将编码后的特征以及其他特征输入到 Field-aware Factorization Machine（FFM） 中进行建模。方案链接：Display Advertising Challenge | Kaggle 3rd place：Quadratic Feature Generation + FTRL。传统特征工程和 FTRL 线性模型的结合。方案链接：Display Advertising Challenge | Kaggle 4th place：Feature Engineering + Sparse DNN 3.4.4 常用工具 Vowpal Wabbit: JohnLangford/vowpal_wabbit XGBoost: dmlc/xgboost LIBFFM: LIBFFM: A Library for Field-aware Factorization Machines 3.5 点击率预估II3.5.1 任务名称Avazu Click-Through Rate Prediction 3.5.2 任务详情点击率预估比赛。提供了 10 天的训练数据，1 天的测试数据，并且提供时间，banner 位置，site, app, device 特征等，8个脱敏类别特征。 3.5.3 获奖方案1st place：Feature Engineering + FFM + Ensemble。还是台大的队伍，这次比赛，他们大量使用了 FFM，并只基于 FFM 进行集成。方案链接：Click-Through Rate Prediction | Kaggle 2nd place：Feature Engineering + GBDT 特征编码 + FFM + Blending。Owenzhang（曾经长时间雄霸 Kaggle 排行榜第一）的竞赛方案。Owenzhang 的特征工程做得非常有参考价值。方案链接：owenzhang/kaggle-avazu 3.5.4 常用工具 LIBFFM: LIBFFM: A Library for Field-aware Factorization Machines XGBoost: dmlc/xgboost 四、参考资料[1] Owenzhang 的分享： Tips for Data Science Competitions [2] Algorithms for Hyper-Parameter Optimization [3] MLWave博客：Kaggle Ensembling Guide [4] Jeong-Yoon Lee 的分享：Winning Data Science Competitions [5] Ensemble Selection from Libraries of Models [6] Practical Lessons from Predicting Clicks on Ads at Facebook 五、结语作为曾经的学生党，十分感激和庆幸有 Kaggle 这样的平台，提供了不同领域极具挑战的任务以及丰富多样的数据。让我这种空有满（yi）腔（xie）理（wai）论（li）的数据挖掘小白，可以在真实的问题场景和业务数据中进行实操练手，提升自己的数据挖掘技能，一不小心，还能拿名次，赢奖金。如果你也跃跃欲试，不妨选一个合适的任务，开启数据挖掘之旅吧。 转载自知乎：Kaggle 数据挖掘比赛经验分享]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
        <tag>EDA</tag>
        <tag>特征工程</tag>
        <tag>Voting</tag>
        <tag>Stacking</tag>
        <tag>Blending</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（19）：机器学习性能评价指标]]></title>
    <url>%2F2017%2F05%2F24%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8819%EF%BC%89%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%A7%E8%83%BD%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[一、分类问题的评价指标1.1 混淆矩阵对一个二分类问题，将实例分成正类（postive）或者负类（negative），但在实际分类中，会出现以下四种情况： True Positive（真正，TP）：将正类预测为正类数 True Negative（真负，TN）：将负类预测为负类数 False Positive（假正，FP）：将负类预测为正类数 False Negative（假负，FN）：将正类预测为负类数 从下图可以直观的看出四者的关系： 混淆矩阵（Confusion matrix）又被称为错误矩阵，它是一种特定的矩阵来呈现算法性能的可视化呈现。其每一列代表预测值，每一行代表的是实际的类别，这个名字来源于他是否可以非常容易的表明多个类别是否有混淆（也就是一个class被预测为另一个class）混淆矩阵的$i$行$j$列是列别$i$被分为类别$j$的样本个数。 1.2 精确率、召回率与F1值 精确率（precision rate）定义为： P=\frac{TP}{TP+FP} 这里需要注意的是精确率（precision）和准确率（accuracy）是不一样的 ACC=\frac{TP+TN}{TP+TN+FP+FN} 在非平衡数据的情况下，准确率这个评价指标有很大的缺陷。比如在互联网广告里面，点击的数量是很少的，一般只有千分之几，如果用Accuracy，即使全部预测成负类（不点击），ACC也达到了99%以上，这就没有意义了。 召回率（Recall rate）定义为： R=\frac{TP}{TP+FN}此外，还有F1值，它是精确率和召回率的调和均值，即 \frac{2}{F_1}=\frac{1}{P}+\frac{1}{R} F_1=\frac{2TP}{2TP+FP+FN}精确率与召回率都很高时，$F_1$值也会很高。 1.4 通俗理解通俗来讲，精确率是针对我们的预测结果而言的，他表示的是预测为正的样本中有多少是对的，那么预测为正就有两种可能了，一种就是把正类预测为正类（TP），另一种就是把负类预测为正类（FP）。 而召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类（TP），另一种就是把原来的正类预测为负类（FN）。 在信息搜索领域，精确率和召回率又被称为查准率和查全率 查准率=\frac{检索出的相关信息量}{检索出的信息总量}查全率=\frac{检索出的相关信息量}{系统中的相关信息总量}1.5 ROC曲线ROC曲线首先是由二战中的电子工程师和雷达工程师发明的，用来侦测战场上的敌军载具（飞机、船舰），也就是信号检测理论。之后很快就被引入了心理学来进行信号的知觉检测。数十年来，ROC分析被用于医学、无线电、生物学、犯罪心理学领域中，而且最近在机器学习（machine learning）和数据挖掘（data mining）领域也得到了很好的发展。 下图是一个ROC曲线的示例图。 在这个ROC曲线的示例图中，横坐标为false positive rate(FPR)，纵坐标为true positive rate（TPR）。由混淆矩阵可得到横纵轴的计算公式。 1）$TPR=\frac{TP}{TP+FN}$ 代表分类器预测的正类中实际正实例占所有正实例的比例。直观上代表能将正例分对的概率。 2）$FPR=\frac{FP}{FP+TN}$ 代表分类器预测的正类中实际负实例占所有负实例的比例。直观上代表将负类错分为正例的概率。 假设采用逻辑回归分类器，其给出针对每个实例为正类的概率，那么通过设定一个阈值如0.6，概率大于等于0.6的为正类，小于0.6的为负类。对应的就可以算出一组(FPR,TPR)，随着阈值的逐渐减小，越来越多的实例被划分为正类，但是这些正类中同样也掺杂着更多的负实例，即TPR和FPR会同时增大。阈值最大时，对应坐标点（0，0），阈值最小时，对应坐标点（1，1）。 接下来我们考虑ROC曲线图中的四个点和一条线。第一个点，(0,1)，即FPR=0, TPR=1，这意味着FN（false negative）=0，并且FP（false positive）=0。这是一个完美的分类器，它将所有的样本都正确分类。第二个点，(1,0)，即FPR=1，TPR=0，类似地分析可以发现这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。第三个点，(0,0)，即FPR=TPR=0，即FP（false positive）=TP（true positive）=0，可以发现该分类器预测所有的样本都为负样本（negative）。类似的，第四个点（1,1），分类器实际上预测所有的样本都为正样本。经过以上的分析，我们可以断言，ROC曲线越接近左上角，该分类器的性能越好。 下面考虑ROC曲线图中的虚线y=x上的点。这条对角线上的点其实表示的是一个采用随机猜测策略的分类器的结果，例如(0.5,0.5)，表示该分类器随机对于一半的样本猜测其为正样本，另外一半的样本为负样本。 如何绘制ROC曲线呢？ 假设已经得出一系列样本被划分为正类的概率，然后按照大小排序，下图是一个示例，图中共有20个测试样本，“class”一栏表示每个测试样本真正的标签（P表示正样本，n表示负样本），“Score”表示每个测试样本属于正样本的概率。 接下来，我们从高到低，依次将“Score”值作为阈值的threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第四个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。每次选取一个不同的threshold，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。这样一来，我们一共得到了20组FPR和TPR的值，将它们画在ROC曲线的结果如下图： 1.6 AUCAUC（Area under Curve）指的是ROC曲线下的面积，介于0和1之间。AUC作为数值可以直观地评价分类器的好坏，值越大越好。 The AUC value is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example. 首先AUC是一个概率值，当你随机挑选一个正样本以及负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值，AUC值越大，当前分类算法越有可能将正样本排在负样本前面，从而能够更好地分类。 以下是根据AUC判断分类器优劣的标准： 1）AUC=1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数场合，不存在完美的分类器。 2）0.5&lt;AUC&lt;1，优于随机猜测。这个分类器妥善设定阈值的话，能有预测价值。 3）AUC=0.5，跟随机猜测一样（如丢硬币），模型没有预测价值。 4）AUC&lt;0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。 那我们为什么使用ROC曲线呢？ 既然已经有那么多的评价标准，为何还要使用ROC和AUC曲线呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现非平衡数据的现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。下图是ROC曲线和Precision-Recall曲线的对比： 在上图中，a和c为ROC曲线，b和d为Precision-Recall曲线。a和b展示的是分类器在原始测试集（正负样本分布平衡）的结果，c和d是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall则变化较大。 二、回归问题的评价指标2.1 平均绝对误差平均绝对误差MAE（Mean Absolute Reeor）又被称为L1范数损失（L1-norm loss）： {\rm MAE}(y, \hat{y})=\frac{1}{n_{\rm samples}}\sum\limits_{i=1}^{n_{\rm samples}}|y_i-\hat{y}_i|2.2 平均平方误差平均平方误差MSE（Mean Squared Error）又被称为L2范数损失（L2-norm loss）: {\rm MSE}(y, \hat{y})=\frac{1}{n_{\rm samples}}\sum\limits_{i=1}^{n_{\rm samples}}(y_i-\hat{y}_i)^2]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>评价指标</tag>
        <tag>精确率</tag>
        <tag>召回率</tag>
        <tag>ROC</tag>
        <tag>AUC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（12）：pytorch实现卷积神经网络]]></title>
    <url>%2F2017%2F05%2F21%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9Apytorch%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[1.载入模块12345import torch import torch.nn as nnimport torchvision.datasets as dsetsimport torchvision.transforms as transformsfrom torch.autograd import Variable 其中torchvision.transforms 用于数据预处理，torchvision.datasets加载内置数据集 2.设置参数123num_epochs = 5batch_size = 100learning_rate = 0.001 迭代次数num_epochs设置为5；批处理样本数batch_size设置为100；学习率learning_rate设置为0.001。 3.加载数据集加载训练集，将MNIST数据集自动从网上下载并解压，train=true表示取出训练集部分，并变换为张量。 1234train_dataset = dsets.MNIST(root='../data/', train=True, transform=transforms.ToTensor(), download=True) 加载测试集，train=False即表示取出测试集部分，并变换为张量。 123test_dataset = dsets.MNIST(root='../data/', train=False, transform=transforms.ToTensor()) 将训练集的60000张图片划分成600份，每份100张图，用于mini-batch输入。同时将测试集的10000张图片分成100份，每份100张图。shffule=True在表示不同批次的数据遍历时，打乱顺序，反之则不打乱顺序。 123456train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) 4.CNN模型（两个卷积层）1234567891011121314151617181920212223class CNN(nn.Module): def __init__(self): super(CNN, self).__init__() self.layer1 = nn.Sequential( nn.Conv2d(1, 16, kernel_size=5, padding=2),#卷积：1 input image channel, 16 output channels, 5x5 square convolution kernel，2 zero padding） nn.BatchNorm2d(16),#归一化 nn.ReLU(),#非线性激活函数ReLU nn.MaxPool2d(2))#池化层 self.layer2 = nn.Sequential( nn.Conv2d(16, 32, kernel_size=5, padding=2), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2)) self.fc = nn.Linear(7*7*32, 10)#全连接层，in_features, out_features, bias=True def forward(self, x): out = self.layer1(x) out = self.layer2(out) out = out.view(out.size(0), -1) out = self.fc(out) return out# 正常情况下, 我们都会用类进行封装一个网络 cnn = CNN() 5.损失函数与优化方法12criterion = nn.CrossEntropyLoss()#损失函数，这里为交叉熵optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)#优化方法，这里使用Adam 6.训练模型12345678910111213141516for epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): # wrap them in Variable images = Variable(images) labels = Variable(labels) # Forward + Backward + Optimize optimizer.zero_grad() outputs = cnn(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics if (i+1) % 100 == 0: print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0])) Epoch [1/5], Iter [100/600] Loss: 0.1363 Epoch [1/5], Iter [200/600] Loss: 0.0487 Epoch [1/5], Iter [300/600] Loss: 0.0688 Epoch [1/5], Iter [400/600] Loss: 0.1273 Epoch [1/5], Iter [500/600] Loss: 0.0283 Epoch [1/5], Iter [600/600] Loss: 0.0375 Epoch [2/5], Iter [100/600] Loss: 0.0398 Epoch [2/5], Iter [200/600] Loss: 0.0595 Epoch [2/5], Iter [300/600] Loss: 0.0793 Epoch [2/5], Iter [400/600] Loss: 0.0166 Epoch [2/5], Iter [500/600] Loss: 0.0235 Epoch [2/5], Iter [600/600] Loss: 0.0128 Epoch [3/5], Iter [100/600] Loss: 0.0273 Epoch [3/5], Iter [200/600] Loss: 0.0507 Epoch [3/5], Iter [300/600] Loss: 0.0384 Epoch [3/5], Iter [400/600] Loss: 0.0150 Epoch [3/5], Iter [500/600] Loss: 0.0086 Epoch [3/5], Iter [600/600] Loss: 0.0616 Epoch [4/5], Iter [100/600] Loss: 0.0243 Epoch [4/5], Iter [200/600] Loss: 0.0112 Epoch [4/5], Iter [300/600] Loss: 0.0391 Epoch [4/5], Iter [400/600] Loss: 0.0140 Epoch [4/5], Iter [500/600] Loss: 0.0324 Epoch [4/5], Iter [600/600] Loss: 0.0053 Epoch [5/5], Iter [100/600] Loss: 0.0358 Epoch [5/5], Iter [200/600] Loss: 0.0109 Epoch [5/5], Iter [300/600] Loss: 0.0066 Epoch [5/5], Iter [400/600] Loss: 0.0028 Epoch [5/5], Iter [500/600] Loss: 0.0380 Epoch [5/5], Iter [600/600] Loss: 0.0518 7.模型测试1234567891011cnn.eval()correct = 0total = 0for images,labels in test_loader: images = Variable(images) outputs = cnn(images) _,predicted = torch.max(outputs.data,1) total += labels.size(0) correct += (predicted == labels).sum()print ('Test Accuracy of model on the 10000 test images:%d %%'%(100*correct/total)) Test Accuracy of model on the 10000 test images:99 % 8.保存模型1torch.save(cnn.state_dict(),'cnn.pkl')]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（11）：神经网络防止过拟合的方法]]></title>
    <url>%2F2017%2F05%2F20%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[过拟合（overfitting）是指在模型参数拟合过程中的问题，由于训练数据包含抽样误差，训练时，复杂的模型将抽样误差也考虑在内，将抽样误差也进行了很好的拟合。具体表现就是最终模型在训练集上效果好，而在测试集上的效果很差，模型的泛化能力比较弱。 那为什么要解决过拟合现象呢？这是因为我们拟合的模型一般是用来预测未知的结果（不在训练集内），过你个虽然在训练集上效果很好，但在实际使用时（测试集）效果很差。同时，在很多问题上，我们无法穷尽所以状态，不可能将所有情况都包含在训练集上。所以，必须要解决过拟合问题。 之所以过拟合在机器学习中比较常见，就是因为机器学习算法为了满足尽可能复杂的任务，其模型的拟合能力一般远远高于问题复杂度，也就是说，机器学习算法有“拟合出正确规则的前提下，进一步拟合噪声”的能力。 过拟合主要是有两个原因造成的：数据太少+模型太复杂。所以，我们可以通过使用合适复杂度的模型来防止过拟合问题，让其足够拟合真正的规则，同时又不至于拟合太多抽样误差。 通过上图可以看出，随着模型训练的进行，模型的复杂度会增加，此时模型在训练数据集上的训练误差会逐渐减小，但是在模型的复杂度达到一定程度时，模型在验证集上的误差反而随着模型的复杂度增加而增大。此时便发生了过拟合，即模型的复杂度升高，但是该模型在除训练集之外的数据集上却不work。 为了防止过拟合，我们需要用到一些方法，如下所示： 一、获取更多的数据所有的过拟合无非就是训练样本的缺乏和训练参数的增加。一般要想获得更好的模型，需要大量的训练参数，这也是为什么CNN网络越来越深的原因之一，而如果训练样本缺乏多样性，那再多的训练参数也毫无意义，因为这造成了过拟合，训练的模型泛化能力相应也会很差。大量数据带来的特征多样性有助于充分利用所有的训练参数。 在数据挖掘领域流行着这样的一句话，“有时候往往拥有更多的数据胜过一个好的模型”。因为我们在使用训练数据训练模型，通过这个模型对将来的数据进行拟合，而在这之间又一个假设便是，训练数据与将来的数据是独立同分布的。即使用当前的训练数据来对将来的数据进行估计与模拟，而更多的数据往往估计与模拟地更准确。因此，更多的数据有时候更优秀。但是往往条件有限，如人力物力财力的不足，而不能收集到更多的数据，如在进行分类的任务中，需要对数据进行打标，并且很多情况下都是人工得进行打标，因此一旦需要打标的数据量过多，就会导致效率低下以及可能出错的情况。所以，往往在这时候，需要采取一些计算的方式与策略在已有的数据集上进行手脚，以得到更多的数据。通俗得讲，数据扩增即需要得到更多的符合要求的数据，即和已有的数据是独立同分布的，或者近似独立同分布的。 如何获取更多的数据，一般有以下几个方法： 1）从数据源头获取更多数据：这个是容易想到的，例如物体分类，我就再多拍几张照片好了；但是，在很多情况下，大幅增加数据本身就不容易；另外，我们不清楚获取多少数据才算够； 2）根据当前数据集估计数据分布参数，使用该分布产生更多数据：这个一般不用，因为估计分布参数的过程也会代入抽样误差。 3）通过一定规则扩充数据，即数据增强（Data Augmentation）。如在物体分类问题里，物体在图像中的位置、姿态、尺度，整体图片明暗度等都不会影响分类结果。我们就可以通过图像平移、翻转、缩放、切割等手段将数据库成倍扩充，以下为具体的方案： 二、使用合适的模型2.1 限制权值 Weight Decay常用的weight decay有L1和L2正则化，L1较L2能够获得更稀疏的参数，但L1零点不可导。在损失函数中，weight decay是放在正则项（regularization）前面的一个系数，正则项一般指示模型的复杂度，所以weight decay的作用是调节模型复杂度对损失函数的影响，若weight decay很大，则复杂的模型损失函数的值也就大。 L1和L2正则化是很重要的过拟合方法，后边专门用一篇文章来讲。 2.2 训练时间 Early stopping提前停止其实是另一种正则化方法，就是在训练集和验证集上，一次迭代之后计算各自的错误率，当在验证集上的错误率最小，在没开始增大之前停止训练，因为如果接着训练，训练集上的错误率一般是会继续减小的，但验证集上的错误率会上升，这就说明模型的泛化能力开始变差了，出现过拟合问题，及时停止能获得泛化更好的模型。如下图（左边是训练集错误率，右图是验证集错误率，在虚线处提前结束训练）： Early stopping方法的具体做法是，在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练。这种做法很符合直观感受，因为accurary都不再提高了，在继续训练也是无益的，只会提高训练的时间。那么该做法的一个重点便是怎样才认为validation accurary不再提高了呢？并不是说validation accuracy一降下来便认为不再提高了，因为可能经过这个Epoch后，accuracy降低了，但是随后的Epoch又让accuracy又上去了，所以不能根据一两次的连续降低就判断不再提高。一般的做法是，在训练的过程中，记录到目前为止最好的validation accuracy，当连续10次Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了（Early Stopping）。这种策略也称为“No-improvement-in-n”，n即Epoch的次数，可以根据实际情况取，如10、20、30。 在神经网络中，对于每个神经元而言，其激活函数在不同的区间的性能是不同的： 当网络权值较小时，神经元的激活函数工作在线性区，此时神经元的拟合能力较弱（类似线性神经元）。有了以上共识之后，就可以解释为什么训练时间（early stopping）有用：因为我们在初始化网络的时候一般都是初始为较小的权值。训练时间越长，部分网络权值可能越大。如果我们在合适时间停止训练，就可以将网络的能力限制在一定范围内。 2.3 网络结构这个很好理解，减少网络的层数、神经元个数等均可以限制网络的拟合能力。 2.4 增加噪声给网络加噪声也有很多方法： 2.4.1 在输入中加噪声噪声会随着网络传播，按照权值的平方放大，并传播到输出层，对误差 Cost 产生影响。推导直接看 Hinton 的 PPT 吧： 在输入中加高斯噪声，会在输出中生成$\sum_i\sigma _i^2w_i^2$的干扰项。训练时，减小误差，同时也会对噪声产生的干扰项进行惩罚，达到减小权值的平方的目的，达到与L2 regularization类似的效果（对比公式）。 2.4.2 在权值上加噪声在初始化网络的时候，用0均值的高斯分布作为初始化。Alex Graves 的手写识别 RNN 就是用了这个方法： Graves, Alex, et al. “A novel connectionist system for unconstrained handwriting recognition.” IEEE transactions on pattern analysis and machine intelligence 31.5 (2009): 855-868. It may work better, especially in recurrent networks (Hinton) 2.4.3 对网络的响应加噪声如在前向传播过程中，让某些神经元的输出变为 binary 或 random。显然，这种有点乱来的做法会打乱网络的训练过程，让训练更慢，但据 Hinton 说，在测试集上效果会有显著提升 （But it does significantly better on the test set!）。 三、结合多种模型简而言之，训练多个模型，以每个模型的平均输出作为结果。 从 N 个模型里随机选择一个作为输出的期望误差$&lt;[(t-y_i)]^2&gt;$ ，会比所有模型的平均输出的误差$&lt;[(t-\bar{y})]^2&gt;$大: 大概基于这个原理，就可以有很多方法了。 3.1 Bagging和Boost简单理解，就是分段函数的概念：用不同的模型拟合不同部分的训练集。以随机森林（Rand Forests）为例，就是训练了一堆互不关联的决策树。但由于训练神经网络本身就需要耗费较多自由，所以一般不单独使用神经网络做Bagging。 bagging和boosting详细可见机器学习算法系列（6）：AdaBoost 3.2 Dropout正则是通过在代价函数后面加上正则项来防止模型过拟合的。而在神经网络中，有一种方法是通过修改神经网络本身结构来实现的，其名为Dropout。该方法是在对网络进行训练时用一种技巧（trick）， Dropout是hintion最近2年提出的，源于其文章Improving neural networks by preventing co-adaptation of feature detectors.中文大意为：通过阻止特征检测器的共同作用来提高神经网络的性能。 在训练时，每次随机（如50%概率）忽略隐层的某些节点；这样，我们相当于随机从$2^H$个模型中采样选择模型；同时，由于每个网络只见过一个训练数据（每次都是随机的新网络），所以类似 bagging 的做法，这就是我为什么将它分类到「结合多种模型」中； 此外，而不同模型之间权值共享（共同使用这 H 个神经元的连接权值），相当于一种权值正则方法，实际效果比 L2 regularization 更好。 正则化方法：L1和L2 regularization、数据集扩增、dropout]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
        <tag>dropout</tag>
        <tag>过拟合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（10）：DMC—卷积神经网络分享]]></title>
    <url>%2F2017%2F05%2F19%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%8810%EF%BC%89%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[PDF链接：卷积神经网络PPT链接（动图）：百度云]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
        <tag>Batch Normalization</tag>
        <tag>dropout</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（9）：Batch Normalization]]></title>
    <url>%2F2017%2F05%2F14%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%889%EF%BC%89%EF%BC%9ABatch%20Normalization%2F</url>
    <content type="text"><![CDATA[batch normalization(Ioffe and Szegedy, 2015) 是优化深度神经网络中最激动人心的创新之一。实际上它并不是一个优化算法，而是一个自适应的重新参数化 的方法，试图解决训练非常深层模型的困难。Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift机器学习领域有一个很重要的假设：iid独立同分布假设，就是假设训练数据和测试数据满足相同分布，这是通过训练数据训练出来的模型能够在测试集上获得好的效果的一个基本保证。Batch Normalization就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布。 一、Internal covariate shift首先给出covariate shift的定义：模型实例集合中的输入值X的分布总是变化，违背了idd独立通同布假设。 深度学习网络包含很多隐层的网络结构，在训练过程中参数会不断发生改变，导致后续每一层输入的分布也面临着covariate shift，也就是在训练过程中，隐层的输入分布总是发生改变，这就是所谓的Internal covariate shift，Internal指的是深层网络的隐层，covariate shift发生在深度神经网络内部，就被称作Internal covariate shift。 在DNN的实验中，对数据进行预处理时，例如白化或者zscore，甚至是简单的减均值操作都是可以加速收敛的。为什么减均值、白化可以加快训练，作如下分析： 首先，图像数据的每一维一般都是0~255之间的数字，因此数据点智慧落在第一象限，而且图像数据具有很强的相关性，比如第一个灰度值为30，比较黑，那它旁边的一个像素值一般不会超过100，否则给人的感觉就像噪声一样。由于强相关性，数据点仅会落在第一象限的小区域内，形成类似第一个图的狭长分布。 其次，神经网络模型在初始化的时候，权重W都是随机采样生成的，一般都是零均值，因此起初的拟合y=Wx+b，基本过原点附近，如图b红色虚线。因此，网络需要经过多次迭代学习才能逐步达到如紫色实线的拟合，即收敛的比较慢。更何况，这里只是个二维的演示，数据占据四个象限中的一个，但如果是几百、几千、上万维呢？而且数据在第一象限也只是占了很小的一部分区域而已，可想而知若不对数据进行预处理带来了多少运算资源的浪费，而且大量的数据外分割面在迭代时很可能会在刚进入数据中是就遇到了一个局部最优，导致overfit的问题。如果我们对输入数据先作减均值操作，如图c，数据点就不再只分布在第一象限，这是一个随机分界面落入数据分布的概率增加了$2^n$倍，大大加快学习。更进一步的，我们对数据再进行去相关操作，例如PCA和ZCA白化，数据不再是一个狭长的分布，随机分界面有效的概率就又大大增加了，使得数据更加容易区分，这样又会加快训练，如图d。 不过计算协方差的特征值太耗时也太耗空间，一般最多只用到z-score处理，即每一维减去自身均值，再除以自身标准差，这样能使数据点在每维上具有相似的宽度，可以起到增大数据分布范围，进而使更多随机分界面有意义的作用。 二、Batch Normalization2.1 直观解释Batch Normalization的基本思想其实很直观：因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B,U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或者正值），所以这导致反向传播的时候低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因，而BN就是通过一定的规范化手段，对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛地快。 但是这里有个问题，如果都通过Batch Normalization，那么不就跟把非线性函数替换成线性函数效果相同了？我们知道，如果是多层的线性函数变换，其实这个深层是没有意义的，因为多层线性网络跟一层线性网络是等价的。这意味着网络的表达能力下降了，这也意味着深度的意义就没有了。比如下图，在使用sigmoid激活函数的时候，如果把数据限制到零均值单位方差，那么相当于只使用了激活函数中近似线性的部分，这显然会降低模型的表达能力。BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale*x+shift)，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把这个值从标准正态分布左移或者由移一点并长胖一点或者变瘦一点，每个实例挪动的程度不一样，这样等价于非线性函数的值从正中心周围的线性区往非线性区动了动，让因训练所需而“刻意”加入的BN能够有可能还原最初的输入。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。从而保证整个网络的capacity。 2.2 算法过程假设对于一个深层神经网络来说，其中两层结构如下：要对每个隐藏神经元的激活值做BN，可以想象成每个隐层又加上了一层BN操作层，它位于X=WY+B激活值获得之后，非线性函数变换之前，其图示如下：对Mini-Batch SGD来说，一次训练过程里面包含m个训练实例，其具体BN操作就是对于隐层内每个神经元的激活值来说，进行如下变换： \hat{x}^{\left(k\right)}=\frac{x^{\left(k\right)}-E\left[x^{\left(k\right)}\right]}{\sqrt{var\left[x^{\left(k\right)}\right]}}要注意，这里t层某个神经元的$x(k)$不是指原始输入，就是说不是$t-1$层每个神经元的输出，而是$t$曾这个神经元的激活$x=WU+B$，这里的$U$才是$t-1$层神经元的输出。还有一点，上述公式中用到了均值和方差，在理想情况下均值和方差是针对整个数据集的，但显然这是不现实的，因此，作者做了简化，用一个Batch的均值和方差作为对整个数据集均值和方差的估计。这个变换就是：某个神经元对应的原始的激活$x$减去Mini-Batch内$m$个激活$x$求得的均值$E(x)$并除以求得的方差$Var(x)$来进行转换。 上文说过经过这个变换后某个神经元的激活$x$形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大梯度，增强反向传播信息流行性，加快训练收敛速度。但是这样会导致网络表达能力下降，为了防止这一点，每个神经元增加两个调节参数（scale和shift），这俩个参数是通过训练来学习的，用来对变换后的激活反变换，使得网络表达能力增强，即对变换后的激活进行如下的scle和shift操作，这其实是变换的反操作： y^{\left(k\right)}=\gamma^{\left(k\right)}\hat{x}^{\left(k\right)}+\beta^{\left(k\right)}其整个算法流程如下： 2.3 推理过程BN在训练的时候可以根据Mini-Batch数据里可以得到的统计量，那就想其他办法来获得这个统计量，就是均值和方差。可以用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量，因为本来就打算用全局的统计量，知识因为计算量等太大所以才会用Mini-Batch这种简化方式的，那么在推理的时候直接用全局统计量即可。 决定了获得统计量的数据范围，那么接下来的问题就是如何获得均值和方差的问题。很简单，因为每次做Mini-Batch训练时，都会有那个Mini-Batch里m个训练实例获得的均值和方差，现在要全局统计量，只要把每个Mini-batch的均值和方差统计量记住，然后对这些均值和方差求其对应的数学期望即可得出全局统计量，即： E\left[x\right]\gets E_{\beta}\left[\mu_{\beta}\right] Var\left[x\right]\gets\frac{m}{m-1}E_{\beta}\left[\sigma_{\beta}^{2}\right]有了均值和方差，每个隐藏神经元也已经有对应训练好的Scaling参数和Shift参数，就可以在推导的时候对每个神经元的激活数据计算BN进行变换了，在推理过程中进行BN采取如下方式： y=\frac{\gamma}{\sqrt{Var\left[x\right]+\epsilon}}·x+\left(\beta -\frac{\gamma E\left[x\right]}{\sqrt{Var\left[x\right]+\epsilon}}\right)这个公式其实和训练时 y^{\left(k\right)}=\gamma^{\left(k\right)}\hat{x}^{\left(k\right)}+\beta^{\left(k\right)}是等价的，通过简单的合并计算推导就可以得出这个结论。在实际运行时，按照这种变体形式可以减少计算量，因为对每一个隐节点来说：$\frac{\gamma}{\sqrt{Var\left[x\right]+\epsilon}}$和$\frac{\gamma E\left[x\right]}{\sqrt{Var\left[x\right]+\epsilon}}$都是固定值，这样两个值可以实现算好存起来，在推理的时候直接用就行了，比原始的公式每一步骤都少了出发的运算过程，乍一看也没少多少计算量，但是如果隐层节点个数多的话节省的计算量就比较多了。 2.4 参数训练以上是对算法原理的讲述，在反向传导的时候，我们需要求最终的损失函数对$\gamma$和$\beta$两个参数的导数，还要求损失函数对Wx+b中x的导数，一遍使误差继续向后传播。几个主要的公式如下，主要用到了链式法则。 三、Experiments作者在文章中也做了很多实验对比，这里简要说明两个： 下图a说明，BN可以加速训练。图b和c分别展示了训练过程中输入数据分布的变化情况。! 下表是一个实验结果的对比，需要注意的是在使用BN的过程中，算法对sigmoid激活函数的提升非常明显，解决了困扰学术界十几年的sigmoid过饱和的问题，但sigmoid在分类问题上确实没有ReLU好用，大概是因为sigmoid的中间部分太“线性”了，不像ReLU一个很大的转折，在拟合复杂非线性函数的时候可能没那么高效。 四、算法优势论文中罗列了Batch Normalization的很多作用，一一列举如下： 1）可以使用很高的学习率。如果每层的scale不一致，实际上每层需要的学习率是不一样的，同一层不同维度的scale往往也需要不同大小的学习率，通常需要使用最小的那个学习率才能保证损失函数有效下降，Batch Normalization 2）移除或使用较低的dropout。dropout是常用的防止overfitting的方法，而导致overfitting的位置往往在数据边界处，如果初始化权重就已经落在数据内部，overfitting现象就可以得到一定的缓解。论文中最后的模型分别使用10%、5%和0%的dropout训练模型，与之前的40%~50%相比，可以大大提高训练速度。 3） 降低L2权重衰减系数。 还是一样的问题，边界处的局部最优往往有几维的权重（斜率）较大，使用L2衰减可以缓解这一问题，现在用了Batch Normalization，就可以把这个值降低了，论文中降低为原来的5倍。 4）取消Local Response Normalization层。 由于使用了一种Normalization，再使用LRN就显得没那么必要了。而且LRN实际上也没那么work。 5）减少图像扭曲的使用。 由于现在训练epoch数降低，所以要对输入数据少做一些扭曲，让神经网络多看看真实的数据。 说完BN的优势，自然可以知道什么时候用BN比较好。例如，在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Batch Normalization</tag>
        <tag>过拟合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（8）：激活函数]]></title>
    <url>%2F2017%2F05%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%888%EF%BC%89%EF%BC%9A%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[深度学习的基本原理是基于人工神经网络，信号从一个神经元进入，经过非线性的activation function，传入到下一层神经元；再经过该层神经元的activate function，继续往下传递，如此循环往复，直到输出层。其中的激活函数的主要作用是提供网络的非线性建模能力，使得神经网络有足够的capacity来抓取复杂的pattern，在各个领域取得state-of-the-art的结果。 现在假设一个神经网络中仅包含线性激励和全连接运算，那么该网络仅仅能够表达线性映射，即使增加网络的深度也依旧还是线性映射，即输出都是输入的线性组合，失去了隐藏层存在的意义，难以有效建模实际环境中非线性分布的数据。加入非线性激活函数之后，深度学习网络可以逼近任意函数，具备了分层的非线性映射学习能力。加拿大蒙特利尔大学的Bengio教授在 ICML 2016 的文章中给出了激活函数的定义：激活函数是映射 h:R→R，且几乎处处可导。从定义来看，几乎所有连续可导函数都可以用作激活函数。但目前常见的多是分段线性和具有指数形状的非线性函数。 显而易见，activation function在深度学习中举足轻重，也是很活跃的研究领域之一。目前来讲，选择怎样的activation function不在于它能否模拟真正的神经元，而在于能否便于优化整个深度神经网络。 一、软饱和与硬饱和激活函数Bengio 教授等将具有 1）在定义域内处处可导 2）两侧导数逐渐趋近于0，即$\lim_{x\rightarrow\infty}f’\left(x\right)=0$。的激活函数定义为软饱和激活函数。 与极限的定义类似，饱和也分为左饱和与右饱和，左侧软饱和为： \lim_{x\rightarrow -\infty}f'\left(x\right)=0右侧软饱和为： \lim_{x\rightarrow +\infty}f'\left(x\right)=0与软饱和激活函数相对的是硬饱和激活函数，即： f'(x)=0, 当|x|>c,c为常数同理，应饱和也分为左饱和与右饱和，左侧硬饱和为： f'(x)=0, 当-x>c, c为正数右侧硬饱和为： f'(x)=0, 当x>c, c为正数二、sigmoidsigmoid非线性函数的数学公式为： \sigma\left(x\right)=\frac{1}{1+e^{-x}}函数图像及梯度函数图像如下所示：它将输入实数值“挤压”到0-1范围内。更具体地说，很大的负数变成0，很大的正数变成1。它是便于求导的平滑函数，其导数为$\sigma(x)(1-\sigma(x))$，这是它的优点。sigmoid 在定义域内处处可导，且两侧导数逐渐趋近于0，即：$\lim_{x\rightarrow\infty}f’\left(x\right)=0$ 然而现在sigmoid函数已经不太受欢迎，实际很少使用了，这是因为它有三个主要缺点： 1）梯度消失。Sigmoid 的软饱和性，使得深度神经网络在二三十年里一直难以有效的训练，是阻碍神经网络发展的重要原因。具体地，我们知道优化神经网络的方法是Back Propagation，即导数的反向传递：先计算输出层对应的loss，然后将loss以导数的形式不断向上一层网络传递，修正相应的参数，达到降低loss的目的。sigmoid反向传导的梯度包含了一个f’(x) 因子（sigmoid关于输入的导数），因此一旦输入落入饱和区，f’(x) 就会变得接近于0，导致了向底层传递的梯度也变得非常小。此时，网络参数很难得到有效训练。这种现象被称为梯度消失。一般来说， sigmoid 网络在 5 层之内就会产生梯度消失现象。我们也可以在图中看出原因，主要在于两点：(1) 在上图中容易看出，当$\sigma(x)$中x较大或较小时，导数接近0，而后向传递的数学依据是微积分求导的链式法则，当前层的导数需要之前各层导数的乘积，几个小数的相乘，结果会很接近0 (2) Sigmoid导数的最大值是0.25，这意味着导数在每一层至少会被压缩为原来的1/4，通过两层后被变为1/16，…，通过10层后为1/1048576。请注意这里是“至少”，导数达到最大值这种情况还是很少见的。梯度消失问题至今仍然存在，但被新的优化方法有效缓解了，例如DBN中的分层预训练，Batch Normalization的逐层归一化，Xavier和MSRA权重初始化等代表性技术。 2）Sigmoid函数的输出不是Zero-centered的。这个性质并不是我们想要的，因为在神经网络后面层中的神经元得到的数据将不是零中心的。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数(比如在$f=w^Tx+b$中每个元素都x&gt;0),那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式f而定）。这将会导致梯度下降权重更新时出现z字型的下降（如下图所示）。然而，可以看到整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。因此，该问题相对于上面的神经元饱和问题来说只是个小麻烦，没有那么严重。 3）幂运算相对耗时：相对于前两项，这其实并不是一个大问题，我们目前是具备相应计算能力的，但面对深度学习中庞大的计算量，最好是能省则省。之后我们会看到，在ReLU函数中，需要做的仅仅是一个thresholding，相对于幂运算来讲会快很多。 三、tanhtanh非线性函数的数学公式为： tanh x = \frac{e^x-e^{-x}}{e^x+e^{-x}}函数图像及梯度函数图像如下所示：如上图所示，计算可以知道：$tanh(x)=2sigmoid(2x)-1$，它其实是一个简单放大的sigmoid神经元，和sigmoid神经元一样，也具有软饱和性。但是和sigmoid神经元不同的是，它解决了zero-centered的输出问题，因此，在实际操作中，tanh非线性函数比sigmoid非线性函数更受欢迎。然而，gradient vanishing的问题和幂运算的问题仍然存在。Xavier在文献[]中分析了sigmoid与tanh的饱和现象及特点，具体见原论文。此外，文献[]中提到了tanh网络的收敛速度要比sigmoid块。因为tanh的输出均值比sigmoid更接近0，SGD会更接近natural gradient（一种二次优化技术），从而降低所需的迭代次数。 四、ReLUReLU非线性函数的数学公式为： ReLU(x)=max(0,x)函数图像及梯度函数图像如下所示：虽然2006年Hinton教授提出通过分层无监督预训练解决深层网络训练困难的问题，但是深度网络的直接监督式训练的最终突破，最主要的原因是新型激活函数ReLU。它有以下几大优点： 1）解决了gradient vanishing问题：ReLU在$x0$时导数为1，所以，ReLU能够在$x&gt;0$时保持梯度不衰减，从而缓解梯度消失问题。 2）计算速度非常快。对比sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，ReLU可以简单地通过对一个矩阵进行阈值计算得到。 3）收敛速度非常快。相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用。下图是从 Krizhevsky 等的论文中截取的图表，指明使用ReLU比使用tanh的收敛快6倍。 4）ReLU另外一个性质是提供神经网络的稀疏表达能力，在Bengio教授的Deep Sparse Rectifier Neural Network[6]一文中被认为是ReLU带来网络性能提升的原因之一。但后来的研究发现稀疏性并非性能提升的必要条件，文献 RReLU [9]也指明了这一点。 PReLU[10]、ELU[7]等激活函数不具备这种稀疏性，但都能够提升网络性能。本文作者在文章[8]中给出了一些实验比较结果。首先，在cifar10上采用NIN网络，实验结果为 PReLU &gt; ELU &gt; ReLU，稀疏性并没有带来性能提升。其次，在 ImageNet上采用类似于[11] 中model E的15 层网络，实验结果则是ReLU最好。为了验证是否是稀疏性的影响，以 LReLU [12]为例进一步做了四次实验，负半轴的斜率分别为1，0.5，0.25, 0.1，需要特别说明的是，当负半轴斜率为1时，LReLU退化为线性函数，因此性能损失最大。实验结果展现了斜率大小与网络性能的一致性。综合上述实验可知，ReLU的稀疏性与网络性能之间并不存在绝对正负比关系。 ReLU也有几个缺点： 1）Dead ReLU Problem。随着训练的推进，部分输入会落入硬饱和区，某些神经元可能永远不会被激活，这个ReLU单元在训练中将不可逆转的死亡，导致相应的参数永远不能被更新，使得数据多样化丢失。这种现象被称为“神经元死亡”。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。 2）偏移现象。即输出均值恒大于零。偏移现象和Dead ReLU Problem会共同影响网络的收敛性。 尽管存在上述几个问题，ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试！ 五、Leaky ReLULeaky ReLU非线性函数的数学公式为： f(x)=max(0.01x,x)函数图像及梯度函数图像如下所示：人们为了解决Dead ReLU Problem，提出了将ReLU的前半段设为0.01x而非0。理论上来说，Leaky ReLU拥有ReLU的所有优点，外加不会有Dead ReLU problem，但是在实际操作中，并没有完全证明Leaky ReLU总是好于ReLU。有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定。 六、PReLUParametric ReLU非线性函数的数学公式为： f(x)=max(\alpha x,x)PReLU是ReLU和LReLU的改进版本，具有非饱和性。与LReLU相比，PReLU中的负半轴斜率$\alpha$由back propagation学习而非固定。原文献建议初始化$\alpha$为0.25，不采用正则。 虽然PReLU 引入了额外的参数，但基本不需要担心过拟合。例如，在cifar10+NIN实验中， PReLU比ReLU和ELU多引入了参数，但也展现了更优秀的性能。所以实验中若发现网络性能不好，建议从其他角度寻找原因。 与ReLU相比，PReLU收敛速度更快。因为PReLU的输出更接近0均值，使得SGD更接近natural gradient。证明过程参见原文[10]。 七、RReLU数学形式与PReLU类似，但RReLU[9]是一种非确定性激活函数，其参数是随机的。这种随机性类似于一种噪声，能够在一定程度上起到正则效果。作者在cifar10/100上观察到了性能提升。 综上，ReLU家族讲完了，总结如下图：其中表格为在cifar10上采用NIN网络的实验结果。 八、MaxoutMaxout[13]是ReLU的推广，其发生饱和是一个零测集事件（measure zero event）。正式定义为： max(w_1^Tx+b_1,w^T_2+b_2,···,w_n^Tx+b_n)Maxout网络能够近似任意连续函数，且Maxout是对ReLU和leaky ReLU的一般化归纳，当$w_2,b_2,···,w_n,b_n$为0时，退化为ReLU。其实，Maxout的思想在视觉领域存在已久。例如，在HOG特征里有这么一个过程：计算三个通道的梯度强度，然后在每一个像素位置上，仅取三个通道中梯度强度最大的数值，最终形成一个通道。这其实就是Maxout的一种特例。 所以Maxout神经元就拥有ReLU单元的所有优点（线性操作和不饱和，能够缓解梯度消失），而没有它的缺点（死亡的ReLU单元）。然而和ReLU对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。 九、ELUELU（Exponential Linear Units）非线性函数的数学公式为： f(x)=max(0,x)+\alpha·min(0,exp(x)-1)函数图像及梯度函数图像如下所示：ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点，并有自身的特点，罗列如下： 1）右侧线性部分使得ELU能够缓解梯度消失，而左侧软饱和能够燃ELU对输入变换或噪声更加鲁棒。 2）ELU的输出均值接近于零，即zero-centered，所以收敛速度更快。经ELU的作者实验，ELU的收敛性质的确优于ReLU和PReLU。在cifar10上，ELU 网络的loss 降低速度更快；在 ImageNet上，不加 Batch Normalization 30 层以上的 ReLU 网络会无法收敛，PReLU网络在MSRA的Fan-in （caffe ）初始化下会发散，而 ELU 网络在Fan-in/Fan-out下都能收敛 。 它的一个小问题在于计算量稍大，类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据证明ELU总是优于ReLU。 十、Noisy Activation FunctionsBengio教授在ICML2016提出了一种激活策略[1]，可用于多种软饱和激活函数，例如sigmoid和tanh。当激活函数发生饱和时，网络参数还能够在两种动力下继续更新：正则项梯度和噪声梯度。引入适当的噪声能够扩大SGD的参数搜索范围，从而有机会跳出包河区。在激活函数中引入噪声的更早工作可追溯到[5]，但文献[5]的工作并不考虑噪声引入的时间和大小。本篇的特点在于，只在饱和区引入噪声，且噪声量与饱和程度相关（原式与泰勒展开式一次项之差$\delta$）。算法1中g表示sigmoid，用于归一化$\delta$。注意，ReLU的$\delta$恒为0，无法直接加噪声，所以作者把噪声加在了输入上。 CReLUMPELU十一、小结建议用ReLU非线性函数。但是要注意初始化和learning rate的设置，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，不要再用sigmoid了。也可以试试tanh，但是其效果应该不如ReLU或者Maxout。 参考资料[1] Gulcehre, C., et al., Noisy Activation Functions, in ICML 2016. 2016.[2] Glorot, X. and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. AISTATS 2010.[3] LeCun, Y., et al., Backpropagation applied to handwritten zip code recognition. Neural computation, 1989. 1(4): p. 541-551.[4] Amari, S.-I., Natural gradient works efficiently in learning. Neural computation, 1998. 10(2): p. 251-276.[5] Nair, V. and G.E. Hinton. Rectified linear units improve Restricted Boltzmann machines. ICML 2010.[6] Glorot, X., A. Bordes, and Y. Bengio. Deep Sparse Rectifier Neural Networks.AISTATS 2011.[7] Djork-Arné Clevert, T.U., Sepp Hochreiter. Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs). ICLR 2016.[][9] Xu, B., et al. Empirical Evaluation of Rectified Activations in Convolutional Network. ICML Deep Learning Workshop 2015.[10] He, K., et al. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. ICCV 2015.[11] He, K. and J. Sun Convolutional Neural Networks at Constrained Time Cost. CVPR 2015.[12] Maas, A.L., Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural network acoustic models. in ICML 2013.[13] Goodfellow, I.J., et al. Maxout Networks. ICML 2013..]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>激活函数</tag>
        <tag>ReLU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（7）：优化算法]]></title>
    <url>%2F2017%2F05%2F11%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%887%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、Gradient Descent [Robbins and Monro, 1951, Kiefer et al., 1952]机器学习中，梯度下降法常用来对相应的算法进行训练。常用的梯度下降法包含三种不同的形式，分别是BGD、SGD和MBGD，它们的不同之处在于我们在对目标函数进行梯度更新时所使用的样本量的多少。 以线性回归算法来对三种梯度下降法进行比较。一般线性回归函数的假设函数为： h_{\theta}=\sum_{j=0}^n{\theta_jx_j}（即有n个特征）对应的损失函数为 L\left(\theta\right)=\frac{1}{2m}\sum_{i=1}^m{\left(h\left(x_i\right)-y_i\right)^2}下图即为一个二维参数$\theta _0$和$\theta _1$组对应的损失函数可视化图像： 1.1 BGD（Batch Gradient Descent）批量梯度下降法（Batch Gradient Descent，简称BGD）是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新，其数学形式如下： 1）对上述的损失函数求偏导： \frac{\partial L\left(\theta\right)}{\partial\theta_j}=-\frac{1}{m}\sum_{i=1}^m{\left(y^{\left(i\right)}-h_{\theta}\left(x^{\left(i\right)}\right)\right)}x_{j}^{\left(i\right)} 2）由于是最小化损失函数，所以按照每个参数$\theta$的梯度负方向来更新每个$\theta$：\theta_{j}^{'}=\theta_j+\frac{1}{m}\sum_{i=1}^m{\left(y^{\left(i\right)}-h_{\theta}\left(x^{\left(i\right)}\right)\right)x_{j}^{\left(i\right)}}其伪代码如下： 123for i in range(nb_epochs): params_grad = evaluate_gradient(loss_function, data, params) params = params - learning_rate * params_grad 从上面的公式可以看到，它得到的是全局最优解，但是每迭代一步，都要用到训练集所有的数据，若样本数目$m$很大，那么迭代速度会大大降低。其优缺点如下： 优点：全局最优解；易于并行实现； 缺点：当样本量很大时，训练过程会很慢 1.2 SGD（Stochastic Gradient Descent）由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常缓慢。随机梯度下降法（Stochastic Gradient Descent，简称SGD）正是为了解决批量梯度下降法这一弊端而提出的。对每个样本的损失函数对$\theta$求偏导得到对应的梯度，来更新$\theta$： \theta_{j}^{'}=\theta_j+\left(y^{\left(i\right)}-h_{\theta}\left(x^{\left(i\right)}\right)\right)x_{j}^{\left(i\right)}具体的伪代码形式为 12345for i in range(nb_epochs): np.random.shuffle(data) for example in data: params_grad = evaluate_gradient(loss_function, example, params) params = params - learning_rate * params_grad 随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已将将$\theta$迭代到最优解了，对比上面的批量梯度下降，迭代一次不可能最优，如果迭代十次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。其优缺点如下： 优点：训练速度快； 缺点：准确度下降，并不是全局最优；不易于并行实现。 从迭代次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图表示如下： 1.3 MBGD（Mini-batch Gradient Descent）从上述的两种梯度下降法可以看出，其各自均有优缺点，那么能否在两种方法的性能之间取得一个折中呢？即，算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这正是小批量梯度下降法（Mini-batch Gradient Descent，简称MBGD）的初衷。 下面的伪代码中，我们每轮迭代的mini-batches设置为50： 12345for i in range(nb_epochs): np.random.shuffle(data) for batch in get_batches(data, batch_size=50): params_grad = evaluate_gradient(loss_function, batch, params) params = params - learning_rate * params_grad 1.4 梯度下降算法的局限虽然梯度下降算法效果很好，并且被广泛的使用，但它存在着一些需要解决的问题： 1）首先选择一个合适的学习速率很难。若学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么会阻碍收敛，即在极值点附近振荡 2）学习速率调整（又称学习速率调度，Learning rate schedules）试图在每次更新过程中，改变学习速率，如模拟退火按照预先设定的调度算法或者当相邻的迭代中目标变化小于一个阈值时候减小学习速率。但是梯度下降算法的调度和阈值需要预先设置，无法对数据集特征进行自适应。 3）模型所有的参数每次更新都是使用相同的学习速率。如果我们的数据很稀疏并且我们的特征出现的次数不同，我们可能不会希望所有的参数以某种相同的幅度进行更新，而是针对很少出现的特征进行一次大幅度更新。 4）在神经网络中常见的极小化highly non-convex error functions的一个关键挑战是避免步入大量的suboptimal local minima。Dauphin等人认为实践中的困难来自saddle points而非local minima。这些saddle points（鞍点）经常被一个相等误差的平原包围，导致SGD很难摆脱，因为梯度在所有方向都近似于0。 二、Momentum这是一种启发式算法。形式如下： v_t=\gamma v_{t-1}+\eta\nabla_{\theta}J\left(\theta\right) \theta =\theta -v_t我们用物理上的动能势能转换来理解它。即物体在这一时刻的动能=物体在上一时刻的动能+上一时刻的势能差。由于有阻力和转换时的损失，所以两者都乘以一个系数。 就像一个小球从坡上向下滚，当前的速度取决于上一时刻的速度和势能的改变量。 这样在更新参数时，除了考虑到梯度以外，还考虑了上一时刻参数的历史变更幅度。例如，参数上一次更新幅度较大，并且梯度也较大，那么在更新时是不是得更加猛烈一些了。这样的启发式算法，从直观感知上确实有道理。 下面两张图直观的展示了Momentum算法，其中绿色箭头表示上一时刻参数的变更幅度，红色箭头表示梯度，两者向量叠加即得到蓝色箭头即真实的更新幅度。 三、NAG（Nesterov accelerated gradient） [Nesterov, 1983]还是以上面小球的例子来看，momentum方式下小球完全是盲目被动的方式滚下的。这样有个缺点就是在邻近最优点附近是控制不住速度的。我们希望小球可以预判后面的“地形”，要是后面地形还是很陡峭，那就继续坚定不移地大胆走下去，不然的话就减缓速度。 当然，小球自己也不知道真正要走到哪里，这里以 \theta - \gamma v_{t-1}作为下一个位置的近似，将动量的公式更改为： v_t=\gamma v_{t-1}+\eta\nabla_{\theta}J\left(\theta - \gamma v_{t-1}\right) \theta =\theta -v_t相比于动量方式考虑的是上一时刻的动能和当前点的梯度，而NAG考虑的是上一时刻的梯度和近似下一点的梯度，这使得它可以先往前探探路，然后慎重前进。 Hinton的slides是这样给出的：其中两个blue vectors分别理解为梯度和动能，两个向量和即为momentum方式的作用结果。 而靠左边的brown vector是动能，可以看出它那条blue vector是平行的，但它预测了下一阶段的梯度是red vector，因此向量和就是green vector，即NAG方式的作用结果。 momentum项和nesterov项都是为了使梯度更新更加灵活，对不同情况有针对性。但是，人工设置一些学习率总还是有些生硬，接下来介绍几种自适应学习率的方法 四、学习率退火训练深度网络的时候，可以让学习率随着时间退火。因为如果学习率很高，系统的动能就过大，参数向量就会无规律地变动，无法稳定到损失函数更深更窄的部分去。对学习率衰减的时机把握很有技巧：如果慢慢减小，可能在很长时间内只能浪费计算资源然后看着它混沌地跳动，实际进展很少；但如果快速地减少，系统可能过快地失去能量，不能到达原本可以到达的最好位置。通常，实现学习率退火有三种方式： 1）随步数衰减：每进行几个周期就根据一些因素降低学习率。通常是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的十分之一。这些数值的设定是严重依赖具体问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。 2）指数衰减。数学公式是$\alpha=\alpha_0e^{-kt}$，其中$\alpha_0,k$是超参数，$t$是迭代次数（也可以使用周期作为单位）。 3）$1/t$衰减的数学公式是$\alpha=\alpha_0/(1+kt)$，其中$\alpha_0,k$是超参数，t是迭代次数。 在实践中，我们发现随步数衰减的随机失活（dropout）更受欢迎，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比k更有解释性。但如果你有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。 五、自适应学习率方法5.1 Adagrad [Duchi et al., 2011]之前的方法中所有参数在更新时均使用同一个Learning rate。而Learning rate调整是一个非常耗费计算资源的过程，所以如果能够自适应地对参数进行调整的话，就大大降低了成本。在Adagrad的每一个参数的每一次更新中都使用不同的learning rate。这样的话，令第$t$步更新时对第$i$个参数的梯度为 g_{t,i}=\nabla_{\theta}J\left(\theta_j\right)参数的更新的一般形式为： \theta_{t+1,i}=\theta_{t,i}-\eta g_{t,i}如上所述，Adagrad的差异之处正是在于learning rate不同于其他，将learning rate改为如下： \theta_{t+1,i}=\theta_{t,i}-\frac{\eta}{\sqrt{\sum_{i=0}^t{\left(g^i\right)^2}+\epsilon}}·g_{t,i}实质上是对学习率形成了一个约束项regularizer：$\frac{1}{\sqrt{\sum_{i=0}^t{\left(g^i\right)^2}+\epsilon}}$，${\sum_{i=0}^t{\left(g^i\right)^2}}$是对直至t次迭代的梯度平方和的累加和，$\epsilon $是一个防止分母为0的很小的平滑项。不用平方根操作，算法性能会变差很多 我们可以将到累加的梯度平方和放在一个对角矩阵中$G_t\in\mathbb{R}^{d×d}$中，其中每个对角元素$(i,i)$是参数$\theta_i$到时刻$t$为止所有时刻梯度的平方之和。由于$G_t$的对角包含着所有参数过去时刻的平方之和，我们可以通过在$G_t$和$g_t$执行element-wise matrix vector mulitiplication来向量化我们的操作： \theta_{t+1}=\theta_t-\frac{\eta}{\sqrt{G_t+\epsilon}}\odot g_t 优点：Adagrad让学习速率自适应于参数，在前期$g_t$较小的时候，regularizer较大，能够放大梯度；后期$g_t$较大的时候，regularizer较小，能够约束梯度；因为这一点，它非常适合处理稀疏数据。Dean等人发现Adagrad大大地提高了SGD的鲁棒性并在谷歌的大规模神经网络训练中采用了它进行参数更新，其中包含了在Youtube视频中进行猫脸识别。此外，由于低频词（参数）需要更大幅度的更新，Pennington等人在GloVe word embeddings的训练中也采用了Adagrad。 缺点：由公式可以看出，仍依赖于人工设置一个全局学习率；$\eta$设置过大的话，会使得regularizer过于敏感，对梯度的调节太大；中后期，分母上梯度平方的累加将会越来越大，使得梯度为0，训练提前结束。 5.2 RMSprop [Hinton]RMSprop是一个没有公开发表的适应性学习率方法，它是Hinton在他的课上提出的一种自适应学习速率方法。有趣的是，每个使用这个方法的人在他们的论文中都引用自Geoff Hinton的Coursera课程的第六课的第29页PPT。它用了一种很简单的方式修改了Adagrad方法，让它不过于激进而过早停止学习。具体说来就是，它使用了一个梯度平方的滑动平均，仍然是基于梯度的大小来对每个权重的学习率进行修改，效果不错。但是和Adagrad不同的是，其更新不会让学习率单调变小。 下图展示了RMSprop的计算过程，其中$\alpha$是一个超参数，常用的值是[0.9,0.99,0.999]： 5.3 Adadelta [Zeiler, 2012]Adadelta是Adagrad的一种扩展，以缓解Adagrad学习速率单调递减问题的算法。Adadelta不是对过去所有时刻的梯度平方进行累加，而是将累加时刻限制在窗口大小为的$w$区间。 但梯度累加没有采用简单的存储前$w$个时刻的梯度平方，而是递归地定义为过去所有时刻梯度平方的decaying average$E[g^2]_t$。$t$时刻的running average仅仅依赖于之前average和当前的梯度： E\left[g^2\right]_t=\gamma E\left[g^2\right]_{t-1}+\left(1-\gamma\right)g_{t}^{2}类似momentum term，我们将$\gamma$取值在0.9附近。简介起见，我们从参数更新向量$\Delta\theta_t$角度重写普通SGD的参数更新： \Delta\theta_t=-\eta ·g_{t,i} \theta_{t+1}=\theta_t+\Delta\theta_tAdagrad中我们推导的参数更新向量现在就以下述形式出现： \Delta \theta_t=-\frac{\eta}{\sqrt{G_t+\epsilon}}\odot g_t现在我们简单地将对角矩阵替换为过去时刻梯度平方的decaying average $E[g^2]_t$： \Delta \theta_t=-\frac{\eta}{\sqrt{E[g^2]_t+\epsilon}}\odot g_t由于分母是root mean squared (RMS) error criterion of the gradient，则上面公式可以替换为： \Delta \theta_t=-\frac{\eta}{RMS[g]_t}作者发现（和SGD，Momentum或者Adagrad一样）上述更新中的单元不匹配，即只有部分参数进行更新，也就是参数和更新应该有着相同的hypothetical units。为了实现这个目的，他们首先定义了另外一个exponentially decaying average，这一次对更新参数的平方进行操作，而不只是对梯度的平方进行操作： E[\Delta\theta^2]_t=\gamma·E[\Delta\theta^2]_t+(1-\gamma)\Delta\theta^2参数更新中的root mean squared error则为： RMS[\Delta\theta]_t=\sqrt{E[\Delta\theta^2]_t+\epsilon}将以前的更新规则中的学习速率替换为参数更新的RMS，则得到Adadelta更新规则: \Delta\theta_t=-\frac{RMS[\Delta\theta]_t}{RMS[g]_t}·g_t$$$$\theta_{t+1}=\theta_t+\Delta\theta由于Adadelta更新规则中没有了学习速率这一项，我们甚至都不用对学习速率进行设置。 5.4 Adam [Kingma and Ba, 2014]Adaptive Moment Estimation (Adam)是另外一种对每个参数进行自适应学习速率计算的方法，除了像Adadelta和RMSprop一样保存去过梯度平方和的exponentially decaying average外，Adam还保存类似momentum一样过去梯度的exponentially decaying average。它看起来像是RMSProp的动量版。 m_t = \beta_1·m_{t-1}+(1-\beta)·g_tv_t = \beta_2·v_{t-1}+(1-\beta_2)·g^2_t$m_t$和$v_t$分别是分别是梯度的一阶矩（均值）和二阶距（偏方差）的估计，由于$m_t$和$v_t$由全零的向量来初始化，Adam的作者观察到他们会被偏向0，特别是在initial time steps或decay rates很小的时候（即$\beta_1$和$\beta_2$都接近于1）,于是他们通过计算bias-corrected一阶矩和二阶矩的估计低消掉偏差。 \hat{m}=\frac{m}{1-\beta_{1}^{t}} \hat{v}=\frac{v}{1-\beta_{2}^{t}}然后使用上述项和Adadelta和RMSprop一样进行参数更新，可以得到Adam的更新规则： \theta_{t+1}=\theta_t-\frac{\eta}{\sqrt{\hat{v}+\epsilon}}\hat{m}Adam的完整更新过程如下图所示，其中它推荐默认设置$\alpha=0.001,\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-8}$，在实际操作中，推荐将Adam作为默认的算法，一般而言跑起来比RMSProp要好一些。但也可以试试SGD+Nesterov动量。 六、算法可视化下面两幅动画让我们直观感受一些优化算法的优化过程。 在第一幅动图中，我们看到他们随着时间推移在损失表面的轮廓（contours of a loss surface）的移动。注意到Adagrad、Adadelta和RMSprop几乎立刻转向正确的方向并快速收敛，但是Momentum和NAG被引导偏离了轨道。这让我们感觉就像看滚下山的小球。然而，由于NAG拥有通过远眺所提高的警惕，它能够修正他的轨迹并转向极小值。 第二幅动图中为各种算法在saddle point（鞍点）上的表现。所谓saddle point也就是某个维度是positive slope，其他维度为negative lope。前文中我们已经提及了它给SGD所带来的困难。注意到SGD、Momentum和NAG很难打破对称，虽然后两者最后还是逃离了saddle point。然而Adagrad, RMSprop, and Adadelta迅速地沿着negative slope下滑。 七、二阶方法在深度网络背景下，第二类常用的最优化方法是基于牛顿法的，其迭代如下： \displaystyle x\leftarrow x-[Hf(x)]^{-1}\nabla f(x)这里$Hf(x)$是Hessian矩阵，它是函数的二阶偏导数的平方矩阵。$\nabla f(x)$是梯度向量，这和梯度下降中一样。直观理解上，Hessian矩阵描述了损失函数的局部曲率，从而使得可以进行更高效的参数更新。具体来说，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进。需要重点注意的是，在这个公式中是没有学习率这个超参数的，这相较于一阶方法是一个巨大的优势。 然而上述更新方法很难运用到实际的深度学习应用中去，这是因为计算（以及求逆）Hessian矩阵操作非常耗费时间和空间。举例来说，假设一个有一百万个参数的神经网络，其Hessian矩阵大小就是[1,000,000 x 1,000,000]，将占用将近3,725GB的内存。这样，各种各样的拟-牛顿法就被发明出来用于近似转置Hessian矩阵。在这些方法中最流行的是L-BFGS，L-BFGS使用随时间的梯度中的信息来隐式地近似（也就是说整个矩阵是从来没有被计算的）。 然而，即使解决了存储空间的问题，L-BFGS应用的一个巨大劣势是需要对整个训练集进行计算，而整个训练集一般包含几百万的样本。和小批量随机梯度下降（mini-batch SGD）不同，让L-BFGS在小批量上运行起来是很需要技巧，同时也是研究热点。 实践时在深度学习和卷积神经网络中，使用L-BFGS之类的二阶方法并不常见。相反，基于（Nesterov的）动量更新的各种随机梯度下降方法更加常用，因为它们更加简单且容易扩展。 参考资料[1] Kiefer, J., Wolfowitz, J., et al. (1952). Stochastic estimation of the maximum of a regression function. The Annals of Mathematical Statistics, 23(3):462–466.[2] Nesterov, Y. (1983). A method of solving a convex programming problem with convergence rate o (1/k2). In Soviet Mathematics Doklady, volume 27, pages 372–376.[3] Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.[4] Hinton. Neural Networks for Machine Learning[5] Zeiler, M. D. (2012). Adadelta: An adaptive learning rate method.[6] Kingma, D. and Ba, J. (2014). Adam: A method for stochastic optimization.[7] CS231n Convolutional Neural Networks for Visual Recognition.[8] Sebastian Ruder. An overview of gradient descent optimization algorithms]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>优化方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（6）：递归神经网络]]></title>
    <url>%2F2017%2F04%2F26%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%886%EF%BC%89%EF%BC%9A%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[上一篇我们学习了循环神经网络，它可以用来处理包含序列的信息。然而，除此之外，信息往往还存在着诸如树结构、图结构等更复杂的结构。对于这种复杂的结构。循环神经网络就无能为力了。本文学习一种更为强大、复杂的神经网络：递归神经网络（Recursive Neural NetWork，RNN），以及它的训练算法BPTS（Back Propagation Through Structure）。顾名思义，递归神经网络可以处理诸如树、图这样的递归网络。 一、递归神经网络的定义因为神经网络的输入层单元个数是固定的，因此必须用循环或者递归的方式来处理长度可变的输入。循环神经网络实现了前者，通过将长度不定的输入分割为等长度的小块，然后再依次的输入到网络中，从而实现了神经网络对变长输入的处理。一个典型的例子是，当我们处理一句话的时候，我们可以把一句话看作是词组成的序列，然后，每次向循环神经网络输入一个词，如此循环直至整句话输入完毕，循环神经网络将产生对应的输出。如此，我们就能处理任意长度的句子了。如下图所示：然而，有时候把句子看作是词的序列是不够的，比如下面这句话“两个外语学院的学生”：上图显示了这句话的两个不同的语法解析树。可以看出这句话有歧义，不同的语法解析树则对应了不同的意思。一个是『两个外语学院的/学生』，也就是学生可能有许多，但他们来自于两所外语学校；另一个是『两个/外语学院的学生』，也就是只有两个学生，他们是外语学院的。为了能够让模型区分出两个不同的意思，我们的模型必须能够按照树结构去处理信息，而不是序列，这就是递归神经网络的作用。当面对按照树/图结构处理信息更有效的任务时，递归神经网络通常都会获得不错的结果。 递归神经网络可以把一个树、图结构信息编码为一个向量，也就是把信息映射到一个语义向量空间中。这个语义向量空间满足某类性质，比如语义相似的向量距离更近。也就是说，如果两句话（尽管内容不容）它的意思是相似的，那么把它们分别编码后的两个向量的距离也更近；反之，如果两句话的意思截然不同，那么编码后的距离则更远。如下图所示： 从上图我们可以看到，递归神经网络将所有的词、句都映射到一个2维向量空间中。句子“the country of my birth”和句子“the place where I was born”的意思是非常接近的，所以表示它们的两个向量在向量空间中的距离很近。另外两个词“Germany”和“France”因为表示的都是地点，它们的向量与上面两句话的向量的距离，就比另外两个表示时间的词“Monday”和“Tuesday”的向量的距离近得多。这样，通过向量的距离，就得到了一种语义的表示。 上图还显示了自然语言可组合的性质：词可以组成句、句可以组成段落、段落可以组成篇章，而更高层的语义取决于底层的语义以及它们的组合方式。递归神经网络是一种表示学习，它可以将词、句、段、篇按照他们的语义映射到同一个向量空间中，也就是把可组合（树/图结构）的信息表示为一个个有意义的向量。比如上面这个例子，递归神经网络把句子”the country of my birth”表示为二维向量[1,5]。有了这个『编码器』之后，我们就可以以这些有意义的向量为基础去完成更高级的任务（比如情感分析等）。如下图所示，递归神经网络在做情感分析时，可以比较好的处理否定句，这是胜过其他一些模型的：在上图中，蓝色表示正面评价，红色表示负面评价。每个节点是一个向量，这个向量表达了以它为根的子树的情感评价。比如”intelligent humor”是正面评价，而”care about cleverness wit or any other kind of intelligent humor”是中性评价。我们可以看到，模型能够正确的处理doesn’t的含义，将正面评价转变为负面评价。 尽管递归神经网络具有更为强大的表示能力，但是在实际应用中并不太流行。其中一个主要原因是，递归神经网络的输入是树/图结构，而这种结构需要花费很多人工去标注。想象一下，如果我们用循环神经网络处理句子，那么我们可以直接把句子作为输入。然而，如果我们用递归神经网络处理句子，我们就必须把每个句子标注为语法解析树的形式，这无疑要花费非常大的精力。很多时候，相对于递归神经网络能够带来的性能提升，这个投入是不太划算的。 二、递归神经网络的前向计算接下来，我们详细介绍一下递归神经网络是如何处理树/图结构的信息的。在这里，我们以处理树型信息为例进行介绍。 递归神经网络的输入是两个子节点（也可以是多个），输出就是将这两个子节点编码后产生的父节点，父节点的维度和每个子节点是相同的。如下图所示： $c_1$和$c_2$分别是表示两个子节点的向量，$p$是表示父节点的向量。子节点和父节点组成一个全连接神经网络，也就是子节点的每个神经元都和父节点的每个神经元两两相连。我们用矩阵$W$表示这些连接上的权重，它的维度将是$d×2d$，其中，$d$表示每个节点的维度。父节点的计算公式可以写成： p=\tan\textrm{h}\left(W\left[\begin{array}{c} c_1\\ c_2\\ \end{array}\right]+b\right)在上式中，tanh是激活函数（当然也可以用其它的激活函数），是偏置项，它也是一个维度为的向量。 然后，我们把产生的父节点的向量和其他子节点的向量再次作为网络的输入，再次产生它们的父节点。如此递归下去，直至整棵树处理完毕。最终，我们将得到根节点的向量，我们可以认为它是对整棵树的表示，这样我们就实现了把树映射为一个向量。在下图中，我们使用递归神经网络处理一棵树，最终得到的向量$p_3$，就是对整棵树的表示：举个例子，我们使用递归神将网络将”两个外语学校的学生”映射为一个向量，如下图所示：最后得到的向量$p_3$就是对整个句子”两个外语学校的学生”的表示。由于整个结构是递归的，不仅仅是根节点，事实上每个节点都是以其为根的子树的表示。比如，在左边的这棵树中，向量$p_2$是短语”外语学院的学生”的表示，而向量$p_1$是短语”外语学院的”的表示。 p=\tan\textrm{h}\left(W\left[\begin{array}{c} c_1\\ c_2\\ \end{array}\right]+b\right)该式就是递归神经网络的前向计算算法，它和全连接神经网络没有什么区别，只是在输入的过程中需要根据输入的树结构依次输入每个子节点。 需要特别注意的是，递归神经网络的权重$W$和偏置项$b$在所有节点都是共享的。 三、递归神经网络的训练递归神经网络的训练算法和循环神经网络类似，两者不同之处在于，前者需要将残差$\delta$从根节点反向传播到各个子节点，而后者是将残差$\delta$从当前时刻$t_k$反向传播到初始时刻$t_1$。 下面，我们介绍适用于递归神经网络的训练算法，也就是BPTS算法。 3.1 误差项的传递首先，我们先推导将误差从父节点传递到子节点的公式，如下图：定义$\delta_p$为误差函数E相对于父节点$p$的加权输入$net_p$的导数，即： \delta_p=\frac{\partial E}{\partial net_p}设$net_p$是父节点的加权输入，则 net_p=W\left[\begin{array}{c} c_1\\ c_2\\ \end{array}\right]+b在上述式子里，$net_p、c_1、c_2$都是向量，而$W$是矩阵。为了看清楚它们的关系，我们将其展开： \left[\begin{array}{c} net_{p1}\\ net_{p2}\\ ···\\ net_{pn}\\ \end{array}\right]=\left[\begin{matrix} w_{p1c11}& w_{p1c12}& ···& w_{p1c21}···\\ w_{p2c11}& w_{p2c12}& ···& w_{p2c21}···\\ ···& ···& ···& ···\\ w_{pnc11}& w_{pnc12}& ···& w_{pnc21}···\\ \end{matrix}\right]\left[\begin{array}{c} net_{c11}\\ net_{c12}\\ ···\\ net_{c21}\\ net_{c22}\\ ···\\ \end{array}\right]在上面的公式中，$p_i$表示父节点$p$的第i个分量；$c_{1i}$表示子节点的第i个分量；$c_{2i}$表示$c_2$子节点的第$i$个分量；$w_{p_ic_{jk} }$表子节点$c_j$的第k个分量到父节点p的第i个分量的权重。根据上面展开后的矩阵乘法形式，我们不难看出，对于子节点$c_{jk}$来说，它会影响父节点所有的分量。因此，我们求误差函数E对$c_{jk}$的导数时，必须用到全导数公式，也就是： \frac{\partial E}{\partial c_{jk}}=\sum_i{\frac{\partial E}{\partial net_{p_i}}\frac{\partial net_{p_i}}{\partial c_{jk}}}=\sum_i{\delta_{p_i}w_{p_ic_{jk}}}有了上式，我们就可以把它表示为矩阵形式，从而得到一个向量化表达： \frac{\partial E}{\partial c_j}=U_j\delta_p其中，矩阵$U_j$是从矩阵$W$中提取部分元素组成的矩阵。其单元为$u_{j_{ik}}=w_{p_k}c_{ji}$上式看上出可能有点抽象，从下图，我们可以直观的看到$U_j$到底是啥。首先我们把$W$矩阵拆分为两个矩阵$W_1$和$W_2$，如下图所示： 显然，子矩阵$W_1$和$W_2$分别对应子节点$c_1$和$c_2$的到父节点$p$权重。则矩阵$U_j$为： U_j=W_j^T也就是说，将误差项反向传递到相应子节点$c_j$的矩阵$U_j$就是其对应权重矩阵$W_j$的转置。 现在，我们设$net_{c_j}$是子节点$c_j$的加权输入，$f$是子节点$c$的激活函数，则： c_j=f(net_{c_j})这样，我们得到： \delta_{c_j}=\frac{\partial E}{\partial net_{c_j}}=\frac{\partial E}{\partial c_j}\frac{\partial c_j}{\partial net_{c_j}}=W_{j}^{T}\delta_p°f'\left(net_{c_j}\right)如果我们将不同子节点$c_j$对应的误差项$\delta_{c_j}$连接成一个向量 \delta_c=\left[\begin{array}{c} \delta_{c_1}\\ \delta_{c_2}\\ \end{array}\right]那么，上式可以写成 \delta_c=W^T\delta_p°f'\left(net_c\right)它就是将误差项从父节点传递到其子节点的公式。注意上式中的$net_c$也是将两个子结点的加权输入$net_{c_1}$和$net_{c_2}$连在一起的向量。有了传递一层的公式，我们就不难写出逐层传递的公式。上图是在树型结构中反向传递项的全景图，反复应用上式，在已知$\delta_p^{(3)}$的情况下，我们不难算出$\delta_p^{(1)}$为： \delta^{\left(2\right)}=W^T\delta_{p}^{\left(3\right)}°f'\left(net^{\left(2\right)}\right) \delta_{p}^{\left(2\right)}=\left[\delta^{\left(2\right)}\right]_p \delta^{\left(1\right)}=W^T\delta_{p}^{\left(2\right)}°f'\left(net^{\left(1\right)}\right) \delta_{p}^{\left(1\right)}=\left[\delta^{\left(1\right)}\right]_p在上面的公式中 \delta^{\left(2\right)}=\left[\begin{array}{c} \delta_{c}^{\left(2\right)}\\ \delta_{p}^{\left(2\right)}\\ \end{array}\right]$\left[\delta^{\left(2\right)}\right]_p$表示取向量$\delta^{(2)}$属于节点p的部分。 3.2 权重梯度的计算根据加权输入的计算公式： net_p^{(l)}=Wc^{(l)}+b其中，$net_p^{(l)}$表示第$l$层的父节点的加权输入，$c^{(l)}$表示第$l$层的子节点。W是权重矩阵，$b$是偏置项，将其展开可得： net_{p_j}^{l}=\sum_i{w_{ji}c_{i}^{l}}+b_j那么，我们可以求得误差函数在第$l$层对权重的梯度为： \frac{\partial E}{\partial w_{ji}^{\left(l\right)}}=\frac{\partial E}{\partial net_{p_j}^{\left(l\right)}}\frac{\partial net_{p_j}^{\left(l\right)}}{\partial w_{ji}^{\left(l\right)}}=\delta_{p_j}^{\left(l\right)}·c_{i}^{\left(l\right)}上式是针对一个权重项$w_{ji}$的公式，现在需要把它扩展为对所有的权重项的公式。我们可以把上式写成写成矩阵的形式（在下面的公式中，m=2n）: \frac{\partial E}{\partial W^{\left(l\right)}}=\delta^{\left(l\right)}·\left(c^{\left(l\right)}\right)^T这就是第$l$层权重项的梯度计算公式。我们知道，由于权重$W$是在所有层共享的，所以和循环神经网络一样，递归神经网络的最终权重梯度是各个层权重梯度之和。即： \frac{\partial E}{\partial W}=\sum_l{\frac{\partial E}{\partial W^{\left(l\right)}}}和循环神经网络一样，递归神经网络最终梯度之和是各层梯度之和。 接下来，我们求偏置项$b$的梯度计算公式。先计算误差函数对第$l$层偏置项$b^{(l)}$的梯度： \frac{\partial E}{\partial b_{j}^{\left(l\right)}}=\frac{\partial E}{\partial net_{p_j}^{\left(l\right)}}\frac{\partial net_{p_j}^{\left(l\right)}}{\partial b_{j}^{\left(l\right)}}=\delta_{p_j}^{\left(l\right)}把上式扩展为矩阵的形式： \frac{\partial E}{\partial b^{\left(l\right)}}=\delta_{p}^{\left(l\right)}最终的偏置项梯度是各个层偏置项梯度之和，即： \frac{\partial E}{\partial b}=\sum_l{\frac{\partial E}{\partial b^{\left(l\right)}}}3.3 权重更新如果使用梯度下降优化算法，那么权重更新公式为： W\gets W+\eta\frac{\partial E}{\partial W}其中，$\eta$是学习速率常数。把之前的式子代入上式，即可完成权重的更新。同理，偏置项的更新公式为： b\gets b+\eta\frac{\partial E}{\partial b}同样把之前求得式子代入上式，即可完成偏置项的更新。 这就是递归神经网络的训练算法BPTS。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>递归神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（5）：长短时记忆网络（LSTM）]]></title>
    <url>%2F2017%2F04%2F25%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%885%EF%BC%89%EF%BC%9A%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%EF%BC%88LSTM%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、长期依赖问题（Long-Term Dependencies）循环神经网络（RNN）在实际应用中很难处理长距离依赖的问题。 有的时候，我们仅仅需要知道先前的信息来完成预测任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词，比如我们预测“the clouds are in the sky”最后的词的时候，我们不需要任何其他的上下文，很显然下一个词就是sky。在这种情况下，相关的信息与需要预测的词位置之间的间隔很小，而RNN可以学会使用较近距离的信息。 但是到了一个更加复杂的场景，假设我们试着预测“I grew up in France……I speak fluent French”中最后的词，从这句话的信息来看，下一个词很有可能是一种语言的名字，但具体到是哪种语言，我们就需要在与之距离较远的“I grew up in France”中得到。这说明相关信息与当前预测位置之间的间隔就肯定变得相当的大。 不幸的是，在这个间隔不断增大时，RNN会丧失学习到连接如此远的信息的能力。 当然，在理论上，RNN绝对可以处理这样的长期依赖问题。人们可以通过调参来解决，但是在实践中，RNN肯定不能够成功学习到这些知识。Bengio, et al. (1994)等人对该问题进行了深入的研究，它们发现一些使训练RNN变得非常困难的相当根本的原因。 既然找到了问题的原因，那我们就能解决它。从问题的定位到解决，科学家们大概花了7、8年的时间。终于有一天，Hochreiter和Schmidhuber两位科学家发明出长短时记忆网络，一举解决了这个问题。 二、LSTM的核心思想Long Short Term网络，一般就叫做LSTM，是一种特殊的RNN变体，它可以学习长期依赖信息。LSTM由Hochreiter和Schmidhuber在1997年提出，并在近期被Alex Graves进行了改良和推广。在很多问题上，LSTM都取得了相当巨大的成功，并得到了广泛的使用。LSTM通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是LSTM的默认属性，而非需要付出很大的代价才能获得的能力！所有的RNN都具有一种重复神经网络模块的链式的形式。在标准的RNN中，这个重复的模块只有一个非常简单的结构，例如一个tanh层。LSTM同样是这样的结构，但是其中重复的模块拥有一个不同的结构。不同于单一神经网络层，这里有四个以非常特殊的方式进行交互的小器件。图中每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表pointwise的操作，比如向量的和，而黄色的矩阵就是学习到的神经网络层。 LSTM的关键在于细胞（Cell），水平线在细胞内贯穿运行。细胞类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在水平线上很容易保持不变。 LSTM通过精心设计“门”结构来去除或者增加信息到Cell上。门是一种让信息选择式通过的方法（过滤器）。它们包含一个sigmoid神经网络层和一个pointwise乘法操作。 Sigmoid层输出0到1之间的数值，描述每个部分有多少量可以通过。0代表“不许任何量通过”，1就指“允许任意量通过” 三、LSTM的前向计算LSTM用两个门来控制单元状态Cell的内容，一个是遗忘门（forget gate），它决定了上一时刻的单元状态$c_t-1$有多少保留到当前时刻$c_t$；另一个是输入门（input gate），他决定了当前时刻网络的输入$x_t$有多少保存到单元状态$c_t$。LSTM用输出门（output gate）来控制单元状态$c_t$有多少输出到LSTM的当前输出值$h_t$。 3.1 遗忘门我们先看一下遗忘门： f_t=\sigma(W_f·[h_{t-1,x_t}]+b_f)上式中，$W_f$是遗忘门的权重矩阵，$[h_{t-1},x_t]$表示把两个向量连接成一个更长的向量，$b_f$是遗忘门的偏置项，$\sigma$是sigmoid函数。若输入的维度是$d_x$，隐藏层的维度是$d_h$，单元状态的维度是$d_c$（通常$d_c=d_h$），则遗忘门的权重矩阵$W_f$维度是$d_c×(d_h+d_x)$。事实上，权重矩阵$W_f$都是两个矩阵拼接而成的：一个是$W_{fh}$，它对应着输入项$h_{t-1}$，其维度为$d_c×d_h$；一个是$W_{fx}$，它对应着输入项$x_t$，其维度为$d_c×d_x$。$W_f$可以写为： \left[W_f\right]\left[\begin{array}{c} h_{t-1}\\ x_t\\ \end{array}\right]=\left[\begin{matrix} W_{fh}& W_{fx}\\ \end{matrix}\right]\left[\begin{array}{c} h_{t-1}\\ x_t\\ \end{array}\right]=W_{fh}·h_{t-1}+W_{fx}x_t所以总结一下，遗忘门的作用为控制有多少上一时刻的memory cell中的信息可以累积到当前时刻的memory cell中。其数学公式可以写作： f_t = sigmoid(W_{fx}·x_t+W_{fh}·h_{t-1}+b_i)其计算图示如下： 3.2 输入门接下来看输入门： i_t=\sigma(W_i·[h_{t-1},x_t]+b_i)上式中，$W_i$是输入们的权重矩阵，$b_i$是输入门的偏置项。下图表示了输入门的计算： 接下来，我们计算用于描述当前输入的单元状态$\tilde{c}_t$，它是根据上一次的输出和本次输入来计算的： \tilde{c}_t=\tan\textrm{h}\left(W_c·\left[h_{t-1},x_t\right]+b_c\right)下图是$\tilde{c}_t$的计算：现在，我们计算当前时刻的单元状态$c_t$。它是由上一次的单元状态$c_{t-1}$按元素乘以遗忘门$f_t$，再用当前输入的单元状态$\tilde{c}_t$按元素乘以输入门$i_t$，再将两个积加和产生的： c_t=f_t°c_{t-1}+i_t°\tilde{c}_t下图是$c_t$的计算图示：这样，我们就把LSTM关于当前的记忆$\tilde{c}_t$和长期的记忆$c_{t-1}$组合在一起，形成了新的单元状态$c_t$。由于遗忘门的控制，它可以保存很久很久之前的信息，由于输入门的控制，它又可以避免当前无关紧要的内容进入记忆。 3.3 输出门下面，我们要看看输入门，它控制了长期记忆对当前输出的影响： o_t=\sigma(W_o·[h_{t-1},x_t]+b_o)下图表示输出门的计算：LSTM最终的输出，是由输出门和单元状态共同确定的： h_t=o_t°\tan\textrm{h}\left(c_t\right)下图表示LSTM最终输出的计算： 四、LSTM的训练LSTM的训练算法仍然是反向传播算法，它主要有下面三个步骤： 1）前向计算每个神经元的输出值，对于LSTM来说，即$f_t、i_t、c_t、o_t、h_t$五个向量的值。 2）反向计算每个神经元的误差项$\delta$值。与循环神经网络一样，LSTM误差项的反向传播也是包括两个方向：一个是沿着时间的反向传播，即从当前t时刻开始，计算每个时刻的误差项；一个是将误差项向上一层传播。 3）根据相应的误差项，计算每个权重的梯度。 首先，我们队推导中用到的一些公式、符号做一下必要的说明。 接下来的推导中，我们设定gate的激活函数为sigmoid函数，输出的激活函数为tanh函数。它们的导数分别为： \sigma\left(z\right)=y=\frac{1}{1+e^{-z}} \sigma '\left(z\right)=y\left(1-y\right) \tan\textrm{h}\left(z\right)=y=\frac{e^z-e^{-z}}{e^z+e^{-z}} \tan\textrm{h'}\left(z\right)=1-y^2从上面可以看出，sigmoid和tanh函数的导数都是原函数的函数。这样，我们一旦计算原函数的值，就可以用它来计算出导数的值。 LSTM需要学习的参数共有8组，分别是：遗忘门的权重矩阵$W_f$和偏置项$b_f$、输入门的权重矩阵$W_i$和偏置项$b_i$、输出门的权重矩阵$W_o$和偏置项$b_o$，以及计算单元状态的权重矩阵$W_c$和偏置项$b_c$，因为权重矩阵的两部分在反向传播中使用不同的公式，因此在后续的推导中，权重矩阵$W_f、W_i、W_c、W_o$都会被写成分开的两个矩阵：$W_{fh}、W_{fx}、W_{ih}、W_{ix}、W_{oh}、W_{ox}、W_{ch}、W_{cx}$。 我们解释一下按元素乘$o$符号。当$o$作用于两个向量时，运算如下： a°b=\left[\begin{array}{c} a_1\\ a_2\\ ···\\ a_n\\ \end{array}\right]°\left[\begin{array}{c} b_1\\ b_2\\ ···\\ b_n\\ \end{array}\right]=\left[\begin{array}{c} a_1b_1\\ a_2b_2\\ ···\\ a_nb_n\\ \end{array}\right]当$o$作用于一个向量和一个矩阵时，运算如下： a°X=\left[\begin{array}{c} a_1\\ a_2\\ ···\\ a_n\\ \end{array}\right]°\left[\begin{matrix} x_{11}& x_{12}& ···& x_{1n}\\ x_{21}& x_{22}& ···& x_{2n}\\ ···& ···& ···& ···\\ x_{n1}& x_{n2}& ···& x_{nn}\\ \end{matrix}\right]=\left[\begin{matrix} a_1x_{11}& a_1x_{12}& ···& a_{1n}x_{1n}\\ a_2x_{21}& a_2x_{22}& ···& a_2x_{2n}\\ ···& ···& ···& ···\\ a_nx_{n1}& a_nx_{n2}& ···& a_nx_{nn}\\ \end{matrix}\right]当$o$作用于两个矩阵时，两个矩阵对应位置的元素相乘。按元素乘可以再某些情况下简化矩阵和向量的运算。例如，当一个对角矩阵右乘一个矩阵时，相当于用对角矩阵的对角线组成的向量按元素乘那个矩阵：$diag[a]·X=a °X$当一个行向量右乘一个对角矩阵时，相当于这个行向量按元素乘那个矩阵对角线组成的向量： a^T·diag[b]=a°b上面这俩点，在后续推导中会多次用到。 在t时刻，LSTM的输出值为$h_t$。我们定义t时刻的误差项$\delta_t$为： \delta_t=\frac{\partial E}{\partial h_t}注意，这里假设误差项是损失函数对输出值的导数，而不是对加权输入$net^l$的导数。因为LSTM有四个加权输入，分别对应$f_t、i_t、c_t、o_t$，我们希望往上一层传递一个误差项而不是四个。但我们仍然要定义出这四个加权输入，以及他们对应的误差项。 net_{f,t}=W_f[h_{t-1},x_t]+b_f=W_{fh}h_{t-1}+W_{fx}x_t+b_fnet_{i,t}=W_i[h_{t-1},x_t]+b_i=W_{ih}h_{t-1}+W_{ix}x_t+b_inet_{\tilde{c},t}=W_c\left[h_{t-1},x_t\right]+b_c=W_{ch}h_{t-1}+W_{cx}x_t+b_cnet_{o,t}=W_o\left[h_{t-1},x_t\right]+b_o=W_{oh}h_{t-1}+W_{ox}x_t+b_o \delta_{f,t}=\frac{\partial E}{\partial net_{f,t}} \delta_{i,t}=\frac{\partial E}{\partial net_{i,t}} \delta_{\tilde{c},t}=\frac{\partial E}{\partial net_{c,t}} \delta_{o,t}=\frac{\partial E}{\partial net_{o,t}}4.1 误差项沿时间的反向传播沿时间反向传导误差项，就是要计算出$t-1$时刻的误差项$\delta_{t-1}$。 \delta_{t-1}^{T}=\frac{\partial E}{\partial h_{t-1}} =\frac{\partial E}{\partial h_t}\frac{\partial h_t}{\partial h_{t-1}} =\delta_{t}^{T}\frac{\partial h_t}{\partial h_{t-1}}我们知道，$\frac{\partial h_t}{\partial h_{t-1}}$是一个jacobian矩阵。如果隐藏层$h$的维度是N的话，那么它就是一个$N×N$矩阵。为了求出它，我们列出$h_t$的计算公式： c_t=f_t°c_{t-1}+i_t°\tilde{c}_th_t=o_t°\tan\textrm{h}\left(c_t\right)显然，$o_t、f_t、i_t、\tilde{c}_t$都是$h_{t-1}$的函数，那么，利用全导数公式可得： 4.2 将误差项传递到上一层4.3 权重梯度的计算五、LSTM的变体—GRU（Gated Recurrent Unit）前面我们讲了一种最为普通的LSTM，事实上LSTM存在很多变体，许多论文中的LSTM都或多或少的不太一样。只要遵守几个关键点，就可以根据需求设计需要的Gated RNNS。在众多的LSTM变体中，GRU也许是最成功的一种。它对LSTM做了很多简化，同时却保持着和LSTM相同的效果。因此，GRU最近变得越来越流行。 GRU对LSTM做了两个大改动： 1）将输入门、遗忘门、输出门变为两个门：更新门（Update Gate）$z_t$和重置门（Reset Gate）$r_t$。 2）将单元状态与输出合并为一个状态：$h$ GRU的前向计算公式为： z_t=\sigma(W_z·[h_{t-1},x_t])r_t=\sigma(W_r·[h_{t-1},x_t]) \tilde{h}_t=\tan\textrm{h}\left(W·\left[r_t°h_{t-1},x_t\right]\right) h=\left(1-z_t\right)°h_{t-1}+z_t°\tilde{h}_t下图是GRU的示意图：GRU的训练算法比LSTM相对也要简单一些 当然还有很多其他的变体，如 Gers &amp; Schmidhuber (2000) 提出的LSTM变体增加了“peephole connection”；另一种变体使用coupled 遗忘和输入门对遗忘和需要的信息一同做出决定。Yao, et al. (2015) 提出的Depth Gated RNN。还有用一些完全不同的观点来解决长期依赖的问题，如Koutnik, et al. (2014) 提出的Clockwork RNN。 但Greff, et al. (2015)给出了流行变体的比较，结论是它们基本上是一样的。Jozefowicz, et al. (2015) 则在超过一万种RNN架构上进行了测试，发现一些架构在某些任务上也取得了比LSTM更好的结果。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>长短时记忆网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（4）：循环神经网络（RNN）]]></title>
    <url>%2F2017%2F04%2F23%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88RNN%EF%BC%89%2F</url>
    <content type="text"><![CDATA[之前学习了全连接神经网络和卷积神经网络，以及它们的训练与应用。它们都只能单独的去处理单个的输入，且前后的输入之间毫无关系。但是，在一些任务中，我们需要更好的去处理序列的信息，即前后的输入之间存在关系。比如，在理解一整句话的过程中，孤立理解组成这句话的词是不够的，我们需要整体的处理由这些词连接起来的整个序列；当我们处理视频时，我们也不能单独地仅仅分析每一帧，而要分析这些帧连接起来的整个序列。这就引出了深度学习领域中另一类非常重要的神经网络：循环神经网络（Recurrent Neural Network）。 一、语言模型RNN是在自然语言处理领域中最先使用的，如RNN可以为语言模型来建模。那么，何为语言模型？ 自然语言从它产生开始，逐渐演变成一种上下文相关的信息表达和传递的方式，因此让计算机处理自然语言，一个基本的问题就是为自然语言这种上下文相关的特性建立数学模型。这个数学模型就是在自然语言处理中常说的统计语言模型（Statistical Language Model）。它最先由贾里尼克提出。 我们可以和电脑玩一个游戏，我们写出一个句子前面的一些词，然后，让电脑帮我们写下接下来的一个词，比如下面这句： 我昨天上学迟到了，老师批评了____。 我们给电脑展示了这句话前面这些词，然后让电脑写下接下来的一个词。在这个例子中，接下来的这个词最有可能是“我”，而不可能是“小明”，甚至是“吃饭”。语言模型的出发点很简单：一个句子是否合理，就看看它的可能性大小如何。 语言模型有很多用处，比如在语音转文本（STT）的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要语言模型来从这些候选词中选择一个最有可能的。当然，它同样也可以用在图像到文本的识别中（OCR技术）。 在使用RNN之前，语言模型主要是采用N-Gram。N可以是一个自然数，比如2或者3.它的含义是，假设一个词出现的概率只和前面N个词相关。我们以2-Gram为例。首先，对前面的一句话进行切词： 我 昨天 上学 迟到 了 ， 老师 批评 了 ____。 如果用2-Gram进行建模，那么电脑在预测时，只会看到前面的“了”，然后，电脑会在语料库中，搜索“了”后面最有可能的一个词。不管最后电脑选的是不是“我”，这个模型看起来并不是那么靠谱，因为“了”前面的一大堆实际上丝毫没起作用。如果是3-Gram模型呢，会搜索“批评了”后面最有可能的词，看齐俩感觉比2-Gram靠谱了不少，但还是远远不够的。因为这句话最关键的信息“我”，远在9个词之前！ 似乎我们可以不断提升N的值，比如4-Gram、9-Gram·······。实际上，这个想法是没有实用性的。在实际应用中最多的是N=3的三元模型，更高阶的模型就很少使用了。主要有两个原因。首先，N元模型的大小（或者说空间复杂度）几乎是N的指数函数，即$O(|V|^N)$，这里$|V|$是一种语言词典的词汇量，一般在几万到几十万个。然后，使用N元模型的速度（或者说时间复杂度）也几乎是一个指数函数，即$O(|V|^{N-1})$。因此N不能太大。当N从1到2，再从2到3时，模型的效果上升显著。而当模型从3到4时，效果的提升就不是很显著了，而资源的耗费增加却非常快，所以，除非是不惜资源为了做到极致，很少有人使用四元以上的模型。Google的罗塞塔翻译系统和语言搜索系统，使用的是四元模型，该模型存储于500台以上的Google服务器中。 RNN就解决了N-Gram的缺陷，它在理论上可以往前看（往后看）任意多个词。 二、基本循环神经网络开始前，我们先回顾一下，简单的MLP三层神经网络模型：其中x是一个向量，它表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的权重矩阵；o也是一个向量，它表示输出层的值；V是隐藏层到输出层的权重矩阵。 再看下图中一个简单的循环神经网络图，它由输入层、一个隐藏层和一个输出层组成。我们可以看到，循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵W就是隐藏层上一次的值作为这一次的输入的权重。如果我们把上面的图展开，循环神经网络也可以画成下面这个样子： 现在看起来就清楚不少了，这个网络在t时刻接收到输入$X_t$之后，隐藏层的值是$S_t$，输出值是$o_t$。关键一点是，$s_t$的值不仅仅取决于$X_t$，还取决于$S_{t-1}$。我们可以使用下面的公式来表示循环神经网络的计算方法： o_t=g(Vs_t)\\s_t=f(Ux_t+Ws_{t-1})式1是输出层的计算公式，输出层是一个全连接层，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的权重矩阵，g是激活函数。式2是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次的值$s_{t-1}$作为这一次的输入的权重矩阵，f是激活函数。 从上面的公式可以看出，循环层和全连接层的区别就是多了一个权重矩阵W。 若反复把式2代入带式1，我们将得到： o_t=g(Vs_t)=g(Vf(Ux_t+Ws_{t-1}))=g(Vf(Ux_t+Wf(Ux_{t-1}+Ws_{t-2 })))=g(Vf(Ux_t+Wf(Ux_{t-1}+Wf(Ux_{t-2}+Ws_{t-3 }))))从上面可以看出，循环神经网络的输出值$o_t$，是受前面历次输入值$x_{t}$、$x_{t-1}$、$x_{t-2} …$的影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。 三、双向循环神经网络对于语言模型来说，很多时候光看前面的词是不够的，比如下面这句话： 我的手机坏了，我打算____一部新手机。 可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的，但是如果我们也看到了后面的词是“一部新手机”，那么横线上的词填“买”的概率就大很多了。 而这个在单向循环神经网络是无法建模的，因此我们需要双向循环神经网络，如下图所示： 我们先考虑$y_2$的计算，从上图可以看出，双向卷积神经网络的隐藏层要保存两个值，一个A参与正向计算，另一个$A’$参与反向计算。最终的输出值$y_2$取决于$A_2$和$A_2’$，其计算方法为： y_2=g(VA_2+V'A_2')$A_2$和$A_2’$则分别计算： A_2=f(WA_1+Ux_2) A_2'=f(W'A_3'+U'x_2 )现在，我们已经可以看出一般的规律：正向计算时，隐藏层的值$s_t$与$s_{t-1}$有关；反向计算时，隐藏层的值$s_t’$与$s_{t+1}’$有关；最终的输出取决于正向和反向计算的加和。现在，我们仿照式1和式2，写出双向循环神经网络的计算方法： o_t=g(Vs_t+V's_t') s_t=f(Ux_t+Ws_{t-1 }) s_t'=f(U'x_t+W's_{t+1}')从上面三个公式我们可以看到，正向计算和反向计算不共享权重，也就是说$U$和$U’$、$W$和$W’$、$V$和$V’$都是不同的权重矩阵。 四、深度循环神经网络前面我们介绍的循环神经网络只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了深度循环神经网络。如下图所示： 我们把第$i$个隐藏层的值表示为$s_t^{(i)}、s_t^{‘(i)}$，则深度循环神经网络的计算方式可以表示为： o_t=g(V^{(i)}s_t^{(i)}+V^{'(i)}s_t^{'(i)}) s_t^{(i)}=f(U^{(i)}s_t^{i-1}+W^{(i)}) s_t^{'(i)}=f(U^{'(i)}s_t^{'(i-1)}+W^{'(i)}s_{t+1}')··· s_t^{(1)}=f(U^{(1)}x_t+W^{(1)}s_{t-1}) s_t^{'(1)}=f(U^{'(1)}x_t+W^{'(1)}s_{t+1}')五、循环神经网络的训练算法：BPTTBPTT算法是针对循环层的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤： 1）前向计算每个神经元的输出值； 2）反向计算每个神经元的误差项$\delta_j$，它是误差函数E对神经元$j$的加权输入$net_j$的偏导数； 3）计算每个权重的梯度。 4）最后再用随机梯度下降算法更新权重。 循环层如下图所示： 5.1 前向计算使用前面的式2对循环层进行前向计算： s_t=f(Ux_t+Ws_{t-1})注意，上面的$s_t、x_t、s_{t-1}$都是向量，用黑体字表示；而$U、V$是矩阵，用大写字母表示。向量的下标表示时刻，例如，$s_t$表示在$t$时刻向量$s$的值。 我们假设输入向量x的维度是$m$，输出向量的维度是$n$，则矩阵$U$的维度是$n×m$，矩阵$W$的维度是$n×n$。下面是上式展开成矩阵的样子，看起来更直观一点： \left[\begin{array}{c} s_{1}^{t}\\ s_{2}^{t}\\ ·\\ s_{n}^{t}\\ \end{array}\right]=f\left(\left[\begin{matrix} u_{11}& u_{12}& ··& u_{1m}\\ u_{21}& u_{22}& ··& u_{2m}\\ ·& ·& ·& ·\\ u_{n1}& u_{n2}& ···& u_{nm}\\ \end{matrix}\right]\left[\begin{array}{c} x_1\\ x_2\\ ···\\ x_m\\ \end{array}\right]+\left[\begin{matrix} w_{11}& w_{12}& ···& w_{1n}\\ w_{21}& w_{22}& ···& w_{2n}\\ ··& ··& ···& ··\\ w_{n1}& w_{n2}& ···& w_{nn}\\ \end{matrix}\right]\left[\begin{array}{c} s_{1}^{t-1}\\ s_{2}^{t-1}\\ ···\\ s_{n}^{t-1}\\ \end{array}\right]\right)在这里我们用手写体字母表示向量的一个元素，它的下标表示它是这个向量的第几个元素，它的上标表示第几个时刻。例如，$s_j^t$表示向量$s$的第$j$个元素在$t$时刻的值。$u_{ji}$表示输入层第$i$个神经元到循环层第$j$个神经元的权重。$w_{ji}$表示循环层第$t-1$时刻的第$i$个时刻的第$j$个神经元的权重。 5.2 误差项的计算BTPP算法将第$l$层$t$时刻的误差项$\delta _t^l$值沿两个方向传播，一个方向是其传递到上一层网络，得到$\delta _t^{l-1}$，这部分只和权重矩阵$U$有关；另一个方向是将其沿着时间线传递到初始$t_1$时刻，得到$\delta_1^l$，这部分只和权重矩阵$W$有关。 我们用向量$net_j$表示神经元在$t$时刻的加权输入，因为： net_j=Ux_t+Ws_{t-1}\\s_{t-1}=f(net_{t-1})因此： \frac{\partial net_t}{\partial net_{t-1}}=\frac{\partial net_t}{\partial s_{t-1}}\frac{\partial s_{t-1}}{\partial net_{t-1}}我们用$a$表示列向量，用$a^T$表示行向量。上式的第一项是向量函数对向量求导，其结果为Jacobian矩阵： \frac{\partial net_t}{\partial s_{t-1}}=\left[\begin{matrix} w_{11}& w_{12}& ···& w_{1n}\\ w_{21}& w_{22}& ···& w_{2n}\\ ···& ···& ···& ···\\ w_{n1}& w_{n2}& ···& w_{nn}\\ \end{matrix}\right]=W上式第二项也是一个jacobian矩阵： \frac{\partial s_{t-1}}{\partial net_{t-1}}=\left[\begin{matrix} \frac{\partial s_{1}^{t-1}}{\partial net_{1}^{t-1}}& \frac{\partial s_{1}^{t-1}}{\partial net_{2}^{t-1}}& ···& \frac{\partial s_{1}^{t-1}}{\partial net_{n}^{t-1}}\\ \frac{\partial s_{2}^{t-1}}{\partial net_{1}^{t-1}}& \frac{\partial s_{2}^{t-1}}{\partial net_{2}^{t-1}}& ···& \frac{\partial s_{2}^{t-1}}{\partial net_{n}^{t-1}}\\ ···& ···& ···& ···\\ \frac{\partial s_{n}^{t-1}}{\partial net_{1}^{t-1}}& \frac{\partial s_{n}^{t-1}}{\partial net_{2}^{t-1}}& ···& \frac{\partial s_{n}^{t-1}}{\partial net_{n}^{t-1}}\\ \end{matrix}\right] =\left[\begin{matrix} f'\left(net_{1}^{t-1}\right)& 0& ···& 0\\ 0& f'\left(net_{2}^{t-1}\right)& 0& 0\\ 0& 0& ···& 0\\ 0& 0& 0& f'\left(net_{n}^{t-1}\right)\\ \end{matrix}\right] =diag\left[f'\left(net_{t-1}\right)\right]最后，将俩项合在一起，可得： \frac{\partial net_t}{\partial net_{t-1}}=\frac{\partial net_t}{\partial s_{t-1}}\frac{\partial s_{t-1}}{\partial net_{t-1}}=W·diag\left[f'\left(net_{t-1}\right)\right]上式描述了将$\delta$沿时间往前传递一个时刻的规律，有了这个规律，我们就可以求得任意时刻$k$的误差项$\delta_k$： \delta_{k}^{T}=\frac{\partial E}{\partial net_k}=\frac{\partial E}{\partial net_t}·\frac{\partial net_t}{\partial net_{t-1}}·\frac{\partial net_{t-1}}{\partial net_{t-2}}···\frac{\partial net_{k+1}}{\partial net_k} =\delta_{t}^{T}Wdiag\left[f'\left(net_{t-1}\right)\right]Wdiag\left[f'\left(net_{t-2}\right)\right]···Wdiag\left[f'\left(net_k\right)\right] =\delta_{t}^{T}\prod_{i=k}^{t-1}{Wdiag\left[f'\left(net_i\right)\right]}这个就是将误差项沿着时间反向传播的算法。 循环层将误差项反向传递到上一层网络，与普通的全连接层是完全一样的，在此简要描述一下：循环曾的加权输入$net^l$与上一层的加权输入$net^{l-1}$关系如下： net^l_t=Ua_t^{l-1}+Ws_{t-1} a_t^{l-1}=f^{l-1}(net_t^{l-1})上式中$net_t^l$是第$l$层神经元的加权输入（假如第$l$是循环层）；$net_t^{l-1}$是$l-1$层神经元的加权输入；$a_t^{l-1}$是第$l-1$层神经元的输出；$f^{l-1}$是第$l-1$层的激活函数。 \frac{\partial net_{t}^{l}}{\partial net_{t}^{l-1}}=\frac{\partial net_{t}^{l}}{\partial a_{t}^{l-1}}\frac{\partial a_{t}^{l-1}}{\partial net_{t}^{l-1}}=U ·diag\left[f'^{l-1}\left(net_{t}^{l-1}\right)\right]所以 \delta_{t}^{l-1}=\frac{\partial E}{\partial net_{t}^{l-1}}=\frac{\partial E}{\partial net_{t}^{l}}\frac{\partial net_{t}^{l}}{\partial net_{t}^{l-1}} =\delta_{t}^{l}·U·diag\left[f'^{l-1}\left(net_{t}^{l-1}\right)\right]上式就是将误差项传递到上一层算法。 5.3 权重梯度的计算接下来是BPTT算法的最后一步：计算每个权重的梯度。首先我们计算误差函数$E$对权重矩阵$W$的梯度 \frac{\partial E}{\partial W} 上图展示了我们到目前为止，在前两步中已经计算得到的量，包括每个时刻$t$循环层的输出值$s_t$，以及误差项$\delta_t$。 我们只要知道了任意一个时刻的误差项$\delta_t$，以及上一个时刻循环层的输出值$s_{t-1}$，就可以按照下面的公式求出权重矩阵在$t$时刻的梯度： \nabla_{w_t}E=\left[\begin{matrix} \delta_{1}^{t}s_{1}^{t-1}& \delta_{1}^{t}s_{2}^{t-1}& ···& \delta_{1}^{t}s_{n}^{t-1}\\ \delta_{2}^{t}s_{1}^{t-1}& \delta_{2}^{t}s_{2}^{t-1}& ···& \delta_{2}^{t}s_{n}^{t-1}\\ ···& ···& ···& ···\\ \delta_{n}^{t}s_{1}^{t-1}& \delta_{n}^{t}s_{2}^{t-1}& ···& \delta_{n}^{t}s_{n}^{t-1}\\ \end{matrix}\right]上式中，$\delta_i^t$表示$t$时刻误差项向量的第$i$个分量；$s_i^{t-1}$表示$t-1$时刻循环层第$i$个神经元的输出值。 下面我们简单推导一下上式。 我们知道 net_t=Ux_t+Ws_{t-1} \left[\begin{array}{c} net_{1}^{t}\\ net_{2}^{t}\\ ·\\ net_{n}^{t}\\ \end{array}\right]=Ux_t+\left[\begin{matrix} w_{11}& w_{12}& ···& w_{1n}\\ w_{21}& w_{22}& ···& w_{2n}\\ ···& ···& ···& ···\\ w_{n1}& w_{n2}& ···& w_{nn}\\ \end{matrix}\right]\left[\begin{array}{c} s_{1}^{t-1}\\ s_{2}^{t-1}\\ ···\\ s_{n}^{t-1}\\ \end{array}\right] =Ux_t+\left[\begin{array}{c} w_{11}s_{1}^{t-1}+w_{12}s_{2}^{t-1}+···+w_{1n}s_{n}^{t-1}\\ w_{21}s_{1}^{t-1}+w_{22}s_{2}^{t-1}+···+w_{2n}s_{n}^{t-1}\\ ···\\ w_{n1}s_{1}^{t-1}+w_{n2}s_{2}^{t-1}+···+w_{nn}s_{n}^{t-1}\\ \end{array}\right]因为对$W$求导与$Ux_t$无关，我们不加考虑。现在，我们考虑对权重项$w_{ji}$求导。通过观察上式我们可以看到$w_{ji}$只与$net^t_j$有关，所以： \frac{\partial E}{\partial w_{ji}}=\frac{\partial E}{\partial net_{j}^{t}}\frac{\partial net_{j}^{t}}{\partial w_{ji}}=\delta_{j}^{t}s_{i}^{t-1}按照这个规律就可以生成梯度矩阵$\nabla_{w_t}E$了。 我们已经求得权重矩阵$W$在$t$时刻的梯度$\nabla_{w_t}E$，最终的梯度$\nabla_{w_t}E$是各个时刻的梯度之和： =\left[\begin{matrix} \delta_{1}^{t}s_{1}^{t-1}& \delta_{1}^{t}s_{2}^{t-1}& ···& \delta_{1}^{t}s_{n}^{t-1}\\ \delta_{2}^{t}s_{1}^{t-1}& \delta_{2}^{t}s_{2}^{t-1}& ···& \delta_{2}^{t}s_{n}^{t-1}\\ ···& ···& ···& ···\\ \delta_{n}^{t}s_{1}^{t-1}& \delta_{n}^{t}s_{2}^{t-1}& ···& \delta_{n}^{t}s_{n}^{t-1}\\ \end{matrix}\right]+···+\left[\begin{matrix} \delta_{1}^{1}s_{1}^{0}& \delta_{1}^{1}s_{2}^{0}& ···& \delta_{1}^{1}s_{n}^{0}\\ \delta_{2}^{1}s_{1}^{0}& \delta_{2}^{1}s_{2}^{0}& ···& \delta_{2}^{1}s_{n}^{0}\\ ···& ···& ···& ···\\ \delta_{n}^{1}s_{1}^{0}& \delta_{n}^{1}s_{2}^{0}& ···& \delta_{n}^{1}s_{n}^{0}\\ \end{matrix}\right]这就是计算循环曾权重矩阵$W$的梯度的公式。 前面介绍了权重梯度的计算方法，看上去比较直观。但为什么最终的梯度的是各个时刻的梯度之和呢？我们前面只是直接用了这个结论，实际上这里面是有道理的。 我们从这个式子开始：net_t=Ux_t+Wf(net_{t-1})因为$Ux_t$与$W$完全无关，我们把它看做常量。现在，考虑第一个式子加号右边的部分，因为$W$和$f(net_{t-1})$都是$W$的函数，所以，对其求偏导得到： \frac{\partial net_t}{\partial W}=\frac{\partial W}{\partial W}f\left(net_{t-1}\right)+W\frac{\partial f\left(net_{t-1}\right)}{\partial W}我们最终需要计算的是 \nabla_WE=\frac{\partial E}{\partial W}=\frac{\partial E}{\partial net_t}\frac{\partial net_t}{\partial W}=\delta_{t}^{T}\frac{\partial W}{\partial W}f\left(net_{t-1}\right)+\delta_{t}^{T}W\frac{\partial f\left(net_{t-1}\right)}{\partial W}我们先计算加号左边的部分。$\frac{\partial W}{\partial W}$是矩阵对矩阵求导，其结果是一个四维张量（tensor），如下所示： \frac{\partial W}{\partial W}=\left[\begin{matrix} \frac{\partial w_{11}}{\partial W}& \frac{\partial w_{12}}{\partial W}& ···& \frac{\partial w_{1n}}{\partial W}\\ \frac{\partial w_{21}}{\partial W}& \frac{\partial w_{22}}{\partial W}& ···& \frac{\partial w_{2n}}{\partial W}\\ ···& ···& ···& ···\\ \frac{\partial w_{n1}}{\partial W}& \frac{\partial w_{n2}}{\partial W}& ···& \frac{\partial w_{nn}}{\partial W}\\ \end{matrix}\right] =\left[\begin{matrix} \left[\begin{matrix} \frac{\partial w_{11}}{\partial w_{11}}& ···& \frac{\partial w_{11}}{\partial w_{1n}}\\ ···& ···& ···\\ \frac{\partial w_{11}}{\partial w_{n1}}& ···& \frac{\partial w_{11}}{\partial w_{nn}}\\ \end{matrix}\right]& ···& ···& \left[\begin{matrix} \frac{\partial w_{1n}}{\partial w_{11}}& ···& \frac{\partial w_{1n}}{\partial w_{1n}}\\ ···& ···& ···\\ \frac{\partial w_{1n}}{\partial w_{n1}}& ···& \frac{\partial w_{1n}}{\partial w_{nn}}\\ \end{matrix}\right]\\ ···& ···& ···& ···\\ ···& ···& ···& ···\\ \left[\begin{matrix} \frac{\partial w_{n1}}{\partial w_{11}}& ···& \frac{\partial w_{n1}}{\partial w_{1n}}\\ ···& ···& ···\\ \frac{\partial w_{n1}}{\partial w_{n1}}& ···& \frac{\partial w_{n1}}{\partial w_{nn}}\\ \end{matrix}\right]& ···& ···& \left[\begin{matrix} \frac{\partial w_{nn}}{\partial w_{11}}& ···& \frac{\partial w_{nn}}{\partial w_{1n}}\\ ···& ···& ···\\ \frac{\partial w_{nn}}{\partial w_{n1}}& ···& \frac{\partial w_{nn}}{\partial w_{nn}}\\ \end{matrix}\right]\\ \end{matrix}\right] =\left[\begin{matrix} \left[\begin{matrix} 1& 0& ···& 0\\ 0& 0& ···& 0\\ ···& ···& ·& ···\\ 0& 0& 0& 0\\ \end{matrix}\right]& \left[\begin{matrix} 0& 1& 0& 0\\ 0& 0& ···& 0\\ ···& ···& ···& 0\\ 0& 0& ···& 0\\ \end{matrix}\right]& ···& ···\\ ···& ···& ···& ···\\ ···& ···& ···& ···\\ ···& ···& ···& ···\\ \end{matrix}\right]接下来，我们知道$s_{t-1=f(net_{t-1})}$，它是一个列向量。我们让上面的四维张量与这个向量相乘，得到了一个三维张量，再左乘行向量$\delta_t^T$，最终得到一个矩阵： \delta_{t}^{T}\frac{\partial W}{\partial W}f\left(net_{t-1}\right)=\delta_{t}^{T}\left[\begin{matrix} \left[\begin{matrix} 1& 0& ···& 0\\ 0& 0& ···& 0\\ ···& ···& ·& ···\\ 0& 0& 0& 0\\ \end{matrix}\right]& \left[\begin{matrix} 0& 1& 0& 0\\ 0& 0& ···& 0\\ ···& ···& ···& 0\\ 0& 0& ···& 0\\ \end{matrix}\right]& ···& ···\\ ···& ···& ···& ···\\ ···& ···& ···& ···\\ ···& ···& ···& ···\\ \end{matrix}\right]\left[\begin{array}{c} s_{1}^{t-1}\\ s_{2}^{t-1}\\ ···\\ s_{n}^{t-1}\\ \end{array}\right] =\left[\begin{matrix} \delta_{1}^{t}& \delta_{2}^{t}& ···& \delta_{n}^{t}\\ \end{matrix}\right]\left[\begin{matrix} \left[\begin{array}{c} s_{1}^{t-1}\\ 0\\ ···\\ 0\\ \end{array}\right]& \left[\begin{array}{c} s_{2}^{t-1}\\ 0\\ ···\\ 0\\ \end{array}\right]& ···& ···\\ ···& ···& ···& ···\\ ···& ···& ···& ···\\ ···& ···& ···& ···\\ \end{matrix}\right] =\left[\begin{matrix} \delta_{1}^{t}s_{1}^{t-1}& \delta_{1}^{t}s_{2}^{t-1}& ···& \delta_{1}^{t}s_{n}^{t-1}\\ \delta_{2}^{t}s_{1}^{t-1}& \delta_{2}^{t}s_{2}^{t-1}& ···& \delta_{2}^{t}s_{n}^{t-1}\\ ···& ···& ···& ···\\ \delta_{n}^{t}s_{1}^{t-1}& \delta_{n}^{t}s_{2}^{t-1}& ···& \delta_{n}^{t}s_{n}^{t-1}\\ \end{matrix}\right] =\nabla_{W_t}E接下来，我们计算加号右边的部分： \delta_{t}^{T}W\frac{\partial f\left(net_{t-1}\right)}{\partial W}=\delta_{t}^{T}W\frac{\partial f\left(net_{t-1}\right)}{\partial net_{t-1}}\frac{\partial net_{t-1}}{\partial W} =\delta_{t}^{T}Wf'\left(net_{t-1}\right)\frac{\partial net_{t-1}}{\partial W} =\delta_{t}^{T}\frac{\partial net_t}{\partial net_{t-1}}\frac{\partial net_{t-1}}{\partial W} =\delta_{t-1}^{T}\frac{\partial net_{t-1}}{\partial W}我们得到了如下递推公式： \nabla_WE=\frac{\partial E}{\partial W}=\nabla_{W_t}E+\delta_{t-1}^{T}\frac{\partial net_{t-1}}{\partial W} =\nabla_{W_t}E+\nabla_{W_{t-1}}E+\delta_{t-2}^{T}\frac{\partial net_{t-2}}{\partial W} =\nabla_{W_t}E+\nabla_{W_{t-2}}E+···+\nabla_{W_1}E =\sum_{k=1}^t{\nabla_{W_k}E}与权重矩阵$W$类似，我们可以得到权重矩阵$U$的计算方法。 \nabla_{U_t}E=\left[\begin{matrix} \delta_{1}^{t}x_{1}^{t}& \delta_{1}^{t}x_{2}^{t}& ···& \delta_{1}^{t}x_{m}^{t}\\ \delta_{2}^{t}x_{1}^{t}& \delta_{2}^{t}x_{2}^{t}& ···& \delta_{2}^{t}x_{m}^{t}\\ ···& ···& ···& ···\\ \delta_{n}^{t}x_{1}^{t}& \delta_{n}^{t}x_{2}^{t}& ···& \delta_{n}^{t}x_{m}^{t}\\ \end{matrix}\right]它是误差函数在$t$时刻对权重矩阵$U$的梯度。和权重矩阵$W$一样，最终的梯度也是各个时刻的梯度之和： \nabla_UE=\sum_{i=1}^t{\nabla_{U_i}E}具体地证明与上述类似。 六、梯度爆炸与梯度消失不幸的是，实践中前面介绍的集中RNNs并不能很好地处理较长的序列。一个主要的原因是，RNN在训练中很容易发生梯度爆炸和梯度消失，这导致训练时梯度不能再较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。 为什么RNN会产生梯度爆炸和梯度消失问题呢？我们根据下式来分析。之前推导过程中得到： \delta_{k}^{T}=\delta_{t}^{T}\prod_{i=k}^{t-1}{Wdiag\left[f'\left(net_i\right)\right]} ||\delta_{k}^{T}||\le ||\delta_{t}^{T}||\prod_{i=k}^{t-1}{||W||||diag\left[f'\left(net_i\right)\right]||} \le ||\delta_{t}^{T}||\left(\beta_W\beta_f\right)^{t-k}上式的$\beta$定义为矩阵的模的上界。因为上式是一个指数函数，如果$t_k$很大的话（也就是向前看得很远的时候），会导致对应的误差项的值增长或缩小的非常快，这样就会导致相应的梯度爆炸和梯度消失问题（取决于$\beta$大于1还是小于1）。 通常来说，梯度爆炸更容易处理一些。因为梯度爆炸时，我们的程序会收到NaN的错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。 梯度消失更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题： 1）合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。 2）使用ReLu代替sigmoid和tanh作为激活函数。 3）使用其他结构的RNNs，比如长短时记忆网络（LSTM）和Gated Recurrenr Unit（GRU），这是最流行的做法。 纠错词：]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>循环神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（3）：卷积神经网络（CNN）]]></title>
    <url>%2F2017%2F04%2F21%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[本文将要介绍一种更适合图像、语音识别任务的神经网络结构——卷积神经网络(Convolutional Neural Network, CNN)。说卷积神经网络是最重要的一种神经网络也不为过，它在最近几年大放异彩，几乎所有图像、语音识别领域的重要突破都是卷积神经网络取得的。它在 2012 年崭露头角，Alex Krizhevsky 凭借它们赢得了那一年的 ImageNet 挑战赛（大体上相当于计算机视觉的年度奥林匹克），他把分类误差记录从 26% 降到了 15%，在当时震惊了世界。自那之后，大量公司开始将深度学习用作服务的核心。Facebook 将神经网络用于自动标注算法、谷歌将它用于图片搜索、亚马逊将它用于商品推荐、Pinterest 将它用于个性化主页推送、Instagram 将它用于搜索架构。打败李世石的AlphaGo也用到了这种网络。本文将详细介绍卷积神经网络的结构以及它的训练算法 一、初识卷积神经网络1.1 全连接神经网络与卷积神经网络全连接神经网络之所以不太适合图像识别任务，主要有三个方面的问题： 1）参数数量太多：考虑一个输入为1000×1000像素的图片（100万像素，现在已经不能算大图了），输入层有100万个节点。假设第一个隐藏层有100个节点（这个数量并不多），那么仅这一层就有（1000×1000+1）×100=1亿 的参数，这实在是太多了！我们看到图像只扩大一点，参数数量就会多很多，因此其扩展性很差。显而易见，这种全连接方式效率低下，大量的参数也很快会导致过拟合。 2）没有利用像素之间的位置信息。对于图像识别任务来说，每个像素和其周围像素的联系是比较紧密的，和离得很远的像素的联系就比较小了。若一个神经元和上一层所有神经元相连，那么就相当于对于一个像素来说，把图像的所有像素都等同对待，这不符合实际情况。当我们完成每个连接权值的学习之后，最终可能会发现，有大量的权值，它们的值都是很小的也就是这些连接其实都是无关紧要的。努力学习大量并不重要的权值，这样的学习必将是非常低效的。我们的任务就是想识别这只猫，按照之前的做法就是把这幅图片转换成一个一维向量，然后作为神经网络的输入。对于人眼的物体识别来说，虽然对人眼识别物体的原理并没有研究明白，但绝不是通过把物体转换为一维向量再做识别的。一张图片必然存在着一定的位置关系，比如猫的鼻子下面有嘴巴、鼻子上面有眼睛，这些都是很明确的位置关系，但要是转换成了一维向量，这些位置关系就被掩盖了。 3）网络层数限制：我们知道网络层数越多，其表达能力越强，但是通过梯度下降方法训练深度全连接神经网络很困难，因为全连接神经网络的梯度很难传递超过三层。因此，我们不可能得到一个很深的全连接神经网络，也就限制了它的能力。 那卷积神经网络是怎么解决全连接神经网络的这些问题的呢？主要有以下几个方面。 1.2 激活函数——Relu最近几年卷积神经网络中，激活函数往往不选择sigmoid或者tanh函数，而是选择relu函数。relu函数的定义是： f\left(x\right)=\max\left(x,0\right)Relu函数图像如下所示： Relu函数作为激活函数，有以下几大优势： 1）速度快：采用sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大。而Relu函数其实就是一个max（x,0），整个过程的计算量节省很多。 2）减轻梯度消失问题：回忆一下计算梯度的公式$\nabla =\sigma^,\delta x$。其中$\sigma ^,$是sigmoid函数的导数。在使用反向传播算法进行梯度计算时，没经过一层sigmoid神经元，梯度就要乘上一个$\sigma ^, $。从下图可以看出，$\sigma^,$函数最大值是$\frac{1}{4}$。因此，乘一个$\sigma ^,$会导致梯度越来越小，这对于深层网络的训练是个很大的问题。而Relu函数的导数是1，不会导致梯度变小。当然，激活函数仅仅是导致梯度减小的一个因素，但无论如何在这方面Relu的表现强于sigmoid。使用Relu激活函数可以让你训练更深的网络。 3）通过对大脑的研究发现，大脑在工作的时候只有大约5%的神经元是激活的，而采用sigmoid激活函数的人工神经网络，其激活率大约是50%。有论文声称人工神经网络在15%~30%的激活率时是比较理想的。因为Relu函数在输入小于0的时候是完全不激活的，因此可以获得一个更低的激活率。 1.3 局部感受野（local receptive fields）在之前的全连接神经网络中，一个样例的输入被转换为一个一维向量。但在一个卷积网络中，把输入看作是一个按照28×28排列的正方形，或者当有颜色通道的时候，比如28x28x3，就是宽高都是28，且有3个颜色通道。比如下图就代表了一个输入 然后，我们通常把输入像素连接到一个隐藏层的神经元，但和全连接神经网络那样每个输入都连接一个隐藏层神经元不同的是，这里我们只是把输入图像进行局部的连接。如此不断地重复，构建起第一个隐藏层。注意如果我们有一个28×28的输入图像，5×5的局部感受野，那么隐藏层中就会有24×24个神经元。这是因为在抵达抵达最右边或最底部的输入图像之前，我们只能把局部感受野向右或向下移动23个神经元。 如上图所示，把图中间的那个看作是可以“滑动的窗口”，他的作用是和输入相应的“感受域”下的像素做运算得到新的值。这个运算就是“卷积”运算了。图上面有详细的运算过程。实际上就是每个相应元素的值相乘，然后把得到的都加起来。这个窗口的本质是其中的数字和一个偏置构成的，通常就把这个窗口叫做滤波器或者卷积核。上图是对于一个颜色通道的输入做卷积操作，但通常是三个颜色通道。中间那个“窗口”是可以滑动的，每次的滑动步长可以人为指定。 1.4 共享权值与偏置（Shared weights and biases）权值共享是指在一个模型的多个函数中使用相同的参数。 在传统的神经网络中，当计算一层的输出时，权值矩阵的每一个元素只使用一次， 当它乘以输入的一个元素后就再也不会用到了。而在卷积神经网络中，我们对24×24的隐藏层神经元的每一个使用相同的权重和偏置，这样可以很好地使用图像的平移不变性（例如稍稍移动一副猫的图像，它仍然是一副猫的图像）。因为这个原因，我们有时候把输入层到隐藏层的映射称为一个特征映射。把定义特征映射的权重称为共享权重，把以这种方式定义特征映射的偏置称为共享偏置。共享权值和偏置通常被称为一个卷积核或者滤波器。共享权值和偏置有一个很大的优点就是，它大大较少了参与卷积网络的参数，它的平移不变性将会使训练更快，有助于我们使用卷积层建立深度网络。 1.5 池化（pooling）在连续的卷积层之间会周期性地插入一个池化层。经过池化层前后，发生的变化如下图所示：它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。上图是一个MAX Pooling的过程，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。 二、卷积神经网络的层首先，让我们对卷积神经网络有一个感性的认识，下图就是一个卷积神经网络的示意图：如上图所示，一个神经网络由若干卷积层（CONV）、Pooling层（POOL）、全连接层（FC）组成。你可以构建各种不同的卷积神经网络，它的常用架构模式为： INPUT\rightarrow\left[\left[CONV\right]\times N\rightarrow POOL\right]\times M\rightarrow\left[FC\right]\times K也就是N个卷积层叠加，然后叠加一个Pooling层（可选），重复这个结构M次，最后叠加K个全连接层。 对于上图来说，该卷积神经网络的架构为： INPUT\rightarrow\left[\left[CONV\right]\times 1\rightarrow POOL\right]\times 2\rightarrow\left[FC\right]\times 2也就是$N=1,M=2,K=2$ 从中我们可以发现卷积神经网络和全连接神经网络的层结构有很大不同。全连接网络每层的神经元是按照一维排列的，也就是排成一条线的样子；而卷积神经网络每层的神经元是按照三维排列的，也就是排成一个长方体的样子，有宽度、高度和深度。 我们看到输入层的宽度和高度对应于输入图像的宽度和高度，而他的深度为1。接着第一个卷积层对这幅图像进行了卷积操作，得到了三个Feature Map。实际上这个卷积层包含三个Filter，也就是三套参数，每个Filter都可以把原始输入图像卷积得到一个Feature Map，三个Filter就可以得到三个Feature Map。至于一个卷积层可以有多少个Filter，那是可以自由设定的。也就是说，卷积层的Filter个数也是一个超参数。我们可以把Feature Map可以看做是通过卷积变换提取到的图像特征，三个Filter就对原始图像提取出三组不同的特征，也就是得到了三个Feature Map，也称做三个通道(channel)。 在第一个卷积层之后，Pooling层对三个Feature Map做了下采样，得到了三个更小的Feature Map。接着，是第二个卷积层，它有5个Filter。每个Fitler都把前面下采样之后的3个Feature Map卷积在一起，得到一个新的Feature Map。这样，5个Filter就得到了5个Feature Map。接着，是第二个Pooling，继续对5个Feature Map进行下采样，得到了5个更小的Feature Map。 最后两层是全连接层。第一个全连接层的每个神经元，和上一层5个Feature Map中的每个神经元相连，第二个全连接层(也就是输出层)的每个神经元，则和第一个全连接层的每个神经元相连，这样得到了整个网络的输出。 至此，我们对卷积神经网络有了最基本的感性认识。接下来，我们将介绍卷积神经网络中各种层的计算和训练。 2.1 卷积层卷积层的参数是一些可学习的滤波器集合（卷积核）构成，滤波器的宽度和高度一般不大，深度与其输入数据保持一致。见下图：在上面的过程中，原图像是32×32×3的图像，我们有一个滤波器（卷积核）为5×5×3，5×5的宽高相比起32×32来说，不怎么大，深度3和输入数据保持一致。一个卷积核在原图像上滑动，可以生成一个activation map，这里有6个不同的卷积核，得到的6个不同的activation map分别表示诸如边缘特征、形状特征等特征图，将这些activation map映射在深度方向上层叠起来就生成了输出数据。所以在用了6个过滤器（卷积层）之后，我们可以得到28×28×6的激活图。对各个activation map的直观感受可以看下图，其中每一个activation map代表着不同层次的特征。 2.1.1 卷积层输出值的计算我们使用一个简单的例子来讲述如何计算卷积，然后，抽象出卷积层的一些重要概念和计算方法。 假设有一个5×5的图像，使用一个3×3的滤波器进行卷积，想得到3×3的Feature Map，如下所示：为了清楚地描述卷积的计算过程，我们首先对图像的每个像素进行编号，用$x_{i,j}$表示图像的第$i$行第$j$列元素；对filter的每个权重进行编号，用$w_{m,n}$表示第$m$行第$n$列权重，用$w_b$表示filter的偏置项；对Feature Map的每个元素进行编号，用$a_{i,j}$表示Feature Map的第$i$行第$j$列元素；用$f$表示激活函数（此处使用Relu函数作为激活函数）。然后使用下列公式计算卷积： a_{i,j}=f\left(\sum_{m=0}^2{\sum_{n=0}^2{w_{m,n}x_{m+i,n+j}}}+w_b\right)例如，对于Feature Map的左上角元素$a_{0,0}$来说，其卷积计算方法为： a_{0,0}=f\left(\sum_{m=0}^2{\sum_{n=0}^2{w_{m,n}x_{m+0,n+0}}}+w_b\right)=Relu\left(4\right)=4按照这个公式可以依次计算出Feature Map中所有的值，下面的动画显示了整个Feature Map的计算过程：上面的计算过程中，步幅（stride）为1。当然步幅可以设为大于1的数。例如，当步幅为2时，Feature Map计算如下：这里我们可以看到，当把步幅设置为2时，Feature Map就变成了2×2了。这说明图像大小、步幅和卷积后的Feature Map大小是有关系的。我们设卷积前的图像宽度为$N$，步幅为$S$，filter的边长为$F$，卷积后Feature Map宽度为$N_f$，如图所示则它们之间的关系为： N_f=\frac{N-F}{S}+1但是这样持续卷积运算下去会出现一些问题，比如下图： 每经过一个filter，得到的激活图就会小一些，要是经过好几个，会导致最后消失殆尽。所以我们通过零填充（zero padding）的方法在原始图像周围补上几圈0，将补上的圈数设为$P$，则改写我们之前的关系式为： N_f=\frac{N+2P-F}{S}+1这里$P$乘以2是加了一圈之后两侧都加了1。例如上方那幅图中，$N=5$，$F=3$，$S=1$，我们想保持卷积前后的尺寸保持不变，即$N_f=N=5$，则零填充的$P$为： P=\frac{(N_f-1)S+F-N}{2}=\frac{(5-1)×1+3-5}{2}=1到此我们讲了深度为1的卷积层的计算方法，如果深度大于1怎么计算呢？其实也是类似的。如果卷积前的图像深度为$D$，那么相应的filter的深度也必须为$D$。我们扩展一些之前的式子，得到深度为$D$的卷积计算公式： a_{i,j}=f\left(\sum_{d=0}^{D-1}{\sum_{m=0}^{M-1}{\sum_{n=0}^{N-1}{w_{d,m,n}x_{d,m+i,n+j}}}}+w_b\right)该式中，$D$为深度；$w_{d,m,n }$表示filter的第d层第m行第n列的权重；$a_{d,i,j }$表示图像的第d层第i行第j列像素。我们前面还曾提到，每个卷积层可以有多个filterr。每个filter和原始图像进行卷积之后，都可以得到一个Feature Map。因此，卷积后Feature Map的深度和卷积层的filter个数是相同的。 下面的动画显示了包含两个filter的卷积层的计算。我们可以看到7×7×3的输入，经过两个3×3×3filter的卷积，其步幅为2，得到了3×3×2的输出，另外我们也会看到下图的Zero Padding是1，也就是在输入元素的周围补了一圈0。Zero Padding对图像边缘部分的特征提取是很有帮助的。 以上就是卷积层的计算方法。这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连（卷积计算规则），且filter的权值对于上一层所有神经元都是一样的。对于包含两个3×3×3的filter的卷积层来说，其参数数量仅有$(3×3×3+1)×2=56$个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。 2.1.2 用卷积公式来表达卷积层计算之前计算卷积层输出的式子很繁冗，最好可以简化一下 2.2 池化层（Pooling layer）Pooling层的主要作用就是通过下采样，去掉Feature Map中不重要的样本，进一步减少参数数量，降低了计算成本，而且可以控制过拟合（overfitting）。池化层并不会对Feature map的深度有影响，即还是会保持原来的深度。 Pooling的方法很多，最常用的是Max Pooling，它实际上就是在$n×n$的样本中取最大值，作为采样后的样本值。下图是2×2 Max Pooling： 此外，还有平均池化（average pooling）和L2-norm池化。平均汇聚历史上比较常用，但是现在已经很少使用了。因为实践证明，最大汇聚的效果比平均汇聚要好。池化层背后的直观推理是：一旦我们知道了原始输入中一个特定的特征，它与其他特征的相对位置就比它的绝对位置更重要。 很多人不喜欢汇聚操作，认为可以不使用它。比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃池化层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用池化层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，无池化层的结构不太可能扮演重要的角色。 2.3 归一化层在卷积神经网络的结构中，提出了很多不同类型的归一化层，有时候是为了实现在生物大脑中观测到的抑制机制。但是这些层渐渐都不再流行，因为实践证明它们的效果即使存在，也是极其有限的。 2.4 全连接层全连接层输出值的计算经网络和全连接神经网络是一样的，这里就不再赘述了。 三、卷积神经网络的训练和全连接神经网络相比，卷积神经网络的训练要复杂一些。但训练的原理是一样的：利用链式求导计算损失函数对每个权重的偏导数（梯度），然后根据梯度下降公式更新权值。训练算法依然是反向传播算法。 我们知道神经网络和反向传播那一节中介绍的反向传播算法，它的整个算法分为三个基本步骤： 1）前向计算每个神经元的输出值$a_j$（$j$表示网络的第$j$个神经元，以下同）； 2）反向计算每个神经元的误差项$\delta_j$，$\delta_j$在有的文献中也叫作敏感度（sensitivity）。它实际上是网络的损失函数$E_d$对神经元加权输出$net_j$的偏导数，即$\delta_j=\frac{\partial E_d}{\partial net_j}$； 3）计算每个神经元连接权重$w_{ij}$的梯度（$w_{ij}$表示从神经元$i$连接到神经元$j$的权重，公式为$\frac{\partial E_d}{\partial w_{ji}}=a_i\delta_j$）,其中，$a_i$表示神经元$i$的输出。 4）根据梯度下降法更新每个权重即可 对于卷积神经网络，由于涉及到局部连接、下采样等操作，影响到了第二部误差项$\delta$的具体计算方法，而权值共享影响了第三步权重$w$的梯度的计算方法。接下来，我们分别介绍卷积层和池化层的训练算法。 3.1 卷积层的训练3.2 Pooling层的训练]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（18）：方差偏差权衡（Bias-Variance Tradeoff）]]></title>
    <url>%2F2017%2F04%2F19%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8818%EF%BC%89%EF%BC%9A%E6%96%B9%E5%B7%AE%E5%81%8F%E5%B7%AE%E6%9D%83%E8%A1%A1%EF%BC%88Bias-Variance%20Tradeoff%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、定义1.1 感性解释Bias和Variance是针对Generalization（泛化、一般化）来说的。在机器学习中，我们用训练数据集学习一个模型，我们通常会定义一个损失函数（Loss Function），然后将这个Loss（或者叫error）的最小化过程，来提高模型的性能（performance）。然而我们学习一个模型的目的是为了解决实际的问题（即将训练出来的模型运用于预测集），单纯地将训练数据集的Loss最小化，并不能保证解决更一般的问题时模型仍然是最优的，甚至不能保证模型是可用的。这个训练数据集的Loss与一般化的数据集（预测数据集）的Loss之间的差异就叫做Generalization error。 而Generalization error又可以细分为Random Error、Bias和Variance三个部分。 首先需要说的是随机误差。它是数据本身的噪声带来的，这种误差是不可避免的。其次如果我们能够获得所有可能的数据集合，并在这个数据集合上将Loss最小化，这样学习到的模型就可以称之为“真实模型”，当然，我们是无论如何都不能获得并训练所有可能的数据的，所以真实模型一定存在，但无法获得，我们的最终目标就是去学习一个模型使其更加接近这个真实模型。 Bias和Variance分别从两个方面来描述了我们学习到的模型与真实模型之间的差距（除去随机误差）。 Bias描述的是对于测试数据集，“用所有可能的训练数据集训练出的所有模型的输出预测结果的期望”与“真实模型”的输出值（样本真实结果）之间的差异。简单讲，就是在样本上拟合的好不好。要想在bias上表现好，low bias，就是复杂化模型，增加模型的参数，但这样容易过拟合 (overfitting)。 Variance则是“不同的训练数据集训练出的模型”的输出值之间的差异。 在一个实际系统中，Bias与Variance往往是不能兼得的。如果要降低模型的Bias，就一定程度上会提高模型的Variance，反之亦然。造成这种现象的根本原因是，我们总是希望试图用有限训练样本去估计无限的真实数据。当我们更加相信这些数据的真实性，而忽视对模型的先验知识，就会尽量保证模型在训练样本上的准确度，这样可以减少模型的Bias。但是，这样学习到的模型，很可能会失去一定的泛化能力，从而造成过拟合，降低模型在真实数据上的表现，增加模型的不确定性。相反，如果更加相信我们对于模型的先验知识，在学习模型的过程中对模型增加更多的限制，就可以降低模型的variance，提高模型的稳定性，但也会使模型的Bias增大。Bias与Variance两者之间的trade-off是机器学习的基本主题之一，机会可以在各种机器模型中发现它的影子。 1.2 图示解释下图将机器学习任务描述为一个打靶的活动：根据相同算法、不同训练数据集训练出的模型，对同一个样本进行预测；每个模型作出的预测相当于是一次打靶。 左上角的示例是理想状况：偏差和方差都非常小。如果有无穷的训练数据，以及完美的模型算法，我们是有办法达成这样的情况的。然而，现实中的工程问题，通常数据量是有限的，而模型也是不完美的。因此，这只是一个理想状况。 右上角的示例表示偏差小而方差大。靶纸上的落点都集中分布在红心周围，它们的期望落在红心之内，因此偏差较小。另一方面，落点虽然集中在红心周围，但是比较分散，这是方差大的表现。 左下角的示例表示偏差大而方差小。显而易见，靶纸上的落点非常集中，说明方差小。但是落点集中的位置距离红心很远，这是偏差大的表现。 右下角的示例则是最糟糕的情况，偏差和方差都非常大。这是我们最不希望看到的结果。 再看一个来自PRML的例子：这是一个曲线拟合的问题，对同分布的不同数据集进行了多次的曲线拟合，左边表示方差（variance），右边表示偏差（bias），绿色是真实值函数。$In \lambda$表示的是模型的复杂度，这个值越小，表示模型的复杂程度越高，在第一行，大家的复杂度都很低的时候，方差是很小的，但是偏差很大；但是到了最后一幅图，我们可以得到，每个人的复杂程度都很高的情况下，不同的函数就有着天壤之别了，所以方差就很大，但此时偏差就很小了。 1.3 数学解释排除人为的失误，人们一般会遇到三种误差来源：随机误差、偏差和方差。 首先需要说明的是随机误差。随机误差是数据本身的噪声带来的，这种误差是不可避免的。一般认为随机误差服从高斯分布，记作$\varepsilon ~N\left(0,\sigma_{\varepsilon}\right)$。因此，若有变量$y$作为预测值，以及$X$作为自变量（协变量），那么我们将数据背后的真实规律$f$记作 y=f(X)+\epsilon偏差和方差则需要在统计上做对应的定义。 偏差（Bias）描述的是通过学习拟合出来的结果的期望，与真实结果之间的差距，记作Bias\left(X\right)=E\left[\hat{f}\left(X\right)\right]-f\left(X\right) 方差（Variance）即为统计学中的定义，描述的是通过学习拟合出来的结果自身的不稳定性，记作E\left[\left(\hat{f}\left(X\right)-E\left[\hat{f}\left(X\right)\right]\right)\right]^2 以均方误差为例，有如下推论： Err\left(X\right)=E\left[\left(y-\hat{f}\left(X\right)\right)^2\right]\\ =E\left[\left(f\left(X\right)+\varepsilon -\hat{f}\left(X\right)\right)^2\right] =\left(E\left[\hat{f}\left(X\right)\right]-f\left(X\right)\right)^2+E\left[\left(\hat{f}\left(X\right)-E\left[\hat{f}\left(X\right)\right]\right)\right]^2+\sigma_{\varepsilon}^{2} =Bias^2+Variance+Random\ Error二、如何Tradeoff2.1 最佳平衡点假设我们现在有一组训练数据，需要训练一个模型（基于梯度的学习）。在训练的起始，Bias很大，因为我们的模型还没有来得及开始学习，也就是与“真实模型”差距很大。然而此时variance却很小，因为训练数据集（training data）还没有来得及对模型产生影响，所以此时将模型应用于“不同的”训练数据集也不会有太大的差异。 而随着训练过程的进行，Bias变小了，因为我们的模型变得“聪明”了，懂得了更多关于“真实模型”的信息，输出值与真实值之间更加接近了。但是如果我们训练得太久了，variance就会变得很大，因为我们除了学习到关于真实模型的信息，还学到了许多具体的，只针对我们使用的训练集（真实数据的子集）的信息。而不同的可能的训练数据集（真实数据的子集）之间的某些特征和噪声是不一致的，这就导致了了我们在很多其他的数据集上就无法获得很好地效果，也就是所谓的Overfitting（过拟合）。 考虑到模型误差是偏差与方差的加和，因此我们可以绘制出这样的图像。 图中的最优位置，实际上是Total Error曲线的拐点。我们知道，连续函数的拐点意味着此处一阶导数的值为0。即 \frac{d\left(Total\ Error\right)}{d\left(Complexity\right)}=\frac{d\left(Bias+Variance\right)}{d\left(Complexity\right)}=\frac{d\left(Bias\right)}{d\left(Complexity\right)}+\frac{d\left(Variance\right)}{d\left(Complexity\right)}=0这个公式给出了寻找最优平衡点的数学描述。若模型复杂度小于平衡点，则模型的偏差会偏高，模型倾向于欠拟合；若模型复杂度大于平衡点，则模型的方差会偏高，模型倾向于过拟合。 3.2 过拟合与欠拟合的外在表现尽管有了上述的数学表述，但是在现实环境中，有时候我们很难计算模型的偏差与方差。因此，我们需要通过外在表现，判断模型的拟合状态：是欠拟合还是过拟合。 同样地，在有限的训练数据集中，不断增加模型的复杂度，意味着模型会尽可能多地降低在训练集上的误差。因此在训练集上，不断地增加模型的复杂度，训练集上的误差会一直下降。 我们把数据分为三个部分：训练数据集、验证数据集、测试数据集。 因此，我们可以绘制出这样的图像。在上图左边区域，训练集与验证集的误差都很高，这块区域的偏差比较高。在右边区域，在验证集上误差很高，但是在训练集上偏差很低，这块区域的方差比较高。我们希望在中间的区域得到一个最优平衡点。 所以，偏差较高（欠拟合）有以下两个特征： 1）训练集误差很高 2）验证集误差和训练集误差差不多大 方差较高（过拟合） 1）训练集误差较低 2）非常高的验证集误差 3.3 如何处理欠拟合与过拟合有了以上的分析，我们就能比较容易地判断模型所处的拟合状态。接下来，我们可以参考Ng提供的处理模型欠拟合与过拟合的一般方法了。 当模型处于欠拟合状态时，根本的办法是增加模型的复杂度。我们一般有以下一些办法： 1）增加模型迭代次数； 2）训练一个复杂度更高的模型：比如在神经网络中增加神经网络层数、在SVM中用非线性SVM（核技术）代替线性SVM 3）获取更多的特征以供训练使用：特征少，对模型信息的刻画就不足够了 4）降低正则化权重：正则化正是为了限制模型的灵活度（复杂度）而设定的，降低其权值可以在模型训练中增加模型复杂度。 当模型处于过拟合状态时，根本的办法是降低模型的复杂度。我们一般有以下一些办法： 1）获取更多的数据：训练数据集和验证数据集是随机选取的，它们有不同的特征，以致在验证数据集上误差很高。更多的数据可以减小这种随机性的影响。 2）减少特征数量 3）增加正则化权重：方差很高时，模型对训练集的拟合很好。实际上，模型很有可能拟合了训练数据集的噪声，拿到验证集上拟合效果就不好了。我们可以增加正则化权重，减小模型的复杂度。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Tradeoff</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（17）：非平衡数据处理]]></title>
    <url>%2F2017%2F04%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8817%EF%BC%89%EF%BC%9A%E9%9D%9E%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[一、Introduction常用的分类算法一般假设不同类的比例是均衡的，现实生活中经常遇到不平衡的数据集，比如广告点击预测（点击转化率一般都很小）、商品推荐（推荐的商品被购买的比例很低）、信用卡欺诈检测等等。 对于不平衡数据集，一般的分类算法都倾向于将样本划分到多数类，体现在模型整体的准确率很高。 但对于极不均衡的分类问题，比如仅有1%的人是坏人，99%的人是好人，最简单的分类模型就是将所有人都划分为好人，模型都能得到99%的准确率，显然这样的模型并没有提供任何的信息。 在类别不平衡的情况下，对模型使用F值或者AUC值是更好的选择。 处理不平衡数据，可以从两方面考虑：一是改变数据分布，从数据层面使得类别更为平衡； 二是改变分类算法，在传统分类算法的基础上对不同类别采取不同的加权方式，使得模型更看重少数类。 本部分对数据层面的一些方法做一个介绍，改变数据分布的方法主要是重采样： 1）过采样：增加少数类样本的数量 2）欠采样：减少多数类样本的数量 3）综合采样：将过采样和欠采样结合 二、过采样2.1 随机过采样采样算法通过某一种策略改变样本的类别分布，以达到将不平衡分布的样本转化为相对平衡分布的样本的目的，而随机采样是采样算法中最简单也最直观易懂的一种方法。 随机过抽样是增加少数类样本数量，可以事先设置多数类与少数类最终的数量比例，在保留多数类样本不变的情况下，根据比例随机复制少数类样本，在使用的过程中为了保证所有的少数类样本信息都会被包含，可以先完全复制一份全量的少数类样本，再随机复制少数样本使得满足数量比例，具体步骤如下： 1.首先在少数类$S_{min}$集合中随机选中一些少数类样本 2.然后通过复制所选样本生成样本集合$E$ 3.将它们添加到$S_{min}$中来扩大原始数据集从而得到新的少数类集合$S_{min-new}$ $S_{min}$中的总样本数增加了 $|E| $个新样本，且$S_{min-new}$ 的类分布均衡度进行了相应的调整，如此操作可以改变类分布平衡度从而达到所需水平。 重复样本过多，容易造成分类器的过拟合 2.2 SMOTE算法(Synthetic Minority Oversampling Technique)在合成抽样技术方面，Chawla NY等人提出的SMOTE过抽样技术是基于随机过采样算法的一种改进方案，由于随机过采样简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题，即使模型学习到的信息过于特别（Specific）而不够泛化(General)。 SMOTE的主要思想是利用特征空间中现存少数类样本之间的相似性来建立人工数据，特别是，对于子集$S_{min}$ $\subset$ $S$，对于每一个样本$x_i\subset S_{min}$使用K-近邻法，其中K-近邻被定义为考虑$S_{min}$中的K个元素本身与$x_i$的欧氏距离在n维特征空间X中表现为最小幅度值的样本。由于不是简单地复制少数类样本，因此可以在一定程度上避免分类器的过度拟合，实践证明此方法可以提高分类器的性能。但是由于对每个少数类样本都生成新样本，因此容易发生生成样本重叠（overlapping）的问题。算法流程如下： 1）对于少数类中的每一个样本$(x_i)$，以欧氏距离为标准计算它到少数类样本集$S_{min}$中所有样本的距离，得到K近邻； 2）根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本$x_i$，从其K近邻中随机选择若干个样本，假设选择的近邻为$\tilde{x}$； 3）对于每一个随机选出的近邻$\tilde{x}$，分别与原样本按照如下的公式构建新的样本:$x_{new}=x+rand\left(0,1\right)\times\left(\tilde{x}-x\right)$ 2.3 Borderline-SMOTE算法原始的SMOTE算法对所有的少数类样本都是一视同仁的，但实际建模过程中发现那些处于边界位置的样本更容易被错分，因此利用边界位置的样本信息产生新样本可以给模型带来更大的提升。Borderline-SMOTE便是将原始SMOTE算法和边界信息算法结合的算法。算法流程如下： 1.首先，对于每个$x_{i}\subset S_{min}$确定一系列K-近邻样本集，称该数据集为$S_{i-kNN}$，且$S_{i-kNN}\subset S$； 2.然后，对每个样本$x_{i}$，判断出最近邻样本集中属于多数类样本的个数，即：|$S_{i-kNN}\cap S_{maj}$|； 3.最后，选择满足下面不等式的$x_{i}$:$\frac{k}{2}$&lt;|$S_{i-kNN} \cap S_{maj}$|&lt;$k$,将其加入危险集$DANGER$， 对危险集中的每一个样本点（最容易被错分的样本），采用普通的$SMOTE$算法生成新的少数类样本。 三、欠采样3.1 随机欠采样减少多数类样本数量最简单的方法便是随机剔除多数类样本，可以事先设置多数类与少数类最终的数量比例，在保留少数类样本不变的情况下，根据比例随机选择多数类样本。 1）首先我们从$S_{maj}$中随机选取一些多数类样本$E$ 2）将这些样本从$S_{maj}$中移除，就有|$S_{maj-new}|=|S_{maj}-|E$| 优点在于操作简单，只依赖于样本分布，不依赖任何距离信息，属于非启发式方法；缺点在于会丢失一部分多数类样本的信息，无法充分利用已有信息。 3.2 Tomek Links方法定义：Tomek links被定义为相反类最近邻样本之间的一对连接。 符号约定：给定一个样本对$\left(x_i,x_j\right)$，其中$x_{i}$ $\in$ $S_{maj}$，$x_{j}$ $\in$ $S_{min}$，记$d\left(x_i,x_j\right)$是样本$x_i$和$x_j$之间的距离 公式表示：如果不存在任何样本$x_k$，使得$d\left( x_i,x_k \right)$ &lt;$d\left( x_i,x_j \right)$ ，那么样本对$\left(x_i,x_j\right)$被称为Tomek Links 使用这种方法，如果两个样本来自Tomek Links，那么他们中的一个样本要么是噪声要么它们都在两类的边界上。所以Tomek Links一般有两种用途：在欠采样中：将Tomek Links中属于是多数类的样本剔除；在数据清洗中，将Tomek Links中的两个样本都剔除。 3.3 NearMiss方法NearMiss方法是利用距离远近剔除多数类样本的一类方法，实际操作中也是借助KNN，总结起来有以下几类： 1）NearMiss-1：在多数类样本中选择与最近的三个少数类样本的平均距离最小的样本 2）NearMiss-2：在多数类样本中选择与最远的3个少数类样本的平均距离最小的样本 3）NearMiss-3：对于每个少数类样本，选择离它最近的给定数量的多数类样本 NearMiss-1和NearMiss-2方法的描述仅有一字之差，但其含义是完全不同的：NearMiss-1考虑的是与最近的3个少数类样本的平均距离，是局部的；NearMiss-2考虑的是与最远的3个少数类样本的平均距离，是全局的。 NearMiss-1方法得到的多数类样本分布也是”不均衡“的，它倾向于在比较集中的少数类附近找到更多的多数类样本，而在孤立的（或者说是离群的）少数类附近找到更少的多数类样本，原因是NearMiss-1方法考虑的局部性质和平均距离。 NearMiss-3方法则会使得每一个少数类样本附近都有足够多的多数类样本，显然这会使得模型的精确度高、召回率低。 实验结果表明得到NearMiss-2的不均衡分类性能最优。 四、Informed UnderstandingInformed欠抽样算法可以解决传统随机欠采样造成的数据信息丢失问题，且表现出较好的不均衡数据分类性能。其中有一些集成（ensemble）的想法，主要有两种方法，分别是EasyEnsemble算法和BalanceCascade算法。 4.1 EasyEnsemble算法它把数据划分为两部分，分别是多数类样本和少数类样本，对于多数类样本$S_{maj}$，通过$n$次有放回抽样生成$n$份子集，少数类样本$S_{min}$分别和这$n$份样本合并训练AdaBoost分类器，这样可以得到$n$个模型，最终的模型采用加权多数表决的方法，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率小的弱分类器的权值，使其在表决中起较小的作用。这里假设多数类样本为N，少数类样本为P，算法流程如下： EasyEnsemble的想法是多次随机欠抽样，尽可能全面地涵盖所有信息，算法特点是利用boosting减小偏差（Adaboost）、bagging减小方差（集成分类器）。实际应用的时候也可以尝试选用不同的分类器来提高分类的效果。 4.2 BalanceCascade算法EasyEnsemble算法训练的子过程是独立的，BalanceCascade则是一种级联算法，这种级联的思想在图像识别中用途非常广泛。算法流程如下： BalanceCascade算法得到的是一个级联分类器，将若干个强分类器由简单到复杂排列，只有和少数类样本特征比较接近的才有可能输入到后面的分类器，比如边界点，因此能更充分地利用多数类样本的信息，一定程度上解决随机欠采样的信息丢失问题。 五、综合采样目前为止我们使用的重采样方法几乎都是只针对某一类样本：对多数类样本欠采样，对少数类样本过采样。也有人提出将欠采样和过采样综合的方法，解决样本类别分布不平衡和过拟合问题，本部分介绍其中的SMOTE+Tomek Links和SMOTE+ENN。 5.1 SMOTE+Tomek LinksSMOTE+Tomek Links方法的算法流程非常简单： 1.利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T 2.剔除T中的Tomek Links对 普通的SMOTE方法生成的少数类样本是通过线性插值得到的，在平衡类别分布的同时也扩张了少数类的样本空间，产生的问题是可能原本属于多数类样本的空间被少数类“入侵”，容易造成模型的过拟合。 Tomek Links对寻找的是那种噪声点或者边界点，可以很好地解决“入侵”的问题，下图红色加号为SMOTE产生的少数类样本，可以看到，红色样本“入侵”到原本属于多数类样本的空间，这种噪声数据问题可以通过Tomek Links很好地解决。 由于第一步SMOTE方法已经很好地平衡了类别分布，因此在使用Tomek Links对的时候考虑剔除所有的Tomek Links对。 5.2 SMOTE+ENNSMOTE+ENN方法和SMOTE+Tomek Links方法的想法和过程都是很类似的： 1）利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T 2）对T中的每一个样本使用KNN（一般K取3）方法预测，若预测结果与实际类别标签不符，则剔除该样本。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>非平衡数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（2）：神经网络MNIST实战]]></title>
    <url>%2F2017%2F04%2F14%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CMNIST%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[近似非常不错 一、数据集与任务介绍MNIST数据集是一个基本的手写字体识别数据集，该数据原本是包含60000个训练图像和10000个测试图像，但这里我们事先对数据进行了划分，从训练样本中抽取10000个数据作为验证集，所以处理后的数据集包含50000个训练样本（training data）、10000个验证样本（validation data）10000个测试样本（test data），都是28乘以28的分辨率。 我们可以先将数据集从GitHub上Clone下来： 1➜ fig git:(master) ✗ git clone https://github.com/lisa-lab/DeepLearningTutorials 可以从这个链接了解对该数据集的加载和处理。 这里任务就是构建神经网络来实现对于MNIST数据集的手写字体识别分类。从任务和输入就能够得到大概的网络结构：损失函数为平方误差损失函数，激活函数为sigmoid函数。 二、读取数据读取数据由mnist_loader.py这个文件实现。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# -*- coding: utf-8 -*-"""mnist_loader~~~~~~~~~~~~A library to load the MNIST image data. For details of the datastructures that are returned, see the doc strings for ``load_data``and ``load_data_wrapper``. In practice, ``load_data_wrapper`` is thefunction usually called by our neural network code."""#### Libraries# Standard libraryimport cPickleimport gzip# Third-party librariesimport numpy as np#从数据集中载入数据def load_data(): f = gzip.open('../data/mnist.pkl.gz', 'rb') training_data, validation_data, test_data = cPickle.load(f) f.close() return (training_data, validation_data, test_data)#改变数据集的格式def load_data_wrapper(): tr_d, va_d, te_d = load_data() #训练集 training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] training_results = [vectorized_result(y) for y in tr_d[1]] training_data = zip(training_inputs, training_results) #验证集 validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] validation_data = zip(validation_inputs, va_d[1]) #测试集~~~~ test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] test_data = zip(test_inputs, te_d[1]) return (training_data, validation_data, test_data)def vectorized_result(j): """Return a 10-dimensional unit vector with a 1.0 in the jth position and zeroes elsewhere. This is used to convert a digit (0...9) into a corresponding desired output from the neural network.""" e = np.zeros((10, 1)) e[j] = 1.0 return e 2.1 load_data函数12345def load_data(): f = gzip.open('../data/mnist.pkl.gz', 'rb') training_data, validation_data, test_data = cPickle.load(f) f.close() return (training_data, validation_data, test_data) load_data()函数的主要作用就是解压数据集，然后从数据集中把数据取出来。取出来之后的几个变量代表的数据的格式分别如下： training_data：是一个由两个元素构成的元组。其中一个元素是测试图片集合，是一个50000✖️784的numpy ndarray（其中50000行就是样本个数，784列就是一个维度，即一个像素）；第二个元素就是一个测试图片的标签集，是一个50000✖️1的Numpy ndarray，其中指明了每一个样本是什么数字，通俗来说就是这个样子：validation_data 和 test_data 的结构和上面的training_data是一样的，只是数量不一样，这两个是10000行。 2.2 load_data_wrapper()函数1234567891011121314def load_data_wrapper(): tr_d, va_d, te_d = load_data() #训练集 training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] training_results = [vectorized_result(y) for y in tr_d[1]] training_data = zip(training_inputs, training_results) #验证集 validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] validation_data = zip(validation_inputs, va_d[1]) #测试集~~~~ test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] test_data = zip(test_inputs, te_d[1]) return (training_data, validation_data, test_data) 之前的load_data返回的格式虽然很漂亮，但是并不是非常适合我们这里计划的神经网络的结构，因此我们在load_data的基础上使用load_data_wrapper（）函数来进行一点点适当的数据集变换，使得数据集更加适合我们的神经网络训练。 以训练集的变换为例。对于training_inputs来说，就是把之前的返回的training_data[0]，即第一个元素的所有样例都放到一个列表中，简单的来说如下所示： 同样可以知道training_labels的样子为： 然后training_data为zip函数组合，那么training_data为一个列表，其中每个元素是一个元组，二元组又有一个training_inputs和一个training_labels的元素组合而成，如下图： 同理可以推出其他数据的形状。 三、神经网络代码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143# -*- coding: utf-8 -*-import randomimport numpy as npclass Network(object): #初始化神经网络 def __init__(self, sizes): """The list ``sizes`` contains the number of neurons in the respective layers of the network. For example, if the list was [2, 3, 1] then it would be a three-layer network, with the first layer containing 2 neurons, the second layer 3 neurons, and the third layer 1 neuron. The biases and weights for the network are initialized randomly, using a Gaussian distribution with mean 0, and variance 1. Note that the first layer is assumed to be an input layer, and by convention we won't set any biases for those neurons, since biases are only ever used in computing the outputs from later layers.""" self.num_layers = len(sizes)#神经网络层数 self.sizes = sizes#储存各层神经元个数的列表 self.biases = [np.random.randn(y, 1) for y in sizes[1:]]#随机初始化偏置 self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]#随机初始化权重 #前向传播算法 def feedforward(self, a): """Return the output of the network if ``a`` is input.""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a #随机梯度下降（训练数据，迭代次数，小样本数量，学习率，是否有测试集（默认为无）） def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None): """Train the neural network using mini-batch stochastic gradient descent. The ``training_data`` is a list of tuples ``(x, y)`` representing the training inputs and the desired outputs. The other non-optional parameters are self-explanatory. If ``test_data`` is provided then the network will be evaluated against the test data after each epoch, and partial progress printed out. This is useful for tracking progress, but slows things down substantially.""" if test_data: n_test = len(test_data)#若有测试集，则计算其大小 n = len(training_data)#训练集大小 #迭代过程 for j in xrange(epochs): # shuffle() 方法对训练集随机排序 random.shuffle(training_data) # mini_batch是列表中切割之后的列表 mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, eta) if test_data: print "Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;".format(j, self.evaluate(test_data), n_test) else: print "Epoch &#123;0&#125; complete".format(j) def update_mini_batch(self, mini_batch, eta): """Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta`` is the learning rate.""" #存储C对于各个参数的偏导，格式和self.biases和self.weights是一模一样的 nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] #mini_batch中的一个实例调用梯度下降得到各个参数的偏导 for x, y in mini_batch: 从一个实例得到的梯度 delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] #每一个mini_batch更新一下参数 self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] #反向传播（对于每一个实例） def backprop(self, x, y): """Return a tuple ``(nabla_b, nabla_w)`` representing the gradient for the cost function C_x. ``nabla_b`` and ``nabla_w`` are layer-by-layer lists of numpy arrays, similar to ``self.biases`` and ``self.weights``.""" # 存储C对于各个参数的偏导，格式和self.biases和self.weights是一模一样的 nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] # 前向过程 activation = x #存储所有的激活值，一层一层的形式 activations = [x] # list to store all the activations, layer by layer #存储所有的中间值（weighted sum） zs = [] # list to store all the z vectors, layer by layer for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # 反向过程 # 输出层error delta = self.cost_derivative(activations[-1], y) * \ sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Note that the variable l in the loop below is used a little # differently to the notation in Chapter 2 of the book. Here, # l = 1 means the last layer of neurons, l = 2 is the # second-last layer, and so on. It's a renumbering of the # scheme in the book, used here to take advantage of the fact # that Python can use negative indices in lists. # 非输出层 for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def evaluate(self, test_data): """Return the number of test inputs for which the neural network outputs the correct result. Note that the neural network's output is assumed to be the index of whichever neuron in the final layer has the highest activation.""" test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data] return sum(int(x == y) for (x, y) in test_results) # 输出层cost函数对于a的导数 def cost_derivative(self, output_activations, y): """Return the vector of partial derivatives \partial C_x / \partial a for the output activations.""" return (output_activations-y)#### Miscellaneous functions#sigmoid函数def sigmoid(z): """The sigmoid function.""" return 1.0/(1.0+np.exp(-z))#sigmoid函数的导数def sigmoid_prime(z): """Derivative of the sigmoid function.""" return sigmoid(z)*(1-sigmoid(z)) 四、结果比较将隐藏层设为30层，随机梯度下降的迭代次数为30次，小批量数量大小为10，学习速率为3.0 123456789101112131415In [77]: import mnist_loaderIn [78]: import networkIn [80]: training_data,validation_data,test_data = mnist_loader.load_data_wrapper()In [81]: net = network.Network([784,30,10])In [82]: net.SGD(training_data,30,10,3.0,test_data = test_data)Epoch 0: 8185 / 10000Epoch 1: 8363 / 10000Epoch 2: 8404 / 10000Epoch 3: 8447 / 10000······Epoch 25: 9492 / 10000Epoch 26: 9494 / 10000Epoch 27: 9468 / 10000Epoch 28: 9504 / 10000Epoch 29: 9507 / 10000 经过30次迭代之后，神经网络的识别率为95%左右。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>MNIST</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（1）：神经网络与反向传播算法]]></title>
    <url>%2F2017%2F04%2F10%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、神经元首先我们从最简单的神经网络——神经元讲起，以下即为一个神经元（Neuron）的图示： 这个神经元是一个以$x_1,x_2,···,x_K$以及截距$b$为输入值的运算单元，其输出为 \alpha =\sigma\left(w^Ta+b\right)=\sigma\left(w_1a_1+w_2a_2+···+w_Ka_K+b\right)其中$w$为权值项，$b$为偏置项，函数$\sigma$被称为“激活函数”。之前在学习感知机的时候，我们知道感知机的激活函数是阶跃函数；而当我们说神经元的时，激活函数往往选择sigmoid函数或tanh函数。激活函数的作用就是将之前加法器输出的函数值$z$进行空间映射，如下图所示： 可以看出，这个单一神经元的输入输出的映射关系其实就是一个逻辑回归（logistic regression）。 关于sigmoid阶跃函数的性质，在逻辑回归中已经了解过了，有一个等式我们会用到：$f^,(z)=f(z)(1-f(z))$。现在我们简要看一下双曲正切函数（tanh）。它的表达式为： f\left(z\right)=\tan\textrm{h}\left(z\right)=\frac{e^z-e^{-z}}{e^z+e^{-z}}它们图像为tanh（z）函数是sigmoid函数的一种变体，它的取值范围为[-1,1]，而不是sigmoid函数的[0,1]，它的导数为$f^，(z)=1-(f(z))^2$ 二、神经网络模型2.1 神经网络模型所谓神经网络就是将许多神经元联结在一起，这样，一个神经元的输出就可以是另一神经元的输入。例如，下图就是一个简单的神经网络：我们使用圆圈来表示神经网络的输入，标上”+1”的圆圈被称为偏置节点，也就是截距项。神经网络最左边的一层叫做输入层，最右边的一层叫做输出层（本例中，输出层只有一个节点）。中间所有节点组成的一层叫做隐藏层，如此命名是因为我们不能在训练样本中观测到它们的值。同时可以看到，以上神经网络的例子中有3个输入单元（偏置单元不算在内），三个隐藏单元及一个输出单元。 我们用$n_l$来表示神经网络的层数，本例中$n_l=3$，我们将第$l$层记为$L_l$，于是$L_1$是输入层，输出层是$L_{nl}$。本例神经网络有参数$(W,b)=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})$，其中$W_{ij}^{(l)}$是第$l$层第$j$个单元与第$l+1$层第$i$单元之间的联接参数（其实就是连接线上的权重，注意标号前后顺序），$b_i^{(l)}$是第$l+1$层第$i$个单元的偏置项。偏置单元没有输入，即没有单元连向偏置单元，它们总是输出$+1$。同时，我们用$s_l$表示第$l$层的节点数（偏置单元不计在内）。 我们用$a_i^{(l)}$表示第$l$层第$i$个单元的激活值（输出值）。当$l=1$时，$a_i^{(1)}=x_i$，也就是第$i$个输入值（输入值的第$i$个特征）。对于给定参数集合$W,b$，我们的神经网络就可以按照函数$h_{W,b}{(x)}$来计算结果。本例中神经网络的计算步骤如下： a_{1}^{\left(2\right)}=f\left(W_{11}^{\left(1\right)}x_1+W_{12}^{\left(1\right)}x_2+W_{13}^{\left(1\right)}x_3+b_{1}^{\left(1\right)}\right) a_{2}^{\left(2\right)}=f\left(W_{21}^{\left(1\right)}x_1+W_{22}^{\left(1\right)}x_2+W_{23}^{\left(1\right)}x_3+b_{2}^{\left(1\right)}\right) a_{3}^{\left(2\right)}=f\left(W_{31}^{\left(1\right)}x_1+W_{32}^{\left(1\right)}x_2+W_{33}^{\left(1\right)}x_3+b_{3}^{\left(1\right)}\right) h_{w,b}\left(x\right)=a_{1}^{\left(3\right)}=f\left(W_{11}^{\left(1\right)}x_1+W_{12}^{\left(1\right)}x_2+W_{13}^{\left(1\right)}x_3+b_{1}^{\left(2\right)}\right)2.2 具体举例接下来举一个具体的例子来说明这个过程，我们先给神经网络的每个单元写上编号。图中，输入层有三个节点，我们将其依次编号为1，2，3；隐藏层的4个节点，编号依次为4，5，6，7；最后输出层的两个节点编号为8，9。因为我们这个神经网络是全连接网络，所以可以看到每个节点都和上一层的所有节点有链接。比如我们可以看到隐藏层的节点4，它和输入层的三个节点1，2，3之间都有连接，其连接上的权重分别为$w_{41},w_{42},w_{43}$。那么，我们怎样计算节点4的输出值$a_4$呢？ 为了计算节点4的输出值，我们必须先得到其所有上游节点（也就是节点1，2，3）的输出值。节点1、2、3是输入层的节点，所以，他们的输出值就是向量$\vec{x}$。按照上图画出的对应关系，可以看到节点1、2、3的输出值分别是$x_1,x_2,x_3$。 一旦我们有了节点1、2、3的输出值，我们就可以计算节点4的输出值$a_4$： a_4=f(\vec{w}·\vec{x})=f(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})其中$w_{4b}$是节点4的偏置项，图中没有画出来。而$w_{41},w_{42},w_{43} $分别为节点1、2、3到节点4连接的权重，在给权值$w_{ij}$编号时，我们把目标节点的编号$i$放在前面，把源节点的编号$i$放在后面。 同样，我们可以继续计算出节点5、6、7的输出值$a_5,a_6,a_7$。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值$y_1$: y_1=f(\vec{w}·\vec{x})=f(w_{84}a_4+w_{85}a_5+w_{86}a_6+w_{87}a_7+w_{8b})同理，我们还可以计算出$y_2$的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量$\vec{x}=\left[\begin{array}{c} x_1\\ x_2\\ x_3\\\end{array}\right]$时，神经网络的输出向量$\vec{y}=\left[\begin{array}{c} y_1\\ y_2\\\end{array}\right]$。这里我们也看到，输出向量的维度和输出层神经元个数相同。 2.3 神经网络的矩阵表示神经网络的计算如果用矩阵来表示会很方便，我们先来看看隐藏层的矩阵表示。首先我们把隐藏层4个节点的计算依次排列出来： a_4=f(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})a_5=f(w_{51}x_1+w_{52}x_2+w_{53}x_3+w_{5b})a_6=f(w_{61}x_1+w_{62}x_2+w_{63}x_3+w_{6b})a_7=f(w_{71}x_1+w_{72}x_2+w_{73}x_3+w_{7b})接着，定义神经网络的输入向量$\vec{x}$和隐藏层每个节点的权重向量$\vec{w_j}$。令 \vec{x}=\left[\begin{array}{c} x_1\\ x_2\\ x_3\\ 1\\ \end{array}\right]\vec{w_4}=[w_{41},w_{42},w_{43},w_{4b}]\vec{w_5}=[w_{51},w_{52},w_{53},w_{5b}]\vec{w_6}=[w_{61},w_{62},w_{63},w_{6b}]\vec{w_7}=[w_{71},w_{72},w_{73},w_{7b}]代入之前的一组式子，得到 a_4=f(\vec{w_4}·\vec{x})a_5=f(\vec{w_5}·\vec{x})a_6=f(\vec{w_6}·\vec{x})a_7=f(\vec{w_7}·\vec{x})现在，我们把上述计算$a_4,a_5,a_6,a_7$的四个式子写到一个矩阵里面，每个式子作为矩阵的一行，就可以利用矩阵来表示他们的计算了。令 \vec{a}=\left[\begin{array}{c} a_4\\ a_5\\ a_6\\ a_7\\ \end{array}\right]\vec{W}=\left[\begin{array}{c} \vec{w_4}\\ \vec{w_5}\\ \vec{w_6}\\ \vec{w_7}\\ \end{array}\right]=\left[\begin{matrix} w_{41}& w_{42}& w_{43}& w_{4b}\\ w_{51}& w_{52}& w_{53}& w_{5b}\\ w_{61}& w_{62}& w_{63}& w_{6b}\\ w_{71}& w_{72}& w_{73}& w_{7b}\\ \end{matrix}\right] f\left(\left[\begin{array}{c} x_1\\ x_2\\ ··\\ ··\\ \end{array}\right]\right)=\left[\begin{array}{c} f\left(x_1\right)\\ f\left(x_2\right)\\ f\left(x_3\right)\\ ···\\ \end{array}\right]代入前面的一组式子，得到\vec{a}=f(W·\vec{x})在上式中，$f$是激活函数，在本例中为sigmoid函数；$W$是某一层的权重矩阵；$\vec{x}$是某层的输入向量；$\vec{a}$是某层的输出向量。它说明了神经网络的每一层的作用实际上就是先将输入向量左乘一个数组进行线性变换，得到一个新的向量，然后再对这个向量逐元素应用一个激活函数。 每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重举证分别为$W_1,W_2,W_3,W_4$,每个隐藏层的输出分别是$\vec{a_1},\vec{a_2} ,\vec{a_3} $，神经网络的输入为$\vec{x}$，神经网络的输入为$\vec{y}$，如下图所示：则每一层的输出向量的计算可以表示为： \vec{a_1}=f(W_1·\vec{x})\vec{a_2}=f(W_2·\vec{a_1})\vec{a_3}=f(W_3·\vec{a_2})\vec{y}=f(W_4·\vec{a_3})这就是神经网络输出值的计算方法。 三、反向传导算法3.1 损失函数与正则化项假设我们有一个固定样本集$\{(x^{(1)},y^{(1)}),···,(x^{(m)},y^{(m)})\}$,它包含$m$个样本。我们可以用批量梯度下降法来求解神经网络。具体来讲，对于单个样例$(x,y)$，其代价函数为： J(W,b;x,y)=\frac{1}{2}||h_{W,b}{(x)}-y||^2这是一个平方误差损失函数。对于包含$m$个样本的数据集，我们可以定义整体的损失函数为： J\left(W,b\right)=\left[\frac{1}{m}\sum_{i=1}^m{J\left(W,b;x^{\left(i\right)},y^{\left(j\right)}\right)}\right]+\frac{\lambda}{2}\sum_{l=1}^{n_l-1}{\sum_{i=1}^{s_l}{\sum_{j=1}^{s_{l+1}}{\left(W_{ji}^{\left(l\right)}\right)^2}}}=\left[\frac{1}{m}\sum_{i=1}^m{\frac{1}{2}}\parallel h_{W,b}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\parallel^2\right]+\frac{\lambda}{2}\sum_{l=1}^{n_l-1}{\sum_{i=1}^{s_l}{\sum_{j=1}^{s_{l+1}}{\left(W_{ij}^{\left(l\right)}\right)^2}}}以上关于$J(W,b)$定义中的第一项是均方误差项，第二项是一个正则化项，也叫权重衰减项，其目的就是减小权重的幅度，防止过度拟合。权重衰减参数$\lambda$用于控制公式中两项的相对重要性。需要注意的是，$J(W,b;x,y)$是针对单个样本计算得到的方差代价函数；$J(W,b)$是整体样本代价函数，它包含权重衰减项。 3.2 反向传播算法反向传播算法其实就是链式求导法则的应用。然而，这个如此简单且显而易见的方法，却是在Roseblatt剔除感知机算法将近30年之后才被发明和普及的。接下来，我们用链式求导法则来推导反向传播算法。 按照机器学习的通用套路，我们先确定神经网络的目标函数，然后用随机梯度下降优化算法去求目标函数最小值时的参数值。 假设我们的参数集合为$\theta =\{w_1,w_2,···,b_1,b_2···\}$，设初始参数为$\theta^0$，将损失函数$L(\theta)$分别对参数求导： \nabla L\left(\theta\right) =\left[\begin{array}{c} \partial L\left(\theta\right)/\partial w_1\\ \partial L\left(\theta\right)/\partial w_2\\ ···\\ \partial L\left(\theta\right)/\partial b_1\\ \partial L\left(\theta\right)/\partial b_2\\ ···\\ \end{array}\right]计算$\nabla L(\theta^0)$，参数更新 \theta ^1=\theta ^0-\eta \nabla L(\theta^0)计算$\nabla L(\theta ^1)$，参数更新 \theta ^2=\theta ^1-\eta \nabla L(\theta^1)因为推导过程需要用到链式法则，具体如下图所示：我们定义整体损失函数为： L\left(\theta\right)=\sum_{n=1}^N{C^n\left(\theta\right)}对参数$w$求偏导： \frac{\partial L\left(\theta\right)}{\partial w}=\sum_{n=1}^N{\frac{\partial C^n\left(\theta\right)}{\partial w}}因此我们只需要求出单个样例的偏导数，就可以推导出整体损失函数的偏导数。根据链式法则，对于某一个节点，如下所示： \frac{\partial C}{\partial w}=\frac{\partial z}{\partial w}\frac{\partial C}{\partial z}容易得到 \partial z/\partial w_1=x_1 \partial z/\partial w_2=x_2我们可以利用前向传导的方法计算出所有层的$\frac{\partial z} {\partial w}$ 我们已经求出了整个偏导数的左半部分，接下来看右半部分，即$\frac{\partial C}{\partial z}$。 根据链式法则得到： \frac{\partial C}{\partial z}=\frac{\partial a}{\partial z}\frac{\partial C}{\partial a}对于$\frac{\partial a}{\partial z}$，我们知道就是激活函数对加法器的偏导，知道了激活函数便知道了$\frac{\partial a}{\partial z}$，我们设其求导结果为$\partial ‘ (z)$，因为$z$在前向传播中已经确定，所以$\partial ‘ (z)$其实是一个常数。接下来看$\frac{\partial C}{\partial a}$根据链式求导法则 \frac{\partial C}{\partial a}=\frac{\partial z'}{\partial a}\frac{\partial C}{\partial z'}+\frac{\partial z''}{\partial a}\frac{\partial C}{\partial z''}易知$\frac{\partial z’}{\partial a}$即为权值，而$\frac{\partial C}{\partial z’}$假设其已知，则我们可以得到 \frac{\partial C}{\partial z}=\sigma '\left(z\right)\left[w_3\frac{\partial C}{\partial z'}+w_4\frac{\partial C}{\partial z''}\right]而对于$\frac{\partial C}{\partial z}$的求导，我们需要区分输出层和隐藏层两种情况： 第一种情况。如果已经是输出层了，如下图所示我们可以直接求得。 第二种情况。如果还处于隐藏层，我们可以根据上述算法不断递归的计算$\partial C /\partial z$，直到抵达输出层。 最后总结一下，我们根据前向传播算法求得所有的$\frac{\partial z}{\partial w}$，根据反向传播算法求得所有的$\frac{\partial C}{\partial z}$（需要用到前向传播算法求得的$\frac{\partial a}{\partial z}$，即$\sigma ‘\left(z\right)$）。这样就可以用更新公式对参数进行迭代更新了。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>反向传播算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人大应统部落（1）：2016人大应统学长倾心经验贴]]></title>
    <url>%2F2017%2F04%2F04%2F%E4%BA%BA%E5%A4%A7%E5%BA%94%E7%BB%9F%E5%AD%A6%E9%95%BF%E5%80%BE%E5%BF%83%E7%BB%8F%E9%AA%8C%E8%B4%B4%2F</url>
    <content type="text"><![CDATA[扫一扫，关注【应统联盟】公众号或添加微信 zhanghua63170140 2016人大考研总算是落下了帷幕，大半年来的努力终于落定。在北京的春天里，窗外飘着杨絮，在图书馆的沙发上码下这难忘时光的的锤炼与凝结。 初试成绩：总分392（政治68、英语73、数学三121、统计学130） 复试成绩：总分292（英语笔试40、英语面试40、专业课笔试77、专业课面试135 复习起始：初试： 8月10日-12月26日 | 复试： 2月20日-3月11日 日常安排 时刻 事项 6:30-7:30 起床洗漱早餐、七点半准时到海洋楼坐定，考研四个小伙伴轮流占座。 7:30-8:30 扇贝单词打卡一小时（这个习惯坚持到了考研最后一天还是蛮给力的） 8:30-11:00 数学（雷打不动的看书做题） 11:00-11:30 午餐（总会看到一个法兰西女孩，像西西里的美丽传说里的莫妮卡一样风姿卓越，食欲大增） 11:30-13:00 数学（消化午餐的同时还能做做数学） 13:00-14:00 午睡（这个是每日必备良药，不午睡下午的效率等于0） 14:00-17:00 英语（有几天没午睡，这段时间就会是睡过去的） 17:00-17:30 晚餐（晚餐的时候见不到那个法兰西女孩，食欲大减） 17:30-20:00 政治（每天两个半小时足够了） 20:00-23:00 专业课（因为跨专业、这个是真的有很努力在学） 当然每天都不会完完全全的严丝合缝的来，到后期就是固定这个模式了，考研的四个小伙伴也是在不断地磨合中才基本保持了一致的计划，抱团的作用就在于此，当你投入不进去的时候，自然会有外力来催促你，除非你的个人毅力可以抵制该死的懒癌，你求知的欲望超过你对舒适的渴求，否则，建议抱团。 一、考研专业选择本科学的工科，学的不精，但对计算机有接触，算是有点编程的底子，慢慢的了解到大数据专业，自己也摸索着学习了一些有关数据挖掘的模型算法，蛮感兴趣的。最近在拜读吴军博士的《数学之美》，科普下这个领域，更深入的也正处于摸索阶段，之前在人大大数据老胡学长那里要到了几本大块头的书，英文版，头疼中，坚持啃完。当然敲代码的日子我想应该就会换来蓬头垢面的自己吧。 个人对这种跨领域的东西特别着迷，就像最近博弈论的发展方向，复旦韦森教授总结道：“沿着道德哲学、政治哲学把休谟、卢梭、康德到罗尔斯的思想和理论程式化，致力于回复经济学的亚当·斯密的古典传统，即从审慎推理（prudential reasoning——即目前经济学家所言的理性最大化推理）和道德推理（moral reasoning）两个维度研究人们的经济与社会行为，并从中折射出制度的伦理维度和道德基础来。在这个研究向量上的经济学家主要有已过世的诺奖得主哈森伊、宾默尔以及萨金。笔者也发现，由一些谙熟现代博弈论分析工具的一些当代思想家——如美国加州大学洛杉矶分校的人类学家Rob Boyd，美国麻省大学的经济学家Herbert Gintis，慕尼黑大学的经济学家Ernst Fehr，以及麻省理工学院的著名博弈论大师弗登博格（DrewFudenberg）等，最近从文化、互惠合作（reciprocity）、利己和利他行为的产生及其在社会选择中作用等相关领域的探索非常值得我们注意。笔者这里贸然推断，这些文化人类学、经济学和博弈论的跨学科的合作研究，也许在不久的将来会汇合哈森伊、宾默尔以及萨金在伦理与社会选择探索向量上的已有理论探索。这一研究向量的理论从任何当今一门社会科学的学科视角来看均无疑代表了人类认识社会和自身的目前最前沿思考，且与经济学的制度分析的最深层基础密切相关联（这是我把他们的工作也视作为经济学的制度分析的主要理由）。笔者目前甚至乐观地估计，如果在这一研究向量中以哈森伊、宾默尔所代表的“经济学—伦理学—政治哲学—博弈论的交叉分析”与另一方面的“文化人类学—经济学—博弈论”的跨学科研究汇融起来的话，这又将会在二十一世纪谱写并演奏出一首宏大与辉煌的“理论交响曲”，从而极大地推进人类对自身所处的社会和人本身的认识和理解。” 之所以摘录那么长一段，是因为个人对于像韦森教授的文本话语中所展现出来的厚重的学术穿透力，像何怀宏教授这样的对正义美好的生活崇尚、像香港中文大学周保松教授对自由民主的理性而温柔的认可与支持、像钱理群教授对生命的关切和对知识的渴求所呈现的高尚的个人品格尤为佩服，他们的精神与智慧必将永远的接续下去，徜徉于他们的文字中，我最为真切感受到的莫过于他们所严肃实践的一种生活理念，这是一种热切的关怀、是一种直抵生命深处的质问、亦是一种现实世界中的身体力行的实践思想，它们似乎与主流思维相悖，这种影响逐渐在我们这一辈的学子中发生，生根发芽，我们开始从无数年被灌输出来的满脑子“绝对真理”中走出来，开始接受世界的丰富性，这即是自由的开端。感谢他们这样的勇士。 二、考研院校选择本科在北京，这个学校唯一让我眷恋的就是，他可能是全中国最幸福的院校，从学校出发去北大人大，半小时车程，清华徒步即可，那时候挚爱哲学，跑去北大人大哲学院蹭课，每个老师的课都会零零碎碎的听一点，北大戴锦华老师、人大周濂老师的课尤为喜爱。北大国家发展研究院（ccer）的课也去蹭了一学期，那时候汪丁丁的新政治经济学和行为经济学，第一次去上课的时候，斗胆坐在第一排，捧着那本融合了脑神经学、认知心理学、经济学、哲学、政治学、社会学、伦理学、宗教和神秘主义的《新政治经济学》，扑面而来的知识模块，令人头晕目眩，结果便是全程睡过去的，万圣书园刘苏里称道他或许是我们这个时代少有的文艺复兴式的知识人，考研完了再次拜读老师的书，书中呈现出来的渊博之魅让人瞠目结舌。北京最让人眷恋的便是这种藏匿在深处的知识图景，学识对撞的读书会、严肃学术的研讨会、陶冶人心的音乐会、感同身受的小剧场话剧，纵然周围是雾霾缭绕，但当你挖掘到内核，或许就再也不想离开。对北京的这种念想让我毅然决然的选择了北京的研究生院，考研之前，对高校的应用统计专业略微地进行了考查。 人大：经济统计、精算、医学统计、国民经济这几块人大是老字号了，在全国都名列前茅。但最让我热血沸腾的是近两年刚开办的五校协同交叉培养大数据方向专业硕士，对于这个平台，袁卫老师有很赞的叙述：“5所学校参与培养，就是出于学科交叉的考虑。中国人民大学统计学院的学科、专业设置是综合的、应用的，理论和应用兼而有之，应用领域涉及卫生、健康、经济、社会、管理等，总体实力较强。而北京大学和中国科学院大学，大家都知道，他们在计算机、数学和统计理论研究方面相当强，掌握大数据分析技术的前沿。中央财经大学和首都经贸大学是财经类为主的院校，这两所学校侧重于应用人才的培养，特别是面向经济、管理、社会这样的领域。他们和很多行业企业、金融机构有着密切联系。这5所高校分别属于教育部直属高校、中国科学院的高校和地方高校3种类型，各有特色，优势互补，能够建成一个很好的、学科交叉的人才培养协同体。”所以，心里其实早就锁定这个了。 清华：招生人数太少，信息严重不对称，也清楚认识到自己没那么强悍的能力，就没怎么考虑。 北大：之前在网上看到北大444哥考应用统计的雄文，看得我是荷尔蒙暴增，一冲动就买来了所有的参考书，结果发现，专业课的难度自己很难把握。而且北大分数线蛮高的，几乎每年都在390左右。学长应该是毕业了，感谢他，因为我的很多经验都会参考他，当时是打印出来一个字一个字拜读。给出他的经验贴链接吧：我的444分北大考研成功经验谈 其他：考研帮有个帖子，比较全面的分析了全国各考研院校的应用统计专业，大家可以参考下：应用统计全国高校分析 三、各科复习方法3.1 英语复习用书： 扇贝APP 张剑考研真题黄皮书 张剑阅读理解150篇 王江涛高分写作 复习方案： 单词：没有用很厚的单词书，感觉会压垮自己，所以选择了app来背，一开始使用的是新东方的乐词，但是亲测效果不佳，后来经研友推荐上了扇贝的船，从此每天早上就开始打卡背诵，保证一个小时的量。特别要注意的是背单词必须要保持连续性，不能中间隔开半个月或者一个月搁置在那里而不背单词了，每天背，这是一个积少成多的过程，我就是一个反面例子，中间有段时间特别抵触背单词，荒了快一个月，结果就是做阅读的时候单词在耳边就是想不起来意思。到考研前要达到的效果就是看到一个单词就立马反应过来，背上三四遍之后就比较省力了，有的时候一小时可以背诵七百个，当然这是建立在前期不断反复不断反复的基础上，基本每个单词app都是依照记忆曲线帮你安排任务，所以也不用担心会漏背或者背不熟。 完形填空、新题型、翻译：这三部分分值占的比较小，基本上每个人都可以得到个基本分数，所以就没太花时间在这上面，只做了历年真题里面的这些部分，然后看黄皮书的分析，掌握一些技巧即可。 阅读： 张剑黄皮书系列的真题基本上是人手一册，真题的研究对于阅读来说是至关重要的，至少要保证两遍以上的联系和琢磨，网上都说英语考80分以上的都是真题研究了四遍五遍的，这是有一定的道理的，阅读的正确与你对命题老师的出题思路的熟悉程度正相关，基本上每一类题都会有特定的规律性，只有当你顺应了老师们的思维模式，在琢磨选项的正误时你都可以类似于套圈子一样套进去，这正是真题的重要性所在，我们可以接受这种思维模式训练的第一手资料就是真题，任何模拟题都无法与之比拟。当然不是说模拟题不需要，在这里推荐张剑的150篇，还是比较贴近真题的，但他的功效仅仅在于提高对新篇章文本的适应性，在考场上难免会遇到和历年真题风格不一致的文章，这就是模拟题的优势所在，这本书基本涵盖了考研英语阅读出现率比较高的话题，你可以拿它当做拓宽英语话语体系的佐料。我当时是花了大约一个月的时间来做这本书，每天下午四十分钟左右做两篇阅读，其他的时间研究前一天做的那两篇，一直循环下去。这里还要说下怎么研究模拟题和真题。拿到一篇文章，按照你的方法做完，然后就是挨个查单词，分析长难句，挨个解读选项和分析，自己从文中找出依据来，最好自己搭建一个文章的框架。 作文：今年的作文幸亏了王江涛，一开始心存侥幸想背个模板就完事了，后来越来越不安，觉得模板作文我难以驾驭的好，如果成篇的万能句型上去，老师必然不买账，估计就是个低分的下场。当然如果真的可以把一个模板变通到炉火纯青，那也一样OK。但对于我这样四级飘过的，六级未过的naïve青年来说怎么会有那么好的英语修养呢，所以最最后一个月算是逼着自己按照王道长的指示安安分分的背诵了8篇范文，滚瓜烂熟，倒背如流，只能这样。道长说了，英语作文看的是你的语言功底，只要和主题搭上边了，没啥语法错误，词汇量足够，还能写出漂亮的句型来，那就不会差到哪里去。所以，背诵历年真题就是最好的办法，踏踏实实的背诵、默写、仿写，王江涛在书里说的都很清楚。 友情提醒：再一次华丽丽的证明了自己的心理素质是多么不堪一击，哈哈，我边上的童鞋老抖腿，以至于我总是有意无意的要观察他的抖腿频率，这让我几近奔溃，没举报老师怪我太善良，结果就是做阅读完全不在状态，基本上就靠语感在做了，我谢谢他全家，我不造其他人被影响到没，怪只怪自己心理素质就像一块薄冰，一碰就会碎。故，学弟学妹们在考场上一定要杜绝这种危害社会健康的事情发生，一经发现，向朝阳群众学习，立马举报上级。 3.2 政治考了68，不敢拿出来献丑，对于这种被人戏谑的学科，其实是拒绝介绍经验的，先说几个好玩的政治段子：“靠别人，你永远是右倾投降主义，靠自己你才是工农武装割据”、“如果全世界都对你恶语相加，我愿对你说上一世纪社会主义核心价值观”、“你我之间本无缘分，全靠党的章程死撑”、“别低头！GDP会掉！别落泪！资本主义会笑”、“想和你谈一场弘扬社会主义正能量的恋爱，你却要我好好做自己的中国梦”。茶余饭后看点段子还是蛮有意思的。 复习用书：（按出场顺序排列） 《思想政治理论考研大纲解析》 肖秀荣《命题人1000题》 风中劲草《冲刺背诵核心考点》 肖秀荣《命题人冲刺八套卷》 启航《20天20题》 肖秀荣《命题人形势与政策》 肖秀荣《终极预测四套卷》、任汝芬《最后四套卷》、任燕翔《考前预测4套卷》 复习方案： 9月5日-10月10日：九月份考研大纲解析发布，开始快马加鞭的看，一块让人厌恶的砖头，不过你还是得静下心来去啃，尝试着一字一句的过，网上流传着一张图，内容是这样的：恩格斯问大胡子马克思先生：“你在干嘛呢”。马克思心平气和的回答道：“管他呢，反正又不是我背”。中国学生对其抵触的心理可见一斑。但其实真正的马克思先生的思想很可贵，以至于20世纪法国解构主义哲学大师德里达在他的著作《马克思的幽灵》中写道：“不能没有马克思，没有马克思，没有对马克思的记忆，没有马克思的遗产，也就没有将来；无论如何得有某个马克思，得有他的才华，至少得有他的某种精神。现在该维护马克思的幽灵们了。”只是在这个极权盛行的国度，马克思变成了鬼魂，笼罩这苍茫的大地，最后沦落为官方口腔，这才是知识分子真正的悲哀，依附于权力而放弃说理。言归正传，大纲解析的脉络其实是很清晰的，看不懂也没有大碍，拿出肖秀荣先生的《1000题》，看完大纲一个章节的内容你就要把《1000题》上对应的章节的选择题给做了，不要看书，把答案写在一张A4上，注意1000题是要做三遍的，第一遍做都会错的惨不忍睹，错了没事，切忌欺骗自己看看答案把错的题给改对了，因为学长就是这样喜欢自我欺骗的前车之鉴哈哈。马克思主义和毛中特部分或许会让你略微头疼，这种理论性质的东西充斥着新闻联播的气质，但也务必沉住气，到后面解决史纲和思修就是分分钟的事，在高中阶段谁都学过维新变法、辛亥革命之类的，学起来还可能会让你增生一些兴趣。在这里安利一部良心巨制《走向共和》，看完这部电视剧会让你对中国近代史的基本脉络有一个清晰的呈现，记得本科的时候看徐中约先生的《中国近代史》，看得我是心力交瘁，后经学长推荐才去看的这部戏，看完后再回头看那本书，不适感就下降很多。下一步要做的事很重要，就是把你做错的题目，从大纲解析里面找答案，用晨光彩色标记笔标注出来。好了这一遍下来，务必请你自己做一个粗略的知识回顾和框架的搭建，每个章节在讲什么内容，拿毛中特部分举个例子，第一章提纲挈领先介绍了马克思主义中国化的两大理论：毛爷爷思想和中特理论体系，也是同上一部分的马克思主义的衔接，然后从第二章开始，分别是第二章新民主主义革命（1919-1949），第三章社会主义改造（1949-1956），第四章社会主义建设（1956-1978），第五章和第六章插播了总依据和总任务，因为接下来的是第七章改革开放了（1978-不知道啥时候结束），再然后是考研政治的重点，第八章总布局，这章内容及其丰富，包括中特经济、政治、文化、社会、生态文明。紧接着就是祖国统一、外交国际战略，最后两章是建设中特的相关问题，总结陈词就是党好党棒棒哒领导好领导棒棒哒。你可以自己建立一个思维导图，带着这个脉络你可以顺利的进入到下一关，这些基础工作是为后期服务的。 10月10日-11月5日：第二遍重复上一步的内容，看大纲，做1000题，纠错回大纲标注。注意第二遍的时候你可以把答案写在1000题上了，然后看1000题后面的答案，把错的以及你觉得好的题的解析在题目边上标注下，要知道为什么错，举一反三。 11月6日-考研结束：刷完了两编大纲和1000题，这时候会出现一本震撼人心的资料出现—风中劲草，这本书的编排和印刷是下了一番功夫的，他的细节之处可以让你真的佩服这本书的作者，跟进大纲，条理清晰，标注分明，重点突出，考研资料中的扛鼎之作。所以，你一定要把这本书当做是你考研政治的制胜法宝，你该怎么做呢？看，一个字一个字的看，我当时就有种心态，自己可以假装看懂普鲁斯特的《追忆似水年华》，我就必须要看透杨杰先生的《风中劲草》，两个时代的回响多璀璨。直到考研之前，你也不要放下这本书，看的遍数越积越多，你就会达到你自己都意想不到的层次，就是合上书，你大概可以知道哪个知识点在那一页的哪块位置。我保持的速度大概是1天15页左右，15天一本书，到考研结束加起来看了三遍。马原毛中特部分你可以多看几遍，四遍五遍无上限。对了，这时候我还同时做了一件事，就是第一遍的时候只是把1000题上的每一道题都在风中劲草上标注，若是单选题第一题，就标注“单1”，多选类推，同时用彩色笔标注。此外，肖秀荣的《形势与政策》也出来了，买来利用空余时间看两遍。 12月15日-考研结束：各种模拟预测题纷纷登场，这里首推肖秀荣的《8套卷》和《4套卷》，可以去学校打印店购买，便宜实惠，八套卷你都要当做考试一样对待，基本上三十分钟就可以完成一套，完成一套之后看下答案分析，纠错，在风中劲草上标注，如肖秀荣第三套第1题，就标注“肖8三1”。因为之前咱们已经完成了以下任务：大纲解析两遍，1000题两遍，风中劲草两遍左右，再加上不断的标注，8套卷就是检验你复习成果的最佳试题，肖秀荣老师编的资料都棒呆，个人灰常喜欢他的讲课风格，一口流利的方言普通话，考研期间要随时关注肖老师的微博微信，都是同步的，关注一个就可以，把它发布的一些重要文件下载下来，打印研究。八套卷你可以做两遍三遍，注意要举一反三，尤其是错题。然后过几天4套卷就会隆重登场，基本是人手一套，不要迟疑，买来赶紧把客观题做了，按照之前的流程对待客观题。接下来就是万众瞩目的主观题，大家从开始到现在还没有接触过主观题，4套卷就是专门为了主观题准备的，如果你不想留太多的时间和精力在政治上（毕竟政治只有100分，数学和专业课才是重中之重），那就建议你只背诵4套卷的主观题，之前提到的启航20天20题可以翻翻，虽然好，但知识点太多，没时间应付，咱们把赌注押在肖秀荣的4套卷上，背诵的时候主要要挑关键词背诵，自己想法子变通式的背下来，切忌原封不动的机械背诵，虽然这几年老爷子押原题的能力衰弱了，但每年的知识点还是压得相当准的，当然像今年很多人说蒋中挺几乎全压中原题了，没怎么看过他的资料，不予置评。 考场上：考政治的时候觉得选择题so easy啊，做到主观题，有点懵逼了，肖大大押中的题的答案全变成了考研的题干，只能硬着头皮上了，相关的知识点全答上去了，生死未卜的赶脚，有一道家庭美德的就全靠扯了。所以，我的复习方案可以给大家敲响警钟，要是想要考高分的，主观题也是要早点准备的，尽量多参考几个考研机构的预测卷，稍微整理下答题的思路，预防真题出现一些偏题。 说在后面：考研政治的时效性特别明显，16年开了十八届五中全会，那么有关它的内容一定会是考研的重头戏，今年客观题和主观题都有一定的体现，而且比重还不低。所以，形势与政治也要多加注意，有时间就多看几遍，尤其是考前那几天，加深印象，有的关键词列点背诵。肖大大解答过考研命题人如何出题：先确定要考的知识点，然后去报纸和杂志上找相应的材料。所以，我们必须要对知识点分外敏感，在不同的模拟题中间总结出知识点来。 另外推荐几个不错的复习资料： 肖秀荣的【马克思主义基本原理概论逻辑图】，哲学的主观题就全靠这个资料来沥青脉络了。 肖秀荣的【近代史时间轴】：把近代发生的事件按照时间顺序排列，一些重要的知识点也有叙述。 肖秀荣的《知识点提要》八个附录：网上也有，可以看看背背啥的。 这样下来政治需要看的东西也蛮多的，时间要自己控制好，到了后期，专业课要背，英语作文要背，政治也要背，别被他们压垮，挺过去！ 3.3 数学三 复习资料 张宇数学三的视频课 《李永乐复习全书》大红色 《李永乐660题》 《李永乐历年真题》 40套模拟题（《张宇8套卷》、《4套卷》、《永乐6套卷》、《历年合工大最后五套卷》、《400题》） 复习方案： 8月10日-9月1日：花了将近一个月的时间来看张宇的视频，他的整个讲解的框架体系蛮成熟的，按照他的指示把笔记全部抄下来，然后自己尝试着背诵，这样下来就可以搭建起数学三的整体的脉络，知道要考的知识点和题型。我个人认为这个框架的搭建对于学习数学来说太重要了，他可以帮助你以一种高屋建瓴的视角来面对你所遇见的各种题型，而不至于迷失在茫茫的题海中无法自拔，它就像在你的脑海中植入了一份详尽的探险地图，遇见一个题，你可以将其归入体系中的某个知识点，这样的训练增加之后，对你的做题速度和准确率也会有很大的提升。一定要有这样的意识将知识归整而不是碎片化存在于你的大脑中，一个成熟的知识体系都会是如此，麻省理工大学的数学大咖林达华在讲解自己的数学体系时，必然脑海中有这样的一个完备的详尽的清晰的图景。 9月1日-10月15日：进入考研攻坚期，复习全书是必备的，因为数学是上午考，我也象征性的把复习时间安排在了上午，每天看10页左右，消化不了太多，一些原则：1、必须自己拿笔写，切忌眼高手低以为看看就会了/2、切忌还没怎么思考就看答案解析，不会做没事，自己思考的过程尤为关键，在每道题的边上写下自己的思考推算过程以及这道题的关键之处、3、琢磨好久都搞不明白的，可以询问大神研友，或者自己做个标记，以后来解决（我后来忘记我曾经有不会的题了，就是这么大马哈）。数学其实有点像练书法，一开始可能你的水准只能够临摹大师们的作品，还只能学个皮毛，但重复训练达到一定的层次之后，你会形成自己的笔法（数学思维方式），在之后对于从未涉猎的新帖（新题型）也可以驾轻熟重。 10月16日-11月10日：复习全书完了之后，一本虐人无数的660题登场，别以为他全是选择题和填空题，但他的每一道题都是精心锤炼过得，所体现的数学思想方法绝对会让你获益匪浅，他的题目的设置真的恰到好处，细细的琢磨每一道题的精髓，虽然真题是绝对达不到这样的难度的，关键的是思想方法。我大概刷了20多天，但有些题后来又忘记回过头去考虑了，这就落下不少病根， 所以建议复习的早一点，可以有更多的时间来调整自己的复习计划。 11月11日-11月20日：从光棍节那天清晨开始，我拿起了真题，花了十天时间每天完成一套卷子，因为很多题其实在你做全书的时候已经遇到过了，所以其实真题对于真实水平的评估还是有很大偏差的，对完答案，订正，错的题的解题思路思考一遍就完事，当时的分数基本保持在120-140之间，也没有太大的失常，第一，历年真题相对于之前的训练还是简单一些的，第二，平时的训练不紧张，三个小时基本上都是轻松愉快的度过的，那时候每天早上起床就盼着可以做数学真题了，就像恋爱一样。 11月20日-12月23日：接下来来到了我个人最为推崇的一种方法，就是数学套卷模拟，在这个阶段中，每做一套模拟题，就可以把所有的章节的重要内容复习一遍，尽管无法覆盖每一个知识点，但模拟题的编写还是有一定的规律的，可以让你随机的复习到一些重要的知识点。要遵守几个原则：1.三小时一套，时间到了就停止答题，然后根据答案自己批分数。2.必须严格遵守考研数学设定的考场规则，不能看书，不能交头接耳，不能询问学神研友，不能嚼口香糖。3.交叉训练法，每个老师出题的风格可能不一样，你可以先做两套张宇的，再做两套李永乐的，这样循环着做，可以增强你的适应能力，亲测有效。至于该做哪些模拟题，我推荐的都在上边写着，这个方法也是借鉴北大444哥的。为什么要建议这个办法呢，因为考研数学今年风格大变，像线性代数和概率论与数理统计的大题都是很难遇到的，那怎么办呢，就是不断地训练模拟题，不断地遇到新题，不断地提高自己的解题能力，而且这样的实战模拟也会让你开始意识到考场上的时间分配是何其重要，3个小时，挑大肉吃，有的是在太难的，抛开也无妨，考场的战略是需要在平时的训练中积累的。此外，你的书写也需要在这段时间里训练，张宇的8套卷和4套卷都提供了和考研一模一样的答题纸，不要大手大脚的乱答题，解题的条理性要注意。合工大的那几套题真的很不错，今年在考场上做高数的时候相当顺利的原因就是这些题型基本都在那15套卷子里遇到过了，所以，直到高数大题做完，我只花了1个小时20分钟，最后剩下一个半小时左右的时间来解决线代和概率大题，但是，我真是个天生的考场悲剧制造者，详情见下。 考场上： 因为考研期间基本上很少运动，打个球跑个步都是奢侈的不行，散步也成了浪费时间的活儿，后来自己也尝到恶果了，我的小心脏承受不住了，有时候会突然之间心跳加速到200多次每分钟，去校医院检查的时候医生说说是有阵发性室上性心动过速，吓得我够呛，问医生为啥，他说是我的心脏短路了，很多时候是因为焦虑不安或者过度兴奋造成的，好吧，那时候都到了考研的冲刺期，我也只能硬挺着，反正医生说没什么生命危险，然后就听到了考研的那天。在考研数学的考场上，我因为一个小时十分钟就完成了高数部分，high的不行了，一兴奋，犯病了，心跳开始砰砰砰的上去，以至于我无法正常答题，短时间治愈这个病的办法就是蹲下去深呼吸，于是我就申请去走廊自己做深蹲，然后深呼吸，深蹲深呼吸，当时真的快要奔溃了，以为这场试就这么完蛋了。深蹲深呼吸了好久，大约过了十五分钟，还是没有恢复过来，感觉真的没救了，老师也一直在边上看着我，该咋办，我的天哪，情急之下我开始捶自己的胸部，一锤倒是好了，但是背部还是有明显的不适感，总觉得有东西在怼我，就在这样的状态下，勉强答完了题，再加上线代和概率题和之前的训练风格太不一样了，我就有点崩溃了，连时间都看错了，原本是11点半结束，我却以为11点就结束了，情急之下把线代大题答得满卷子都是，感觉都看不清楚了，菩萨保佑吧，希望老师可以手下留情。所以，从我身上可以吸取的教训就是平时要注意身体，有时间就去跑跑步打打球，千万不能输在身体上。最后数学考了121分，也算是谢天谢地了。 3.4 432统计学复习用书： 贾俊平《统计学》第四版（经管类） 贾俊平《统计学》第六版（21世纪统计学系列教材） 何晓群《多元统计分析》 王燕 《时间序列分析》 何晓群《应用回归分析》 复习方案： 贾俊平《统计学》：一开始看的是第六版，作为门外汉的我觉得这本书还是蛮简单的，因为之前学过数理统计的一些课程，所以理解起来也不难，而且框架体系也比较清晰，基本上一个章节一天就OK，看了两遍，然后整理了自己的笔记。后来了解到原来第四版的内容更饱满一些，就把第六版没有的内容补看了下，做了笔记。而且今年出事的时候出了一道实验设计的题，最后阶段预测的时候是万万没有想到会出这个题，所以，建议看第四版，内容全。但我又比较喜欢第六版的表述，两本结合着看吧，但是第六版上没有的内容一定要补全，像实验设计、哑变量、指数平滑、主成分和因子分析、聚类分析（这俩个属于多元统计分析）都要添加上去。非参数统计我看了一遍，整理了下笔记，稍微背诵了下，不过复试的时候有人被问到了，所以也要好好看。复试的时候老师问了我关于哑变量的，幸好看了第四版。 何晓群《多元统计分析》：说实在的这本书写得像哲学，感觉是直接从英文版翻译过来的，很多表述没有那么通俗易懂。我只看到了第八章典型相关分析，真的很难说会不会考之后那几章，就目前来看是小概率事件。这本书最关键的是统计分析方法的基本理论原理、分析步骤和以及去对应可以解决的问题，不需要去死抠推导过程，这不是432需要重视的，但是对于理解还是有帮助的，有兴趣的可以推推看。16年没考这部分内容，但不能预计17年会不会考，要复习的全面一些。一些问答题都要结合历年真题自己根据课本进行总结。 王燕《时间序列分析》：这本书的编写就相对好得多，条理很清晰，思路引导很顺畅，一些例题也比较易懂，重点是各种预测描述模型，今年考了一道08年学硕考过的题：有趋势有季节变动可建立的模型，写出模型形式并简要说明。可见学硕的历年真题也是很有借鉴意义的。之前也考过差分运算的，复习的时候也要注意这种细节，但是这本书里面的例子特别好，几道题对应相应的知识点，只要你一点点看下来理解了，然后把笔记整理好，后期再背诵下，应该没啥问题。 何晓群《应用回归分析》:一直以为何晓群老师是个女老师，后来复试的时候才了解到并非如此。这本书写的也很有条理，多重共线性的后果诊断处理已经多次考到，自相关性和异方差还没出过，今年考了一个判定系数的解释，当时预测了几道觉得会考的题，里面就有判定系数和回归模型的综合评价，初试的时候就考到了，这个虽然比较简单，但可以尝试的方法就是在考试之前，自己预测一些题，自己给自己出题做，涵盖面广一些，会有意想不到的结果的。 关于真题：真题强调上百遍都不夸张，他对于你复习的方向有很大的启示作用。我当时的做法就是把真题整理成八个专题，分别是：《专题一：图表展示与概括性度量》、《专题二：统计量与抽样分布》、《专题三：参数估计与假设检验》、《专题四：分类数据分析》、《专题五：方差分析与实验设计》、《专题六：回归分析》、《专题七：时间序列分析》、《专题八：多元统计分析》，学硕和专硕的历年真题都要整理分类，基本上人大每年考的都包含在八个专题之间，你需要做的就是自己认认真真的从课本上找出答案来，然后总结一遍，一些学硕要求的比较偏数理的可以忽略，需要明确的是，重点一定会反反复复的考，而且乐此不疲，像今年时序和回归的题都是曾经考过的，几乎一模一样。 关于笔记：自己整理的笔记的字迹一定要清晰，条理要很清楚，但这是建立在你把书看了几遍理解透了之后才可以做到的事，当然一开始不理解，到后面反复的背诵就会逐渐清晰起来了。当时我是和真题一样分了八个专题，参照人大大数据陈思聪学长的笔记整理了手写的笔记，学长的笔记结构完整，内容完善，当时是如获至宝，每天看着它整理自己的笔记的心情相当愉悦，对我的专业课起到了至关重要的作用，在这里谢谢学长。在复习过程中，我发现自己常常会对知识的首次记忆有所偏颇，只知其一不知其二，以为已经完全理解了其确切的意思，但其实当我在复试复习的时候再回过头来看往往会有更多新奇的发现，此时的知识域相对来说也会完善一些。 关于背诵：心理学中有一个广为认可的记忆机制，即：我们在记忆的时候将许多线索（诸如对一个原理的发散性理解、当时联想的事物的多样性）一并编码进入记忆中，能否长时间的保持知识的新鲜感或者说在大脑中的活跃度，取决于这些线索是否足够丰富，这就为理解记忆提供了有力的证词。贯彻于专业课的背诵上，其实各个统计方法知识中包含了精确的概念、严谨的逻辑、一般的原则、生动的背景等无数的记忆线索，而并非是孤立的、任意的文本序列，各个点之间具有并列、递进、相互排斥的种种关系，推导和演绎出这种联系，从而由点到面，搭建成一个大的框架体系，就是我个人比较推崇的思维导图，如此进行下去到考研前几天可以看着那张大的框架图自己逐条背诵，口头表达可以和原文范本有出入，但是关键词必须要锁定，大致意思要接近。 其他：大家如果有专业课的问题，可以向我询问，我尽力解答。最近也在恶补专业知识，毕竟是跨专业，害怕一进人大就被各路大神碾压，大家互相学习吧，或许我的专业素养还比不上学弟学妹呢。和考研小伙伴一起建立了一个微信群，大家伙可以加进来在群里分享应用统计的资料、讨论复习过程中遇到的难题、分享考研路上的酸甜苦辣，啥啥啥都可以。因为微信群已满100人，可以加我的个人微信：zhanghua63170140，拉学弟学妹进群。 四、QA师妹皱着眉头问： 师哥好，我也想考人大统计，本科统计，但只是普通一本。旁边人都说人大太难考，因为我是师范学校，而且我们数科院好几年没有人考到人大，感觉挺迷茫的，也不知道该不该换个学校，但是总觉得不甘心，为什么别人能考上我不能啊？ 师兄皱着眉头答： 在现实世界中，我们的决策往往会倚赖过往的历史经验，就像你所述的，你所在的师范学校的数科院没有人考入过人大统计学院，看到这么惨淡的景象，畏惧心理在所难免，既然他们已经为你趟过这条深水，且已证明这不是一条容易的路，那为何我还要继续当做下一个被湍急的河流卷走的“微弱的个体”呢？且不说投入进去的时间成本以及其他一些不可控因素给自身带来难以计数的艰难险阻，万一这一年的所有努力在成绩出来的那一刹那都付之一炬，名校梦从而化为泡影，岂不是做了一次失败的买卖吗？ 可我想告诫你的却是，人大必须要去考，而且要义无反顾的前往，不要有所畏惧，从你的描述中得知并没有太多的现实因素的阻挠，你仅仅在惧怕强大而无耻的经验施加在你身上的不能承受的阻抗。先说说你会在这条幽深曲折的路上看到哪些曼妙的风景，你或许可以涉猎到从未踏入的知识盲区，当你被无数的知识模块所充盈，你会感受到这样的缓慢累积会给你带来前所未有的愉悦与渴求欲的满足，我们时代的知识分隔已经异常凸显出来了，每一个领域会将拥有一套成熟的体系，而当你掌控着庞大细密的知识网时，你便拥有了铠甲，他将带你在众人面前展示话语的力量，那种力量就是需要这些知识来支撑的。其次，你可能可以收获几个志同道合的朋友，网络的延伸将你的诉求与宣告呈现在他们面前，就像现在我正在尝试着与你促膝长谈一样，也必定会有无数的这样的人尝试着与你建立精神上的关联与挂钩，你们摸索着同一片黑夜，也凝望着同一片蓝天，为这笃定的信念挥汗。你要坚信，总有一天，你会与你精神气质相合的那些可爱的人相聚，就像家人一样聚在一起，所以，不要抗拒孤独，那仅仅是你还没有那样强烈的遇见。最后，也是最重要的，你将迎来新的人生，你说你想考人大统计，我相信你的内心必然会有一股洪荒之力在不断地催促着你，会有一种声音在耳边呼唤你，那便是我们最大的动力，这样的声音会在你颓然之时支起你的躯体，无论什么样的生活的贫乏无趣都驱散不了这种称之为信仰的东西，只要你持续地温存这样的声音，去战胜所有的困惑与不安，正是这种不甘让我们变成一个撑起自己所有维度的勇士。是的，你会变成一个勇士，即使被现实迫害的遍体鳞伤，你也依然可以坚毅地挺立原地，然后，舔舐自己的伤口，继续热烈地往前走。那种热烈，只能自己亲手栽培，别人无法给予，你也不能凭空取得，那是一个个白天黑夜的伏案所换来的最盛大的生命花园，你要在漫长的年华里种上玫瑰、植入梧桐、嵌进宝石，让它灿烂的更彻底点吧，即使荒败了，也要在极度的繁盛中逝去。 酷酷的师弟问： 师兄，我是跨专业，感觉对专业课比较迷惘，不知如何下手，可不可以建议一个比较摸得着套路的专业课复习方案呀？ 严肃的师兄答： 432统计学复习流程建议 贾俊平《统计学》第六版+圣才《贾俊平统计学 笔记与课后习题详解》：花费20天左右的时间进行全篇阅读，不遗漏任何一个点，包括概念、公式、解释、注释、表格、图片、例题，每一个字都要盯上至少一秒钟。每看完一章节的所有内容之后，在A4纸上写下课后思考题与练习题的答案，可以翻阅课本，但必须要自己动手整理归纳或者解答一遍，切记眼高手低，能写入教科书的例题就必然会有其存在的必要性，它可以帮助你梳理课本知识，也可以帮你抓住章节重点，整理这些问答题和计算题的过程也是再一次深入理解知识点的过程，绝不可废弃之。课后的思考题和练习题的答案可以在圣才出版的《贾俊平统计学 笔记与课后习题详解》找到，我只找到了第五版对应的（一共331页），已经上传到网盘里。大家也可以参考人大配套的学习指导书，网上可以买到。这样一遍下来对这本书的框架有一定的了解，强烈建议看完每一章节之后画一张框架结构图，理清知识脉络。重点章节是”第3章：数据的图表展示“、”第4章：数据的概括性度量“（这两个章节联合起来会在真题中考察一道大题）；第6章，统计量概念和中心极限定理是重点；第7-13章，所有的都是重点。第1、2、5、14章可以粗略看一下，非重点。 贾俊平《统计学》第二遍：第二遍依旧要有如第一遍的细致程度，且在第一遍的基础上加深理解，争取可以简单的使用自己的话简述一遍，同时要开始做笔记，按照书本的结构组织笔记，且必须要把这本笔记当做是一样艺术品，用心编排、用心写字、用心画图，重点分明、内容完整、结构清晰，切不可潦草糊弄过关，这本笔记是你在日后的复习中常常会碰面的，翻阅起来可以大大地提高效率，而且看起来愉悦舒心一目了然，何乐而不为？切勿盲目求快，做笔记是一个梳理知识点、更深入理解知识点的过程，欲速则不达，抄一遍了事对理解没有丝毫的助益，下笔之前想明白这句话所指涉的是什么、是否还存留我尚未领会的含义、我能否清晰的在脑海里梳理分析流程等等等，这些都可以增益你对细节的深入探索，慢工出细活，相信我，循序渐进的来，一定会有很大的成效。同时这一遍笔记要把管理学第四版中出现的新知识补充到笔记中，其中的新知识点包括：正态性的评估、实验设计、哑变量回归、非参数统计（注：时间序列分析和多元统计分析的部分内容会在其他两本书中会详细展开，不添加进去也无妨）。 《应用回归分析》：这本书囿于时间只看了前八章，也就是到主成分回归与偏最小二乘估计这一章为止，因为在《统计学》书中已经对这部分的内容有了基本的了解，加上《应用回归分析》书中的推导也不是特别艰深，基本上每一步思路都很清晰，大家可以尝试着推导一遍，加深理解，但这并不是重点，重点在于诸如违背回归方程基本假设条件的三种情况（异方差、自相关、多重共线性）的原因、影响、诊断、处理这类偏向论述、步骤与原理的知识点，所以，如果推导有困难，也不必强求。但其实后两章的非线性回归与定性变量回归模型也比较容易理解，虽然初试考察的概率不大，但为兼顾知识结构的完整性以及复试的时候有可能被老师问及，看一下肯定是有好处的。这本《应用回归分析》也是要看一遍，再做一遍笔记，做笔记的方法与上述一样，不再赘述。 《应用时间序列分析》：这本书的条理狠清晰，大致就是平稳时间序列分析、非平稳时间序列的确定性分析和非平稳时间序列的随机性分析三块内容，考试重点在于若干个时间序列分析模型的结构与性质，譬如AR模型、MA模型、ARMA模型、ARIMA模型、指数平滑法、分解模型、含哑变量的多元回归预测模型等等，要搞清楚每一种模型所适用的时间序列类型、模型中每一个参数代表的含义以及分析的思路与步骤，其他的重点包括差分运算、一些基础的概念等。最后一章考的概率不高，但时间序列最后一章内容在时序分析所占地位是很高的（虚假回归、单位根检验、单整与协整）要是想扩充知识点，，也建议看一遍。同样，整个过程也是看一遍书，整理一遍笔记。 《多元统计分析》：聚类分析、判别分析、主成分分析、因子分析、对应分析、典型相关分析必看，后几章个人认为考的几率不大，看个人时间分配。重点考点是这些多元统计分析方法的基本思想、过程细节、重点性质之类的，历年考过因子分析、判别分析、典型相关分析的相关内容，考的概率蛮大的，绝不能弃看。过程还是一样，第一遍，看书、理解、适当推导，第二遍，做笔记，再次理解，加深印象。 八个专题整理：这是学长根据历年真理的考题分布情况总结出来的八个专题，已经在上述的复习中有所呈现，依次是：“专题一：数据的图表展示与概括性度量”；“专题二：统计量与抽样分布”；“专题三：参数估计与假设检验”；“专题四：分类数据分析”；“专题五：方差分析与实验设计”；“专题六：相关分析与回归分析”；“专题七：时间序列分析”；“专题八：多元统计分析”。几乎每一年都是在这八个专题中抽取七个专题的知识点，例如2015年432真题的排布分别为：第一题属于专题一（数据的图表展示与概括性度量）、第二题属于专题四（分类数据分析）、第三题属于专题五（方差分析与实验设计）、第四题属于专题八（多元统计分析）、第五题属于专题三（参数估计与假设检验）、第六题属于专题六（相关分析与回归分析）、第七题属于专题七（时间序列分析）。还是比较有代表性的，其他年份的分布学弟学妹们也都可以总结一下规律，有助于自主预测考题。大家可以以这个思路去归纳整理自己最终的一份笔记，八个专题，每一个专题都要有结构框架，可以借助思维导图这个工具，每一个专题包括四个部分（第一部分：这一专题的课本内容有序的整理；第二部分：重要问答题整理；第三部分：属于这一专题的真题整理（每一道题的答案一定要完整有序地整理）；第四部分：这一专题的思维导图）。整理完这份专题笔记之后时间也就剩下一个月左右，接下来的时间就是背背背，当然要理解地去背，把笔记与思维导图结合起来，背到滚瓜烂熟，背到天昏地暗，到最后阶段会特别难熬，英语作文要背，政治大题要背，专业课要背，抗住压力就是了。 可爱的小师妹问：你的专业课思维导图咋搞咯？还有专业课的复习时间怎么分配？难点不懂怎么办？师兄你有笔记吗？： 依旧严肃的师兄答： 这八个专题的思维导图我已经整理完并放置在百度云群里了，这里给出百度云链接，人大432专业课思维导图 密码: pwu9。当然思维导图要随时自己更新，如果自己觉得需要补充的，可以在思维导图上添加。要想使这个思维导图的效用最大化，就得把框架熟记在心中，在答题的时候一定会有帮助的，会让你的答案有结构有条理，列点回答更加轻松自如，不知道怎么把题目答得全面闪亮？只需把思维导图的一个个点用书本的内容或者你整理的笔记来填充就好啦。不过，话说在前头，理解才是关键。它只是一个框架工具，核心在于知识点。 时间怎么分配呢？：基本上每天我都会花三个小时复习专业课，有的时候白天数学的任务没完成，也会适当压缩专业课的时间，大致的时间安排是《统计学》阅读及笔记25天、《应用回归分析》阅读及笔记20天、《应用时间序列分析》阅读及笔记20天、《多元统计分析》阅读及笔记20天、《八个专题整理》25天、背诵30天。 难点不懂咋办办？：在多元统计分析或者时序分析中会出现一些自己无法理解的地方，诸如推导过程和计算证明，这些确实不是432统计学的考察重点，但是如果不搞清楚这些，对知识点就会感觉隔着一层迷雾，无法透彻地解析整个过程总会叫人不爽快，不求甚解是深层次理解知识点的大敌。但是，时间所迫，实在搞不懂这些玩意儿咋办呢，那就只能退而求其次，可以大概的知道这个推导是在干什么以及它在整个过程中的作用。也足够应对432统计学了。 学长你有笔记吗？：哈哈哈，到最后了学长要黄婆卖瓜自卖自夸了，简要说下学长的专业课笔记，笔记分为七个部分其中前六份都是亲手整理的，就是依据上述的复习过程一步步整理下来，并且经过了精心的排版，保证大家的用户体验一级棒。如果想深入了解资料的细节，可以私聊学长，随时等候你的到来。欢迎添加个人微信：zhanghua63170140 五、复试攻略5.1 经验之谈先凭借自己的记忆简要说说复试的整个流程 第一天 英语笔试（50分）： 一张卷子上有两部分考题，其一为听力，其二为翻译。 听力部分：依照往年师兄师姐的经验，把2005年至2016年所有的六级真题的听力部分拿出来，每一年的听力大概都听上两三遍，仅仅包含听力选择题，不包含听力，直接记答案，直至不听就可以直接把答案写出来。我记得去年有一个哥们并不知道听力的这个套路，最后因为复试英语挂了，而被刷下，好遗憾的，大家引以为鉴。还有一个就是，复试听力应该会和历年真题的某一年的某一张卷子的听力部分一模一样，而不会有任何顺序的变化或者很多年的题拼凑在一起，想想也是，要是学校变换题的顺序，听力录音就得自己搞了，工程量虽然不大，但多一事不如少一事嘛。 翻译部分：两道题，英译中，中译英。出卷子的老师给了我们一个难题，就是英译中的题是中译英的答案，这可如何是好，到底是按照标准答案一模一样写上去呢，还是有创造性的自己翻译呢。我选择了前者，英语蛮差的，就没有自讨没趣了。最后得分也没有特别离谱。今年应该不会出现这样的情况了吧，要是再出现，我觉得这老师肯定是喜欢考验一个人的意志力和创造性。 专业课笔试（100分）： 七八道简答题，出题风格和初试几乎一样，最后那道题，如果非统计科班出生的很难答出来，考的是数据挖掘里头分类器组合方法之一bootstrap（完全不知道是个啥），具体什么题我忘记掉了。不知道当时有没有人考完了然后回忆下来的，这几天我找找看。至于怎么复习，依今年初试的专业课风格来看，会相当注重很细的知识点，大家基本按照初试的复习感觉来吧，加一些自己觉得重要的细节，补充到思维导图上去，分成几个专题来背。那时候复试复习的时候相当迷惘，也是费尽了脑筋去找题，后来是从我的冤家学长小杰克先生那里获得了一些往年真题，才不至于瞎搞。还有，初试成绩出来了，大家可以在群里面找同样进复试的小伙伴，结伴复习，可能会更明朗一些。 第二天 英语面试： 当时是把专硕学硕安排在了一个教室里候试，进去之后，老师会安排面试的顺序，英语面试和专业课面试分开进行。有的人可能两场都安排在早上，也有的都安排在下午，也有可能早上下午各安排异常，这样就相当煎熬了。等待的过程很漫长，大家都静坐，没有人交头接耳，气氛很紧张。 英语面试的内容大概就是，进去之后，老师会让你抽一道篇文章，然后让你完整的念一遍，完了之后提问另三个关于这篇文章的问题，问题都比较简单，比如说这篇短文的主题是什么？都是显而易见的问题，所以你要在读的过程中思考些许，不要一股脑儿读了一遍就完事。这一步结束之后，老师会让你自我介绍下，这个自我介绍提前准备好，两三分钟。说下自己的简介，兴趣所在，本科学了啥，做过什么统计相关的，会什么软件，为什么考人大统计等等，简单说说。自我介绍结束还会问几个简单的问题了解下你，比如你对哪一门专业课最感兴趣？你学过哪几门专业课？注意用英语表达，我好像突然之间还真想不起来这些词，所以提前准备下好了。 专业课面试： 进去之后，同样是抽一道题，题目涉及统计各方面的内容，也基本和初试的内容差不多，但是有小伙伴抽到了国民经济和抽样的题，复试前攒下人品应该就不会抽到了（希望是如此）。回答之前要在脑海里清晰的列好点，要回答几个关键的点。老师们会根据你的回答扩展提问，也都是课本范围内的。能答上来就答，不会的话就实话实说好了，老师也不会为难你。抽到国民和抽样的题，不要慌，老师一定要你说点啥的时候，咱们虽然没有学过（时间充裕的觉初试分数处在边缘的同学可以提前看一下这俩本书，我也没看过，据说知识点很多，短时间内很难攻克），可以根据自己的理解答一些也蛮好的。 接下来是自我介绍，和英语面试一样，介绍下自己，说说自己的兴趣，做过什么小项目，为什么考人大统计，会什么统计软件，将来想研究的大致方向。自我介绍完了，老师会根据你的介绍提一些问题，所以你在自我介绍的时候尽量往自己掌握的比较好的方向引。五个老师面试，我的感觉应该是属于同一个研究方向的老师会在一起，有的运气好的，可能你个人的感兴趣的是数据挖掘，而这五个老师恰好都是来自精算教研室，这样就不至于问的太深。当然，老师们都是涉猎广泛的，我们那点三脚猫功夫很容易就露馅了哈哈。 5.2 复试资料集合 六级听力：搜一下APP就好了，这个是我当时用的，也可以用其他APP替代。 翻译：不知道什么资料好，用初试真题的翻译部分训练也行。然后再背背单词啥的，不至于到时候遗忘了。 专业课笔试面试：链接: https://pan.baidu.com/s/1dF0VQA 密码: sxub 5.3 其他学习建议所以，在复试之前还有很长一段时间，这段时间可以给自己安排一些除了复试内容之外的学习任务，比如学点R语言、学点算法啥的，我在下面列出大家可以自学的东西，基本是大数据班会学的，大家在毕业设计之余或者实习之余可以一步步的学，我也只是个初学者，只能推荐一些我觉得相对比较好的资料和教程，当然也结合了其他的学长学姐的建议和网络的推荐。复试前不用把所有的都学了，但建议把R语言看下，那样面试的时候就可以突出你学过以及会R了。人大是R语言国内的开山鼻祖，每年的R语言会议都会吸引数据分析界很多大牛来讲演。我觉得这些可以放到复试之后，那么长一段时间，找个实习啥的，有空的时候学学。 R语言 《R语言实战》：从基础开始讲，基本涵盖了R语言基础的内容，因为研究生很多作业都会用R来实现，提前学会了，到时候就可以直接上手了，不至于像我们这一届蛮多人都是重头开始学R。 薛薇老师《R语言数据挖掘算法及应用》：这本书是薛薇老师上课的教材，我个人觉得写的很好，而且老师的讲课水平很高，会把一些艰深的算法用相对易懂的方式讲授出来。里面用R语言实现算法，课堂作业也会让你用R实现，所以，提前学起来很有好处。需要的数据集在下面的链接找 PYTHON 廖雪峰的python2.7教程：python基础，比外国人写的书易懂多了 《集体智慧编程》：算法的python实现 SQL w3school的SQL教程：SQL入门很快，基本上几天就能学会，基本的语法在下面教程里有 Linux 慕课网的linux达人养成计划 机器学习算法 李航的《统计学习方法》：全是干货 六、写在最后这一路下来，感谢的人很多，人大陈思聪学长（见过学长本人，沉稳贴心哈哈，考研的时候不会的题去问他都会耐心解答），小杰克学长（这个群真的建设的太赞了，复试的资料是向学长要的，帮助很大），老胡学长（最近给我发了好几本砖头书，全英文版，据说是装逼神器），以及在中海洋读研的王淼淼童鞋（跨专业考统计真心不容易，磕磕碰碰了很多次，王淼淼鼎力相助；我后期整理了一个专业课思维导图，也是王淼淼给我的灵感），在幼儿园种花种草的冯涵小朋友，三个考研小伙伴大象、赫姐还有小姨妈（每次拿小姨妈开玩笑真的屡试不爽），还有强悍的北大444哥（他的经验贴真的是棒呆）。最后想感谢的是我的女朋友，她脸上的笑容总能够化解我的忧愁与不安，每次复习到夜深，想起在这座城市的另一边有那么一个人与我一同牵手向前，就会充满力量。 在本科期间，我们所接受的更多的是老师所教授的知识，而到了研究生期间，我们要准备开始制造新的知识，更高层次的目标便是对人类普遍的知识有所贡献，而达到如此境界的来源正是你对知识的渴求与不断地追索，你不再满足于单一的知识面，而渴望搭建更坚固的知识架构。我们不能功利的对待这场磨练你意志的战役，不能仅仅把它看成是获取更多外在利益的工具，虽然确实可以达到这样的效果，但这样的动力绝对不会持久的催促你往更高的知识领域探索。 很多人在中途放弃或是马马虎虎的应付，就是自己内心缺乏行动的信念支撑，倘若只是觉得考上研就一劳永逸了，就没有必要花费那么厚重的时间成本了，因为，考上研只是一个起点，更艰难的路，在前方。 祝愿学弟学妹可以在考研路上发现更多的风景，顺利考上人大。]]></content>
      <categories>
        <category>人大应统部落</category>
      </categories>
      <tags>
        <tag>人大应统</tag>
        <tag>考研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（16）：统计学习概论]]></title>
    <url>%2F2017%2F03%2F20%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8816%EF%BC%89%EF%BC%9A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[一、统计学习1.1 特点统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。赫尔伯特·西蒙曾对学习定义为：“如果一个系统能够执行某个过程改进它的性能，这就是学习。”按照这一观点，统计学习就是计算机系统通过数据及统计方法提高系统性能的机器学习。 1.2 对象统计学习的对象是数据，从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到对数据的分析与预测中去。数据包括存在于计算机及网络上的各种数字、文字、图像、视频、音频及它们的组合。统计学习对于数据的基本假设是同类数据具有一定###的统计规律性。 1.3 目的考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测与分析，同时也要尽可能地提高学习效率。 1.4 方法统计学习由监督学习（supervised learning）、非监督学习（unsupervised learning）、半监督学习（semi-supervised learning）、强化学习（reinforcement learning）等组成 监督学习：从给定的、有限的、用于学习的训练数据（training data）集合出发，假设数据独立同分布，并且假设要学习的模型属于某个函数的集合，称为假设空间（hypothesis space），应用某个评价准则（evaluation criterion），从假设空间中选取一个最优的模型，使它对已知训练数据及未知测试数据在给定的评价准则下有最优的预测；最优模型的选取由算法实现。 三要素：模型的假设空间（模型）、模型选择的准则（策略）、模型学习的算法（算法） 步骤： 1）得到一个有限的训练数据集合 2）确定包含所有可能的模型的假设空间，即学习模型的集合 3）确定模型选择的准则，即学习的策略 4）实现求解最优模型的算法，即学习的算法 5）通过学习方法选择最优模型 6）利用学习的最优模型对新数据进行预测或分析 二、监督学习2.1 定义监督学习的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做一个好的预测。它从训练数据（training data）集合中学习模型，对测试数据（test data）进行预测。 2.2 基本概念2.2.1 输入空间、特征空间与输出空间 输入空间与输出空间：输出与输出所有可能值的集合。通常输出空间远远小于输入空间 特征空间：所有特征向量存在的空间。特征空间的每一维对应于一个特征。模型都定义在特征空间上。 输入实例$x$的特征向量： x=\left(x^{\left(1\right)},x^{\left(2\right)},···,x^{\left(i\right)},···,x^{\left(n\right)}\right)^T其中$x^{(i)}$表示$x$的第$i$个特征，$x_i$表示多个输入向量的第$i$个，即 x_i=\left(x_i^{\left(1\right)},x_i^{\left(2\right)},···,x_i^{\left(n\right)}\right)^T 训练数据和测试数据由输入输出对（即样本）组成，通常表示为： T=\left\{\left(x_1,y_1\right),\left(x_2,y_2\right),···,\left(x_N,y_N\right)\right\} 回归问题：输入变量与输出变量均为连续变量的预测问题。 分类问题：输出变量为有限个离散变量的预测问题。 标注问题：输入变量与输出变量均为变量序列的预测问题。 2.2.2 联合概率分布统计学习假设数据存在一定的统计规律，监督学习的基本假设为$X$和$Y$具有联合概率分布的假设，我们把训练数据与测试数据看作是依联合概率分布$P(X,Y)$独立同分布产生的。 2.2.3 假设空间监督学习的目的在于找到由输入到输出的映射模型集合中最好的一个。这个集合即假设空间。模型可以是概率模型或非概率模型，由条件概率分布$P(Y|X)$或决策函数$Y=f(X)$表示。 三、统计学习三要素3.1 模型模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的条件概率分布（概率模型）或决策函数（非概率模型）。假设空间用$\mathscr{F}$表示，它是由一个参数向量决定的决策函数族： \mathscr{F}=\left\{f\ |\ Y=f_{\theta}\left(X\right),\theta\in R^n\right\}参数向量$\theta$取值于$n$维欧式空间$R^n$，称为参数空间。也可以是一个参数向量决定的条件概率分布族： \mathscr{F}=\left\{P\ |\ P_{\theta}\left(Y|X\right),\theta\in R^n\right\}3.2 策略有了模型的假设空间，接下来需要考虑按照什么样的准则学习或选择最优的模型。 3.2.1 损失函数和风险函数损失函数（loss function）度量一次预测的好坏。损失函数越小，模型就越好常用的损失函数有： 0-1损失函数（0-1 loss function）： L=\left\{\begin{matrix}{} 1& Y\ne f\left(X\right)\\ 0& Y=f\left(X\right)\\ \end{matrix}\right. 平方损失函数（quadratic loss function）: L=\left(Y-f\left(X\right)\right)^2 绝对损失函数（absolute loss function）： L=|Y-f\left(X\right)| 对数损失函数（logarithmic loss function）： L=-\log P\left(Y|X\right) 风险函数（risk function）或期望损失（expected loss）是理论上模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失。 R_{\exp}\left(f\right)=E_p\left[L\left(Y,f\left(X\right)\right)\right]=\int_{}{L\left(y,f\left(x\right)\right)P\left(x,y\right)dxdy}我们学习的目标就是选择期望风险最小的模型。由于联合分布$P(X,Y)$未知，风险函数不能直接计算。这样，一方面根据期望风险最小学习模型要用到联合分布，另一方面联合分布又是未知的，所以监督学习就沦为病态问题。但我们可以计算训练数据集的平均损失，即经验风险（empirical risk）或经验损失（empirical loss）： \textrm{R}_{emp}\left(f\right)=\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}根据大数定理，当样本容量$N$趋于无穷时，经验风险趋于期望风险。自然而然想到可以使用经验风险来估计期望风险。但现实中训练样本数目很小，这种估计往往不理想，需要矫正，以此引出经验风险最小化和结构风险最小化。 3.2.2 经验风险最小化和结构风险最小化经验风险最小化（empirical risk minimization）认为经验风险最小的模型是最优的模型，即求解最优化问题： \underset{f\in\mathscr{F}}{\min}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}当样本容量足够大的时候，经验风险最小化学习效果良好。比如极大似然估计，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。 但是当样本容量很小时，经验风险最小化学习会产生过拟合（over-fitting）的现象。这就引出了结构风险最小化，它等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term），它的定义为： R_{srm}\left(f\right)=\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)其中$J(f)$为模型的复杂度，模型$f$越复杂，复杂度$J(f)$就越大；反之，模型越简单，复杂度$J(f)$就越小，即复杂度表示了对复杂模型的惩罚。$\lambda≥0$是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险和模型复杂度同时小。结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。比如贝叶斯估计中的最大后验概率估计就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。结构风险最小化的策略认为结构风险最小的模型是最优的模型，求解最优模型即求解最优化问题： \min_{f\in\mathscr{F}}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)这样，监督学习问题变成了经验风险或结构风险函数的最优化问题。 3.3 算法学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优化。如何用数值计算求解，如何保证找到全局最优化，并使求解过程高效，是一个重要的问题。 四、模型评估与模型选择4.1 训练误差与测试误差训练误差（training error）是模型关于训练数据集的平均损失： R_{emp}\left(\hat{f}\right)=\frac{1}{N_1}\sum_{i=1}^{N_1}{L\left(y_i,\hat{f}\left(x_i\right)\right)}测试误差(test error)是模型关于测试数据集的平均损失： R_{emp}\left(\hat{f}\right)=\frac{1}{N_2}\sum_{i=1}^{N_2}{L\left(y_i,\hat{f}\left(x_i\right)\right)}测试误差反映了学习方法对未知的测试数据集的预测能力，即泛化能力。 4.2 过拟合与模型选择我们希望选择或学习一个合适的模型。若在空间中存在“真模型”，那我们所选择的模型要与真模型的参数个数相同，所选择的模型的参数向量与真模型的参数向量相近。 过拟合指的是我们以为追求提高模型对训练数据的预测能力，所选模型的复杂度往往会比真模型更高。即学习时选择的模型所包含的参数过多，以致于出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。 模型选择旨在避免过拟合并提高模型的预测能力，模型选择时，不仅要考虑对已知数据的预测能力，而且还要考虑对未知数据的预测能力。下图描述了训练误差和测试误差与模型的复杂度之间的关系： 当模型复杂度增大时，训练误差会逐渐减小并趋于0；而测试误差会先减小，达到最小值后又增大。当选择的模型复杂度过大时，过拟合现象就会发生。所以要选择复杂度适当的模型，已达到测试误差最小的目的。以此引出正则化与交叉验证。 五、正则化与交叉验证5.1 正则化5.1.1 定义模型选择的典型方法是正则化（regularzation）。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如，正则化项可以是模型参数向量的范数。它的一般形式如下： \min_{f\in\mathscr{F}}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)第一项是经验风险，第二项是正则化项，$\lambda≥0$为调整两者之间关系的系数。 5.1.2 不同形式正则化项可以取不同的形式。例如，回归问题中，损失函数是平方误差，正则化项可以是参数向量的$L_2$范数: L\left(w\right)=\frac{1}{N}\sum_{i=1}^N{\left(f\left(x_i;w\right)-y_i\right)^2}+\frac{\lambda}{2}||w||^2也可以是参数向量的$L_1$范数： L\left(w\right)=\frac{1}{N}\sum_{i=1}^N{\left(f\left(x_i;w\right)-y_i\right)^2}+\lambda ||w||_1第一项的经验风险较小的模型可能较复杂（有多个非零参数），这时第二项的模型复杂度会较大。正则化的作用是选择经验风险与模型复杂度同时较小的模型。 5.1.3 奥卡姆剃刀正则化符合奥卡姆剃刀原理，应用于模型选择时变为：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型。从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。可以假设复杂的模型有很小的先验概率，简单的模型有较大的先验概率。 5.2 交叉验证5.2.1 定义如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别是训练集（training set）用来训练模型、验证集（validation set）用于模型的选择、测试集（test set）用于最终对学习方法的评估，最终选择对验证集有最小预测误差的模型。 但是实际应用中数据不充足，所以我们采用交叉验证，它的基本思想是重复的使用数据，把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试和模型选择。 5.2.2 方法 简单交叉验证：首先随机地将已给数据分为两个部分，一部分作为训练集（70%），另一部分作为测试集（30%）；然后用训练集在各种条件下（如不同的参数个数）训练模型，从而得到不同的模型；在测试机上评价各个模型的测试误差，选出测试误差最小的模型。 S折交叉验证（S-fold cross validation）：应用最广泛。首先随即将已给数据切分为S个互不相交的大小相同的子集；然后利用S-1个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的S种选择重复进行；最后选出S次评测中平均测试误差最小的模型。 留一交叉验证leave-one-out cross validation）：S折交叉验证的特殊情形是S=N（N为给定数据集的容量），往往在数据缺乏的情况下使用 六、泛化能力6.1 泛化误差泛化能力是指由该方法学习到的模型对未知数据的预测能力。现实中常常通过测试误差来评价学习方法的泛化能力，但因为测试数据及有限，评价结果不一定可靠。 理论上，通过泛化误差来反映学习方法的泛化能力，泛化误差即用学习到的模型对未知数据预测的误差： R_{\exp}\left(f\right)=E_p\left[L\left(Y,f\left(X\right)\right)\right]=\int_{}{L\left(y,f\left(x\right)\right)P\left(x,y\right)dxdy}泛化误差越小，模型效果就好。泛化误差就是所学习到的模型的期望风险。 七、生成模型与判别模型7.1 判别模型判别模型由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型。它关心的是对给定的输入$X$，应该预测什么样的输出$Y$。典型的判别模型包括：K近邻法、感知机、决策树、逻辑斯谛回归、最大熵模型、支持向量机、提升方法、条件随机场。 判别方法的特点： 直接学习的是条件概率$P(Y|X)$或决策函数$f(X)$，直接面对预测，往往学习的准确率很高； 由于直接学习$P(Y|X)$或$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。 7.2 生成模型生成模型由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型： P\left(Y|X\right)=\frac{P\left(X,Y\right)}{P\left(X\right)}因为模型表示了给定输入$X$产生输出$Y$的生成关系，所以被称为生成模型。典型的生成模型有：朴素贝叶斯、隐马尔科夫模型 生成方法的特点： 生成方法可以还原出联合概率分布$P(X,Y)$，而判别方法不能； 生成方法的学习收敛速度快，即当样本容量增加时，学到的模型可以很快收敛于真实模型； 当存在隐变量时，仍可以用生成方法学习，此时判别方法不能用。 八、分类问题8.1 定义在监督学习中，当输出变量$Y$取有限个离散值时，预测问题便成为分类问题。它从数据中学习一个分类模型或分类决策函数，即学习一个分类器，然后对新的输入进行输出的预测，即进行分类。分为多类分类和二类分类问题。 8.2 学习过程如图所示，分类问题包括学习和分类两个过程。在学习过程中，根据已知的训练数据集利用有效的学习方法学习一个分类器；在分类过程中，利用学习的分类器对新的输入实例进行分类。 8.3 分类准确率8.3.1 定义评价分类器性能的指标一般是分类准确率（accuracy）,即对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。也即损失函数是0-1损失时测试数据集上的准确率。 8.3.2 常用指标通常将关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作：TP（正类预测为正类数）、FN（正类预测为负类书）、FP（负类预测为正类数）、TN（负类预测为负类数） 精确率： P=\frac{TP}{TP+FP} 召回率： R=\frac{TP}{TP+FN} $F_1$值，是精确率和召回率的调和均值，即 \frac{2}{F_1}=\frac{1}{P}+\frac{1}{R} F_1=\frac{2TP}{2TP+FP+FN}精确率与召回率都很高时，$F_1$值也会很高。 8.3.3 分类方法与应用 常见分类统计方法：K近邻、感知机、朴素贝叶斯、决策树、决策列表、逻辑斯谛回归、支持向量机、提升、贝叶斯网络、神经网络Winnow 应用：银行业务构建客户分类模型，对客户按照贷款风险大小分类；网络非法入侵检测；人脸是否出现的检测；网页分类；文本分类等。 九、标注问题9.1 定义标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。它的目的在于学习一个模型，使它能够对观测序列给出标记序列作为预测。标注问题分为学习和标注两个过程： 9.2 应用标注常用的统计学习方法有：隐马尔科夫模型、条件随机场 它在信息抽取、自然语言处理领域被广泛应用。 自然语言处理的词性标注：给定一个由单词组成的句子，对这个句子中的每一个单词进行词性标注，即对一个单词序列预测其对应的词性标记序列。 十、回归问题10.1 定义回归模型表示输入变量和输出变量之间映射的函数，等价于函数拟合：选择一条函数曲线使其很好地拟合已知数据且很好地预测未知数据。可分为一元回归和多元回归，线性回归和非线性回归。它最常用的损失函数为平方损失函数，可以用最小二乘法求解。回归问题分为学习和标注两个过程： 10.2 应用股价预测：将影响股价的信息视作自变量，将股价视为因变量，将过去的数据作为训练数据，学习一个回归模型，并对未来的股价进行预测。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学习概论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（15）：EM算法]]></title>
    <url>%2F2017%2F03%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8815%EF%BC%89%EF%BC%9AEM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[期望最大值（Expectation Maximization，简称EM算法）是在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐藏变量。其主要思想就是通过迭代来建立完整数据的对数似然函数的期望界限，然后最大化不完整数据的对数似然函数。本文将尽可能详尽地描述EM算法的原理。并结合高斯混合模型介绍EM算法是如何求解的。 一、定义EM算法是一种迭代算法，用于含有隐变量（hidden variable）的改了吧模型参数的极大似然估计或极大后验概率估计。EM算法的每次迭代由两步组成：E步-求期望（expectation）；M步-求极大（maximization）。故称为期望极大算法（expectation maximization），简称EM算法。 二、Jensen不等式设$f$是定义域为实数的函数，如果对于所有的实数$x$，$f^{‘’}(x)≥0$，那么$f$是凸函数。当$x$是向量时，如果其$hessian$矩阵$H$是半正定的即$H≥0$，那么$f$是凸函数。如果$f^{‘’}(x)&gt;0$或$H&gt;0$，那么称$f$是严格凸函数。 $Jensen$不等式表述如下： 如果$f$是凸函数，$x$是随机变量，那么：$E[f(x)]≥f(E[x])$。特别地，如果$f$是严格凸函数，$E[f(x)]≥f(E[x])$，那么当且仅当$P(x=E[x])=1$(也就是说$x$是常量)，$E[f(x)]=f(E[x])$; 如果$f$是凹函数，$x$是随机变量，则$E[f(x)]≥f(E[x])$。当$f$是（严格）凹函数当且仅当$-f$是（严格）凸函数。 通过下面这张图，我们可以加深理解： 上图中，函数$f$是凸函数，$X$是随机变量，有0.5的概率为$a$，有0.5的概率是b（就像抛硬币一样）。$X$的期望值就是a和b的中值了，图中可以看到$E[f(x)]≥f(E[x])$成立。 三、EM思想3.1 极大似然估计EM算法推导过程中，会使用到极大似然估计参数。 极大似然估计是一种概率论在统计学的应用。已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察结果，利用结果推出参数的大概值。极大似然估计建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。 这里再给出求极大似然估计值的一般步骤： 1）写出似然函数； 2）对似然函数取对数，并整理； 3）求导数，令导数为0，得到似然方程； 4）解似然方程，得到的参数即为所求； 关于极大似然估计的实例，可以参考wikipedia最大似然估计条目 3.2 EM算法思想下面介绍EM算法的思想： 给定的训练样本是$x^{(1)}，x^{(2)}，···，x^{(m)}$，样例间相互独立，但每个样本对应的类别$z^{(i)}$是未知的，也即隐含变量。我们想找到每个样例隐含的类别$z$，能使得$P(x,z)$最大。$P(x,z)$的最大似然估计如下： l\left(\theta\right)=\sum_{i=1}^m{\log p\left(x;\theta\right)}=\sum_{i=1}^m{\log\sum_z{}p\left(x,z;\theta\right)}第一步是对极大似然函数取对数，第二步是对每个样本实例的每个可能的类别$z$求联合分布概率之和。但是直接求$\theta$一般比较困难，因为有隐藏变量$z$存在，如果$z$是一个已知的数，那么使用极大似然估计来估算会很容易。在这种$z$不确定的情形下，EM算法就派上用场了。 EM算法是一种解决存在隐变量优化问题的有效方法。对于上述情况，由于存在隐变量，不能直接最大化$l(\theta)$，我们可以不断地建立$l$的下界（E步），然后优化下界（M步），依次迭代，直至算法收敛到局部最优。这就是EM算法的核心思想，简单的归纳一下： EM算法通过引入隐变量，使用MLE进行迭代求解参数。通常引入隐含变量后会有两个参数，EM算法首先会固定其中的第一个参数，然后使用MLE计算第二个变量值；接着通过固定第二个变量，再使用MLE估计第一个变量值，依次迭代，直至收敛到局部最优解。 四、EM推导下面来推导EM算法： 对于每一个样例$i$，让$Q_i$表示该样例隐含变量$z$的某种分布，$Q_i$满足的条件是 \sum_z{Q_i\left(z\right)=1\ \\ Q_i\left(z\right)\geqslant 0}（如果$z$是连续的，那么$Q_i$是概率密度函数，需要将求和符号换做积分符号）。比如要将班上学生聚类，假设隐藏变量$z$是身高，那么就是连续的高斯分布。如果是按照隐藏变量是男女，那么就是伯努利分布。 可以由前面阐述的内容得到下面的公式： \sum_i{\log p\left(x^{\left(i\right)};\theta\right)=\sum_i{\log\sum_{z^{\left(i\right)}}{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}}} ········（1） =\sum_i{\log\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}} ········（2） \geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\left(\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}\right)}}······（3）上面三个式子中，式（1）是根据联合概率密度下某个变量的边缘密度求解的（这里把$z$当做是随机变量）。对每一个样本$i$的所有可能类别求等式右边的联合概率密度函数和，也就是得到等式左边为随机变量$x$的边缘概率密度。由于对式（1）直接求导非常困难，我们可以做一个简单的变化，将其分子分母都乘以一个相等的函数$Q_i(Z^{(i)})$，得到式（2）。那么如何从式（2）推导出式（3）呢，这就需要用到之前提到的Jensen不等式。 以下为具体的分析过程： 首先，把（1）式中的$log$函数看成是一个整体，即令$f(x)=log(x)$，因为$(log(x))^”=-1/x^{2}&lt;0$，根据定理可知其为凹函数。 再根据凹函数的Jensen不等式：$f(E[X])&gt;=E[f(x)]$。 到这里，我们可以观察到，在式（2）中，当把$log(x)$看成$f(x)$时，后边的 \sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}其实就是可以类比为离散型随机变量的期望公式。具体的求解可以参照下图中离散型随机变量的期望公式。 我们可以把$Q_i^{(z^{(i)})}$看成是相应的概率$p_i$，把 \frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}看作是$z^{(i)}$的函数$g(z)$，根据期望公式$E\left[g\left(x\right)\right]=\sum_{i=1}^{\infty}{g\left(x_i\right)·p_i}$可以得到： E\left(\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z\right)}\right)=\sum_{z^{\left(i\right)}}{Q_i\left(z\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z\right)}}把上述根据Jensen不等式整合到一起得到： f\left[E\left(g\left(X\right)\right)\right]=\log\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}} \geqslant E\left[f\left(g\left(X\right)\right)\right]=\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}这样我们就得到了式（3）。 现在我们把式（2）和式（3）的不等式次而成：似然函数$L(\theta)≥J(z,Q)$的形式，其中$z$为隐变量，那么我们可以通过不断地最大化$J$的下界，来使得$L(\theta)$不断提高，最终达到它的最大值。借助下图来解释下这个过程： 首先我们固定$\theta$，调整$Q(z)$使下界（绿色曲线）$J(z,Q)$沿着绿色虚线上升至与$L(\theta)$在此点$\theta$处相等（绿色曲线至蓝色曲线），然后固定$Q(z)$，调整$\theta$使下界$J(z,Q)$达到最大值($\theta {_t}$至$\theta _{t+1}$)，然后再固定$\theta$，调整$Q(z)$…….直到收敛到似然函数$L(\theta)$的最大值处的$\theta^*$ 这里有两个问题： 什么时候下界$J(z,Q)$与$L(\theta)$在此点$\theta$处相等？ 为什么一定会收敛？ 首先来解释下第一个问题。在Jensen不等式中说到，当自变量$X=E(X)$时，即为常数的时候，等式成立。而在这里，为： \frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}=c对该式做个变换，将分母移到等号右边，并对所有的$z$求和，得到第一个等号；又因为前面提到的$\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)=1}$得到第二个等号。 \sum_{z^{\left(i\right)}}{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}=\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)c}=c根据上面两个式子可以得到 Q_i\left(z^{\left(i\right)}\right)=\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{\sum_z{p\left(x^{\left(i\right)},z;\theta\right)}} \\\\\ =\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{p\left(x^{\left(i\right)};\theta\right)} \\\\ =p\left(z^{\left(i\right)}|x^{\left(i\right)};\theta\right)到这里，我们推出了在固定参数$\theta$后，使下界拉升的$Q(z)$的计算公式就是后验概率（条件概率），解决了$Q(z)$如何选择的问题。此步就是EM算法的E步，目的是建立$L(\theta)$的下界。接下来的M步，目的是在给定$Q(z)$后，调整$\theta$，从而极大化$L(\theta)$的下界$J$（在固定$Q(z)$后，下界还可以调整的更大）。那么一般的EM算法的步骤如下： 第一步：初始化分布参数$\theta$； 第二步：重复E步和M步直到收敛： E步：根据参数的初始值或上一次迭代的模型参数来计算出的因变量的后验概率（条件概率），其实就是隐变量的期望值，来作为隐变量的当前估计值： \\\\ Q_i\left(z^{\left(i\right)}\right)=p\left(z^{\left(i\right)}|x^{\left(i\right)};\theta\right) M步：最大化似然函数从而获得新的参数值： \theta :=arg\underset{\theta}{\max}\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}} 通过不断地迭代，然后就可以得到使似然函数$L(\theta)$最大化的参数$\theta$了。 接下来我们看第二个问题。上面多次说到直到收敛，那为什么一定会收敛呢？证明如下： 假定$\theta^{(t)}$和$\theta^{(t+1)}$是EM第t次和t+1次迭代后的结果。如果我们证明了$l(\theta^{(t)})≤l(\theta^{(t+1)})$，也就是说极大似然估计单调增加，那么最终我们就会得到极大似然估计的最大值。 下面来证明，选定$\theta^{(t)}$后，我们得到E步： \\\\ Q_i^{(t)}\left(z^{\left(i\right)}\right)=p\left(z^{\left(i\right)}|x^{\left(i\right)};\theta\right)这一步保证了在给定$\theta^(t)$时，Jensen不等式中的等式成立，也就是 l\left(\theta^{\left(t\right)}\right)=\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}然后进行M步，固定$Q_i^{(t)}(z^{(i)})$，并将$\theta^{(t)}$视作变量，对上面的$l(\theta^{(t)})$求导后，得到$\theta^{(t+1)}$,这样经过一些推导会有以下式子成立： l\left(\theta^{\left(t+1\right)}\right)\geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t+1\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}······\textrm{（4）} \geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}······\textrm{（5）} =l\left(\theta^{\left(t\right)}\right)······\textrm{（6）}解释第（4）步，得到$\theta^{(t+1)}$时，只是最大化$l(\theta^{(t)})$，也就是$l(\theta^{(t+1)})$的下界，而没有使等式成立，要想使等式成立只有在固定$\theta$，并按E步得到$Q_i$时才能成立。况且根据我们前面得到的下式，对于所有的$Q_i$和$\theta$都成立 l\left(\theta^{\left(t\right)}\right)\geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}第（5）式利用的M步的定义，M步就是将$\theta^{(t)}$调整到$\theta^{(t+1)}$，使得下界最大化。这样（5）、（6）就都证明成立了。 再结合之前那个图解释一下这几步推导： 首先（4）对所有的参数都满足，而其等式成立条件只是在固定$\theta$，并调整好$Q$时成立，而第（4）步只是固定$Q$，调整$\theta$，不能保证等式一定成立。对应到图上就是蓝色曲线的峰值与$l(\theta^{(t+1)})$的关系，要使它们相等还必须要固定$\theta$，调整好$Q$；（4）到（5）就是M步的定义，也就是固定$Q$，调整$\theta^{(t)}$至$\theta^{(t+1)}$，对应到图上即为蓝色曲线与红色曲线交点处至蓝色曲线峰值。（5）到（6）是前面E步所保证等式成立条件。也就是说E步会将下界拉到与$l(\theta)$一个特定值（这里为$\theta^{(t)}$）一样的高度，而此时发现下界仍然可以上升，因此经过$M$步后，下界又被拉升，但达不到与$l(\theta)$另外一个特定值（$\theta^{(t+1)}$）一样的高度，之后E步又将下界拉到了与这个特定值一样的高度，循环往复，直到达到最大值。 这样就证明了$l(\theta)$会单调增加。如果要判断收敛情况，可以这样来做：一种收敛方法是$l(\theta)$不再变化，还有一种就是变化幅度很小，即根据$l(\theta^{(t+1)})=l(\theta^{(t)})$的值来决定。 从前面的推导中我们知道$l(\theta)≥J(Q,\theta)$，EM也可以看做是$J$的坐标上升法，如下图所示： 图中的直线式迭代优化的路径，可以看到每一步都会向最优值前进一步，而且前进路线是平行于坐标轴的，因为每一步只优化一个变量。这犹如在x-y坐标系中找一个曲线的极值，然而曲线函数不能直接求导，因此什么梯度下降方法就不适用了。但固定一个变量后，另外一个可以通过求导得到，因此可以使用坐标上升法，一次固定一个变量，对另外的求极值，最后逐步逼近极值。对应到EM上，E步：固定θ，优化Q；M步：固定Q，优化θ；交替将极值推向最大。 五、EM的应用：混合高斯模型待补充 六、EM的应用：EM聚类以下的聚类图来自维基百科，可以生动的看出 待补充 七、参考资料The EM Algorithm混合高斯模型和EM算法cs229-notes8]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>EM算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（14）：关联分析]]></title>
    <url>%2F2017%2F03%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8814%EF%BC%89%EF%BC%9A%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[一、关联分析1.1 引言在数据挖掘与机器学习中，关联规则（Association Rules）是一种较为常用的无监督学习算法，与分类、聚类等算法不同的是，这一类算法的主要目的在于发掘数据内在结构特征之间的关联性。 简单一点来说，就是在大规模的数据集中寻找一些有意义有价值的关系。有了这些关系，一方面，可以帮助我们拓宽对数据及其特征的理解；另一方面，则可以实现推荐系统的构建与应用（例如购物篮分析等）。 在对关联规则有了基本的认识后，我们对其进行进一步的细分，以日常生活中的关联性举例，在逛超市的顾客中，购买面包的人很大程度上会购买牛奶，这一类的关联性被称为简单关联规则；再例如，购买汽车遮阳板的很多顾客会在近期内购买零度玻璃水，这样的事例不仅反映了事物间的关联关系，而且还具有时间上的先后顺序，因此这一类的关联性被称为序列关联规则。 广义上的关联规则包含了简单关联和序列关联，接下来我们分别对这两块知识进行深入学习。 1.2 简单关联规则初探首先我们需要明确关联分析中的一些基本概念： 事务：指关联分析中的分析对象，我们可以把它理解成为一种宽泛行为（例如顾客的一次超市购买行为，电脑的使用者的一次网页浏览行为等都可以称之为事务），由事务标识（TID）与项目集合组成。 项集：即事务中的一组项目的集合，单个的项目可以是一种商品、一个网页链接等。假设$X$为项集，$I$为项目全体且$I=\{i_1,i_2,···,i_n\}$，那么项集$X\subseteq I$。进一步的，如果$X$中包含$p$个项目，则称该项集为$p-$项集。以上图为例，这里包含了4个事务，$I$包含了5个项目。对于第一个事务而言，由于$X$包含了三个项目，所以该$X$是一个$3-$项集。明确了基本概念后，接下来学习关联规则的一般表现形式 X\rightarrow Y\left(S=s\%,C=c\%\right)其中： $X$和$Y$分别为规则的前项和后项，前项为项目或项集，后项表示某种结论或事实。 $S=s\%$表示规则支持度为$s\%$，$C=c\%$表示规则置信度为$c\%$ 到这里大家可能会疑惑，直接得到关联规则不就可以了吗？为什么要在结论中加入支持度和置信度呢？这就涉及到关联分析中非常重要的一块内容——有效性的判别 1.3 简单关联规则的有效性实际上，在数据中使用关联分析进行探索时，我们可以找出很多关联规则，但并非所有的关联规则都是有效的，有的可能令人信服的程度并不高，也有的可能适用范围很有限，带有这些特征的所谓“关联规则”，我们则称之为不具有“有效性”。判断一条关联规则是否有效，需要用到以下两大测度指标，即规则置信度与规则支持度。 1.规则置信度（Confidence）置信度是对简单关联规则准确度的测量，定义为包含项目$A$的事务中同时也包含项目$B$的概率，数学表述为： Confidence\left(A\rightarrow B\right)=P\left(B|A\right)=\frac{P\left(AB\right)}{P\left(A\right)}置信度的本质就是我们所学过的条件概率，置信度越高，则说明$A$出现则$B$出现的可能性也就越高。假设在电脑$\rightarrow$杀毒软件的关联规则中，置信度$C=60\%$，表示购买电脑的顾客中有$60\%$的顾客也购买了杀毒软件。 2.规则支持度（Support） 支持度测量了简单关联规则应用的普适性，定义为项目$A$与项目$B$同时出现的概率，数学表述为： Support\left(A\rightarrow B\right)=P\left(B\cap A\right)=P\left(AB\right)假设某天共有100个顾客到商场购买物品，其中有10个顾客同时购买了电脑和杀毒软件，那么上述关联规则的支持度就为10%，同样，支持度越高，表明某一关联规则的适用性就越大。 一个有效的简单关联规则，势必同时具有较高的置信度与支持度。因为，如果支持度较高而置信度较低，则证明规则的可信度差；而相反，如果支持度较低而置信度较高，则说明规则的应用范围较小。 举例来说，假设在1000个顾客购买行为的事务中，只有一个顾客购买了烧烤炉，同时也只有他购买了碳，虽然规则“烧烤炉$\rightarrow$碳”的置信度很高，为100%，但支持度仅有0.1%，说明这条规则缺乏普遍性，应用价值不高。 所以一个有效的关联规则，必须具有较高的置信度与支持度，那么在实际应用中，我们就需要给定最小的置信度$C_{min}$与支持度$S_{min}$，只要同时大于$C_{min}$和$S_{min}$的规则，我们才可以将其定义为是“有效”的。 1.4 简单关联规则的实用性在对关联规则的有效性有一个基本的掌握后，我们在此基础上进行进一步的探讨——关联规则的实用性。 关联规则的实用性主要体现在以下两个方面： 1）是否具有实际意义。例如“怀孕$\rightarrow $女性”的关联规则就没有实用价值。 2）是否具有指导意义，即帮助我们在现有的基础上做出有价值的优化。 对第二点进一步展开说明，假设“牛奶$\rightarrow $男性顾客（$S=40\%，C=40\%$）”在$C_{min}$和$S_{min}$均为20%时是一条有效规则时，如果进一步计算发现顾客中男性的比例也为40%，也就是说购买牛奶的男性顾客等于所有顾客中的男性比例，那么这条规则就是一条前后项无关的随机性关联，因此它就没有有意义的指导信息，不具有实用性。 如何衡量关联规则具有实用性呢？这里我们就需要借助规则的提升度了。 规则提升度（Lift）：置信度与后项支持度之比，数学表述为： Lift\left(A\rightarrow B\right)=\frac{Confidence\left(A\rightarrow B\right)}{P\left(B\right)}=\frac{P\left(AB\right)}{P\left(A\right)P\left(B\right)}提升度反映了项目$A$的出现对项目$B$出现的影响程度。从统计角度来看，如果$A$的出现对项$B$的出现没有影响，即$A$与$B$相互独立的化，$P(AB)=P(A)P(B)$，此时规则提升度为1。所以，具有实用性的关联规则应该是提升度大于1的规则，即$A$的出现对$B$的出现有促进作用。同样，提升度越大，证明规则实用性越强。 这样我们就阐述清楚了关联规则的一些基本假定与判别标准，当数据集较小时，关联规则的使用较为简单，但是如果数据集很大的话，如何在这海量的数据中快速找出关联规则呢？这就引出了进一步要叙述的内容——简单关联规则下的$Apriori$算法。 二、Apriori算法2.1 简介在数据量庞大的前提下，由于简单搜索可能产生大量无效的关联规则，并导致计算效率底下。出于克服这些弊端的目的，Apriori算法应运而生，该算法自1996年提出后，经过不断地完善和发展，已成为简单关联分析中的核心算法。 2.2 频繁项集的相关定义频繁项集很好理解，他是指大于等于最小支持度$S_{min}$的项集。其中，若频繁项集中包含一个项目，则成为频繁$1-$项集，记为$L_1$；若包含$k$个项目，则成为频繁$k-$项集，记为$L_k$。频繁项集具有以下两个性质，这俩条性质将应用于我们后面频繁项集及其关联规则的寻找中： 1）频繁项集的子集必为频繁项集（假设项集$\{A,C\}$是频繁项集，那么$\{A\}$和$\{C\}$也为频繁项集） 2）非频繁集的超集一定也是非频繁的（假设项集$\{D\}$不是频繁项集，那么$\{A,D\}$和$\{C,D\}$也不是频繁项集） 进一步，当某一个$L_k$的所有超集都是频繁项集时，我们就可以称此$L_k$为最大频繁$k-$项集，确定它的目的就在于使之后的到的关联规则具有较高的普适性。 2.3 寻找频繁项集对频繁项集的寻找，是Apriori算法提高寻找规则效率的关键。它采用迭代的方式逐层寻找下层的超集，并在超集中发现频繁项集。经过层层迭代，直到最顶层得到最大频繁项集为止。在每一轮的迭代中都包含以下两个步骤： 1）产生候选集$C_k$，它是有可能成为频繁项集的项目集合； 2）修剪候选集$C_k$，即基于$C_k$计算相应的支持度，并依据最小支持度$S_{min}$对候选集$C_k$进行删减，得到新的候选集$C_{k+1}$，如此循环迭代，直到无法产生候选项集为止，这样最后一轮所得到的频繁项集就是Apriori所要求的最大频繁项集。 接下来我们以一个下例子帮助理解：假设我们指定的最小支持阀度为0.5（计数≥2） 在第一轮迭代过程中，由于$D$的支持度小于0.5（只有0.25），所以没有进入频繁项集，其余均进入频繁项集，定义为$L_1$。 在第二轮迭代中，候选集$C_2$是$L_1$中所有项目的组合，计算各项目支持度，淘汰$\{A,B\}$和$\{A,E\}$，其余进入频繁项集，定义为$L_2$。 在第三轮迭代中，只有$\{B,C,E\}$进入候选集$C_3$，而其余都没有进入，之所以会这样，是因为这里使用到了前面所提到的频繁项集的第二个性质：非频繁项集的超集一定也是非频繁的。所以，包含$\{A,B\}$与$\{A,E\}$的超集是不可能成为频繁项集的。 由于$L_3$不能继续构成候选集$C_4$，所以迭代结束，得到的最大频繁项集为$L_3\{B,C,E\}$。 2.4 在最大频繁项集的基础上产生简单关联规则得到最大频繁项集并不是最终的目的。之前在判断关联规则的有效性时，我们学习了置信度与支持度两个指标。其中，支持度已经在寻找最大频繁项集的过程中发挥了作用，那么，在接下来关联规则的产生上，就轮到置信度大显身手了。 首先，每个频繁项集都需要计算所有非空子集$L^*$的置信度，公式为 C_{L'\rightarrow\left\{L-L'\right\}}=\frac{P\left(L\right)}{P\left(L'\right)}如果所求得的$C_{L’\rightarrow\left\{L-L’\right\}}$大于我们自行指定的$C_{min}$，则生成相应的关联规则${L’\rightarrow\left\{L-L’\right\}}$ 在上面的例子中，$L_3{\{B,C,E\}}$的非空子集就包括$\{B\}$，$\{C\}$，$\{E\}$，$\{B,C\}$，$\{B,E\}$，$\{C,E\}$，举例来说，根据公式可计算得到 C_{C\rightarrow\left\{B,E\right\}}=\frac{P\left(B,C,E\right)}{P\left(C\right)}=\frac{2}{3}=66.7\%其余置信度依次为：$C_{B\rightarrow\left\{C,E\right\}}=66.7\%$，$C_{E\rightarrow\left\{B,C\right\}}=66.7\%$，$C_{\left\{B,C\right\}\rightarrow E}=100\%$，$C_{\left\{B,E\right\}\rightarrow C}=66.7\%$，$C_{\left\{C,E\right\}\rightarrow B}=100\%$ 如果我么设定$C_{min}=80\%$的话，只有$C_{\left\{C,E\right\}\rightarrow B}$和$C_{\left\{B,C\right\}\rightarrow E}$可以入围，如果设定为$50\%$，那么六条规则就都是有效规则了。置信度的选取和支持度一样，只有结合具体应用情况，算法才能给到我们切合实际的结论。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>关联分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（13）：推荐系统（3）—矩阵分解技术]]></title>
    <url>%2F2017%2F03%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8813%EF%BC%89%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%883%EF%BC%89%E2%80%94%E2%80%94%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[随着Netflix Prize推荐比赛的成功举办，近年来隐语义模型（Latent Factor MOdel，LFM）受到越来越多的关注。隐语义模型最早在文本挖掘领域被提出，用于寻找文本的隐含语义，相关的模型常见的有潜在语义分析（Latent Semantic Analysis,LSA）、LDA（Latent Dirichlet Allocation）的主题模型（Topic Model）、矩阵分解（Matrix Factorization）等等。 其中矩阵分解技术是实现隐语义模型使用最广泛的一种方法，其思想也正来源于此，注明的推荐领域大神Yehuda Koren更是凭借矩阵分解模型勇夺Netflix Prize推荐比赛冠军，以矩阵分解为基础，Yehuda Koren在数据挖掘和机器学习相关的国际顶级会议（SIGIR,SIGKDD,RecSys等）发表了很多文章，将矩阵分解模型的优势发挥得淋漓尽致。实验结果表明，在个性化推荐中使用矩阵分解模型要明显优于传统的基于邻域的协同过滤(又称基于领域的协同过滤)方法，如UserCF、ItemCF等，这也使得矩阵分解成为了目前个性化推荐研究领域中的主流模型。 需要说明的是，协同过滤方法分为两大类，一类为上述基于领域的方法，第二类为基于模型的方法，即隐语义模型，矩阵分解模型是隐语义模型最为成功的一种实现，不作特别说明的情况下，本文将隐语义模型和矩阵分解看做同一概念，User-Item矩阵和User-Item评分矩阵为同一概念。 一、传统的SVD算法说到矩阵分解，我们首先想到的就是奇异值分解SVD。我们可以把User-Item评分矩阵M进行SVD分解，并通过选择部分较大的一些奇异值来同时进行降维，也就是说矩阵M此时分解为： M_{m\times n}=U_{m\times k}\Sigma_{k\times k}V^T_{k\times n}其中k是矩阵MM中较大的部分奇异值的个数，一般会远远的小于用户数和物品数。 如果我们要预测第i个用户对第j个物品的评分$m_{ij}$,则只需要计算$u^T_iΣv_j$即可。通过这种方法，我们可以将评分表里面所有没有评分的位置得到一个预测评分，通过找到最高的若干个评分对应的物品推荐给用户。 可以看出这种方法简单直接，似乎很有吸引力。但是有一个很大的问题我们忽略了，就是SVD分解要求矩阵是稠密的，也就是说矩阵的所有位置不能有空白。有空白时我们的MM是没法直接去SVD分解的。大家会说，如果这个矩阵是稠密的，那不就是说我们都已经找到所有用户物品的评分了嘛，那还要SVD干嘛! 的确，这是一个问题，传统SVD采用的方法是对评分矩阵中的缺失值进行简单的补全，比如用全局平均值或者用用户物品平均值补全，得到补全后的矩阵。接着可以用SVD分解并降维。但填充本身会造成很多问题，其一，填充大大增加了数据量，增加了算法复杂度。其二，简单粗暴的数据填充很容易造成数据失真。 虽然有了上面的补全策略，我们的传统SVD在推荐算法上还是较难使用。因为我们的用户数和物品一般都是超级大，随便就成千上万了。这么大一个矩阵做SVD分解是非常耗时的。那么有没有简化版的矩阵分解可以用呢？我们下面来看看实际可以用于推荐系统的矩阵分解。 二、Funk-SVD算法2.1 基本思想Funk-SVD的核心思想认为用户的兴趣只受少数几个因素的影响，因此将稀疏且高维的User-Item评分矩阵分解为两个低维矩阵，即通过User、Item评分信息来学习到的用户特征矩阵P和物品特征矩阵Q，通过重构的低维矩阵预测用户对产品的评分。由于用户和物品的特征向量维度比较低，因而可以通过梯度下降(Gradient Descend)的方法高效地求解，分解示意图如下所示。 2.2 Funk-SVDSimon Funk在博客上公开发表了一个只考虑已有评分记录的矩阵分解方法，称为Funk-SCD，也就是被Yehuda Koren称为隐语义模型的矩阵分解方法。 它的出发点为，既然将一个矩阵做SVD分解成3个矩阵很耗时，同时还面临稀疏的问题，那么我们能不能避开稀疏问题，同时只分解成两个矩阵呢？也就是说，现在期望我们的矩阵M这样进行分解： M_{m\times n}=P^T_{m\times k}Q_{k\times n}我们知道SVD分解已经很成熟了，但是Funk-SVD如何将矩阵M分解成为P和Q呢？这里采用了线性回归的思想。我们的目标是让用户的评分和用矩阵乘积得到的评分残差尽可能的小，也就是说，可以用均方差作为损失函数，来寻找最终的P和Q。 对于某一个用户评分$m_{ij}$如果用Funk-SVD进行矩阵分解，则对应的表示为$q_j^Tp_i$，采用均方差作为损失函数，则我们期望$(m_{ij}-q_j^Tp_i)^2$尽可能的小，如果考虑所有的物品和样本的组合，则我们期望最小化下式： \sum_{i，j}(m_{ij}-q^T_jp_i)^2只要我们能够最小化上面的式子，并求出极值所对应的$p_i,q_j$，则我们最终可以得到矩阵P和Q，那么对于任意矩阵M任意一个空白评分的位置，我们可以通过$q_j^Tp_i$计算预测评分，很漂亮的方法！ 当然，在实际应用中，为了防止过拟合，会加入一个$L_2$的正则化项，因此正是的Funk-SVD的优化目标函数$J(p,q)$是这样的： arg \underset {p_jq_j}{min}\sum_{(i,j)\in K}(m_{ij}-q_j^Tp_i)^2+\lambda (||p_i||^2_2+||q_j||^2_2)其中$K$为已有评分记录的$(i,j)$对集合，$m_{ij}$为用户$i$对物品$$j$的真实评分，$\lambda$是正则化系数，需要调参。假设输入评分矩阵为M，它是$M \times N$维矩阵，通过直接优化以上损失函数得到用户特征矩阵$P(M\times N)$和物品特征矩阵$Q(K\times N)$,其中$K&lt;&lt;M，N$。对于这个优化问题，一般通过梯度下降法来进行优化得到结果。 将上式分别对$p_i,q_j$求导我们得到： \frac{∂J}{∂p_i}=-2(m_{ij}-q_j^Tp_i)q_j+2\lambda p_i$$$$\frac{∂J}{∂q_j}=-2(m_{ij}-q_j^Tp_i)p_i+2\lambda q_j在梯度下降法迭代时，$p_i,q_j$的迭代公式为： p_i=p_i=\alpha [(m_{ij}-q_j^Tp_i)q_j-\lambda p_i]q_j=q_j+\alpha [(m_{ij}-q_j^Tp_i)p_i-\lambda q_j ]通过迭代我们最终可以得到P和Q，进而用于推荐。Funk-SVD算法虽然思想很简单，但在实际应用中效果非常好，这真是验证了大道至简。 三、Bias-SVD在Funk-SVD算法火爆之后，出现了很多Funk-SVD的改进版算法。其中Bias算是改进的比较成功的一种算法。 Funk-SVD方法通过学习用户和物品的特征向量进行预测，即用户和物品的交互信息。用户的特征向量代表了用户的兴趣，物品的特征向量代表了物品的特点，且每一个维度相互对应，两个向量的内积表示用户对该物品的喜好程度。但是我们观测到的评分数据大部分都是都是和用户或物品无关的因素产生的效果，即有很大一部分因素是和用户对物品的喜好无关而只取决于用户或物品本身特性的。例如，对于乐观的用户来说，它的评分行为普遍偏高，而对批判性用户来说，他的评分记录普遍偏低，即使他们对同一物品的评分相同，但是他们对该物品的喜好程度却并不一样。同理，对物品来说，以电影为例，受大众欢迎的电影得到的评分普遍偏高，而一些烂片的评分普遍偏低，这些因素都是独立于用户或产品的因素，而和用户对产品的的喜好无关。 我们把这些独立于用户或独立于物品的因素称为偏置(Bias)部分，将用户和物品的交互即用户对物品的喜好部分称为个性化部分。事实上，在矩阵分解模型中偏好部分对提高评分预测准确率起的作用要大大高于个性化部分所起的作用，以Netflix Prize推荐比赛数据集为例为例，Yehuda Koren仅使用偏置部分可以将评分误差降低32%，而加入个性化部分能降低42%，也就是说只有10%是个性化部分起到的作用，这也充分说明了偏置部分所起的重要性，剩下的58%的误差Yehuda Koren将称之为模型不可解释部分，包括数据噪音等因素。 偏置部分主要由三个子部分组成，分别是 训练集中所有评分记录的全局平均数$\mu$，表示了训练数据的总体评分情况，对于固定的数据集，它是一个常数。 用户偏置$b_i$，独立于物品特征的因素，表示某一特定用户的打分习惯。例如，对于批判性用户对于自己的评分比较苛刻，倾向于打低分；而乐观型用户则打分比较保守，总体打分要偏高。 物品偏置$b_j$，特立于用户兴趣的因素，表示某一特定物品得到的打分情况。以电影为例，好片获得的总体评分偏高，而烂片获得的评分普遍偏低，物品偏置捕获的就是这样的特征。 则偏置部分表示为 b_{ij}=\mu+b_i+b_j则加入了偏置项以后的优化目标函数$J(p,q)$是这样 arg \underset {p_jq_j}{min}\sum_{(i,j)\in K}(m_{ij}-\mu-b_i-b_j-q_j^Tp_i)^2+\lambda (||p_i||^2_2+||q_j||^2_2+||b_i||_2^2+||b_j||_2^2)这个优化目标也可以采用梯度下降法求解。和Funk-SVD不同的是，此时我们多了两个偏置项$b_i,b_j$,$p_i,p_j$的迭代公式和$Funk-SVD$类似，只是每一步的梯度导数稍有不同而已。而$b_i,b_j$一般可以初始设置为0向量，然后参与迭代。 b_i=b_i+α(m_{ij}−μ−b_i−b_j−q^T_jp_i−λb_i)b_j=b_j+α(m_{ij}−μ−b_i−b_j−q^T_jp_i−λb_j)通过迭代我们最终可以得到$P$和$Q$，进而用于推荐。Bias-SVD增加了一些额外因素的考虑，因此在某些场景会比FunkSVD表现好。 四、SVD++SVD++算法在Bias-SVD算法上进一步做了增强，这里它增加考虑用户的隐式反馈。 对于某一个用户$i$，它提供了隐式反馈的物品集合定义为$N(i)$，这个用户对某个物品$j$对应的隐式反馈修正的评分值为$c_{ij}$，那么该用户所有的评分修正值为$\sum_{s\in N(i)} c_{sj}$，一般我们将它们表示为用$q_j^Ty_s$形式，则加入了隐式反馈以后的优化目标函数$J(p,q)$是这样的： arg \underset {p_jq_j}{min}\sum_{(i,j)\in K}(m_{ij}-\mu-b_i-b_j-q_j^Tp_i-q_j^T|N(i)|^{-1/2}\Sigma_{s\in N(i)} y_s)^2+\lambda (||p_i||^2_2+||q_j||^2_2+||b_i||_2^2+||b_j||_2^2+\Sigma_{s\in N(i)}||y_s||^2)其中，引入$|N(i)|^{-1/2}$是为了消除不同$|N(i)|$个数引起的差异。如果需要考虑用户的隐式反馈时，使用SVD++是个不错的选择。 五、矩阵分解的优缺点矩阵分解方法将高维User-Item评分矩阵映射为两个低维用户和物品矩阵，解决了数据稀疏性问题。 使用矩阵分解具有以下优点： 比较容易编程实现，随机梯度下降方法依次迭代即可训练出模型。比较低的时间和空间复杂度，高维矩阵映射为两个低维矩阵节省了存储空间，训练过程比较费时，但是可以离线完成；评分预测一般在线计算，直接使用离线训练得到的参数，可以实时推荐。 预测的精度比较高，预测准确率要高于基于领域的协同过滤以及内容过滤等方法。 非常好的扩展性，很方便在用户特征向量和物品特征向量中添加其它因素，例如添加隐性反馈因素的SVD++，此方法的详细实现参见文献《Koren Y. Factorization meets the neighborhood: a multifaceted collaborative filtering model》；添加时间动态time SVD++，此方法将偏置部分和用户兴趣都表示成一个关于时间的函数，可以很好的捕捉到用户的兴趣漂移，欲知详细实现请阅读文献《Koren Y. Collaborative filtering with temporal dynamics》。 矩阵分解的不足主要有： 模型训练比较费时。 推荐结果不具有很好的可解释性，分解出来的用户和物品矩阵的每个维度* 无法和现实生活中的概念来解释，无法用现实概念给每个维度命名，只能理解为潜在语义空间。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（13）：推荐系统（2）—基于领域的协同过滤]]></title>
    <url>%2F2017%2F03%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8813%EF%BC%89%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E9%A2%86%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%2F</url>
    <content type="text"><![CDATA[基于邻域的算法是推荐系统中最基本的算法，在学术界和业界都有广泛研究与应用。它分为两大类，一类是基于用户的协同过滤算法，另一类是基于物品的协同过滤算法。 一、基于用户的协同过滤算法（user-based collaborative filtering）UserCF是推荐系统的元老级算法，也是最为著名的算法，它标志了推荐系统的诞生。这里首先介绍最基础的算法，然后在此基础上提出不同的改进方法。 1.1 基础算法在一个在线个性推荐系统中，当一个用户A需要个性化推荐时，可以先找到和他有相似兴趣的其他用户，然后将那些用户喜欢的、而用户A没有听说过的物品推荐给A。这种方法就是基于用户的协同过滤算法 我们收集到用户-电影评价矩阵，假设用户A对于物品D的评价为NULL，这时我们对比用户A、用户B、用户C的特征向量（以物品评价为特征），可以发现用户A和用户C的相似度较大，这时我们可以认为，对于用户C喜欢的物品D，用户A也应该喜欢它，这时就把物品D推荐给用户A。 UserCF主要包括两个步骤： 1）寻找和目标用户兴趣相似的用户集合这里的关键就是计算两个用户的兴趣相似度，这里，协同过滤算法主要利用行为的相似度计算兴趣的相似度。给定用户u和用户v，令$N(u)$表示用户$u$曾经有过正反馈（用户的行为倾向于指用户喜欢该物品，反之，负反馈指用户的行为倾向于指用户不喜欢该物品）的物品集合，令$N(v)$为用户$v$曾经有过正反馈的物品集合。我们可以使用Jaccard或者余弦相似度来计算它们之间的兴趣相似度。 Jaccard公式： w_{uv}=\frac{|N\left(u\right)\cap N\left(v\right)|}{|N\left(u\right)\cup N\left(v\right)|} 余弦相似度： w_{uv}=\frac{|N\left(u\right)\cap N\left(v\right)|}{\sqrt{|N\left(u\right)||N\left(v\right)|}} 2）找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给目标用户。 1.2 离线算法评测书中通过MovieLens数据集上的离线试验来测评算法的性能。UserCF只有一个重要参数K，即为每个用户选出K个和他兴趣最相似的用户，然后推荐那K个用户感兴趣的物品。下图为选择不同的K值时算法的性能。逐一分析各个指标： 准确率和召回率：准确率、召回率与K不成线性关系，在此数据集中，K=80左右会获得比较高的准确率和召回率。选择合适的K对获得高的推荐系统精要比较重要，但对K值不是很敏感，保持在一定区域内即可。 流行度：K越大则UserCF推荐结果就越热门，流行度就越高。因为K决定了UserCFA给你做推荐时参考多少和你兴趣相似的其他用户的兴趣，如果K越大，参考的人越多，结果就越来越趋近于全局热门的物品。 覆盖率：覆盖率随着K的增大而减小。因为随着K的增大，UserCF越来越倾向于推荐热门的物品，从而对长尾物品的推荐越来越少，覆盖率就越来越小了 此外对于Random算法（每次随机挑选10个用户没有产生过行为的物品推荐给当前用户）和MostPopular算法（按照物品的流行度给用户推荐他没有产生过行为的物品中最热门的10个物品）这两种基础算法（选取K=80），MostPopular算法准确率和召回率很高，但覆盖率非常低。与这两个极端相比，UserCF的准确率和召回率高得多，覆盖率也很高。 1.3 用户相似度计算的改进用余弦相似度来度量用户之间兴趣相似度过于粗糙，因为两个用户对热门物品采取过相同的行为并不能说明他们兴趣相似，但对于冷门物品才去过相同的行为更能说明他们兴趣的相似度。John S.Breese提出了以下公式： w_{uv}=\frac{\sum_{i\in N\left(u\right)\cap N\left(v\right)}{\frac{1}{\log\left(1+|N\left(i\right)|\right)}}}{\sqrt{|N\left(u\right)||N\left(v\right)|}}该公式通过$1/\log\left(1+|N\left(i\right)|\right)$惩罚了用户$u$和用户$v$共同兴趣列表中热门物品对他们相似度的影响。将基于上述用户相似度公式的UserCF算法记为User-IIF算法。 1.4 实际应用相比基于物品的协同过滤算法ItemCF，UserCFA在目前的实际应用中使用并不多。其中最著名的使用者是$Digg$，它的推荐思路为：用户在$Digg$中主要通过”顶”和”踩”两种行为表达自己对文章的看法。当用户顶了一篇文章，$Digg$就认为该用户对这篇文章有兴趣，而且愿意把这篇文章推荐给其他用户。然后$Digg$找到所有在该用户顶文章之前也顶了这一篇文章的其他用户，然后给他推荐那些人最近顶的其他文章。 二、基于物品的协同过滤算法（item-based collaborative filtering）基于物品的协同过滤算法是目前业界应用最多的算法。亚马逊、Netflix、Hulu、Youtube的推荐算法的基础都是ItemCF。 2.1 基础算法UserCF存在一些缺点： 1）随着网站的用户数目越来越大，计算用户兴趣相似度矩阵将越来越困难，其运算时间复杂度和空间复杂度的增长和用户数的增长近似于平方关系 2）很难对推荐结果作出解释 所以，亚马逊提出了基于物品的协同过滤算法。它给用户推荐那些和他们之前喜欢的物品相似的物品，主要通过分析用户的行为记录计算物品之间的相似度，它认为物品A和物品B具有很大的相似度是因为喜欢物品A的用户大都也喜欢物品B，它也可以利用用户的历史行为给推荐结果提供解释。 同样，我们对比物品A、物品B、物品C的特征向量（以用户对该物品的喜好程度为特征），发现物品A和物品C很像，就把用品C推荐给喜欢物品A的用户C。 ItemCF主要包括两个步骤： 1）计算物品之间的相似度 可以使用下面的公式定义物品的相似度： w_{ij}=\frac{|N\left(i\right)\cap N\left(j\right)|}{|N\left(i\right)|}分母$|N(i)|$是喜欢物品$i$的用户数，而分子$|N\left(i\right)\cap N\left(j\right)|$是同时喜欢物品$i$和物品$j$的用户数。可以理解为喜欢物品$i$的用户中有多少比例的用户也喜欢物品$j$。但是如果$j$很热门，很多人喜欢，那么$w_{ij}$就会很大，接近1，也就是会造成任何物品都会和热门的物品有很大的相似度。为了避免推荐出热门的物品，可以使用下面的公式： w_{ij}=\frac{|N\left(i\right)\cap N\left(j\right)|}{\sqrt{|N\left(i\right)||N\left(j\right)|}}它惩罚了物品$j$的权重，因此减轻了热门物品会和很多物品相似的可能性。从上面的定义可以看到，在协同过滤中两个物品产生相似度是因为它们共同被很多用户喜欢，也就是每个用户都可以通过它们的历史兴趣列表给物品“贡献”相似度。 2）根据物品的相似度和用户的历史行为给用户生成推荐列表]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（13）：推荐系统（1）—简介]]></title>
    <url>%2F2017%2F03%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8813%EF%BC%89%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[一、定义1.1 从搜索引擎说起人们在寻找信息时，常常需要借助搜索引擎，主动地提供准确的关键词来进行相应的搜索，但是当用户无法找到准确描述自己需求的关键字时，搜索引擎就无能为力了。 和搜索引擎一样，推荐系统也是一种帮助用户快速发现有用信息的工具。但是和搜索引擎不同的是，推荐系统不需要用户提供明确的需求，而是通过分析用户的历史行为给用户的兴趣建模，从而主动给用户推荐能够满足它们兴趣与需求的信息。 1.2 推荐系统的具体应用近十年来，推荐引擎对互联网用户来说随处可见。以亚马逊为例，简单说说实际的应用。Amazon有个性化商品推荐列表和相关商品的推荐列表，以下为一种基于物品的的推荐算法（item-based method），它会给我推荐那些和之前我喜欢的物品相似的物品，因为我之前查找过关于算法数据结构相关的书籍，所以就给我推荐了以下这些计算机领域的经典图书除了个性化推荐列表，亚马逊另一个重要的推荐应用就是相关推荐列表。一种是包含购买此商品的顾客也同时购买，另一种是包含浏览过这个商品的顾客购买的其他商品.此外，还有一个应用就是打包销售，同时购买这些商品往往会给你一定的折扣。除了亚马逊等电子商务领域的推荐系统应用，此外还有像Netflix会向用户推荐电影，豆瓣电台给用户推荐喜欢的歌曲，Facebook给用户推荐个性化物品和好友，Flipboard给特定的用户推荐特定领域的阅读资讯，雅虎的个性化广告投放等等。 二、推荐系统评测一个好的推荐系统不仅仅能够准确预测用户的行为，而且能够扩展用户的视野，帮助用户发现那些他们可能会感兴趣，但却不那么容易发现的东西。同时还要能帮助商家将那些被埋没在长尾的好商品介绍给可能对他们感兴趣的用户。 2.1 推荐系统实验方法首先，人们需要通过实验的办法来计算和获取一些指标，从而全面地测评推荐系统的好坏。主要的实验方法有离线实验、用户调查、在线AB测试。 2.1.1 离线实验一般离线试验的步骤如下： 1）通过日志系统获取用户行为数据，并按照一定格式生成一个标准的数据集； 2）将数据集按照一定的规则分成训练集和测试集； 3）在训练集上训练用户兴趣模型，在测试集上进行预测； 4）通过事先定义的离线指标评测算法在测试集上的预测结果。离线试验不需要有对实际系统的控制权、不需要用户参与实验、速度快、可以测试大量算法。但是它无法计算商业上关心的指标（点击率、转化率）、离线试验的指标和商业指标存在差距。 2.1.2 用户调查因为离线实验的指标与实际的商业指标存在差距，所以需要将算法直接上线测试，但在这之前必须进行用户调查，否则直接进行在线实验会有较高的风险，因为对算法会不会降低用户满意度谁都没有把握。用户调查可以获得很多体现用户主观感受的指标，相对在线实验风险很低，出现错误后很容易弥补。但是招募测试用户代价较大，很难组织大规模的测试一款能过户，因此会使得测试结果的统计意义不足。 2.1.3 在线实验AB测试时最常用的在线测评算法的实验方法。它通过一定的规则将用户随机分成机组，并对不同组的用户采用不同的算法，然后通过统计不同组用户的各种不同的评测指标比较不同算法，比如可以统计不同组用户的点击率，通过点击率比较不同算法的性能。AB测试可以公平获得不同算法实际在线时的性能指标，包括商业上关注的指标。但是周期较长，必须进行长期的实验才能得到可靠的结果，因此常常只会用它测试那些在离线实验和用户调查中表现很好的算法。以上就是一个新的推荐算法最终上线所需要做的事。总结一下： 1）首先，需要通过离线试验证明它在很多离线指标上优于现有算法； 2）然后，需要通过用户调查确定它的用户满意度不低于现有算法。 3）最后，通过在线的AB测试确定它在我们关心的指标上优于现有的算法。 2.2 离线评测指标令$R(u)$是根据用户在训练集上的行为给用户作出的推荐列表，而$T(u)$是用户在测试集上的行为列表。 准确率（Precison）: Precision=\frac{\sum_{u\in U}{|R\left(u\right)\cap T\left(u\right)|}}{\sum_{u\in U}{|R\left(u\right)|}} 召回率（Recall）： \textrm{Re}call=\frac{\sum_{u\in U}{|R\left(u\right)\cap T\left(u\right)|}}{\sum_{u\in U}{|T\left(u\right)|}} 覆盖率(Coverage)，描述了一个推荐系统对物品长尾的发掘能力， 最简单的定义为推荐系统能够推荐出来的物品占总物品集合的比例 信息熵 : H=-\sum_{i=1}^n{p\left(i\right)\log p\left(i\right)} 基尼系数： Gini=\sum_{i=1}^n{p_i\left(1-p_i\right)} 马太效应：即强者更强，弱者更弱。若一个系统会增大热门物品和非热门物品的流行度差异，让热门的物品更加热门，不认的物品更加不热门，那么这个系统就有马太效应。推荐系统的初衷是希望消除马太效应，使得各种物品都能被展示给它们感兴趣的某一类人群。但是现在主流的推荐算法都具有马太效应。可以使用基尼系数来评测推荐系统是否具有马太效应。如果G1是从初始用户行为中计算出的物品流行度的基尼系数，G2是从推荐列表中计算出的物品流行度的基尼系数，那么如果G2&gt;G1，就说明推荐系统具有马太效应。 多样性：如果推荐列表比较多样，覆盖了用户绝大多数的兴趣点，那么就会增加用户找到感兴趣物品的概率。因此给用户的推荐列表也需要满足用户广泛的兴趣，即多样性。 多样性描述了推荐列表物品两两之间的不相似性。假设$s(i,j)\in[0,1]$定义了物品$i$和$j$之间的相似度，那么用户$u$的推荐列表$R(u)$的多样性定义如下： Diversity\left(R\left(u\right)\right)=1-\frac{\sum_{i,j\in R\left(u\right),i\ne j}{s\left(i,j\right)}}{\frac{1}{2}|R\left(u\right)|\left(|R\left(u\right)-1|\right)} 而推荐系统整体的多样性可以定义为所有用户推荐列表多样性的平均值： Diversity=\frac{1}{|U|}\sum_{u\in U}{Diversity\left(R\left(u\right)\right)} 新颖性：给用户推荐那些它们以前没有听说过的物品。最简单方法是利用推荐结果的平均流行度，因为越不热门的物品越可能让用户觉得新颖。但是这个指标比较粗略，因为不同用户不知道的东西是不同的。需要用户调查来准确统计新颖性。 惊喜度：惊喜度和新颖性两者之间是有区别的，如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（12）：SVM（6）—SVM与LR的异同]]></title>
    <url>%2F2017%2F03%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%886%EF%BC%89%E2%80%94%E2%80%94SVM%E4%B8%8ELR%E7%9A%84%E5%BC%82%E5%90%8C%2F</url>
    <content type="text"><![CDATA[本文讲述LR与SVM的异同点 一、LR与SVM的相同点 LR和SVM都是分类算法，都是监督学习算法。 如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。LR也是可以用核函数的，至于为什么通常在SVM中运用核函数而不在LR中运用，后面讲到他们之间区别的时候会重点分析。总之，原始的LR和SVM都是线性分类器，这也是为什么通常没人问你决策树和LR什么区别，决策树和SVM什么区别，你说一个非线性分类器和一个线性分类器有什么区别？ LR和SVM都是判别模型。判别模型会生成一个表示P(Y|X)的判别函数（或预测模型），而生成模型先计算联合概率p(Y,X)然后通过贝叶斯公式转化为条件概率。简单来说，在计算判别模型时，不会计算联合概率，而在计算生成模型时，必须先计算联合概率。或者这样理解：生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。当然，这也是为什么很少有人问你朴素贝叶斯和LR以及朴素贝叶斯和SVM有什么区别。 LR和SVM在学术界和工业界都广为人知并且应用广泛。 二、LR与SVM的不同点1.损失函数 我们先来看一下带松弛变量的 SVM 和正则化的逻辑回归它们的损失函数：其中 $g(z)=(1+exp(−z))^{−1}$可以将两者统一起来:这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重,两者的根本目的都是一样的。即支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。 影响SVM决策面的样本点只有少数的支持向量，当在支持向量外添加或减少任何样本点对分类决策面没有任何影响；而在LR中，每个样本点都会影响决策面的结果。用下图进行说明： 支持向量机改变非支持向量样本并不会引起决策面的变化逻辑回归中改变任何样本都会引起决策面的变化 因此线性SVM不直接依赖于数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要先对数据做平衡处理。​ 2.核技巧 在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。 ​这个问题理解起来非常简单。分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。通过上面的第二点不同点可以了解，在计算决策面时，SVM转化为对偶问题后，只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的），这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。​ 3.正则项​​根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。但是逻辑回归相对来说模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些。但是SVM的理论基础更加牢固，有一套结构化风险最小化的理论基础，虽然一般使用的人不太会去关注。 4.异常值 两者对异常的敏感度也不一样。同样的线性分类情况下，如果异常点较多的话，无法剔除，首先LR，LR中每个样本都是有贡献的，最大似然后会自动压制异常的贡献，SVM+软间隔对异常还是比较敏感，因为其训练只需要支持向量，有效样本本来就不高，一旦被干扰，预测结果难以预料。 5.normalization 两个模型对数据和参数的敏感程度不同，Linear SVM比较依赖penalty的系数和数据表达空间的测度，而（带正则项的）LR比较依赖对参数做L1 regularization的系数。但是由于他们或多或少都是线性分类器，所以实际上对低维度数据overfitting的能力都比较有限，相比之下对高维度数据，LR的表现会更加稳定，为什么呢？ 因为Linear SVM在计算margin有多“宽”的时候是依赖数据表达上的距离测度的，换句话说如果这个测度不好（badly scaled，这种情况在高维数据尤为显著），所求得的所谓Large margin就没有意义了，这个问题即使换用kernel trick（比如用Gaussian kernel）也无法完全避免。所以使用Linear SVM之前一般都需要先对数据做normalization，而求解LR（without regularization）时则不需要或者结果不敏感。 Linear SVM 和 LR 有什么异同？]]></content>
      <tags>
        <tag>LR</tag>
        <tag>机器学习</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（12）：SVM（5）—对偶]]></title>
    <url>%2F2017%2F03%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%885%EF%BC%89%E2%80%94%E2%80%94%E5%AF%B9%E5%81%B6%2F</url>
    <content type="text"><![CDATA[原文地址：支持向量机：Duality在之前关于 support vector 的推导中，我们提到了 dual ，这里再来补充一点相关的知识。这套理论不仅适用于 SVM 的优化问题，而是对于所有带约束的优化问题都适用的，是优化理论中的一个重要部分。简单来说，对于任意一个带约束的优化都可以写成这样的形式： 形式统一能够简化推导过程中不必要的复杂性。其他的形式都可以归约到这样的标准形式，例如一个 $maxf(x)$ 可以转化为 $min−f(x)$等。假如 $f_0,f_1,…,f_m $全都是凸函数，并且 $h_1,…,h_p$ 全都是仿射函数（就是形如 $Ax+b$ 的形式），那么这个问题就叫做凸优化（Convex Optimization）问题。凸优化问题有许多优良的性质，例如它的极值是唯一的。不过，这里我们并没有假定需要处理的优化问题是一个凸优化问题。 虽然约束条件能够帮助我们减小搜索空间，但是如果约束条件本身就是比较复杂的形式的话，其实是一件很让人头痛的问题，为此我们希望把带约束的优化问题转化为无约束的优化问题。为此，我们定义 Lagrangian 如下：它通过一些系数把约束条件和目标函数结合在了一起。当然 Lagrangian 本身并不好玩，现在让我们来让他针对 $λ$ 和 $ν$ 最大化，令：这里 $λ⪰0$ 理解为向量 $λ$ 的每一个元素都非负即可。这个函数 $z(x)$ 对于满足原始问题约束条件的那些 $x$ 来说，其值等于 $f_0(x)$ ，这很容易验证，因为满足约束条件的$x$ 会使得 $h_i(x)=0$ ，因此最后一项消掉了，而 $f_i(x)≤0$ ，并且我们要求了 $λ⪰0$ ，因此 $λ_if_i(x)≤0$ ，所以最大值只能在它们都取零的时候得到，这个时候就只剩下 $f_0(x)$ 了。因此，对于满足约束条件的那些 $x$ 来说，$f_0(x)=z(x)$ 。这样一来，原始的带约束的优化问题其实等价于如下的无约束优化问题：因为如果原始问题有最优值，那么肯定是在满足约束条件的某个 $x^∗$ 取得，而对于所有满足约束条件的 $x ，z(x) 和 f_0(x)$ 都是相等的。至于那些不满足约束条件的 $x$ ，原始问题是无法取到的，否则极值问题无解。很容易验证对于这些不满足约束条件的$x$ 有 $z(x)=∞$，这也和原始问题是一致的，因为求最小值得到无穷大可以和“无解”看作是相容的。 到这里，我们成功把带约束问题转化为了无约束问题，不过这其实只是一个形式上的重写，并没有什么本质上的改变。我们只是把原来的问题通过 Lagrangian 写作了如下形式：这个问题（或者说原始的带约束的形式）称作 primal problem 。如果你看过之前关于 SVM 的推导，那么肯定就知道了，相对应的还有一个 dual problem ，其形式非常类似，只是把 min 和 max 交换了一下：交换之后的 dual problem 和原来的 primal problem 并不相等，直观地，我们可以这样来理解：胖子中最瘦的那个都比瘦骨精中最胖的那个要胖。当然这是很不严格的说法，而且扣字眼的话可以纠缠不休，所以我们还是来看严格数学描述。和刚才的 $z(x)$ 类似，我们也用一个记号来表示内层的这个函数，记：并称 $g(λ,ν)$ 为 Lagrange dual function （不要和 L 的 Lagrangian 混淆了）。g 有一个很好的性质就是它是 primal problem 的一个下界。换句话说，如果 primal problem 的最小值记为 $p^∗$ ，那么对于所有的 $λ⪰0$ 和 $ν$ ，我们有：因为对于极值点（实际上包括所有满足约束条件的点）$x^∗$，注意到 $λ⪰0$ ，我们总是有因此于是这样一来就确定了 g 的下界性质，于是 实际上就是最大的下界。这是很自然的，因为得到下界之后，我们自然地就希望得到最好的下界，也就是最大的那一个——因为它离我们要逼近的值最近呀。记 dual problem 的最优值为 $d^∗$ 的话，根据上面的推导，我们就得到了如下性质：这个性质叫做 weak duality ，对于所有的优化问题都成立。其中 $p^∗−d^∗$ 被称作 duality gap 。需要注意的是，无论 primal problem 是什么形式，dual problem 总是一个 convex optimization 的问题——它的极值是唯一的（如果存在的话），并且有现成的软件包可以对凸优化问题进行求解（虽然求解 general 的 convex optimization 实际上是很慢并且只能求解规模较小的问题的）。 这样一来，对于那些难以求解的 primal problem （比如，甚至可以是 NP 问题），我们可以通过找出它的 dual problem ，通过优化这个 dual problem 来得到原始问题的一个下界估计。或者说我们甚至都不用去优化这个 dual problem ，而是（通过某些方法，例如随机）选取一些 $λ⪰0$ 和 $ν$ ，带到 $g(λ,ν)$ 中，这样也会得到一些下界（只不过不一定是最大的那个下界而已）。当然要选 λ 和 ν 也并不是总是“随机选”那么容易，根据具体问题，有时候选出来的 $λ$ 和 $ν$ 带入 $g$ 会得到 $−∞ $，这虽然是一个完全合法的下界，然而却并没有给我们带来任何有用的信息。 故事到这里还没有结束，既然有 weak duality ，显然就会有 strong duality 。所谓 strong duality ，就是 这是一个很好的性质，strong duality 成立的情况下，我们可以通过求解 dual problem 来优化 primal problem ，在 SVM 中我们就是这样做的。当然并不是所有的问题都能满足 strong duality ，在讲 SVM 的时候我们直接假定了 strong duality 的成立，这里我们就来提一下 strong duality 成立的条件。 不过，这个问题如果要讲清楚，估计写一本书都不够，应该也有不少专门做优化方面的人在研究这相关的问题吧，我没有兴趣（当然也没有精力和能力）来做一个完整的介绍，相信大家也没有兴趣来看这样的东西——否则你肯定是专门研究优化方面的问题的了，此时你肯定比我懂得更多，也就不用看我写的介绍啦。 所以，这里我们就简要地介绍一下 Slater 条件和 KKT 条件。Slater 条件是指存在严格满足约束条件的点 $x$ ，这里的“严格”是指 $f_i(x)≤0$ 中的“小于或等于号”要严格取到“小于号”，亦即，存在 x 满足 我们有：如果原始问题是 Convex 的并且满足 Slater 条件的话，那么 strong duality 成立。需要注意的是，这里只是指出了 strong duality 成立的一种情况，而并不是唯一情况。例如，对于某些非 convex optimization 的问题，strong duality 也成立。这里我们不妨回顾一下 SVM 的 primal problem ，那是一个 convex optimization 问题（QP 是凸优化问题的一种特殊情况），而 Slater 条件实际上在这里就等价于是存在这样的一个超平面将数据分隔开来，亦即是“数据是可分的”。当数据不可分是，strong duality 不能成立，不过，这个时候我们寻找分隔平面这个问题本身也就是没有意义的了，至于我们如何通过把数据映射到特征空间中来解决不可分的问题，这个当时已经介绍过了，这里就不多说了。 让我们回到 duality 的话题。来看看 strong duality 成立的时候的一些性质。 假设 $x^∗$ 和 $(λ^∗,ν^∗)$ 分别是 primal problem 和 dual problem 的极值点，相应的极值为 $p^∗$ 和 $d^∗$ ，首先 $p^∗=d^∗$ ，此时我们可以得到由于两头是相等的，所以这一系列的式子里的不等号全部都可以换成等号。根据第一个不等号我们可以得到 $x^∗$ 是 $L(x,λ^∗,ν^∗)$ 的一个极值点，由此可以知道 $L(x,λ^∗,ν^∗)$ 在 $x^∗$ 处的梯度应该等于 0 ，亦即：此外，由第二个不等式，又显然$ λ^∗_if_i(x^∗)$ 都是非正的，因此我们可以得到这个条件叫做 complementary slackness 。显然，如果 $λ^∗_i&gt;0$，那么必定有 $f_i(x^∗)=0$ ；反过来，如果 $f_i(x^∗)&lt;0$ 那么可以得到 $λ^∗_i=0$ 。这个条件正是我们在介绍支持向量的文章末尾时用来证明那些非支持向量（对应于 $f_i(x^∗)&lt;0$）所对应的系数 $α_i$ （在本文里对应 $λ_i$ ）是为零的.)再将其他一些显而易见的条件写到一起，就是传说中的 KKT (Karush-Kuhn-Tucker) 条件：任何满足 strong duality （不一定要求是通过 Slater 条件得到，也不一定要求是凸优化问题）的问题都满足 KKT 条件，换句话说，这是 strong duality 的一个必要条件。 不过，当原始问题是凸优化问题的时候（当然还要求原函数是可微的，否则 KKT 条件的最后一个式子就没有意义了），KKT 就可以升级为充要条件。换句话说，如果 primal problem 是一个凸优化问题，且存在 $x^˜$ 和 $(λ^˜,ν^˜)$ 满足 KKT 条件，那么它们分别是 primal problem 和 dual problem 的极值点并且 strong duality 成立。其证明也比较简单，首先 primal problem 是凸优化问题的话，$g(λ,ν)=min_xL(x,λ,ν)$ 的求解对每一组固定的 $(λ,ν)$ 来说也是一个凸优化问题，由 KKT 条件的最后一个式子，知道 $x^˜$ 是 $min_xL(x,λ^˜,ν^˜)$ 的极值点（如果不是凸优化问题，则不一定能推出来），亦即： 最后一个式子是根据 KKT 条件的第二和第四个条件得到。由于 g 是 f0 的下界，这样一来，就证明了 duality gap 为零，也就是说，strong duality 成立。 到此为止，做一下总结。我们简要地介绍了 duality 的概念，基本上没有给什么具体的例子。不过由于内容比较多，为了避免文章超长，就挑了一些重点讲了一下。总的来说，一个优化问题，通过求出它的 dual problem ，在只有 weak duality 成立的情况下，我们至少可以得到原始问题的一个下界。而如果 strong duality 成立，则可以直接求解 dual problem 来解决原始问题，就如同经典的 SVM 的求解过程一样。有可能 dual problem 比 primal problem 更容易求解，或者 dual problem 有一些优良的结构（例如 SVM 中通过 dual problem 我们可以将问题表示成数据的内积形式从而使得 kernel trick 的应用成为可能）。此外，还有一些情况会同时求解 dual 和 primal problem ，比如在迭代求解的过程中，通过判断 duality gap 的大小，可以得出一个有效的迭代停止条件。]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
        <tag>对偶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（12）：SVM（4）—SMO]]></title>
    <url>%2F2017%2F03%2F02%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%884%EF%BC%89%E2%80%94%E2%80%94SMO%2F</url>
    <content type="text"><![CDATA[SMO算法是一种启发式算法，其基本思想是：如果所有变量的解都满足最优化问题的KKT条件，那么这个优化问题的解就得到了，因为KKT条件是该优化问题的充分必要条件。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题。这个二次规划问题关于这两个变量的解应该是更接近原始二次规划问题的解，因为这会使得原始二次规划问题的目标函数值变得更小。重要的是，这时子问题可以通过解析方法求解，这样就可以大大提升整个算法的计算速度。子问题有两个变量，一个是违反KKT条件最严重的那个，另一个由约束条件自动确定。如果SMO算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。 四、序列最小最优化算法（SMO）通常对于优化问题，我们没有办法的时候就会想到最笨的办法，也就是梯度下降。注意我们这里的问题是要求最大值，只要在前面加上一个负号就可以转化为求最小值，所以$Gradient Descent$和$Gradient Ascend$并没有什么本质的区别，其基本思想直观上来说就是：梯度是函数值增幅最大的方向，因此只要沿着梯度的反方向走，就能使得函数值减小得越大，从而期望迅速达到最小值。当然普通的$Gradient Descent$并不能保证达到最小值，因为很有可能陷入一个局部极小值。不过对于二次规划问题，极值只有一个，所以是没有局部极值的问题。 另外还有一种叫做$Coordinate Descend$的变种，它每次只选择一个维度，例如$a=(a_1,···,a_n)$，它每次选取$a_i$为变量，而将其他都看成是常数，从而原始的问题在这一步编程一个一元函数，然后针对这个一元函数求最小值，如此反复轮换不同的维度进行迭代。$Coordinate Descend$的主要用处在于那些原本很复杂，但是如果只限制在一维的情况下则变得很简单甚至可以直接求极值的情况，例如我们这里的问题，暂且不管约束条件，如果只看目标函数的话，当$a$只有一个分量是变量的时候，这就是一个普通的一元二次函数的极值问题，初中生也会做，带入公式即可。 然后这里还有一个问题就是约束条件的存在，其实如果没有约束条件的话，本身就是一个多元的二次规划问题，也是很好求解的。但是有了约束条件，结果让$Coordinate Descend$变得很尴尬了，直接根据第二个约束条件$\sum_{i=1}^N{a_iy_i=0}$,$a_1$的值立即就可以定下来，事实上，迭代每个坐标维度，最后发现优化根本进行不下去，因为迭代了一轮之后会发现根本没有任何进展，一切停留在初始值。 所以SMO一次选取了两个坐标来进行优化。例如，我们假设现在选取$a_1$和$a_2$为变量，其余为常量，则根据约束条件我们有： \sum_{i=1}^N{a_iy_i=0}\Longrightarrow a_2=\frac{1}{y_2}\left( -\sum_{i=3}^N{a_iy_i-a_1y_1} \right) \Longleftrightarrow y_2\left( K-a_1y_1 \right)其中那个从3到n的作和都是常量，我们统一记作K。将这个式子代入原来的目标函数中，可以消去$a_2$，从而变成一个一元二次函数。总之现在变成了一个带区间约束的一元二次函数极值问题。唯一要注意的就是这里的约束条件，一个就是$a_1$本身需要满足$0≤a_i≤C$,然后由于$a_2$也要满足同样的约束，即：$0≤y_2(K-a_1y_1)≤C$，可以得带$a_1$的一个可行区间，同$[0,C]$交集即可得到最终的可行区间。投影到$a_1$轴上所对应的区间即是$a_1$的取值范围，在这个区间内求二次函数的最大值即可完成SMO的一步迭代。 同$Coordinate Descent$一样，SMO也会选取不同的两个$coordinate$维度进行优化，可以看出由于每一个迭代步骤实际上是一个可以直接求解的一元二次函数极值问题，所以求解非常高效。此外，SMO也并不是一次或随机地选取两个坐标函数极值问题，而是有一些启发式的策略来选取最优的两个坐标维度。]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
        <tag>SMO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（12）：SVM（3）—非线性支持向量机]]></title>
    <url>%2F2017%2F03%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%883%EF%BC%89%E2%80%94%E2%80%94%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。此为核方法，是比支持向量机更为一般的机器学习方法。 三、非线性支持向量机与核函数3.1 核技巧前面我们介绍了线性情况下的支持向量机，他通过寻找一个现行的超平面来达到对数据线性分类的目的。不过，由于是线性方法，所以对非线性的数据就没有办法处理了。 例如图中的两类数据，分别分布为两个圆圈的形状，不论是任何高级的分类器，只要他是线性的，就没有办法处理，SVM也不行。因为这样的数据本身就是线性不可分的。 此数据集为两个半径不同的圆圈加上了少量的噪音得到，所以一个理想的分界应该是一个圆圈而不是一条直线。如果用$X_1和X_2$来表示这个二维平面的两个坐标的话，则此方程可以写作 a_1X_1+a_2X_{1}^{2}+a_3X_2+a_4X_{2}^{2}+a_5X_1X_2+a_6=0注意上面的形式，如果我们构造另外一个无谓的空间，其中五个坐标的值分别为$Z_1=X_1,Z_2=X_1^2,Z_3=X_2,Z_4=X_2^2,Z_5=X_1·X_2$ 那么显然，上面的方程在新的坐标系下可以写作： \sum_{i=1}^5{a_iZ_i+}a_6=0如果我们做一个映射$\phi :R^2\rightarrow R^5$，将$X$按照上面的规则映射为$Z$那么在新的空间中原来的数据将变成线性可分的，从而使用之前我们推倒的线性分类算法就可以进行处理了。这正是核方法处理非线性问题的基本思想。 总结一下，用线性分类方法求解非线性分类问题分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。核技巧就属于这样的方法。 现在回到SVM的情形，假设原始的数据是非线性的，我们通过一个映射$\phi \left( · \right) $将其映射到一个高维空间中，数据变得线性可分了，这个时候，我们就可以使用原来的推导来进行计算，只是所有的推导现在是在新的空间，而不是原始空间中进行。当然，推导过程也并不是可以简单地直接类比的，例如，原本我们要求超平面的法向量$w$，但是如果映射之后得到的新空间的维度是无穷维的（确实会出现这样的情况，比如后面会提到的高斯核函数），要表示一个无穷维的向量描述起来就比较麻烦。 我们似乎可以这样做，拿到非线性数据，就找一个映射$\phi（·） $，然后一股脑把原来的数据映射到新空间，再做线性SVM即可。但是在之前对一个二维空间做映射，选择的新空间是原始空间的所有一阶和二阶的组合，得到了五个维度；但如果原始空间是三维，我们就会得到19维的新空间，这个数目是呈爆炸性增长的，这给映射的计算带来了很大困难，而且如果遇到无穷维的情况，就根本无从计算了，所以就需要核函数出马了。 核技巧的想法是，再学习与预测中只定义核函数$K\left( x,z \right) $，而不显式地定义映射函数$\phi（·） $。不像之前是映射到高维空间中，然后再根据内积公式进行计算，现在我们直接在原来的低维空间中进行计算，而不需要显式的写出映射后的结果。通常，直接计算$K(x,z)$比较容易，而通过$\phi \left( x \right) \mathrm{和}\phi \left( z \right) $计算$K(x,z)$并不容易。 最理想的情况下，我们希望知道数据的具体形状和分布，从而得到一个刚好可以将数据映射成线性可分的$\phi（·） $，然后通过这个$\phi（·） $得到对应的$K(·，·)$进行内积计算。然而，第二步通常是非常困难甚至完全没法做的。不过，由于第一步也是几乎无法做到的，因为对于任意的数据分析其形状找到合适的映射本身就不是什么容易的事情，所以，人们通常是“胡乱”选择一个核函数即可——我们直到她对应了某个映射，虽然我们不知道这个映射具体是什么，由于我们的计算只需要核函数即可，所以我们也并不关心也没有必要求出所对应的映射的具体形式。 我们注意到在线性支持向量机的对偶问题中，无论是目标函数还是决策函数（分离超平面）都只涉及输入实例与实例之间的内积。在对偶问题的目标函数中的内积$x_i·x_j$可以用核函数$K(x_i·x_j)=\phi \left( x_i \right) ·\phi \left( x_j \right) $来代替，此时对偶问题的目标函数成为： \underset{a}{\max}\ \ \ \ \sum_{i=1}^N{a_i-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_jK⟨ x_i,x_j ⟩}}}同样，分类决策函数中的内积也可以用核函数代替，而分类决策函数式成为 sign\left( \sum_{i=1}^N{a_{i}^{*}y_iK⟨ x_i,x⟩}+b \right)在核函数$K(x,z)$给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐性地在特征空间进行的，不需要显式地定义特征空间和映射函数。这样的技巧称为核技巧，它是巧妙地利用线性分类学习方法与核函数解决非线性问题的技术。在实际应用中，往往依赖领域知识直接选择核函数，核函数选择的有效性需要通过实验验证。 3.2 常用核函数通常人们会从一些常用的核函数中选择，根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数。 多项式核函数polynomial kernel function K\left( x,z \right) =\left( x·z+1 \right) ^p对应的支持向量机是一个p次多项式分类器。在此情形下，分类决策函数成为f\left( x \right) =sign\left( \sum_{i=1}^N{a_iy_i\left( x_i·x+1 \right) ^p+b} \right) 高斯核函数gaussian kernel function K\left( x,z \right) =\exp \left( -\frac{||x-z||^2}{2\sigma ^2} \right)这个核就是会将原始空间映射为无穷维空间的那个家伙。不过如果$\sigma$选得很大的话，高次特征上的权重实际上衰减的非常快，所以实际上相当于一个低维的子空间；反过来，如果$\sigma$选得很小，则可以将任意的数据映射为线性可分，当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数$\sigma$，高斯核实际上具有相当高的灵活性，也是使用最为广泛的核函数之一。它对应的支持向量机是高斯径向基函数分类器，在此情形下，分类决策函数称为f\left( x \right) =sign\left( \sum_{i=1}^N{a_iy_i\exp \left( -\frac{||x-z||^2}{2\sigma ^2} \right) +b} \right)! 字符串核函数：核函数不仅可以定义在欧式空间上，还可以定义在离散数据的集合上，比如，字符串核实定义在字符串集合上的核函数，字符串核函数在文本分类、信息检索、生物信息学等方面都有应用。 线性核 $κ(x_1,x_2)=⟨x_1,x_2⟩$ ，这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了。 最后，总结一下：对于非线性的情况，SVM 的处理方法是选择一个核函数 κ(⋅,⋅) ，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。由于核函数的优良品质，这样的非线性扩展在计算量上并没有比原来复杂多少，这一点是非常难得的。当然，这要归功于核方法——除了 SVM 之外，任何将计算表示为数据点的内积的方法，都可以使用核方法进行非线性扩展。 非线性支持向量机学习算法步骤如下： 选取适当的核函数$K(x,z)$和适当的参数$C$，构造并求解最优化问题 \underset{a}{\max}\,\,\,\,\,\,\,\,\sum_{i=1}^N{a_i-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_jK\left( x_i,x_j \right)}}} s.t.\ \ \ \ \sum_{i=1}^N{a_iy_i=0} 0\le a_i\le C\ \ ,\ \ i=1,2,···,N求得最优解 a^*=\left( a_{1}^{*},a_{2}^{*},···,a_{N}^{*} \right) ^T 选择$a^$的一个正分量$a_i^$适合约束条件$0&lt;a_i&lt;C$,计算 b^*=y_j-\sum_{i=1}^N{y_ia_{i}^{*}K\left( x_i·x_j \right)} 构造决策函数： f(X)=sign\left( \sum_{i=1}^N{a_{i}^{*}y_iK\left( x_i,x \right)}+b \right)当$K(x,z)$是正定核函数时，该问题为凸二次规划问题，解是存在的。]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
        <tag>非线性支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（12）：SVM（2）—线性支持向量机]]></title>
    <url>%2F2017%2F02%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[当训练数据近似线性可分时，通过软间隔最大化学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机。 二、线性支持向量机与软间隔最大化2.1 线性支持向量机通常情况是，训练数据中有一些特异点outlier，将这些特异点除去后，剩下大部分的样本点组成的集合是线性可分的。 线性不可分意味着某些样本点不能满足函数间隔大于等于1的约束条件。为了解决这个问题，可以对每个样本点引进一个松弛变量$\xi \geqslant 0$，使函数间隔加上松弛变量大于等于1.这样，约束条件变成 y_i\left( w·x_i+b \right) \geqslant 1-\xi _i同时，对每个松弛变量$\xi \geqslant 0$，支付一个代价$\xi \geqslant 0$。当然，如果我们允许$\xi \geqslant 0$任意大的话，那任意的超平面都是符合条件的了。所以，我们在原来的目标函数后面加上一项，使得这些 $\xi \geqslant 0$的总和也要最小：目标函数由原来的$\frac{1}{2}||w||^2$变成 \frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i}这里，$C&gt;0$称为惩罚参数，一般事先由应用问题决定，控制目标函数中两项（“寻找 $margin$ 最大的超平面”和“保证数据点偏差量最小”）之间的权重，$C$越大时对误分类的惩罚增大，$C$值小时对误分类的惩罚减小。最小化目标函数包含两层含义：使$\frac{1}{2}||w||^2$尽量小即间隔尽量大，同时使误分类点的个数尽量小，C是调和二者的系数。则有以下优化问题： \underset{w,b,\xi}{\min}\ \frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i} s.t.\ \ y_i\left( w·x_i+b \right) \geqslant 1-\xi _i\ ,\ i=1,2,···,N \xi _i\geqslant 0,\ i=1,2,···\mathrm{，}N可证明$w$的解是唯一的，但$b$的解不唯一，$b$的解存在于一个区间。 用之前的方法将限制加入到目标函数中，得到如下原始最优化问题的拉格朗日函数： L\left( w,b,\xi ,a,u \right) =\frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i-\sum_{i=1}^N{a_i\left( y_i\left( w·x_i+b \right) -1+\xi _i \right) -\sum_{i=1}^N{u_i\xi _i}}}首先求拉格朗日函数针对$w,b,\xi $的极小。 \frac{\partial L}{\partial w}=0\Rightarrow w=\sum_{i=1}^N{a_iy_ix_i} \frac{\partial L}{\partial b}=0\Rightarrow \sum_{i=1}^N{a_iy_i=0} \frac{\partial L}{\partial \xi _i}=0\Rightarrow C-a_i-u_i=0，i=1,2,3···,N将它们代入拉格朗日函数，得到和原来一样的目标函数。 \underset{a}{\max}\ \ -\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) +\sum_{i=1}^N{a_i}}} s.t.\ \sum_{i=1}^N{a_iy_i=0} C-a_i-u_i=0 a_i\geqslant 0 u_i\geqslant 0不过，由于我们得到$C-a_i-u_i=0$，而又有$u_i&gt;0$（作为拉格朗日乘子的条件）,因此有$a_i≤C$,所以整个dual问题现在写作： \underset{a}{\max}\ \ -\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j⟨x_i·x_j ⟩+\sum_{i=1}^N{a_i}}} s.t.\ \sum_{i=1}^N{a_iy_i=0} 0\le a_i\le C\ ,\ \ i=1,2,···,N和之前的结果对比一下，可以看到唯一的区别就是现在拉格朗日乘子$a$多了一个上限$C$。而 Kernel 化的非线性形式也是一样的，只要把$⟨x_i,x_j⟩$ 换成 $κ(x_i,x_j)$ 即可。 构造并求解上述二次规划问题后求得最优解 a^*=\left( a_{1}^{*},a_{2}^{*},···,a_{N}^{*} \right) ^T然后计算 w^*=\sum_{i=1}^N{a_{i}^{*}y_ix_i}选择$a^$的一个分量$a_i^$适合约束条件$0&lt;a_i&lt;C$,计算 b^*=y_j-\sum_{i=1}^N{a_{i}^{*}y_i⟨x_i·x_j ⟩}对任一适合条件都可求得一个$b^*$，但是由于原始问题对$b$的求解并不唯一，所以实际计算时可以取在所有符合条件的样本点上的平均值。 2.2 支持向量再现性不可分的情况下，将对偶问题的解中对应于$a_i^*&gt;0$的样本点$(x_i,y_i)$的实例$x_i$称为支持向量（软间隔的支持向量）。如图所示，这时的支持向量要比线性可分时的情况复杂一些。 图中，分离超平面由实线表示，间隔边界由虚线表示。正例点由$。$表示，负例点由$×$表示。图中还标出了实例$x_i$到间隔边界的距离$\frac{\xi _i}{||w||}$。 软间隔的支持向量$x_i$要么在间隔边界上，要么在间隔边界与分离超平面之间，要么在分离超平面误分类一侧。 若$a_i^*&lt;C$，则$\xi _i=0$，支持向量恰好落在间隔边界上； 若$a_i^*=C,0&lt;\xi _i&lt;1$，则分类正确，$x_i$在间隔边界与分离超平面之间； 若$a_i^*=C，\xi _i=1$则$x_i$在分隔超平面上； 若$a_i^*=C,\xi _i&gt;1$，则$x_i$位于分离超平面误分一侧。 2.3 Hinge损失函数线性支持向量机学习除了原始最优化问题，还有另外一种解释，就是最优化以下目标函数： \sum_i^{N}[1-y_i(w·x_i+b)]_++\lambda||w||^2目标函数的第一项是经验损失或经验风险，函数 L(y·(w·x+b))=[1-y(w·x+b)]_+称为合页损失函数（hinge loss function）。下标”+”表示以下取正值的函数： \left[z\right]_+=\left\{\begin{array}{l} z\ ,\ z>0\\ 0\ ,\ z\le 0\\ \end{array}\right.这就是说，当样本点$(x_i,y_i)$被正确分类且函数间隔（确信度）$y_i(w·x_i+b)$大于1时，损失是0，否则损失是$1-y_i(w·x_i+b)$。目标函数的第二项是系数为$\lambda$的$w$的$L_2$范数，是正则化项。 接下来证明线性支持向量机原始最优化问题： \underset{w,b,\xi}{\min}\ \frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i} s.t.\ \ y_i\left( w·x_i+b \right) \geqslant 1-\xi _i\ ,\ i=1,2,···,N \xi _i\geqslant 0,\ i=1,2,···\mathrm{，}N等价于最优化问题 \underset{w,b}{min }\sum_i^{N}[1-y_i(w·x_i+b)]_++\lambda||w||^2先令$[1-y_i(w·x_i+b)]_+=\xi_i$，则$\xi_i≥0$，第二个约束条件成立；由$[1-y_i(w·x_i+b)]_+=\xi_i$，当$1-y_i(w·x_i+b)&gt;0$时，有$y_i(w·x_i+b)=1-\xi_i$;当$1-y_i(w·x_i+b)≤0$时，$\xi_i=0$，有$y_i(w·x_i+b)≥1-\xi_i$，所以第一个约束条件成立。所以两个约束条件都满足，最优化问题可以写作 \underset{w,b}{min}\sum_{i=1}^N\xi_i+\lambda||w||^2若取$\lambda =\frac{1}{2C}$则\underset{w,b}{min} \frac{1}{C}(\frac{1}{2} ||w||^2+C\sum_{i=1}^N \xi_i)与原始最优化问题等价。 合页损失函数图像如图所示，横轴是函数间隔$y(w·x+b)$，纵轴是损失。由于函数形状像一个合页，故名合页损失函数。 图中还画出了0-1损失函数，可以认为它是一个二类分类问题的真正的损失函数，而合页损失函数是0-1损失函数的上界。由于0-1损失函数不是连续可导的，直接优化其构成的目标函数比较困难，可以认为线性支持向量机是优化由0-1损失函数的上界（合页损失函数）构成的目标函数。这时的上界损失函数又称为代理损失函数（surrogate function）。图中虚线显示的是感知机的损失函数$[-y_i(w·x_i+b)]_+$。这时当样本点$(x_i,y_i)$被正确分类时，损失是0，否则损失是$-y_i(w·x_i+b)$，相比之下，合页损失函数不仅要分类正确，而且确信度足够高时损失才是0，也就是说，合页损失函数对学习有更高的要求]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
        <tag>线性支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（12）：SVM（1）—线性可分支持向量机]]></title>
    <url>%2F2017%2F02%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[当训练数据线性可分时，通过硬间隔最大化，学习一个线性可分支持向量机。 一般地，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求分离超平面，解是唯一的。也就是它不仅将正负实例点分开，而且对最难分的实例点（离分离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。 一、线性可分支持向量机与硬间隔最大化1.1 线性可分支持向量机假设给定一个特征空间上的训练数据集 T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\}其中$x_i\in R^n,y_i\in \left\{ +1,-1 \right\} ,i=1,2,···,N$，$x_i$为第$i$个特征向量，也称为实例，$y_i$为$x_i$的类标记，当$y_i=+1$时，称$x_i$为正例；当$y_i=-1$时，称$x_i$为负例，$(x_i,y_i)$称为样本点。再假设训练数据集是线性可分的。 给定线性可分训练数据集，通过间隔最大化得到的分离超平面为 w^T·x+b=0以及相应的分类决策函数 f\left( x \right) =sign\left( w^T·x+b \right)该决策函数称为线性可分支持向量机 1.2 函数间隔与几何间隔一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面确定的情况下$|w^T·x+b|$能够相对地表示点$x$距离超平面的远近。而$w^T·x+b$的符号与类标记的符号是否一致能够表示分类是否正确，所以可用$y(w^T·x+b)$来表示分类的正确性与确信度，这就是函数间隔functional margin的概念 但是，函数间隔有一个不足之处，就是在选择分离超平面时，只要成比例地改变$w$和$b$，超平面并没有变化，而函数间隔却以同样比例变化了。因此，我们可以对分离超平面的法向量$w$加上某些约束，使得间隔确定，此时函数间隔成为几何间隔geometric margin。 对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为 \gamma _i=y_i\left( \frac{w}{||w||}·x_i+\frac{b}{||w||} \right)定义超平面$(w,b)$关于训练数据集$T$的几何间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的几何间隔之最小值，即 \gamma =\underset{i=1,···,N}{\min}\gamma _i超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的带符号的距离，当样本点被超平面正确分类时就是实例点到超平面的距离。 函数间隔与几何间隔的关系为 \gamma =\frac{\hat{\gamma}}{||w||}若$||w||=1$，那么函数间隔和几何间隔相等。如果超平面参数$w$和$b$成比例地改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变。 1.3 间隔最大化支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。几何间隔最大的分离超平面是唯一的。 间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据记性分类。即，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。 最大间隔分离超平面 接下来求一个几何间隔最大的分离超平面，即最大间隔分离超平面。具体地，可以表示为下面的约束最优化问题： \underset{w,b}{\max}\,\,\gamma s.t\,\,\,\,y_i\left( \frac{w}{||w||}·x_i+\frac{b}{||w||} \right) \geqslant \gamma \,\,,\,\,i=1,2,···,N即最大化超平面$(w,b)$关于训练数据集的几何间隔$\gamma $，约束条件表示的是超平面$(w,b)$关于每个训练样本点的几何间隔至少是$\gamma $ 根据几何间隔和函数间隔的关系，可以将此问题改写为 \underset{w,b}{\max}\ \frac{\hat{\gamma}}{||w||} s.t\ \ y_i\left( w·x_i+b \right) \geqslant \hat{\gamma}\ ,\ i=1,2,···,N函数间隔$\hat{\gamma}$的取值不影响最优化的解。函数间隔因为$w$，$b$按比例改变为$\lambda w\mathrm{，}\lambda b$而成为$\lambda \hat{\gamma}$，但是对最优化问题中的不等式约束没有影响，对目标函数的优化也没有影响，即两者等价。这样，我们可以取$ \hat{\gamma}=1$，代入后注意到最大化$\frac{1}{||w||}$和最小化$\frac{1}{2}||w||^2$是等价的，因为我们关心的并不是最优情况下目标函数的具体数值。于是就得到下面的线性可分支持向量机学习的最优化问题。 构造并求解约束最优化问题： \underset{w,b}{\min}\frac{1}{2}||w||^2 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s.t\ \ y_i\left( w·x_i+b \right) -1\geqslant 0,\ i=1,2,···,N求得最优解$w^$,$b^$ 由此得到分割超平面： w^*·x+b^*=0分类决策函数 f\left( x \right) =sign\left( w^*·x+b \right) 这其实是一个凸二次规划convex quadratic programming 问题，凸优化问题是指约束最优化问题 \underset{w}{\min}\ \ \ f\left( w \right) \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s.t\ \ \ g_i\left( w \right) \le 0\ ,\ i=1,2,···,k \ \ \ \ \ \ \ \ \ \ \ \ \ h_i\left( w \right) =0\ ,\ i=1,2,···,l\其中，目标函数$f(w)$和约束函数$g_i(w)$都是$R^n$上的连续可微的凸函数，约束函数$h_i(w)$是$R^n$上的仿射函数。当目标函数$f(w)$是二次函数且约束函数$g_i(w)$是仿射函数时，上述凸优化问题成为凸二次规划问题。 支持向量和间隔边界 在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例成为支持向量support vector。支持向量是使约束条件式等号成立的点，即 y_i\left( w·x_i+b \right) -1=0对$y_i=+1$的实例点，支持向量在超平面 H_1\mathrm{：}w·x_i+b=1对$y_i=-1$的负例点，支持向量在超平面 H_2\mathrm{：}w·x_i+b=-1 可以看到两个支撑着中间的长带的超平面，它们到中间的分离超平面的距离相等，为什么一定是相等的呢？，即我们所能得到的最大的几何间隔 $\tilde{\gamma}$ 。而“支撑”这两个超平面的必定会有一些点，试想，如果某超平面没有碰到任意一个点的话，那么我就可以进一步地扩充中间的 gap ，于是这个就不是最大的 margin 了。由于在 $n$ 维向量空间里一个点实际上是和以原点为起点，该点为终点的一个向量是等价的，所以这些“支撑”的点便叫做支持向量。 注意到$H_1$和$H_2$平行，并且没有实例点落在他们中间。在$H_1$和$H_2$之间形成一条长带，分离超平面与他们平行且位于他们中间。长带的宽度，即$H_1$与$H_2$之间的距离成为间隔margin，间隔依赖于分离超平面的法向量$w$，等于$\frac{2}{||w||}$。$H_1$和$H_2$称为间隔边界。 在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。如果移动支持向量将改变所求的解；但是如果在将俄边界以外移动其他实例点，甚至去掉这些点，则解是不会改变的。由于支持向量在确定分离超平面中起着决定性作用，所以将这种分类模型称为支持向量机。支持向量机的个数一般都很少，所以支持向量机由很少的“重要的”训练样本确定。 很显然，由于这些 supporting vector 刚好在边界上，所以它们是满足 $y(w^Tx+b)=1$ ，而对于所有不是支持向量的点，也就是在“阵地后方”的点，则显然有$y(w^Tx+b)&gt;1$ 。事实上，当最优的超平面确定下来之后，这些后方的点就完全成了路人甲了，它们可以在自己的边界后方随便飘来飘去都不会对超平面产生任何影响。这样的特性在实际中有一个最直接的好处就在于存储和计算上的优越性，例如，如果使用 100 万个点求出一个最优的超平面，其中是支持向量的有 100 个，那么我只需要记住这 100 个点的信息即可，对于后续分类也只需要利用这 100 个点而不是全部 100 万个点来做计算。 1.4 学习的对偶算法为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分支持向量机的对偶算法dual algorithm。 这样做的优点是，一是对偶问题往往更容易求解；二是引入核函数，进而推广到非线性分类问题。 首先构建拉格朗日函数，为此，对每一个不等式约束引进拉格朗日乘子$a_i≥0，i=1,2,···,N$定义拉格朗日函数： L\left( w,b,a \right) =\frac{1}{2}||w||^2-\sum_{i=1}^N{a_i(y_i\left( w·x_i+b )-1\right)}其中，$a=\left( a_1,a_2,···,a_N \right) ^T$然后我们令 θ(w)=\underset{a_i≥0}{max}L(w,b,α)容易验证，当某个约束条件不满足时，例如 $y_i(w^Tx_i+b)&lt;1$，那么我们显然有 $θ(w)=∞$ （只要令$α_i=∞$即可）。而当所有约束条件都满足时，则有 $θ(w)=\frac{1}{2}||w||_2$ ，亦即我们最初要最小化的量。因此，在要求约束条件得到满足的情况下最小化 $\frac{1}{2}||w||_2$ 实际上等价于直接最小化 $θ(w)$（当然，这里也有约束条件，就是 $α_i≥0,i=1,…,n$），因为如果约束条件没有得到满足，$θ(w)$ 会等于无穷大，自然不会是我们所要求的最小值。 具体写出来，我们现在的目标函数变成了： \underset{w,b}{min}\theta(w)=\underset{w,b}{\min}\underset{a_i≥0}{\max}L\left( w,b,a \right) =p^*这里用 $p^∗$ 表示这个问题的最优值，这个问题和我们最初的问题是等价的。不过，现在我们来把最小和最大的位置交换一下： \underset{a_i≥0}{\max}\underset{w,b}{\min}L\left( w,b,a \right) =d^∗当然，交换以后的问题不再等价于原问题，这个新问题的最优值用 $d^∗$ 来表示。并，我们有 $d^∗≤p^∗$，这在直观上也不难理解，最大值中最小的一个总也比最小值中最大的一个要大吧！总之，第二个问题的最优值$d^∗$ 在这里提供了一个第一个问题的最优值$p^∗$ 的一个下界，在满足某些条件的情况下，这两者相等，这个时候我们就可以通过求解第二个问题来间接地求解第一个问题。具体来说，就是要满足 KKT 条件，这里暂且先略过不说，直接给结论：我们这里的问题是满足 KKT 条件的，因此现在我们便转化为求解第二个问题。 为了得到对偶问题的解，需要先求$L(w,b,a)$对$w,b$的极小，再求对$a$的极大。 求$\underset{w,b}{\min}L\left( w,b,a \right)$将拉格朗日函数$L(w,b,a)$分别对$w,b$求偏导数并令其为$0$。 \nabla _wL\left( w,b,a \right) =w-\sum_{i=1}^N{a_iy_ix_i}=0 \nabla _bL\left( w,b,a \right) =\sum_{i=1}^N{a_iy_i}=0得到 w=\sum_{i=1}^N{a_iy_ix_i} \sum_{i=1}^N{a_iy_i=0}将其代入拉格朗日函数，得到 L\left( w,b,a \right) =\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) -\sum_{i=1}^N{a_iy_i\left( \left( \sum_{j=1}^N{a_jy_jx_j} \right) ·x_i+b \right) +\sum_{i=1}^N{a_i}}}} =-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) +\sum_{i=1}^N{a_i}}} 求$\underset{w,b}{\min}L\left( w,b,a \right)$对$a$的极大，即是对偶问题 \underset{a}{\max} \sum_{i=1}^N{a_i}-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right)}} s.t.\ \sum_{i=1}^N{a_iy_i=0} a_i\geqslant 0,\ \ i=1,2,···,N 将目标函数由求极大转换为极小，就得到下面与之等价的对偶最优化问题。 \underset{a}{\min}\ \ \ \frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right)}}-\sum_{i=1}^N{a_i} s.t.\ \sum_{i=1}^N{a_iy_i=0} a_i\geqslant 0,\ \ i=1,2,···,N让我们先来看看推导过程中得到的一些有趣的形式。首先就是关于我们的 hyper plane ，对于一个数据点 x 进行分类，实际上是通过把 x 带入到 $f(x)=w^Tx+b$ 算出结果然后根据其正负号来进行类别划分的。而前面的推导中我们得到$f(w)=∑^n_{i=1}α_iy_ix_i$ ，因此 f(x)=(∑^n_{i=1}α_iy_ix_i)^Tx+b=∑^n_{i=1}α_iy_i ⟨x_i,x⟩+b这里的形式的有趣之处在于，对于新点 x 的预测，只需要计算它与训练数据点的内积即可（这里 ⟨⋅,⋅⟩ 表示向量内积），这一点至关重要，是之后使用 Kernel 进行非线性推广的基本前提。此外，所谓 Supporting Vector 也在这里显示出来——事实上，所有非 Supporting Vector 所对应的系数 α 都是等于零的，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据即可。 为什么非支持向量对应的 α 等于零呢？直观上来理解的话，就是这些“后方”的点——正如我们之前分析过的一样，对超平面是没有影响的，由于分类完全有超平面决定，所以这些无关的点并不会参与分类问题的计算，因而也就不会产生任何影响了。 这个结论也可由刚才的推导中得出，回忆一下我们刚才通过 Lagrange multiplier 得到的目标函数： \underset{a_i≥0}{max}L(w,b,a)=\underset{a_i≥0}{max}\frac{1}{2}||w||^2-\sum_{i=1}^na_i(y_i(w^Tx_i+b)-1)注意到如果 $x_i$ 是支持向量的话，上式中$(y_i(w^Tx_i+b)-1)$部分是等于 0 的（因为支持向量的 functional margin 等于 1 ），而对于非支持向量来说，函数间隔会大于 1 ，因此这个部分是大于零的，而 $a_i$ 又是非负的，为了满足最大化，$α_i$ 必须等于 0 。 线性可分支持向量机学习算法 输入：线性可分训练数据集$T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\} $,其中,$x_i\in R^n,y_i\in \left\{ +1,-1 \right\} ,i=1,2,···,N$ 输出：最大间隔分离超平面和分类决策函数 步骤如下 构造并求解约束最优化问题 \underset{a}{\min}\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) -\sum_{i=1}^N{a_i}}} s.t.\ \ \ \ \sum_{i=1}^N{a_iy_j=0} a_i\geqslant 0,\ i=1,2,···,N求得最优解$a^=(a_1^,a_2^,···,a_N^)$ 计算 w^*=\sum_i^{}{a_{i}^{*}y_ix_i}并选择$a^$的一个正分量$a_j^&gt;0$，计算 b^*=y_j-\sum_{i=1}^N{a_{i}^{*}y_i\left( x_i·x_j \right)} 求得分离超平面 w^*·x+b^*=0分类决策函数： f(x)=sign(w^*·x+b^*) 在线性可分支持向量机中，$w^和b^$只依赖于训练数据中对应于$a_i^&gt;0$的样本点$x_i,y_i$,而其他样本点对$w^和b^$没有影响。我们将训练数据中对应于$a_i^&gt;0$的实例点$x_i\in R^n$称为支持向量。 对于线性可分问题，上述线性可分支持向量机的学习（硬间隔最大化）算法是完美的。但是，训练数据集线性可分是理想的情形。在现实问题中，训练数据集往往是线性不可分的，即在样本中出现噪声或特异点。此时，有更一般的学习算法。 1.5 KKT条件对于包含等式和不等式约束的一般优化问题KKT条件（$x^*$是最优解的必要条件）为 上式便称为不等式约束优化问题的KKT（Karush-Kuhn-Tucker）条件.$\mu _{j}$称为KKT乘子，当约束起作用时$\mu_1&gt;0,g_1(x)=0$，当约束不起作用时$\mu_1=0,g_1(x)&lt;0$ 更细致的推导可以看这篇文章：浅谈最优化问题的KKT条件 证明可以将线性可分支持向量机的原始问题和对偶问题等同起来的充分必要条件KKT条件，即得 \nabla _wL\left( w^*,b^*,a^* \right) =w^*-\sum_{i=1}^N{a_iy_ix_i=0} \nabla _bL\left( w^*,b^*,a^* \right) =-\sum_{i=1}^N{a_{i}^{*}y_i=0} a_{i}^{*}\left( y_i\left( w^*·x_i+b^* \right) -1 \right) =0\ ,\ i=1,2,···,N y_i\left( w^*·x_i+b^* \right) -1\geqslant 0\ ,\ 1,2,···,N a_{i}^{*}\geqslant 0\ ,\ i=1,2,···,N由此得 w^*=\sum_i^{}{a_{i}^{*}y_ix_i}其中至少有一个$a_j^&gt;0$(反证法，假设$a^=0$，由上可知$w^=0$，而$w^=0$不是原始最优化问题的解，产生矛盾)，对此$j$有 y_j\left( w^*·x_j+b^* \right) -1=0 a_{j}^{*}y_jx_j·x_i+b^*=1/y_j=y_j b^*=y_j-\sum_{i=1}^N{a_{i}^{*}y_i\left( x_i·x_j \right)}综上所述，对于给定的线性可分训练数据集，可以首先求对偶问题的解$a^$;再利用求得原始问题的解$w^,b^*$,从而得到分离超平面及分类决策函数。这种算法称为线性可分支持向量机的对偶学习算法，是线性可分支持向量机学习的基本算法。]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（11）：聚类（4）—密度最大值聚类]]></title>
    <url>%2F2017%2F02%2F24%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%884%EF%BC%89%2F</url>
    <content type="text"><![CDATA[五、密度最大值聚类5.1 引言2014年6月，Alex Rodriguez和Alessandro Laio在$Science$上发表了一篇名为《Clustering by fast search and find of density peaks》的文章，提供了一种简洁而优美的聚类算法，是一种基于密度的聚类方法，可以识别各种形状的类簇，并且参数很容易确定。它克服了DBSCAN中不同类的密度差别大、邻域范围难以设定的问题，鲁棒性强。在文章中提出的聚类方法DPCA算法（Desity Peaks Clustering Algorithm）基于这样一种假设：对于一个数据集，聚类中心被一些低局部密度的数据点包围，而且这些低局部密度点距离其他有高局部密度的点的距离都比较大。 5.2 若干概念 局部密度$\rho_i$的定义为：\rho_i=\sum_j{\chi\left(d_{ij}-d_c\right)}其中， \chi\left(x\right)=\left\{\begin{array}{l} 1\ if\ x\rho_i}{\min}\left(d_{ij}\right)即在局部密度高于对象$i$的所有对象中，到对象$i$最近的距离。而极端地，对于密度最大的那个对象，我们设置$\delta=max(d_{ij})$；只有那些密度是局部或者全局最大的点才会有远大于正常值的高局部密度点距离。 5.3 聚类过程这个聚类实例摘自作者的PPT讲演，在一个二维空间中对数据进行聚类，具体步骤如下： 1、首先计算每一个点的局部密度$\rho_i$，如图中，$\rho_1=7,\rho_8=5,\rho_{10}=4$ 2、然后对于每一个点$i$计算在局部密度高于对象$i$的所有对象中，到对象$i$最近的距离$\delta$ 3、对每一个点，绘制出局部密度与高局部密度点距离的关系散点图 4、图上的异常点即为簇中心。如图所示，1和10两点的局部密度和高局部密度距离都很大，将其作为簇中心。 5、将其他的点分配给距离其最近的有着更高的局部密度的簇。（Assign each point to the same cluster of its nearest neighbor of higher density）左图是所有点在二维空间的分布，右图是以$\rho$为横坐标，以$\delta$为纵坐标绘制的决策图。容易发现，1和10两个点的$\rho_i$和$\delta_i$都比较大，作为簇的中心点。26、27、28三个点的$\delta$也比较大，但是$\rho比较小$，所以是异常点。 5.4 一些关键点 簇中心的识别 那些有着比较大的局部密度$\rho_i$和很大的高局部密度$\delta_i$的点被认为是簇的中心；而高局部密度距离$\delta_i$较大但局部密度$\rho_i$较小的点是异常点；确定簇中心之后，其他点按照距离已知簇的中心最近进行分类，也可以按照密度可达的方法进行分类。但是，这里我们在确定聚类中心时，没有定量地分析，而是通过肉眼观察，包含很多的主观因素。在上图中可以分明地用肉眼判断聚类中心，但是有些情况下无法用肉眼来判断。不过，对于那些在决策图中无法用肉眼判断出聚类中心的情形，作者在文中给出了一种确定聚类中心个数的提醒：计算一个将$\rho$值和$\delta$值综合考虑的量 \gamma_i=\rho_i\delta_i显然$\gamma$值越大，越有可能是聚类中心。因此，只需对其降序排列，然后从前往后截取若干个数据点作为聚类中心就可以了。我们把排序后的$\gamma$在坐标平面（下标为横轴，$\gamma$值为纵轴）画出来，由图可见，非聚类中心的$gamma$值比较平滑，而从非聚类中心过渡到聚类中心时$\gamma$有一个明显的跳跃，这个跳跃用肉眼或数值检测应该可以判断出来。作者在文末还提到，对于人工随机生成的数据集，$\gamma$的分布还满足幂次定律，即$log\gamma$，且斜率依赖于数据维度。 截断距离$d_c$的选择 一种推荐做法是选择$d_c$，使得平均每个点的邻居数为所有点的1%~2%。参数$d_c$的选取，从某种意义上决定这聚类算法的成败，取得太大或者太小都不行：如果取得太大，将使得每个数据点的$\rho$值都很大以致区分度不高，极端情况是取$d_c&gt;d_{max}$，则所有的数据点都归属于一个Cluster了；如果$d_c$取得太小，同一个Cluster中就可能被拆分成多个，极端情况是$d_c&lt;d_{min}$，则每个数据点都单独称为一个Cluster。作者将比例锁定在数据量的1%~2%，也是基于肉感数据集的经验值。 选定簇中心之后 在聚类分析中, 通常需要确定每个点划分给某个类簇的可靠性. 在该算法中, 可以首先为每个类簇定义一个边界区域(border region), 亦即划分给该类簇但是距离其他类簇的点的距离小于$d_c$的点(这个区域由这样的数据点构成：它们本身属于该Cluster，但在与其距离不超过$d_c$的范围内，存在属于其他Cluster的数据点). 然后为每个类簇找到其边界区域的局部密度最大的点, 令其局部密度为$\rho_h$. 该类簇中所有局部密度大于$\rho_h$的点被认为是类簇核心的一部分(亦即将该点划分给该类簇的可靠性很大), 其余的点被认为是该类簇的光晕(halo), 亦即可以认为是噪音. 图例如下A图为生成数据的概率分布，B、C二图为分别从该分布中生成了4000，1000个点。D,E分别是B,C两组数据的决策图（decision tree），可以看到两组数据都只有五个点有比较大的$\rho_i$和很大的$\delta_i$，这些点作为类簇的中心，在确定了类簇的中心之后，每个点被划分到各个类簇（彩色点），或者划分到类簇光晕（黑色点），F图展示的是随着抽样点数量的增多，聚类的错误率在逐渐下降，说明该算法是鲁棒的。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（11）：聚类（3）—DBSCAN]]></title>
    <url>%2F2017%2F02%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[四、DBSCAN算法4.1 密度聚类方法密度聚类方法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。这类算法能克服基于距离的算法只能发现“类圆”（凸）的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。其代表算法为DBSCAN算法和密度最大值算法。 4.2 DBSCAN算法原理DBCSAN（Density-Based Spatial Clustering of Applications with Noise）是一个比较有代表性的基于密度的聚类算法。与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在有“噪声”的数据中发现任意形状的聚类。 4.3 若干概念 对象的$\varepsilon -$领域：给定对象在半径$\varepsilon$内的区域 核心对象：对于给定的数目$m$，如果一个对象的$\varepsilon -$领域至少包含$m$个对象，则称该对象为核心对象。 直接密度可达：给定一个对象集合$D$，如果p是在q的$\varepsilon -$领域内，而q是一个核心对象，我们说对象p从对象q出发时直接密度可达的。如图$\varepsilon =1,m=5$，q是一个核心对象，从对象q出发到对象p是直接密度可达的。 密度可达：如果存在一个对象链$p_1p_2···p_n$，$p_1=q,p_n=p$，对$p_i\in D,(1≤i≤n)$,$p_{i+1}$是从$p_i$关于$\varepsilon$和$m$直接密度可达的，则对象$p$是从对象$q$和$m$密度可达的。 密度相连：如果对象集合$D$中存在一个对象$O$，使得对$p$和$q$是从$O$关于$\varepsilon $和$m$密度可达的，那么对象$p$和$q$是关于$\varepsilon $和$m$密度相连的。 簇：一个基于密度的簇是最大的密度相连对象的集合。 噪声：不包含在任何簇中的对象称为噪声。 4.4 算法步骤下面这张图来自WIKI，图上有若干个点，其中标出了A、B、C、N这四个点，据此来说明这个算法的步骤： 1、首先随机选择A点为算法实施的切入点，我们将$\varepsilon $设置为图中圆的半径，对象个数$m（minPts）$设定为4。这里我们看到，A点的$\varepsilon - $领域包含4个对象（自己也包含在内），大于等于$m(minPts)$，则创建A作为核心对象的新簇，簇内其他点都（暂时）标记为边缘点。 2、然后在标记的边缘点中选取一个重复上一步，寻找并合并核心对象直接密度可达的对象。对暂时标记为边缘点反复递归上述算法，直至没有新的点可以更新簇时，算法结束。这样就形成了一个以A为起始的一个聚类，为图中红色的中心点和黄色的边缘点 3、如果还有Points未处理，再次新产生一个类别来重新启动这个算法过程。遍历所有数据，如果有点既不是边缘点也不是中心点，将其标记为噪音。 从上述算法可知： 每个簇至少包含一个核心对象； 非核心对象可以是簇的一部分，构成了簇的边缘（edge）； 包含过少对象的簇被认为是噪声； 4.5 总结 优点 无需确定聚类个数：DBSCAN does not require one to specify the number of clusters in the data a priori, as opposed to k-means. 可以发现任意形状的聚类：DBSCAN can find arbitrarily shaped clusters. It can even find a cluster completely surrounded by (but not connected to) a different cluster. Due to the MinPts parameter, the so-called single-link effect (different clusters being connected by a thin line of points) is reduced. 对噪声具有鲁棒性，可有效处理噪声：DBSCAN has a notion of noise, and is robust to outliers. 只需两个参数，对数据输入顺序不敏感：DBSCAN requires just two parameters and is mostly insensitive to the ordering of the points in the database. (However, points sitting on the edge of two different clusters might swap cluster membership if the ordering of the points is changed, and the cluster assignment is unique only up to isomorphism.) 加快区查询：DBSCAN is designed for use with databases that can accelerate region queries, e.g. using an R* tree. 参数可由领域专家设置：The parameters minPts and ε can be set by a domain expert, if the data is well understood. 缺点 边界点不完全确定性：DBSCAN is not entirely deterministic: border points that are reachable from more than one cluster can be part of either cluster, depending on the order the data is processed. Fortunately, this situation does not arise often, and has little impact on the clustering result[citation needed]: both on core points and noise points, DBSCAN is deterministic. DBSCAN*[4] is a variation that treats border points as noise, and this way achieves a fully deterministic result as well as a more consistent statistical interpretation of density-connected components. 维数灾导致欧几里得距离度量失效：The quality of DBSCAN depends on the distance measure used in the function regionQuery(P,ε). The most common distance metric used is Euclidean distance. Especially for high-dimensional data, this metric can be rendered almost useless due to the so-called “Curse of dimensionality”, making it difficult to find an appropriate value for ε. This effect, however, is also present in any other algorithm based on Euclidean distance. 不能处理密度差异过大（密度不均匀）的聚类（会导致参数无法适用于所有聚类）：DBSCAN cannot cluster data sets well with large differences in densities, since the minPts-ε combination cannot then be chosen appropriately for all clusters. 参数选择在数据与规模不能很好理解的情况下，很难选择，若选取不当，聚类质量下降： If the data and scale are not well understood, choosing a meaningful distance threshold ε can be difficult.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（11）：聚类（2）—Kmeans]]></title>
    <url>%2F2017%2F02%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[三、K-Means算法3.1 原理K-Means算法属于基于划分的聚类算法，对N 维欧氏空间中的点进行聚类，是一种最简单的无监督学习方法。它通过迭代来实现，其基本思想是：每次确定K个类别中心，然后将各个结点归属到与之距离最近的中心点所在的Cluster，然后将类别中心更新为属于各Cluster的所有样本的均值，反复迭代，直至类别中心不再发生变化或变化小于某阈值。 3.2 基本假设K-Means聚类需要对数据进行一个基本假设：对于每一个 cluster ，我们可以选出一个中心点 (center) ，使得该 cluster 中的所有的点到该中心点的距离小于到其他 cluster 的中心的距离。虽然实际情况中得到的数据并不能保证总是满足这样的约束，但这通常已经是我们所能达到的最好的结果，而那些误差通常是固有存在的或者问题本身的不可分性造成的。例如下图所示的两个高斯分布，从两个分布中随机地抽取一些数据点出来，混杂到一起，现在要让你将这些混杂在一起的数据点按照它们被生成的那个分布分开来：由于这两个分布本身有很大一部分重叠在一起了，例如，对于数据点 2.5 来说，它由两个分布产生的概率都是相等的，你所做的只能是一个猜测；稍微好一点的情况是 2 ，通常我们会将它归类为左边的那个分布，因为概率大一些，然而此时它由右边的分布生成的概率仍然是比较大的，我们仍然有不小的几率会猜错。而整个阴影部分是我们所能达到的最小的猜错的概率，这来自于问题本身的不可分性，无法避免。因此，我们将 k-means 所依赖的这个假设看作是合理的。 3.3 算法步骤假定输入样本为$S=x_1,x_2,···,x_n$，则算法步骤为： 1、选择初始的K个类别中心$\mu_1,\mu_2,···,\mu_k$。这个过程通常是针对具体地问题有一些启发式的选取方法，或者大多数情况下采用随机选取的办法。因为K-Means并不能保证全局最优，而是否能收敛到全局最优解其实和初值的选取有很大的关系，所以有时候我们会多次选取初值跑一个K-Means，并取其中最好的一次结果。 2、对于每个样本$x_i$，将其标记为距离类别中心最近的类别，即：label_i=arg\underset{1\le j\le k}{\min}||x_i-\mu_j|| 3、将每个类别中心更新为隶属于该类别的所有样本的均值 \mu_j=\frac{1}{|c_j|}\sum_{i\in c_j}{x_i} 4、重复前两步，直到类别中心的变化小于某阈值或者达到最大迭代次数 3.4 理论分析基于上述的假设，我们导出K-Means所要优化的目标函数：设我们一共有N个数据点需要分为K个Cluster，K-Means需要最小化的损失函数为： J=\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^K{r_{ij}||x_i-\mu_j||^2}}这个函数，其中$r_{ij}$在数据点$n$被归类到$Cluster(j) $的时候为1，否则为0.直接寻找$r_{ij}$和$\mu_j$来最小化$J$并不容易，不过我们可以通过反复迭代以下两步的方法来进行： 1、先固定$\mu_j$，选择最优的$r_{ij}$，很容易看出，只要将数据点归类到离它最近的那个中心就能保证$J$最小，通俗来讲，因为每个样本点都有一个$r_{ij}$，不是0就是1，那么我们要想让$J$最小，就要保证当一个样本的$r_{ij=1}$时，与类别中心距离的平方和达到最小。这一步即 2、然后固定$r_{ij}$，再求最优的$\mu_j$。将$J$对$\mu_k$求导并令导数等于零，即令 \frac{\partial J}{\partial\mu_j}=\sum_{i=1}^{N_j}{r_{ij}\left(x_i-\mu_j\right)}=0很容易得到$J$最小的时候$\mu_j$应该满足 \mu_j=\frac{\sum_i{r_{ij}x_i}}{\sum_i{r_{ij}}} $\mu_j$的值是所有$Cluster(j)$中的数据点的平均值。由于每一次迭代都是取到$J$的最小值，因此$J$智慧不断地减小或者保持不变，而不会增加，这保证了K-Means最终或到达一个极小值。虽然K-Means并不能保证总是得到全局最优解，但是对于这样的问题，像K-Means这样复杂度的算法，这样的结果已经是很不错了。 3.5 算法演练下面看一个来自WIKI的实例 1、随机生成三个初始的中心点（这个中心点不一定是样本点），即图中红、绿、蓝三个小圈； 2、计算每个样本点与这三个中心店的距离，并将它们归属到离得最近的中心点对应的Cluster。此时图中分成了三个簇，分别是红色、绿色、蓝色部分； 3、重新分别计算三个簇中所有样本点的类别中心，指定为新的类别中心。此时红色、绿色、蓝色类的中点都发生了迁移。 4、反复迭代第2步和第3步，直至收敛。 3.6 总结 优点： 是解决聚类问题的一种经典算法，简单、快速 对处理大数据集，该算法保持可伸缩性和高效率 当簇近似为高斯分布时，它的效果较好 缺点 在簇的平均值可被定义的情况下才能使用，可能不适用于某些应用 必须事先给出K，而且对初值敏感，对于不同的初始值，结果可能不同 只能发现球状Cluster，不适合于发现非凸形状的簇或者大小差别很大的簇 对噪声和孤立点数据敏感，如簇中含有异常点，将导致均值偏离严重。因为均值体现的是数据集的整体特征，容易掩盖数据本身的特性。比如数组1，2，3，4，100的均值为22，显然距离“大多数”数据1、2、3、4比较远，如果改成数组的中位数3，在该实例中更为稳妥，这种聚类也叫作K-mediods聚类]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（11）：聚类（1）—简介]]></title>
    <url>%2F2017%2F02%2F20%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、引言聚类（Clustering）算法就是对大量未知标注的数据集，按照数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。聚类是一种无监督算法。给定一个有$N$个对象的数据集，构造数据的$K$个簇，$k≤n$，同时满足，每个簇至少包含一个对象，每一个对象属于且仅属于一个簇，将满足上述条件的$K$个簇称作一个合理划分。它的主要思想是对于给定的类别数目$K$，首先给出初始划分，通过迭代改变样本和簇的隶属关系，使得每一次改进之后的划分方案都较前一次好。 聚类算法主要包括以下五类： 基于分层的聚类（hierarchical methods） 这种方法对给定的数据集进行逐层，直到某种条件满足为止。具体可分为合并型的“自下而上”和分裂型的“自下而上”两种方案。如在“自下而上”方案中，初始时每一个数据记录都组成一个单独的组，在接下来的迭代中，它把那些相互邻近的组合并成一个组，直到所有的记录组成一个分组或者某个条件满足为止。代表算法有：BIRCH算法（1996）、CURE算法、CHAMELEON算法等。 基于划分的聚类（partitioning methods） 给定一个有N个记录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K&lt;N,而且这K个分组满足下列条件：（1）每一个分组至少包含一个数据记录；（2）每一个数据记录属于且仅属于一个分组（咋某些模糊聚类算法中可以放宽条件）。对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准是：同一分组中的记录越近越好，而不同分组中的记录越远越好。使用这个基本思想的算法有：K-means算法、K-medoids算法、CLARANS算法 基于密度的聚类（density-based methods） 基于密度的方法和其他方法的一个根本区别是：它不是基于各种各样的距离的，而是基于魔都的，这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。这个方法的指导思想为：只要一个区域的点的密度大过某个阈值，就把它加到与之相近的聚类中去，代表算法有：DBSCAN（Density-Based Spatial Clustering of Applic with Noise）算法（1996）、OPTICS（Ordering Points to Identify Clustering Structure）算法（1999）、DENCLUE算法（1998）、WaveCluster算法（1998，具有O（N）时间复杂性，但只适用于低维数据） 基于网格的聚类（grid-based methods） 这种方法首先将数据空间划分成为有限个单元（cell）的网络结构，所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关，它只与把数据空间分成多少个单元有关。代表算法有：STING（Statistical Information Grid）、CLIQUE（Clustering In Quest）算法（1998）、WaveCluster算法。其中STRING算法把数据空间层次地划分为单元格，依赖于存储在网格单元中的统计信息进行聚类；CLIQUE算法结合了密度和网格的方法。 基于模型的聚类（model-based methods） 基于模型的方法给每一个聚类假定一个模型，然后去寻找能够很好地满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。通常有两种尝试方向：统计的方案和神经网络的方案。 二、相似度、距离计算方法 给定$n$维空间$R^n$中的两个向量$X=(x_1,x_2,···,x_n)^T$和$y=(y_1,y_2,···,y_n)^T$，$x,y$之间的距离可以反映两者的相似程度，一般采用$L_p$距离 dist\left( X,Y \right) =\left( \sum_{i=1}^n{|x_i-y_i|^p} \right) ^{\frac{1}{p}}其中$p≥1$，也称为闵可夫斯基距离（Minkowski）距离。常用的$p$为$1,2,+\infty$，此时相应的距离公式分别为 1.当$p=1$时，称为曼哈顿距离（Manhattan distance），改名字的由来起源于在纽约市去测量街道之间的距离就是由人不行的步数来确定的。 d\left(x,y\right)=\sum_{i=1}^n{|x_i-y_i|} 当$p=2$时，称为欧几里得距离（Euclidean distance） d\left(x,y\right)=\left(\sum_{i=1}^n{\left(x_i-y_i\right)^2}\right)^{\frac{1}{2}} 当$p=+\infty$时，称为最大值距离（Maximum distance） d\left(x,y\right)=\underset{1\le i\le n}{\max}|x_i-y_i| 杰卡德相似系数（Jaccard） J\left( A,B \right) =\frac{|A\cap B|}{|A\cup B|} 余弦相似度（Cosine Similarity） \cos\left(\theta\right)=\frac{x^Ty}{|x|·|y|} pearson相似系数 \rho _{XY}=\frac{cov\left( X,Y \right)}{\sigma _x\sigma _y}=\frac{E\left[ \left( x-u_x \right) \left( y-u_y \right) \right]}{\sigma _x\sigma _y} 相对熵（K-L）距离 D\left(p||q\right)=\sum_x{p\left(x\right)\log\frac{p\left(x\right)}{q\left(x\right)}}=E_{p\left(x\right)}\log\frac{p\left(x\right)}{q\left(x\right)} Hellinger距离 D_a\left(p||q\right)=\frac{2}{1-a^2}\left(1-\int{p\left(x\right)^{\frac{1+a}{2}}q\left(x\right)^{\frac{1-a}{2}}}dx\right) 余弦相似度与pearson相似系数的比较 $n$维向量$x$和$y$的夹角记作$\theta$，根据余弦定理，其余弦值为： \cos\left(\theta\right)=\frac{x^Ty}{|x|·|y|}=\frac{\sum_{i=1}^n{x_iy_i}}{\sqrt{\sum_{i=1}^n{x_{i}^{2}}}·\sqrt{\sum_{i=1}^n{y_{i}^{2}}}}这两个向量的相关系数是： \rho_{XY}=\frac{cov\left(X,Y\right)}{\sigma_x\sigma_y}=\frac{E\left[\left(x-u_x\right)\left(y-u_y\right)\right]}{\sigma_x\sigma_y} =\frac{\sum_{i=1}^n{\left(x_i-\mu_x\right)\left(y_i-\mu_y\right)}}{\sqrt{\sum_{i=1}^n{\left(x_i-\mu_x\right)^2}}\sqrt{\sum_{i=1}^n{\left(y_i-\mu_y\right)^2}}}相关系数即将$x,y$坐标向量各自平移到原点后的夹角余弦。这即揭示了为何文档间求距离使用夹角余弦，因为这个物理量表征了文档去均值化后的随机向量间的相关系数。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（10）：朴素贝叶斯]]></title>
    <url>%2F2017%2F02%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8810%EF%BC%89%EF%BC%9A%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯（Naive Bayes）是基于贝叶斯定理与特征条件假设的分类方法。 对于给定的训练数据集，首先基于特征条件独立假设学习输入、输出的联合分布；然后基于此模型，对给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$。 朴素贝叶斯实现简单，学习与预测的效率都很高，是一种常用的方法。 一、朴素贝叶斯的学习与分类1.1贝叶斯定理先看什么是条件概率 $P(A|B$表示事件$B已经发生的前提下，事件$A发生的概率，叫做事件$B$发生下事件$A$的条件概率。其基本求解公式为 P\left(A|B\right)=\frac{P\left(AB\right)}{P\left(B\right)}贝叶斯定理便是基于条件概率，通过$P(A|B)$来求$P(B|A)$： P\left(B|A\right)=\frac{P\left(A|B\right)·P\left(B\right)}{P\left(A\right)}顺便提一下，上式中的分母，可以根据全概率公式分解为： P\left(A\right)=\sum_{i=1}^n{P\left(B_i\right)P\left(A|B_i\right)}1.2 特征条件独立假设这一部分开始朴素贝叶斯的理论推导，从中你会深刻地理解什么是特征条件独立假设。 给定训练数据集$(X,Y)$，其中每个样本$X$都包括$n$维特征，即$x=(x_1,x_2,···,x_n)$，类标记集合含有$K$种类别，即$y=(y_1,y_2,···,y_k)$ 如果现在来了一个新样本$x$我们要怎么判断它的类别?从概率的角度来看，这个问题就是给定$x$，它属于哪个类别的概率更大。那么问题就转化为求解$P(y_1|x),P(y_2|x),P(y_k|x)$中最大的那个，即求后验概率最大的输出：$arg\underset{y_k}{\max}P\left(y_k|x\right)$ 那$P(y_k|x)$怎么求解？答案就是贝叶斯定理： P\left(y_k|x\right)=\frac{P\left(x|y_k\right)·P\left(y_k\right)}{P\left(x\right)}根据全概率公式，可以进一步分解上式中的分母： P\left(y_k|x\right)=\frac{P\left(x|y_k\right)·P\left(y_k\right)}{\sum_{i=1}^n{P\left(x|y_k\right)P\left(y_k\right)}} （公式1）先不管分母，分子中的$P(y_k)$是先验概率，根据训练集就可以简单地计算出来，而条件概率$P(x|y_k)=P(x_1,x_2,···,x_n|y_k)$，它的参数规模是指数数量级别的，假设第$i$维特征$x_i$可取值的个数有$S_i$个，类别取值个数为$k$个，那么参数个数为$k\prod_{j=1}^n{S_j}$ 这显然是不可行的。针对这个问题，朴素贝叶斯算法对条件概率分布做了独立性的假设，通俗地讲就是说假设各个维度的特征$x_1,x_2,···,x_n$互相独立，由于这是一个较强的假设，朴素贝叶斯算法也因此得名。在这个假设的前提上，条件概率可以转化为： P\left(x|y_i\right)=P\left(x_1,x_2,···,x_n|y_i\right)=\prod_{i=1}^n{P\left(x_i|y_i\right)} （公式2）这样参数规模就降到了$\sum_{i=1}^n{S_ik}$ 以上就是针对条件概率所作出的特征条件独立性假设，至此，先验概率$P(y_k)$和条件概率$P(x|y_k)$的求解问题就都解决了，那么我们是不是可以求解我们所需要的后验概率$P(y_k|x)$了 答案是肯定的。我们继续上面关于$P(y_k|x)$的推导，将公式2代入公式1中得到： P\left(y_k|x\right)=\frac{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}{\sum_k{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}}于是朴素贝叶斯分类器可表示为： f\left(x\right)=arg\underset{y_k}{\max}P\left(y_k|x\right)=arg\underset{y_k}{\max}\frac{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}{\sum_k{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}}因为对于所有的$y_k$，上式中的分母的值都是一样的（为什么？注意到全加符号就容易理解了），所以可以忽略分母部分，朴素贝叶斯分裂期最终表示为： f\left(x\right)=arg\underset{y_k}{\max}P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}二、朴素贝叶斯法的参数估计2.1 极大似然估计根据上述，可知朴素贝叶斯要学习的东西就是$P(Y=c_k)$和$P(X^{j}=a_{jl}|Y=c_k)$，可以应用极大似然估计法估计相应的概率（简单讲，就是用样本来推断模型的参数，或者说是使得似然函数最大的参数）。 先验概率$P(Y=c_k)$的极大似然估计是 P\left(Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(y_i=c_k\right)}}{N},\,\,k=1,2,···,K也就是用样本中$c_k$的出现次数除以样本容量。 推导如下： 设第$j$个特征$x^{(j)}$可能取值的集合为${a_{j1},a_{j2},···,a_{jl}}$，条件概率$P(X^{j}=a_{jl}|Y=c_k)$的极大似然估计是： P\left(X^{\left(j\right)}=a_{jl}|Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(x_{i}^{\left(j\right)}=a_{jl},y_{i=}c_k\right)}}{\sum_{i=1}^N{I\left(y_i=c_k\right)}}式中，$x_i^{j}$是第$i$个样本的第$j$个特征。 例题如下： 2.2 贝叶斯估计极大似然估计有一个隐患，假设训练数据中没有出现某种参数与类别的组合怎么办？比如上例中当$Y=1$对应的$X^{(1)}$的取值只有$1$和$2$。这样可能会出现所要估计的概率值为0的情况，但是这不代表真实数据中就没有这样的组合。这时会影响到后验概率的计算结果，使分类产生偏差。解决办法是贝叶斯估计。 条件概率的贝叶斯估计： P_{\lambda}\left(X^{\left(j\right)}=a_{jl}\parallel Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(x_{i}^{\left(j\right)}=a_{jl},y_{i=}c_k\right)}+\lambda}{\sum_{i=1}^N{I\left(y_i=c_k\right)}+S_j\lambda}其中$\lambda≥0$，$S_j$表示$x_j$可能取值的中数。分子和分母分别比极大似然估计多了一点东西，其意义为在随机变量各个取值的频数上赋予一个正数$\lambda≥0$。当$\lambda=0$时就是极大似然估计。常取$\lambda=1$，这时称为拉普拉斯平滑。 先验概率的贝叶斯估计： P_{\lambda}\left(Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(y_i=c_k\right)}+\lambda}{N+K\lambda}例题如下： 三、python代码实现3.1 朴素贝叶斯文档分类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129# -*- coding: utf-8 -*-"""Created on 下午5:28 22 03 2017bayes algorithm: classify a words as good or bad [text classify]@author: plushunter"""from numpy import *class Naive_Bayes: def __init__(self): self._creteria = "NB" #创建不重复词集 def _creatVocabList(self,dataSet): vocabSet = set([]) # 创建一个空的SET for document in dataSet: vocabSet = vocabSet | set(document) # 并集 return list(vocabSet) # 返回不重复词表（SET的特性） #文档词集向量模型 def _setOfWordToVec(self,vocabList, inputSet): """ 功能:给定一行词向量inputSet，将其映射至词库向量vocabList，出现则标记为1，否则标记为0. """ returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 return returnVec #文档词袋模型 def _bagOfsetOfWordToVec(self,vocabList, inputSet): """ 功能：对每行词使用第二种统计策略，统计单个词的个数，然后映射到此库中 输出：一个n维向量，n为词库的长度，每个取值为单词出现的次数 """ returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] += 1 #更新此处代码 return returnVec def _trainNB0(self,trainMatrix, trainCategory): """ 输入：训练词矩阵trainMatrix与类别标签trainCategory,格式为Numpy矩阵格式 功能：计算条件概率p0Vect、p1Vect和类标签概率pAbusive """ numTrainDocs = len(trainMatrix)#样本个数 numWords = len(trainMatrix[0])#特征个数，此处为词库长度 pAbusive = sum(trainCategory) / float(numTrainDocs)#计算负样本出现概率（先验概率） p0Num = ones(numWords)#初始词的出现次数为1，以防条件概率为0，影响结果 p1Num = ones(numWords)#同上 p0Denom = 2.0#类标记为2，使用拉普拉斯平滑法, p1Denom = 2.0 #按类标记进行聚合各个词向量 for i in range(numTrainDocs): if trainCategory[i] == 0: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) else: p1Num += trainMatrix[i] p1Denom += sum(trainMatrix[i]) p1Vect = log(p1Num / p1Denom)#计算给定类标记下，词库中出现某个单词的概率 p0Vect = log(p0Num / p0Denom)#取log对数，防止条件概率乘积过小而发生下溢 return p0Vect, p1Vect, pAbusive def _classifyNB(self,vec2Classify, p0Vec, p1Vec, pClass1): """ 该算法包含四个输入: vec2Classify表示待分类的样本在词库中的映射集合， p0Vec表示条件概率P(wi|c=0)P(wi|c=0)， p1Vec表示条件概率P(wi|c=1)P(wi|c=1)， pClass1表示类标签为1时的概率P(c=1)P(c=1)。 p1=ln[p(w1|c=1)p(w2|c=1)…p(wn|c=1)p(c=1)] p0=ln[p(w1|c=0)p(w2|c=0)…p(wn|c=0)p(c=0)] log取对数为防止向下溢出 功能:使用朴素贝叶斯进行分类,返回结果为0/1 """ p1 = sum(vec2Classify * p1Vec) + log(pClass1) p0 = sum(vec2Classify * p0Vec) + log(1 - pClass1) if p1 &gt; p0: return 1 else: return 0 #test def testingNB(self,testSample): "step1：加载数据集与类标号" listOPosts, listClasses = loadDataSet() "step2：创建词库" vocabList = self._creatVocabList(listOPosts) "step3：计算每个样本在词库中出现的情况" trainMat = [] for postinDoc in listOPosts: trainMat.append(self._bagOfsetOfWordToVec(vocabList, postinDoc)) p0V, p1V, pAb = self._trainNB0(trainMat, listClasses) "step4：测试" thisDoc = array(self._bagOfsetOfWordToVec(vocabList, testSample)) result=self._classifyNB(thisDoc, p0V, p1V, pAb) print testSample, 'classified as:', result # return result#### 加载数据集def loadDataSet(): postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] classVec = [0, 1, 0, 1, 0, 1] # 1 is abusive, 0 not return postingList, classVec#测试if __name__=="__main__": clf = Naive_Bayes() testEntry = [['love', 'my', 'girl', 'friend'], ['stupid', 'garbage'], ['Haha', 'I', 'really', "Love", "You"], ['This', 'is', "my", "dog"], ['maybe','stupid','worthless']] for item in testEntry: clf.testingNB(item) 3.2 使用朴素贝叶斯过滤垃圾邮件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# -*- coding: utf-8 -*-"""Created on 下午8:47 22 03 2017Email_Classify @author: plushunter """import reimport Bayesfrom numpy import *# mysent='This book is the best book on Python or M.L I have ever laid eyes upon.'# regEx = re.compile('\\W*')# listOfTokens=regEx.split(mysent)# tok=[tok.upper() for tok in listOfTokens if len(tok)&gt;0]# print tok## emailText=open('email/ham/6.txt').read()# listOfTokens=regEx.split(emailText)# print listOfTokensdef textParse(bigString): import re listOfTokens=re.split(r'\w*',bigString) return [tok.lower() for tok in listOfTokens if len(tok)&gt;2]def spamTest(): clf = Bayes.Naive_Bayes() docList=[] classList=[] fullText=[] for i in range(1,26): wordList=textParse(open('email/spam/%d.txt'%i).read()) docList.append(wordList) fullText.extend(wordList) classList.append(1) wordList=textParse(open('email/ham/%i.txt'%i).read()) docList.append(wordList) fullText.extend(wordList) classList.append(0) vocabList=clf._creatVocabList(docList) trainingSet=range(50);testSet=[] for i in range(10): randIndex=int(random.uniform(0,len(trainingSet))) testSet.append(trainingSet[randIndex]) del(trainingSet[randIndex]) trainMatix=[];trainClasses=[] for docIndex in trainingSet: trainMatix.append(clf._bagOfsetOfWordToVec(vocabList,docList[docIndex])) trainClasses.append(classList[docIndex]) p0V,p1V,pSpam=clf._trainNB0(array(trainMatix),array(trainClasses)) errorCount = 0 for docIndex in testSet: wordVector = clf._bagOfsetOfWordToVec(vocabList,docList[docIndex]) if clf._classifyNB(array(wordVector), p0V, p1V, pSpam)!=classList[docIndex]: errorCount+=1 print 'the error rate is :',float(errorCount)/len(testSet) 四、参考资料维基百科：Naive Bayes classifier数学之美番外篇：平凡而又神奇的贝叶斯方法朴素贝叶斯理论推导与三种常见模型机器学习实战]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（9）：感知机]]></title>
    <url>%2F2017%2F02%2F10%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%889%EF%BC%89%EF%BC%9A%E6%84%9F%E7%9F%A5%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[Introduction感知机（perceptron）是二类分类的线性分类模型，输入为实例的特征向量，输出为实例的类别，取+1和-1二值。 感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，导入基于误分类的损失函数，利用梯度下降对损失函数进行极小化，求得感知机模型，属于判别模型 感知机学习算法简单易于实现，分为原始形式和对偶形式。1957年由Rosenblatt提出，是神经网络和支持向量机的基础 本章框架如下： 感知机模型 感知机的学习策略（损失函数） 感知机学习算法（原始形式与对偶形式），并证明算法的收敛性 一、 感知机模型1.1 感知机模型感知机是一种线性分类器，属于判别模型。 假设我们的输入空间（特征空间）是$\chi \subseteq R^{\boldsymbol{n}}$，输出空间是$\boldsymbol{y}=\left\{ +1,-1 \right\}$。输入$\boldsymbol{x}\in \boldsymbol{\chi }$表示实例的特征向量，对应于输入空间（特征空间）的点；输出$y\in \boldsymbol{y}$表示实例的类别。由输入空间到输出空间的函数 f\left( x \right) =\mathrm{sign}\left( \boldsymbol{w}·x+\boldsymbol{b}\right)其中，$\boldsymbol{w}\in \boldsymbol{R}^{\boldsymbol{n}}$为权值或权值向量，$\boldsymbol{b}\in \boldsymbol{R}^{\boldsymbol{n}}$叫做偏置，$\mathrm{sign}$是符号函数，即 \mathrm{sign}\left( \mathrm{x} \right) =\left\{ \begin{array}{l} +1\mathrm{，\ x}\geqslant 0\\ -1\mathrm{，\ x}0误分类点到超平面的距离： -\frac{1}{||w||}y_i\left( w·x+b \right)则误分类点到超平面的总距离： -\frac{1}{||w||}\sum_{x_i\in M}{y_i\left( w·x_i+b \right)}据上述我们定义损失函数为: L\left( w,b \right) =-\sum_{x_i\in M}{y_i\left( w·x_i+b \right)}其中$M$为误分类点的集合，此即为感知机学习的经验风险函数。一个特定样本点的损失函数，在误分类时是参数$w,b$的线性函数，在正确分类时是0.因此，给定训练数据集$T$，损失函数$L(w,b)$是$w,b$的连续可导函数。感知机学习的策略就是在假设空间中选取使损失函数最小的模型参数，即感知机模型。 三、感知机学习算法这样我们就把感知机的学习问题转化为求解损失函数的最优化问题，最优化的方法是随机梯度下降法。 3.1 感知机学习算法首先我们确定要求解的最优化问题是： \min_{w,b}L\left( w,b \right) =-\sum_{x_i\in M}{y_i\left( w·x_i+b \right)}通过随机梯度下降法来求解最优化问题。首先，任意选择一个超平面$w_0,b_0$，然后用梯度下降法不断地极小化目标函数，一次随机选取一个误分类点使其梯度下降，而不是一次使$M$中所有误分类点的梯度下降。 计算得到梯度为： \nabla _wL\left( w,b \right) =-\sum_{x_i\in M}{y_ix_i} \nabla _bL\left( w,b \right) =-\sum_{x_i\in M}{y_i}对权值进行更新： w\gets w+\eta y_ix_i b\gets b+\eta y_i其中$\eta$称为学习率，通过迭代可以期待损失函数不断减小，直到为0. 对于上述算法过程，我们可以有一个直观的解释：当一个实例点被误分类，则调整$w,b$的值，使分离超平面向该误分类点的一侧移动，以较少该误分类点与超平面的距离，直至超平面越过该误分类点使其被正确分类。当然感知机学习算法由于采用不同的初值或选取不同的误分类点，解可以不同。 3.2 对偶形式对偶形式的基本想法是，将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$w$和$b$，我们假设初始值$w_0$和$b_0$均为0。对误分类点($x_i$,$y_i$)通过 w\gets w+\eta y_ix_i b\gets b+\eta y_i逐步修改$w,b$,设修改$n$次，则最后学习到的$w,b$可以分别表示为 w=\sum_{i=1}^N{n_i\eta y_ix_i}=\sum_{i=1}^N{a_iy_ix_i} b=\sum_{i=1}^N{a_iy_i}当$\eta =1$时，表示第$i$个实例点由于误分而进行更新的次数。实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分类、换句话说，这样的实例对学习结果影响最大。 因为对偶形式的训练实例仅以内积的形式出现。为了方便，可预先将训练实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵。 G=\left[ x_i·x_j \right] _{N\times N}与原始形式一样，感知机学习算法的对偶形式迭代是收敛的，存在多个解。 总结感知机学习算法的对偶形式如下： 输入：线性可分的数据集训练数据集$T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\} $，其中$x_i\in \chi =\boldsymbol{R}^{\boldsymbol{n}}$$y_i\in \boldsymbol{y}=\left\{ -1,+1 \right\} ,i=1,2,···,N $，学习率$\eta \left( 0&lt;\eta \le 1 \right) $； 输出：$a,b$；感知机模型 f\left( x \right) =sign\left( \sum_{j=1}^N{a_jy_jx_j}·x+b \right)其中$a=\left( a_1,a_2,···,a_N \right) ^T$ 1）$a\gets 0,b\gets 0$ 2）在训练集中选取数据$\left( x_i,y_i \right)$ 3）如果$y_i\left( \sum_{j=1}^N{a_jy_jx_j·x_i+b} \right) \le 0$ a_i\gets a_i+\eta b\gets b+\eta y_i 4）转至(2)直到没有误分类数据 四、参考资料李航《统计学习方法》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>感知机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笑忘录（7）：观影清单]]></title>
    <url>%2F2017%2F01%2F30%2F%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%887%EF%BC%89%EF%BC%9A%E8%A7%82%E5%BD%B1%E6%B8%85%E5%8D%95%2F</url>
    <content type="text"><![CDATA[个人观影清单 广播影评人协会最佳影片 年份/片名 导演 类型 国家 豆瓣评分 (2017)水形物语 吉尔莫·德尔·托罗 剧情 / 奇幻 / 冒险 美国 7.6 (29654人评价) ✔️(2016)爱乐之城 达米恩·查泽雷 剧情 / 爱情 / 歌舞 美国 8.3 (356505人评价) ✔️(2015)聚焦 汤姆·麦卡锡 剧情 / 传记 美国/加拿大 8.8 (123984人评价) ✔️ (2014)少年时代 理查德·林克莱特 剧情 / 家庭 美国 8.5 (138281人评价) (2013)为奴十二年 史蒂夫·麦奎因 剧情 / 传记 / 历史 美国/英国 7.9 (105351人评价) ✔️(2012)逃离德黑兰 本·阿弗莱克 剧情 / 惊悚 / 历史 美国 8.2 (172185人评价) (2011)艺术家 阿扎纳维西于斯 剧情 / 喜剧 / 爱情 法国/比利时/美国 8.4 (75976人评价) ✔️(2010)社交网络 大卫·芬奇 剧情 / 传记 美国 8.1 (269858人评价) (2009)拆弹部队 凯瑟琳·毕格罗 剧情 / 惊悚 / 战争 美国 7.7 (106224人评价) ✔️(2008)贫民窟的百万富翁 丹尼·博伊尔 / 洛芙琳·坦丹 剧情 / 爱情 英国/美国 8.5 (403554人评价) ✔️(2007)老无所依 伊桑·科恩 / 乔尔·科恩 犯罪 / 剧情 / 惊悚 美国 8.0 (130631人评价) (2006)无间道风云 马丁·斯科塞斯 犯罪 / 剧情 / 惊悚 美国/香港 7.2 (75973人评价) ✔️(2005)断背山 李安 剧情 / 爱情 / 同性 / 家庭 美国/加拿大 8.6 (331163人评价) (2004)杯酒人生 亚历山大·佩恩 剧情 / 喜剧 / 爱情 美国/匈牙利 7.9 (34580人评价) ✔️(2003)指环王3：王者无敌 彼得·杰克逊 剧情 / 动作 / 奇幻 / 冒险 美国/新西兰 9.1 (302742人评价) (2002)芝加哥 罗伯·马歇尔 喜剧 / 歌舞 / 犯罪 美国/德国 8.6 (62999人评价) ✔️(2001)美丽心灵 朗·霍华德 传记 / 剧情 美国 8.9 (321117人评价) (2000)角斗士 雷德利·斯科特 剧情 / 动作 / 历史 / 冒险 英国/美国 8.4 (125775人评价) (2000)美国丽人 萨姆·门德斯 剧情 / 爱情 / 家庭 美国 8.4 (186250人评价) ✔️(1999)拯救大兵瑞恩 史蒂文·斯皮尔伯格 剧情 / 历史 / 战争 美国 8.9 (250317人评价) ✔️(1998)洛城机密 柯蒂斯·汉森 犯罪 / 剧情 / 悬疑 / 惊悚 美国 8.6 (77813人评价) (1997)冰血暴 乔尔·科恩 犯罪 / 剧情 / 惊悚 美国/英国 7.9 (43584人评价) ✔️(1996)理智与情感 李安 剧情 / 爱情 美国/英国 8.3 (59885人评价) 戛纳电影节金棕榈奖 年份/片名 导演 类型 国家 豆瓣评分 （2017）方形 鲁本·奥斯特伦德 剧情 / 喜剧 瑞典 / 德国 / 法国 / 丹麦 7.9 (3642人评价) （2016）我是布莱克 肯·洛奇 剧情 英国 / 法国 / 比利时 8.2 (13369人评价) （2015）流浪的迪潘 雅克·欧迪亚 剧情 / 犯罪 法国 7.1 (3580人评价) （2014）冬眠 努里·比格·锡兰 剧情 土耳其 / 法国 / 德国 8.1 (13478人评价) （2013）阿黛尔的生活 阿布戴·柯西胥 剧情 / 爱情 / 同性 法国/比利时/西班牙/突尼斯 8.3 (104583人评价) （2012）爱 迈克尔·哈内克 剧情 / 爱情 美国 8.5 (49522人评价) （2011）生命之树 泰伦斯·马力克 剧情 / 奇幻 / 家庭 美国 6.9 (30703人评价) （2010）能召回前世的布米叔叔 韦拉斯哈古 剧情/奇幻 泰国/英国/法国/德国/西班牙 6.8 (5439人评价) （2009）白丝带 迈克尔·哈内克 剧情 / 悬疑 德国/奥地利/法国/意大利 8.1 (19552人评价) （2008）课室风云 劳伦·冈泰 剧情 法国 7.7 (4800人评价) （2007）四月三周两天 克里斯蒂安·蒙吉 剧情 罗马尼亚 8.1 (19042人评价) （2006）风吹麦浪 肯·洛奇 剧情 / 历史 / 战争 爱尔兰/英国/德国 8.0 (8477人评价) （2005）孩子 让-皮埃尔·达内 / 吕克·达内 犯罪 / 剧情 / 爱情 比利时/法国 7.9 (4768人评价) （2004）华氏911 迈克尔·摩尔 历史 / 纪录片 美国 7.9 (16924人评价) （2003）大象 格斯·范·桑特 犯罪 / 剧情 美国 7.8 (53690人评价) （2002）钢琴家 罗曼·波兰斯基 剧情 / 传记 / 历史 / 战争 法国/德国/英国/波兰 9.0 (212542人评价) （2001）儿子的房间 南尼·莫莱蒂 剧情 / 悬疑 / 家庭 法国/意大利 7.9 (6632人评价) （2000）黑暗中的舞者 拉斯·冯·提尔 剧情 / 歌舞 丹麦/西班牙/阿根廷 8.3 (64065人评价) （1999）罗塞塔 让-皮埃尔·达内 / 吕克·达内 剧情 法国/比利时 8.1 (6241人评价) （1998）永恒和一日 西奥·安哲罗普洛斯 剧情 法国/意大利/希腊/德国 8.9 (13001人评价) （1997）鳗鱼 今村昌平 剧情 / 犯罪 日本 7.9 (6402人评价) （1997）樱桃的滋味 阿巴斯·基亚罗斯塔米 剧情 法国/伊朗 7.9 (14689人评价) （1996）秘密与谎言 迈克·李 剧情 / 喜剧 / 家庭 英国/法国 8.2 (2673人评价) （1995）地下 埃米尔·库斯图里卡 喜剧 / 剧情 / 战争 法国/南斯拉夫/德国 9.1 (25316人评价) （1994）低俗小说 昆汀·塔伦蒂诺 剧情 / 喜剧 / 犯罪 美国 8.8 (350669人评价) （1993）钢琴课 简·坎皮恩 剧情 / 爱情 / 音乐 新西兰/澳大利亚/法国 8.0 (65334人评价) （1993）霸王别姬 陈凯歌 剧情 / 爱情 / 同性 中国大陆/香港 9.5 (696448人评价) （1992）善意的背叛 比利·奥古斯特 传记 / 剧情 / 爱情 瑞典/德国/英国 8.5 (596人评价) （1991）巴顿·芬克 乔尔·科恩 / 伊桑·科恩 剧情 / 悬疑 / 惊悚 美国/英国 8.1 (18000人评价) （1990）我心狂野 大卫·林奇 犯罪 / 爱情 / 惊悚 美国 7.4 (17336人评价) （1989）性、谎言和录像带 史蒂文·索德伯格 剧情 / 情色 美国 7.6 (26294人评价) （1988）征服者佩尔 比利·奥古斯特 剧情 丹麦/瑞典 8.6 (2413人评价) （1987）在撒旦的阳光下 莫里斯·皮亚拉 剧情 法国 7.3 (876人评价) （1986）教会 罗兰·约菲 冒险 / 剧情 / 历史 英国 7.8 (2003人评价) （1985）爸爸去出差 埃米尔·库斯图里卡 剧情 南斯拉夫 8.6 (2829人评价) （1984）德州巴黎 维姆·文德斯 剧情 英国/法国/美国/西德 8.6 (25934人评价) （1983）楢山节考 今村昌平 剧情 日本 8.9 (16664人评价) （1982）自由之路 塞里夫·格仁 / 尤马兹·古尼 剧情 / 爱情 法国/瑞士/土耳其 8.0 (341人评价) （1981）大失踪 科斯塔-加夫拉斯 剧情 / 历史 / 悬疑 / 惊悚 美国 7.7 (871人评价) （1980）铁人 安杰伊·瓦伊达 剧情 / 历史 波兰 7.7 (486人评价) 威尼斯电影节金狮奖 年份/片名 导演 类型 国家 豆瓣评分 （2017）水形物语 吉尔莫·德尔·托罗 剧情 / 奇幻 / 冒险 美国 7.6 (29462人评价) （2016）离开的女人 拉夫·迪亚兹 剧情 菲律宾 6.9 (200人评价) （2015）来自远方 洛伦佐·维加斯 剧情 / 同性 委内瑞拉/墨西哥 7.1 (1868人评价) （2014）寒枝雀静 罗伊·安德森 剧情 / 喜剧 瑞典/德国/挪威/法国 7.7 (11981人评价) （2013）罗马环城高速 吉安弗兰科·罗西 纪录片 意大利 6.2 (352人评价) （2012）圣殇 金基德 剧情 韩国 7.7 (33407人评价) （2011）浮士德 亚历山大·索科洛夫 剧情 俄罗斯 6.7 (2493人评价) （2010）在某处 索菲亚·科波拉 剧情 美国/英国/意大利/日本 6.5 (9365人评价) （2009）黎巴嫩 塞缪尔·毛茨 剧情 / 战争 以色列/德国/法国/黎巴嫩 7.2 (4506人评价) （2008）摔角王 达伦·阿伦诺夫斯基 剧情 / 运动 美国/法国 8.3 (32441人评价) （2007）色，戒 李安 剧情 / 爱情 / 情色 美国/中国大陆/台湾/香港 8.2 (254213人评价) （2006）三峡好人 贾樟柯 剧情 / 爱情 中国大陆 8.0 (51655人评价) （2005）断背山 李安 剧情 / 爱情 / 同性 / 家庭 美国/加拿大 8.6 (331126人评价) （2004）维拉·德雷克 迈克·李 犯罪 / 剧情 法国/英国 8.0 (1433人评价) （2003）回归 安德烈·萨金塞夫 剧情 / 家庭 俄罗斯 8.7 (12845人评价) （2002）玛德莲堕落少女 彼得·穆兰 剧情 爱尔兰/英国 7.9 (1802人评价) （2001）季风婚宴 米拉·奈尔 喜剧 / 剧情 / 爱情 印度/美国/法国/意大利/德国 7.6 (1484人评价) （2000）生命的圆圈 贾法·帕纳西 剧情 伊朗/意大利/瑞士 7.6 (909人评价) （1999）一个都不能少 张艺谋 喜剧 / 剧情 中国大陆 7.5 (84395人评价) （1998）他们微笑的样子 吉安尼·阿梅利奥 剧情 意大利 8.0 (339人评价) （1997）花火 北野武 犯罪 / 剧情 / 爱情 / 惊悚 日本 8.5 (37717人评价) （1996）傲气盖天 尼尔·乔丹 传记 / 剧情 / 惊悚 / 战争 英国/爱尔兰/美国 7.7 (1626人评价) （1995）三轮车夫 陈英雄 犯罪 / 剧情 越南/法国/香港 7.6 (9394人评价) （1994）暴雨将至 米尔科·曼彻夫斯基 剧情 / 战争 马其顿/法国/英国 8.7 (17880人评价) （1993）爱情万岁 蔡明亮 剧情 / 同性 台湾 7.7 (14282人评价) （1993）蓝白红三部曲之蓝 基耶斯洛夫斯基 剧情 / 爱情 / 音乐 法国/波兰/瑞士 8.5 (68155人评价) （1993）银色·性·男女 罗伯特·奥特曼 喜剧 / 剧情 美国 7.8 (2485人评价) （1992）秋菊打官司 张艺谋 剧情 中国大陆/香港 7.8 (46678人评价) （1991）蒙古精神 尼基塔·米哈尔科夫 剧情 法国/苏联 8.5 (2386人评价) （1990）君臣人子小命呜呼 汤姆·斯托帕德 喜剧 / 剧情 英国/美国 8.5 (2522人评价) （1989）悲情城市 侯孝贤 剧情 台湾/香港 8.8 (38473人评价) （1988）圣洁酒徒的传奇 埃曼诺·奥尔米 剧情 意大利/法国 7.6 (176人评价) （1987）再见，孩子们 路易·马勒 剧情 / 战争 法国/意大利/西德 8.6 (7466人评价) （1986）绿光 埃里克·侯麦 剧情 / 爱情 法国 8.1 (6031人评价) （1985）天涯沦落女 阿涅斯·瓦尔达 剧情 法国 8.2 (2152人评价) （1984）寂静太阳年 克日什托夫·扎努西 剧情 / 爱情 德国/波兰/美国 7.8 (513人评价) （1983）芳名卡门 让-吕克·戈达尔 喜剧 / 犯罪 / 剧情 / 音乐 / 爱情 法国 7.6 (2314人评价) （1982）事物的状态 维姆·文德斯 剧情 西德/葡萄牙/美国 7.5 (649人评价) （1981）德国姊妹 玛加蕾特·冯·特罗塔 剧情 / 历史 西德 7.7 (171人评价) （1980）女煞葛洛莉 约翰·卡萨维茨 犯罪 / 剧情 / 惊悚 美国 7.6 (1103人评价) （1979）大西洋城 路易·马勒 犯罪 / 剧情 / 爱情 加拿大/法国 7.6 (568人评价) （1964）红色沙漠 米开朗基罗·安东尼奥尼 剧情 法国/意大利 8.1 (6893人评价) （1963）城市上空的手 弗朗西斯科·罗西 剧情 意大利/法国 7.7 (286人评价) （1962）伊万的童年 安德烈·塔可夫斯基 剧情 / 战争 苏联 8.6 (11751人评价) （1962）家庭日记 瓦莱瑞奥·苏里尼 剧情 意大利 7.9 (75人评价) （1961）去年在马里昂巴德 阿伦·雷乃 剧情 / 爱情 / 悬疑 法国/意大利 8.2 (8281人评价) （1960）横渡莱茵河 安德烈·卡耶特 剧情 意大利/法国/西德 暂无评分 （1959）大战争 马里奥·莫尼切利 喜剧 / 剧情 / 战争 意大利/法国 8.0 (142人评价) （1959）罗维雷将军 罗伯托·罗西里尼 剧情 / 战争 意大利/法国 8.3 (427人评价) （1958）无法松的一生 稻垣浩 剧情 日本 8.4 (1399人评价) （1957）大河之歌 萨蒂亚吉特·雷伊 剧情 印度 8.7 (1397人评价) （1955）词语 卡尔·西奥多·德莱叶 剧情 丹麦 8.3 (1056人评价) （1954）罗密欧与朱丽叶 雷纳托·卡斯特拉尼 剧情 / 爱情 意大利/英国 7.1 (450人评价) （1952）禁忌的游戏 雷内·克莱芒 剧情 / 战争 法国 8.5 (3369人评价) （1951）罗生门 黑泽明 犯罪 / 剧情 / 悬疑 日本 8.7 (126878人评价) （1950）刑事法庭 安德烈·卡耶特 法国 7.4 (93人评价) （1949）情妇玛侬 亨利-乔治·克鲁佐 剧情 / 犯罪 法国 7.3 (242人评价) 柏林电影节金熊奖 年份/片名 导演 类型 国家 豆瓣评分 （2017）肉与灵 伊尔蒂科·茵叶蒂 剧情 / 爱情 匈牙利 7.5 (5166人评价) （2016）海上火焰 吉安弗兰科·罗西 纪录片 意大利/法国 6.6 (1223人评价) （2015）出租车 贾法·帕纳西 剧情 / 喜剧 伊朗 8.0 (11116人评价) （2014）白日焰火 刁亦男 剧情 / 犯罪 / 悬疑 中国大陆 7.2 (171426人评价) （2013）孩童姿势 卡林·皮特·内策尔 剧情 / 家庭 罗马尼亚 7.3 (1400人评价) （2012）凯撒必须死 保罗·塔维亚尼 剧情 意大利 7.8 (4326人评价) （2011）一次别离 阿斯哈·法哈蒂 剧情 / 家庭 伊朗/法国 8.7 (134072人评价) （2010）蜂蜜 赛米·卡普拉诺格鲁 剧情 土耳其/德国/法国 7.8 (3399人评价) （2009）伤心的奶水 克劳迪亚·略萨 剧情 / 音乐 秘鲁/西班牙 7.2 (3860人评价) （2008）精英部队 若泽·帕迪里亚 剧情 / 动作 / 犯罪 / 惊悚 巴西/美国/阿根廷 7.9 (27653人评价) （2007）图雅的婚事 王全安 剧情 / 爱情 中国大陆 8.0 (19483人评价) （2006）格巴维察 亚斯米拉·日巴尼奇 剧情 波黑/克罗地亚/德国/奥地利 7.7 (718人评价) （2005）卡雅利沙的卡门 Mark Dornford-May 歌舞 / 剧情 / 爱情 南非 6.2 (81人评价) （2004）勇往直前 法提赫·阿金 剧情 / 爱情 德国/土耳其 8.2 (3654人评价) （2003）尘世之间 迈克尔·温特伯顿 剧情 英国 8.0 (1166人评价) （2002）血腥星期天 保罗·格林格拉斯 剧情 / 历史 / 战争 爱尔兰/英国 8.0 (1581人评价) （2002）千与千寻 宫崎骏 剧情 / 动画 / 奇幻 日本 9.2 (722352人评价) （2001）亲密 帕特里斯·夏侯 剧情 / 情色 法国/英国/德国/西班牙 6.4 (7251人评价) （2000）木兰花 保罗·托马斯·安德森 剧情 美国 8.2 (19274人评价) （1999）细细的红线 泰伦斯·马力克 动作 / 剧情 / 战争 美国 7.8 (16590人评价) （1998）中央车站 沃尔特·塞勒斯 剧情 巴西/法国 8.7 (74630人评价) （1997）性书大亨 米洛斯·福尔曼 剧情 / 传记 / 情色 美国 7.9 (8996人评价) （1996）理智与情感 李安 剧情 / 爱情 美国/英国 8.3 (59881人评价) （1995）手到擒来 贝特朗·塔维涅 犯罪 / 剧情 法国 7.3 (220人评价) （1994）因父之名 吉姆·谢里丹 传记 / 剧情 爱尔兰/英国/美国 8.8 (21533人评价) （1993）喜宴 李安 剧情 / 喜剧 / 爱情 / 同性 / 家庭 台湾/美国 8.8 (143174人评价) （1992）香魂女 谢飞 剧情 中国大陆 7.9 (4149人评价) （1991）大峡谷 劳伦斯·卡斯丹 犯罪 / 剧情 美国 7.2 (86人评价) （1990）失翼灵雀 伊利·曼佐 喜剧 / 剧情 / 爱情 捷克斯洛伐克 8.4 (1116人评价) （1989）雨人 巴瑞·莱文森 剧情 美国 8.6 (214389人评价) （1988）红高粱 张艺谋 剧情 / 历史 / 爱情 / 战争 中国大陆 8.2 (96309人评价) （1961）夜 米开朗基罗·安东尼奥尼 剧情 / 爱情 意大利/法国 8.8 (5544人评价) （1958）野草莓 英格玛·伯格曼 剧情 / 爱情 / 家庭 瑞典 8.7 (23479人评价) （1957）十二怒汉 西德尼·吕美特 剧情 美国 9.4 (183099人评价) （1953）恐惧的代价 亨利-乔治·克鲁佐 剧情 / 惊悚 / 冒险 法国/意大利 8.7 (5163人评价) （1951）仙履奇缘 克莱德·杰洛尼米 爱情 / 动画 / 奇幻 / 歌舞 美国 8.1 (27899人评价) Metacritic历年最佳电影 片名/年份 导演 类型 豆瓣评分 其他 （2017）脸庞，村庄 阿涅斯·瓦尔达 / 让·热内 纪录片 9.2 (12453人评价) 95分。2—10：敦刻尔克94，伯德小姐94，普通女人93，请以你的名字呼唤我93，狐步舞92，佛罗里达乐园92，书缘：纽约公共图书馆91，我叫他摩根90，无爱可诉90，魅影缝匠90 （2016）月光男孩 巴里·詹金斯 剧情 / 同性 7.2 (70138人评价) 99分。2—10：我不是你的黑鬼97，海边的曼彻斯特96，托尼厄德曼95，爱乐之城93，再一次体悟92，红海龟92，校塔枪击案92，抽搐症候群91，第十三修正案90，冈仁波齐90，帕特森90 （2015）卡罗尔 托德·海因斯 剧情 / 爱情 / 同性 8.2 (131245人评价) 95分。2—10：45周年94，头脑特工队94，聚焦93，夏尔巴人93，失常92，廷巴克图92，沉默之像92，出租车91，诉讼90 （2014）少年时代 理查德·林克莱特 剧情 / 家庭 8.5 (138262人评价) 100分。2—10：维龙加95，透纳先生94，利维坦91，Big Men 90，修女伊达90，两天一夜90，夜宿人89，国家美术馆89，辉夜姬物语89 （2013）鲜为人知的秘密 萨曼莎·布克 纪录片 暂无评分 100分。2—10：为奴十二载97，地心引力96，爱在午夜降临前94，隔代表亲94，基甸的部队93，醉乡民谣93，Out of the Clear Blue Sky 92，我们讲述的故事91，守门人91 （2012）猎杀本·拉登 凯瑟琳·毕格罗 剧情 / 惊悚 / 历史 7.6 (36287人评价) 95分。2—10：爱94，奥迈耶的痴梦92，这不是一部电影90，伊莲娜87，单车少年87，Gregory Crewdson: Brief Encounters 87，林肯86，瘟疫求生指南86，芭芭拉86。 （2011）一次别离 阿斯哈·法哈蒂 剧情 / 家庭 8.7 (134069人评价) 95分。2—10：曾几何时94，我的改革90，诗89，艺术家89，树荫88，失恋男人旅行日记87，能召回前世的布米叔叔87，哈利波特与死亡圣器二87，尼古拉齐奥塞斯库的自传87 （2010）社交网络 大卫·芬奇 剧情 / 传记 8.1 (269844人评价) 95分。2—10：卡洛斯94，玩具总动员三92，预言者90，冬天的骨头90，未完成的电影88，监守自盗88，国王的演讲88，45365 88，橄榄球星之死88 （2009）拆弹部队 凯瑟琳·毕格罗 剧情 / 惊悚 / 战争 7.7 (106215人评价) 94分。2—10：蓝调之歌94，35杯朗姆酒92，步履不停89，再见索罗89，图班嫁给我89，飞屋环游记88，格莫拉87，阿涅斯的海滩86，悬崖上的金鱼姬86 （2008）四月三周两天 克里斯蒂安·蒙吉 剧情 8.1 (19042人评价) 97分。2—10：机器人总动员94，课室风云92，和巴什尔跳华尔兹91，走钢丝的人89，贫民窟的百万富翁86，红气球之旅86，在人生的另一边85，亚力山娜85，属于我们的圣诞节84 （2007）美食总动员 布拉德·伯德 / 简·皮克瓦 喜剧 / 动画 / 奇幻 8.2 (174622人评价) 96分。2—10：潜水钟与蝴蝶92，血色将至92，老无所依91，我在伊朗长大90，一望无际89，曾经88，柳暗花明88，一夜大肚85，赎罪85 （2006）潘神的迷宫 吉尔莫·德尔·托罗 剧情 / 奇幻 / 悬疑 / 战争 7.8 (126292人评价) 98分。2—10：女王91，93航班91，窃听风暴89，波拉特89，硫磺岛的来信89，孩子87，命运无常87，大急救86，我们每日的面包86 （2005）卡波特 贝尼特·米勒 剧情 / 传记 / 犯罪 / 同性 7.9 (14485人评价) 88分。2—10：无人知晓88，超级无敌掌门狗：人兔的诅咒87，断背山87，轮椅上的竞技87，灰熊人87，乌龟也会飞85，达尔文的恶梦84，国王与王后84，僵尸新娘84 （2004）杯酒人生 亚历山大·佩恩 剧情 / 喜剧 / 爱情 7.9 (34579人评价) 94分。2—10：割礼龙凤斗91，爱在日落黄昏时90，超人总动员90，十面埋伏89，美丽心灵的永恒阳光89，万福玛利亚87，诅咒87，百万美元宝贝86，洛杉矶影话86 （2003）指环王3：王者无敌 彼得·杰克逊 剧情 / 动作 / 奇幻 / 冒险 9.1 (302713人评价) 94分。2—10：疯狂约会美丽都91，海底总动员90，美国荣耀90，追捕弗雷德曼家族90，迷失东京89，山村犹有读书声87，战争迷雾87，栗色伊拉克86，十段生命的律动86 （2002）千与千寻 宫崎骏 剧情 / 动画 / 奇幻 9.2 (722321人评价) 94分。2—10：冰原快跑人91，血腥星期天90，你妈妈也一样88，失序年代88，指环王二88，俄罗斯方舟86，我要回家86，对她说86，钢琴家86 （2001）指环王1：魔戒再现 彼得·杰克逊 剧情 / 动作 / 奇幻 / 冒险 8.9 (318973人评价) 92分。2—10：鲸鱼马戏团92，现代启示录91，我的意大利之旅90，高斯福庄园90，幽灵世界88，意外边缘86，沙之下86，地下孩童85，南极坚韧号85 （2000）卧虎藏龙 李安 剧情 / 动作 / 爱情 / 武侠 / 古装 8.0 (179373人评价) 93分。2—10：一一93，军中禁恋91，几近成名90，小鸡快跑88，随风而逝86，毒品网络86，当黑夜降临85，你可以信赖我85，无声的呐喊85 （1999）酣歌畅戏 迈克·李 剧情 / 喜剧 / 音乐 / 歌舞 / 传记 7.2 (458人评价) 90分。2—10：成为约翰马尔科维奇90，玩具总动员二88，关于我母亲的一切87，史崔特先生的故事86，人生七年六86，男孩不哭86，美国丽人86，钢铁巨人85，导演狂想曲84 （1998）拯救大兵瑞恩 史蒂文·斯皮尔伯格 剧情 / 历史 / 战争 8.9 (250282人评价) 90分。2—10：楚门的世界90，恋爱中的莎士比亚87，青春年少86，战略高手85，绝地计划82，爱就让我快乐81，长岛爱与死80，中央车站80，苦难79 （1997）洛城机密 柯蒂斯·汉森 犯罪 / 剧情 / 悬疑 / 惊悚 8.6 (77810人评价) 90分。2—10：意外的春天90，四个小女孩89，不羁夜85，来自天上的声音83，变脸82，一诺千金82，与男人同行81，赌城纵横78，仲夏夜玫瑰78 （1996）秘密与谎言 迈克·李 剧情 / 喜剧 / 家庭 8.2 (2673人评价) 91分。2—10：闪亮的风采87，英国病人87，冰血暴87，弹簧刀84，欢迎光临娃娃屋83，当我们是拳王的日子83，猜火车83，寒冷舒适的农庄81，与灾难调情81 （1995）克鲁伯 泰利·茨威戈夫 纪录片 / 传记 8.4 (641人评价) 93分。2—10：玩具总动员92，不惜一切86，理智与情感84，小公主83，小猪宝贝83，离开拉斯维加斯82，矮子当道82，乔治亚81，邮差81 日本《电影旬报》最佳日本电影 年份/片名 导演 类型 国家 豆瓣评分 （2017）夜空总有最大密度的蓝色 石井裕也 剧情 / 爱情 日本 7.4 (4183人评价) （2016）在这世界的角落 片渊须直 剧情 / 战争 / 动画 日本 7.6 (19079人评价) （2015）恋人们 桥口亮辅 剧情 日本 7.4 (4948人评价) （2014）只在那里发光 吴美保 剧情 日本 7.2 (4848人评价) （2013）去见小洋葱的母亲 森崎东 剧情 / 喜剧 / 家庭 日本 7.6 (2341人评价) （2012）家族的国度 梁英姬 剧情 日本 7.7 (1769人评价) （2011）一封明信片 新藤兼人 剧情 日本 7.3 (489人评价) （2010）恶人 李相日 剧情 日本 7.7 (17172人评价) （2009）亲爱的医生 西川美和 剧情 日本 8.0 (3861人评价) （2008）入殓师 泷田洋二郎 剧情 日本 8.8 (329224人评价) （2006）即使这样也不是我做的 周防正行 剧情 日本 8.3 (7048人评价) （2006）扶桑花女孩 李相日 喜剧 / 剧情 日本 8.0 (21876人评价) （2004）无敌青春 井筒和幸 动作 / 喜剧 / 剧情 / 爱情 日本 7.7 (1040人评价) （2004）无人知晓 是枝裕和 剧情 日本 9.0 (53392人评价) （2002）雾岛美丽的夏天 黑木和雄 剧情 日本 7.2 (168人评价) （2002）黄昏的清兵卫 山田洋次 剧情 / 爱情 日本 8.7 (16621人评价) （2001）GO!大暴走 GO 行定勋 剧情 日本 8.2 (13954人评价) （2000）颜 阪本顺治 喜剧 / 剧情 日本 8.1 (390人评价) （1998）啊，春天 相米慎二 剧情 日本 8.0 (420人评价) （1997）花火 北野武 犯罪 / 剧情 / 爱情 / 惊悚 日本 8.5 (37717人评价) （1997）鳗鱼 今村昌平 剧情 / 犯罪 日本 7.9 (6402人评价) （1996）谈谈情跳跳 周防正行 剧情 / 喜剧 / 爱情 / 歌舞 日本 8.2 (9111人评价) （1995）午后的遗言 新藤兼人 剧情 日本 8.1 (805人评价) （1994）全身小说家 原一男 纪录片 日本 8.2 (251人评价) （1993）龙猫 宫崎骏 儿童 / 动画 / 奇幻 / 家庭 日本 9.1 (451130人评价) （1985）其后 森田芳光 剧情 日本 8.6 (3808人评价) （1984）葬礼 伊丹十三 喜剧 日本 8.3 (1287人评价) （1982）蒲田进行曲 深作欣二 喜剧 / 爱情 日本 7.9 (2073人评价) （1981）泥之河 小栗康平 剧情 日本 8.7 (2334人评价) （1979）复仇在我 今村昌平 犯罪 / 剧情 日本 8.3 (3448人评价) （1977）幸福的黄手帕 山田洋次 喜剧 / 剧情 日本 8.1 (7225人评价) （1974）望乡 熊井启 剧情 / 历史 / 战争 日本 8.6 (5863人评价) （1968）诸神的欲望 今村昌平 剧情 日本 8.2 (1290人评价) （1967）夺命剑 小林正树 剧情 日本 8.9 (3719人评价) （1965）红胡子 黑泽明 剧情 日本 8.5 (3129人评价) （1964）砂之女 敕使河原宏 剧情 / 惊悚 日本 8.4 (4595人评价) （1963）日本昆虫记 今村昌平 剧情 日本 8.2 (1656人评价) （1958）楢山节考 木下惠介 剧情 日本 8.7 (3808人评价) （1955）浮云 成濑巳喜男 剧情 / 爱情 日本 8.6 (5024人评价) （1954）二十四只眼睛 木下惠介 剧情 日本 8.6 (2630人评价) （1952）生之欲 黑泽明 剧情 日本 9.0 (11690人评价) （1951）麦秋 小津安二郎 剧情 / 家庭 日本 8.8 (5221人评价) （1949）晚春 小津安二郎 剧情 / 家庭 日本 8.7 (9695人评价) （1948）泥醉天使 黑泽明 犯罪 / 剧情 日本 8.1 (2336人评价) 东京电影节 年份/片名 导演 类型 国家 豆瓣评分 (2016)昨日之花 克里斯·克劳斯 剧情 / 喜剧 / 爱情 奥地利/德国/法国 6.5 (358人评价) (2015)尼斯·疯狂的心 罗伯托·柏林厄 剧情 / 传记 / 历史 巴西 8.2 (3697人评价) (2014)天知道 本·萨弗迪 / 约书亚·萨弗迪 剧情 美国 6.9 (649人评价) (2013)我们是最棒的！ 鲁卡斯·穆迪森 剧情 / 音乐 瑞典 7.3 (889人评价) (2012)他人之子 罗兰娜·利维 剧情 法国 7.4 (1106人评价) (2011)触不可及 奥利维埃·纳卡什 剧情 / 喜剧 法国 9.1 (394864人评价) (2010)亲密文法 尼尔·伯格曼 剧情 以色列 7.8 (257人评价) (2007)乐队来访 艾伦·科勒林 喜剧 / 剧情 / 音乐 以色列/法国/美国 8.1 (4593人评价) (2004)暖 霍建起 爱情 / 剧情 中国大陆 7.8 (11047人评价) (2000)爱情是狗娘 冈萨雷斯·伊纳里图 剧情 / 惊悚 墨西哥 8.2 (33898人评价) (1999)黑暗之光 张作骥 剧情 台湾 8.1 (1959人评价) (1998)睁开你的双眼 亚历杭德罗·阿梅纳瓦尔 剧情 / 悬疑 / 爱情 / 科幻 / 惊悚 西班牙/法国/意大利 8.1 (8446人评价) (1997)走出寂静 卡罗莉内·林克 剧情 / 音乐 德国 8.3 (999人评价) (1996)给我一个爸 扬·斯维拉克 剧情 / 喜剧 / 音乐 捷克/英国/法国 8.4 (3906人评价) (1993)蓝风筝 田壮壮 剧情 / 历史 中国大陆 8.6 (27532人评价) (1987)老井 吴天明 剧情 / 爱情 中国 7.9 (7345人评价) (1985)台风俱乐部 相米慎二 剧情 / 爱情 日本 8.0 (1247人评价) 圣丹斯电影节山形国际纪录片电影节布宜诺斯艾利斯电影节奥斯卡金像奖最佳影片]]></content>
      <categories>
        <category>笑忘录</category>
      </categories>
      <tags>
        <tag>广播影评人</tag>
        <tag>金棕榈奖</tag>
        <tag>金狮奖</tag>
        <tag>金熊奖</tag>
        <tag>Metacritic</tag>
        <tag>电影旬报</tag>
        <tag>圣丹斯</tag>
        <tag>金像奖</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（8）：XgBoost]]></title>
    <url>%2F2017%2F01%2F26%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%888%EF%BC%89%EF%BC%9AXgBoost%2F</url>
    <content type="text"><![CDATA[一、XGBoost简介在数据建模中，经常采用Boosting方法通过将成百上千个分类准确率较低的树模型组合起来，成为一个准确率很高的预测模型。这个模型会不断地迭代，每次迭代就生成一颗新的树。但在数据集较复杂的时候，可能需要几千次迭代运算，这将造成巨大的计算瓶颈。 针对这个问题。华盛顿大学的陈天奇博士开发的XGBoost（eXtreme Gradient Boosting）基于C++通过多线程实现了回归树的并行构建，并在原有Gradient Boosting算法基础上加以改进，从而极大地提升了模型训练速度和预测精度。 在Kaggle的希格斯子信号识别竞赛，XGBoost因为出众的效率与较高的预测准确度在比赛论坛中引起了参赛选手的广泛关注，在1700多支队伍的激烈竞争中占有一席之地。随着它在Kaggle社区知名度的提高，最近也有队伍借助XGBoost在比赛中夺得第一。其次，因为它的效果好，计算复杂度不高，也在工业界中有大量的应用。 二、监督学习的三要素因为Boosting Tree本身是一种有监督学习算法，要讲Boosting Tree，先从监督学习讲起。在监督学习中有几个逻辑上的重要组成部件，粗略地可以分为：模型、参数、目标函数和优化算法。 2.1 模型模型指的是给定输入$x_i$如何去预测输出$y_i$。我们比较常见的模型如线性模型（包括线性回归和Logistic Regression）采用线性加和的方式进行预测 \hat{y}_i=\sum_j{w_jx_{ij}}这里的预测值$y$可以由不同的解释，比如我们可以把它作为回归目标的输出，或者进行$sigmoid$变换得到概率（即用$\frac{1}{1+e^{-\hat{y}_i}}$来预测正例的概率），或者作为排序的指标等。而一个线性模型根据$y$的解释不通（以及设计对应的目标函数）用到回归、分类或者排序等场景。 2.2 参数参数就是我们根据模型要从数据里头学习的东西，比如线性模型中的线性系数： \varTheta =\left\{w_j|j=1,2,···,d\right\}2.3 目标函数：误差函数+正则化项模型和参数本身指定了给定输入我们如何预测，但是没有告诉我们如何去寻找一个比较好的参数，这个时候就需要目标函数函数登场了。一般地目标函数包含两项：一项是损失函数，它说明了我们的模型有多拟合数据；另一项是正则化项，它惩罚了复杂模型。 1）$L(\varTheta)$：损失函数$L=\sum_{i=1}^n{l\left(y_i,\hat{y}_i\right)}$，常见的损失函数有： 平方损失：$l\left(y_i,\hat{y}_i\right)=\left(y_i-\hat{y}_i\right)^2$ Logistic损失：$l\left(y_i,\hat{y}_i\right)=y_i\ln\left(1+e^{-y_i}\right)+\left(1-y_i\right)\ln\left(1+e^{y_i}\right)$ 2）$\varOmega\left(\varTheta\right)$：正则化项，之所以要引入它是因为我们的目标是希望生成的模型能准确地预测新的样本（即应用于测试数据集），而不是简单地拟合训练集的结果（这样会导致过拟合）。所以需要在保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能。而正则化项就是用于惩罚复杂模型，避免模型过分拟合训练数据。常用的正则有$L1$正则与$L2$正则 $L1$正则（lasso）：$\varOmega\left(w\right)=\lambda ||w||_1$ $L2$正则：$\varOmega\left(w\right)=\lambda ||w||^2$ 这样目标函数的设计来自于统计学习里面的一个重要概念叫做Bias-variance tradeoff（偏差-方差权衡），比较感性的理解，$Bias$可以理解为假设我们有无限多数据的时候，可以训练出最好的模型所拿到的误差。而$Variance$是因为我们只有有限数据，其中随机性带来的误差。目标中误差函数鼓励我们的模型尽量去拟合训练数据，这样相对来说最后的模型会有比较少的$Bias$。而正则化项则鼓励更加简单的模型。因为当模型简单之后，有限数据拟合出来结果的随机性比较小，不容易过拟合，使得最后模型的预测更加稳定。 2.4 优化算法上面三个部分包含了机器学习的主要成分，也是机器学习工具划分模型比较有效的办法。其实这几部分之外，还有一个优化算法，就是给定目标函数之后怎么学的问题。有时候我们往往只知道“优化算法”，而没有仔细考虑目标函数的设计问题，比如常见的例子如决策树的学习算法的每一步去优化基尼系数，然后剪枝，但是没有考虑到后面的目标是什么。而这些启发式优化方法背后往往隐含了一个目标函数，理解了目标函数本身也有利于我们设计相应的学习算法。 三、回归树与树集成3.1 回归树在介绍$XGBoost$之前，首先得了解一下回归树和树集成的概念，其实在$AdaBoost$算法中已经详细讲述过这一部分了。Boosting Tree最基本的组成部分叫做回归树（regression tree），下面就是一个回归树的例子。它把输入根据输入的属性分配到各个叶子节点，而每个叶子节点上面都会有一个实数分数。具体地，下图给出了一个判断用户是否会喜欢电脑游戏的回归树模型，每个树叶的得分对应了该用户有多可能喜欢电脑游戏（分值越大可能性越大）。 3.2 树集成上图中的回归树只用到了用户年龄和性别两个信息，过于简单，预测的准确性自然有限。一个回归树往往过于简单无法有效地预测，因此一个更加强有力的模型叫做tree ensemble。在上图中使用两个回归树对用户是否喜欢电脑游戏进行了预测，并将两个回归树的预测结果加和得到单个用户的预测结果。在实际的预测模型建立过程中，我们通过不断地增加新的回归树，并给每个回归树赋予合适的权重，在此基础上综合不同的回归树得分获得更为准确的预测结果，这也就是树集成的基本思路。在预测算法中，随机森林和提升树都采用了树集成的方法，但是在具体地模型构造和参数调整的方法有所差别。 在这个树集成模型中，我们可以认为参数对应了树的结构，以及每个叶子节点上面的预测分数。 那么我们如何来学习这些参数。在这一部分，答案可能千奇百怪，但是最标准的答案始终是一个：定义合理的目标函数，然后去尝试优化这个目标函数。决策树学习往往充满了启发式算法，如先优化基尼系数，然后再剪枝，限制最大深度等等。其实这些启发式算法背后往往隐含了一个目标函数，而理解目标函数本身也有利于我们设计学习算法。 四、XGBoost的推导过程4.1 XGBoost的目标函数与泰勒展开对于tree ensemble，我们可以把某一个迭代后集成的模型写成为：\hat{y}_i=\sum_{k=1}^K{f_k\left(x_i\right)},\ f_k\in\mathscr{F}其中每个$f$是一个在函数空间($\mathscr{F}$)里面的函数，而$\mathscr{F}$对应了所有regression tree的集合。我们设计的目标函数也需要遵循前面的主要原则，包含两部分 Obj\left(\varTheta\right)=\sum_{i=1}^n{l\left(y_i,\hat{y}_i\right)}+\sum_{k=1}^K{\varOmega\left(f_k\right)}其中第一部分是训练损失，如上面所述的平方损失或者Logistic Loss等，第二部分是每棵树的复杂度的和。因为现在我们的参数可以认为是在一个函数空间里面，我们不能采用传统的如SGD之类的算法来学习我们的模型，因此我们会采用一种叫做additive training的方式。即每次迭代生成一棵新的回归树，从而使预测值不断逼近真实值（即进一步最小化目标函数）。每一次保留原来的模型不变，加入一个新的函数$f$到模型里面：其中$\hat{y}_i\left(t-1\right)$就是前$t-1$轮的模型预测，$f_t{(x_i)}$为新$t$轮加入的预测函数。这里自然就涉及一个问题：如何选择在每一轮中加入的$f(x_i)$呢？答案很直接，选取的$f(x_i)$必须使得我们的目标函数尽量最大地降低（这里应用到了Boosting的基本思想，即当前的基学习器重点关注以前所有学习器犯错误的那些数据样本，以此来达到提升的效果）。先对目标函数进行改写，表示如下：如果我们考虑平方误差作为损失函数，公式可改写为：更加一般的，对于不是平方误差的情况，我们可以采用如下的泰勒展开近似来定义一个近似的目标函数，方便我们进行下一步的计算。 泰勒展开一般表达式为：用泰勒展开来近似我们原来的目标：首先定义得到如果移除掉常数项，我们会发现这个目标函数有一个非常明显的特点，它只依赖于每个数据点的在误差函数上的一阶导数和二阶导数。可能有人会问，这个方式似乎比我们之前学过的决策树学习难懂。为什么要花这么多力气来做推导呢？ 这是因为，这样做首先有理论上的好处，它会使我们可以很清楚地理解整个目标是什么，并且一步一步推导出如何进行树的学习。然后这一个抽象的形式对于工程商实现机器学习工具也是非常有帮助的。因为它包含所有可以求到的目标函数，也就是说有了这个形式，我们写出来的代码可以用来求解包括回归、分类和排序的各种问题，正式的推导可以使得机器学习的工具更加一般化。 4.2 决策树的复杂度到目前为止我们讨论了目标函数中训练误差的部分。接下来我们讨论如何定义树的复杂度。我们先对于$f$的定义做一下细化，把树拆分成结构部分$q$和叶子权重部分$w$。其中结构部分$q$把输入映射到叶子的索引号上面去，而$w$给定了每个索引号对应的叶子分数是什么。当我们给定了如上定义之后，我们可以定义一棵树的复杂度如下。这个复杂度包含了一棵树里面节点的个数，以及每个树叶子节点上面输出分数的$L2$范数平方。当然这不是唯一的一种定义方式，不过这一定义方式学习出的树效果一般都比较不错。下图给出了复杂度计算的一个例子。 4.3 目标函数的最小化接下来是最关键的一步，在这种新的顶一下，我们可以把目标函数进行如下改写，其中$I$被定义为每个叶子上面样本集合$I_j=\{i| q(x_i)=j\}$这一目标包含了$T$个互相独立的单变量二次函数!我们可以定义那么这个目标函数可以进一步改写成如下的形式，假设我们已经知道树的结构$q$，我们可以通过这个目标函数来求解出最好的$w$，以及最好的$w$对应的目标函数最大的增益可以观察到上式是由$T$个相互独立的单变量二次函数再加上$L1$范数构成。这样的特性意味着单个树叶的权重计算与其他树叶的权重无关，所以我们可以非常方便计算第$j$个树叶的权重，以及目标函数。由此，我们将目标函数转换为一个一元二次方程求最小值的问题（在此式中，变量为$w_j$，函数本质上是关于$w_j$的二次函数），略去求解步骤，最终结果如下所示：乍一看目标函数的计算与回归树的结构$q$函数没有什么关系，但是如果我们仔细回看目标函数的构成，就会发现其中$G_j$和$H_j$的取值是由第$j$个树叶上数据样本所决定的。而第$j$个树上所具有的数据样本则是由树结构$q$函数决定的。也就是说，一旦回归树的结构$q$确定，那么相应的目标函数就能够根据上式计算出来。那么回归树的生成问题也就转换为找到一个最优的树结构$q$，使得它具有最小的目标函数。 计算求得的$Obj$代表了当指定一个树的结构的时候，目标函数上面最多减少多少。我们可以把它叫做结构分数（structure score）。可以把它认为是类似于基尼系数一样更加一般的对于树结构进行打分的函数。下面是一个具体的打分函数计算的例子，它根据决策树的预测结果得到各样本的梯度数据，然后计算出实际的结构分数。这个分数越小，代表这个树的结构越好： 4.4 枚举树的结果——贪心法在前面分析的基础上，当寻找到最优的树结构时，我们可以不断地枚举不同树的结构，利用这个打分函数来寻找一个最优结构的树，加入到我们的模型中，然后再重复这样的操作。不过枚举所有树结构这个操作不太可行，在这里XGBoost采用了常用的贪心法，即每一次尝试区队已有的叶子加入一个分割。对于一个剧透的分割方案，我们可以获得的增益可以由如下公式计算得到： 这个公式形式上跟ID3算法（采用信息熵计算增益）或者CART算法（采用基尼指数计算增益） 是一致的，都是用分裂后的某种值减去分裂前的某种值，从而得到增益。为了限制树的生长，我们可以加入阈值，当增益大于阈值时才让节点分裂，上式中的$\gamma$即阈值，它是正则项里叶子节点数T的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。另外，上式中还有一个系数$\lambda$，是正则项里leaf score的$L2$模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性。 对于每次扩展，我们还是要枚举所有可能的分割方案，那么如何高效地枚举所有的分割呢？假设需要枚举所有$x&lt;a$这样的条件，那么对于某个特定的分割$a$我们要计算$a$左边和右边的导数和，在实际应用中如下图所示： 我们可以发现对于所有的$a$，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度与$G_L$和$G_R$。然后用上面的公式计算每个分割方案的分数就可以了。 但需要注意是：引入的分割不一定会使得情况变好，因为在引入分割的同时也引入新叶子的惩罚项。所以通常需要设定一个阈值，如果引入的分割带来的增益小于一个阀值的时候，我们可以剪掉这个分割。此外在XGBoost的具体实践中，通常会设置树的深度来控制树的复杂度，避免单个树过于复杂带来的过拟合问题。 以上介绍了如何通过目标函数优化的方法比较严格地推导出boosted tree的学习的整个过程。因为有这样一般的推导，得到的算法可以直接应用到回归，分类排序等各个应用场景中去。 五、QA5.1 机器学习算法中GBDT和XGBOOST的区别有哪些？ 基分类器的选择：传统GBDT以CART作为基分类器，XGBoost还支持线性分类器，这个时候XGBoost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 二阶泰勒展开：传统GBDT在优化时只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，XGBoost工具支持自定义损失函数，只要函数可一阶和二阶求导。 方差-方差权衡：XGBoost在目标函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数$T$、每个叶子节点上输出分数的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性。 Shrinkage（缩减）：相当于学习速率（xgboost中的$\epsilon$）。XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率） 列抽样（column subsampling）：XGBoost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是XGBoost异于传统GBDT的一个特性。 缺失值处理：XGBoost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。即对于特征的值有缺失的样本，XGBoost可以自动学习出它的分裂方向。 XGBoost工具支持并行：Boosting不是一种串行的结构吗?怎么并行的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第$t$次迭代的损失函数里包含了前面$t-1$次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block(块)结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 线程缓冲区存储：按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer（缓冲区），主要是结合多线程、数据压缩、分片的方法，然后再计算，提高算法的效率。 可并行的近似直方图算法：树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。 5.2 为什么在实际的 kaggle 比赛中 gbdt 和 random forest 效果非常好？转载自知乎这是一个非常好，也非常值得思考的问题。换一个方式来问这个问题：为什么基于 tree-ensemble 的机器学习方法，在实际的 kaggle 比赛中效果非常好？ 通常，解释一个机器学习模型的表现是一件很复杂事情，而这篇文章尽可能用最直观的方式来解释这一问题。 我主要从三个方面来回答楼主这个问题。 理论模型 （站在 vc-dimension 的角度） 实际数据 系统的实现 （主要基于 xgboost） 通常决定一个机器学习模型能不能取得好的效果，以上三个方面的因素缺一不可。 5.2.1 站在理论模型的角度统计机器学习里经典的 vc-dimension 理论告诉我们：一个机器学习模型想要取得好的效果，这个模型需要满足以下两个条件： 模型在我们的训练数据上的表现要不错，也就是trainning error 要足够小。 模型的vc-dimension要低。换句话说，就是模型的自由度不能太大，以防overfit. 当然，这是我用大白话描述出来的，真正的 vc-dimension 理论需要经过复杂的数学推导，推出 vc-bound. vc-dimension 理论其实是从另一个角度刻画了一个我们所熟知的概念，那就是 bias variance trade-off. 好，现在开始让我们想象一个机器学习任务。对于这个任务，一定会有一个 “上帝函数” 可以完美的拟合所有数据（包括训练数据，以及未知的测试数据）。很可惜，这个函数我们肯定是不知道的 （不然就不需要机器学习了）。我们只可能选择一个 “假想函数” 来 逼近 这个 “上帝函数”，我们通常把这个 “假想函数” 叫做 hypothesis. 在这些 hypothesis 里，我们可以选择 svm, 也可以选择 logistic regression. 可以选择单棵决策树，也可以选择 tree-ensemble (gbdt, random forest). 现在的问题就是，为什么 tree-ensemble 在实际中的效果很好呢？ 区别就在于 “模型的可控性”。先说结论，tree-ensemble 这样的模型的可控性是好的，而像 LR 这样的模型的可控性是不够好的（或者说，可控性是没有 tree-ensemble 好的）。为什么会这样？别急，听我慢慢道来。 我们之前说，当我们选择一个 hypothsis 后，就需要在训练数据上进行训练，从而逼近我们的 “上帝函数”。我们都知道，对于 LR 这样的模型。如果 underfit，我们可以通过加 feature，或者通过高次的特征转换来使得我们的模型在训练数据上取得足够高的正确率。而对于 tree-enseble 来说，我们解决这一问题的方法是通过训练更多的 “弱弱” 的 tree. 所以，这两类模型都可以把 training error 做的足够低，也就是说模型的表达能力都是足够的。但是这样就完事了吗？没有，我们还需要让我们的模型的 vc-dimension 低一些。而这里，重点来了。在 tree-ensemble 模型中，通过加 tree 的方式，对于模型的 vc-dimension 的改变是比较小的。而在 LR 中，初始的维数设定，或者说特征的高次转换对于 vc-dimension 的影响都是更大的。换句话说，tree-ensemble 总是用一些 “弱弱” 的树联合起来去逼近 “上帝函数”，一次一小步，总能拟合的比较好。而对于 LR 这样的模型，我们很难去猜到这个“上帝函数”到底长什么样子（到底是2次函数还是3次函数？上帝函数如果是介于2次和3次之间怎么办呢？）。所以，一不小心我们设定的多项式维数高了，模型就 “刹不住车了”。俗话说的好，步子大了，总会扯着蛋。这也就是我们之前说的，tree-ensemble 模型的可控性更好，也即更不容易 overfitting. 5.2.2 站在数据的角度除了理论模型之外, 实际的数据也对我们的算法最终能取得好的效果息息相关。kaggle 比赛选择的都是真实世界中的问题。所以数据多多少少都是有噪音的。而基于树的算法通常抗噪能力更强。比如在树模型中，我们很容易对缺失值进行处理。除此之外，基于树的模型对于 categorical feature 也更加友好。 除了数据噪音之外，feature 的多样性也是 tree-ensemble 模型能够取得更好效果的原因之一。通常在一个kaggle任务中，我们可能有年龄特征，收入特征，性别特征等等从不同 channel 获得的特征。而特征的多样性也正是为什么工业界很少去使用 svm 的一个重要原因之一，因为 svm 本质上是属于一个几何模型，这个模型需要去定义 instance 之间的 kernel 或者 similarity （对于linear svm 来说，这个similarity 就是内积）。这其实和我们在之前说过的问题是相似的，我们无法预先设定一个很好的similarity。这样的数学模型使得 svm 更适合去处理 “同性质”的特征，例如图像特征提取中的 lbp 。而从不同 channel 中来的 feature 则更适合 tree-based model, 这些模型对数据的 distributation 通常并不敏感。 5.2.3 站在系统实现的角度除了有合适的模型和数据，一个良好的机器学习系统实现往往也是算法最终能否取得好的效果的关键。一个好的机器学习系统实现应该具备以下特征： 正确高效的实现某种模型。我真的见过有些机器学习的库实现某种算法是错误的。而高效的实现意味着可以快速验证不同的模型和参数。 系统具有灵活、深度的定制功能。 系统简单易用。 系统具有可扩展性, 可以从容处理更大的数据。 到目前为止，xgboost 是我发现的唯一一个能够很好的满足上述所有要求的 machine learning package. 在此感谢青年才俊 陈天奇。在效率方面，xgboost 高效的 c++ 实现能够通常能够比其它机器学习库更快的完成训练任务。在灵活性方面，xgboost 可以深度定制每一个子分类器，并且可以灵活的选择 loss function（logistic，linear，softmax 等等）。除此之外，xgboost还提供了一系列在机器学习比赛中十分有用的功能，例如 early-stop， cv 等等 在易用性方面，xgboost 提供了各种语言的封装，使得不同语言的用户都可以使用这个优秀的系统。 最后，在可扩展性方面，xgboost 提供了分布式训练（底层采用 rabit 接口），并且其分布式版本可以跑在各种平台之上，例如 mpi, yarn, spark 等等。 有了这么多优秀的特性，自然这个系统会吸引更多的人去使用它来参加 kaggle 比赛。 综上所述，理论模型，实际的数据，良好的系统实现，都是使得 tree-ensemble 在实际的 kaggle 比赛中“屡战屡胜”的原因]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>XgBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（7）：GBDT]]></title>
    <url>%2F2017%2F01%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%887%EF%BC%89%EF%BC%9AGBDT%2F</url>
    <content type="text"><![CDATA[一、引言GBDT（Gradient Boosting Decision Tree）是一种迭代的决策树算法，又叫 MART（Multiple Additive Regression Tree)，它通过构造一组弱的学习器（树），并把多颗决策树的结果累加起来作为最终的预测输出。该算法将决策树与集成思想进行了有效的结合。 GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合。自算法的诞生之初，它就和SVM一起被认为是泛化能力（generalization）较强的算法。近些年来更因为被用于构建搜索排序的机器学习模型而引起广泛的关注。它最早见于yahoo，后被广泛应用在搜索排序、点击率预估上。业界中，Facebook使用其来自动发现有效的特征、特征组合，来作为LR模型中的特征，以提高 CTR预估（Click-Through Rate Prediction）的准确性；GBDT在淘宝的搜索及预测业务上也发挥了重要作用。 除此之外，GBDT还是目前竞赛中最为常用的一种机器学习算法，因为它不仅可以适用于多种场景，而且相比较于其他算法还有着出众的准确率，如此优异的性能也让GBDT收获了机器学习领域的“屠龙刀”这一赞誉。 本文首先介绍GBDT中的DT，即回归树，这是它的基础算法；然后叙述提升树，它是以决策树为基函数的提升方法；接着介绍GBDT中的GB，即梯度提升；最后导出GBDT算法的整个流程。 二、Regression Desicion Tree：回归树2.1 回归树简介树模型也分为决策树和回归树，决策树常用来分类问题，回归树常用来预测问题。决策树常用于分类标签值，比如用户性别、网页是否是垃圾页面、用户是不是作弊；而回归树常用于预测真实数值，比如用户的年龄、用户点击的概率、网页相关程度等等。 回归树总体流程类似于分类树，区别在于，回归树的每一个节点都会得到一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值寻找最优切分变量和最优切分点，但衡量的准则不再是分类树中的基尼系数，而是平方误差最小化。也就是被预测错误的人数越多，平方误差就越大，通过最小化平方误差找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 由于GBDT的核心在与累加所有树的结果作为最终结果，而分类树得到的离散分类结果对于预测分类并不是这么的容易叠加（稍等后面会看到，其实并不是简单的叠加，而是每一步每一棵树拟合的残差和选择分裂点评价方式都是经过公式推导得到的），而对基于回归树所得到的数值进行加减是有意义的（例如10岁+5岁-3岁=12岁），这是区别于分类树的一个显著特征（毕竟男+女=是男是女?，这样的运算是毫无道理的），GBDT在运行时就使用到了回归树的这个性质，它将累加所有树的结果作为最终结果。所以GBDT中的树都是回归树，而不是分类树，它用来做回归预测，当然回归树经过调整之后也能用来做分类。 2.2 回归树的生成首先看一个简单的回归树生成实例： 接下来具体说说回归树是如何进行特征选择生成二叉回归树的。假设$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集 D=\{(x_1,y_1),(x_2,y_2),···,(x_N,y_N)\}我们利用最小二乘回归树生成算法来生成回归树$f(x)$，即在训练数据集所在的输入空间中，递归地将每个区域分为两个子区域并决定每个子区域上的输出值，构建二叉决策树，步骤如下： 1）选择最优切分变量$j$与切分点$s$，求解 \min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1\left(j,s\right)}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2\left(j,s\right)}{\left(y_i-c_2\right)^2}\right]遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值得对$j,s$ 2）用选定的对$(j,s)$划分区域并决定相应的输出值： R_1\left(j,s\right)=\left\{x|x^{\left(j\right)}\le s\right\}\ ,\ R_2\left(j,s\right)=\left\{x|x^{\left(j\right)}>s\right\} \hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_2\left(j,s\right)}{y_i}\ ,\ x\in R_m\ ,\ m=1,2 3）继续对两个子区域调用步骤（1），（2），直至满足停止条件。 4）将输入空间划分为$M$个区域$R_1,R_2,···,R_M$，在每个单元$R_m$上有一个固定的输出值$c_m$，生成决策树： f\left(x\right)=\sum_{m=1}^M{\hat{c}_m\textrm{I}\left(\textrm{x}\in\textrm{R}_{\textrm{m}}\right)} 三、Boosting Decision Tree：提升树3.1 提升树模型提升方法采用加法模型（即基函数的线性组合）与前向分布算法。以决策树为基函数的提升方法称为提升树（Boosting tree）。对分类问题构建的决策树是二叉分类树，对回归问题构建决策树是二叉回归树。提升树是迭代多棵回归树来共同决策。当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差 = 真实值 - 预测值 。提升树即是整个迭代过程生成的回归树的累加。提升树模型可以表示为决策树的加法模型： f_M\left(x\right)=\sum_{m=1}^M{T\left(x;\varTheta_m\right)}其中$T\left(x;\varTheta_m\right)$表示决策树；$\varTheta_m$为决策树的参数；$M$为树的个数。 3.2 提升树算法对回归问题的提升树算法来说，给定当前模型 $f_{m-1}{(x)}$只需要简单地拟合当前模型的残差。现将回归问题的提升树算法叙述如下： 1）初始化$f_0{(x)}=0$ 2）对$m=1,2,···,M$ a）计算残差r_{mi}=y_i-f_{m-1}\left(x_i\right)\ ,\ i=1,2,···,N b）拟合残差$r_{mi}$学习一个回归树，得到 $T\left(x;\varTheta_m\right)$ c）更新$f_m{(x)}=f_{m-1}{(x)}+T(x;\varTheta_m )$ 3）得到回归问题提升树 f_M\left(x\right)=\sum_{m=1}^M{T\left(x;\varTheta_m\right)} 接下来通过训练一个用于预测年龄的模型来展现算法的运行流程 1）首先，训练集有4个人$A,B,C,D$，它们的年龄分别是$14,16,24,26$，其中$A,B$分别是高一和高三学生；$C,D$分别是应届毕业生和工作两年的员工，可用于分枝的特征包括上网时长、购物金额、上网时段和对百度知道的使用方式。如果是一棵传统的回归决策树来训练，会得到下图所示结果： 2）但是如果用GBDT来做这件事，由于数据太少，我们限定叶子节点最多有两个，即每棵树都只有一个分枝，并且限定只限定两棵树。我们会得到如下所示结果：第一棵树的分枝与之前一样，也是使用购物金额进行区分，两拨人各自用年龄均值作为预测值，得到残差值-1、1、-1、1，然后拿这些残差值替换初始值去训练生成第二棵回归树，如果新的预测值和残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。第一棵树的分枝与之前一样，也是使用购物金额进行区分，两拨人各自用年龄均值作为预测值，得到残差值-1、1、-1、1，然后拿这些残差值替换初始值去训练生成第二棵回归树，如果新的预测值和残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。第二棵树只有两个值1和-1，直接可分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。 3）将两棵回归树预测结果进行汇总，解释如下： A：14岁高一学生；购物较少；经常问学长问题；预测年龄A = 15 – 1 = 14 B：16岁高三学生；购物较少；经常被学弟问问题；预测年龄B = 15 + 1 = 16 C：24岁应届毕业生；购物较多，经常问师兄问题；预测年龄C = 25 – 1 = 24 D：26岁工作两年员工；购物较多，经常被师弟问问题；预测年龄D = 25 + 1 = 26 对比初始的回归树与GBDT所生成的回归树，可以发现，最终的结果是相同的，那我们为什么还要使用GBDT呢？ 答案就是对模型过拟合的考虑。过拟合是指为了让训练集精度更高，学到了很多“仅在训练集上成立的规律”，导致换一个数据集后，当前规律的预测精度就不足以使人满意了。毕竟，在训练精度和实际精度（或测试精度）之间，后者才是我们想要真正得到的。 在上面这个例子中，初始的回归树为达到100%精度使用了3个特征（上网时长、时段、网购金额），但观察发现，分枝“上网时长&gt;1.1h”很显然过拟合了，不排除恰好A上网1.5h, B上网1小时，所以用上网时间是不是&gt;1.1小时来判断所有人的年龄很显然是有悖常识的。 而在GBDT中，两棵回归树仅使用了两个特征（购物金额与对百度知道的使用方式）就实现了100%的预测精度，其分枝依据更合乎逻辑（当然这里是相比较于上网时长特征而言），算法在运行中也体现了“如无必要，勿增实体”的奥卡姆剃刀原理。 3.3 提升树实例下表为训练数据，$x$的取值范围为区间$[0.5,10.5]$，$y$的取值范围为区间$[5.0,10.0]$，学习这个回归问题的提升树模型，考虑只用二叉树作为基函数：（1）步骤一：求$f_1(x)$即回归树$T_1(x)$ 1）首先通过以下优化问题： \min_s\left[\min_{c_1}\sum_{x_i\in R_1}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2}{\left(y_i-c_2\right)^2}\right]求解训练数据的切分点$s$： R_1=\left\{x|x\le s\right\},R_2=\left\{x|x>s\right\}容易求得在$R_1$，$R_2$内部使平方误差达到最小值的$c_1,c_2$为 c_1=\frac{1}{N_1}\sum_{x_i\in R_1}{y_i}\\ ,\\ c_2=\frac{1}{N_2}\sum_{x_i\in R_2}{y_i}这里$N_1,N_2$是$R_1,R_2$的样本点数。 2）具体地，求解训练数据的切分点。根据所给数据，考虑如下切分点： 1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5对各切分点，不难求出相应的$R_1,R_2,c_1,c_2$及 m\left(s\right)=\min_{c_1}\sum_{x_i\in R_1}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2}{\left(y_i-c_2\right)^2}例如，当$s=2.5$时， R_1=\{1,2\}，R_2=\{3,4,···,9,10\}，c_1=5.63,c_2=7.73 m\left(s\right)=\min_{c_1}\sum_{x_i\in R_1}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2}{\left(y_i-c_2\right)^2}=12.07遍历所有的$s$，计算$m(s)$，结果列表如下：可知当$s=6.5$时$m(s)$达到最小值，此时 R_1=\{1,2,···,6\},R_2=\{7,8,9,10\},c_1=6.24,c_2=8.91所以回归树$T_1(x)$为 T_2\left(x\right)=\left\{\begin{matrix} 6.24& x]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（6）：AdaBoost]]></title>
    <url>%2F2017%2F01%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%886%EF%BC%89%EF%BC%9AAdaBoost%2F</url>
    <content type="text"><![CDATA[一、集成学习1.1 定义所谓集成学习（ensemble learning），是指通过构建多个弱学习器，然后结合为一个强学习器来完成分类任务。并相较于弱分类器而言，进一步提升结果的准确率。严格来说，集成学习并不算是一种分类器，而是一种学习器结合的方法。 下图显示了集成学习的整个流程：首次按产生一组“个体学习器”，这些个体学习器可以是同质的（homogeneous）（例如全部是决策树），这一类学习器被称为基学习器（base learner），相应的学习算法称为“基学习算法”；集成也可包含不同类型的个体学习器（例如同时包含决策树和神经网络），这一类学习器被称为“组件学习器”（component learner）。 集成学习通过将多个学习器进行结合，可获得比单一学习器显著优越的泛化性能，它基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好，直观一点理解，就是我们平时所说的“三个臭皮匠，顶个诸葛亮”，通过使用多个决策者共同决策一个实例的分类从而提高分类器的泛化能力。 1.2 集成学习的条件当然，这种通过集成学习来提高学习器（这里特指分类器）的整体泛化能力也是有条件的： 首先，分类器之间应该具有差异性，即要有“多样性”。很容易理解，如果使用的是同一个分类器，那么集成起来的分类结果是不会有变化的。‘ 其次，每个个体分类器的分类精度必须大于0.5，如果$p&lt;0.5$那么随着集成规模的增加，分类精度会下降；但如果是大于0.5的话，那么最后最终分类精度是可以趋于1的。 因此，要获得好的集成，个体学习器应该“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”，即学习器间具有差异。 1.3 集成学习的分类当前，我们可以立足于通过处理数据集生成差异性分类器，即在原有数据集上采用抽样技术获得多个训练数据集来生成多个差异性分类器。根据个体学习器的生成方式，目前集成学习方法大致可分为两大类：第一类是个体学习器之间存在强依赖关系、必须串行生成的序列化方法，这种方法的代表是“Boosting”；第二类是个体学习器间不存在强依赖关系、可同时生成的并行化方法，它的代表是“Bagging”和“Random Forest” Bagging：通过对原数据进行有放回的抽取，构建出多个样本数据集，然后用这些新的数据集训练多个分类器。因为是有放回的采用，所以一些样本可能会出现多次，而其他样本会被忽略。该方法是通过降低基分类器方法来改善泛化能力，因此Bagging的性能依赖于基分类器的稳定性，如果基分类器是不稳定的，Bagging有助于减低训练数据的随机扰动导致的误差，但是如果基分类器是稳定的，即对数据变化不敏感，那么Bagging方法就得不到性能的提升，甚至会降低。 Boosting：提升方法是一个迭代的过程，通过改变样本分布，使得分类器聚集在那些很难分的样本上，对那些容易错分的数据加强学习，增加错分数据的权重，这样错分的数据再下一轮的迭代就有更大的作用（对错分数据进行惩罚）。 Bagging与Boosting的区别： 二者的主要区别是取样方式不同。Bagging采用均匀取样，而Boosting根据错误率来取样，因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各轮训练集之间相互独立，而Boostlng的各轮训练集的选择与前面各轮的学习结果有关；Bagging的各个预测函数没有权重，而Boosting是有权重的；Bagging的各个预测函数可以并行生成，而Boosting的各个预测函数只能顺序生成。对于象神经网络这样极为耗时的学习方法。Bagging可通过并行训练节省大量时间开销。 bagging是减少variance，而boosting是减少bias。Bagging 是 Bootstrap Aggregating 的简称，意思就是再取样 (Bootstrap) 然后在每个样本上训练出来的模型取平均，所以是降低模型的 variance. Bagging 比如 Random Forest 这种先天并行的算法都有这个效果。Boosting 则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行加权，所以随着迭代不断进行，误差会越来越小，所以模型的 bias 会不断降低。这种算法无法并行。 二、AdaBoost算法2.1 AdaBoost算法思想对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确地分类规则（强分类器）容易得多。提升算法就是从弱学习算法出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。 这样，对提升方法来说，有两个问题需要回答：一是在每一轮如果改变训练数据的权值或概率分布；二是如何将弱分类器组合成一个强分类器。对于第一个问题，AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注，于是，分类问题就被一系列的弱分类器“分而治之”。至于第二个问题，即弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率较大的弱分类器的权值，使其在表决中起较小的作用。 AdaboostBoost的算法的框架如下图所示具体来说，整个AdaBoost算法包括以下三个步骤： 1）初始化训练样本的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：$1/N$。 2）训练弱分类器。具体训练过程中，如果某个样本已经被准确地分类，那么在构造下一个训练集中，它的权值就会被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本被用于训练下一个分类器，整个训练过程如果迭代地进行下去，使得分类器在迭代过程中逐步改进。 3）将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中权重较大，否则较小。得到最终分类器。 2.2 AdaBoost算法流程现在叙述AdaBoost算法。假定给定一个二类分类的训练数据集 T=\{(x_1,y_1),(x_2,y_2),···,(x_n,y_n)\}其中$y_i$属于二分类的标记组合，即$y_i\in\{+1,-1\}$，AdaBoost算法利用一下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合成一个强分类器。 步骤一：首先，初始化训练数据的权值分布。假设每一个训练样本最开始时都被赋予相同的权值：$1/N$，即每个训练样本在基本分类器的学习中作用相同，这一假设保证步骤一能够在原始数据上学习基本分类器$G_1{(x)}$，数学化的语言表示为： D_1=\left(w_{11},w_{12},···,w_{1i},···,w_{1N}\right)\ ,\ w_{1i}=\frac{1}{N}\ ,i=1,2,···,N步骤二：AdaBoost反复学习基本分类器，在每一轮$m=1,2,···,M$顺次执行下列操作： 1）使用当前权值分布为$D_m$的训练数据集，学习得到基分类 G_m\left(x\right):\chi\rightarrow\left\{-1,+1\right\} 2）计算上一步得到的基分类器$G_m{(x)}$在训练数据集上的分类误差率$e_m$为 e_m=P\left(G_m\left(x\right)\ne y_i\right)=\frac{\sum_{i=1}^N{w_{mi}I\left(G_m\left(x_i\right)\ne y_i\right)}}{\sum_{i=1}^N{w_{mi}}}=\sum_{i=1}^N{w_{mi}I\left(G_m\left(x_i\right)\ne y_i\right)}这里$w_{mi}$表示第$m$轮中第$i$个实例的权值，$\sum_{i=1}^N{w_{mi}=1}$。这表明，$G_m{(x)}$在加权的训练数据集上的分类误差率是被$G_m{(x)}$误分类样本的权值之和，由此可以看出数据权值分布$D_m$与基本分类器$G_m{(x)}$的分类误差率的关系。 3）计算$G_m$前面的权重系数$a_m$，该系数表示$G_m$在最终分类器中的重要程度，目的在于使我们得到基分类器在最终分类器中所占的权值，系数计算公式如下： a_m=\frac{1}{2}\log\frac{1-e_m}{e_m}这里的对数是自然对数，由表达式可知，当$e_m≤\frac{1}{2}$时，$a_m≥0$，并且$a_m$随着$e_m$的减小而增大，意味着分类误差越小的基本分类器在最终分类器的作用越大，而$e_m≥\frac{1}{2}$则刚好相反，这正好验证了集成学习中每个个体分类器的分类精度必须大于0.5的前提条件。 4）更新训练数据集的权值分布为下一轮作准备 D_{m+1}=\left(w_{m+1,1},w_{m+1,2},···,w_{m+1,i},···,w_{m+1,N}\right)\,\,其中 w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right),i=1,2,···,N我们也可以写成： w_{m+1,i}=\left\{\begin{matrix} \frac{w_{mi}}{Z_m}e^{-a_m}\ ,& G_m\left(x_i\right)=y_i\\ \frac{w_{mi}}{Z_m}e^{a_m}\ ,& G_m\left(xi\right)\ne y_i\\ \end{matrix}\right.由此可知，被基本分类器$G_m{(x)}$误分类样本的权值得以扩大，而被正确分类样本的权值得以缩小。两两比较，误分类样本的权值$e^{2a_m}=\frac{e_m}{1-e_m}$倍。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作业，这是AdaBoost的一个特点。这里我们还引入了一个规范化因子，它的作用在于使$D_{m+1}$成为一个概率分布。具体公式为Z_m=\sum_{i=1}^N{w_{mi}}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right),i=1,2,···,N重复步骤二中的1至4步骤，得到一系列的权重参数$a_m$和基分类器$G_m$。 步骤三：将上一步得到的基分类器根据权重参数线性组合 f\left(x\right)=\sum_{m=1}^M{a_mG_m\left(x\right)}得到最终分类器$G_{(x)}$ G\left(x\right)=sign\left(f\left(x\right)\right)=sign\left(\sum_{m=1}^M{a_mG_m\left(x\right)}\right)线性组合$f(x)$实现了$M$个基本分类器的加权表决。系数$a_m$表示了基本分类器$G_m{(x)}$的重要性，这里，所有的$a_m$之和并不为1。$f(x)$的符号决定实例$x$的类，$f(x)$的绝对值表示分类的确信度，利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点。 2.3 AdaBoost算法的一个实例下图为给定的训练样本，假设$Y\in \{+1,-1\}$，且弱分类器由$xv$产生（$v$为阈值，目的在于使分类器在训练样本上的分类误差率最低），接下来我们就要使用AdaBoost算法得到一个强分类器。 首先，初始化训练数据的权值分布，得到： D_1=(w_{11},w_{12},w_{1,10}) , w_{1i}=\frac{1}{10},i=1,2,····,10 在此基础上，开始M轮迭代。 根据X和Y的对应关系，要把这10个数据分为两类，一类是1，一类是-1，根据数据的特点发现：$（0，1，2，6，7，8）$对应的类是1，$(3,4,5,9)$对于的类是-1，抛开孤独的9不说，$(0,1,2),(3,4,5),(6,7,8)$这是3类不同的数据，分别对应的类是$(1,-1,1)$,直观上推测可知，可以找到对应的数据分界点，比如$2.5、5.5、8.5$，将这几类数据分成两类。 1.第一次迭代（m=1）: 1）在第一次迭代时，已知$w_{1i}=\frac{1}{10}$，经过计算可得：在权值分布为$D_1$的训练数据上，阈值$v$取2.5或8.5时分类误差率为0.3，取5.5时分类误差率为0.4，遵循分类误差率最低原则，从2.5或8.5 中任意选取一个阈值，这里选取2.5，故基本分类器为G_1\left(x\right)=\left\{\begin{matrix}1&x2.5\\\end{matrix}\right. 2）$G_1{(x)}$在训练集上的误差率：e_1=P(G_1{(x_i)≠y_i})=0.3 3) 根据$e_1$计算得到$G_1{(x)}$的系数：a_1=\frac{1}{2}\log\frac{1-e_1}{e_1}=0.4236这个系数就代表$G_1{(x)}$在最终的分类函数中所占的权值。 4）更新训练数据的权值分布，用于下一轮迭代 D_{m+1}=\left(w_{m+1,1},w_{m+1,2},···,w_{m+1,i},···,w_{m+1,N}\right)\,\,其中 w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right),i=1,2,···,N由此得到$D_2=(0.0715,0.0715,0.715,0.0715,0.0715,0.715,0.166,0.166,0.166,0.0715)$根据$D_2$可知，分对的样本权重由0.1下降到了0.0715，分错的样本$(6,7,8)$的权值由0.1上升至0.166。此时分类函数为$f_1{(x)}=0.4236G_1{(x)}$，第一个分类器$sign[f_1{(x)}]$在训练样本上有三个误分类点（第一轮的误分类点即第一个基分类器的误分类点）。 2.第二次迭代（m=2）: 1）在上一轮迭代中，我们获知了新一轮的权重分布$D_2$，在此基础上，经过计算可得，阈值$v$是8.5时分类误差率最低，因此第二个基本分类器为G_2\left(x\right)=\left\{\begin{matrix}1&x8.5\\\end{matrix}\right. 2）误差率：e_2=P(G_2{(x_i)≠y_i})=0.0715×3=0.2143 3）$G_2{(x)}$的系数为:a_2=\frac{1}{2}\log\frac{1-e_2}{e_2}=0.6496 4）更新训练样本的权值分布，得到$D_3=(0.0455,0.0455,0.0455,0.1667，0.1667,0.1667,0.1060,0.1060,0.1060,0.0455)$，相较于$D_2$，被分错的样本3，4，5的权值变大，其他被分对的样本的权值变小。经过第二轮迭代后，分类函数为$f_2{(x)}=0.4236G_1{(x)}+0.6496G_2{(x)}$，第二个分类器为$sign[f_2{(x)}]=sign[0.4236G_1{(x)}+0.6496G_2{(x)}]$。将10个样本点依次带入到第二个分类器中，可得到此时依然有着3个误分类点$(3,4,5)$，为此需要进行下一轮迭代。 3.第三次迭代（m=3）: 1）在上一轮迭代中，我们获知了新一轮的权重分布$D_3$，在此基础上，经过计算可得，阈值$v$是5.5时分类误差率最低，因此第三个基本分类器为 G_3\left(x\right)=\left\{\begin{matrix}{} 1& x>5.5\\ -1& x]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（5）：随机森林]]></title>
    <url>%2F2017%2F01%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%885%EF%BC%89%EF%BC%9A%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[一、基本原理顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。 我们可以这样比喻随机森林算法：每一棵决策树就是一个精通于某一个窄领域的专家（因为我们从M个特征中选择m个让每一棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。 下图为随机森林算法的示意图： 随机森林算法有很多优点： 在数据集上表现良好，两个随机性的引入，使得随机森林不容易陷入过拟合 在当前的很多数据集上，相对其他算法有着很大的优势，两个随机性的引入，使得随机森林具有很好的抗噪声能力 它能够处理很高维度（feature很多）的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化 可生成一个$Proximities=（p_{ij}）$矩阵，用于度量样本之间的相似性： $p_{ij}=a_{ij}/N$,$ a_{ij}$表示样本i和j出现在随机森林中同一个叶子结点的次数，N随机森林中树的颗数 在创建随机森林的时候，对generlization error使用的是无偏估计 训练速度快，可以得到变量重要性排序（两种：基于OOB误分率的增加量和基于分裂时的GINI下降量 在训练过程中，能够检测到feature间的互相影响 容易做成并行化方法 实现比较简单 二、随机森林的生成2.1 生成步骤步骤如下： 1）如果训练集大小为$N$，对于每棵树而言，随机且有放回地从训练集中抽取$N$个训练样本（bootstrap抽样方法），作为该树的训练集；每棵树的训练集都是不同的，但里面包含重复的训练样本 2）如果每个样本的特征维度为$M$，指定一个常数$m$，且$m$&lt;$M$，随机地从$M$个特征中选取$m$个特征子集，每次树进行分裂时，从这$m$个特征中选择最优的； 3）每棵树都尽可能最大程度地生长，并且没有剪枝过程。 2.2 影响分类效果的参数随机森林的分类效果（即错误率）与以下两个因素有关： 1）森林中任意两棵树的相关性：相关性越大，错误率越大 2）森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低 减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。 2.3 袋外误差率如何选择最优的特征个数m，要解决这个问题，我们主要依据计算得到的袋外错误率oob error（out-of-bag error）。 对于一个样本，它在某一次含m个样本的训练集的随机采样中，每次被采集到的概率是$\frac{1}{m}$。不被采集到的概率为$1-\frac{1}{m}$。如果m次采样都没有被采集中的概率是$(1−\frac{1}{m})^m$，当$m→∞$时，$(1−\frac{1}{m})m→\frac{1}{e}≃0.368$。也就是说，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。对于这部分大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。 袋外错误率（oob error）计算方式如下： 1）对每个样本计算它作为oob样本的树对它的分类情况 2）以简单多数投票作为该样本的分类结果 3）最后用误分个数占样本总数的比率作为随机森林的oob误分率 三、随机采样与完全分裂 在建立每一棵决策树的过程中，有两点需要注意，分别是采样与完全分裂。 3.1 随机采样首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M个feature中，选择m个(m &lt;&lt; M)。 3.1.1 有放回抽样的解释如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是”有偏的”，都是绝对”片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是”求同”，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是”盲人摸象”。 3.1.2 对Bagging的改进随机森林对Bagging的改进就在于随机采用的不同，即以下两点： 1）Random forest是选与输入样本的数目相同多的次数（可能一个样本会被选取多次，同时也会造成一些样本不会被选取到），而bagging一般选取比输入样本的数目少的样本； 2）bagging是用全部特征来得到分类器，而Random forest是需要从全部特征中选取其中的一部分来训练得到分类器； 一般Random forest效果比bagging效果好！ 3.2 完全分裂之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。一般很多的决策树算法都一个重要的步骤 - 剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。 按这种算法得到的随机森林中的每一棵都是很弱的，但是大家组合起来就很厉害了。 四、随机森林的变体也可以使用SVM、Logistic回归等其他分 类器，习惯上，这些分类器组成的“总分类器”，仍然叫做随机森林。 比如回归问题，图中离散点为臭氧(横轴)和温度(纵轴)的关系，试拟合变化曲线，记原始数据为D，长度为N(即图中有N个离散点) 算法过程为： 1）做100次bootstrap，每次得到的数据Di，Di的长度为N 2）对于每一个Di，使用局部回归(LOESS)拟合一条曲线(图 中灰色线是其中的10条曲线) 3）将这些曲线取平均，即得到红色的最终拟合曲线 4）显然，红色的曲线更加稳定，并且没有过拟合明显减弱]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>随机森林</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（4）：决策树]]></title>
    <url>%2F2017%2F01%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[本文结合李航博士的《统计学习方法》与周志华老师的《机器学习》决策树部分，《统计学习方法》重理论的证明推导，《机器学习》注重讲解算法的特点与扩展。 INTRODUCTION决策树（Decision Tree）是数据挖掘中一种基本的分类和回归方法，它呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，可以认为是$if-then$规则的集合，也可认为是定义在特征空间与类空间上的条件概率分布。下图是一个简单的决策树示例： 决策树模型的主要优点是模型具有可读性，分类速度快。在学习时，利用训练数据，根据损失函数最小化原则建立决策树模型；而在预测时，对新的数据，利用决策树模型进行分类。主要的决策树算法有ID3算法、C4.5算法和CART算法。 一个性能良好的决策树，是一个与训练数据矛盾较小的决策树，同时又具有很好地泛化能力。言外之意就是说，好的决策树不仅对训练样本有很好的分类效果，对于测试集也有较低的误差率。一个决策树的学习过程包括三个步骤：特征选择、决策树的生成以及决策树的修剪。 一、决策树模型的两种解释1.1 决策树模型分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶节点。内部结点表示一个特征或属性，叶节点表示一个类。 1.1.1 决策树与if-then规则可以将决策树看成一个if-then规则的集合。即由决策树的根结点到叶节点的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。 决策树的路径或其对应的if-then规则集合的重要性质：互斥且完备（每一个实例都被一条路径或一条规则所覆盖，且只被一条路径或一条规则所覆盖，这里的覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件） 1.1.1 决策树与条件概率分布决策树还表示给定特征条件下类的条件概率分布，它定义在特征空间的一个划分。将特征空间划分为互不相交的单元，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的每一条路径对应于划分中的一个单元。 假设$X$为表示特征的随机变量，$Y$为表示类的随机变量，那么这个条件概率分布可以表示为$P(X|Y)$,各叶结点上的条件概率往往偏向于某一个类，即属于某一类的概率越大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。 二、特征选择2.1 特征选择问题若利用一个特征进行分类的结果与随机分类的结果没有很大差异，则称这个特征是没有分类能力的。特征选择的准则是信息增益或信息增益比。直观上，若一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割为子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益可以表示这一直观的准则。 2.2 信息增益2.2.1 熵在信息论与概率统计中，熵表示随机变量不确定性的度量。设$X$是一个取有限个值得离散随机变量，其概率分布为 P\left( X=x_i \right) =p_i,i=1,2,···,n则随机变量$X$的熵定义为 H\left( X \right) =-\sum_{i=1}^n{p_i\log p_i}若$p_i$等于0，定义$0log0=0$，熵的单位为比特或者纳特。 2.2.2 条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望 H\left( Y|X \right) =\sum_{i=1}^n{p_iH\left( Y|X=x_i \right)}经验熵和经验条件熵：当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵和条件经验熵。 2.2.3 信息增益信息增益表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度。特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即 g\left( D,A \right) =H\left( D \right) -H\left( D|A \right)一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。 于是我们可以应用信息增益准则来选择特征，信息增益表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。对数据集$D$而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。 2.2.4 信息增益算法根据信息增益准则的特征选择方法为对训练数据集（或子集）$D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。 在描述算法前，先对符号进行说明：设训练数据集为$D$，$|D|$表示其样本容量，即样本个数。设有$K$个类$C_k$,$k=1,2,···,K$,$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^K{|C_k|=|D|}$。设特征$A$有$n$个不同的取值${a_1,a_2,···,a_n}$,根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,···,D_n$,$|D_i|$为$D_i$的样本个数，$\sum_{i=1}^n{|D_i|=|D|}$。记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$,即$D_{ik}=D_i\cap{C_k}$,$D_{ik}$为$D_{ik}$的样本个数。 具体算法步骤如下： 1）计算数据集$D$的经验熵$H(D)$H\left( D \right) =-\sum_{k=1}^K{\frac{|C_k|}{|D|}\log _2\frac{|C_k|}{|D|}} 2）计算特征$A$对数据集$D$的经验条件熵$H(D|A)$H\left( D|A \right) =\sum_{i=1}^n{\frac{|D_i|}{|D|}H\left( D_i \right)}=-\sum_{i=1}^n{\frac{|D_i|}{|D|}\sum_{k=1}^K{\frac{|D_{ik}|}{|D_i|}\log _2\frac{|D_{ik}|}{|D_i|}}} 3）计算信息增益g\left( D,A \right) =H\left( D \right) -H\left( D|A \right) 2.3 信息增益比以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。 信息增益比表示特征$A$对训练数据集$D$的信息增益比。$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比，即 g_R\left( D,A \right) =\frac{g\left( D,A \right)}{H_A\left( D \right)}2.4 基尼系数分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼系数定义为 Gini\left( p \right) =\sum_{k=1}^K{p_k\left( 1-p_k \right) =1-\sum_{k=1}^K{p_k^2}}若样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分，即 D_1=\left\{ \left( x,y \right) \in D|A\left( x \right) =0 \right\} \mathrm{，}D_2=D-D_1则在特征A的条件下，集合$D$的基尼指数定义为 Gini\left( D,A \right) =\frac{|D_1|}{|D|}Gini\left( D_1 \right) +\frac{|D_2|}{|D|}Gini\left( D_2 \right)基尼系数Gini(D)表示集合$D$的不确定性，表示经A=a分割后集合D的不确定性。基尼系数越大，样本集合的不确定性越大，与熵类似。 从下图可以看出基尼指数和熵之半的曲线很接近，都可以近似地代表分类误差率。 三、决策树的生成3.1 ID3算法ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地建构决策树。 其具体方法为：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。ID3相当于用极大似然法进行概率模型的选择。但是ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。 其算法步骤如下： 1） 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将类$C_k$作为该结点的类标记，返回$T$; 2）若$A=\varnothing $,则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$; 3） 否则，按算法5.1计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$; 4） 如果$A_g$的信息增益小于阈值$\varepsilon $，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$ 5） 否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$,将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子节点构成树$T$,返回$T$ 6） 对第i个子结点，以$D_i$为训练集,以$A-{A_g}$为特征集，递归地调用（1）~（5），得到子树$T_i$，返回$T$。 3.2 C4.5与ID3算法相似，C4.5算法对ID3算法进行了改进，C4.5在生成的过程中，用信息增益比来选择特征 3.3 CART分类树与回归树（classification and regression tree，CART）模型（Breiman）由特征选择、树生成及剪枝组成，既可用于分类也可用于回归。CART是在给定输入随机变量X条件下输出变量Y的条件概率分布的学习方法。它假定决策树是二叉树，内部取值为“是”（左分支）和“否”（右分支）。它的基本步骤为 1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大。 2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这是用损失函数最小作为剪枝的标准。 3.3.1 分类树对分类树用基尼系数（Gini index）最小化准则，进行特征选择，生成二叉树。 具体算法步骤如下： 1）设结点的训练数据集为D，计算现有特征对该数据集的基尼指数。此时，对每一个特征A，对其可能取的每个值$a$，根据样本点对$A=a$的测试为”是”或者“否”将D分割为$D_1$和$D_2$两部分，计算其基尼系数。 2）在所有可能的特征A以及他们所有可能的切分点$a$中，选择基尼系数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。 3）对两个子结点递归地调用上述两个步骤，直至满足停止条件。 4）生成CART决策树 3.3.2 回归树首先看一个简单的回归树生成实例： 接下来具体说说回归树是如何进行特征选择生成二叉回归树的。假设$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集 D=\{(x_1,y_1),(x_2,y_2),···,(x_N,y_N)\}我们利用最小二乘回归树生成算法来生成回归树$f(x)$，即在训练数据集所在的输入空间中，递归地将每个区域分为两个子区域并决定每个子区域上的输出值，构建二叉决策树，步骤如下： 1）选择最优切分变量$j$与切分点$s$，求解 \min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1\left(j,s\right)}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2\left(j,s\right)}{\left(y_i-c_2\right)^2}\right]遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值得对$j,s$ 2）用选定的对$(j,s)$划分区域并决定相应的输出值： R_1\left(j,s\right)=\left\{x|x^{\left(j\right)}\le s\right\}\ ,\ R_2\left(j,s\right)=\left\{x|x^{\left(j\right)}>s\right\} \hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_2\left(j,s\right)}{y_i}\ ,\ x\in R_m\ ,\ m=1,2 3）继续对两个子区域调用步骤（1），（2），直至满足停止条件。 4）将输入空间划分为$M$个区域$R_1,R_2,···,R_M$，在每个单元$R_m$上有一个固定的输出值$c_m$，生成决策树： f\left(x\right)=\sum_{m=1}^M{\hat{c}_m\textrm{I}\left(\textrm{x}\in\textrm{R}_{\textrm{m}}\right)} 四、决策树的剪枝4.1 剪枝决策树的过拟合指的是学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决过拟合的办法是考虑决策树的复杂度，对已生成的决策树进行简化，即剪枝（从已生成的树上裁剪调一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型）。 设树$T$的叶结点个数为$|T|$,$t$是树$T$的叶结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,···,K$，$H_t(T)$为叶结点$t$上的经验熵，$a≥0$为参数，则决策树学习的损失函数可以定义为 { C }_{ a }(T)=\sum _{ t=1 }^{ |T| }{ { N }_{ t }{ H }_{ t }(T)+a } |T|其中经验熵为 { H }_{ t }(T)=-\sum _{ k }{ \frac { { N }_{ tk } }{ N_{ t } } log\frac { { N }_{ tk } }{ { N }_{ t } } }在损失函数中，将右端第一项记作 C\left( T \right) =\sum_{t=1}^{|T|}{N_tH_t\left( T \right) =-\sum_{t=1}^{|T|}{\sum_{k=1}^K{N_{tk}\log \frac{N_{tk}}{N_t}}}}这时有 C_a\left( T \right) =C\left( T \right) +a|T|其中，$C(T)$表示模型对训练数据的预测误差,即模型与训练数据的拟合程度，$|T|$表示模型复杂度，参数$a≥0$控制两者之间的影响。较大的$a$促使选择较简单的模型，较小的$a$促使选择较复杂的模型。$a=0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。 决策树生成只考虑了通过信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。此损失函数的极小化等价于正则化的极大似然估计，即利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。 4.2 CART剪枝CART剪枝算法从“完全生长”的决策树的底端减去一些子树，使决策树变小（模型变简单），从而能够对未知数据有更准确的预测其具体步骤如下： 1）首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个字数序列$\left\{ T_0,T_1,T_{2,….,}T_n \right\}$; 在剪枝过程中，计算子树的损失函数： C_a\left( T \right) =C\left( T \right) +a|T|其中，$T$为任意子树，$C(T)$为对训练数据的预测误差（如基尼系数），$|T|$为子树的叶结点个数，$a≥0$为参数，$C_a(T)$为参数是$a$时的子树$T$的整体损失。参数$a$权衡训练数据的拟合程度与模型的复杂度。 对固定的$a$，一定存在使损失函数$C_a(T)$最小的子树，将其表示为$T_a$。$T_a$在损失函数$C_a(T)$最小的意义下是最优的，且是唯一的。$a$大的时候，最优子树$T_a$偏小；当$a$小的时候，最优子树$T_a$偏大。极端情况，$a=0$时，整体树是最优的。当$a\rightarrow \infty$，根结点组成的单结点树是最优的。 Breiman等人证明：可以用递归地方法对树进行剪枝。将$a$从小增大，$0=a_0&lt;a_1&lt;…..a_n&lt;+\infty$产生一系列的区间$[a_i,a_{i+1})$,$i=0,1,…,n$;剪枝得到的子树序列对应着区间$a\in \left[ a_i,a_{i+1} \right)$，$i=0,1,2,…,n$的最优子树序列为$\left\{ T_0,T_1,T_2,…,T_n \right\}$，序列的子树是嵌套的。 具体地，从整体树$T_0$开始剪枝，对$T_0$的人以内部结点$t$，以$t$为单结点树的损失函数是 C_a\left( t \right) =C\left( t \right) +\alpha以$t$为根结点的子树$T_t$的损失函数是 C_a(T_t)=C(T_t)+a|T_t|当$a=0$及$a$充分小时，有不等式 C_a\left( T_t \right)小于C_a\left( T \right)当$a$增大时，在某一$a$有 C_a(T_t)等于C_a(t)当$a$再增大时，有不等式 C_a(T_t)大于C_a(T)只要$\alpha =\frac{C\left( t \right) -C\left( T_t \right)}{|T_t|-1}$ ，$T_t$与$t$有相同的损失函数值，而$t$的结点少，因此$t$比$T_t$更可取，对$T_t$进行剪枝。 为此，对$T_0$中的每一个内部结点$t$，计算 g\left( t \right) =\frac{C\left( t \right) -C\left( T_t \right)}{|T_t|-1}它表示剪枝后整体损失函数减少的程度。在$T_0$中剪去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$a_1$，$T_1$为区间$[a_1,a_2)$的最优子树。 如此剪枝下去，直至得到根结点。在这一过程中，不断得增加$a$的值，产生新的区间。 2）在剪枝得到的子树序列$T_0,T_1,…,T_n$中通过交叉验证选取最优子树$T_a$ 具体地，利用独立的验证数据集，测试子树序列$T_0,T_1,…,T_n$中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中，每棵子树$T_0,T_1,…,T_n$都对应一个参数$a_1,a_2,…,a_n$。所以当最优子树$T_k$确定时，对应的$a_k$也就确定了，即得到最由决策树$T_a$. 五、参考资料李航《统计学习方法》周志华《机器学习》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笑忘录（6）：胡乱记]]></title>
    <url>%2F2017%2F01%2F15%2F%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%886%EF%BC%89%EF%BC%9A%E8%83%A1%E4%B9%B1%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[其一：人女人的风度在于表达她对自己的看法，以及界定别人对待她的分寸。她的风度从姿态、声音、见解、表情、服饰、品味和选定的场合上体现出来——实际上，她所作的一切，无一不为她的风度增色。……男性观察女性；女性注意自己被别人观察。这不仅决定了大多数的男女关系，还决定了女性自己的内在关系，女性自身的观察者是男性，而被观察者为女性。这样被动主动的交互构成了男人们最基本的窥淫欲和恋物癖的现实表现以及女人在这个环境下被预设为被男人品赏的艺术品的尴尬局面。 知识分子对人民的引导应该是一个缓慢感化逐渐深入的过程，一开始就让老百姓穿高端的品牌，吃喝上等的烟酒是不切实际的，他们穿着不合适，吃进去还反胃，最后还弄个“文化强行入侵”的罪名，如此悲剧来源于知识分子自以为是的旨趣，该杜绝。阳春白雪对下里巴人所持有的态度不应是嫌弃，该是怜悯。 我们都是时代的弄潮儿，这不错，但是必须要清楚地认识到，我们弄得潮有大也有小，也有压根儿就不知道怎么弄潮，你不该为之抱怨，这是历史的必然，总会有潜藏的智慧不被挖掘，总会有好马遇不到伯乐。但是，于自身来说，你不该愤世骇俗，你得用尽全力去弄点潮出来，加把劲，再加把劲，说不定就弄出大潮来了嘞。 其实我们真的太年轻，年轻到不懂得原来爱情到了考虑现实的时候，到了和金钱权利挂钩之后，就再也不是原来的摸样的了。所以，趁自己还没有沾染那些东西的时候，好好地珍惜这份来之不易的爱情。如果你要考虑现实，那就去考虑吧，跟着自己的心走。每个人确实需要更长远的眼光，要看到远方的事情，但是青春真的就那么就一眨眼就不在了，有多少人做着自己喜欢的事情，爱着真正爱的人，过着内心喜欢和向往的生活？有多少人在不停忙碌着，却不知是为了什么？有多少人在混混沌沌着，也不知是为了什么？有太多的牵绊，有太多的不舍，有太多的不甘心。待到我们放下这些所谓的不舍、牵绊与不甘心，也正是我们真正活着的时候。一个人永远守护另一个人是不可能的。希望你记住我，记住我曾经这样存在过。记忆这东西总有些不可思议。身临其境的时候，几乎未曾意识到那片风景，未曾觉得它有什么撩人情怀之处，但到了十几年之后我们可能还无法忘怀。我老是介于“不充分”和“完全不够”之间，我总是感到饥渴，真想拼着劲儿得到一次爱。容许我百分之百的任性。村上春树讲：“ 把人生当做饼干罐就可以了。饼干罐不是装了各种各样的饼干，喜欢的和不喜欢的都在里面吗？如果先一个劲儿的挑你喜欢吃的，那么剩下的就全是不大喜欢的。每次遇到麻烦我就这样想，先把这个应付过去，往下就好办了。 计划实在是一样满足虚荣心的好玩意儿，你欢腾于欣赏顺利达成的自己并且毫不自禁的开始畅想那种虚无的满足感，这样的力量实在是软弱的，毫无生机的，既然没实质的胜利，那你就是个十足的阿q，精神胜利的恶魔在肆虐你，让你分不清方向，你依旧在狂笑，不带一点点对自己的歉意。 所谓的有力不取决于肉体所能承载的负重和劳累，每天听到上千种声音，每天走6000步，每天看到1000种颜色，每天接触不一样滋味的空气，我的器官接受了周围的一切顺其自然的，你想看到的或者想避开的，都会一溜烟的在你神经的深处划过轨迹，记住的便是你的选择和潜意识里面想要的得到的，所以珍惜你记住的同时缅怀你没记住的。 其二：物电影的发展不是绝对的，而是在一盘散沙里面掏出金来，时不时又混点泥沙进去，往复循环，进步的地方我们看的很明显，但是退步的趋势也是在很大一部分的元素里面可以显而易见的。看电影是一个艺术门道，主观的感觉是用来解决温饱的，那么深刻地剖析就是让你开始挑剔一部好电影的“色香味”，才可以真的把整个电影的精髓挖掘出来。 谁都有你想不到的难题，困扰于心的在别人看来都是些鸡毛蒜皮的小事。在这里可以学到一个受用的经验：当缠绕着你的那些忧思无法被自己打败以及翻越的时候，把视线放远些，不要局限于一个角度，你看到的就不会是墙角的草，而是大草原的葱茏，那是何其美妙的世界。当然，这需要有一定的阅历（阅读，经历很重要），去扩展你的经历簿。 如果把马蜂窝移到自己的居室，帮里头的蜜蜂精心准备好繁茂生长的花朵，恰到好处的阳光，各种生命的迹象，以至于让他们察觉不到自己身处异境，即使有人类在他们周围逡巡，也不会折回或做出抵抗。让这样的环境持续尽量久一些，蜜蜂也会在自己毫无察觉的情况下发生细微的变化，那是依赖于环境的惯性适应。强行改变人类的行为亦是如此，扭曲变形，失去重心没有防线，离开安全区域的不安与彷徨，偏离轨道与甚者再也无可复原。 对于哲学问题除非你长期为之苦思冥想，否则你根本说不清到底是些什么问题。要对哲学史有很好的说明，你必须竭尽所能从其”内部“看清各个哲学问题，设身处地地进入你所讨论的哲学家们的内心世界。你必须弄清那些问题对为之乐此不疲的哲学家意味着什么，弄清哲学家们始终关注的焦点。不如此”艰辛“是不可能写出真正的思想史的。另外，除非你自己专注于相关领域并进行深入研究，否则你无法写出他人在该领域艰难跋涉的历史。意识形态的历史，严格来说，只有那些热衷于意识形态问题并懂得如何思考意识形态问题的人才能写。哲学解决的是观念之间、词语之间或表述方法之间的冲突产生出来的各种疑难。不同于经验问题。关于生活的目的、善和恶、自由和必然、客观性和相对性等的一系列问题，既不能考查阅最高级的辞典来解决，也不能用经验方法或数学推理方法来解决。设身处地地进入思想家们的内心和世界观是必要的，移情也是不可或缺的，尽管这样做面临证据不足和不确定性，乃至困难重重。诸如在研究马克思时，应该力图使自己像马克思本人在柏林、在巴黎、在布鲁塞尔和在伦敦那样，思考它的各种概念、范畴及其德语词汇。他们的思想是怎样产生的？在什么特定的时间、地点、社会条件下产生的？他们的思想可能很多人都有同感，但毕竟那是属于他们自己的。你必须不断反问自己，是什么东西让他们烦恼？什么东西使他们对这些问题苦苦思索？他们的理论或著作是怎么样在他们头脑中成熟的？人们不能完全抽象地超历史地谈论各种思想；但是，人们也不能孤立地仅在具体的历史环境中来描述各种思想，好像这些思想在他们的框架之外没有任何意义似的。这是一种复杂的、含糊不清的、需要借助心理学视野以及丰富想象力的研究工作，他不可能获得什么必然性的结论、在多数情况下，只能达到高度的持之有故和言之有理，达到理智能力的首尾一贯和清楚明白，还有独创性和有效性。]]></content>
      <categories>
        <category>笑忘录</category>
      </categories>
      <tags>
        <tag>电影</tag>
        <tag>哲学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（3）：逻辑斯谛回归]]></title>
    <url>%2F2017%2F01%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9A%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[一、逻辑斯谛分布介绍逻辑斯谛回归模型之前，首先看一个并不常见的概率分布，即逻辑斯谛分布。设$X$是连续随机变量，$X$服从逻辑斯谛分布是指$X$具有下列的分布函数和密度函数： F\left(x\right)=P\left(X\le x\right)=\frac{1}{1+e^{-\left(x-\mu\right)/\gamma}} $$f\left(x\right)=F^,\left(x\right)=\frac{e^{-\left(x-\mu\right)/\gamma}}{\gamma\left(1+e^{-\left(x-\mu\right)/\gamma}\right)^2} 式中，$\mu$为位置参数，$\gamma>0 $为形状参数。逻辑斯谛的分布的密度函数$f(x)$和分布函数$F(x)$的图形如下图所示。其中分布函数属于逻辑斯谛函数，其图形为一条$S$形曲线。该曲线以点$(\mu,\frac{1}{2})$为中心对称，即满足F\left(-x+\mu\right)-\frac{1}{2}=-F\left(x+\mu\right)+\frac{1}{2} 曲线在中心附近增长较快，在两端增长速度较慢。形状参数$\gamma$的值越小，曲线在中心附近增长得越快。 ![](https://i.loli.net/2019/07/31/5d413f436513732414.jpg) ## 二、逻辑斯谛回归模型 线性回归的应用场合大多是回归分析，一般不用在分类问题上。原因可以概括为以下两个： - 1）回归模型是连续型模型，即预测出的值都是连续值（实数值），非离散值； - 2）预测结果受样本噪声的影响比较大。 ### 2.1 LR模型表达式 LR模型表达式为参数化的逻辑斯谛函数（默认参数$\mu=0,\gamma=1$）,即h_{\theta}\left(x\right)=\frac{1}{1+e^{-\theta^Tx}}$$其中$h_\theta{(x)}$作为事件结果$y=1$的概率取值。这里,$x\in R^{n+1},y\in \{1,0\},\theta\in R^{n+1}$是权值向量。其中权值向量$w$中包含偏置项，即$w=(w_0,w_1,···,w_n)，x=(1,x_1,x_2,···,x_n)$ 2.2 理解LR模型2.2.1 对数几率一个事件发生的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是$p$，那么该事件的几率为$\frac{p}{1-p}$，该事件的对数几率（log odds）或logit函数是： logit\left(p\right)=\log\frac{p}{1-p}对LR而言，根据模型表达式可以得到： \log\frac{h_{\theta}\left(x\right)}{1-h_{\theta}\left(x\right)}=\theta^Tx即在LR模型中，输出$y=1$的对数几率是输入$x$的线性函数。或者说输出$y=1$的对数几率是由输入$x$的线性函数表示的模型，即LR模型 2.2.2 函数映射除了从对数几率的角度理解LR外，从函数映射也可以理解LR模型。 考虑对输入实例$x$进行分类的线性表达式$\theta^T$，其值域为实数域。通过LR模型表达式可以将线性函数$\theta^Tx$的结果映射到(0,1)区间，取值表示为结果为1的概率（在二分类场景中）。 线性函数的值越接近于正无穷大，概率值就越接近1；反之，其值越接近于负无穷，概率值就越接近0。这样的模型就是LR模型。 LR本质上还是线性回归，知识特征到结果的映射过程中加了一层函数映射（即sigmoid函数），即先把特征线性求和，然后使用sigmoid函数将线性和约束至（0，1）之间，结果值用于二分或回归预测。 2.2.3 概率解释LR模型多用于解决二分类问题，如广告是否被点击（是/否）、商品是否被购买（是/否）等互联网领域中常见的应用场景。但是实际场景中，我们又不把它处理成“绝对的”分类问题，而是用其预测值作为事件发生的概率。 这里从事件、变量以及结果的角度给予解释。 我们所能拿到的训练数据统称为观测样本。问题：样本是如何生成的？ 一个样本可以理解为发生的一次事件，样本生成的过程即事件发生的过程。对于0/1分类问题来讲，产生的结果有两种可能，符合伯努利试验的概率假设。因此，我们可以说样本的生成过程即为伯努利试验过程，产生的结果（0/1）服从伯努利分布。这里我们假设结果为1的概率为$h_\theta{(x)}$，结果为0的概率为$1-h_\theta{(x)}$。 那么对于第$i$个样本，概率公式表示如下： P(y^{(i)}=1|x^{(i)};\theta )=h_\theta{(x^{(i)})}$$$$P(y^{(i)}=0 |x^{(i)};\theta )=1- h_\theta{(x^{(i)})}将上面两个公式合并在一起，可得到第$i$个样本正确预测的概率： P(y^{(i)}|x^{(i)};\theta)=(h_\theta(x^{(i)})^{y(i)})·（1-h_\theta(x^{(i)}))^{1-y(i)}上式是对一个样本进行建模的数据表达。对于所有的样本，假设每条样本生成过程独立，在整个样本空间中（N个样本）的概率分布（即似然函数）为： P\left(Y|X;\theta\right)=\prod_{i=1}^N{\left(h_{\theta}\left(x^{\left(i\right)}\right)^{y^{\left(i\right)}}\left(1-h_{\theta}\left(x^{\left(i\right)}\right)^{1-y^{\left(i\right)}}\right)\right)}通过极大似然估计（Maximum Likelihood Evaluation，简称MLE）方法求概率参数。具体地，第三节给出了通过随机梯度下降法（SGD）求参数。 三、模型参数估计3.1 Sigmoid函数 上图所示即为sigmoid函数，它的输入范围为$-\infty\rightarrow +\infty$，而值域刚好为$(0,1)$，正好满足概率分布为$(0,1)$的要求。用概率去描述分类器，自然要比阈值要来的方便。而且它是一个单调上升的函数，具有良好的连续性，不存在不连续点。 此外非常重要的，sigmoid函数求导后为：以下的推导中会用到，带来了很大的便利。 3.2 参数估计推导上一节的公式不仅可以理解为在已观测的样本空间中的概率分布表达式。如果从统计学的角度可以理解为参数$\theta$似然性的函数表达式（即似然函数表达式）。就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大。参数在整个样本空间的似然函数可表示为： L\left(\theta\right)=P\left(\overrightarrow{Y}|X;\theta\right) =\prod_{i=1}^N{P\left(y^{\left(i\right)}\parallel x^{\left(i\right)};\theta\right)} =\prod_{i=1}^N{\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)^{y\left(i\right)}\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)^{1-y^{\left(i\right)}}}为了方便参数求解，对这个公式取对数，可得对数似然函数： l\left(\theta\right)=\sum_{i=1}^N{\log l\left(\theta\right)} =\sum_{i=1}^N{y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)}最大化对数似然函数其实就是最小化交叉熵误差（Cross Entropy Error）。先不考虑累加和，我们针对每一个参数$w_j$求偏导： \frac{\partial}{\partial\theta_j}l\left(\theta\right)=\left(y\frac{1}{h_{\theta}\left(x\right)}-\left(1-y\right)\frac{1}{1-h_{\theta}\left(x\right)}\right)\frac{\partial}{\partial\theta_j}h_{\theta}\left(x\right) =\left(\frac{y\left(1-h_{\theta}\left(x\right)\right)-\left(1-y\right)h_{\theta}\left(x\right)}{h_{\theta}\left(x\right)\left(1-h_{\theta}\left(x\right)\right)}\right)h_{\theta}\left(x\right)\left(1-h_{\theta}\left(x\right)\right)\frac{\partial}{\partial\theta_j}\theta^Tx =\left(y-h_{\theta}\left(x\right)\right)x_j最后，通过扫描样本，迭代下述公式可求得参数： \theta_j:=\theta_j+a\left(y^{\left(i\right)}-h_{\theta}\left(x^{\left(i\right)}\right)\right)x_{j}^{\left(i\right)}其中$a$表示学习率，又称学习步长。此外还有Batch GD，共轭梯度，拟牛顿法（LBFGS），ADMM分布学习算法等都可以用来求解参数。另作优化算法一章进行补充。 以上的推导是LR模型的核心部分，在机器学习相关面试中，LR模型公式推导可能是考察频次最高的一个点。要将其熟练推导。 3.3 分类边界知道如何求解参数后，我们看一下模型得到的最后结果是什么样的。假设我们的决策函数为： y^∗=1, \ \ if \ \ P(y=1|x)>0.5选择0.5作为阈值是一个一般的做法，实际应用时特定的情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。很容易看出，当$\theta ^Tx&gt;0$时，$y=1$，否则$y=0$。$\theta ^Tx=0$是模型隐含的分类平面（在高维空间中，我们说是超平面）。所以说逻辑回归本质上是一个线性模型，但是这不意味着只有线性可分的数据能通过LR求解，实际上，我们可以通过特征变换的方式把低维空间转换到高维空间（kernel trick），而在低维空间不可分的数据，到高维空间中线性可分的几率会高一些。 不过，通常使用的kernel都是隐式的，也就是找不到显式地把数据从低维映射到高维的函数，而只能计算高维空间中的数据点的内积。在这种情况下，logistic regression模型就不能再表示成$w^Tx+b$的形式（原始形式primal form），而只能表示成$\sum_ia_i+b$的形式（对偶形式dual form）。忽略b，则原始形式的模型蚕食只有$w$，只需要一个数据点那么多的存储量；而对偶形式的模型不仅需要存储各个$a_i$，还要存储训练数据$x_i$本身，这个存储量就大了。 SVM也具有原始形式和对偶形式，相比之下，SVM的对偶形式是稀疏的，即只有支持向量的$a_i$才非零，才需要存储相应的$x_i$，所以，在非线性可分的情况下，SVM用的更多。 四、延伸4. 1 生成模型与判别模型逻辑回归是一种判别模型，表现为直接对条件概率$P(y|x)$建模，而不关心背后的数据分布$P(x,y)$。而高斯贝叶斯（Gaussian Naive Bayes）是一种生成模型，先对数据的联合分布建模，再通过贝叶斯公式来计算属于各个类别的后验概率，即： p(y|x)=\frac{P(x|y)P(y)}{\sum P(x|y)P(y)}通常假设$P(x|y)$是高斯分布，$P(y)$是多项式分布，相应的参数可以通过最大似然估计得到。如果我们考虑二分类问题，通过简单的变化可以得到： log\frac{P(y=1|x)}{P(y=0|x)}=log\frac{P(x|y=1)}{P(x|y=0)}+log\frac{P(y=1)}{P(y=0)}=-\frac{(x-\mu_1)^2}{2\sigma_1^2}+\frac{(x-\mu_0)^2}{2\sigma_0^2}+\theta_0如果$\sigma_1=\sigma_0$，二次项会抵消，我们得到一个简单的线性关系： log\frac{P(y=1|x)}{P(y=0|x)}=\theta^Tx上式进一步可以得到： P(y=1|x)=\frac{e^{\theta^Tx}}{1+e^{e^Tx}}=\frac{1}{1+e^{-\theta^Tx}}可以看到，这个概率和逻辑回归中的形式是一样的，这种情况下高斯贝叶斯和LR会学习到同一个模型。实际上，在更一般的假设（P(x|y)的分布属于指数分布族）下，我们都可以得到类似的结论。 4.2 多分类如果$y$不是在$[0,1]$中取值，而是在$K$个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当$K$个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果$K$个类别是互斥的，即 $y=i$ 的时候意味着 $y$ 不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）。模型通过 softmax 函数来对概率建模，具体形式如下： P(y=i|x,\theta)=\frac{^{e^{\theta^T_ix}}}{\sum_j^K e^{\theta_j^Tx}}$$而决策函数为：$$y^*=argmax_iP(y=i|x,\theta )对于的损失函数为 J(\theta)=-\frac{1}{N}\sum_i^N\sum_j^KP(y_i=j)log\frac{e^{\theta_i^Tx}}{\sum e^{\theta_k^Tx}}类似的，我们也可以通过梯度下降或其他高阶方法来求解该问题，这里不再赘述。 4.3 应用这里以预测用户对品类的购买偏好为例，介绍一下美团是如何用逻辑回归解决工作中问题的。该问题可以转换为预测用户在未来某个时间段是否会购买某个品类，如果把会购买标记为1，不会购买标记为0，就转换为一个二分类问题。我们用到的特征包括用户在美团的浏览，购买等历史信息，见下表：其中提取的特征的时间跨度为30天，标签为2天。生成的训练数据大约在7000万量级（美团一个月有过行为的用户），我们人工把相似的小品类聚合起来，最后有18个较为典型的品类集合。如果用户在给定的时间内购买某一品类集合，就作为正例。有了训练数据后，使用Spark版的LR算法对每个品类训练一个二分类模型，迭代次数设为100次的话模型训练需要40分钟左右，平均每个模型2分钟，测试集上的AUC也大多在0.8以上。训练好的模型会保存下来，用于预测在各个品类上的购买概率。预测的结果则会用于推荐等场景。 由于不同品类之间正负例分布不同，有些品类正负例分布很不均衡，我们还尝试了不同的采样方法，最终目标是提高下单率等线上指标。经过一些参数调优，品类偏好特征为推荐和排序带来了超过1%的下单率提升。 此外，由于LR模型的简单高效，易于实现，可以为后续模型优化提供一个不错的baseline，我们在排序等服务中也使用了LR模型。 逻辑回归的数学模型和求解都相对比较简洁，实现相对简单。通过对特征做离散化和其他映射，逻辑回归也可以处理非线性问题，是一个非常强大的分类器。因此在实际应用中，当我们能够拿到许多低层次的特征时，可以考虑使用逻辑回归来解决我们的问题。 4.4 LR与SVM两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。但是逻辑回归相对来说模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些。但是SVM的理论基础更加牢固，有一套结构化风险最小化的理论基础，虽然一般使用的人不太会去关注。还有很重要的一点，SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。 两者对异常的敏感度也不一样。同样的线性分类情况下，如果异常点较多的话，无法剔除，首先LR，LR中每个样本都是有贡献的，最大似然后会自动压制异常的贡献，SVM+软间隔对异常还是比较敏感，因为其训练只需要支持向量，有效样本本来就不高，一旦被干扰，预测结果难以预料。 参考资料对线性回归，logistic回归和一般回归的认识 Logistic Regression 模型简介]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑斯谛回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笑忘录（5）：离别记]]></title>
    <url>%2F2017%2F01%2F10%2F%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%885%EF%BC%89%EF%BC%9A%E7%A6%BB%E5%88%AB%2F</url>
    <content type="text"><![CDATA[离别的仪式以其该有的苍白的方式展开了，一个拥抱，一声保重，一段你们来时走过的路，它竭力地想被赋予更多的力量与情感，那么看吧，看看在我们心灵藏匿最深的地方，那里骚动起了真诚而又岌岌可危的猛兽，在故意掩藏的角落里缠绵、撕咬、交姌。你可以尽情嗅到生命的律动，几近沸腾的血液在体内加快步伐，血泵察觉到了仪式的情绪，它已经完全失去控制，它发了疯似的让灵动的双眼淌着热泪，身体仿佛接受了呼啸的寒风开始战栗起来，此刻即将分别，每一个鲜活的生命就要前往这个国度的各个角落，那里的星辰大海，下一次见面的时候请一定告诉我。 生活总需要特定的仪式来配合鼓舞，四年前我们仪式着来，四年后仪式着走，这无关乎虚荣，也绝不是空穴矫情，站在这个时间的关口，回望过去的日子，它不温不火，但也一定不平凡。当年踏上这片土地å，驾着云轻吟功与名，如今洒脱地背着旗帜四处宣告这四年的狂热与藏匿，我们汲汲求索，生活让我们的心灵焦灼躁动，却反而褪出一种光泽来，它们每一样都隐秘而伟大，孕育着一股神奇的力量。 我们曾如孩童般痴笑着欢送旧时光，如酒神般渴求下一秒思想的闪耀，过去哪怕是闯入一块新天地的边境，也要花光我们所有的力气，迟来的矍铄如同一杯烈酒，蓄满无畏与重生的快感，愿我们承受得住这些深渊，一直自命不凡地在里面挣扎，生命不止，它最好一刻也不要停止，也愿我们每一次触碰这个星球时都可以感受到有一种声音在呼喊，那声音在遥远的未来回响，也发轫于不朽的丰碑与偶像存留的尘土。 我们即将面临一次非同寻常、却令人困倦的旅程，我们今天就要乘上一列所谓的失控列车。没有人可以诚恳地向我们展示前方有什么，而那些落在后面的人也会因接续我们这一毫无秩序的开端而爱莫能助。但我们必须要承认，这段旅途有去无回，我们可以宽慰自身的也只能是这样的思想：无论遇到什么不开心的事情，无论是哪一个车站都会一闪而过，那只是影片的一个人微妙的段落，列车绝不会在一个车站停留地太久。今天我们一起驻守的这块地方，即将成为过去的一站，我们只能挥手道别，在它还保持着正常的模样之前，在它还没有成为一张照片之前，就让我们怀着我们所有的温情再看它一眼，那也是在打量我们的过去。 我只是吝啬地留了一个苦涩的脸庞在你们记忆里，而你们在我的视野里留下了一个个鲜活的梦。篮球梦、炸金花梦、LOL梦，亦或是妹子梦、富帅梦、伟光正梦，它们都即将循着自己的路子熠熠生辉，都端正着自己的仪式带着未完成的梦离开吧，在812留下的痕迹就让我们在年迈的时候再来品味，倘若你们还记得床底的乌烟瘴气与泛黄纯白色纸巾，就算是重走了一回青春。]]></content>
      <categories>
        <category>笑忘录</category>
      </categories>
      <tags>
        <tag>笑忘录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（2）：线性回归]]></title>
    <url>%2F2017%2F01%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[一、线性回归模型线性回归假设特征和结果满足线性关系。其实线性关系的表达能力非常强大，每个特征对结果的影响强弱可以由前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后再参与线性计算。这样就可以表达特征与结果之间的非线性关系。 我们可以有这样的模型表达： y=\theta_0+\theta_1x_1+\theta_2x_2+···+\theta_nx_n其中，$x_1,x_2,···,x_n$表示自变量（特征分量），$y$表示因变量，$\theta_i$表示对应自变量（特征）的权重，$\theta_0$是偏倚项（又称为截距）。 对于参数$\theta$，在物理上可以解释为：在自变量（特征）之间相互独立的前提下，$\theta_i$反映自变量$x_i$对因变量$y$的影响程度，$\theta_i$越大，说明$x_i$对结果$y$的影响越大。因此，我们可以通过每个自变量（特征）前面的参数，可以很直观的看出那些特征分量对结果的影响比较大。 如果令$x_0=1,y=h_\theta{(x)}$，可以将上述模型写成向量形式，即： h_\theta\left(x\right)=\sum_{i=0}^n{\theta_ix_i}=\theta^Tx其中$\theta=(\theta_0,\theta_1,···,\theta_n)，x=(1,x_1,x_2,···,x_n)$均为向量，$\theta^T$为$\theta$的转置。 在上述公式中，假设特征空间与输入空间$x$相同。准确地讲，模型表达式要建立的是特征空间与结果之间的关系。在一些应用场合中，需要将输入空间映射到特征空间中，然后建模，定义映射函数为$\varPhi\left(x\right)$，因此我们可以把公式写成更通用的表达公式： h_\theta\left(x\right)=\theta^T\varPhi\left(x\right)特征映射相关技术，包括特征哈希、特征学习、$Kernel$等。 二、目标函数2.1 目标函数上面的公式的参数向量$\theta$是$n+1$维的，每个参数的取值是实数集合，也就是说参数向量$\theta$在$n+1$维实数空间中取值结果有无穷种可能。 那么，如何利用一个规则或机制帮助我们评估求得的参数$\theta$，并且使得线性模型效果最佳呢？直观地认为，如果求得参数$\theta$线性求和后，得到的结果$h_\theta{(x)}$与真实值$y$之差越小越好。 这时我们需要映入一个函数来衡量$h_\theta{(x)}$表示真实值$y$好坏的程度，该函数称为损失函数（loss function，也称为错误函数）。数学表示如下： J\left(\theta\right)=\frac{1}{2}\sum_{i=1}^n{\left(\left(h_\theta\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)\right)^2} \min_\theta J\left(\theta\right)这个损失函数用的是$x^{(i)}$的预测值$h_\theta{(x^{(i)})}$与真实值$y^{(i)}$之差的平方和。如果不考虑诸如过拟合等其他问题，这就是我们需要优化的目标函数。 2.2 目标函数的概率解释一般地，机器学习中不同的模型会有相应的目标函数。而回归模型（尤其是线性回归类）的目标函数通常用平方损失函数来作为优化的目标函数（即真实值与预测值之差的平方和）。为什么要选用误差平方和作为目标函数呢？答案可以从概率论中的中心极限定理、高斯分布等知识中找到。 2.2.1 中心极限定理目标函数的概率解释需要用到中心极限定理。中心极限定理本身就是研究独立随机变量和的极限分布为正态分布的问题。 中心极限定理的公式表示为：设$n$个随机变量$X_1,X_2,···,X_n$相互独立，均具有相同的数学期望与方差，即$E(X_i)=\mu ;D(X_i)=\sigma^2$，令$Y_n$为随机变量之和，有 Y_n=X_1+X_2+···+X_n Z_n=\frac{Y_n-E\left(Y_n\right)}{\sqrt{D\left(Y_n\right)}}=\frac{Y_n-n\mu}{\sqrt{n}\sigma}\rightarrow N\left(0,1\right)称随机变量$Z_n$为$n$个随机变量$X_1,X_2,···,X_n$的规范和。 它的定义为：设从均值为$\mu$、方差为$\sigma^2$（有限）的任意一个总体中抽取样本量为$n$的样本，当$n$充分大时，样本均值的抽样分布$\frac{Y_n}{n}$近似服从于均值为$\mu$、方差为$\sigma^2$的正态分布。 2.2.2 高斯分布假设给定一个输入样例$x^{(i)}$根据公式得到预测值$\theta^Tx^{(i)}$与真实值$y^{(i)}$之间存在误差，即为$\varepsilon^{\left(i\right)}$。那么，它们之间的关系表示如下： y^{\left(i\right)}=\theta^Tx^{\left(i\right)}+\varepsilon^{\left(i\right)}而这里假设误差$\varepsilon^{\left(i\right)}$服从标准高斯分布是合理的。 解释如下： 回归模型的最终目标是通过函数表达式建立自变量$x$与结果$y$之间的关系，希望通过$x$能较为准确地表示结果$y$。而在实际的应用场合中，很难甚至不可能把导致$y$的所有变量（特征）都找出来，并放到回归模型中。那么模型中存在的$x$通常认为是影响结果$y$最主要的变量集合（又称为因子，在ML中称为特征集）。根据中心极限定理，把那些对结果影响比较小的变量（假设独立同分布）之和认为服从正态分布是合理的。 可以用一个示例来说明误差服从高斯分布是合理的： $Andrew Ng$的课程中第一节线性回归的例子中，根据训练数据建立房屋的面积$x$与房屋的售价$y$之间的函数表达。它的数据集把房屋面积作为最为主要的变量。除此之外我们还知道房屋所在的地段（地铁、学区、城区、郊区），周边交通状况，当地房价、楼层、采光、绿化面积等等诸多因素会影响房价。 实际上，因数据收集问题可能拿不到所有影响房屋售价的变量，可以假设多个因素变量相互独立，根据中心极限定理，认为变量之和服从高斯分布。即： \epsilon^{\left(i\right)}=y^{\left(i\right)}-\theta^Tx^{\left(i\right)}那么$x$和$y$的条件概率可表示为： p\left(y^{\left(i\right)}|x^{\left(i\right)};\theta\right)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}{2\sigma^2}\right)2.2.3 极大似然估计与损失函数极小化等价根据上述公式估计得到一条样本的结果概率，模型的最终目标是希望在全部样本上预测最准，也就是概率积最大，这个概率积就是似然函数。优化的目标函数即为似然函数，表示如下： \max_\theta L\left(\theta\right)=\prod_{i=1}^m{\frac{1}{\sqrt{2\pi}\sigma}}\exp\left(-\frac{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}{2\sigma^2}\right)对$L(x)$取对数，可得对数似然函数： \max_\theta l\left(\theta\right)=-m\log\sqrt{2\pi}\sigma -\frac{1}{2\sigma^2}\sum_{i=1}^m{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}由于$n,\sigma$都为常数，因此上式等价于 \min_\theta\frac{1}{2}\sum_{i=1}^m{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}我们可以发现，经过最大似然估计推导出来的待优化的目标函数与平方损失函数是等价的。因此可以得出结论： 线性回归误差平方损失极小化与极大似然估计等价。其实在概率模型中，目标函数的原函数（或对偶函数）极小化（或极大化）与极大似然估计等价，这是一个带有普遍性的结论。比如在最大熵模型中，有对偶函数极大化与极大似然估计等价的结论。 那上面为什么是条件概率$p(y|x;\theta)$呢？因为我们希望预测值与真实值更接近，这就意味着希望求出来的参数$\theta$，在给定输入$x$的情况下，得到的预测值等于真实值得可能性越大越好。而$\theta$，$x$均为前提条件，因此用条件概率$p(y|x;\theta)$表示。即$p(y|x;\theta)$越大，越能说明估计的越准确。当然也不能一味地只有该条件函数，还要考虑拟合过度以及模型的泛化能力问题。 三、参数估计如何调整参数$\theta$使得$J(\theta)$取得最小值？方法有很多，这里介绍几种比较经典的方法，即最小二乘法、梯度下降法以及牛顿法。 3.1 最小二乘法3.1.1 目标函数的矩阵形式将$m$个$n$维样本组成矩阵$X$： \left(\begin{matrix} 1& x_{1}^{\left(1\right)}& x_{1}^{\left(2\right)}& ···& x_{1}^{\left(n\right)}\\ 1& x_{2}^{\left(1\right)}& x_{2}^{\left(2\right)}& ···& x_{2}^{\left(n\right)}\\ ···& ···& ···& & \\ 1& x_{m}^{\left(1\right)}& x_{m}^{\left(2\right)}& ···& x_{m}^{\left(n\right)}\\ \end{matrix}\right)则目标函数的矩阵形式为 J\left(\theta\right)=\frac{1}{2}\sum_{i=1}^m{\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^2}=\frac{1}{2}\left(X\theta -y\right)^T\left(X\theta -y\right)3.1.2 最小二乘法求解对$\theta$求导，梯度（矩阵求导）： \nabla_\theta J\left(\theta\right)=\nabla_\theta\left(\frac{1}{2}\left(X\theta-y\right)^T\left(X\theta-y\right)\right) =\nabla_\theta\left(\frac{1}{2}\left(\theta^TX^TX\theta-\theta^TX^Ty-y^Ty\right)\right) =\frac{1}{2}\left(2X^TX\theta-X^Ty-\left(y^TX\right)^T\right) =X^TX\theta-X^Ty令其为零，求得驻点： \theta=\left(X^TX\right)^{-1}X^Ty3.2 梯度下降法梯度下降法是按下面的流程进行的： 1）首先对$\theta$赋值，这个值可以是随机的，也可是让$\theta$是一个全零的向量； 2）改变$\theta$的值，使得$J(\theta)$按梯度下降的方向进行减少。 为了更清楚，给出下面的图： 这是一个表示参数$\theta$与目标函数$J(\theta)$的关系图，红色的部分是表示$J(\theta)$有比较高的取值，我们需要的是，能够让$J(\theta)$的值尽量的低。也就是深蓝色的部分。$\theta_0$和$\theta_1$表示$\theta$向量的两个维度。 在上面提到梯度下降法的第一步是给$\theta$一个初值，假设随机给的初值是在图上的十字点。然后我们将$\theta$按照梯度下降的方向进行调整，就会使得$J(\theta)$往更低的方向进行变化，如图所示，算法的结束将是在$\theta$下降到无法继续下降为止。 当然，可能梯度下降的最终点并非是全局最小点，可能是一个局部最小点，比如下面这张图中描述的就是一个局部最小点，这是我们重新选择了一个初始点得到的，看来我们这个算法会在很大程度上被初始点的选择影响而陷入局部最小点。 下面对于目标函数$J(\theta)$求偏导数： \frac{\partial}{\partial\theta_j}J\left(\theta\right)=\frac{\partial}{\partial\theta_j}\frac{1}{2}\left(h_{\theta}\left(x\right)-y\right)^2 =2·\frac{1}{2}\left(h_{\theta}\left(x\right)-y\right)\frac{\partial}{\partial\theta_j}\left(h_{\theta}\left(x\right)-y\right) =\left(h_{\theta}\left(x\right)-y\right)x_j下面是更新的过程，也就是$\theta_i$会向着梯度最小的方向进行减少。$\theta$表示更新之前的值，$a$表示步长，也就是每次按照梯度减少的方向变化多少，由于求得是极小值，因此梯度方向是偏导数的反方向，结果为 \theta :=\theta_j+a\left(h_{\theta}\left(x\right)-y\right)x_j一个很重要的地方值得注意的是，梯度是有方向的，对于一个向量$\theta$，每一维分量$\theta_i$都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管他是全局的还是局部的。 在对目标函数$J(\theta)$求偏导时，可以用更简单的数学语言（倒三角表示梯度）进行描述： \nabla_{\theta}J=\left[\begin{array}{c} \frac{\partial}{\partial\theta_0}J\\ ···\\ ···\\ \frac{\partial}{\partial\theta_n}J\\ \end{array}\right] \theta :=\theta +a\nabla_{\theta}J将梯度下降法应用到线性回归有三种方式：批处理梯度下降法、随机梯度下降法。 3.2.1 批量梯度下降法（BGD） 可以看出，参数$\theta$的值每更新一次都要遍历样本集中的所有的样本，得到新的$\theta_j$，看是否满足阈值要求，若满足，则迭代结束，根据此值就可以得到；否则继续迭代。注意到，虽然梯度下降法易受到极小值的影响，但是一般的线性规划问题只有一个极小值，所以梯度下降法一般可以收敛到全局的最小值。例如，$J$是二次凸函数，则梯度下降法的示意图为： 图中，一圈上表示目标函数的函数值类似于地理上的等高线，从外圈开始逐渐迭代，最终收敛全局最小值。 3.2.2 随机梯度下降算法（SGD）在这个算法中，我们每次更新只用到一个训练样本，若根据当前严格不能进行迭代得到一个，此时会得到一个，有新样本进来之后，在此基础上继续迭代，又得到一组新的和，以此类推。 批量梯度下降法，每更新一次，需要用到样本集中的所有样本；随机梯度下降法，每更新一次，只用到训练集中的一个训练样本，所以一般来说，随机梯度下降法能更快地使目标函数达到最小值（新样本的加入，随机梯度下降法有可能会使目标函数突然变大，迭代过程中在变小。所以是在全局最小值附近徘徊，但对于实际应用俩说，误差完全能满足要求）。另外，对于批量梯度下降法，如果样本集增加了一些训练样本，就要重新开始迭代。由于以上原因，当训练样本集较大时，一般使用随机梯度下降法。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（1）：K近邻]]></title>
    <url>%2F2017%2F01%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9AK%E8%BF%91%E9%82%BB%2F</url>
    <content type="text"><![CDATA[一、K近邻算法K近邻算法简单、直观。首先给出一张图，根据这张图来理解最近邻分类器。 根据上图所示，有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形或者红色小三角形），下面，我们就要解决这个问题：给这个绿色的圆分类。 我们常说，物以类聚，人以群分，判别一个人是一个什么样的人，常常可以从他身边的朋友入手，所谓观其友，而识其人。我们不是要判别上图中那个绿色的圆是属于那一类数据么，好说，从他的另据下手。但一次性看多少个邻居呢？从上图中，你还可以看到： 如果K=3,绿色圆点最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。 如果K=5,绿色圆点的最近5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色这个待分类点属于蓝色的正方形一类。 于此，我们看到，KNN算法为给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，这K个实例的多数属于某个类，就把该输入实例分为这个类。 K近邻法算法步骤如下： 输入：训练数据集$T={(x_1,y_1),(x_2,y_2),···，（x_N,y_N）}$，其中，$x_i$是实例的特征向量，$y_i$是实例的类别；新实例的特征向量$x$ 输出：新实例$x$所属的类别$y$ 1)根据给定的距离度量，在训练集$T$中找出与$x$最邻近的$k$个点，涵盖这k个点的$x$领域记作$N_k(x)$; 2)在$N_k(x)$中根据分类决策规则（如多数表决）决定$x$的类别$y$: y=arg\underset{c_j}{\max}\sum_{x_i\in N_k\left( x \right)}^{}{I\left( y_i=c_i \right) \ \ ,\ \ i=1,2,···,N;j=1,2,···,K}其中$I$为指示函数，即当$y_i=c_i$时为1，否则为0. 二、K近邻模型2.1 模型k近邻法中，当训练集、距离度量、K值以及分类决策规则确定后，对于任何一个新的输入实例，它所属的类唯一地确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。 2.2 距离度量特征空间中两个实例点的距离可以反映出两个实力点之间的相似性程度。K近邻模型的特征空间一般是N维实数向量空间，使用的距离可以是欧式距离，也可以是其他距离。 欧氏距离：最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中 d\left( x,y \right) =\sqrt{\sum_{i=1}^n{\left( x_i-y_i \right) ^2}} 曼哈顿距离：我们可以定义曼哈顿距离的正式意义为$L1$距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投射的距离总和。 通俗来讲，想想你在曼哈顿要从一个十字路口开车到另一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”,此即曼哈顿距离名称的来源，同时，曼哈顿距离也称为城市街区距离。 d\left( x,y \right) =\sum_{i=1}^n{|x_i-y_i|} 切比雪夫距离： d\left( x,y \right) =\underset{k\rightarrow \infty}{\lim}\left( \sum_{i=1}^n{|x_i-y_i|^k} \right) ^{\frac{1}{k}} 闵可夫斯基距离：它不是一种距离，而是一组距离的定义。 d\left( x,y \right) =\left( \sum_{i=1}^n{|x_i-y_i|^k} \right) ^{\frac{1}{k}} 当p=1时，即曼哈顿距离 当p=2时，即欧式距离 当$p\rightarrow \infty$，即切比雪夫距离 标准化欧氏距离：对样本集先进行标准化$\hat{x}_i=\frac{x_i-\bar{x}}{s}$经过简单的推导就可以得到来标准化欧氏距离。 d\left( x,y \right) =\sqrt{\sum_{i=1}^n{\left( \frac{x_i-y_i}{s} \right) ^2}} 夹角余弦：几何中夹角余弦可用来衡量两个向量方向的相似度，机器学习中借用这一概念来衡量向量之间的相似度。 \cos \left( \theta \right) =\frac{a·b}{|a|·|b|}2.3 K值的选择K值得选择会对K近邻法的结果产生重大影响。 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值得减小就意味着整体模型变得复杂，容易发生过拟合（容易受到训练数据的噪声而产生的过拟合的影响）。 如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减小学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远的训练实例也会对预测器作用，使预测发生错误，且K值得增大就意味着整体的模型变得简单。 如果K=N。那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的（最近邻列表中可能包含远离其近邻的数据点），如下图所示。在实际应用中，K值一般取一个比较小的数值。通常采用交叉验证法来选取最优的K值（经验规则：K一般低于训练样本数的平方根）。 2.4 分类决策规则K近邻法中的分类决策规则往往是多数表决，即由输入实例的K个邻近的训练实例中的多数类决定输入实例的类。 三、K近邻的优缺点3.1 优点 简单、易于理解、易于实现、无需估计参数、无需训练。 适合对稀有事件进行分类（如大概流式率很低时，比如0.5%，构造流失预测模型）； 特别适合多酚类问题，如根据基因特征来判断其功能分类，KNN比SVM的表现要好。 3.2 缺点 懒惰算法，对测试样本分类时的计算量大，内存开销大，评分慢。 可解释性较差，无法给出决策树那样的规则。 当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 KNN是一种懒惰算法，平时不好好学习，考试（对测试样本分类）时才临阵磨枪（临时去找K个近邻），懒惰的后果，构造模型很简单，但在测试样本分类地系统开销大，因为要扫描全部训练样本并计算距离。已经有一些方法提高计算的效率，例如压缩训练样本量。 决策树和基于规则的分类器都是积极学习eager learner的例子，因为一旦训练数据可用，它们就开始学习从输入属性到类标号的映射模型。一个相反的策略是推迟对训练数据的建模，直到需要分类测试样例时再进行。采用这种策略的技术被称为消极学习法lazy learner。最近邻分类器就是这样的一种方法。 四、python代码实现4.1 K-近邻算法简单示例KNN算法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#!/usr/bin/python# coding=utf-8"""Created on Feb 22 2017KNN@author: plushunter"""# coding=utf-8#!/usr/bin/pythonfrom numpy import *import operatorclass KNNClassifier(): def __init__(self,k=3): self._k=k def _calDistance(self,inputX,trainX): dataSetSize=trainX.shape[0] # tile for array and repeat for matrix in Python diffMat=tile(inputX,(dataSetSize,1))-trainX sqDiffMat=diffMat**2 # take the sum of difference from all dimensions,axis=0是按列求和,axis=1 是按行求和 sqDistances=sqDiffMat.sum(axis=1) distances=sqDistances**0.5 # argsort returns the indices that would sort an array.argsort函数返回的是数组值从小到大的索引值 # http://www.cnblogs.com/100thMountain/p/4719503.html # find the k nearest neighbours sortedDistIndicies = distances.argsort() return sortedDistIndicies def _classify(self,sample,trainX,trainY): if isinstance(sample,ndarray) and isinstance(trainX,ndarray) and isinstance(trainY,ndarray): pass else: try: sample=array(sample) trainX=array(trainX) trainY=array(trainY) except: raise TypeError("numpy.ndarray required for trainX and ..") sortedDistIndicies=self._calDistance(sample,trainX) classCount=&#123;&#125;#create the dictionary for i in range(self._k): label=trainY[sortedDistIndicies[i]] classCount[label]=classCount.get(label,0)+1 #get(label,0) : if dictionary 'classCount' exist key 'label', return classCount[label]; else return 0 sorteditem=sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True) #operator.itemgetter(1) can be substituted by 'key = lambda x: x[1]' return sorteditem[0][0] def classify(self,inputX,trainX,trainY): if isinstance(inputX,ndarray) and isinstance(trainX,ndarray) \ and isinstance(trainY,ndarray): pass else: try: inputX = array(inputX) trainX = array(trainX) trainY = array(trainY) except: raise TypeError("numpy.ndarray required for trainX and ..") d = len(shape(inputX)) results=[] if d == 1: result = self._classify(inputX,trainX,trainY) results.append(result) else: for i in range(len(inputX)): result = self._classify(inputX[i],trainX,trainY) results.append(result) return resultsif __name__=="__main__": trainX = [[1,1.1], [1,1], [0,0], [0,0.1]] trainY = ['A','A','B','B'] clf=KNNClassifier(k=3) inputX = [[0,0.1],[0,0]] result = clf.classify(inputX,trainX,trainY) print result#output which type these belongs to/Users/HuaZhang/anaconda2/bin/python /Users/HuaZhang/Desktop/GitHub/machine-lerning/KNN/KNN.py['B', 'B']Process finished with exit code 0 4.2 KNN实现手写识别系统1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/usr/bin/python# coding=utf-8"""Created on Mar 22 2017KNN: Hand Writing@author: plushunter"""from numpy import *import operatorfrom os import listdirimport KNNdef img2vector(filename): returnVect = zeros((1, 1024)) fr = open(filename) for i in range(32): lineStr = fr.readline() # n = len(lineStr) # if not n: # continue for j in range(32): returnVect[0, 32 * i + j] = int(lineStr[j]) return returnVectdef loadDataSet(filedir): FileList = listdir(filedir) m = len(FileList) X = zeros((m,1024)) Y = [] for i in range(m): fileNameStr = FileList[i] classNumStr = int(fileNameStr.split('_')[0]) Y.append(classNumStr) X[i, :] = img2vector(filedir + "/" + fileNameStr) return X,Ydef handWritingClassTest(inputX,inputY,trainX,trainY): cls=KNN.KNNClassifier(k=3) error=0.0 result = cls.classify(inputX,trainX,trainY) for i in range(len(result)): if result[i] != inputY[i]: error+=1 precision_rate =1- error /len(inputY) print precision_rate # return errorRatedef main(): trainDir = "digits/trainingDigits" testDir = "digits/testDigits" trainX,trainY = loadDataSet(trainDir) inputX,inputY = loadDataSet(testDir) handWritingClassTest(inputX,inputY,trainX,trainY)if __name__=="__main__": main()#output precision_rate/Users/HuaZhang/anaconda2/bin/python /Users/HuaZhang/Desktop/GitHub/machine-lerning/KNN/HandWriting.py0.988372093023Process finished with exit code 0]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>K近邻</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笑忘录（4）：二零一七第一天]]></title>
    <url>%2F2017%2F01%2F01%2F%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%884%EF%BC%89%EF%BC%9A%E4%BA%8C%E9%9B%B6%E4%B8%80%E4%B8%83%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[其一二零一六就像是一出异乎寻常的戏码，在戏里，我扮演了一个再寻常不过的角色，狼狈得被汹涌的剧情推搡着向前，迷失在混乱的舞台，舞台中央的鲜花与热泪，在光环底下肆意飞扬，那个失落的孩子站在舞台边呆呆凝视，视力模糊，它们在晃动，狂妄的审视，或者只是在给生活与艺术授予赞词。 如何才可以抓住风？如何才可以让仅存的模样愈加沉醉？如何让这伟大的世界变得安静啊？ 我想要回感受力，我察觉到它们正离我而去，嗅觉不再敏锐，耳旁嘈杂不堪，视觉里容不下美，身体的每一个部位都在宣告，你不要再挣扎，你已结束兴盛的时代，那个王朝早已落幕，你只能徘徊在硝烟四起的边疆了罢。 我不甘啊，我还那么年轻。 其二二零一七呐，有好多期待，我要带着不甘重新上路了。 第一件，物。神奇的物，帮助我们和这个曼妙的世界沟通。它们总能贴合我们的感官，让那些险些失去的感觉聚焦，消沉从而变得细腻可触，欢喜也可以沉浸许久，它们与我们的身体交融。 私人物品就像一种符号，为其在和自己关系密切的现实世界与社交网络中的身份提供具象化的证据。它是生活故事的标签，这些故事依附于私人物品铺展开来，串联起一个人的过去、现在与未来。 携带着与气质相符的物，让物呈现你的身体与灵魂吧，又何尝不可呢，不要说话，闭上眼睛，不要透露心声，关上襟怀。物不是其它，它就是你。 第二件，人。怎么会有那么多可爱的人呢？我历数不过来了，我默默看着他们走过、停留、消逝，然后有一天，又重新出现，谁知道是哪一天，谁又知道会出现在哪个场景里，我唯一可以做的，便是等待着随便哪一种未来。 有的人，因为音乐而靠近有的人，因为日常运作而汇合有的人，因为城市的穿梭而擦肩而过有的人，你永远遇见不到，但它们都有繁盛的心灵花园，你要学会自己去探望。 第三件，事。 我做不好很多事情，它们有的躺在我的计划簿子里，有的早已糜烂，有的做了一半就没有兴致了，有的也因为心有余而力不足，变得苍白烂尾。能做的就那么多，唯一希望的便是可以做的更好一些。 忽然觉得计划这玩意儿真神奇，它跨越时间区间去规定通往未来之路，让未来似乎变得清晰可触，在计划里，每个人都是自恋的，他们看到了更加完好的自己，从这种虚假的完备换取了瞬时的慰藉，每次向计划簿里添加精致的清单之时，便会看到镜像中的自己，绚丽地燃烧。而慢慢地，热情燃烧殆尽，才发觉，自己偏移的太远。 其三二零一七要做更多有趣的事啊，生活雅致还得有。 计算机真是一个让人着迷的东西，还有很多的未知领域需要探索，让好奇心与求知欲都尽情地来驱动自己吧。 机器学习同样让人痴迷，16年下半年的时间支离破碎的，都没有系统的梳理下，作为今年的主线吧。 你好久不读书了，清单正在消沉，逐渐变得索然无味，当你再次拾起的时候，我相信它们会踊跃呼唤你的名。 美食、音乐、旅行，这些美妙生活的配方，没有它们，可能就没有那些人了，你也失去了让感受力生根发芽的机会。 分享真是是一件让人振奋的事，一个人的日志，可以直接通往他的心灵花园，所以很多人选择把它藏起来，那是属于自己的东西。很多人只与自己说真话，虽然看起来他和很多人对话；也有很多人不与别人说话，它们还没有准备好，它们害怕话一说出口就会变得浅薄无趣，就索性让自己保有最温热的部分了。]]></content>
      <categories>
        <category>笑忘录</category>
      </categories>
      <tags>
        <tag>笑忘录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笑忘录（3）：享书记]]></title>
    <url>%2F2016%2F12%2F24%2F%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%883%EF%BC%89%EF%BC%9A%E4%BA%AB%E4%B9%A6%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[小序现在书就在我的手上，约摸有个十来本，我想说说我接下来和他们即将擦出的火花。 对，书还没放上书架，买来只是随意堆在床边，还没有带来和井然有序俱来的那种轻微无聊的单调滋味，我还没有准备好把淘来的书展示给朋友们看，就像是没有精心剪辑的影片，那种随意拼凑出来的画面固然有吸引力，但是我会赋予更多关于藏书的意义（每本书的每一册都有其命运）。 其一 从我接手一本书那一时刻起，我便承担起了这本书的美妙的命运，或者说这本书将在我的手上重生，这会是一次有价值的邂逅，和自己产生共鸣的那一刻我便认定了这孩子，他让我同样获取了孩童心态，我就像是个孩子一样抚摸每一页，感受那种苍老或幽静的感觉，我为他取一个专属于我与他之间的名字，遇到一个好的主人成为你存在在这个世界上的荣耀，你可以安详的躺在我的书架上，享受散发你香味的漫长的一生，哪一天或许我心血来潮了，我会来看看你，或者和你深入的交往沟通，你需要理解我占有你时陶醉的心情，我也会因为复活你的生命沾沾自喜，于你而言，就像是复活了一个时代，那是我最深层次的动机。 我享受这样的快感——我将你们锁入我的圈子，永久成为我无法拒绝的暧昧对象。每一个关于你的回忆和念头，便是我的所有财富的基座，支架和锁钥，我打开每一个味觉嗅觉视觉触觉神经细胞，探索你的每一个细节，你的精髓我会暗自藏在心里，由此引发的灵感仿佛可以透过你们看到遥远的过去，出版那一天的情形，那位伟岸的作者在字里行间留下的最捉摸不透的暗号，装帧师傅设计封面时所被浸透的头脑风暴，你以前的那个主人如何把你捧在手心私密的对话。 其二该是遇到怎样的多舛而繁华的命运才可以到我手上，我感谢每一位呵护你照顾你帮助你与你彻夜畅谈的每一位先人们，现在我是“老者”，你会陪伴我走向我最寂寥最繁荣的日子。 我或许不会再回头去看你，就如曾经有个庸人赞美一番阿那托尔·法朗士的书斋，最后问了一个常见的 问题：“法朗士先生，这些书您都读过了吗？”回答是足以说明问题的： “还不到十分之一。不过我想您并不是每天都用您的塞弗尔瓷器吧？” 我想你安安静静的躺在我帮你精心设计的书架上，和周围的那些你的兄弟姐妹们和谐的交谈，就像是智者之间毫无遮拦的对话，那是你们伟大的灵魂有缘的碰撞和融合，又偶尔，我会来和你们打个招呼，或和你们中的某一位共处出在小屋子中，一盏台灯， 一杯清茶，度过一个思想火花四溅的午后。 我突然敞开怀笑了，哦， 藏书者的一大乐事，散逸人的一大福祉。我愿无声无息，无誉无毁躲在施比兹韦格的“书虫”面具后。没有人比这样的人更有富足感 了，因为附体的神灵或是精怪，会让爱着你们的人——与你们保持着最为亲密的拥有关系。就像是你们活在我的精神世界里面一样，我不会忍心舍弃你们而去。]]></content>
      <categories>
        <category>笑忘录</category>
      </categories>
      <tags>
        <tag>阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笑忘录（2）：疯子]]></title>
    <url>%2F2016%2F12%2F20%2F%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%882%EF%BC%89%EF%BC%9A%E7%96%AF%E5%AD%90%2F</url>
    <content type="text"><![CDATA[其一二十一世纪的第十三个年头，顺势来到的一天，圣索菲亚教堂前，游客不多，一些人交流，一些人快步，一些人，像我一样，对着教堂说话，像是对着自己。 丹尼斯大街与路易斯大街交汇的十字路口中央，高空中悬着一个雕塑，底下刷成灰色的塑像，凝视状。街边的的花满楼里，歌妓一刻不停，欢歌轻舞，北上的旅人在肉欲慰藉里且度今宵。 一个神经病突然发出声，左边，右边，上头，后面，四周的人头朝向了一个声音，他开始搜索眼神，轻率，讶异；唇角，僵硬，微微收拢；线条轮廓，百无聊赖的平淡，妇女的乳房变得苍白，持续的缓慢的最终鲜明确凿的凸现：抑郁寡欢。 格格不入，对峙，退却。他捂住耳朵，周围的表情幻化成压抑的声源，他没有停止搜寻的迹象，那是一件危险的事。 那是两天前发生的事，他来到了夜幕下的哈尔滨。 其二为了躲开原本那座城市刺眼的眼神，他才开始北上。几天前，他读毕一本书，八百余页，一整天，于是就再也看不清路人了，他没有变瞎，慢跑到紫禁城红墙下，已是深夜，红墙看不明颜色，但他知道颜色恰巧到中古悠然，也知道这是最契合这座城市意蕴的地方，他不准备停留，沿着府右街过景山前街，在南北池子大街的交汇处的东华门处停下，路上一对夜归的老夫妻互相搀扶前行，凝视十分钟，他没有意识到自己正对着他们傻笑，老夫妻被吓得赶忙加快脚步，一瘸一拐，他对着空白的街景继续傻笑。 十分钟时间不长，对他的外在来讲也不长，只是意识流开始缠上身，前两分钟，唐宋时代的风俗人情，三四分钟；来自柏拉图的洞穴的幻影浮现；五六分钟，霸王别姬，哪吒传奇；六七分钟，竖排繁体的《脂砚斋重评石头记》，优雅笃定的当下感，博尔赫斯在图书馆，沈从文的边城；最后三分钟，《洛丽塔》，《人间失格》，《一个陌生女人的来信》，《2666》，《荒原狼》，《树上的男爵》，《瓦尔登湖》，《伊豆的舞女》，《到灯塔去》，《都柏林人》，若即若离。 他累了，找个旅馆睡下，一头扎进了一个没有亮光的胡同，那是一家老店。沦落为蜗居在老城区角落的廉价旅馆，早已徒有虚名。窄小巷子中的灰白色混凝土小楼，如同所有以临时心态搭建的建筑，苟且度日。接待处服务员，胖而迟钝的中年妇女，磕瓜子看着面前的电视屏幕，麻木的表情。走廊上铺陈一长条红色地毯，清洗。睡觉对他来说还不算是一件煎熬的事，可以在十分钟内很快睡去。夜深了。 其三这是他常会有的那几天，业已习惯了这样的日子，例如还有几次，他一个人乘着火车，火车往南开，去的是一个中部城市，城市很大。 火车上，乘客不多。一些时间说话，一些时间睡觉，一些时间喝水与看望黑夜，一些时间思考不着边际的问题。10个小时后，火车抵达值夏的城市。下车，出地道。出站口两扇敞开木门，一角灰蓝色天空。微风缭绕。广场上出租汽车和三轮车颇显冷落，生意寥寥。在清晨的光景里，这个城略显闹腾，低矮旧楼被雨水洗刷成暗色，路边耸立广告牌上，词汇带有时光倒退30年的落伍气息。他的精神一振，知道来到正确的地方，稍许加快了脚步，往前。]]></content>
      <categories>
        <category>笑忘录</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[笑忘录（1）：第二十二封情书]]></title>
    <url>%2F2016%2F12%2F05%2F%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%881%EF%BC%89%EF%BC%9A%E7%AC%AC%E4%BA%8C%E5%8D%81%E4%BA%8C%E5%B0%81%E6%83%85%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[还记得上一次给你写信是你从南方回到北方的那个夜晚，我激动地关掉播放到一半的电影，颇似少女祈祷般，浸润在回忆过往的文字中，静静地等待着那个北方姑娘，回到属于她的地方。正是那天，我感觉到我在这里的生活渐渐有了生机。现在，距离你来的那一天已经过去快三年了，我已记不清我们有过多少次争吵，但回想起来那些模糊的场景，却总觉还是温暖的，那正是我们共同度过寻常生活的印记。 昨天我们又争吵了。 回想这些天发生在我们之间的争吵、不合拍、嫌弃、甚至厌恶，我不知道有一些情感是否来的真实，但它确实从我们内心生发出来了，回头看那些生涩的言语，都让我倍感无助，感觉到心情的起伏已经无法被理智控制，就像河水猛兽般汹涌的侵蚀自己敏感却又极力克制的心，无法逆转，无法平复，根本无法像平常那样和周围的人说话。我总会在想：爱情是不痛苦的，它是纯快乐，不该掺进别的，尤其不该掺进痛苦，记得一首外国诗这样说：“啊！“爱情”！他们大大的误解了你！他们说你的甜蜜是痛苦，当你丰富的果实比任何果实都甜蜜。” 然而，我们真切的在爱情里尝到过痛苦，撕心裂肺的感觉。面对离别，我们却只能远望对方的背影渐渐离去；面对不理解，脑海里只会倍增对对方的不满；如若行为与本人价值观不符，便会用自己的意志控制对方，使对方的自由关进牢笼；如若争辩，我们也异常坚定的秉持自己的立场，尝试使用任何办法说服，如果对方的乖张让你感到舒适，战斗才停歇下来，否则就是一场无穷无尽的无聊的口舌之争，对，我们也常常使用这个套数让对方处于劣势（我不想在这里和你发生这些无聊的口舌之争）；服软的那一方看似更爱对方一些，不服输的总是多爱自己一些。 我们都在做一些我们自己也无法理解的事，我们沉浸在不理性，肆意泼洒的情绪里，我们在戏剧舞台上疯狂地表演，我们虚伪地接受爱情荒诞可笑的模样。有了爱，我们便有了最长的触角，伸向我们不曾触及的外部世界，伸向早已麻木倦怠的感官，但同时也把我们最柔软的部分显露在对方面前，大部分时间它得到了铠甲的庇护，而总有一天会有数不尽的痛触，让你质问爱情的真面目。 “什么是爱，爱是恒久忍耐，爱是彼此包容，爱是相对付出”出自圣经，高中的时候，和班上的同学你问我答，说得好轻巧，但当注入真实体验的时候，我们已经开始怀疑，我们所经历的爱情，到底是不是真正的爱情了。在现实的情境与企盼的盛景有落差时，我常有幻灭的感觉，爱情难道不是那样吗？爱情不该是这样的，它不会辜负良苦用心的我们的吧？ 罗兰巴特对爱情的不同阶段的场景有这么一段描述： 尽管恋人的表述仅仅是纷纭的情境，他们骚动起来全无秩序可言，不比在屋子里胡飞乱舞的苍蝇的轨迹更有规律，我还是能——至少是在回忆或者想像中——给爱情的发展找出一定的规律来。爱情的旅程似乎分为三个阶段（或者三幕戏）：首先是一见钟情，是闪电般的“迷上”“被俘虏”（我被一个形象迷住了）；然后便是一连串的相逢（约会、电话、情书、短途旅行），在此期间，我如痴如醉地发掘着情偶的完美，也就是说，对象与我的欲望之间那种完全出乎我意料的契合：这是初时的柔情，田园诗一般的光阴。在这幸福时光之后便是“一连串”恋爱的麻烦——持续不断的痛苦、创伤、焦虑、忧愁、怨恨、失望、窘迫还有陷阱——我们都成了里面的困兽，老是提心吊胆，生怕爱情衰退，怕这衰退不仅会毁了对方和我，还会毁了当初的缘分，那种神奇的情投意合。从这漫长的隧道中走出来，我又能重见天日了：这也许是因为我成功的找到了解决了不幸爱情的辩证出路——维持爱情、但脱离梦幻，冷静现实的面对它。） 我想我们正在经历如他所述的爱情低谷，我们眼前一片黑暗，对方的形象也因此而模糊、歪曲。爱情在这个年纪注定是不能冷静的，在荷尔蒙的鼓动下，它露出了狂傲不羁的模样，我们的一半是孩子，一半又竭力挣脱这样的形象。但总有一天会磨合出一个更为安静的环境供爱情生长，愿爱情如我们所期盼的那样，我们会脱离梦境、从那漫长的黑暗隧道中走出来，如你所说：内心笃定，爱则长久。我们为了彼此，做最好的自己，然后留给岁月。在长久的陪伴中，我们越来越相似于彼此，直到连微笑的弧度都一样。直到看着越来越像彼此的自己埋怨不起来。我想，这就是我们的爱情，它来源的没有任何预兆，也没有任何可以消失的理由。它并不逊色生离死别的爱情，相比之下更多了青春奋斗的最美好的回忆。 晚安。]]></content>
      <categories>
        <category>笑忘录</category>
      </categories>
      <tags>
        <tag>笑忘录</tag>
      </tags>
  </entry>
</search>
