<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[日知录（4）：2018年人大应用统计专业课真题]]></title>
    <url>%2F2017%2F12%2F29%2F%E6%97%A5%E7%9F%A5%E5%BD%95%EF%BC%884%EF%BC%89%EF%BC%9A2018%E5%B9%B4%E4%BA%BA%E5%A4%A7%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E4%B8%93%E4%B8%9A%E8%AF%BE%E7%9C%9F%E9%A2%98%E5%8F%8A%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[2018年人大应用统计专业课真题 解析待补充 一、第一题【20分】1.1 请说明雷达图和箱线图的基本要点。1.2 下面的数据集为8个同学的数学、语文和英语的成绩，如何利用雷达图和箱线图来描述这个数据集？ 姓名 数学 语文 英语 甲 83 86 82 乙 93 89 93 丙 85 79 90 丁 79 81 75 戊 79 81 75 己 75 70 94 庚 69 62 94 辛 67 62 94 二、第二题【20分】2.1 说明在方差已知的条件下，正态总体均值区间估计的宽度与样本量的关系。 2.2 现在有一组来自正态总体的随机样本，可以由此得到在方差已知和方差未知两种条件下的置信区间，请分析这两个置信区间的中点和宽度的异同。 三、第三题【20分】3.1 给出一个列联表，写出可以描述上述数据的所有的图形，并说明这些图形的用途，3.2 写出可以分析上述数据所有可能的方法，并说明用途。 四、第四题【10分】设因变量为$y$ ,自变量为$x_1,x_2,x_3·····x_k$，写出建立多元线性回归建模的基本思路。 五、第五题【20分】5.1 方差分析有哪些基本假定？5.2 简要说明检验这些假定的方法。 六、第六题【20分】【20分】在同一个概率空间中是否存在三个随机事件$A,B,C$使得同时成立下面三个不等式： P(A|B,C)≤P(A|\bar {B},C) \\\ P(A|B,\bar {C})P(A|\bar B)如果存在，请列举一个例子；若不存在，证明你的结论。 七、第七题【20分】设$x_1,x_2······,x_n$为一个来自均值为$\mu$，方差为$\sigma^2$的分布的样本，$\mu$和$\sigma ^2$未知，考虑均值$\mu$的线性无偏估计类 L=\{T\left(X\right)：T\left(X\right)=\sum_{i=1}^n{x_i}.c_i \} \\\ 其中c_i是常数求出$L$中$T(X)$为$\mu$的无偏估计的充要条件，并求出无偏估计类中方差一致最小的估计。 八、第八题【10分】设$X$是一个正值随机变量，方差有界，证明：对于$\forall 0&lt;\lambda &lt;1$, 有 P\left\{X>\lambda EX\right\}\geqslant\left(1-\lambda\right)^2\frac{\left(EX\right)^2}{EX^2}九、第九题【10分】 设地区生产总之（亿元）为因变量，固定资产投资（亿元）、社会消费品零售总额（亿元）、出口总额（亿美元）、地方财政收入（亿元）、电力消费量（亿千瓦时）、居民消费水平（元）为自变量，根据32个样本数据得到回归结果如下： Coefficients Estimate Std. Error t value Pr(t) -2.377 e+03 1.166 e+03 -2.038 0.05270 固定资产投资 4.504 e-01 8.166 e-02 5.515 1.14 e-05 * 社会消费品零售总额 1.110 e+00 1.572 e-01 7.060 2.68 e-0.7 * 出口总额 1.887 e+01 6.379 e+00 2.958 0.00686 ** 地方财政收入 9.596 e-01 6.959 e-01 1.379 0.18061 电力消费量 6.683 e-01 5.671 e-01 1.178 0.25016 居民消费水平 1.194 e-01 6.949 e-02 1.718 0.09868 Residual standard error: 1526 自由度 24 Multiple R-Squared: 0.9944 Adjusted R-squared 0.993 F -statistic: 708.8 P-Value &lt; 2.2 e-16 对该回归模型进行综合分析，评价是否需要改进，并给出思路。【10分】]]></content>
      <categories>
        <category>日知录</category>
      </categories>
      <tags>
        <tag>人大应统</tag>
        <tag>考研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日知录（2）：心理账户]]></title>
    <url>%2F2017%2F12%2F12%2F%E6%97%A5%E7%9F%A5%E5%BD%95%EF%BC%882%EF%BC%89%EF%BC%9A%E5%BF%83%E7%90%86%E8%B4%A6%E6%88%B7%2F</url>
    <content type="text"><![CDATA[2002年10月9日，瑞典皇家科学院将诺贝尔经济学奖授予普林斯顿大学心理学教授DannielKahneman和乔治·梅森大学经济学教授VernonSmith。根据瑞典皇家科学院的新闻公报，卡尼曼“将心理学的深入分析融入到了经济学中，从而为一个崭新的经济学研究领域奠定了基础”。1981年，DannielKahneman及其合作者AmosTversky在《科学》杂志发表论文，研究人们决策过程的认知心理规律。文章介绍了“演出实验” 【实验情境A】：你打算去剧院看一场演出，票价是10美元，在你到达剧院的时候，发现自己丢了一张10美元的钞票。你是否会买票看演出？实验表明：88%的调查对象选择会；12%的调查对象选择不会。（调查对象为183人） 【实验情境B】：你打算去看一场演出而且花10美元钱买了一张票。在你到达剧院的时候，发现门票丢了。如果你想看演出，必须再花10美元，你是否会买票？实验结果表明：46%的调查对象选择会，54%的调查对象不会。（调查对象为200人） Kahneman和Tversky认为:两种实验情境出现明显不同结果的原因在于:在考虑情境A的决策结果时，人们把丢失的10美元钞票和买演出票的10美元分别考虑；而在情境B中，则把已经购买演出票的钱和后来买票的钱放在同一个账户估价，一部分人觉得“太贵了”改变自己的选择。为此，Kahneman和Tversky引入理查德·萨勒教授（RichardThaler）提出的“心理账户”概念，对实验结果进行了深入的分析。 一、概念发展1.1 理查德·萨勒1980年，芝加哥大学著名行为金融和行为经济学家理查德·萨勒（RichardThaler）首次提出“Psychic Accounting（心理账户）”概念，用于解释个体在消费决策时为什么会受到“沉没成本效应（sunk cost effert）”的影响。萨勒认为：人们在消费行为中之所以受到“沉没成本”的影响，一个可能的解释是卡尼曼教授等提出的“前景理论”，另一个可能的解释就是推测个体潜意识中存在的“心理账户系统”（Psychic Accounting system）。人们在消费决策时把过去的投入和现在的付出加在一起作为总成本，来衡量决策的后果。这种对金钱分门别类的分账管理和预算的心理过程就是“心理账户”的估价过程。 1.2 丹尼尔·卡尼曼1981年，丹尼尔·卡尼曼和特韦尔斯基(Amos Tversky)在对“演出实验”的分析中使用“Psychological Account(心理账户)”概念，表明消费者在决策时根据不同的决策任务形成相应的心理账户。卡尼曼认为，心理账户是人们在心理上对结果(尤其是经济结果)的分类记账、编码、估价和预算等过程。 1984年，卡尼曼教授和特韦尔斯基教授认为“心理账户”概念用“mental account”表达更贴切。卡尼曼认为:人们在做出选择时，实际上就是对多种选择结果进行估价的过程。究竟如何估价，最简单也最基本的估价方式就是把选择结果进行获益与损失(得失)的评价。因此，他提出了“值函数”假设和“决策权重”函数来解释人们内在的得失评价机制。 1.3 理查德·萨勒1985年，萨勒教授发表“心理账户与消费者行为选择”一文，正式提出“心理账户”理论，系统地分析了心理账户现象，以及心理账户如何导致个体违背最简单的经济规律。萨勒认为:小到个体、家庭，大到企业集团，都有或明确或潜在的心理账户系统。在作经济决策时，这种心理账户系统常常遵循一种与经济学的运算规律相矛盾的潜在心理运算规则，其心理记账方式与经济学和数学的运算方式都不相同。因此经常以非预期的方式影响着决策，使个体的决策违背最简单的理性经济法则。萨勒列举了4个典型现象阐明心理账户对传统经济规律的违背，并提出了心理账户的“非替代性”特征。 1.4 特韦尔斯基1996年Tversky提出，心理账户是一种认知幻觉，这种认知幻觉影响金融市场的投资者，使投资者们失去对价格的理性关注，从而产生非理性投资行为。Kivetz(1999)认为，心理账户是人们根据财富的来源不同进行编码和归类的心理过程，在这一编码和分类过程中“重要性-非重要性”是人们考虑的一个维度。有学者从行为的角度对“心理账户”进行定义，认为心理账户是个人或家庭用来管理、评估和记录经济活动的一套认知操作系统，这套认知操作系统导致一系列非理性的“心理账户”决策误区。 1.5 理查德·萨勒1999年，萨勒发表“mental accounting matters”一文，这是对近20年“心理账户”研究的一个总结。在文章中，萨勒认为:心理账户的三个部分最受关注，首先是对于决策结果的感知以及决策结果的制定及评价，心理账户系统提供了决策前后的损失——获益分析;第二个部分涉及特定账户的分类活动，资金根据来源和支出划分成不同的类别(住房、食物等)，消费有时要受制于明确或不明确的特定账户的预算;第三个部分涉及账户评估频率和选择框架，账户可以是以每天、每周或每年的频率进行权衡，时间限定可宽可窄。因此，“心理账户”是人们在心理上对结果(尤其是经济结果)的编码、分类和估价的过程，它揭示了人们在进行(资金)财富决策时的心理认知过程。 二、心理账户的非替代性（non-fungibility）按照传统的微观经济学理论，金钱不会被贴上标签，它具有替代性(fungibility)，事实上，越来越多的实证研究表明: 人们并不是把所有的财富放在一个整体账户进行管理，每一元钱与每一元钱可以很好的替换与转移。相反，人们根据财富来源与支出划分成不同性质的多个分账户，每个分账户有单独的预算和支配规则，金钱并不能容易地从一个账户转移到另一个账户。 萨勒将这种金钱不能很好转移，不能完全替换的特点称之为“非替代性”。萨勒教授在研究中发现金钱非替代性的一些表现: 【不同来源】：由不同来源的财富而设立的心理账户之间具有非替代性，例如意外之财和辛苦得来的钱不具替代性。一般来说，人们会把辛苦挣来的钱存起来不舍得花，而如果是一笔意外之财，可能很快就花掉。 【不同消费项目】：不同消费项目而设立的心理账户之间具有非替代性。我们来看一个案例:王先生非常中意商场的一件羊毛衫，价格为1250元，他觉得贵而舍不得买。月底的时候他妻子买下羊毛衫作为生日礼物送给他，他非常开心。尽管王先生的钱和他的妻子的钱是同一家庭的钱，为什么同样的钱以不同的理由开支心理感觉不同?研究表明:自己花费购买羊毛衫，属于生活必需开支，1250元太贵了;而作为生日礼物送给丈夫，属于情感开支。因此人们欣然接受昂贵的礼品却未必自己去买昂贵的物品。可见，为不同的消费项目设立的心理账户之间具有非替代性。 【不同存储方式】：不同存储方式导致心理账户的非替代性。萨勒教授举的一个实例。约翰先生一家存了15000美元准备买一栋理想的别墅，他们计划在5年以后购买，这笔钱放在商业账户上的利率是10%;可最近他们刚刚贷款11000美元买了一部新车，新车贷款3年的利率是15%，为什么他不用自己的15000美元存款买新车呢?通常，人们对已经有了预定开支项目的金钱，不愿意由于临时开支挪用这笔钱，对这个家庭来说，存起来买房的钱，已经放在了购房这一预定账户上，如果另外一项开支(买车)挪用了这笔钱，这笔钱就不存在了。从理性上说，家庭的总财富不变。但因为财富改变了存放的位置，固定账户和临时账户具有非替代性，人们的心理感觉不一样。 三、心理账户的运算规则在日常经济活动中，人们是如何操纵和管理心理账户，这些经济交易在人们心里是如何评估和被体验的呢?萨勒认为:人们在进行各个账户的心理运算时，实际上就是对各种选择的损失-获益进行估价，称之为“得与失的构架(the framing of gains and losses)”，人们在心理运算的过程中并不是追求理性认知上的效用最大化，而是追求情感上的满意最大化。 情感体验在人们的现实决策中起着重要的作用，他将这种运算称之为“享乐主义的加工”（hedonic editing） 3.1 值函数的假设为了更好地探讨心理账户的价值运算如何影响人们的经济决策行为，卡尼曼教授在“前景理论”中提出了“值函数”(value function)这一概念。与以往经济理论中的“效用函数”(utility function)相比，值函数有三个重要的特征。 值函数是人们在决策行为时对于某个参照点的相对得失的详细说明，人们的“得与失”是个相对概念而不是期望效用理论的绝对概念。人们对某一决策结果的主观判断是相对于某个自然参照点而言，而不是绝对的财富或经济。因此，参照点的变化会引起人们主观估价的变化，人们更关注的是围绕参照点引起的改变而不是绝对水平。 “得与失”都表现出敏感性递减的规律。值函数的曲线是一条近似“S”形的曲线，右上角的盈利曲线为下凹形(concave)，左下角的亏损曲线为上凸形(convex)(如图所示)。离参照点(坐标交叉的原点)愈近的差额人们愈加敏感，越是远离参照点的差额越不敏感。因此，不管是获得还是损失，人们感觉到10元到20元的差额似乎比1000元到1010元的差额更大，这反映了价值曲线的边际递减特征。 损失规避。卡尼曼教授认为:同等数量的损失比获益对人的影响更大，因此在决策的时候人们尽量回避损失，表现在价值函数曲线上，损失曲线的斜率比获益曲线的斜率更大(如图1所示)，用公式表示为V(X)&lt;-V(-X)。例如损失1000元钱所带来的痛苦比获得1000元奖金而带来的愉悦更强烈。因此，面临损失时，人们是风险偏好的;面临获得时，人们是风险规避的。 由于值函数的三个典型特征，对心理账户的运算规则至少有三个启示: 相同的决策结果表述为损失或者获益会改变人们的风险决策偏好; 设计不同的参照点会改变人们对决策结果的认知; 同样的价格差额在不同的原始价格下，影响作用是不同的。 3.2 得与失的编码规则根据上述值函数的特点，萨勒在关于心理账户的研究中，将值函数在得与失的不同组合结果中的偏好情况作了分析。 规则一：两笔盈利应分开 假如两笔收入X、Y均为正，分开价值为$V(X)+V(Y)$，整合值为$V(X+Y)$。因价值曲线在右上角为凸形，所以$V(X)+V(Y)&gt;V(X+Y)$，个体更偏好分开体验(如图)。假如想送朋友两件礼物——一套衣服和一个健身器，最好分两次送。每次送一件礼物所带来的心理体验比一次送两件礼物的心理体验高。 规则二：两笔损失应整合 两笔支出对个体而言是“损失”，因价值曲线在左下角为凹形，所以$V(-X)+V(-Y)&lt;V(-X-Y)$，个体更偏好整合价值。这一规律可以解释生活中的很多现象，比如开会收取会务费时，最好一次收齐并留有余地，若有额外开支一次次增收，虽然数量不多，会员仍会牢骚满腹。 规则三:大得小失应整合 两笔收入一正一负:X，-Y，且余额为正，即$X&gt;Y$，从价值曲线看应是$V(X)+V(-Y)&lt;V(X-Y)$，所以人们更偏好整合。这条规则给人们的启示是，如果你有一个大的好消息和一个小的坏消息，应该把这两个消息一起告诉别人。如此整合，坏消息带来的痛苦会被好消息带来的快乐所冲淡，负面效应也就小得多。 规则四:小得大失应具体分析 两笔收入一正一负:X，-Y，且余额为负，即X&lt;Y，此时应分两种情况: 其一，小得大失且悬殊很大，应分开估价。从图中看出$V(X)+V(-Y)&gt;V(X-Y)$，因此，分开估价的心理体验要好，这种现象称为“银衬里(silver lining)”规则。例如(40，-6000)，人们更愿意分开估价，因为价值曲线在-6000元附近相对较平缓，40元的获得与6000元的损失相比几乎没有减少损失的作用，分开估价还能得到40元收益的感觉。 其二，小得大失且悬殊不大，应整合。如(40，-50)，人们更偏好整合价值。表现为$V(X-Y)&gt;V(X)+V(-Y)$。整合估价时，人们在心理会把损失从50元降低到10元，这样的损失就显得小了，心理体验更好，整合估价的作用体现出来。 萨勒进一步把这四条规则概括为: 分离收益; 整合损失; 把小损失与大收益整合一起; 把小收益从大损失中分离出来。 以上心理账户的运算规则对于理解和解释现实经济决策行为有重要的指导意义。 四、应用研究4.1 价格感知——绝对值优惠与相对值优惠1982年，特维尔斯基教授和卡尼曼教授通过设计以下情景实验引入“心理账户”与消费者购买决策行为的研究。 【实验情景A】:假定你要买一件夹克和一个计算器。在某商场夹克的价格是125美元，计算器的价格是15美元。这时候有人告诉你，开车二十分钟后另一个街区的一家商场计算器的价格是10美元。请问:你会去另一个商场买计算器吗? 【实验情景B】:假定你要买一件夹克和一个计算器。在某商场夹克的价格是15美元，计算器的价格是125美元。这时候有人告诉你，开车二十分钟后另一个街区的一家商场计算器的价格是120美元。请问:你会去另一个商场买计算器吗? 在这两个情境中，其实都是对“是否开车20分钟从140美元的总购物款中节省5美元”做出选择。然而，实验对象在两个情境中的回答却不一样。在情境A中，68%的实验对象选择去另一家商场;而在情境B中，只有29%的实验对象选择开车去另一家商场。选择偏好发生了逆转。 卡尼曼提出，消费者在感知价格的时候，是从三个不同的心理账户进行得失评价的。一个是最小账户(minimal account)，就是不同方案所优惠的绝对值。在本实验中的最小账户就是5美元。另一个是局部账户(topical account)，也可称为相对值账户。例如，在实验情境A中开车前往另一家店的“局部账户”表现为计算器价格从15美元降为10美元(相对差额为1/3);而在实验情境B中的“局部账户”表现为计算器价格从125美元降为120美元(相对差额为1/25)。第三个是综合账户(comprehensive account)，综合账户就是总消费账户，该实验的综合账户为140美元。 卡尼曼认为，在上面的实验中，消费者是自发运用了局部账户，即通过相对优惠值来感知价格。情境A有33.3%的优惠;而情境B仅有4%的优惠。因此，人们的购买行为发生了反转。表现为在实验情境A中，68%的实验对象选择去另一家商场;而在实验情境B中，却只有29%。 此后，Philip Moon,Kevin Keasey,Darren Duxbury对卡尼曼的研究进行了重复实验并且提出，当优惠超过某个阈限值的时候，消费者对绝对优惠值同样非常敏感。绝对值优惠与相对值优惠之间存在一种关系。 4.2 行为生命周期理论——心理账户在消费领域的应用经典的生命周期假说和持久收入假说是凯恩斯以后消费函数理论最重要的发展，但他们的理论是建立在完全理性人的假设之上的。例如，生命周期假说就认为:人总是能够深谋远虑，在任何时候都会考虑几十年以后的长远利益，并站在这种高度，根据一生的总财富来合理安排一生中每个阶段的消费，使一生的总效用达到最大。这显然和人们实际的消费行为不符，这种过于理性化的理论也无法解释现实中的许多经济现象。 1988年Shefrin和Thaler提出行为生命周期理论(behavior life cycle hypothesis)修正了传统的生命周期假说，使之能更好地描述现实中人们的消费行为。行为生命周期理论的两个最重要的概念是自我控制和心理账户。 行为生命周期理论引入“心理账户”理论解释消费行为。消费者根据生命周期不同财富的来源和形式，将它们划分为三个心理账户:现期可花费的现金收入账户(I)，现期资产账户(A)和未来收入账户(F)。行为生命周期理论认为:不同账户的财富对消费者的决策行为是不同的。现金收入账户消费的诱惑力最大，因此，将这个账户的收入不消费而储蓄起来的心理成本也最大;现期资产账户的诱惑力和储蓄的心理成本居中;未来收入账户的诱惑力和储蓄的心理成本最小。由于不同的心理账户对消费者的诱惑不同，所以，消费者倾向于较多地通过现金收入账户消费，而较少通过现期资产账户消费，几乎不通过未来收入账户消费。不仅不同的心理账户对消费者的诱惑是不同的，而且同一个心理账户，其中的财富余额不同，对消费者的诱惑也不同。财富余额越多，诱惑越大。 行为生命周期理论的消费函数可表示为$C=f(I,A,F)$，且有:$1≈C/I&gt;C/A&gt;C/F≈0$。这就是说，现金收入账户的边际消费倾向最大，接近于1;现期资产账户次之;未来收入账户最小，接近0。和生命周期持久收入假说的消费函数相比，行为生命周期理论在分析消费者行为时强调的是心理方面的因素，这些心理因素主要是通过心理账户加以描述。所以，心理账户的划分及其性质是理解行为生命周期理论的关键。 4.3 关于消费预算的研究1994年Heath和Soll发现，消费者有为不同的消费支出账户设置心理预算的倾向，并且严格控制该项目支出不超过合适的预算。例如，每个月的娱乐支出300元，每个月的日常餐饮消费1000元等。如果一段时间购买同一支出项目的总消费额超过了预算，人们会停止购买该类产品。即使在同一个消费项目中，不同的消费有不同的预算标准，同是娱乐消费，看电影的消费是200元人民币，买一本武打小说的消费是50元人民币。他们通过实验证明:人们当前在某一类项目的消费支出会减少他们未来在同一类项目的支出，而对其他项目的支出几乎没有什么影响。这是心理账户对每个消费项目会设定一个预算控制。 1996年，Chip和Soll研究认为，心理账户通过心理预算调节人们的消费行为。表现在:人们会为不同的消费设置预算，但预算通常会低估或者高估购买特定商品的价格，因此常使人们产生“穷鬼”和“大富翁”的认知错觉，从而出现消费不足和过度消费的消费误区。他们通过三个实验证明了心理账户的分类预算对消费决策的重要作用。 2006年，EldarShafir和RichardThaler发表Investnow,drinklater,spendnever一文，研究表明:在购买和消费暂时分离的商品交易中，人们会建构多种框架的心理账户。奢侈品的购买更多的被认为是一种“投资”而不是一种消费，因此，当消费很早以前购买的高档产品时，通常被编码为“免费”的或者是储蓄。但如果消费方式不是按原意愿进行时，对该产品的消费预算就会发挥作用。 4.4 行为资产组合理论（BPT）——心理账户在金融投资领域的应用心理账户在金融投资决策领域最广泛的应用是投资组合结构的运用。根据理性投资组合理论，投资者应该只关心他们投资组合的期望收益，而不应该关注某个特定投资部分的收益。可事实相反，投资者倾向于把他们的资金分成安全账户(保障他们的财富水平)和风险账户(试图作风险投机的买卖)。 1997 年 Fisher 和 Statman 提出:人们在投资时会把 资金分别放在不同的投资账户中，即使是基金公司 也建议投资者建立一个资产投资的金字塔，把现金放在金字塔的最低层，把基金放在中间层，把股票放在金字塔的最高层。2000年，Shefrin和Statman提出了行为资产组合理论（Behavioral portfolio theory，BPT-MA）下图就是一个典型的分层金字塔结构，从底端到顶端是按照其风险程度由低到高排列的，从右到左是按其收入价值由低到高 的顺序排列[14]。模型中的每层是根据安全性、潜力 性和期望值这三者相关的投资需求设计的。底层是 为投资者提供安全性而设计的证券，包括货币市场 基金和银行存款保证，上一层是债券，再上一层是 股票和房地产。 在行为金融理论中，行为投资组合理论是建立 在卡尼曼和特维尔斯基的前景理论之上的一个框架 体系。它认为投资者的资产结构应该是金字塔式的 分层结构(这里的层就是心理账户)，投资者对其 资产分层进行管理，每一层对应投资者的一个目标。 底层是投资者为避免贫穷而设立的，所以，其投资 对象通常是短期国债、大额可转让存单、货币市场 基金等有稳定收益、风险小的证券;高层是为使其 富有而设立的，其投资对象通常是外国股票、成长 性股票、彩票等高风险、高收益证券。Shefrin 和 Statman 设计了投资者只有一个心理账户和两个心 理账户的行为资产组合模型，并给出了模型的最优 解。当投资者有两个心理账户时，他们分别在低期 望水平和高期望水平两个心理账户建立投资模型， 并在两个账户之间分配资金。 此外，巴比雷斯和黄明(Barberis and Ming Huang)于 2001 年发表了题为“心理账户、损失规 避与个股回报”的论文，提出了一个较为完整的、 具体的刻画投资者心态的投资模型。并研究了在 两种心理账户下公司股票的均衡回报:一种是投资 者只对所持有的个股价格波动损失规避;另一种是 投资者对所持有的证券组合价格波动损失规避。该 模型在结合心理学、信息学和社会学研究成果的基 础上，对投资者与外部信息之间的互动关系做了崭 新的诠释，对投资者的心态及其决策过程做了具体 的刻画。为人们对投资决策的研究和资产定价的研 究提供了新的思路。]]></content>
      <categories>
        <category>日知录</category>
      </categories>
      <tags>
        <tag>卡尼曼</tag>
        <tag>行为经济学</tag>
        <tag>心理账户</tag>
        <tag>理查德·萨勒</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理系列（4）：Word2Vec]]></title>
    <url>%2F2017%2F12%2F11%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9AWord2Vec%2F</url>
    <content type="text"><![CDATA[待完成······]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>Skip-gram</tag>
        <tag>CBOW</tag>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日知录（1）：前景理论]]></title>
    <url>%2F2017%2F12%2F10%2F%E6%97%A5%E7%9F%A5%E5%BD%95%EF%BC%881%EF%BC%89%EF%BC%9A%E5%89%8D%E6%99%AF%E7%90%86%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[所谓决策，就是在几个方案中选择一个方案，分为风险决策和非风险决策。在非风险决策中，各个方案的结果都是确定的。在风险决策中，有的方案有的结果是不确定的，即可能发生，也可能不发生。我们这里谈及的就是风险决策。 一般来说，风险决策研究有两个途径： 一个是规范性途径，其基本问题是：人类的风险决策应该遵循怎样的规则？ 一个是描述性途径，其基本问题是：人类的风险决策实际遵循怎样的规则？ 规范性途径往往被叫做决策逻辑学，描述性途径往往被叫做决策行为学或决策心理学。我们这里关注的是描述性途径。 关于风险决策的最早理论是期望效用理论，它最初只是规范性理论，但经济学家们逐渐把它当做描述性理论来使用——把它作为经济学中对人的决策行为的基本假定，即认为人的实际决策行为遵循期望效应理论，直到上世纪70年代处有的著名经济学家（如Arrow）仍然采取这种做法。 自1979年以来，Kahneman &amp; Tversky的一系列著作，对期望效用理论作为描述性理论的有效性提出了严峻的挑战，并提出前景理论作为合适的描述理论。由于前景理论能够精确解释、预言许多风险决策行为，它的影响越来越大；后来Kahneman把它应用于经济学领域，对经济学产生了深远的影响，为此，Kahneman于2002年获得了诺贝尔经济学奖。 这里首先介绍期望效用理论；然后介绍Kahneman等以实验事实对期望效用理论作为描述性理论的批评；最后介绍前景理论的主要内容。 一、期望效用理论期望效用函数理论是20世纪50年代，冯·纽曼和摩根斯坦（von Neumann and Morgenstem）在公理化假设的基础上，运用逻辑和数学工具，建立了不确定条件下对理性人（rational actor）选择进行分析的框架。不过，该理论是将个体与群体合而为一的。后来，阿罗和德布鲁（Arrow and Debreu）将其吸收进瓦尔拉斯均衡的框架中，成为处理不确定决策问题的分析范式，进而构建起现代微观经济学并由此展开的包括宏观、金融、计量等在内的宏伟而又优美的理论大厦。 期望效用理论本来是作为规范性理论提出的，但后来在许多经济学著作中被应用为描述性理论，直到上个世纪70年代初仍然如此。 经济学上所使用的期望效用理论包括三方面的内容：Bayes框架；Savage公理；Bernoulli原则。它们分别由不同的人在不同的时期创建。以下分述之。 1.1 Bayes框架早在1662年，Antoine Arnauld就写道：决定一个人必须做什么以获得好处或避免坏处，不仅必须考虑好处和坏处本身，而且必须考虑它发生或不发生的概率。 大约在100年之后出版的Bayes的遗著（1763）年把这个思想系统化、精确化，形成了所谓的风险决策的Bayes框架，其核心思想可以概括为两点： $ED(A_i)=\sum _jP_{ij}·D_{ij}$：即行动$A_i$的估计渴望度（estimated desirability，简称ED；后来改称期望效用，expected utility，EU）等于它的各个可能结果的渴望度$D_{ij}$乘以该可能结果出现的概率所得的积的和；或者说，行动$A_i$的估计渴望度等于它的各个结果的渴望度的加权和，权重为各个结果的概率。（注意，前提是各个可能结果互不相容。） 根据贝叶斯原则进行选择：选择有着最大估计渴望度的一个方案。 请注意到Bayes决策框架所包含的两点假设： 决策权重=概率本身 渴望度（效用）不依赖于参考点。Bayes决策框架并没有要求以一个参考点来衡量渴望度（效用）；实际上，在经济学中人们往往以财富的最终状态来计算效用。 1.2 Savage公理Bayes决策框架一直沿用下来，并有进一步的发展。 Von Neumann &amp; Morgenstern（1944）发展出关于偏好（选择）的公理系统；而Ramsey（1931）和Savage（1954）继续发展了该公理系统，用主观概率代替客观概率，从而使概率理论和决策理论可以适用于更广泛的事情。Savage公理系统是这些公理系统中最为成熟的。其中的公理有： 不变性公理（Invariance Axiom）：方案间的偏好顺序不依赖于方案的描述方式。 优势性公理（Dominance Axiom）：如果方案A在每个方面至少跟方案B一样好，而在至少一个方面比B更好，那么A应该比B更可取。 如果方案B由于方案A，那么它们与任一概率$p≠0$的结合所得的$(B,p)$一定优于$(A,p)$ 这些公理与直觉十分一致；也与Bayes决策框架完全一致。 1.3 Bernoulli原则很早以前Bernoulli就指出人们通常是风险回避的。一个决策是风险回避的，是指：按照结果的表面值（如金额）计算，在确定的结果与有着相等或更高的期望值的不确定的结果之间，决策者选择了确定的结果，如，A. 确定得到80元；B.81%的可能得到100元，按结果的表面值计算，B的期望值高于A，按理应该选择B，但事实上大多数人们会选择A。 为了解释这个现象，Bernoulli提出：人们评价方案，不是用方案的金钱结果值，而是用这些金钱结果值的主观价值，而这个主观价值对于金钱值的函数曲线（效用曲线）是一条凹形的曲线，即$u’’(x)&lt;0$。通俗地说，随着x的不断增大，u的增长越来越慢。用这个原理能轻易解释上段提及的现象。假设80元的主观价值是72，由此可以推出100元的主观价值应该小于90（因为前面80元中每20元的主观价值是18，根据u增长越来越慢的原理，100元超出80元的那20元的主观价值应该小于18，因而100元的主观价值小于90），假设是85.于是，A方案的期望值是72，而B的期望值是$85*81%=68.85$。所以大多数人们选择了A方案。 于是，风险回避和$u’’(x)&lt;0$也成了一些经济学著作对人的决策行为的假定之一。 1.4 小结与问题经济学理论中常常把期望效用理论的上述Bayes框架，Savage公理和Bernoulli原则中的全部或部分作为对人的风险决策行为的基本假定，在此假定和其他假定的基础上构建经济学理论。 这种做法有明显的方法论问题：这些经验假定（经验命题）的真假并没有经过经验方法的系统判明。逻辑学家们提出Bayes框架，Savage公理的初衷是：高斯人们应该怎样决策，或者说怎样决策才是合乎理性的；他们并未考察人们的决策行为是否恰好符合这些规则。而Bernoulli原则知识根据某类风险决策事实归纳得出；并未建立在对各种类型的风险决策事实的全面考察上。人们的实际决策行为是否遵循上述规则，显然是个经验命题；而一个经验命题只有通过经验的方法才能判明它的真伪。而经济学家们，未经经验方法的判定，就把上述规则当做经验命题来使用，这种做法就存在方法论上的问题。 这种方法论上的问题使得采用期望效用理论作为描述性理论可能是错误的。而Kahneman &amp; Tversky的一系列实验表明：采用期望效用理论作为描述性理论确实是错误的。 二、对期望效用理论的实验挑战Kahneman &amp; Tversky的一系列实验对期望效用理论作为描述理论的有效性提出了挑战。下面是他们的部分实验。 2.1 实验一 期望效用理论的“决策权重=概率”成立吗？ 问题一：请选择： A. 有80%的可能性得到4000元； 【20%】 B. 确定地得到3000元。【80%】 方括号指的是该项被试的百分比，以下皆同。 问题二： 请选择： C. 有20%的可能性得到4000元；【65%】 D. 有25%的可能性得到3000元。【35%】 分析： 如前1.1 所述，期望效用理论中，决策权重=概率。假设这点成立，那么有： U(A)=0.80*u(4000);U(B)=u(3000)U(C)=0.2*u(4000)U(D)=0.25*u(3000)注意到$U(A),U(B)$分别乘以0.25就相应得到$U(C),U(D)$，所以有：如果$U(A)]]></content>
      <categories>
        <category>日知录</category>
      </categories>
      <tags>
        <tag>前景理论</tag>
        <tag>卡尼曼</tag>
        <tag>行为经济学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理系列（3）：词向量和语言模型]]></title>
    <url>%2F2017%2F12%2F10%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9A%E8%AF%8D%E5%90%91%E9%87%8F%E5%92%8C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[转载自LICSTAR的博客 这篇博客是我看了半年的论文后，自己对Deep Learnimng在NLP领域中应用的理解和总结，在此分享。其中必然有局限性，欢迎各种交流，随便拍。 Deep Learning算法已经在图像和音频领域取得了惊人的成果，但是在NLP领域中尚未见到如此激动人心的结果。关于这个原因，引一条我比较赞同的微博。 @王威廉：Steve Renals算了以下icassp录取文章题目中包含deep learning 的数量，发现有44偏，而naacl则有0篇。有一种说法是，语言（词、句子、篇章等）属于人类认知过程中产生的高层认知抽象实体，而语言和图像属于较为底层的原始输入信号，所以后两者更适合做deep learning来学习特征。 第一句就先不用管了，毕竟今年的 ACL 已经被灌了好多 Deep Learning 的论文了。第二句我很认同，不过我也有信心以后一定有人能挖掘出语言这种高层次抽象中的本质。不论最后这种方法是不是 Deep Learning，就目前而言，Deep Learning 在 NLP 领域中的研究已经将高深莫测的人类语言撕开了一层神秘的面纱。 我觉得其中最有趣也是最基本的，就是“词向量”了。 将词用“词向量”的方式表示可谓是将 Deep Learning 算法引入 NLP 领域的一个核心技术。大多数宣称用了 Deep Learning 的论文，其中往往也用了词向量。 一、词向量是什么？自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。 NLP中最直观，也是到目前为止最常用的词表示方法是One-Hot Representation，这种方法把没歌词表示为一个很长的向量，这个向量的维度是词表大小，其中绝大多数元素为0，只有一个维度的值为1，这个维度就代表了当前的词。 举个例子： “话筒”表示为[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0····]“麦克”表示为[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0·····]每个词都是茫茫0海中的一个1。 这种One-Hot Representation如果采用稀疏方式存储，会是非常的简介：也就是给每个词分配一个数字ID。比如刚才的例子中，话筒记为3，麦克记为8（假设从0开始记）。如果要编程实现的话，用Hash表给每个词分配一个编号就可以了。这么简洁的表示方式配合上最大熵、SVM、CRF等等算法已经很好地完成了NLP领域的各种主流任务。 当然这种表示方法也存在一个重要的问题就是“词汇鸿沟”现象：任意两个词之间都是孤立的。光从这俩个向量中看不出两个词是否有关系，哪怕是话筒和麦克这样的同义词也不能幸免于难。 Deep Learning中一般用到的词向量并不是刚才提到的用One-Hot Representation表示的那种很长很长的词向量，而是用Distrubuted Representation（不知道这个该怎么翻译，因为还存在一种叫“Distributional Representation”的表示方法，又是另一个不同的概念）表示的一种低维实数向量、这种向量一般长这个样子：[0.792,-0.177,-0.107,0.109,-0.542,…]。维度以50维和100维比较常见。这种向量的表示不是唯一的，后文会提到目前计算这种向量的主流方法。 （个人认为）DIstributed representation最大的贡献是让相关或者相似的词，在距离上更接近了。向量的距离可以用最传统的欧氏距离来衡量，也可以用cos夹角来衡量。用这种方式表示的向量，“麦克”和“话筒”的距离会远远小于“麦克”和“天气”。可能理想情况下“麦克”和“话筒”的表示应该是完全一样的，但是由于有些人会把英文名“迈克”也写成“麦克”，导致“麦克”一词带上了一些人名和语义，因此不会和“话筒”完全一致。 二、词向量的来历Distributed representation最早是Hinton在1986年的论文《Learning distributed representation of concepts》中提出的。虽然这篇文章没有说要将词做Distributed representation，（甚至我很无厘头地猜想那篇文章是为了给他刚提出的BP网络打广告）但至少这种先进的思想在那个时候就在人们的心中埋下了火种，到2000年之后开始逐渐被人重视。 Distributed representation用来表示词，通常被称为“Word Representation”或“Word Embedding”，中文俗称“词向量”。这的只能叫俗称，算不上翻译。半年前我本想翻译的，但是硬是想不出Embedding应该怎么翻译的，后来就这么叫习惯了，如果有好的翻译欢迎提出。（更新：@南大周志华给出了一个合适的翻译：词嵌入）Embedding一次的意义可以参考维基百科的相应页面。后文提到的所有“词向量”都是指用Distributed Representation表示的词向量。 如果用传统的稀疏表示法表示词，在解决某些任务的时候（比如构建语言模型）会造成维数灾难[Bengio 2003]。使用低维的词向量就没这样的问题。同时从实践上看，高维的特征如果要套用Deep Learning，其复杂度几乎是难以接受的，因此低维的词向量在这里也饱受追捧。 同时如上一节提到的，相似词的词向量距离相近，这就让基于词向量设计的一些模型自带平滑功能，让模型看起来非常的漂亮。 三、词向量的训练要介绍词向量是怎么训练的到的，就不得不提到语言模型。到目前为止我了解到的所有训练方法都是在训练语言模型的同时，顺便得到词向量的。 这也比较容易理解，要从一段无标注的自然文本中学习出一些东西，无非就是统计出词频、词的共现、词的搭配之类的信息。而要从自然文本中统计并建立一个语言模型，无疑是要求最为精确的一个任务（也不排除以后有人创造出更好更有用的方法）。既然构建语言模型这一任务要求这么高，其中必然也需要对语言进行更精细的统计和分析，同时也会需要更好的模型，更大的数据来支撑。目前最好的词向量都来自于此，也就不难理解了。 这里介绍的工作均为从大量未标注的普通文本数据中无监督地学习出词向量（语言模型本来就是基于这个想法而来的），可以猜测，如果用上了有标注的语料，训练词向量的方法肯定会更多。不过视目前的语料规模，还是使用未标注的方法靠谱一些。 词向量的训练最经典的有三个工作，C&amp;W 2008、M&amp;H 2008、Mikolov 2010。当然在说这些工作之前，不得不介绍一下这一系列中Bengio的经典之作。 3.1 语言模型简介语言模型其实就是看一句话是不是正常人说出来的。这玩意儿很有用，比如机器翻译、语音识别得到若干候选之后，可以利用语言模型挑一个尽量靠谱的结果。在NLP的其他任务里也都能用到。 语言模型形式化的描述就是给定一个字符串，看它是自然语言的概率$P(w_1,w_2,···,w_t)$。$w_1$到$w_t$依次表示这句话中的各个词。有个很简单的推论是$P(w_1,w_2,…,w_t)=P(w_1)✖️P(w_2|w_1)✖️P(w_3|w_1,w_2)✖️…✖️P(w_t|w_1,w_2,…w_{t-1})$ 常用的语言模型都是在近似地求$P(w_t|w_1,w_2,···,w_{t-1})$，比如$n-gram$模型就是用$P(w_t|w_{t-n+1},…,w_{t-1})$近似表示前者。 顺便提一句，由于后面要接受煎熬的每篇论文使用的符号差异太大，本博文尝试统一使用Bengio 2003的符号系统（略作简化），以便在各方法之间做对比和分析。 3.2 Bengio的经典之作用神经网络训练语言模型的思想最早由百度IDL的徐伟于2000提出。其论文《Can Artificial Neural Networks Learn Language Model？》提出一种用神经网络构建二元语言模型（即$P(w_t|w_{t-1})$）的方法。文中的基本思路和后续的语言模型的差别已经不大了。 训练语言模型的最经典之作，要数Bengio等人在2001年发表在NIPS上的文章《A Neural Probabilistic Language Model》。当然现在要看的话，肯定是要看他在2003年投到JMLR上的同名论文了。 Bengio用了一个三层的神经网络来构建语言模型，同样也是n-gram模型。如图 图中最下方的$w_{t-n+1}，…，w_{t-2},w_{t-1}$就是前$n-1$个词。现在需要根据这已知的$n-1$个词预测下一个词$w_t$。$C(w)$表示词$w$所对应的词向量，整个模型中使用的是一套唯一的词向量，存在矩阵$C$(一个$|V|×m$的矩阵)中。其中$|V|$表示词表的大小（语料中的总词数），m表示词向量的维度。$w$到$C(w)$的转化就是从矩阵中取出一行。 网络的第一层（输入层）是将$C(w_{t-n+1})，…，C(w_{t-2}),C(w_{t-1})$这n-1个向量首尾相拼接起来，形成一个$(n-1)m$维的向量，下面记为x。 网络的第二层（隐藏层）就如同普通的神经网络，直接使用$d+Hx$计算得到。d是一个偏置项。在此之后，使用tanh作为激活函数。 网络的第三层（输出层）一共$|V|$个节点，每个节点$y_i$表示下一个词为$i$的未归一化log概率。最后使用softmax激活函数将输出值y归一化成概率。最终，y的计算公式为： y=b+Wx+Utanh(d+Hx)式子中的$U$(一个$V×h$的矩阵)是隐藏层到输出层的参数，整个模型的多数计算集中在U和隐藏层的矩阵乘法中。后文的提到的3个工作，都有对这一环节的简化，提升计算的速度。 式子中还有一个矩阵$W(|V|×(n-1)m)$,这个矩阵包含了从输入层到输出层的直连边。直连边就是从输入层直接到输出层的一个线性变换，好像也是神经网络中的一种常用技巧。如果不需要直连边的话，将W置为0就可以了。在最后的实验中，Bengio发现直连边虽然不能提升模型效果，但是可以少一半的迭代次数。同时他也猜想如果没有直连边，可能可以生成更好的词向量。 现在万事俱备，用随机梯度下降法把这个模型优化出来就可以了。需要注意的是，一般神经网络的输入层只是一个输入值，而在这里，输入层x也是参数（存在C中），也是需要优化的。优化结束之后，词向量有了，语言模型也有了。 这样得到的语言模型自带平滑，无需传统n-gram模型中那些复杂的平滑算法。Bengio在APNews数据集上做的对比试验也表明他的模型效果比精心设计平滑算法的普通n-gram算法要好10%到20%。 在结束介绍 Bengio 大牛的经典作品之前再插一段八卦。在其 JMLR 论文中的未来工作一段，他提了一个能量函数，把输入向量和输出向量统一考虑，并以最小化能量函数为目标进行优化。后来 M&amp;H 工作就是以此为基础展开的。 他提到一词多义有待解决，9 年之后 Huang 提出了一种解决方案。他还在论文中随口（不是在 Future Work 中写的）提到：可以使用一些方法降低参数个数，比如用循环神经网络。后来 Mikolov 就顺着这个方向发表了一大堆论文，直到博士毕业。 大牛就是大牛。 3.3 C&amp;W的SENNARonan Collobert 和 Jason Weston 在 2008 年的 ICML 上发表的《A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning》里面首次介绍了他们提出的词向量的计算方法。和上一篇牛文类似，如果现在要看的话，应该去看他们在 2011 年投到 JMLR 上的论文《Natural Language Processing (Almost) from Scratch》。文中总结了他们的多项工作，非常有系统性。这篇 JMLR 的论文题目也很霸气啊：从头开始搞 NLP。 他们还把论文所写的系统开源了，叫做 SENNA（主页链接），3500 多行纯 C 代码也是写得非常清晰。我就是靠着这份代码才慢慢看懂这篇论文的。可惜的是，代码只有测试部分，没有训练部分。 实际上 C&amp;W 这篇论文主要目的并不是在于生成一份好的词向量，甚至不想训练语言模型，而是要用这份词向量去完成 NLP 里面的各种任务，比如词性标注、命名实体识别、短语识别、语义角色标注等等。 由于目的的不同，C&amp;W 的词向量训练方法在我看来也是最特别的。他们没有去近似地求 $P(w_t|w_1,w_2,…,w_{t−1})$，而是直接去尝试近似 $P(w_1,w_2,…,w_t)$。在实际操作中，他们并没有去求一个字符串的概率，而是求窗口连续 n 个词的打分 $f(w_{t−n+1},…,w_{t−1},w_t)$。打分 f 越高的说明这句话越是正常的话；打分低的说明这句话不是太合理；如果是随机把几个词堆积在一起，那肯定是负分（差评）。打分只有相对高低之分，并没有概率的特性。 有了这个对 f 的假设，C&amp;W 就直接使用 pair-wise 的方法训练词向量。具体的来说，就是最小化下面的目标函数。 ∑_{x∈𝔛}∑_{w∈𝔇}max \{0,1−f(x)+f(x^{(w)})\}$𝔛$为训练集中的所有连续的n元短语，$𝔇$是整个字典。第一个求和枚举了训练语料中的所有的n元短语，作为正样本。第二个对字典的枚举是构建负样本。$x^{(w)}$是将短语x的最中间的那个词，替换成w。在大多数情况下，在一个正常短语的基础上随便找个词替换掉中间的词，最后得到的短语肯定不是正确的短语，所以这样构造的负样本是非常可用的（多数情况下确实是负样本，极少数情况下把正常短语当做负样本也不影响大局）。同时，由于负样本仅仅是修改了正样本中的一个词，也不会让分类面距离负样本太远而影响分类。再回顾这个式子，x是正样本，$x^{(w)}$是负样本，$f(x)$是对正样本的打分，$f(x^{(w)})$是对负样本的打分。最后希望正样本的打分要比负样本的打分至少高一分。 $f$函数的结构和Bengio 2003中提到的网络结构基本一致。同样是把窗口中的 nn 个词对应的词向量串成一个长的向量，同样是经过一层网络（乘一个矩阵）得到隐藏层。不同之处在于 C&amp;W 的输出层只有一个节点，表示得分，而不像 Bengio 那样的有 $|V|$个节点。这么做可以大大降低计算复杂度，当然有这种简化还是因为 C&amp;W 并不想做一个真正的语言模型，只是借用语言模型的思想辅助他完成 NLP 的其它任务。（其实 C&amp;W 的方法与 Bengio 的方法还有一个区别，他们为了程序的效率用 HardTanh代替 tanh 激活函数。） 他们在实验中取窗口大小 n=11，字典大小 |V|=130000，在维基百科英文语料和路透社语料中一共训练了 7 周，终于得到了这份伟大的词向量。 如前面所说 C&amp;W 训练词向量的动机与其他人不同，因此他公布的词向量与其它词向量相比主要有两个区别： 1.他的词表中只有小写单词。也就是说他把大写开头的单词和小写单词当作同一个词处理。其它的词向量都是把他们当作不同的词处理的。 2.他公布的词向量并不直接是上述公式的优化结果，而是在此基础上进一步跑了词性标注、命名实体识别等等一系列任务的 Multi-Task Learning 之后，二次优化得到的。也可以理解为是半监督学习得到的，而非其他方法中纯无监督学习得到的。 不过好在 Turian 在 2010 年对 C&amp;W 和 M&amp;H 向量做对比时，重新训练了一份词向量放到了网上，那份就没上面的两个“问题”（确切的说应该是差别），也可以用的更放心。后面会详细介绍 Turian 的工作。 关于这篇论文其实还是有些东西可以吐槽的，不过训练词向量这一块没有，是论文其他部分的。把吐槽机会留给下一篇博文了。 3.4 M&amp;H 的 HLBLAndriy Mnih 和 Geoffrey Hinton 在 2007 年和 2008 年各发表了一篇关于训练语言模型和词向量的文章。2007 年发表在 ICML 上的《Three new graphical models for statistical language modelling》表明了 Hinton 将 Deep Learning 战场扩展到 NLP 领域的决心。2008 年发表在 NIPS 上的《A scalable hierarchical distributed language model》则提出了一种层级的思想替换了 Bengio 2003 方法中最后隐藏层到输出层最花时间的矩阵乘法，在保证效果的基础上，同时也提升了速度。下面简单介绍一下这两篇文章。 Hinton 在 2006 年提出 Deep Learning 的概念之后，很快就来 NLP 最基础的任务上试了一把。果然，有效。M&amp;H 在 ICML 2007 上发表的这篇文章提出了“Log-Bilinear”语言模型。文章标题中可以看出他们其实一共提了 3 个模型。从最基本的 RBM 出发，一点点修改能量函数，最后得到了“Log-Bilinear”模型。 模型如果用神经网络的形式写出来，是这个样子： h = ∑_{i=1}^{t-1}H_iC(w_i)y_j=C(w_j)^Th这里的两个式子可以合写成一个$y_j=∑_{i=1}^{n-1}C(w_j)^TH_iC(w_i)$。$C(w)$是词 w 对应的词向量，形如 $x^TMy$ 的模型叫做 Bilinear 模型，也就是 M&amp;H 方法名字的来历了。 为了更好地理解模型的含义，还是来看这两个拆解的式子。h 在这里表示隐藏层，这里的隐藏层比前面的所有模型都更厉害，直接有语义信息。首先从第二个式子中隐藏层能和词向量直接做内积可以看出，隐藏层的维度和词向量的维度是一致的（都是 m维）。$H_i$就是一个 $m×m$的矩阵，该矩阵可以理解为第 i个词经过 $H_i$ 这种变换之后，对第 t个词产生的贡献。因此这里的隐藏层是对前 $t−1$个词的总结，也就是说隐藏层 $h$ 是对下一个词的一种预测。 再看看第二个式子，预测下一个词为 $w_j$ 的 log 概率是 $y_j$，它直接就是 $C(w_j)$和 $h$ 的内积。内积基本上就可以反应相似度，如果各词向量的模基本一致的话，内积的大小能直接反应两个向量的 cos 夹角的大小。这里使用预测词向量 h 和各个已知词的词向量的相似度作为 log 概率，将词向量的作用发挥到了极致。这也是我觉得这次介绍的模型中最漂亮的一个。 这种“Log-Bilinear”模型看起来每个词需要使用上文所有的词作为输入，于是语料中最长的句子有多长，就会有多少个 H 矩阵。这显然是过于理想化了。最后在实现模型时，还是迫于现实的压力，用了类似 n-gram 的近似，只考虑了上文的 3 到 5 个词作为输入来预测下一个词。 M&amp;H 的思路如前面提到，是 Bengio 2003 提出的。经过大牛的实现，效果确实不错。虽然复杂度没有数量级上的降低，但是由于是纯线性模型，没有激活函数（当然在做语言模型的时候，最后还是对 $y_j$ 跑了一个 softmax），因此实际的训练和预测速度都会有很大的提升。同时隐藏层到输出层的变量直接用了词向量，这也就几乎少了一半的变量，使得模型更为简洁。最后论文中 M&amp;H 用了和 Bengio 2003 完全一样的数据集做实验，效果有了一定的提升。 2008 年 NIPS 的这篇论文，介绍的是“hierarchical log-bilinear”模型，很多论文中都把它称作简称“HLBL”。和前作相比，该方法使用了一个层级的结构做最后的预测。可以简单地设想一下把网络的最后一层变成一颗平衡二叉树，二叉树的每个非叶节点用于给预测向量分类，最后到叶节点就可以确定下一个词是哪个了。这在复杂度上有显著的提升，以前是对 $|V|$ 个词一一做比较，最后找出最相似的，现在只需要做 $log_2(|V|)$ 次判断即可。 这种层级的思想最初可见于 Frederic Morin 和 Yoshua Bengio 于 2005 年发表的论文《Hierarchical probabilistic neural network language model》中。但是这篇论文使用 WordNet 中的 IS-A 关系，转化为二叉树用于分类预测。实验结果发现速度提升了，效果变差了。 有了前车之鉴，M&amp;H 就希望能从语料中自动学习出一棵树，并能达到比人工构建更好的效果。M&amp;H 使用一种 bootstrapping 的方法来构建这棵树。从随机的树开始，根据分类结果不断调整和迭代。最后得到的是一棵平衡二叉树，并且同一个词的预测可能处于多个不同的叶节点。这种用多个叶节点表示一个词的方法，可以提升下一个词是多义词时候的效果。M&amp;H 做的还不够彻底，后面 Huang 的工作直接对每个词学习出多个词向量，能更好地处理多义词。 3.5 Mikolov 的 RNNLM前文说到，Bengio 2003 论文里提了一句，可以使用一些方法降低参数个数，比如用循环神经网络。Mikolov 就抓住了这个坑，从此与循环神经网络结下了不解之缘。他最早用循环神经网络做语言模型是在 INTERSPEECH 2010 上发表的《Recurrent neural network based language model》里。Recurrent neural network 是循环神经网络，简称 RNN，还有个 Recursive neural networks 是递归神经网络（Richard Socher 借此发了一大堆论文），也简称 RNN。看到的时候需要注意区分一下。不过到目前为止，RNNLM 只表示循环神经网络做的语言模型，还没有歧义。 在之后的几年中，Mikolov 在一直在RNNLM 上做各种改进，有速度上的，也有准确率上的。现在想了解 RNNLM，看他的博士论文《Statistical Language Models based on Neural Networks》肯定是最好的选择。 循环神经网络与前面各方法中用到的前馈网络在结构上有比较大的差别，但是原理还是一样的。网络结构大致如图。 左边是网络的抽象结构，由于循环神经网络多用在时序序列上，因此里面的输入层、隐藏层和输出层都带上了“(t)”。$w(t)$ 是句子中第 t 个词的 One-hot representation 的向量，也就是说 $w$ 是一个非常长的向量，里面只有一个元素是 1。而下面的 $s(t−1)$ 向量就是上一个隐藏层。最后隐藏层计算公式为： s(t)=sigmoid(Uw(t)+W_s(t−1))从右图可以看出循环神经网络是如何展开的。每来一个新词，就和上一个隐藏层联合计算出下一个隐藏层，隐藏层反复利用，一直保留着最新的状态。各隐藏层通过一层传统的前馈网络得到输出值。 $w(t)$ 是一个词的 One-hot representation，那么 $Uw(t)$ 也就相当于从矩阵 $U $中选出了一列，这一列就是该词对应的词向量。 循环神经网络的最大优势在于，可以真正充分地利用所有上文信息来预测下一个词，而不像前面的其它工作那样，只能开一个 n 个词的窗口，只用前 n 个词来预测下一个词。从形式上看，这是一个非常“终极”的模型，毕竟语言模型里能用到的信息，他全用上了。可惜的是，循环神经网络形式上非常好看，使用起来却非常难优化，如果优化的不好，长距离的信息就会丢失，甚至还无法达到开窗口看前若干个词的效果。Mikolov 在 RNNLM 里面只使用了最朴素的 BPTT 优化算法，就已经比 n-gram 中的 state of the art 方法有更好的效果，这非常令人欣慰。如果用上了更强的优化算法，最后效果肯定还能提升很多。 对于最后隐藏层到输出层的巨大计算量，Mikolov 使用了一种分组的方法：根据词频将$|V|$个词分成 $\sqrt{|V|}$组，先通过$\sqrt{|V|}$次判断，看下一个词属于哪个组，再通过若干次判断，找出其属于组内的哪个元素。最后均摊复杂度约为 $o(\sqrt{|V|})$，略差于 M&amp;H 的 $o(log(|V|))$，但是其浅层结构某种程度上可以减少误差传递，也不失为一种良策。 Mikolov 的 RNNLM 也是开源的（网址）。非常算法风格的代码，几乎所有功能都在一个文件里，工程也很好编译。比较好的是，RNNLM 可以完美支持中文，如果语料存成 UTF-8 格式，就可以直接用了。 最后吐槽一句，我觉得他在隐藏层用 sigmoid 作为激活函数不够漂亮。因为隐藏层要和输入词联合计算得到下一个隐藏层，如果当前隐藏层的值全是正的，那么输入词对应的参数就会略微偏负，也就是说最后得到的词向量的均值不在 0 附近。总感觉不好看。当然，从实验效果看，是我太强迫症了。 3.6 Huang的语义强化与前几位大牛的工作不同，Eric H. Huang 的工作是在 C&amp;W 的基础上改进而成的，并非自成一派从头做起。他这篇发表在 ACL 2012 上的《Improving Word Representations via Global Context and Multiple Word Prototypes》试图通过对模型的改进，使得词向量富含更丰富的语义信息。他在文中提出了两个主要创新来完成这一目标：（其实从论文标题就能看出来）第一个创新是使用全文信息辅助已有的局部信息，第二个创新是使用多个词向量来表示多义词。下面逐一介绍。 Huang 认为 C&amp;W 的工作只利用了“局部上下文（Local Context）”。C&amp;W 在训练词向量的时候，只使用了上下文各 5 个词，算上自己总共有 11 个词的信息，这些局部的信息还不能充分挖掘出中间词的语义信息。Huang 直接使用 C&amp;W 的网络结构计算出一个得分，作为“局部得分”。 然后 Huang 提出了一个“全局信息”，这有点类似传统的词袋子模型。词袋子模型是把文章中所有词的 One-hot Representation 加起来，形成一个向量（就像把词全都扔进一个袋子里），用来表示文章。Huang 的全局模型是将文章中所有词的词向量求个加权平均（权重是词的 idf），作为文章的语义。他把文章的语义向量和当前词的词向量拼接起来，形成一个两倍长度的向量作为输入，之后还是用 C&amp;W 的网络结构算出一个打分。 有了 C&amp;W 方法的得到的“局部得分”，再加上在 C&amp;W 方法基础上改造得到的“全局得分”，Huang 直接把两个得分相加，作为最终得分。最终得分使用 C&amp;W 提出的 pair-wise 目标函数来优化。 加了这个全局信息有什么用处呢？Huang 在实验中发现，他的模型能更好地捕捉词的语义信息。比如 C&amp;W 的模型中，与 markets 最相近的词为 firms、industries；而 Huang 的模型得到的结果是 market、firms。很明显，C&amp;W 的方法由于只考虑了临近词的信息，最后的结果是词法特征最相近的词排在了前面（都是复数形式）。不过我觉得这个可能是英语才有的现象，中文没有词形变化，如果在中文中做同样的实验还不知道会有什么效果。 Huang 论文的第二个贡献是将多义词用多个词向量来表示。Bengio 2003 在最后提过这是一个重要的问题，不过当时他还在想办法解决，现在 Huang 给出了一种思路。 将每个词的上下文各 5 个词拿出来，对这 10 个词的词向量做加权平均（同样使用 idf 作为权重）。对所有得到的上下文向量做 k-means 聚类，根据聚类结果给每个词打上标签（不同类中的同一个词，当作不同的词处理），最后重新训练词向量。 当然这个实验的效果也是很不错的，最后 star 的某一个表示最接近的词是 movie、film；另一个表示最接近的词是 galaxy、planet。 这篇文章还做了一些对比实验，在下一章评价里细讲。 3.7 总结讲完了大牛们的各种方法，自己也忍不住来总结一把。当然，为了方便对比，我先列举一下上面提到的各个系统的现有资源，见下表。对应的论文不在表中列出，可参见最后的参考文献。 Turian 的工作前面只是提了一下，他在做 C&amp;W 向量与 H&amp;M 向量的对比实验时，自己按照论文重新实现了一遍他们的方法，并公布了词向量。后来 C&amp;W 在主页上强调了一下：尽管很多论文把 Turian 实现的结果叫做 C&amp;W 向量，但是与我发布的词向量是不同的，我这个在更大的语料上训练，还花了两个月时间呢！ Turian 公布的 H&amp;M 向量是直接请 Andriy Mnih 在 Turian 做好的语料上运行了一下 HLBL，所以没有代码公布。同时 Turian 自己实现了一份 LBL模型，但是没有公布训练出来的词向量。（这是根据他主页上描述推测的结果，从 Turian 的论文中看，他应该是实现了 HLBL 算法并且算出词向量的。） RCV1 的词数两篇文章中所写的数据差距较大，还不知道是什么原因。 Holger Schwenk 在词向量和语言模型方面也做了一些工作，看起来大体相似，也没仔细读过他的论文。有兴趣的读者可以直接搜他的论文。 事实上，除了 RNNLM 以外，上面其它所有模型在第一层（输入层到隐藏层）都是等价的，都可以看成一个单层网络。可能形式最为特别的是 M&amp;H 的模型，对前面的每个词单独乘以矩阵 HiHi，而不是像其它方法那样把词向量串接起来乘以矩阵 HH。但如果把 HH 看成 HiHi 的拼接： $[H1H2…Ht]$，则会有以下等式： 这么看来还是等价的。 所以前面的这么多模型，本质是非常相似的。都是从前若干个词的词向量通过线性变换抽象出一个新的语义（隐藏层），再通过不同的方法来解析这个隐藏层。模型的差别主要就在隐藏层到输出层的语义。Bengio 2003 使用了最朴素的线性变换，直接从隐藏层映射到每个词；C&amp;W 简化了模型（不求语言模型），通过线性变换将隐藏层转换成一个打分；M&amp;H 复用了词向量，进一步强化了语义，并用层级结构加速；Mikolov 则用了分组来加速。 每种方法真正的差别看起来并不大，当然里面的这些创新，也都是有据可循的。下一章就直接来看看不同模型的效果如何。 四、词向量的评价词向量的评价大体上可以分成两种方式，第一种是把词向量融入现有系统中，看对系统性能的提升；第二种是直接从语言学的角度对词向量进行分析，如相似度、语义偏移等。 4.1 提升现有系统词向量的用法最常见的有两种： 1）直接用于神经网络模型的输入层。如 C&amp;W 的 SENNA 系统中，将训练好的词向量作为输入，用前馈网络和卷积网络完成了词性标注、语义角色标注等一系列任务。再如 Socher 将词向量作为输入，用递归神经网络完成了句法分析、情感分析等多项任务。 2）作为辅助特征扩充现有模型。如 Turian 将词向量作为额外的特征加入到接近 state of the art 的方法中，进一步提高了命名实体识别和短语识别的效果。 具体的用法理论上会在下一篇博文中细讲。 C&amp;W 的论文中有一些对比实验。实验的结果表明，使用词向量作为初始值替代随机初始值，其效果会有非常显著的提升（如：词性标注准确率从 96.37% 提升到 97.20%；命名实体识别 F 值从 81.47% 提升到 88.67%）。同时使用更大的语料来训练，效果也会有一些提升。 Turian 发表在 ACL 2010 上的实验对比了 C&amp;W 向量与 M&amp;H 向量用作辅助特征时的效果。在短语识别和命名实体识别两个任务中，C&amp;W 向量的效果都有略微的优势。同时他也发现，如果将这两种向量融合起来，会有更好的效果。除了这两种词向量，Turian 还使用 Brown Cluster 作为辅助特征做了对比，效果最好的其实是 Brown Cluster，不过这个已经超出本文的范围了。 4.2 语言学评价Huang 2012 的论文提出了一些创新，能提升词向量中的语义成分。他也做了一些实验对比了各种词向量的语义特性。实验方法大致就是将词向量的相似度与人工标注的相似度做比较。最后 Huang 的方法语义相似度最好，其次是 C&amp;W 向量，再然后是 Turian 训练的 HLBL 向量与 C&amp;W 向量。这里因为 Turian 训练词向量时使用的数据集（RCV1）与其他的对比实验（Wiki）并不相同，因此并不是非常有可比性。但从这里可以推测一下，可能更大更丰富的语料对于语义的挖掘是有帮助的。 还有一个有意思的分析是 Mikolov 在 2013 年刚刚发表的一项发现。他发现两个词向量之间的关系，可以直接从这两个向量的差里体现出来。向量的差就是数学上的定义，直接逐位相减。比如 $C(king)−C(queen)≈C(man)−C(woman)$。更强大的是，与 $C(king)−C(man)+C(woman)$最接近的向量就是 $C(queen)$。 为了分析词向量的这个特点， Mikolov 使用类比（analogy）的方式来评测。如已知 a 之于 b 犹如 c 之于 d。现在给出 a、b、c，看 $C(a)−C(b)+C(c)$最接近的词是否是 d。 在文章 Mikolov 对比了词法关系（名词单复数 good-better:rough-rougher、动词第三人称单数、形容词比较级最高级等）和语义关系（clothing-shirt:dish-bowl）。 在词法关系上，RNN 的效果最好，然后是 Turian 实现的 HLBL，最后是 Turian 的 C&amp;W。（RNN-80:19%；RNN-1600:39.6%；HLBL-100:18.7%；C&amp;W-100:5%；-100表示词向量为100维） 在语义关系上，表现最好的还是 RNN，然后是 Turian 的两个向量，差距没刚才的大。（RNN-80:0.211；C&amp;W-100:0.154；HLBL-100:0.146） 但是这个对比实验用的训练语料是不同的，也不能特别说明优劣。 这些实验结果中最容易理解的是：语料越大，词向量就越好。其它的实验由于缺乏严格控制条件进行对比，谈不上哪个更好哪个更差。不过这里的两个语言学分析都非常有意思，尤其是向量之间存在这种线性平移的关系，可能会是词向量发展的一个突破口。 参考文献Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of Machine Learning Research (JMLR), 3:1137–1155, 2003. [PDF] Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research (JMLR), 12:2493-2537, 2011. [PDF] Andriy Mnih &amp; Geoffrey Hinton. Three new graphical models for statistical language modelling. International Conference on Machine Learning (ICML). 2007. [PDF]Andriy Mnih &amp; Geoffrey Hinton. A scalable hierarchical distributed language model. The Conference on Neural Information Processing Systems (NIPS) (pp. 1081–1088). 2008. [PDF] Mikolov Tomáš. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of Technology. 2012. [PDF] Turian Joseph, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for semi-supervised learning. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL). 2010. [PDF] Eric Huang, Richard Socher, Christopher Manning and Andrew Ng. Improving word representations via global context and multiple word prototypes. Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. 2012. [PDF] Mikolov, Tomas, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. Proceedings of NAACL-HLT. 2013. [PDF]]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>Skip-gram</tag>
        <tag>CBOW</tag>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理系列（2）：中文Wiki语料库词向量的训练]]></title>
    <url>%2F2017%2F12%2F09%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9A%E4%B8%AD%E6%96%87Wiki%E8%AF%AD%E6%96%99%E5%BA%93%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E8%AE%AD%E7%BB%83%2F</url>
    <content type="text"><![CDATA[要通过计算机进行自然语言处理，首先就需要将这些文本数字化，目前用的最广泛的方法是词向量，根据训练使用算法的不同，目前主要有Word2Vec和GloVe两大方法，本文主要讲述通过这两个方法分别训练中文维基百科语料库的词向量。 一、获取并处理中文维基百科语料库1.1 下载中文维基百科语料库的下载链接为：https://dumps.wikimedia.org/zhwiki/，本试验下载的是最新的zhwiki-latest-pages-articles.xml.bz2。这个压缩包里面存的是标题、正文部分，该目录下还包括了其他类型的语料库，如仅包含标题，摘要等。 1.2 抽取内容Wikipedia Extractor是一个开源的用于抽取维基百科语料库的工具，由python携程，通过这个工具可以很容易地从语料库中抽取相关内容。使用方法如下： 12$ git clone https://github.com/attardi/wikiextractor.git wikiextractor$ wikiextractor/WikiExtractor.py -b 2000M -o zhwiki_extracted zhwiki-latest-pages-articles.xml.bz2 由于这个工具就是一个python脚本，因此无需安装，-b参数指对提取出来的内容进行切片后每个文件的大小，如果要将所有内容保存在同一个文件，那么就需要把这个参数设置地大一点，-o的参数指提取出来的文件放置的目录，抽取出来的文件的路径为zhwiki_extract/AA/wiki_00。更多的参数可参考其github主页的说明。 抽取后的内容格式为每篇文章被一对&lt;doc&gt;&lt;/doc&gt;包起来，而&lt;doc&gt;中的包含了属性有文章的id、url和title属性，如&lt;doc id=&quot;13&quot; url=&quot;https://zh.wikipedia.org/wiki?curid=13&quot; title=&quot;数学&quot;&gt; 1.3 繁简转换由上一步提取出来的中文维基百科中的语料中既有繁体字也有简体字，这里需要将其统一变为简体字，采用的工具也是开源的OpenCC转换器。安装使用方法如下： 1234$ ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; &lt; /dev/null 2&gt; /dev/null$ brew install opencc$ cd OpenCC$ opencc -i /Users/HuaZhang/Desktop/zhwiki_extracted/AA/zhwiki_extract/AA/wiki_00 -o zhwiki_extract/zhs_wiki -c /Users/HuaZhang/OpenCC/data/config/t2s.json 其中-i表示输入文件路径，-o表示输出的文件，-c表示转换的配置文件，这里使用的繁体转简体的配置文件，OpenCC自带了一系列的转换配置文件，可以参考其github主页的说明。 1.4 去除标点去除标点符号有两个问题需要解决，一个是像下面这种为了解决各地术语，名称不同的问题： 1他的主要成就包括Emacs及後來的GNU Emacs，GNU C 編譯器及-&#123;zh-hant:GNU 除錯器;zh-hans:GDB 调试器&#125;-。 另外一个就是将所有标点符号替换成空字符，通过正则表达式均可解决这两个问题，下面是具体实现的python代码： 1234567891011121314151617181920212223242526#!/usr/bin/python# -*- coding: utf-8 -*- import sysimport reimport ioreload(sys)sys.setdefaultencoding('utf-8')def pre_process(input_file, output_file): multi_version = re.compile(ur'-\&#123;.*?(zh-hans|zh-cn):([^;]*?)(;.*?)?\&#125;-') punctuation = re.compile(u"[-~!@#$%^&amp;*()_+`=\[\]\\\&#123;\&#125;\"|;':,./&lt;&gt;?·！@#￥%……&amp;*（）——+【】、；‘：“”，。、《》？「『」』]") with io.open(output_file, mode = 'w', encoding = 'utf-8') as outfile: with io.open(input_file, mode = 'r', encoding ='utf-8') as infile: for line in infile: line = multi_version.sub(ur'\2', line) line = punctuation.sub('', line.decode('utf8')) outfile.write(line)if __name__ == '__main__': if len(sys.argv) != 3: print "Usage: python script.py input_file output_file" sys.exit() input_file, output_file = sys.argv[1], sys.argv[2] pre_process(input_file, output_file) 经过该步骤处理之后，得到了简体中文的纯净文本，如下所示： &gt;doc id13 urlhttpszhwikipediaorgwikicurid13 title数学数学数学是利用符号语言研究数量结构变化以及空间等概念的一门学科从某种角度看属于形式科学的一种数学透过抽象化和逻辑推理的使用由计数计算量度和对物体形状及运动的观察而产生数学家们拓展这些概念为了公式化新的猜想以及从选定的公理及定义中建立起严谨推导出的定理·······数学奖通常和其他科学的奖项分开数学上最有名的奖为菲尔兹奖创立于1936年每四年颁奖一次它通常被认为是数学的诺贝尔奖另一个国际上主要的奖项为阿贝尔奖创立于2003年两者都颁奖于特定的工作主题包括数学新领域的创新或已成熟领域中未解决问题的解答著名的23个问题称为希尔伯特的23个问题于1900年由德国数学家大卫希尔伯特所提出这一连串的问题在数学家之间有著极高的名望且至少有九个问题已经被解答了出来另一新的七个重要问题称为千禧年大奖难题发表于2000年对其每一个问题的解答都有著一百万美元的奖金而当中只有一个问题黎曼猜想和希尔伯特的问题重复doc 1.5 jieba分词下面需要对其进行分词并且整理成每行一篇文本的格式，从而方便后续的处理。 分词采用 python 的分词工具 jieba，通过 pip install jieba安装即可。且将一篇文章分词后的结果存储在一行，由前面可知，每篇文章存储在一对&lt;doc&gt;&lt;/doc&gt;标签中，由于前面去掉了标点，所以现在变成了doc doc,所以只要判断当前行为doc时即可认为文章结束，从而开始在新的一行记录下一篇文章的分词结果。实现的python代码如下: 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/python# -*- coding: utf-8 -*-import sysimport ioimport jiebareload(sys)sys.setdefaultencoding('utf-8')def cut_words(input_file, output_file): count = 0 with io.open(output_file, mode = 'w', encoding = 'utf-8') as outfile: with io.open(input_file, mode = 'r', encoding = 'utf-8') as infile: for line in infile: line = line.strip() if len(line) &lt; 1: # empty line continue if line.startswith('doc'): # start or end of a passage if line == 'doc': # end of a passage outfile.write(u'\n') count = count + 1 if(count % 1000 == 0): print('%s articles were finished.......' %count) continue for word in jieba.cut(line): outfile.write(word + ' ') print('%s articles were finished.......' %count)if __name__ == '__main__': if len(sys.argv) &lt; 3: print "Usage: python script.py input_file output_file" sys.exit() input_file, output_file = sys.argv[1], sys.argv[2] cut_words(input_file, output_file) 二、通过Word2Vec训练词向量Word2vec中包含了两种训练词向量的方法：Continuous Bag of Words(CBOW)和Skip-gram。CBOW的目标是根据上下文来预测当前词语的概率。Skip-gram刚好相反，根据当前词语来预测上下文的概率。这两种方法都利用人工神经网络作为它们的分类算法。起初，每个单词都是一个随机N维向量。训练时，该算法利用CBOW或者Skip-gram的方法获得了每个单词的最优向量。 最初 Google 开源的 Word2Vec 是用C来写的，后面陆续有了Python ，Java 等语言的版本，这里采用的是 Python 版本的 gensim。通过 gensim 提供的 API 可以比较容易地进行词向量的训练。gensim的建议通过conda install gensim安装 下面是对上面处理后的语料库进行训练的一个简单例子。 123456789101112131415161718192021222324#!/usr/bin/python# -*- coding: utf-8 -*-import os, sysimport multiprocessingimport gensim reload(sys)sys.setdefaultencoding(&apos;utf-8&apos;)def word2vec_train(input_file, output_file): sentences = gensim.models.word2vec.LineSentence(input_file) model = gensim.models.Word2Vec(sentences, size=300, min_count=10, sg=0, workers=multiprocessing.cpu_count()) model.save(output_file) model.wv.save_word2vec_format(output_file + &apos;.vector&apos;, binary=True)if __name__ == &apos;__main__&apos;: if len(sys.argv) &lt; 3: print &quot;Usage: python script.py infile outfile&quot; sys.exit() input_file, output_file = sys.argv[1], sys.argv[2] word2vec_train(input_file, output_file) 上面的训练过程首先将输入的文件转为 gensim内部的 LineSentence对象，要求输入的文件的格式为每行一篇文章，每篇文章的词语以空格隔开。 然后通过gensim.models.Word2Vec初始化一个Word2Vec模型，size参数表示训练的向量的维数；min_count表示忽略那些出现次数小于这个数值的词语，认为他们是没有意义的词语，一般的取值范围为（0，100）；sg表示采用何种算法进行训练，取0时表示采用CBOW模型，取1表示采用skip-gram模型；workers表示开多少个进程进行训练，采用多进程训练可以加快训练过程，这里开的进程数与CPU的核数相等。 假设我们训练好了一个语料库的词向量，当一些新的文章加入这个语料库时，如何训练这些新增的文章从而更新我们的语料库？将全部文章再进行一次训练显然是费时费力的，gensim提供了一种类似于“增量训练”的方法。即可在原来的model基础上仅对新增的文章进行训练。如下所示为一个简单的例子： 12model = gensim.models.Word2Vec.load(exist_model)model.train(new_sentences) 上面的代码先加载了一个已经训练好的词向量模型，然后再添加新的文章进行训练，同样新增的文章的格式也要满足每行一篇文章，每篇文章的词语通过空格分开的格式。这里需要注意的是加载的模型只能 是通过model.save()存储的模型，从model.save_word2vec_format()恢复过来的模型只能用于查询. 三、使用词向量模型训练好的词向量可以供后续的多项自然语言处理工作使用，下面是通过gensim加载训练好的词向量模型并进行查询的例子： 123456789101112131415161718192021222324252627282930# 加载模型import gensimmodel = gensim.models.KeyedVectors.load_word2vec_format('/Users/HuaZhang/Desktop/zhwiki_extracted/output_word2vec.vector',binary = True)# 词向量维度len(model[u'黑格尔'])# 相似度model.similarity(u'叔本华',u'康德')0.62428547493158093# 找出相似度最高的词words = model.most_similar(u"哈耶克")for word in words: print word[0],word[1] 米塞斯 0.776977062225叔本华 0.731572449207黑格尔 0.723797023296巴维克 0.723039865494门格尔 0.719911754131谢林 0.714867889881波普尔 0.714080870152马克思 0.711268663406尼采 0.710071563721博姆 0.705146193504# 找出最不相关的词汇print model.doesnt_match(u"莎士比亚 卡夫卡 卢梭 爱因斯坦".split())爱因斯坦]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>词向量</tag>
        <tag>Skip-gram</tag>
        <tag>CBOW</tag>
        <tag>Wiki</tag>
        <tag>jieba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理系列（1）：词向量与统计语言模型]]></title>
    <url>%2F2017%2F12%2F08%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9A%E8%AF%8D%E5%90%91%E9%87%8F%E4%B8%8E%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Word2Vec是Google在2013年年中开源的一款将词表征为实数值向量的高效工具，采用的模型有CBOW（Continuous Bag-Of-Words，即连续的词袋模型）和Skip-Gram两种。 word2vec一般被外界认为是一个Deep Learning（深度学习）的模型，究其原因，可能和word2vec的作者Tomas Mikolov的Deep Learning背景以及word2vec是一种神经网络模型相关，但我们谨慎认为该模型层次较浅，严格来说还不能算是深层模型。当然如果word2vec上层再套一层与具体应用相关的输出层，比如Softmax，此时更像是一个深层模型。 word2vec通过训练，可以把对文本内容的处理简化为K维向量空间中的向量运算，而向量空间上的相似度可以用来表示文本语义上的相似度。因此，word2vec输出的词向量可以被用来做很多NLP相关的工作，比如聚类、找同义词、词性分析等等。而word2vec被人广为传颂的地方是其向量的加法组合运算（Additive Compositionality），官网上的例子是：$vector(‘Paris’)-vector(‘France’)+vector(‘Italy’)≈vector(‘Rome’)$，$vector(‘king’)-vector(‘man’)+vector(‘women’)≈vector(‘queen’)$。但我们认为这个多少有点被过度炒作了，很多其他降维或主题模型在一定程度也能达到类似效果，而且word2vec也只是少量的例子完美符合这种加减法操作，并不是所有的case都满足。如果换个思路，把词当做feature，那么 word2vec 就可以把 feature映射到 K维向量空间，应该可以为现有模型提供更多的有用信息。 word2vec大受欢迎的另一个原因是其高效性，Mikolov在论文中指出一个优化的单机版本一天可训练上千亿个词。 一、词向量自然语言处理相关任务中，要将自然语言处理交给机器学习中的算法来处理，通常需要首先将语言数学化，因为机器不是人，机器只认得数学符合。向量是人把自然界的东西抽象出来交给机器处理的东西，基本上可以说向量是人对机器输入的主要方式了。 词向量就是用来将语言中的词进行数学化的一种方式，顾名思义，词向量就是把一个词表示成一个向量。主要有两种表示方式： 1.1 One-Hot Representation一种最简单的词向量方式是One-Hot Representation，就是用一个很长的向量来表示一个词，向量的长度为词典的大小，向量的分量只有一个1，其他都为0，1的位置对应该词在词典中的位置。举个例子：“话筒”表示为[0001000000000000]“麦克风”表示[0000000010000000]。每一个词都是茫茫0海中的一个1. 这种One-Hot Representation如果采用稀疏方式存储，会非常地简洁，也就是给每个词分配一个数字ID。比如刚才的例子中，话筒为3，麦克风即为8（假设从0开始记）。如果要编程的话，用Hash表给每个词分配一个编号就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF等等算法已经很好地完成了NLP领域的各种主流任务了。 但这种表示方法有两个缺点：（1）容易受维数灾难的困扰，尤其是将其用于Deep Learning的一些算法时不能很好地刻画词与词之间的相似性：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系，哪怕是话筒和麦克风这样的同义词也不能幸免于难。 1.2 Distributed Representation另一种就是DistributedRepresentation 这种表示，它最早是 Hinton 于 1986 年提出的，可以克服 one-hot representation 的缺点。其基本想法是直接用一个普通的向量表示一个词，这种向量一般长成这个样子：[0.792, −0.177, −0.107, 0.109, −0.542, …]，也就是普通的向量表示形式。维度以 50 维和 100 维比较常见。 当然一个词怎么表示成这么样的一个向量是要经过一番训练的，训练方法较多，word2vec是其中一种，在后面会提到，这里先说它的意义。还要注意的是每个词在不同的语料库和不同的训练方法下，得到的词向量可能是不一样的。 词向量一般维数不高，很少有人闲着没事训练的时候定义一个10000维以上的维数，所以用起来维数灾难的机会现对于one-hot representation表示就大大减少了。 由于是用向量表示，而且用较好的训练算法得到的词向量的向量一般是有空间上的意义的，也就是说，将所有这些向量放在一起形成一个词向量空间，而每一向量则为该空间中的一个点，在这个空间上的词向量之间的距离度量也可以表示对应的两个词之间的“距离”。所谓两个词之间的“距离”，就是这两个词之间的语法，语义之间的相似性。 一个比较爽的应用方法是，得到词向量后，假如对于某个词A，想找出这个词最相似的词，这个场景对人来说都不轻松，毕竟比较主观，但是对于建立好词向量后的情况，对计算机来说，只要拿这个词的词向量跟其他词的词向量一一计算欧式距离或者cos距离，得到距离最小的那个词，就是它最相似的。 Distributed representation 最大的贡献就是让相关或者相似的词，在距离上更接近了。向量的距离可以用最传统的欧氏距离来衡量，也可以用 cos 夹角来衡量。用这种方式表示的向量，“麦克”和“话筒”的距离会远远小于“麦克”和“天气”。可能理想情况下“麦克”和“话筒”的表示应该是完全一样的，但是由于有些人会把英文名“迈克”也写成“麦克”，导致“麦克”一词带上了一些人名的语义，因此不会和“话筒”完全一致。 这样的特性使得词向量很有意义，自然就会吸引比较多的人去研究，前有Bengio发表在JMLR上的论文《A Neural Probabilistic Language Model》，又有Hinton的层次化Log-Bilinear模型，还有google的TomasMikolov 团队搞的word2vec等等。 词向量在机器翻译领域的一个应用，就是google的TomasMikolov 团队开发了一种词典和术语表的自动生成技术，该技术通过向量空间，把一种语言转变成另一种语言，实验中对英语和西班牙语间的翻译准确率高达90%。 其中在介绍算法原理的时候举了一个例子：考虑英语和西班牙语两种语言，通过训练分别得到它们对应的词向量空间 E 和 S。从英语中取出五个词 one，two，three，four，five，设其在 E 中对应的词向量分别为 v1，v2，v3，v4，v5，为方便作图，利用主成分分析（PCA）降维，得到相应的二维向量 u1，u2，u3，u4，u5，在二维平面上将这五个点描出来，如下图左图所示。类似地，在西班牙语中取出（与 one，two，three，four，five 对应的） uno，dos，tres，cuatro，cinco，设其在 S 中对应的词向量分别为 s1，s2，s3，s4，s5，用 PCA 降维后的二维向量分别为 t1，t2，t3，t4，t5，将它们在二维平面上描出来（可能还需作适当的旋转），如下图右图所示： 观察左、右两幅图，容易发现：五个词在两个向量空间中的相对位置差不多，这说明两种语言对应向量空间结构之间具有相似性，从而进一步说明了在词向量空间利用刻画词之间相似性的合理性。 二、语言模型2.1 基本概念语言模型其实就是看一句话是不是正常人说出来的。这玩意很有用，比如机器翻译、语音识别得到若干候选之后，可以利用语言模型挑一个尽量靠谱的结果。在 NLP 的其它任务里也都能用到。语言模型形式化的描述就是给定一个T个词的字符串s，看他是自然语言的概率$P(w_1,w_2,···w_t)$，$w_1$到$w_t$依次表示这句话中的各个词。有个很简单的推论是： P(s)=P(w_1,w_2,···w_T)=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)···P(w_t|w_1,w_2,···,w_{t-1})上面这个概率表示的意义是：第一个词确定后，看后面的词在前面的词出现的情况下出现的概率。如一句话“大家喜欢吃苹果”，总共四个词“大家”、“喜欢”、“吃”、“苹果”，怎么分词现在不讨论，总之词已经分好，就这四个。那么这句话是一个自然语言的概率是： P(大家，喜欢，吃，苹果)=P(大家)P(喜欢|大家)P(吃|大家，喜欢)P(苹果|大家，喜欢，吃)其中，$P(大家)$表示“大家”这个词在语料库里面出现的概率；$P(喜欢|大家)$表示“喜欢”这个词出现在“大家喜欢”后面的概率；$P(吃|大家，喜欢)$表示“吃”这个词出现在“大家喜欢”后面的概率；$P(苹果|大家，喜欢，吃)$表示“苹果”这个词出现在“大家喜欢吃”后面的概率。把这些概率连乘起来，得到的就是这句话平时出现的概率。如果这个概率特别低，说明这句话不常出现，那么就不算是一句自然语言，因为在语料库中很少出现。如果出现的概率高，就说明是一句自然语言。 看到了上面的计算，看有多麻烦：只有四个词的一句话，需要计算的是$P(大家)$，$P(喜欢|大家)$，$P(吃|大家，喜欢)$，$P(苹果|大家，喜欢，吃)$这四个概率，这四个概率还要预先计算好，考虑词的数量，成千上万个，再考虑组合数，$P(吃|大家，喜欢)$这个有“大家”、“喜欢”、“吃”的组合，总会有上亿种情况吧；再考虑$P(苹果|大家,喜欢，吃)$这个概率，总共也会超过万亿种。 从上面的情况来看，计算起来是非常麻烦的，一般都用偷懒的方式。为了表示简单，上面的公式可以用下面的方式表示 P(s)=P(w_1,w_2,···w_t)=\prod_{i=1}^TP(w_i|context_i)其中，如果context是空的话，就是它自己$P(w)$，另外如“吃”的context就是“大家”、“喜欢”，其余的对号入座。 2.2 N-gram模型接下来说怎么计算$P(w_i|context_i)$，上面看的是根据这句话前面的所有词来计算，那么就得计算很多了，比如就得把语料库里面的组合“大家”、“喜欢”、“吃”、“苹果”这种情况全部统计一遍，那么为了计算这句话的概率，就上面那个例子，都得扫描四次语料库。这样一句话有多少个词就得扫描多少趟，语料库一般都比较大，越大的语料库越能提供准确的判断。这样的计算速度在真正使用的时候是万万不可接受的，线上扫描一篇文章是不是一推乱七八糟的没有序列的文字都得扫描很久，这样的应用根本没人考虑。 最好的办法就是直接把所有的$P(w_i|context_i)$提前算好了，那么根据排列组上面的来算，对于一个只有四个词的语料库，总共就有$4!+3!+2!+1!$个情况要计算，那就是24个情况要计算；换成1000个词的语料库，就是$\sum_{i=1}^{1000}i!$个情况需要统计，对于计算机来说，计算这些东西简直是开玩笑。 19世纪到20世纪初，俄罗斯有个数学家叫马尔科夫（Andrey Markov），他给了个偷懒但还颇为有效的方法，也就是每当遇见这种情况时，就假设任意一个词$w_i$出现的概率只同他前面的词$w_{t-1}$有关，于是问题就变得很简单了。这种假设在数学上称为马尔科夫假设。现在，s出现的概率就变得简单了： P(s)=P(w_i)P(w_2|w_1)···P(w_i|w_{i-1})···P(w_n|w_{n-1})这个公式对应的统计语言模型是二元模型（Bigram Model）。 接下来的问题就是如何估计条件概率$P(w_i|w_{i-1})$。根据它的定义： P(w_i|w_{i-1})=\frac{P(w_{i-1},w_i)}{P(w_{i-1})}。估计联合概率$P(w_{i-1},w_i)$和边缘概率$P(w_{i-1})$，现在变得很简单。因为有了大量机读文件，也就是专业人士讲的语料库，只要数一数$w_{i-1},w_i$这对词在统计的文本中前后相邻出现了多少次$ count (w_{i-1},w_i)$，以及$w_{i-1}$本身在同样的文本中出现了多少次$count (w_{i-1})$然后用两个数分别除以语料库的大小，即可得到这些词或二元组的相对频率： f(w_{i-1},w_i)=\frac{count (w_{i-1},w_i)}{count}f(w_{i-1})=\frac{count (w_{i-1})}{count}根据大数定理，只要统计量足够，相对频率就等于概率，即 P(w_{i-1},w_i)≈\frac{count (w_{i-1},w_i)}{count}而$P(w_i|w_{i-1})$就是这两个数的比值，再考虑到上面的两个概率有相同的分母，可以约掉，因此 P(w_i|w_{i-1})≈\frac{count (w_{i-1},w_i)}{count (w_{i-1})}当然，也可以假设一个词由前面N-1个词决定，对应的模型稍微复杂些，被称为N元模型。而N=1的一元模型实际上是一个上下文无关的模型，也就是假定当前词出现的概率与前面的词无关。而在实际应用中最多的是N=3的三元模型，更高阶的模型就很少使用了。 为什么N一般取值都这么小呢？这里面主要有两个原因。首先，N元模型的大小（或者说空间复杂度）几乎是N的指数函数，即$O(|V|^N)$，这里$|V|$是一种语言词典的词汇量，一般在几万到几十万个。而使用N元模型的速度（或者说时间复杂度）也几乎是一个指数函数，即$O(|V|^{N-1})$。因此，N不能太大。当N从1到2，再从2到3时，模型的效果上升显著。而当模型从3到4时，效果的提升就不是很显著了，而资源的耗费增加却非常快。 所以N-gram存在几个问题，总结如下： n-gram 语言模型无法建模更远的关系，语料的不足使得无法训练更高阶的 语言模型。大部分研究或工作都是使用 Trigram，就算使用高阶的模型，其统计 到的概率可信度就大打折扣，还有一些比较小的问题采用 Bigram。 这种模型无法建模出词之间的相似度，有时候两个具有某种相似性的词， 如果一个词经常出现在某段词之后，那么也许另一个词出现在这段词后面的概率 也比较大。比如“白色的汽车”经常出现，那完全可以认为“白色的轿车”也可 能经常出现。 有些n元（n个词的组合，跟顺序有关的）在语料库里面没有出现过，对应出来的条件概率就是0，这样一整句话的概率都是0了，这是不对的，解决的方法主要有两种： 平滑法：最简单的方法是把每个 n 元组的出现次数加 1，那么原来 出现 k 次的某个 n 元组就会记为 k+1 次，原来出现 0 次的 n 元组就会记为出现 1 次。这种也称为 Laplace 平滑。当然还有很多更复杂的其他平滑方法，其本质都 是将模型变为贝叶斯模型，通过引入先验分布打破似然一统天下的局面。而引入 先验方法的不同也就产生了很多不同的平滑方法。 回退法：即利用n-1的元组的概率去替代n元组的概率，有点像决策树中的后剪枝方法，即如果 n 元的概率不到， 那就往上回退一步，用 n-1 元的概率乘上一个权重来模拟。 2.3 上下文无关模型该模型仅仅考虑当前词本身的概率，不考虑该词所对应的上下文环境。这是一种最简单，易于实现，但没有多大实际应用价值的统计语言模型。 𝑝(𝑤_𝑡|Context)= 𝑝(𝑤_𝑡)=\frac{𝑁_{𝑤_𝑡}}{N}这个模型不考虑任何上下文信息，仅仅依赖于训练文本中的词频统计。它是 n-gram 模型中当 n=1 的特殊情形，所以有时也称作 Unigram Model(一元文法统 计模型)。实际应用中，常被应用到一些商用语音识别系统中。 2.4 N-pos模型严格来说 n-pos 只是 n-gram 的一种衍生模型。n-gram 模型假定第 t 个词出现 概率条件依赖它前 N-1 个词，而现实中很多词出现的概率是条件依赖于它前面词 的语法功能的。n-pos 模型就是基于这种假设的模型，它将词按照其语法功能进 行分类，由这些词类决定下一个词出现的概率。这样的词类称为词性 (Part-of-Speech，简称为 POS)。n-pos 模型中的每个词的条件概率表示为: 𝑝(s) = 𝑝(𝑤_1^𝑇)= 𝑝(𝑤_1,𝑤_2,...,𝑤_𝑇)=∏^𝑇_{𝑡=1}𝑝(𝑤_𝑡|𝑐(𝑤_{𝑡−n+1}),𝑐(𝑤_{𝑡−n+2}),...,𝑐(𝑤_{𝑡−1}))c 为类别映射函数，即把 T 个词映射到 K 个类别(1=&lt;K&lt;=T)。实际上 n-Pos使用了一种聚类的思想，使得原来 n-gram 中$𝑤_{𝑡−n+1}, 𝑤_{𝑡−n+2},… , 𝑤_{𝑡−1}$中的可能为$T^{N-1}$ 减少到$𝑐(𝑤_{𝑡−n+1}),𝑐(𝑤_{𝑡−n+2}),…,𝑐(𝑤_{𝑡−1})$中的 $K^{N-1}$，同时这种减少还采用了语义有意义的类别。 2.5 基于决策树的语言模型上面提到的上下文无关语言模型、N-gram语言模型、N-pos语言模型等等，都可以以统计决策树的形式表示出来。而统计决策树中每个结点的决策规则是一个上下文相关的问题。这些问题可以是“前一个词是w吗”“前一个词属于类别$c_i$吗”，当然基于决策树的语言模型还可以灵活一点，可以是一些“前一个是动词？”、“后面有介词吗”之类的复杂语法语义问题。 基于决策树的语言模型优点是：分布数不是预先固定好的，而是根据训练语料库中的实际情况确定，更为灵活。缺点是：构造通即决策树的问题很困难，且时空开销很大。 2.6 最大熵模型最大熵原理是E.T. Jayness于上世纪50年代提出的，其基本思想是:对一个 随机事件的概率分布进行预测时，在满足全部已知的条件下对未知的情况不做任 何主观假设。从信息论的角度来说就是:在只掌握关于未知分布的部分知识时， 应当选取符合这些知识但又能使得熵最大的概率分布。 P(w|context)=\frac{\sum_i\lambda _if_i(context,w)}{z(context)}其中$𝜆_𝑖$是参数，$𝑍(𝐶𝑜𝑛𝑡𝑒𝑥𝑡)$为归一化因子，因为采用的是这种 Softmax 形式， 所以最大熵模型有时候也称为指数模型。 2.7 自适应语言模型前面的模型概率分布都是预先从训练语料库中估算好的，属于静态语言模型。 而自适应语言模型类似是 Online Learning 的过程，即根据少量新数据动态调整模 型，属于动态模型。在自然语言中，经常出现这样现象:某些在文本中通常很少 出现的词，在某一局部文本中突然大量地出现。能够根据词在局部文本中出现的 情况动态地调整语言模型中的概率分布数据的语言模型成为动态、自适应或者基 于缓存的语言模型。通常的做法是将静态模型与动态模型通过参数融合到一起， 这种混合模型可以有效地避免数据稀疏的问题。 还有一种主题相关的自适应语言模型，直观的例子为:专门针对体育相关内容训练一个语言模型，同时保留所有语料训练的整体语言模型，当新来的数据属于体育类别时，其应该使用的模型就是体育相关主题模型和整体语言模型相融合的混合模型。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>词向量</tag>
        <tag>语言模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（40）：机器学习中的数据清洗与特征处理综述]]></title>
    <url>%2F2017%2F09%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8840%EF%BC%89%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E4%B8%8E%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[一、背景随着美团交易规模的逐步增大，积累下来的业务数据和交易数据越来越多，这些数据是美团做为一个团购平台最宝贵的财富。通过对这些数据的分析和挖掘，不仅能给美团业务发展方向提供决策支持，也为业务的迭代指明了方向。目前在美团的团购系统中大量地应用到了机器学习和数据挖掘技术，例如个性化推荐、筛选排序、搜索排序、用户建模等等，为公司创造了巨大的价值。本文主要介绍在美团的推荐与个性化团队实践中的数据清洗与特征挖掘方法。主要内容已经在内部公开课”机器学习InAction系列”讲过，本博客的内容主要是讲座内容的提炼和总结。 二、综述如上图所示是一个经典的机器学习问题框架图。数据清洗和特征挖掘的工作是在灰色框中框出的部分，即“数据清洗=&gt;特征，标注数据生成=&gt;模型学习=&gt;模型应用”中的前两个步骤。灰色框中蓝色箭头对应的是离线处理部分。主要工作是 从原始数据，如文本、图像或者应用数据中清洗出特征数据和标注数据。 对清洗出的特征和标注数据进行处理，例如样本采样，样本调权，异常点去除，特征归一化处理，特征变化，特征组合等过程。最终生成的数据主要是供模型训练使用。 灰色框中绿色箭头对应的是在线处理的部分。所做的主要工作和离线处理的类似，主要的区别在于1.不需要清洗标注数据，只需要处理得到特征数据，在线模型使用特征数据预测出样本可能的标签。2.最终生成数据的用处，最终生成的数据主要用于模型的预测，而不是训练。在离线的处理部分，可以进行较多的实验和迭代，尝试不同的样本采样、样本权重、特征处理方法、特征组合方法等，最终得到一个最优的方法，在离线评估得到好的结果后，最终将确定的方案在线上使用。另外，由于在线和离线环境不同，存储数据、获取数据的方法存在较大的差异。例如离线数据获取可以将数据存储在Hadoop，批量地进行分析处理等操作，并且容忍一定的失败。而在线服务获取数据需要稳定、延时小等，可以将数据建入索引、存入KV存储系统等。后面在相应的部分会详细地介绍。 本文以点击下单率预测为例，结合实例来介绍如何进行数据清洗和特征处理。首先介绍下点击下单率预测任务，其业务目标是提高团购用户的用户体验，帮助用户更快更好地找到自己想买的单子。这个概念或者说目标看起来比较虚，我们需要将其转换成一个技术目标，便于度量和实现。最终确定的技术目标是点击下单率预估，去预测用户点击或者购买团购单的概率。我们将预测出来点击或者下单率高的单子排在前面，预测的越准确，用户在排序靠前的单子点击、下单的就越多，省去了用户反复翻页的开销，很快就能找到自己想要的单子。离线我们用常用的衡量排序结果的AUC指标，在线的我们通过ABTest来测试算法对下单率、用户转化率等指标的影响。 三、特征使用方案在确定了目标之后，下一步，我们需要确定使用哪些数据来达到目标。需要事先梳理哪些特征数据可能与用户是否点击下单相关。我们可以借鉴一些业务经验，另外可以采用一些特征选择、特征分析等方法来辅助我们选择。具体的特征选择，特征分析等方法我们后面会详细介绍。从业务经验来判断，可能影响用户是否点击下单的因素有： 距离，很显然这是一个很重要的特征。如果购买一个离用户距离较远的单子，用户去消费这个单子需要付出很多的代价。 当然，也并不是没有买很远单子的用户，但是这个比例会比较小。 用户历史行为，对于老用户，之前可能在美团有过购买、点击等行为。用户实时兴趣。 单子质量，上面的特征都是比较好衡量的，单子质量可能是更复杂的一个特征。 是否热门，用户评价人数，购买数等等。 在确定好要使用哪些数据之后，我们需要对使用数据的可用性进行评估，包括数据的获取难度，数据的规模，数据的准确率，数据的覆盖率等， 数据获取难度：例如获取用户id不难，但是获取用户年龄和性别较困难，因为用户注册或者购买时，这些并不是必填项。即使填了也不完全准确。这些特征可能是通过额外的预测模型预测的，那就存在着模型精度的问题。 数据覆盖率：数据覆盖率也是一个重要的考量因素，例如距离特征，并不是所有用户的距离我们都能获取到。PC端的就没有距离，还有很多用户禁止使用它们的地理位置信息等。 用户历史行为，只有老用户才会有行为。 用户实时行为，如果用户刚打开app，还没有任何行为，同样面临着一个冷启动的问题。数据的准确率 单子质量，用户性别等，都会有准确率的问题。 四、特征获取方案Ok，在选定好要用的特征之后，我们需要考虑一个问题。就是这些数据从哪可以获取？只有获取了这些数据我们才能用上。否则，提一个不可能获取到的特征，获取不到，提了也是白提。下面就介绍下特征获取方案。 离线特征获取方案：离线可以使用海量的数据，借助于分布式文件存储平台，例如HDFS等，使用例如MapReduce，Spark等处理工具来处理海量的数据等。 在线特征获取方案：在线特征比较注重获取数据的延时，由于是在线服务，需要在非常短的时间内获取到相应的数据，对查找性能要求非常高，可以将数据存储在索引、kv存储等。而查找性能与数据的数据量会有矛盾，需要折衷处理，我们使用了特征分层获取方案，如下图所示。出于性能考虑。在粗排阶段，使用更基础的特征，数据直接建入索引。精排阶段，再使用一些个性化特征等。 五、特征与标注数据清洗在了解特征数据放在哪儿、怎样获取之后。下一步就是考虑如何处理特征和标注数据了。下面3节都是主要讲的特征和标注处理方法 5.1 标注数据清洗首先介绍下如何清洗特征数据，清洗特征数据方法可以分为离线清洗和在线清洗两种方法。 离线清洗数据：离线清洗优点是方便评估新特征效果，缺点是实时性差，与线上实时环境有一定误差。对于实时特征难以训练得到恰当的权重。 在线清洗数据：在线清洗优点是实时性强，完全记录的线上实际数据，缺点是新特征加入需要一段时间做数据积累。 5.2 样本采样与样本过滤特征数据只有在和标注数据合并之后，才能用来做为模型的训练。下面介绍下如何清洗标注数据。主要是数据采样和样本过滤。 数据采样，例如对于分类问题：选取正例，负例。对于回归问题，需要采集数据。对于采样得到的样本，根据需要，需要设定样本权重。当模型不能使用全部的数据来训练时，需要对数据进行采样，设定一定的采样率。采样的方法包括随机采样，固定比例采样等方法。 除了采样外，经常对样本还需要进行过滤，包括 1.结合业务情况进行数据的过滤，例如去除crawler抓取，spam，作弊等数据。 2.异常点检测，采用异常点检测算法对样本进行分析，常用的异常点检测算法包括偏差检测，例如聚类，最近邻等。 基于统计的异常点检测算法 例如极差，四分位数间距，均差，标准差等，这种方法适合于挖掘单变量的数值型数据。全距(Range)，又称极差，是用来表示统计资料中的变异量数(measures of variation) ，其最大值与最小值之间的差距；四分位距通常是用来构建箱形图，以及对概率分布的简要图表概述。 基于距离的异常点检测算法，主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，主要使用的距离度量方法有绝对距离 ( 曼哈顿距离 ) 、欧氏距离和马氏距离等方法。 基于密度的异常点检测算法，考察当前点周围密度，可以发现局部异常点，例如LOF算法 六、特征分类在分析完特征和标注的清洗方法之后，下面来具体介绍下特征的处理方法，先对特征进行分类，对于不同的特征应该有不同的处理方法。 根据不同的分类方法，可以将特征分为(1)Low level特征和High level特征。(2)稳定特征与动态特征。(3)二值特征、连续特征、枚举特征。 Low level特征是较低级别的特征，主要是原始特征，不需要或者需要非常少的人工处理和干预，例如文本特征中的词向量特征，图像特征中的像素点，用户id，商品id等。Low level特征一般维度比较高，不能用过于复杂的模型。High level特征是经过较复杂的处理，结合部分业务逻辑或者规则、模型得到的特征，例如人工打分，模型打分等特征，可以用于较复杂的非线性模型。Low level 比较针对性，覆盖面小。长尾样本的预测值主要受high level特征影响。 高频样本的预测值主要受low level特征影响。 稳定特征是变化频率(更新频率)较少的特征，例如评价平均分，团购单价格等，在较长的时间段内都不会发生变化。动态特征是更新变化比较频繁的特征，有些甚至是实时计算得到的特征，例如距离特征，2小时销量等特征。或者叫做实时特征和非实时特征。针对两类特征的不同可以针对性地设计特征存储和更新方式，例如对于稳定特征，可以建入索引，较长时间更新一次，如果做缓存的话，缓存的时间可以较长。对于动态特征，需要实时计算或者准实时地更新数据，如果做缓存的话，缓存过期时间需要设置的较短。 二值特征主要是0/1特征，即特征只取两种值：0或者1，例如用户id特征：目前的id是否是某个特定的id，词向量特征：某个特定的词是否在文章中出现等等。连续值特征是取值为有理数的特征，特征取值个数不定，例如距离特征，特征取值为是0~正无穷。枚举值特征主要是特征有固定个数个可能值，例如今天周几，只有7个可能值：周1，周2，…，周日。在实际的使用中，我们可能对不同类型的特征进行转换，例如将枚举特征或者连续特征处理为二值特征。枚举特征处理为二值特征技巧：将枚举特征映射为多个特征，每个特征对应一个特定枚举值，例如今天周几，可以把它转换成7个二元特征：今天是否是周一，今天是否是周二，…，今天是否是周日。连续值处理为二值特征方法：先将连续值离散化（后面会介绍如何离散化)，再将离散化后的特征切分为N个二元特征，每个特征代表是否在这个区间内。 6.1 特征归一化，离散化，缺省值处理主要用于单个特征的处理。 归一化不同的特征有不同的取值范围，在有些算法中，例如线性模型或者距离相关的模型像聚类模型、knn模型等，特征的取值范围会对最终的结果产生较大影响，例如二元特征的取值范围为[0，1]，而距离特征取值可能是[0，正无穷)，在实际使用中会对距离进行截断，例如[0，3000000]，但是这两个特征由于取值范围不一致导致了模型可能会更偏向于取值范围较大的特征，为了平衡取值范围不一致的特征，需要对特征进行归一化处理，将特征取值归一化到［0，1］区间。常用的归一化方法包括1.函数归一化，通过映射函数将特征取值映射到［0，1］区间，例如最大最小值归一化方法，是一种线性的映射。还有通过非线性函数的映射，例如log函数等。2.分维度归一化，可以使用最大最小归一化方法，但是最大最小值选取的是所属类别的最大最小值，即使用的是局部最大最小值，不是全局的最大最小值。3.排序归一化，不管原来的特征取值是什么样的，将特征按大小排序，根据特征所对应的序给予一个新的值。 离散化在上面介绍过连续值的取值空间可能是无穷的，为了便于表示和在模型中处理，需要对连续值特征进行离散化处理。常用的离散化方法包括等值划分和等量划分。等值划分是将特征按照值域进行均分，每一段内的取值等同处理。例如某个特征的取值范围为[0，10]，我们可以将其划分为10段，[0，1)，[1，2)，…，[9，10)。等量划分是根据样本总数进行均分，每段等量个样本划分为1段。例如距离特征，取值范围［0，3000000］，现在需要切分成10段，如果按照等比例划分的话，会发现绝大部分样本都在第1段中。使用等量划分就会避免这种问题，最终可能的切分是[0，100)，[100，300)，[300，500)，..，[10000，3000000]，前面的区间划分比较密，后面的比较稀疏。 缺省值处理有些特征可能因为无法采样或者没有观测值而缺失，例如距离特征，用户可能禁止获取地理位置或者获取地理位置失败，此时需要对这些特征做特殊的处理，赋予一个缺省值。缺省值如何赋予，也有很多种方法。例如单独表示，众数，平均值等。 6.2 特征降维在介绍特征降维之前，先介绍下特征升维。在机器学习中，有一个VC维理论。根据VC维理论，VC维越高，打散能力越强，可容许的模型复杂度越高。在低维不可分的数据，映射到高维是可分。可以想想，给你一堆物品，人脑是如何对这些物品进行分类，依然是找出这些物品的一些特征，例如：颜色，形状，大小，触感等等，然后根据这些特征对物品做以归类，这其实就是一个先升维，后划分的过程。比如我们人脑识别香蕉。可能首先我们发现香蕉是黄色的。这是在颜色这个维度的一个切分。但是很多东西都是黄色的啊，例如哈密瓜。那么怎么区分香蕉和哈密瓜呢？我们发现香蕉形状是弯曲的。而哈密瓜是圆形的，那么我们就可以用形状来把香蕉和哈密瓜划分开了，即引入一个新维度：形状，来区分。这就是一个从“颜色”一维特征升维到二维特征的例子。 那问题来了，既然升维后模型能力能变强，那么是不是特征维度越高越好呢？为什么要进行特征降维&amp;特征选择？主要是出于如下考虑：1. 特征维数越高，模型越容易过拟合，此时更复杂的模型就不好用。2. 相互独立的特征维数越高，在模型不变的情况下，在测试集上达到相同的效果表现所需要的训练样本的数目就越大。 3. 特征数量增加带来的训练、测试以及存储的开销都会增大。4.在某些模型中，例如基于距离计算的模型KMeans，KNN等模型，在进行距离计算时，维度过高会影响精度和性能。5.可视化分析的需要。在低维的情况下，例如二维，三维，我们可以把数据绘制出来，可视化地看到数据。当维度增高时，就难以绘制出来了。在机器学习中，有一个非常经典的维度灾难的概念。用来描述当空间维度增加时，分析和组织高维空间，因体积指数增加而遇到各种问题场景。例如，100个平均分布的点能把一个单位区间以每个点距离不超过0.01采样；而当维度增加到10后，如果以相邻点距离不超过0.01小方格采样单位超一单位超正方体，则需要10^20 个采样点。 正是由于高维特征有如上描述的各种各样的问题，所以我们需要进行特征降维和特征选择等工作。特征降维常用的算法有PCA，LDA等。特征降维的目标是将高维空间中的数据集映射到低维空间数据，同时尽可能少地丢失信息，或者降维后的数据点尽可能地容易被区分 PCA算法通过协方差矩阵的特征值分解能够得到数据的主成分，以二维特征为例，两个特征之间可能存在线性关系（例如运动的时速和秒速度），这样就造成了第二维信息是冗余的。PCA的目标是发现这种特征之间的线性关系，并去除。 LDA算法考虑label，降维后的数据点尽可能地容易被区分 6.3 特征选择特征选择的目标是寻找最优特征子集。特征选择能剔除不相关(irrelevant)或冗余(redundant )的特征，从而达到减少特征个数，提高模型精确度，减少运行时间的目的。另一方面，选取出真正相关的特征简化模型，协助理解数据产生的过程。 特征选择的一般过程如下图所示：主要分为产生过程，评估过程，停止条件和验证过程。 6.3.1 特征选择-产生过程和生成特征子集方法 完全搜索(Complete) 广度优先搜索( Breadth First Search )：广度优先遍历特征子空间。枚举所有组合，穷举搜索，实用性不高。 分支限界搜索( Branch and Bound )：穷举基础上加入分支限界。例如：剪掉某些不可能搜索出比当前最优解更优的分支。 其他，如定向搜索 (Beam Search )，最优优先搜索 ( Best First Search )等 启发式搜索(Heuristic) 序列前向选择( SFS ， Sequential Forward Selection )：从空集开始，每次加入一个选最优。 序列后向选择( SBS ， Sequential Backward Selection )：从全集开始，每次减少一个选最优。 增L去R选择算法 ( LRS ， Plus-L Minus-R Selection )：从空集开始，每次加入L个，减去R个，选最优（L&gt;R)或者从全集开始，每次减去R个，增加L个，选最优(L&lt;R)。 其他如双向搜索( BDS ， Bidirectional Search )，序列浮动选择( Sequential Floating Selection )等 随机搜索(Random) 随机产生序列选择算法(RGSS， Random Generation plus Sequential Selection) 随机产生一个特征子集，然后在该子集上执行SFS与SBS算法。 模拟退火算法( SA， Simulated Annealing )：以一定的概率来接受一个比当前解要差的解，而且这个概率随着时间推移逐渐降低 遗传算法( GA， Genetic Algorithms )：通过交叉、突变等操作繁殖出下一代特征子集，并且评分越高的特征子集被选中参加繁殖的概率越高。 随机算法共同缺点:依赖随机因素，有实验结果难重现。 6.3.2 特征选择－有效性分析对特征的有效性进行分析，得到各个特征的特征权重，根据是否与模型有关可以分为1.与模型相关特征权重，使用所有的特征数据训练出来模型，看在模型中各个特征的权重，由于需要训练出模型，模型相关的权重与此次学习所用的模型比较相关。不同的模型有不同的模型权重衡量方法。例如线性模型中，特征的权重系数等。2.与模型无关特征权重。主要分析特征与label的相关性，这样的分析是与这次学习所使用的模型无关的。与模型无关特征权重分析方法包括(1)交叉熵，(2)Information Gain，(3)Odds ratio，(4)互信息，(5)KL散度等 七、特征监控在机器学习任务中，特征非常重要。 个人经验，80%的效果由特征带来。下图是随着特征数的增加，最终模型预测值与实际值的相关系数变化。 对于重要的特征进行监控与有效性分析，了解模型所用的特征是否存在问题，当某个特别重要的特征出问题时，需要做好备案，防止灾难性结果。需要建立特征有效性的长效监控机制 我们对关键特征进行了监控，下面特征监控界面的一个截图。通过监控我们发现有一个特征的覆盖率每天都在下降，与特征数据提供方联系之后，发现特征数据提供方的数据源存在着问题，在修复问题之后，该特征恢复正常并且覆盖率有了较大提升。 在发现特征出现异常时，我们会及时采取措施，对服务进行降级处理，并联系特征数据的提供方尽快修复。对于特征数据生成过程中缺乏监控的情况也会督促做好监控，在源头解决问题。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（39）：实例详解机器学习如何解决问题]]></title>
    <url>%2F2017%2F09%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8839%EF%BC%89%EF%BC%9A%E5%AE%9E%E4%BE%8B%E8%AF%A6%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[一、前言随着大数据时代的到来，机器学习成为解决问题的一种重要且关键的工具。不管是工业界还是学术界，机器学习都是一个炙手可热的方向，但是学术界和工业界对机器学习的研究各有侧重，学术界侧重于对机器学习理论的研究，工业界侧重于如何用机器学习来解决实际问题。我们结合美团在机器学习上的实践，进行一个实战（InAction）系列的介绍（带“机器学习InAction系列”标签的文章），介绍机器学习在解决工业界问题的实战中所需的基本技术、经验和技巧。本文主要结合实际问题，概要地介绍机器学习解决实际问题的整个流程，包括对问题建模、准备训练数据、抽取特征、训练模型和优化模型等关键环节；另外几篇则会对这些关键环节进行更深入地介绍。 下文分为 1）机器学习的概述， 2）对问题建模， 3）准备训练数据， 4）抽取特征， 5）训练模型， 6）优化模型， 7）总结 共7个章节进行介绍。 二、机器学习的概述：2.1 什么是机器学习？随着机器学习在实际工业领域中不断获得应用，这个词已经被赋予了各种不同含义。在本文中的“机器学习”含义与wikipedia上的解释比较契合，如下：Machine learning is a scientific discipline that deals with the construction and study of algorithms that can learn from data. 机器学习可以分为无监督学习（unsupervised learning）和有监督学习（supervised learning），在工业界中，有监督学习是更常见和更有价值的方式，下文中主要以这种方式展开介绍。如下图中所示，有监督的机器学习在解决实际问题时，有两个流程，一个是离线训练流程（蓝色箭头），包含数据筛选和清洗、特征抽取、模型训练和优化模型等环节；另一个流程则是应用流程（绿色箭头），对需要预估的数据，抽取特征，应用离线训练得到的模型进行预估，获得预估值作用在实际产品中。在这两个流程中，离线训练是最有技术挑战的工作（在线预估流程很多工作可以复用离线训练流程的工作），所以下文主要介绍离线训练流程。 2.2 什么是模型？模型，是机器学习中的一个重要概念，简单的讲，指特征空间到输出空间的映射；一般由模型的假设函数和参数w组成（下面公式就是Logistic Regression模型的一种表达，在训练模型的章节做稍详细的解释）；一个模型的假设空间（hypothesis space），指给定模型所有可能w对应的输出空间组成的集合。工业界常用的模型有Logistic Regression（简称LR）、Gradient Boosting Decision Tree（简称GBDT）、Support Vector Machine（简称SVM）、Deep Neural Network（简称DNN）等。 模型训练就是基于训练数据，获得一组参数w，使得特定目标最优，即获得了特征空间到输出空间的最优映射，具体怎么实现，见训练模型章节。 2.3 为什么要用机器学习解决问题？ 目前处于大数据时代，到处都有成T成P的数据，简单规则处理难以发挥这些数据的价值； 廉价的高性能计算，使得基于大规模数据的学习时间和代价降低； 廉价的大规模存储，使得能够更快地和代价更小地处理大规模数据； 存在大量高价值的问题，使得花大量精力用机器学习解决问题后，能获得丰厚收益。 2.4 机器学习应该用于解决什么问题？ 目标问题需要价值巨大，因为机器学习解决问题有一定的代价； 目标问题有大量数据可用，有大量数据才能使机器学习比较好地解决问题（相对于简单规则或人工）； 目标问题由多种因素（特征）决定，机器学习解决问题的优势才能体现（相对于简单规则或人工）； 目标问题需要持续优化，因为机器学习可以基于数据自我学习和迭代，持续地发挥价值。 三、对问题建模本文以DEAL（团购单）交易额预估问题为例（就是预估一个给定DEAL一段时间内卖了多少钱），介绍使用机器学习如何解决问题。首先需要： 收集问题的资料，理解问题，成为这个问题的专家； 拆解问题，简化问题，将问题转化机器可预估的问题。 深入理解和分析DEAL交易额后，可以将它分解为如下图的几个问题： 3.1 单个模型？多个模型？如何来选择？按照上图进行拆解后，预估DEAL交易额就有2种可能模式，一种是直接预估交易额；另一种是预估各子问题，如建立一个用户数模型和建立一个访购率模型（访问这个DEAL的用户会购买的单子数），再基于这些子问题的预估值计算交易额。 不同方式有不同优缺点，具体如下： 选择哪种模式？ 1）问题可预估的难度，难度大，则考虑用多模型； 2）问题本身的重要性，问题很重要，则考虑用多模型； 3）多个模型的关系是否明确，关系明确，则可以用多模型。 如果采用多模型，如何融合？ 可以根据问题的特点和要求进行线性融合，或进行复杂的融合。以本文问题为例，至少可以有如下两种： 3.2 模型选择对于DEAL交易额这个问题，我们认为直接预估难度很大，希望拆成子问题进行预估，即多模型模式。那样就需要建立用户数模型和访购率模型，因为机器学习解决问题的方式类似，下文只以访购率模型为例。要解决访购率问题，首先要选择模型，我们有如下的一些考虑： 主要考虑 1）选择与业务目标一致的模型； 2）选择与训练数据和特征相符的模型。 训练数据少，High Level特征多，则使用“复杂”的非线性模型（流行的GBDT、Random Forest等）； 训练数据很大量，Low Level特征多，则使用“简单”的线性模型（流行的LR、Linear-SVM等）。 补充考虑 1）当前模型是否被工业界广泛使用；2）当前模型是否有比较成熟的开源工具包（公司内或公司外）；3）当前工具包能够的处理数据量能否满足要求；4）自己对当前模型理论是否了解，是否之前用过该模型解决问题。 为实际问题选择模型，需要转化问题的业务目标为模型评价目标，转化模型评价目标为模型优化目标；根据业务的不同目标，选择合适的模型，具体关系如下： 通常来讲，预估真实数值（回归）、大小顺序（排序）、目标所在的正确区间（分类）的难度从大到小，根据应用所需，尽可能选择难度小的目标进行。对于访购率预估的应用目标来说，我们至少需要知道大小顺序或真实数值，所以我们可以选择Area Under Curve（AUC）或Mean Absolute Error（MAE）作为评估目标，以Maximum likelihood为模型损失函数（即优化目标）。综上所述，我们选择spark版本 GBDT或LR，主要基于如下考虑： 1）可以解决排序或回归问题；2）我们自己实现了算法，经常使用，效果很好；3）支持海量数据；4）工业界广泛使用。 四、准备训练数据深入理解问题，针对问题选择了相应的模型后，接下来则需要准备数据；数据是机器学习解决问题的根本，数据选择不对，则问题不可能被解决，所以准备训练数据需要格外的小心和注意： 4.1 注意点：待解决问题的数据本身的分布尽量一致；训练集/测试集分布与线上预测环境的数据分布尽可能一致，这里的分布是指（x,y）的分布，不仅仅是y的分布；y数据噪音尽可能小，尽量剔除y有噪音的数据；非必要不做采样，采样常常可能使实际数据分布发生变化，但是如果数据太大无法训练或者正负比例严重失调（如超过100:1）,则需要采样解决。 4.2 常见问题及解决办法待解决问题的数据分布不一致：1）访购率问题中DEAL数据可能差异很大，如美食DEAL和酒店DEAL的影响因素或表现很不一致，需要做特别处理；要么对数据提前归一化，要么将分布不一致因素作为特征，要么对各类别DEAL单独训练模型。数据分布变化了：1）用半年前的数据训练模型，用来预测当前数据，因为数据分布随着时间可能变化了，效果可能很差。尽量用近期的数据训练，来预测当前数据，历史的数据可以做降权用到模型，或做transfer learning。y数据有噪音：1）在建立CTR模型时，将用户没有看到的Item作为负例，这些Item是因为用户没有看到才没有被点击，不一定是用户不喜欢而没有被点击，所以这些Item是有噪音的。可以采用一些简单规则，剔除这些噪音负例，如采用skip-above思想，即用户点过的Item之上，没有点过的Item作为负例（假设用户是从上往下浏览Item）。采样方法有偏，没有覆盖整个集合：1）访购率问题中，如果只取只有一个门店的DEAL进行预估，则对于多门店的DEAL无法很好预估。应该保证一个门店的和多个门店的DEAL数据都有；2）无客观数据的二分类问题，用规则来获得正/负例，规则对正/负例的覆盖不全面。应该随机抽样数据，进行人工标注，以确保抽样数据和实际数据分布一致。 4.3 访购率问题的训练数据收集N个月的DEAL数据（x）及相应访购率（y）；收集最近N个月，剔除节假日等非常规时间 （保持分布一致）；只收集在线时长&gt;T 且 访问用户数 &gt; U的DEAL （减少y的噪音）；考虑DEAL销量生命周期 （保持分布一致）；考虑不同城市、不同商圈、不同品类的差别 （保持分布一致）。 五、抽取特征完成数据筛选和清洗后，就需要对数据抽取特征，就是完成输入空间到特征空间的转换（见下图）。针对线性模型或非线性模型需要进行不同特征抽取，线性模型需要更多特征抽取工作和技巧，而非线性模型对特征抽取要求相对较低。 通常，特征可以分为High Level与Low Level，High Level指含义比较泛的特征，Low Level指含义比较特定的特征，举例来说： 1234DEAL A1属于POIA，人均50以下，访购率高；DEAL A2属于POIA，人均50以上，访购率高；DEAL B1属于POIB，人均50以下，访购率高；DEAL B2属于POIB，人均50以上，访购率底； 基于上面的数据，可以抽到两种特征，POI（门店）或人均消费；POI特征则是Low Level特征，人均消费则是High Level特征；假设模型通过学习，获得如下预估： 12如果DEALx 属于POIA（Low Level feature），访购率高；如果DEALx 人均50以下（High Level feature），访购率高。 所以，总体上，Low Level 比较有针对性，单个特征覆盖面小（含有这个特征的数据不多），特征数量（维度）很大。High Level比较泛化，单个特征覆盖面大（含有这个特征的数据很多），特征数量（维度）不大。长尾样本的预测值主要受High Level特征影响。高频样本的预测值主要受Low Level特征影响。 对于访购率问题，有大量的High Level或Low Level的特征，其中一些展示在下图： 非线性模型的特征 1）可以主要使用High Level特征，因为计算复杂度大，所以特征维度不宜太高； 2）通过High Level非线性映射可以比较好地拟合目标。 线性模型的特征 1）特征体系要尽可能全面，High Level和Low Level都要有； 2）可以将High Level转换Low Level，以提升模型的拟合能力。 5.1 特征归一化特征抽取后，如果不同特征的取值范围相差很大，最好对特征进行归一化，以取得更好的效果，常见的归一化方式如下： Rescaling：归一化到[0,1] 或 [-1，1]，用类似方式： Standardization：设为x分布的均值，为x分布的标准差； Scaling to unit length：归一化到单位长度向量 5.2 特征选择特征抽取和归一化之后，如果发现特征太多，导致模型无法训练，或很容易导致模型过拟合，则需要对特征进行选择，挑选有价值的特征。 Filter：假设特征子集对模型预估的影响互相独立，选择一个特征子集，分析该子集和数据Label的关系，如果存在某种正相关，则认为该特征子集有效。衡量特征子集和数据Label关系的算法有很多，如Chi-square，Information Gain。 Wrapper：选择一个特征子集加入原有特征集合，用模型进行训练，比较子集加入前后的效果，如果效果变好，则认为该特征子集有效，否则认为无效。 Embedded：将特征选择和模型训练结合起来，如在损失函数中加入L1 Norm ，L2 Norm。 六、训练模型完成特征抽取和处理后，就可以开始模型训练了，下文以简单且常用的Logistic Regression模型（下称LR模型）为例，进行简单介绍。 设有m个（x,y）训练数据，其中x为特征向量，y为label，；w为模型中参数向量，即模型训练中需要学习的对象。所谓训练模型，就是选定假说函数和损失函数，基于已有训练数据（x,y），不断调整w，使得损失函数最优，相应的w就是最终学习结果，也就得到相应的模型。 6.1 模型函数假说函数，即假设x和y存在一种函数关系： 损失函数，基于上述假设函数，构建模型损失函数（优化目标），在LR中通常以（x,y）的最大似然估计为目标： 6.2 优化算法 梯度下降（Gradient Descent）即w沿着损失函数的负梯度方向进行调整，示意图见下图，的梯度即一阶导数（见下式），梯度下降有多种类型，如随机梯度下降或批量梯度下降。随机梯度下降（Stochastic Gradient Descent），每一步随机选择一个样本，计算相应的梯度，并完成w的更新，如下式， 批量梯度下降（Batch Gradient Descent）,每一步都计算训练数据中的所有样本对应的梯度，w沿着这个梯度方向迭代，即 牛顿法（Newton’s Method） 牛顿法的基本思想是在极小点附近通过对目标函数做二阶Taylor展开，进而找到L(w)的极小点的估计值。形象地讲，在wk处做切线，该切线与L(w)=0的交点即为下一个迭代点wk+1（示意图如下）。w的更新公式如下，其中目标函数的二阶偏导数，即为大名鼎鼎的Hessian矩阵。 拟牛顿法（Quasi-Newton Methods）：计算目标函数的二阶偏导数，难度较大，更为复杂的是目标函数的Hessian矩阵无法保持正定；不用二阶偏导数而构造出可以近似Hessian矩阵的逆的正定对称阵，从而在”拟牛顿”的条件下优化目标函数。 BFGS： 使用BFGS公式对H(w)进行近似，内存中需要放H(w),内存需要O(m2)级别； L-BFGS：存储有限次数（如k次）的更新矩阵，用这些更新矩阵生成新的H(w),内存降至O(m)级别； OWLQN: 如果在目标函数中引入L1正则化，需要引入虚梯度来解决目标函数不可导问题，OWLQN就是用来解决这个问题。 Coordinate Descent对于w，每次迭代，固定其他维度不变，只对其一个维度进行搜索，确定最优下降方向（示意图如下），公式表达如下： 七、优化模型经过上文提到的数据筛选和清洗、特征设计和选择、模型训练，就得到了一个模型，但是如果发现效果不好？怎么办？ 【首先】反思目标是否可预估，数据和特征是否存在bug。 【然后】分析一下模型是Overfitting还是Underfitting，从数据、特征和模型等环节做针对性优化。 7.1 Underfitting &amp; Overfitting所谓Underfitting，即模型没有学到数据内在关系，如下图左一所示，产生分类面不能很好的区分X和O两类数据；产生的深层原因，就是模型假设空间太小或者模型假设空间偏离。所谓Overfitting，即模型过渡拟合了训练数据的内在关系，如下图右一所示，产生分类面过好地区分X和O两类数据，而真实分类面可能并不是这样，以至于在非训练数据上表现不好；产生的深层原因，是巨大的模型假设空间与稀疏的数据之间的矛盾。 在实战中，可以基于模型在训练集和测试集上的表现来确定当前模型到底是Underfitting还是Overfitting，判断方式如下表： 7.2 怎么解决Underfitting和Overfitting问题？ 八、总结综上所述，机器学习解决问题涉及到问题建模、准备训练数据、抽取特征、训练模型和优化模型等关键环节，有如下要点： 理解业务，分解业务目标，规划模型可预估的路线图。 数据：y数据尽可能真实客观；训练集/测试集分布与线上应用环境的数据分布尽可能一致。 特征：利用Domain Knowledge进行特征抽取和选择；针对不同类型的模型设计不同的特征。 模型：针对不同业务目标、不同数据和特征，选择不同的模型；如果模型不符合预期，一定检查一下数据、特征、模型等处理环节是否有bug；考虑模型Underfitting和Qverfitting，针对性地优化。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（38）：外卖订单量预测异常报警模型实践]]></title>
    <url>%2F2017%2F09%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8838%EF%BC%89%EF%BC%9A%E5%A4%96%E5%8D%96%E8%AE%A2%E5%8D%95%E9%87%8F%E9%A2%84%E6%B5%8B%E5%BC%82%E5%B8%B8%E6%8A%A5%E8%AD%A6%E6%A8%A1%E5%9E%8B%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[一、前言外卖业务的快速发展对系统稳定性提出了更高的要求，每一次订单量大盘的异常波动，都需要做出及时的应对，以保证系统的整体稳定性。如何做出较为准确的波动预警，显得尤为重要。 从时间上看，外卖订单量时间序列有两个明显的特征（如下图所示）： 周期性。每天订单量的变化趋势都大致相同，午高峰和晚高峰订单量集中。实时性。当天的订单量可能会受天气等因素影响，呈现整体的上涨或下降。订单量波动预警，初期外卖订单中心使用的是当前时刻和前一时刻订单量比较，超过一定阈值就报警的方式，误报率和漏报率都比较大。后期将业务数据上传到美团点评的服务治理平台，使用该平台下的基线报警模型进行监控报警。基线数据模型考虑到了订单量时间序列的周期性特征，但是忽略了实时性特征，在实际使用中误报率依然很高，大量的误报漏报导致RD对于报警已经麻木，出现问题时不能及时响应，因此，急需一种新的异常检测模型，提高报警的准确率。 二、异常检测的定义异常，意为“异于正常”。异常检测，就是从一组数据中寻找那些和期望数据不同的数据。监控数据都是和时间相关的，每一个监控指标只有和时间组合一起才有其具体的含义。按照时间顺序，将监控指标组成一个序列，我们就得到了监控指标的时间序列。 基于预测的异常检测模型如下图所示，xt是真实数据，通过预测器得到预测数据，然后xt和pt分别作为比较器的输入，最终得到输出yt。yt是一个二元值，可以用+1（+1表示输入数据正常），-1（-1表示输入数据异常）表示。 异常检测主要有两种策略： 异常驱动的异常检测（敏感性）：宁愿误报，也不能错过任何一个异常，这适用于非常重要的检测。简单概括，就是“宁可错杀一千，不能放过一个”。 预算驱动的异常检测（准确性）：这种策略的异常检测，从字面理解就是只有定量的一些预算去处理这些报警，那么只能当一定是某种问题时，才能将报警发送出来。 这两种策略不可兼容的。对于检测模型的改善，可以从两个方面入手，一是预测器的优化，二是比较器的优化。我们从这两个方面描述模型的改善。 三、预测器设计预测器，就是用一批历史数据预测当前的数据。使用的历史数据集大小，以及使用的预测算法都会影响最终的预测效果。 外卖订单量具有明显的周期性，同时相邻时刻的订单量数据也有很强的相关性，我们的目标，就是使用上面说的相关数据预测出当前的订单量。下面，我们分析几种常用的预测器实现。 3.1 同比环比预测器同比环比是比较常用的异常检测方式，它是将当前时刻数据和前一时刻数据（环比）或者前一天同一时刻数据（同比）比较，超过一定阈值即认为该点异常。如果用图2.1模型来表示，那么预测器就可以表示为用当前时刻前一时刻或者前一天同一时刻数据作为当前时刻的预测数据。 假如需要预测图中黄色数据，那么环比使用图中的蓝色数据作为预测黄点的源数据，同比使用图中红色数据作为预测黄点的源数据。 3.2 基线预测器同比环比使用历史上的单点数据来预测当前数据，误差比较大。t时刻的监控数据，与$t-1,t-2,…$时刻的监控数据存在相关性。同时，与$t-k,t-2k,…$时刻的数据也存在相关性（k为周期），如果能利用上这些相关数据对t时刻进行预测，预测结果的误差将会更小。 比较常用的方式是对历史数据求平均，然后过滤噪声，可以得到一个平滑的曲线（基线），使用基线数据来预测当前时刻的数据。该方法预测t时刻数据（图中黄色数据）使用到的历史数据如下图所示（图中红色数据）： 基线数据预测器广泛应用在业务大盘监控中，预测效果如图3.3所示。从图中可以看出，基线比较平滑，在低峰期预测效果比较好，但是在外卖的午高峰和晚高峰预测误差比较大。 3.3 Holt-Winters预测器同比环比预测到基线数据预测，使用的相关数据变多，预测的效果也较好。但是基线数据预测器只使用了周期相关的历史数据，没有使用上同周期相邻时刻的历史数据，相邻时刻的历史数据对于当前时刻的预测影响是比较大的。如外卖订单量，某天天气不好，很多用户不愿意出门，那么当天的外卖的订单量就会呈现整体的上涨，这种整体上涨趋势只能从同一周期相邻时刻的历史数据中预测出来。如图3.4所示，预测图中黄色数据，如果使用上图中所有的红色数据，那么预测效果会更好。 本文使用了Holt-Winters来实现这一目标。 Holt-Winters是三次指数滑动平均算法，它将时间序列数据分为三部分：残差数据a(t)，趋势性数据b(t)，季节性数据s(t)。使用Holt-Winters预测t时刻数据，需要t时刻前包含多个周期的历史数据。相关链接：Exponential smoothing、Holt-Winters seasonal method。 各部分的迭代计算公式（周期为k）： 如图所示，(a)显示了某一段时间内外卖订单的原始提单监控数据（分钟统计量，周期为1天），图(b)显示了其Holt-Winters的分解图（四幅图分别对应原始数据、残差数据分量、趋势数据分量、周期数据分量）。将订单量时间序列分解为残差数据a(t)，趋势数据b(t)，周期数据s(t)后，就可以使用下面的公式预测未来不同时刻时刻的订单量，其中h表示未来时刻距离当前时刻的跨度。 外卖订单量，是按分钟统计的离散时间序列，所以如果需要预测下一分钟的订单量，令h=1。 3.4 外卖报警模型中的预测器在外卖订单量异常检测中，使用Holt-Winters预测器实时预测下一分钟订单量，每次需要至少5天以上的订单量数据才能有较好的预测效果，数据量要求比较大。 在实际的异常检测模型中，我们对Holt-Winters预测器进行了简化。预测器的趋势数据表示的是时间序列的总体变化趋势，如果以天为周期看待外卖的订单量时间序列，是没有明显的趋势性的，图3.5(b)的分解图也证明了这一点。因此，我们可以去掉其中的趋势数据部分。 各部分的迭代公式简化为(3-1)： 预测值： h越大，预测值Yhat[t+h] 的误差也就越大。实时的订单流监控，令h=1，每当有新的监控数据时，更新输入序列，然后预测下一分钟数据。 Holt-Winters每一次预测都需要大量的输入数据序列。从上面模型的简化公式可以看出，对残差数据a(t)的预测是对序列(a(t-m),a(t-m+1),…a(t-2),a(t-1))的一次指数滑动平均，对周期数据s(t)的预测是对序列（s(t-mk) ,s(t-(m-1)k),…s(t-k)）的一次滑动平均，大量的输入数据是用于周期数据s(t)的计算。 a(t)和s(t)是互相关联的迭代计算过程，如果从周期性角度看公式(3-1)，可以发现：计算当前周期内的a(t)时，使用的是上一周期计算出来的s(t-k)，当前周期计算出的s(t)是用于下一周期a(t+k)的计算。为了将算法应用到线上的实时预测，我们可以将Holt-Winters算法拆分为两个独立的计算过程： 定时任务计算序列的周期数s(t)。 对残差序列做实时预测。 下面就分别从这两个步骤介绍外卖报警模型中的预测器实现。 3.4.1 计算序列的周期性数据时间序列的周期性数据不需要实时计算，按周期性更新即可，如外卖订单大盘监控，s(t)只需要每天更新一次即可。对于s(t)的计算，可以有多种方法，可以使用上面提到的Holt-Winters按公式(3-1)计算出时间序列的周期性数据（如图3.5b所示），或直接使用前一天的监控数据作为当天的周期数据（这两种方式都需要对输入序列进行预处理，保证算法的输入序列不含有异常数据）。也可以用上面3.2节提到的，将历史数据做平均求出基线作为序列的周期性数据。 目前外卖订单中心报警模型采用的是Holt-Winters计算周期数据的方式。在将该模型推广到外卖其他业务线监控时，使用了计算基线数据作为周期数据的方式，这里简单对比一下两种方式的优劣。 使用Holt-Winters算法计算周期数据 优点：如果序列中含有周期性的陡增陡降点，Holt-Winters计算出的周期数据中会保留这些陡增陡降趋势，因此可以准确的预测出这些趋势，不会产生误报。比如外卖订单的提单数据，在每天的某个时刻都有一个定期陡降，使用该方式可以正确的预测出下降的趋势。如图3.6所示，蓝色线是真实数据，棕色线是预测数据，在该时刻，棕色线准确的预测出了下降点。 缺点：需要对输入数据进行预处理，去除异常数据。如果输入序列中含有异常数据，使用Holt-Winters时可能会把这些异常数据计算到周期数据中，影响下一周期的预测从而产生误报（Holt-Winters理论上也只是滑动平均的过程，因此如果输入数据中含有比较大的异常数据时，存在这种可能性，实际应用中订单的报警模型也出现过这种误报）。 历史数据平均求基线 优点：计算出的周期数据比较平滑，不需要对输入序列进行预处理，计算过程中可以自动屏蔽掉异常数据的影响，计算过程简单，如图3.3所示的基线数据。缺点：周期数据比较平滑，不会出现陡增陡降点，因此对于周期性出现的陡增陡降不能很好的预测，出现误报。比如外卖活动的大盘（如图3.7所示，红线是真实数据，黑线是预测数据），提前下单优惠在每天某个时刻会出现周期性陡降，使用该方式会出现误报。 两种求周期数据的方式各有优劣，可以根据各自的监控数据特点选择合适的计算方式。如果监控数据中含有大量的周期性的陡增陡降点，那么推荐使用方式1，可以避免在这些时间点的误报。如果监控数据比较平滑，陡增陡降点很少，那么推荐方式2，计算简单的同时，也能避免因输入数据预处理不好而造成的意料之外的误报。 3.4.2 残差数据实时预测计算出周期数据后，下一个目标就是对残差数据的预测。使用下面的公式，实际监控数据与周期数据相减得到残差数据，对残差数据做一次滑动平均，预测出下一刻的残差，将该时刻的残差、周期数据相加即可得到该时刻的预测数据。残差序列的长度设为60，即可以得到比较准确的预测效果。 对于实时预测，使用的是当天的周期数据和前60分钟数据。最终的预测结果如图3.8(a)(b)所示，其中蓝色线是真实数据，红色线是预测数据。 四、比较器设计预测器预测出当前时刻订单量的预测值后，还需要与真实值比较来判断当前时刻订单量是否异常。一般的比较器都是通过阈值法，比如实际值超过预测值的一定比例就认为该点出现异常，进行报警。这种方式错误率比较大。在订单模型的报警检测中没有使用这种方式，而是使用了两个串联的Filter（如图4.1所示），只有当两个Fliter都认为该点异常时，才进行报警，下面简单介绍一下两个Filter的实现。 离散度Filter：根据预测误差曲线离散程度过滤出可能的异常点。一个序列的方差表示该序列离散的程度，方差越大，表明该序列波动越大。如果一个预测误差序列方差比较大，那么我们认为预测误差的报警阈值相对大一些才比较合理。离散度Filter利用了这一特性，取连续15分钟的预测误差序列，分为首尾两个序列（e1,e2），如果两个序列的均值差大于e1序列方差的某个倍数，我们就认为该点可能是异常点。 阈值Filter：根据误差绝对值是否超过某个阈值过滤出可能的异常点。利用离散度Filter进行过滤时，报警阈值随着误差序列波动程度变大而变大，但是在输入数据比较小时，误差序列方差比较小，报警阈值也很小，容易出现误报。所以设计了根据误差绝对值进行过滤的阈值Filter。阈值Filter设计了一个分段阈值函数y=f(x)，对于实际值x和预测值p，只有当|x-p|&gt;f(x)时报警。实际使用中，可以寻找一个对数函数替换分段阈值函数，更易于参数调优。 五、报警模型最终效果最终的外卖订单异常报警模型结构图如图5.1所示，每天会有定时Job从ETL中统计出最近10天的历史订单量，经过预处理模块，去除异常数据，经过周期数据计算模块得到周期性数据。对当前时刻预测时，取60分钟的真实数据和周期性数据，经过实时预测模块，预测出当前订单量。将连续15分钟的预测值和真实值通过比较器，判断当前时刻是否异常。 新的报警模型上线后，外卖订单量的异常检测的漏报率和误报率都有显著的提升，上线半年以来，对于每一次的异常都能准确的检测出来，漏报率近乎为0。误报率在通常情况下限制在了每周0~3次误报。 报警模型目前应用在外卖订单量的异常检测中，同时推广到了外卖业务的其他各种大盘监控中，取得了不错的效果。在报警模型上线后，我们发现并解决了一些系统隐患点，如： 点评侧外卖提单量在每天定时有一个下降尖刺，经过排查是因为客户端冷启动短时间内大量的请求，导致SLB性能达到瓶颈，从而导致接口成功率下降。 点评侧外卖订单取消量经常会有尖刺，经过排查发现是由于在支付时，需要进行跨机房的账号转换，专线网络抖动时造成接口超时。 外卖订单量在每天某些时刻都有陡降趋势，经过排查，是因为这些点大量商家开始休息导致的。 六、总结将机器学习中的预测算法运用到外卖订单的异常检测中，极大的提高了异常检测的准确性和敏感性，提升了系统稳定运维的效率。该报警模型也有很广泛的应用场景，美团点评的各个业务线的监控数据，绝大多数都是含有明显周期性的时间序列，本文提出的模型都能运用到这些监控数据的异常检测中。 当然，模型还有进一步完善的空间，如： 历史数据的预处理优化。在进行周期数据计算时，对于输入序列的预处理，如果能够排除绝大部分的异常数据，那么最终检测的误报率将会进一步的降低。 在不会产生持续误报的情况下替换有异常的实时数据。对于当前数据的预测，利用的都是前60分钟的真实数据，但是这些数据可能本身就存在异常数据，那么就存在一种情况，当出现异常时，真实数据开始下跌，预测数据紧接着也会下跌（如图3.8b所示）。这种情况有时候可能满足需求（比如只在异常开始的时候进行报警，异常持续时间内不再报警，防止报警太多造成的信息轰炸），有时候可能不满足需求（比如要求预测数据不跟随异常变化而变化，这种情况可以应用在故障期间的损失统计中）。如果需要预测值不随异常变化而变化，一种可能的方法是，当检测到当前数据是异常数据时，将预测数据替换当前的真实数据，作为下一时刻预测器的输入，这样可以防止异常数据对于下一时刻预测值的影响。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>异常检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（37）：外卖O2O的用户画像实践]]></title>
    <url>%2F2017%2F09%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8837%EF%BC%89%EF%BC%9A%E5%A4%96%E5%8D%96O2O%E7%9A%84%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[美团外卖经过3年的飞速发展，品类已经从单一的外卖扩展到了美食、夜宵、鲜花、商超等多个品类。用户群体也从早期的学生为主扩展到学生、白领、社区以及商旅，甚至包括在KTV等娱乐场所消费的人群。随着供给和消费人群的多样化，如何在供给和用户之间做一个对接，就是用户画像的一个基础工作。所谓千人千面，画像需要刻画不同人群的消费习惯和消费偏好。 外卖O2O和传统的电商存在一些差异。可以简单总结为如下几点： 1）新事物，快速发展：这意味很多用户对外卖的认知较少，对平台上的新品类缺乏了解，对自身的需求也没有充分意识。平台需要去发现用户的消费意愿，以便对用户的消费进行引导。 2）高频：外卖是个典型的高频O2O应用。一方面消费频次高，用户生命周期相对好判定；另一方面消费单价较低，用户决策时间短、随意性大。 3）场景驱动：场景是特定的时间、地点和人物的组合下的特定的消费意图。不同的时间、地点，不同类型的用户的消费意图会有差异。例如白领在写字楼中午的订单一般是工作餐，通常在营养、品质上有一定的要求，且单价不能太高；而到了周末晚上的订单大多是夜宵，追求口味且价格弹性较大。场景辨识越细致，越能了解用户的消费意图，运营效果就越好。 4）用户消费的地理位置相对固定，结合地理位置判断用户的消费意图是外卖的一个特点。 一、外卖产品运营对画像技术的要求如下图所示，我们大致可以把一个产品的运营分为用户获取和用户拓展两个阶段。在用户获取阶段，用户因为自然原因或一些营销事件（例如广告、社交媒体传播）产生对外卖的注意，进而产生了兴趣，并在合适的时机下完成首购，从而成为外卖新客。在这一阶段，运营的重点是提高效率，通过一些个性化的营销和广告手段，吸引到真正有潜在需求的用户，并刺激其转化。在用户完成转化后，接下来的运营重点是拓展用户价值。这里有两个问题：第一是提升用户价值，具体而言就是提升用户的单均价和消费频次，从而提升用户的LTV（life-time value)。基本手段包括交叉销售（新品类的推荐）、向上销售（优质高价供给的推荐）以及重复购买（优惠、红包刺激重复下单以及优质供给的推荐带来下单频次的提升）；第二个问题是用户的留存，通过提升用户总体体验以及在用户有流失倾向时通过促销和优惠将用户留在外卖平台。 所以用户所处的体验阶段不同，运营的侧重点也需要有所不同。而用户画像作为运营的支撑技术，需要提供相应的用户刻画以满足运营需求。根据上图的营销链条，从支撑运营的角度，除去提供常规的用户基础属性（例如年龄、性别、职业、婚育状况等）以及用户偏好之外，还需要考虑这么几个问题：1）什么样的用户会成为外卖平台的顾客（新客识别）；2）用户所处生命周期的判断，用户是否可能从平台流失（流失预警）；3）用户处于什么样的消费场景（场景识别）。后面“外卖O2O的用户画像实践”一节中，我们会介绍针对这三个问题的一些实践。 二、外卖画像系统架构下图是我们画像服务的架构：数据源包括基础日志、商家数据和订单数据。数据完成处理后存放在一系列主题表中，再导入kv存储，给下游业务端提供在线服务。同时我们会对整个业务流程实施监控。主要分为两部分，第一部分是对数据处理流程的监控，利用用内部自研的数据治理平台，监控每天各主题表产生的时间、数据量以及数据分布是否有异常。第二部分是对服务的监控。目前画像系统支持的下游服务包括：广告、排序、运营等系统。 三、外卖O2O的用户画像实践3.1 新客运营新客运营主要需要回答下列三个问题： 1）新客在哪里？ 2）新客的偏好如何？ 3）新客的消费力如何？ 回答这三个问题是比较困难的，因为相对于老客而言，新客的行为记录非常少或者几乎没有。这就需要我们通过一些技术手段作出推断。例如：新客的潜在转化概率，受到新客的人口属性（职业、年龄等）、所处地域（需求的因素）、周围人群（同样反映需求）以及是否有充足供给等因素的影响；而对于新客的偏好和消费力，从新客在到店场景下的消费行为可以做出推测。另外用户的工作和居住地点也能反映他的消费能力。对新客的预测大量依赖他在到店场景下的行为，而用户的到店行为对于外卖是比较稀疏的，大多数的用户是在少数几个类别上有过一些消费行为。这就意味着我们需要考虑选择什么样的统计量描述：是消费单价，总消费价格，消费品类等等。然后通过大量的试验来验证特征的显著性。另外由于数据比较稀疏，需要考虑合适的平滑处理。 我们在做高潜新客挖掘时，融入了多方特征，通过特征的组合最终作出一个效果比较好的预测模型。我们能够找到一些高转化率的用户，其转化率比普通用户高若干倍。通过对高潜用户有针对性的营销，可以极大提高营销效率。 3.2 流失预测新客来了之后，接下来需要把他留在这个平台上，尽量延长生命周期。营销领域关于用户留存的两个基本观点是（引自菲利普.科特勒 《营销管理》）： 获取一个新顾客的成本是维系现有顾客成本的5倍！ 如果将顾客流失率降低5%，公司利润将增加25%~85% 用户流失的原因通常包括：竞对的吸引；体验问题；需求变化。我们借助机器学习的方法，构建用户的描述特征，并借助这些特征来预测用户未来流失的概率。这里有两种做法: 第一种是预测用户未来若干天是否会下单这一事件发生的概率。这是典型的概率回归问题，可以选择逻辑回归、决策树等算法拟合给定观测下事件发生的概率；第二种是借助于生存模型，例如COX-PH模型，做流失的风险预测。下图左边是概率回归的模型，用户未来T天内是否有下单做为类别标记y，然后估计在观察到特征X的情况下y的后验概率P(y|X)。右边是用COX模型的例子，我们会根据用户在未来T天是否下单给样本一个类别，即观测时长记为T。假设用户的下单的距今时长t&lt;T，将t作为生存时长t’；否则将生存时长t’记为T。这样一个样本由三部分构成：样本的类别(flag)，生存时长(t’)以及特征列表。通过生存模型虽然无法显式得到P(t’|X)的概率，但其协变量部分实际反映了用户流失的风险大小。 生存模型中，βTx反映了用户流失的风险，同时也和用户下次订单的时间间隔成正相关。下面的箱线图中，横轴为βTx，纵轴为用户下单时间的间隔。 我们做了COX模型和概率回归模型的对比。在预测用户XX天内是否会下单上面，两者有相近的性能。 美团外卖通过使用了用户流失预警模型，显著降低了用户留存的运营成本。 3.3 场景运营拓展用户的体验，最重要的一点是要理解用户下单的场景。了解用户的订餐场景有助于基于场景的用户运营。对于场景运营而言，通常需要经过如下三个步骤： 场景可以从时间、地点、订单三个维度描述。比如说工作日的下午茶，周末的家庭聚餐，夜里在家点夜宵等等。其中重要的一点是用户订单地址的分析。通过区分用户的订单地址是写字楼、学校或是社区，再结合订单时间、订单内容，可以对用户的下单场景做到大致的了解。 上图是我们订单地址分析的流程。根据订单系统中的用户订单地址文本，基于自然语言处理技术对地址文本分析，可以得到地址的主干名称（指去掉了楼宇、门牌号的地址主干部分）和地址的类型（写字楼、住宅小区等）。在此基础上通过一些地图数据辅助从而判断出最终的地址类型。另外我们还做了合并订单的识别，即识别一个订单是一个人下单还是拼单。把拼单信息、地址分析以及时间结合在一起，我们可以预测用户的消费场景，进而基于场景做交叉销售和向上销售。 四、总结外卖的营销特征，跟其他行业的主要区别在于： 外卖是一个高频的业务。由于用户的消费频次高，用户生命周期的特征体现较显著。运营可以基于用户所处生命周期的阶段制定营销目标，例如用户完成首购后的频次提升、成熟用户的价值提升、衰退用户的挽留以及流失用户的召回等。因此用户的生命周期是一个基础画像，配合用户基本属性、偏好、消费能力、流失预测等其他画像，通过精准的产品推荐或者价格策略实现运营目标。 用户的消费受到时间、地点等场景因素驱动。因此需要对用户在不同的时间、地点下消费行为的差异做深入了解，归纳不同场景下用户需求的差异，针对场景制定相应的营销策略，提升用户活跃度。 另外由于外卖是一个新鲜的事物，在用户对一些新品类和新产品缺乏认知的情况下，需要通过技术手段识别用户的潜在需求，进行精准营销。例如哪些用户可能会对小龙虾、鲜花、蛋糕这样的相对低频、高价值的产品产生购买。可以采用的技术手段包括用户分群、对已产生消费的用户做look-alike扩展、迁移学习等。 同时我们在制作外卖的用户画像时还面临如下挑战： 1）数据多样性，存在大量非结构化数据例如用户地址、菜品名称等。需要用到自然语言处理技术，同时结合其他数据进行分析。 2）相对于综合电商而言，外卖是个相对单一的品类，用户在外卖上的行为不足以全方位地描述用户的基本属性。因此需要和用户在其他场合的消费行为做融合。 3）外卖单价相对较低，用户消费的决策时间短、随意性强。不像传统电商用户在决策前有大量的浏览行为可以用于捕捉用户单次的需求。因此更需要结合用户画像分析用户的历史兴趣、以及用户的消费场景，在消费前对用户做适当的引导、推荐。 面临这些挑战，需要用户画像团队更细致的数据处理、融合多方数据源，同时发展出新的方法论，才能更好地支持外卖业务发展的需要。而外卖的上述挑战，又分别和一些垂直领域电商类似，经验上存在可以相互借鉴之处。因此，外卖的用户画像的实践和经验累积，必将对整个电商领域的大数据应用作出新的贡献！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>用户画像</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（36）：GBDT算法原理深入解析]]></title>
    <url>%2F2017%2F09%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8836%EF%BC%89%EF%BC%9AGBDT%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[梯度提升（Gradient boosting）是一种用于回归、分类和排序任务的机器学习技术[1]，属于Boosting算法族的一部分。Boosting是一族可将弱学习器提升为强学习器的算法，属于集成学习（ensemble learning）的范畴。Boosting方法基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断要好。通俗地说，就是“三个臭皮匠顶个诸葛亮”的道理。梯度提升同其他boosting方法一样，通过集成（ensemble）多个弱学习器，通常是决策树，来构建最终的预测模型。 Boosting、bagging和stacking是集成学习的三种主要方法。不同于bagging方法，boosting方法通过分步迭代（stage-wise）的方式来构建模型，在迭代的每一步构建的弱学习器都是为了弥补已有模型的不足。Boosting族算法的著名代表是AdaBoost，AdaBoost算法通过给已有模型预测错误的样本更高的权重，使得先前的学习器做错的训练样本在后续受到更多的关注的方式来弥补已有模型的不足。与AdaBoost算法不同，梯度提升方法在迭代的每一步构建一个能够沿着梯度最陡的方向降低损失（steepest-descent）的学习器来弥补已有模型的不足。经典的AdaBoost算法只能处理采用指数损失函数的二分类学习任务[2]，而梯度提升方法通过设置不同的可微损失函数可以处理各类学习任务（多分类、回归、Ranking等），应用范围大大扩展。另一方面，AdaBoost算法对异常点（outlier）比较敏感，而梯度提升算法通过引入bagging思想、加入正则项等方法能够有效地抵御训练数据中的噪音，具有更好的健壮性。这也是为什么梯度提升算法（尤其是采用决策树作为弱学习器的GBDT算法）如此流行的原因，有种观点认为GBDT是性能最好的机器学习算法，这当然有点过于激进又固步自封的味道，但通常各类机器学习算法比赛的赢家们都非常青睐GBDT算法，由此可见该算法的实力不可小觑。 基于梯度提升算法的学习器叫做GBM(Gradient Boosting Machine)。理论上，GBM可以选择各种不同的学习算法作为基学习器。现实中，用得最多的基学习器是决策树。为什么梯度提升方法倾向于选择决策树（通常是CART树）作为基学习器呢？这与决策树算法自身的优点有很大的关系。决策树可以认为是if-then规则的集合，易于理解，可解释性强，预测速度快。同时，决策树算法相比于其他的算法需要更少的特征工程，比如可以不用做特征标准化，可以很好的处理字段缺失的数据，也可以不用关心特征间是否相互依赖等。决策树能够自动组合多个特征，它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）不过，单独使用决策树算法时，有容易过拟合缺点。所幸的是，通过各种方法，抑制决策树的复杂性，降低单颗决策树的拟合能力，再通过梯度提升的方法集成多个决策树，最终能够很好的解决过拟合的问题。由此可见，梯度提升方法和决策树学习算法可以互相取长补短，是一对完美的搭档。至于抑制单颗决策树的复杂度的方法有很多，比如限制树的最大深度、限制叶子节点的最少样本数量、限制节点分裂时的最少样本数量、吸收bagging的思想对训练样本采样（subsample），在学习单颗决策树时只使用一部分训练样本、借鉴随机森林的思路在学习单颗决策树时只采样一部分特征、在目标函数中添加正则项惩罚复杂的树结构等。现在主流的GBDT算法实现中这些方法基本上都有实现，因此GBDT算法的超参数还是比较多的，应用过程中需要精心调参，并用交叉验证的方法选择最佳参数。 本文对GBDT算法原理进行介绍，从机器学习的关键元素出发，一步一步推导出GBDT算法背后的理论基础，读者可以从这个过程中了解到GBDT算法的来龙去脉。对于该算法的工程实现，本文也有较好的指导意义，实际上对机器学习关键概念元素的区分对应了软件工程中的“开放封闭原则”的思想，基于此思想的实现将会具有很好的模块独立性和扩展性。 一、机器学习的关键元素先复习下监督学习的关键概念：模型（model）、参数（parameters）、目标函数（objective function） 模型就是所要学习的条件概率分布或者决策函数，它决定了在给定特征向量x时如何预测出目标y。定义$x_i\in R^d$为训练集中的第$i$个训练样本，则线性模型（linear model）可以表示为：$\hat{y}=\sum_jw_jx_{ij }$。模型预测的分数$\hat{y_i}$在不同的任务中有不同的解释。例如在逻辑回归任务中，$1/(1+exp(-\hat{y}_i))$表示模型预测为正例的概率；而在排序学习任务中，$\hat{y_i}$表示排序分。 参数就是我们要从数据中学习得到的内容。模型通常是由一个参数向量决定的函数。例如，线性模型的参数可以表示为：$\Theta=\{w_j|j=1,\cdots,d\}$ 目标函数通常定义为如下形式： Obj(\Theta)=L(\Theta)+\Omega(\Theta)其中，$L(\Theta)$是损失函数，用来衡量模型拟合训练数据的好坏程度；$\Omega(\Theta)$称之为正则项，用来衡量学习到的模型的复杂度。训练集上的损失（Loss）定义为：$L=\sum_{i=1}^n l(y_i, \hat{y}_i)$。常用的损失函数有平方损失（square loss）：$l(y_i, \hat{y}_i)=(y_i - \hat{y}_i)^2$；Logistic损失： $l(y_i, \hat{y}_i)=y_i ln(1+e^{y_i}) + (1-y_i)ln(1+e^{\hat{y}_i})$。常用的正则项有L1范数$\Omega(w)=\lambda \Vert w \Vert_1$和L2范数$\Omega(w)=\lambda \Vert w \Vert_2$。Ridge regression就是指使用平方损失和L2范数正则项的线性回归模型；Lasso regression就是指使用平方损失和L1范数正则项的线性回归模型；逻辑回归（Logistic regression）指使用logistic损失和L2范数或L1范数正则项的线性模型。 目标函数之所以定义为损失函数和正则项两部分，是为了尽可能平衡模型的偏差和方差（Bias Variance Trade-off）。最小化目标函数意味着同时最小化损失函数和正则项，损失函数最小化表明模型能够较好的拟合训练数据，一般也预示着模型能够较好地拟合真实数据（groud true）；另一方面，对正则项的优化鼓励算法学习到较简单的模型，简单模型一般在测试样本上的预测结果比较稳定、方差较小（奥坎姆剃刀原则）。也就是说，优化损失函数尽量使模型走出欠拟合的状态，优化正则项尽量使模型避免过拟合。 从概念上区分模型、参数和目标函数给学习算法的工程实现带来了益处，使得机器学习的各个组成部分之间耦合尽量松散。 二、加法模型GBDT算法可以看成是由K棵树组成的加法模型： \hat{y}_i=\sum_{k=1}^K f_k(x_i), f_k \in F \tag 0其中$F$为所有树组成的函数空间，以回归任务为例，回归树可以看作为一个把特征向量映射为某个score的函数。该模型的参数为：$\Theta=\{f_1,f_2, \cdots, f_K \}$。于一般的机器学习算法不同的是，加法模型不是学习d维空间中的权重，而是直接学习函数（决策树）集合。 上述加法模型的目标函数定义为：$Obj=\sum_{i=1}^n l(y_i, \hat{y}_i) + \sum_{k=1}^K \Omega(f_k)$，其中表示决策树的复杂度，那么该如何定义树的复杂度呢？比如，可以考虑树的节点数量、树的深度或者叶子节点所对应的分数的L2范数等等。 如何来学习加法模型呢？ 解这一优化问题，可以用前向分布算法（forward stagewise algorithm）。因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数（结构），逐步逼近优化目标函数，那么就可以简化复杂度。这一学习过程称之为Boosting。具体地，我们从一个常量预测开始，每次学习一个新的函数，过程如下： \begin{split} \hat{y}_i^0 &= 0 \\ \hat{y}_i^1 &= f_1(x_i) = \hat{y}_i^0 + f_1(x_i) \\ \hat{y}_i^2 &= f_1(x_i) + f_2(x_i) = \hat{y}_i^1 + f_2(x_i) \\ & \cdots \\ \hat{y}_i^t &= \sum_{k=1}^t f_k(x_i) = \hat{y}_i^{t-1} + f_t(x_i) \\ \end{split}那么，在每一步如何决定哪一个函数$f$被加入呢？指导原则还是最小化目标函数。在第$t$步，模型对$x_i$的预测为：$\hat{y}_i^t= \hat{y}_i^{t-1} + f_t(x_i)$，其中$f_t(x_i)$为这一轮我们要学习的函数（决策树）。这个时候目标函数可以写为： \begin{split} Obj^{(t)} &= \sum_{i=1}^nl(y_i, \hat{y}_i^t) + \sum_{i=i}^t \Omega(f_i) \\ &= \sum_{i=1}^n l\left(y_i, \hat{y}_i^{t-1} + f_t(x_i) \right) + \Omega(f_t) + constant \end{split}\tag{1}举例说明，假设损失函数为平方损失（square loss），则目标函数为： \begin{split} Obj^{(t)} &= \sum_{i=1}^n \left(y_i - (\hat{y}_i^{t-1} + f_t(x_i)) \right)^2 + \Omega(f_t) + constant \\ &= \sum_{i=1}^n \left[2(\hat{y}_i^{t-1} - y_i)f_t(x_i) + f_t(x_i)^2 \right] + \Omega(f_t) + constant \end{split}\tag{2}其中$(\hat{y}_i^{t-1} - y_i)$，称之为残差（residual）。因此，使用平方损失函数时，GBDT算法的每一步在生成决策树时只需要拟合前面的模型的残差。 泰勒公式：设$n$是一个正整数，如果定义在一个包含$a$的区间上的函数$f$在$a$点处$n+1$次可导，那么对于这个区间上的任意$x$都有： \displaystyle f(x)=\sum _{n=0}^{N}\frac{f^{(n)}(a)}{n!}(x-a)^ n+R_ n(x)，其中的多项式称为函数在$a$处的泰勒展开式，$R_n(x)$是泰勒公式的余项且是$(x-a)^n$的高阶无穷小。——维基百科 根据泰勒公式把函数$f(x+\Delta x)$在点处二阶展开，可得到如下等式： f(x+\Delta x) \approx f(x) + f'(x)\Delta x + \frac12 f''(x)\Delta x^2 \tag 3由等式(1)可知，目标函数是关于变量$\hat{y}_i^{t-1} + f_t(x_i)$若把变量$\hat{y}_i^{t-1}$看成是等式(3)中的$x$，把变量$f_t(x_i)$看成是等式(3)中的$\Delta x$，则等式(1)可转化为： Obj^{(t)} = \sum_{i=1}^n \left[ l(y_i, \hat{y}_i^{t-1}) + g_if_t(x_i) + \frac12h_if_t^2(x_i) \right] + \Omega(f_t) + constant \tag 4其中，$g_i$定义为损失函数的一阶导数，即$g_i=\partial_{\hat{y}^{t-1}}l(y_i,\hat{y}^{t-1})$；$h_i$定义为损失函数的二阶导数，即$h_i=\partial_{\hat{y}^{t-1}}^2l(y_i,\hat{y}^{t-1})$。 假设损失函数为平方损失函数，则$g_i=\partial_{\hat{y}^{t-1}}(\hat{y}^{t-1} - y_i)^2 = 2(\hat{y}^{t-1} - y_i)$，$h_i=\partial_{\hat{y}^{t-1}}^2(\hat{y}^{t-1} - y_i)^2 = 2$，把$g_i$和$h_i$代入等式(4)即得等式(2)。由于函数中的常量在函数最小化的过程中不起作用，因此我们可以从等式(4)中移除掉常量项，得： Obj^{(t)} \approx \sum_{i=1}^n \left[ g_if_t(x_i) + \frac12h_if_t^2(x_i) \right] + \Omega(f_t) \tag 5由于要学习的函数仅仅依赖于目标函数，从等式(5)可以看出只需为学习任务定义好损失函数，并为每个训练样本计算出损失函数的一阶导数和二阶导数，通过在训练样本集上最小化等式(5)即可求得每步要学习的函数$f(x)$，从而根据加法模型等式(0)可得最终要学习的模型。 二、GBDT算法一颗生成好的决策树，假设其叶子节点个数为$T$，该决策树是由所有叶子节点对应的值组成的向量$w \in R^T$，以及一个把特征向量映射到叶子节点索引（Index）的函数$q:R^d \to \{1,2,\cdots,T\}$组成的。因此，决策树可以定义为$f_t(x)=w_{q(x)}$。 决策树的复杂度可以由正则项$\Omega(f_t)=\gamma T + \frac12 \lambda \sum_{j=1}^T w_j^2$来定义，即决策树模型的复杂度由生成的树的叶子节点数量和叶子节点对应的值向量的L2范数决定。 定义集合$I_j=\{ i \vert q(x_i)=j \}$为所有被划分到叶子节点的训练样本的集合。等式(5)可以根据树的叶子节点重新组织为T个独立的二次函数的和： \begin{split} Obj^{(t)} &\approx \sum_{i=1}^n \left[ g_if_t(x_i) + \frac12h_if_t^2(x_i) \right] + \Omega(f_t) \\ &= \sum_{i=1}^n \left[ g_iw_{q(x_i)} + \frac12h_iw_{q(x_i)}^2 \right] + \gamma T + \frac12 \lambda \sum_{j=1}^T w_j^2 \\ &= \sum_{j=1}^T \left[(\sum_{i \in I_j}g_i)w_j + \frac12(\sum_{i \in I_j}h_i + \lambda)w_j^2 \right] + \gamma T \end{split}\tag 6定义$G_j=\sum_{i \in I_j}g_i$，$H_j=\sum_{i \in I_j}h_i$，则等式(6)可写为： Obj^{(t)} = \sum_{j=1}^T \left[G_iw_j + \frac12(H_i + \lambda)w_j^2 \right] + \gamma T假设树的结构是固定的，即函数$q(x)$确定，令函数$Obj^{(t)}$的一阶导数等于0，即可求得叶子节点对应的值为：$w_j^*=-\frac{G_j}{H_j+\lambda} \tag 7$此时，目标函数的值为 Obj = -\frac12 \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T \tag 8综上，为了便于理解，单颗决策树的学习过程可以大致描述为： 枚举所有可能的树结构$q$ 用等式(8)为每个$q$计算其对应的分数$Obj$，分数越小说明对应的树结构越好。 根据上一步的结果，找到最佳的树结构，用等式(7)为树的每个叶子节点计算预测值 然而，可能的树结构数量是无穷的，所以实际上我们不可能枚举所有可能的树结构。通常情况下，我们采用贪心策略来生成决策树的每个节点。 从深度为0的树开始，对每个叶节点枚举所有的可用特征 针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益） 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集 回到第1步，递归执行到满足特定条件为止 在上述算法的第二步，样本排序的时间复杂度为$O(nlogn)$，假设共用K个特征，那么生成一颗深度为K的树的时间复杂度为$O(dKnlogn)$。具体实现可以进一步优化计算复杂度，比如可以缓存每个特征的排序结果等。 如何计算每次分裂的收益呢？假设当前节点记为,分裂之后左孩子节点记为，右孩子节点记为，则该分裂获得的收益定义为当前节点的目标函数值减去左右两个孩子节点的目标函数值之和：$Gain=Obj_C-Obj_L-Obj_R$，具体地，根据等式(8)可得： Gain=\frac12 \left[ \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma \tag 9其中，$-\gamma$项表示因为增加了树的复杂性（该分裂增加了一个叶子节点）带来的惩罚。等式(9)还可以用来计算输入特征的相对重要程度，具体见下一节 最后，总结一下GBDT的学习算法： 算法每次迭代生成一颗新的决策树 在每次迭代开始之前，计算损失函数在每个训练样本点的一阶导数$g_i$和二阶导数$h_i$ 通过贪心策略生成新的决策树，通过等式(7)计算每个叶节点对应的预测值 把新生成的决策树$f_t(x)$添加到模型中：$\hat{y}_i^t = \hat{y}_i^{t-1} + f_t(x_i)$ 通常在第四步，我们把模型更新公式替换为：$\hat{y}_i^t = \hat{y}_i^{t-1} + \epsilon f_t(x_i)$，其中$\epsilon$称之为步长或者学习率。增加因子的目的是为了避免模型过拟合。 三、特征重要度集成学习因具有预测精度高的优势而受到广泛关注，尤其是使用决策树作为基学习器的集成学习算法。树的集成算法的著名代码有随机森林和GBDT。随机森林具有很好的抵抗过拟合的特性，并且参数（决策树的个数）对预测性能的影响较小，调参比较容易，一般设置一个比较大的数。GBDT具有很优美的理论基础，一般而言性能更有优势。 基于树的集成算法还有一个很好的特性，就是模型训练结束后可以输出模型所使用的特征的相对重要度，便于我们选择特征，理解哪些因素是对预测有关键影响，这在某些领域（如生物信息学、神经系统科学等）特别重要。本文主要介绍基于树的集成算法如何计算各特征的相对重要度。 3.1 优势 使用不同类型的数据时，不需要做特征标准化/归一化 可以很容易平衡运行时效率和精度；比如，使用boosted tree作为在线预测的模型可以在机器资源紧张的时候截断参与预测的树的数量从而提高预测效率 学习模型可以输出特征的相对重要程度，可以作为一种特征选择的方法 模型可解释性好 对数据字段缺失不敏感 能够自动做多组特征间的interaction，具有很好的非性线性 3.2 特征重要度的计算Friedman在GBM的论文中提出的方法： 特征$j$的全局重要度通过特征$j$在单颗树中的重要度的平均值来衡量： \hat{J_{j}^2}=\frac1M \sum_{m=1}^M\hat{J_{j}^2}(T_m)其中，M是树的数量。特征$j$在单颗树中的重要度的如下： \hat{J_{j}^2}(T)=\sum\limits_{t=1}^{L-1} \hat{i_{t}^2} 1(v_{t}=j)其中，$L$为树的叶子节点数量，$L-1$即为树的非叶子节点数量（构建的树都是具有左右孩子的二叉树），是和节点$t$相关联的特征，$\hat{i_t^2}$是节点分裂之后平方损失的减少值。 3.3 实现代码为了更好的理解特征重要度的计算方法，下面给出scikit-learn工具包中的实现，代码移除了一些不相关的部分。 下面的代码来自于GradientBoostingClassifier对象的feature_importances属性的计算方法： 123456def feature_importances_(self): total_sum = np.zeros((self.n_features, ), dtype=np.float64) for tree in self.estimators_: total_sum += tree.feature_importances_ importances = total_sum / len(self.estimators_) return importances 其中，self.estimators_是算法构建出的决策树的数量，tree.feature_importances_ 是单棵树的特征重要度向量，其计算方法如下： 1234567891011121314cpdef compute_feature_importances(self, normalize=True): &quot;&quot;&quot;Computes the importance of each feature (aka variable).&quot;&quot;&quot; while node != end_node: if node.left_child != _TREE_LEAF: # ... and node.right_child != _TREE_LEAF: left = &amp;nodes[node.left_child] right = &amp;nodes[node.right_child] importance_data[node.feature] += ( node.weighted_n_node_samples * node.impurity - left.weighted_n_node_samples * left.impurity - right.weighted_n_node_samples * right.impurity) node += 1 importances /= nodes[0].weighted_n_node_samples return importances 上面的代码关键点是两个： 第一点：weighted_n_node_samples : array of int, shape [node_count] weighted_n_node_samples[i] holds the weighted number of training samples reaching node i. 第二点：impurity : array of double, shape [node_count] impurity[i] holds the impurity (i.e., the value of the splitting criterion) at node i. 当然上面的代码经过了简化，保留了核心思想。计算所有的非叶子节点在分裂时加权不纯度的减少，减少得越多说明特征越重要 不纯度的减少实际上就是该节点此次分裂的收益，因此我们也可以这样理解，节点分裂时收益越大，该节点对应的特征的重要度越高。关于收益的定义就是上一节中等式(9)的定义。 参考资料[1] Gradient Boosting 的更多内容[2] XGBoost是一个优秀的GBDT开源软件库，有多种语言接口[3] Pyramid是一个基于Java语言的机器学习库，里面也有GBDT算法的介绍和实现[4] Friedman的论文《Greedy function approximation: a gradient boosting machine》是比较早的GBDT算法文献，但是比较晦涩难懂，不适合初学者，高阶选手可以进一步学习[5] “A Gentle Introduction to Gradient Boosting”是关于Gradient Boosting的一个通俗易懂的解释，比较适合初学者或者是已经对GBDT算法原理印象不深的从业者[6] 关于GBDT算法调参的经验和技巧可以参考这两篇博文：《GBM调参指南》、《XGBoost调参指南》，作者使用的算法实现工具来自于著名的Python机器学习工具scikit-learn[7] GBDT算法在搜索引擎排序中的应用可以查看这篇论文《Web-Search Ranking with Initialized Gradient Boosted Regression Trees 》，这篇论文提出了一个非常有意思的方法，用一个已经训练好的随机森林模型作为GBDT算法的初始化，再用GBDT算法优化最终的模型，取得了很好的效果 [1] Feature Selection for Ranking using Boosted Trees [2] Gradient Boosted Feature Selection[3] Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（35）：使用Sklearn进行集成学习（实践）]]></title>
    <url>%2F2017%2F09%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8835%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Sklearn%E8%BF%9B%E8%A1%8C%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%AE%9E%E8%B7%B5%EF%BC%89%2F</url>
    <content type="text"><![CDATA[jjjhhh]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（34）：使用Sklearn进行集成学习（理论）]]></title>
    <url>%2F2017%2F09%2F02%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8834%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Sklearn%E8%BF%9B%E8%A1%8C%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、前言很多人在竞赛（Kaggle，天池等）或工程实践中使用了集成学习（例如，RF、GTB等），确实也取得了不错的效果，在保证准确度的同时也提升了模型防止过拟合的能力。但是，我们真的用对了集成学习吗？ sklearn提供了sklearn.ensemble库，支持众多集成学习算法和模型。恐怕大多数人使用这些工具时，要么使用默认参数，要么根据模型在测试集上的性能试探性地进行调参（当然，完全不懂的参数还是不动算了），要么将调参的工作丢给调参算法（网格搜索等）。这样并不能真正地称为“会”用sklearn进行集成学习。 我认为，学会调参是进行集成学习工作的前提。然而，第一次遇到这些算法和模型时，肯定会被其丰富的参数所吓到，要知道，教材上教的伪代码可没这么多参数啊！！！没关系，暂时，我们只要记住一句话：参数可分为两种，一种是影响模型在训练集上的准确度或影响防止过拟合能力的参数；另一种不影响这两者的其他参数。模型在样本总体上的准确度（后简称准确度）由其在训练集上的准确度及其防止过拟合的能力所共同决定，所以在调参时，我们主要对第一种参数进行调整，最终达到的效果是：模型在训练集上的准确度和防止过拟合能力的大和谐！ 本篇博文将详细阐述模型参数背后的理论知识，在下篇博文中，我们将对最热门的两个模型Random Forrest和Gradient Tree Boosting（含分类和回归，所以共4个模型）进行具体的参数讲解。如果你实在无法静下心来学习理论，你也可以在下篇博文中找到最直接的调参指导，虽然我不赞同这么做。 二、集成学习是什么？我们还是花一点时间来说明一下集成学习是什么，如果对此有一定基础的同学可以跳过本节。简单来说，集成学习是一种技术框架，其按照不同的思路来组合基础模型，从而达到其利断金的目的。 目前，有三种常见的集成学习框架：bagging，boosting和stacking。国内，南京大学的周志华教授对集成学习有很深入的研究，其在09年发表的一篇概述性论文《Ensemble Learning》对这三种集成学习框架有了明确的定义，概括如下： bagging：从训练集从进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果： boosting：训练过程为阶梯状，基模型按次序一一进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化。对所有基模型预测的结果进行线性综合产生最终的预测结果： stacking：将训练好的所有基模型对训练基进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测： 有了这些基本概念之后，直觉将告诉我们，由于不再是单一的模型进行预测，所以模型有了“集思广益”的能力，也就不容易产生过拟合现象。但是，直觉是不可靠的，接下来我们将从模型的偏差和方差入手，彻底搞清楚这一问题。 三、偏差和方差广义的偏差（bias）描述的是预测值和真实值之间的差异，方差（variance）描述距的是预测值作为随机变量的离散程度。《Understanding the Bias-Variance Tradeoff》当中有一副图形象地向我们展示了偏差和方差的关系： 3.1 模型的偏差和方差模型的偏差是一个相对来说简单的概念：训练出来的模型在训练集上的准确度。 定义随机变量的值的差异是计算方差的前提条件，通常来说，我们遇到的都是数值型的随机变量，数值之间的差异再明显不过（减法运算）。但是，模型的差异性呢？我们可以理解模型的差异性为模型的结构差异，例如：线性模型中权值向量的差异，树模型中树的结构差异等。在研究模型方差的问题上，我们并不需要对方差进行定量计算，只需要知道其概念即可。 研究模型的方差有什么现实的意义呢？我们认为方差越大的模型越容易过拟合：假设有两个训练集A和B，经过A训练的模型Fa与经过B训练的模型Fb差异很大，这意味着Fa在类A的样本集合上有更好的性能，而Fb反之，这便是我们所说的过拟合现象。 我们常说集成学习框架中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型。但是，并不是所有集成学习框架中的基模型都是弱模型。bagging和stacking中的基模型为强模型（偏差低方差高），boosting中的基模型为弱模型。 在bagging和boosting框架中，通过计算基模型的期望和方差，我们可以得到模型整体的期望和方差。为了简化模型，我们假设基模型的权重、方差及两两间的相关系数相等。由于bagging和boosting的基模型都是线性组成的，那么有： 3.2 bagging的偏差和方差对于bagging来说，每个基模型的权重等于1/m且期望近似相等（子训练集都是从原训练集中进行子抽样），故我们可以进一步化简得到： 根据上式我们可以看到，整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似。同时，整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。但是，模型的准确度一定会无限逼近于1吗？并不一定，当基模型数增加到一定程度时，方差公式第二项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。另外，在此我们还知道了为什么bagging中的基模型一定要为强模型，否则就会导致整体模型的偏差度低，即准确度低。 Random Forest是典型的基于bagging框架的模型，其在bagging的基础上，进一步降低了模型的方差。Random Fores中基模型是树模型，在树的内部节点分裂过程中，不再是将所有特征，而是随机抽样一部分特征纳入分裂的候选项。这样一来，基模型之间的相关性降低，从而在方差公式中，第一项显著减少，第二项稍微增加，整体方差仍是减少。 3.3 boosting的偏差和方差对于boosting来说，基模型的训练集抽样是强相关的，那么模型的相关系数近似等于1，故我们也可以针对boosting化简公式为： 通过观察整体方差的表达式，我们容易发现，若基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，boosting框架中的基模型必须为弱模型。 因为基模型为弱模型，导致了每个基模型的准确度都不是很高（因为其在训练集上的准确度不高）。随着基模型数的增多，整体模型的期望值增加，更接近真实值，因此，整体模型的准确度提高。但是准确度一定会无限逼近于1吗？仍然并不一定，因为训练过程中准确度的提高的主要功臣是整体模型在训练集上的准确度提高，而随着训练的进行，整体模型的方差变大，导致防止过拟合的能力变弱，最终导致了准确度反而有所下降。 基于boosting框架的Gradient Tree Boosting模型中基模型也为树模型，同Random Forrest，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。 3.4 模型的独立性聪明的读者这时肯定要问了，如何衡量基模型的独立性？我们说过，抽样的随机性决定了模型的随机性，如果两个模型的训练集抽样过程不独立，则两个模型则不独立。这时便有一个天大的陷阱在等着我们：bagging中基模型的训练样本都是独立的随机抽样，但是基模型却不独立呢？ 我们讨论模型的随机性时，抽样是针对于样本的整体。而bagging中的抽样是针对于训练集（整体的子集），所以并不能称其为对整体的独立随机抽样。那么到底bagging中基模型的相关性体现在哪呢？在知乎问答《为什么说bagging是减少variance，而boosting是减少bias?》中请教用户“过拟合”后，我总结bagging的抽样为两个过程： 样本抽样：整体模型F(X1, X2, …, Xn)中各输入随机变量（X1, X2, …, Xn）对样本的抽样 子抽样：从整体模型F(X1, X2, …, Xn)中随机抽取若干输入随机变量成为基模型的输入随机变量 假若在子抽样的过程中，两个基模型抽取的输入随机变量有一定的重合，那么这两个基模型对整体样本的抽样将不再独立，这时基模型之间便具有了相关性。 3.5 小结还记得调参的目标吗：模型在训练集上的准确度和防止过拟合能力的大和谐！为此，我们目前做了一些什么工作呢？ 使用模型的偏差和方差来描述其在训练集上的准确度和防止过拟合的能力 对于bagging来说，整体模型的偏差和基模型近似，随着训练的进行，整体模型的方差降低 对于boosting来说，整体模型的初始偏差较高，方差较低，随着训练的进行，整体模型的偏差降低（虽然也不幸地伴随着方差增高），当训练过度时，因方差增高，整体模型的准确度反而降低 整体模型的偏差和方差与基模型的偏差和方差息息相关 这下总算有点开朗了，那些让我们抓狂的参数，现在可以粗略地分为两类了：控制整体训练过程的参数和基模型的参数，这两类参数都在影响着模型在训练集上的准确度以及防止过拟合的能力。 四、Gradient Boosting对基于Gradient Boosting框架的模型的进行调试时，我们会遇到一个重要的概念：损失函数。在本节中，我们将把损失函数的“今生来世”讲个清楚！ 基于boosting框架的整体模型可以用线性组成式来描述，其中$hi$为基模型与其权值的乘积： 根据上式，整体模型的训练目标是使预测值F(x)逼近真实值y，也就是说要让每一个基模型的预测值逼近各自要预测的部分真实值。由于要同时考虑所有基模型，导致了整体模型的训练变成了一个非常复杂的问题。所以，研究者们想到了一个贪心的解决手段：每次只训练一个基模型。那么，现在改写整体模型为迭代式： 这样一来，每一轮迭代中，只要集中解决一个基模型的训练问题：使$Fi$逼近真实值y。 4.1 拟合残差使Fi逼近真实值，其实就是使hi逼近真实值和上一轮迭代的预测值Fi-1之差，即残差（y-Fi-1）。最直接的做法是构建基模型来拟合残差，在博文《GBDT（MART） 迭代决策树入门教程 | 简介》中，作者举了一个生动的例子来说明通过基模型拟合残差，最终达到整体模型F(x)逼近真实值。 研究者发现，残差其实是最小均方损失函数的关于预测值的反向梯度： 也就是说，若$Fi-1$加上拟合了反向梯度的$hi$得到$Fi$，该值可能将导致平方差损失函数降低，预测的准确度提高！这显然不是巧合，但是研究者们野心更大，希望能够创造出一种对任意损失函数都可行的训练方法，那么仅仅拟合残差是不恰当的了。 4.2 拟合反向梯度4.2.1 契机：引入任意损失函数引入任意损失函数后，我们可以定义整体模型的迭代式如下： 在这里，损失函数被定义为泛函。 4.2.2 难题一：任意损失函数的最优化对任意损失函数（且是泛函）的最优化是困难的。我们需要打破思维的枷锁，将整体损失函数L’定义为n元普通函数（n为样本容量），损失函数L定义为2元普通函数（记住！！！这里的损失函数不再是泛函！！！）： 我们不妨使用梯度最速下降法来解决整体损失函数L’最小化的问题，先求整体损失函数的反向梯度： 假设已知样本x的当前预测值为$Fi-1$，下一步将预测值按照反向梯度，依照步长为$r[i]$，进行更新： 步长r[i]不是固定值，而是设计为： 4.2.3 难题二：无法对测试样本计算反向梯度问题又来了，由于测试样本中y是未知的，所以无法求反向梯度。这正是Gradient Boosting框架中的基模型闪亮登场的时刻！在第i轮迭代中，我们创建训练集如下： 也就是说，让基模型拟合反向梯度函数，这样我们就可以做到只输入x这一个参数，就可求出其对应的反向梯度了（当然，通过基模型预测出来的反向梯度并不是准确的，这也提供了泛化整体模型的机会）。 综上，假设第i轮迭代中，根据新训练集训练出来的基模型为$fi$，那么最终的迭代公式为： 4.3 常见的损失函数ls：最小均方回归中用到的损失函数。在之前我们已经谈到，从拟合残差的角度来说，残差即是该损失函数的反向梯度值（所以又称反向梯度为伪残差）。不同的是，从拟合残差的角度来说，步长是无意义的。该损失函数是sklearn中Gradient Tree Boosting回归模型默认的损失函数。 deviance：逻辑回归中用到的损失函数。熟悉逻辑回归的读者肯定还记得，逻辑回归本质是求极大似然解，其认为样本服从几何分布，样本属于某类别的概率可以logistic函数表达。所以，如果该损失函数可用在多类别的分类问题上，故其是sklearn中Gradient Tree Boosting分类模型默认的损失函数。 exponential：指数损失函数，表达式为： 对该损失函数求反向梯度得： 这时，在第i轮迭代中，新训练集如下：脑袋里有什么东西浮出水面了吧？让我们看看Adaboost算法中，第i轮迭代中第j个样本权值的更新公式：样本的权值什么时候会用到呢？计算第i轮损失函数的时候会用到：让我们再回过头来，看看使用指数损失函数的Gradient Boosting计算第i轮损失函数： 天呐，两个公式就差了一个对权值的归一项。这并不是巧合，当损失函数是指数损失时，Gradient Boosting相当于二分类的Adaboost算法。是的，指数损失仅能用于二分类的情况。 4.4 步子太大容易扯着蛋：缩减缩减也是一个相对显见的概念，也就是说使用Gradient Boosting时，每次学习的步长缩减一点。这有什么好处呢？缩减思想认为每次走一小步，多走几次，更容易逼近真实值。如果步子迈大了，使用最速下降法时，容易迈过最优点。将缩减代入迭代公式： 缩减需要配合基模型数一起使用，当缩减率v降低时，基模型数要配合增大，这样才能提高模型的准确度。 4.5 初始模型还有一个不那么起眼的问题，初始模型$F0$是什么呢？如果没有定义初始模型，整体模型的迭代式一刻都无法进行！所以，我们定义初始模型为： 根据上式可知，对于不同的损失函数来说，初始模型也是不一样的。对所有的样本来说，根据初始模型预测出来的值都一样。 4.5 Gradient Tree Boosting终于到了备受欢迎的Gradient Tree Boosting模型了！但是，可讲的却已经不多了。我们已经知道了该模型的基模型是树模型，并且可以通过对特征的随机抽样进一步减少整体模型的方差。我们可以在维基百科的Gradient Boosting词条中找到其伪代码实现。 4.6 小结到此，读者应当很清楚Gradient Boosting中的损失函数有什么意义了。要说偏差描述了模型在训练集准确度，则损失函数则是描述该准确度的间接量纲。也就是说，模型采用不同的损失函数，其训练过程会朝着不同的方向进行！ 五、总结 磨刀不误砍柴功，我们花了这么多时间来学习必要的理论，我强调一次：必要的理论！集成学习模型的调参工作的核心就是找到合适的参数，能够使整体模型在训练集上的准确度和防止过拟合的能力达到协调，从而达到在样本总体上的最佳准确度。有了本文的理论知识铺垫，在下篇中，我们将对Random Forest和Gradient Tree Boosting中的每个参数进行详细阐述，同时也有一些小试验证明我们的结论。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Scikit-Learn</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（33）：特征处理（Feature Processing）]]></title>
    <url>%2F2017%2F08%2F31%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8833%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86%EF%BC%88Feature%20Processing%EF%BC%89%2F</url>
    <content type="text"><![CDATA[特征工程（Feature Engineering）经常被说为机器学习中的black art，这里面包含了很多不可言说的方面。怎么处理好特征，最重要的当然还是对要解决问题的了解。但是，它其实也有很多科学的地方。这篇文章我之所以命名为特征处理（Feature Processing），是因为这里面要介绍的东西只是特征工程中的一小部分。这部分比较基础，比较容易说，所以由此开始。 单个原始特征（或称为变量）通常属于以下几类之一： 连续（continuous）特征； 无序类别（categorical）特征； 有序类别（ordinal）特征。 本文中我主要介绍针对单个特征的处理方法，虽然也会附带介绍基础的特征组合方法。同时处理多个特征，以及更复杂的特征处理方法介绍，以后我再另外细说。下面我由浅入深地逐渐说明针对这三类特征的常用处理方法。 一、初级篇这节要讲的处理技术，应该刚接触机器学习不久的同学都会知道。 1.1 连续特征1.2 无序特征可以使用One-hot（也叫One-of-k）的方法把每个无序特征转化为一个数值向量。比如一个无序特征color有三种取值：red，green，blue。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于red，green，blue。如： color取值 向量表示 red (1, 0, 0) green (0, 1, 0) blue (0, 0, 1) 这种方法在NLP里用的很多，就是所谓的词向量模型。变换后的向量长度对于词典长度，每个词对应于向量中的一个元素。 机器学习书籍里在讲这个的时候介绍的处理方法可能跟我上面说的有点差别。上面说的表达方式里有一个维度是可以省略的。 既然我们知道color一定是取3个值中的一个，那么我们知道向量的前两个元素值，就能推断第3个值是多少。所以，其实用下面的方式就可以表达到底是哪种颜色： color取值 向量表示 red (1, 0) green (0, 1) blue (0, 0) 这样表达的好处是少用了一个维度，降低了转化后特征之间的相关性。但在实际问题中特征基本都或多或少会有些缺失。使用第一种表达方式就可以用全0的向量来表示值缺失，而第二种表达方式是没法表达缺失的。 1.3 有序特征有些特征虽然也像无序特征那样只取限定的几个值，但是这些值之间有顺序的含义。例如一个人的状态status有三种取值：bad, normal, good，显然bad &lt; normal &lt; good。 当然，对有序特征最简单的处理方式是忽略其中的顺序关系，把它看成无序的，这样我们就可以使用处理无序特征的方式来处理它。在实际问题中，这种处理方式其实用的很多。 当然有些问题里有序可能会很重要，这时候就不应该把其中的顺序关系丢掉。一般的表达方式如下： status取值 向量表示 bad (1, 0, 0) normal (1, 1, 0) good (1, 1, 1) 上面这种表达方式很巧妙地利用递进表达了值之间的顺序关系。 二、中级篇最容易让人掉以轻心的，往往就是大家觉得最简单的事。在特征处理中，最容易让刚入门同学忽略的，是对连续特征的处理方式。 以线性分类器Linear Regression (LinearReg)为例，它是通过特征的线性加权来预测因变量y： y=w^Tx但大部分实际情况下，yy与xx都不会是这么简单的线性关系，甚至连单调关系都不会有。举个只有一个特征的例子，如果yy与xx的实际关系如下图： 那么直接把xx扔进LinearReg模型是怎么也得不到好结果的。很多人会想着既然线性分类器搞不定，那就直接找个非线性的好了，比如高斯核的SVM。我们确实可以通过这种简单换算法的方式解决这个简单的问题。但对于很多实际问题（如广告点击率预测），往往特征非常多，这时候时间约束通常不允许我们使用很复杂的非线性分类器。这也是为什么算法发展这么多年，广告点击率预测最常用的方法还是Logistic Regression (LogisticReg)。 对于上面这个问题，有没有什么办法使得LinearReg也能处理得不错？当然是有，就是对原始特征x做转化，把原来的非线性关系转化为线性关系。 2.1 方法一：离散化最常用的转化方式是对xx做离散化(discretization)，也就是把原来的值分段，转化成一个取值为0或1的向量。原始值落在某个段里，向量中此段对应的元素就为1，否则为0。 离散化的目标是y与转化后向量里的每个元素都保持比较好的线性关系。 比如取离散点{0.5,1.5,2.5}，通过判断xx属于(−∞,0.5)，[0.5,1.5)，[1.5,2.5)，[2.5,+∞)中哪段来把它离散化为4维的向量。下面是一些例子的离散结果： 原始值xx 离散化后的值 0.1 (1, 0, 0, 0) 1.3 (0, 1, 0, 0) 3.2 (0, 0, 0, 1) 5.8 (0, 0, 0, 1) 离散化方法的关键是怎么确定分段中的离散点。下面是常用的选取离散点的方法： 等距离离散： 顾名思义，就是离散点选取等距点。我们上面对xx取离散点{0.5,1.5,2.5}就是一种等距离散，见下图。图中垂直的灰线代表离散点。 等样本点离散 选取的离散点保证落在每段里的样本点数量大致相同，见下图。 画图观察趋势 以xx为横坐标，yy为纵坐标，画图，看曲线的趋势和拐点。通过观察下面的图我们发现可以利用3条直线（红色直线）来逐段近似原来的曲线。把离散点设为两条直线相交的各个点，我们就可以把xx离散化为长度为3的向量。 上面介绍的这种离散化为0/1向量的方法有个问题，它在离散时不会考虑到具体的xx到离散边界的距离。比如等距离散中取离散点为{0.5,1.5,2.5}{0.5,1.5,2.5}，那么1.499，1.501和2.49分别会离散为(0, 1, 0, 0)，(0, 0, 1, 0)和(0, 0, 1, 0)。1.499和1.501很接近，可是就因为这种强制分段的离散导致它们离散的结果差距很大。 针对上面这种硬离散的一种改进就是使用软离散，也就是在离散时考虑到xx与附近离散点的距离，离散出来的向量元素值可以是0/1之外的其他值。有兴趣的同学可以去ESL1这本书中找点感觉。 2.2 函数变换函数变换直接把原来的特征通过非线性函数做变换，然后把原来的特征，以及变换后的特征一起加入模型进行训练。常用的变换函数见下表，不过其实你可以尝试任何函数。 常用非线性函数f(x) x的取值范围 $x^α; α∈(−∞,+∞)$ $(−∞,+∞)$ $log(x)$ $(0,+∞)$ $log(\frac{x}{1−x})$ $(0,1)$ 这个方法操作起来很简单，但记得对新加入的特征做归一化。 对于我们前面的问题，只要把$x^2$，$x^3$也作为特征加入即可，因为实际上y就是x的一个三次多项式。 三、高级篇3.1 笛卡尔乘积我们可以使用笛卡尔乘积的方式来组合2个或更多个特征。比如有两个类别特征color和light，它们分别可以取值为red，green，blue和on, off。这两个特征各自可以离散化为3维和2维的向量。对它们做笛卡尔乘积转化，就可以组合出长度为6的特征，它们分别对应着原始值对(red, on)，(red, off)，(green, on)，(green, off)，(blue, on)，(blue, off)。下面的矩阵表达方式更清楚地说明了这种组合。 X on off red green blue 对于3个特征的笛卡尔乘积组合，可以表达为立方的形式。更多特征的组合依次类推。这个方法也可以直接用于连续特征与类别特征之间的组合，只要把连续特征看成是1维的类别特征就好了，这时候组合后特征对应的值就不是0/1了，而是连续特征的取值。 3.2 离散化续篇在上节中我已经介绍了一些常用的离散化单个连续特征的方法，其中一个是画图观察趋势。画图观察趋势的好处是直观、可解释性强，坏处是很麻烦。当要离散化的特征很多时，这种方法可操作性较差。 机器学习中有个很好解释，速度也不错的模型——决策树模型。大白话说决策树模型就是一大堆的if else。它天生就可以对连续特征分段，所以把它用于离散化连续特征合情合理。我称这种方法为决策树离散化方法。例如Gmail在对信件做重要性排序时就使用了决策树离散化方法 决策树离散化方法通常也是每次离散化一个连续特征，做法如下： 单独用此特征和目标值yy训练一个决策树模型，然后把训练获得的模型内的特征分割点作为离散化的离散点。 这种方法当然也可以同时离散化多个连续特征，但是操作起来就更复杂了，实际用的不多。 3.3 核方法核方法经常作为线性模型的一种推广出现。以线性回归模型为例，它对应的核方法如下： f_\theta (x)=\sum_{i=1}^n\theta _iK(x,x_i)其中$\{x_i\}^n_{i=1}$为训练样本点，$K(x_i，x_j)$为核函数，比如常用的高斯核函数为： K(x_i，x_j)=exp(-\frac{||x_i-x_j||_2^2}{2h^2})如果我们把上面模型里的${K(x,x_i)}^n_{i=1}$看成特征，而$θ$看成模型参数的话，上面的模型仍旧是个线性模型。所以可以认为核方法只是特征函数变换的一种方式。 当然，如果把核函数$K(x_i,x_j)$看成一种相似度的话，那上面的模型就是kNN模型了，或者叫做加权平均模型也可以。 因为核方法在预测时也要用到训练样本点，耗内存且计算量大，所以在数据量较大的实际问题中用的并不多。 到此，我已经介绍了不少针对单个特征的处理方法。这些处理方法很难说哪个好哪个不好。有些问题这个好，有些问题那个好，也没什么绝招能直接判断出哪种方法能适合哪些问题。唯一的招就是：Experiment a lot! 参考 Trevor Hastie et al. The Elements of Statistical Learning, 2001. ↩ Douglas Aberdeen et al. The Learning Behind Gmail Priority Inbox, 2010. ↩]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（12）：单例模式]]></title>
    <url>%2F2017%2F08%2F30%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%8812%EF%BC%89%EF%BC%9A%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[一、设计模式设计模式（Design pattern）是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结。使用设计模式是为了可重用代码、让代码更容易被他人理解、保证代码可靠性。面向对象最基本的设计原则有5条，分别是：单一职责原则、开放封闭原则、依赖倒置原则、接口隔离原则和Liskov替换原则。设计模式就是实现了这些原则，从而达到了代码复用、增加可维护性的目的。设计模式分为三种类型，共23类。创建型模式：是处理对象创建的设计模式，试图根据实际情况使用合适的方式创建对象。基本的对象创建方式可能会导致设计上的问题，或增加设计的复杂度。创建型模式通过以某种方式控制对象的创建来解决问题。创建型模式由两个主导思想构成。一是将系统使用的具体类封装起来，二是隐藏这些具体类的实例创建和结合的方式。创建型模式包括：单例模式、抽象工厂模式、建造者模式、工厂模式、原型模式。结构型模式：适配器模式、桥接模式、装饰模式、组合模式、外观模式、享元模式、代理模式。行为型模式：模板方法模式、命令模式、迭代器模式、观察者模式、中介者模式、备忘录模式、解释器模式、状态模式、策略模式、职责链模式、访问者模式。二、单例模式单例模式是较为常用的模式之一，且经常作为考题进行考察。单例模式的意图：保证一个类仅有一个实例，并提供一个访问它的全局访问点。单例模式的结构图：使用单例的优点：单例类只有一个实例共享资源，全局使用节省创建时间，提高性能单例模式有多种写法各有利弊，现在我们来看看各种模式写法。2.1 饿汉式12345678public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123; &#125; public static Singleton getInstance() &#123; return instance; &#125; &#125;这种方式和名字很贴切，饥不择食，在类装载的时候就创建，不管你用不用，先创建了再说，如果一直没有被使用，便浪费了空间，典型的空间换时间，每次调用的时候，就不需要再判断，节省了运行时间。Java Runtime就是使用这种方式，它的源代码如下：1234567891011121314151617public class Runtime &#123; private static Runtime currentRuntime = new Runtime(); /** * Returns the runtime object associated with the current Java application. * Most of the methods of class &lt;code&gt;Runtime&lt;/code&gt; are instance * methods and must be invoked with respect to the current runtime object. * * @return the &lt;code&gt;Runtime&lt;/code&gt; object associated with the current * Java application. */ public static Runtime getRuntime() &#123; return currentRuntime; &#125; /** Don't let anyone else instantiate this class */ private Runtime() &#123;&#125; //以下代码省略&#125;总结：「饿汉式」是最简单的实现方式，这种实现方式适合那些在初始化时就要用到单例的情况，这种方式简单粗暴，如果单例对象初始化非常快，而且占用内存非常小的时候这种方式是比较合适的，可以直接在应用启动时加载并初始化。但是，如果单例初始化的操作耗时比较长而应用对于启动速度又有要求，或者单例的占用内存比较大，再或者单例只是在某个特定场景的情况下才会被使用，而一般情况下是不会使用时，使用「饿汉式」的单例模式就是不合适的，这时候就需要用到「懒汉式」的方式去按需延迟加载单例。2.2 懒汉式（非线程安全）1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123; &#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125;懒汉模式申明了一个静态对象，在用户第一次调用时初始化，虽然节约了资源，但第一次加载时需要实例化，反映稍慢一些，而且在多线程不能正常工作。在多线程访问的时候，很可能会造成多次实例化，就不再是单例了。「懒汉式」与「饿汉式」的最大区别就是将单例的初始化操作，延迟到需要的时候才进行，这样做在某些场合中有很大用处。比如某个单例用的次数不是很多，但是这个单例提供的功能又非常复杂，而且加载和初始化要消耗大量的资源，这个时候使用「懒汉式」就是非常不错的选择。2.3 懒汉式（线程安全）1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123; &#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125;这两种「懒汉式」单例，名字起的也很贴切，一直等到对象实例化的时候才会创建，确实够懒，不用鞭子抽就不知道走了，典型的时间换空间，每次获取实例的时候才会判断，看是否需要创建，浪费判断时间，如果一直没有被使用，就不会被创建，节省空间。因为这种方式在getInstance()方法上加了同步锁，所以在多线程情况下会造成线程阻塞，把大量的线程锁在外面，只有一个线程执行完毕才会执行下一个线程。Android中的 InputMethodManager 使用了这种方式，我们看看它的源码：12345678910111213141516171819public final class InputMethodManager &#123; static InputMethodManager sInstance; /** * Retrieve the global InputMethodManager instance, creating it if it * doesn&apos;t already exist. * @hide */ public static InputMethodManager getInstance() &#123; synchronized (InputMethodManager.class) &#123; if (sInstance == null) &#123; IBinder b = ServiceManager.getService(Context.INPUT_METHOD_SERVICE); IInputMethodManager service = IInputMethodManager.Stub.asInterface(b); sInstance = new InputMethodManager(service, Looper.getMainLooper()); &#125; return sInstance; &#125; &#125;&#125;2.4 双重校验锁（DCL）上面的方法「懒汉式（线程安全）」毫无疑问存在性能的问题 — 如果存在很多次getInstance()的调用，那性能问题就不得不考虑了！让我们来分析一下，究竟是整个方法都必须加锁，还是仅仅其中某一句加锁就足够了？我们为什么要加锁呢？分析一下出现lazy loaded的那种情形的原因。原因就是检测null的操作和创建对象的操作分离了。如果这两个操作能够原子地进行，那么单例就已经保证了。于是，我们开始修改代码，就成了下面的双重校验锁（Double Check Lock）：1234567891011121314151617181920public class Singleton &#123; /** * 注意此处使用的关键字 volatile， * 被volatile修饰的变量的值，将不会被本地线程缓存， * 所有对该变量的读写都是直接操作共享内存，从而确保多个线程能正确的处理该变量。 */ private volatile static Singleton singleton; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if (instance == null) &#123; synchronized(Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125;这种写法在getSingleton()方法中对singleton进行了两次判空，第一次是为了不必要的同步，第二次是在singleton等于null的情况下才创建实例。在这里用到了volatile关键字，不了解volatile关键字的可以查看 Java多线程（三）volatile域 和 java中volatile关键字的含义 两篇文章，可以看到双重检查模式是正确使用volatile关键字的场景之一。「双重校验锁」：既可以达到线程安全，也可以使性能不受很大的影响，换句话说在保证线程安全的前提下，既节省空间也节省了时间，集合了「饿汉式」和两种「懒汉式」的优点，取其精华，去其槽粕。对于volatile关键字，还是存在很多争议的。由于volatile关键字可能会屏蔽掉虚拟机中一些必要的代码优化，所以运行效率并不是很高。也就是说，虽然可以使用“双重检查加锁”机制来实现线程安全的单例，但并不建议大量采用，可以根据情况来选用。还有就是在java1.4及以前版本中，很多JVM对于volatile关键字的实现的问题，会导致“双重检查加锁”的失败，因此“双重检查加锁”机制只只能用在java1.5及以上的版本。2.5 静态内部类另外，在很多情况下JVM已经为我们提供了同步控制，比如：在static {…}区块中初始化的数据访问final字段时因为在JVM进行类加载的时候他会保证数据是同步的，我们可以这样实现：采用内部类，在这个内部类里面去创建对象实例。这样的话，只要应用中不使用内部类 JVM 就不会去加载这个单例类，也就不会创建单例对象，从而实现「懒汉式」的延迟加载和线程安全。12345678910public class Singleton &#123; private Singleton()&#123; &#125; public static Singleton getInstance()&#123; return SingletonHolder.sInstance; &#125; private static class SingletonHolder &#123; private static final Singleton sInstance = new Singleton(); &#125; &#125;第一次加载Singleton类时并不会初始化sInstance，只有第一次调用getInstance方法时虚拟机加载SingletonHolder 并初始化sInstance ，这样不仅能确保线程安全也能保证Singleton类的唯一性，所以推荐使用静态内部类单例模式。然而这还不是最简单的方式，《Effective Java》中作者推荐了一种更简洁方便的使用方式，就是使用「枚举」。2.6 枚举《Java与模式》中，作者这样写道，使用枚举来实现单实例控制会更加简洁，而且无偿地提供了序列化机制，并由JVM从根本上提供保障，绝对防止多次实例化，是更简洁、高效、安全的实现单例的方式。12345678public enum Singleton &#123; //定义一个枚举的元素，它就是 Singleton 的一个实例 INSTANCE; public void doSomeThing() &#123; // do something... &#125; &#125;使用方法如下：1234public static void main(String args[]) &#123; Singleton singleton = Singleton.instance; singleton.doSomeThing();&#125;枚举单例的优点就是简单，但是大部分应用开发很少用枚举，可读性并不是很高，不建议用。2.7 使用容器12345678910111213public class SingletonManager &#123; private static Map&lt;String, Object&gt; objMap = new HashMap&lt;String,Object&gt;(); private Singleton() &#123; &#125; public static void registerService(String key, Objectinstance) &#123; if (!objMap.containsKey(key) ) &#123; objMap.put(key, instance) ; &#125; &#125; public static ObjectgetService(String key) &#123; return objMap.get(key) ; &#125;&#125;这种是用SingletonManager 将多种单例类统一管理，在使用时根据key获取对象对应类型的对象。这种方式使得我们可以管理多种类型的单例，并且在使用时可以通过统一的接口进行获取操作，降低了用户的使用成本，也对用户隐藏了具体实现，降低了耦合度。总结对于以上七种单例，分别是「饿汉式」、「懒汉式(非线程安全)」、「懒汉式(线程安全)」、「双重校验锁」、「静态内部类」、「枚举」和「容器类管理」。很多时候取决人个人的喜好，虽然双重检查有一定的弊端和问题，但我就是钟爱双重检查，觉得这种方式可读性高、安全、优雅（个人观点）。所以代码里常常默写这样的单例，写的时候感觉自己是个伟大的建筑师。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>单例模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（11）：进程与线程]]></title>
    <url>%2F2017%2F08%2F30%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[操作系统可以同时运行多个任务。打个比方，你一边在用浏览器上网，一边在听MP3，一边在用Word赶作业，这就是多任务，至少同时有3个任务正在运行。还有很多任务悄悄地在后台同时运行着，只是桌面上没有显示而已。操作系统轮流让各个任务交替执行，任务1执行0.01秒，切换到任务2，任务2执行0.01秒，再切换到任务3，执行0.01秒……这样反复执行下去。表面上看，每个任务都是交替执行的，但是，由于CPU的执行速度实在是太快了，我们感觉就像所有任务都在同时执行一样。真正的并行执行多任务只能在多核CPU上实现，但是，由于任务数量远远多于CPU的核心数量，所以，操作系统也会自动把很多任务轮流调度到每个核心上执行。对于操作系统来说，一个任务就是一个进程（Process），比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。有些进程还不止同时干一件事，比如Word，它可以同时进行打字、拼写检查、打印等事情。在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的这些“子任务”称为线程（Thread）。由于每个进程至少要干一件事，所以，一个进程至少有一个线程。当然，像Word这种复杂的进程可以有多个线程，多个线程可以同时执行，多线程的执行方式和多进程是一样的，也是由操作系统在多个线程之间快速切换，让每个线程都短暂地交替运行，看起来就像同时执行一样。当然，真正地同时执行多线程需要多核CPU才可能实现。一、进程1.1 定义狭义定义：进程是计算机中正在运行的程序的实例（an instance of a computer program that is being executed）。程序本身只是指令的集合，进程才是程序（那些指令）的真正运行。用户下达运行程序的命令后，就会产生进程。同一程序可产生多个进程（一对多关系），以允许同时有多位用户运行同一程序，却不会相互冲突。进程需要一些资源才能完成工作，比如CPU使用时间、存储器、文件以及I/O设备，且为依序逐一进行，也就是任何时间内仅能运行一项进程。1.2 基本状态通常进程有如下5种状态，其中前三种是进程的基本状态。1）运行状态（执行窗台）：进程正在处理器上运行。在单处理器环境下，每一时刻最多只有一个进程处于运行状态。2）就绪状态：进程已处于准备运行的状态，即进程获得了除处理器之外的一切所需资源，一旦得到处理器即可运行。3）阻塞状态：又称为等待状态，进程正在等待某一事件而暂停运行，如等待某资源为可用（不包括处理器）或等待输入/输出完成。即使处理器空闲，该进程也不能运行。4）创建状态：进程正在被创建，尚未转到就绪状态。5）结束状态：进程正从系统中消失。可能是进程正常结束或其他原因中断退出运行。进程的三个基本状态之间只可以相互转换的，如图所示。具体的说：当一个就绪状态获得处理机时，其状态由就绪变为执行；当一个运行进程被剥夺处理机时，如用完系统分给它的时间片、出现更高优先级别的其他进程，其状态由运行变为就绪；当一个运行进程因某事件受阻时，如所申请资源被占用、启动I/O传输未完成，其状态由执行变为阻塞；当所等待事件发生时，如得到申请资源、I/O传输完成，其状态由阻塞变为就绪1.3 进程与程序的区别进程是程序及其数据在计算机上的一次运行活动，是一个动态的概念。进程的运行实体是程序、离开程序的进程没有存在的意义。从静态角度看，进程是由程序、数据和进程控制块（PCB）三部分组成的。而程序时一组有序的指令集合，是一种静态的概念。进程是程序的一次执行过程，它是动态地创建和消亡的，具有一定的生命期，是暂时存在的；而程序则是一组代码的集合，它是永久存在的，可长期保存。一个进程可以执行一个或几个程序，一个程序也可以构成多个进程。进程可创建进程，而程序不可能形成新的程序。二、线程2.1 定义线程，有时被称为轻量级进程（Lightweight Process，LWP），是程序执行流的最小单元。一个标准的线程由线程ID、当前指令指针（PC）、寄存器集合和堆栈（stack）组成。另外，线程是进程中的一个实体，是被系统独立调度和分派的基本单位，线程自己不拥有系统资源，只拥有一点在运行中必不可少的资源，但它可与同属一个进程的其他线程共享进程所拥有的资源。线程共享的进程环境包括：进程代码段、进程的共有数据（如全局变量，利用这些共享的数据，线程很容易的实现相互之间的通信）、进程打开的文件描述符、信号的处理器、进程的当前目录和进程用户ID与进程组ID。线程拥有这许多共性的同时，还拥有自己的个性。有了这些个性，线程才能够实现并发性。这些个性包括：线程ID：每个线程都有自己的线程ID，这个ID在本进程中是唯一的。进程用此来标识线程；寄存器的值：由于线程间是并发运行的，每个线程有自己不同的运行线索，当从一个线程切换到另一个线程上时，必须将原有线程的寄存器集合的状态进行保存，以便将来该线程在被重新切换时能得以恢复。线程的堆栈（Stack）：堆栈是保证线程独立运行所必须的。线程函数可以调用函数，而被调用函数中又是可以层层嵌套的，所以线程必须拥有自己的函数堆栈，使得函数调用可以正常执行，不受其他线程的影响。在一个进程的线程共享堆区（heap）。错误返回码线程的信号屏蔽码线程的优先级一个线程可以创建和撤销另一个线程，同一进程的多个线程之间可以并发执行。由于县城之间的相互制约，致使线程在运行中呈现间断性。线程也有就绪、阻塞和运行三种基本状态。每一个程序都至少有一个线程，若程序只有一个线程，那就是程序本身。线程是程序中一个单一的顺序控制流程。在单个程序中同时运行多个线程完成不同的工作，称为多线程。引入线程后，进程的内涵发生了变化，进程只作为除CPU以外系统资源的分配单元，线程则作为处理器的分配单元。在同一进程中，线程的切换不会引起进程的切换，但从一个进程中的线程切换到另一个进程中的线程时，将会引起进程的切换。1.2 进程与线程的区别调度：在传统操作系统中，拥有资源和独立调度的基本单位都是进程。引入线程后，线程是独立调度的基本单位，进程是拥有资源的基本单位。在同一进程中，线程的切换不会引起进程切换。在不同进程中进行的线程切换，则会引起进程切换。拥有资源：不论是传统的还是引入线程的操作系统，进程都是拥有资源的基本单位，线程不拥有资源（也有一点必不可少的资源），但线程可以共享其隶属进程的系统资源。并发性：在引入线程的操作系统中，不仅进程可以并发执行，而且同一进程内的多个线程也可以并发执行，从而使操作系统具有具有更好的并发性，大大提高了系统的吞吐量。系统开销：创建和撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O设备等等，因此操作系统所付出的开销远大于创建或撤销线程的开销。类似地，在进程切换时，涉及当前执行进程CPU环境的保存以及新调度的进程CPU环境的设置；而线程切换时只需保存和设置少量寄存器内容，因此开销很小。另外，由于同一进程内的多个线程共享进程的地址空间，因此这些线程之间的同步与通信比较容易实现，甚至无需操作系统的干预。地址空间和其他资源（如打开的文件）：进程的地址空间之间相互独立，同一进程的各线程间共享进程的资源，某进程内的线程对于其他进程不可见。通信方面：进程间通信需要借助操作系统，而线程间可以直接读、写进程数据段（如全局变量）来进行通信。三、进程通信与进程同步多个进程可以共享系统中的各种资源，但其中许多资源一次为能为一个进程使用，我们把一次仅允许一个进程使用的资源成为临界资源。许多物理设备都属于临界资源，如打印机等。对临界资源的访问，必须互斥的进行，在每个进程中，访问临界资源的那段代码成为临界区（Critical Section）。进程通信与同步有如下一些目的。数据传输共享数据通知数据资源共享进程控制Linux进程间通信的几种主要手段简介：管道（Pipe）及有名管道（named Pipe）信号（Signal）Message（消息队列）共享内存（Shared Memory）信号量套接口Linux线程间通信：互斥体（互斥量）、信号量、条件变量Windows进程间通信：管道、共享内存、消息队列、信号量、socketwindows线程间通信：临界区（Critical Section）、互斥量（Mutex）、信号量（信号灯）（Semaphore）、事件（Event）。四、调度算法调度的基本准则包括CPU利用率、系统吞吐量、周转时间、等待时间、响应时间等。系统吞吐量：表示单位时间内CPU完成作业的数量周转时间：作业完成时刻减去作业到达的时刻等待时间：进程处于等处理器状态的时间之和，等待时间越长，用户满意度越低。响应时间：从用户提交请求到系统首次产生响应所用的时间。典型的调度算法包括：实时系统中：FIFO(First Input First Output，先进先出算法)，SJF(Shortest Job First，最短作业优先算法)，SRTF(Shortest Remaining Time First，最短剩余时间优先算法）。交互式系统中：RR(Round Robin，时间片轮转算法)，HPF(Highest Priority First，最高优先级算法)，多级队列，最短进程优先，保证调度，彩票调度，公平分享调度。多级反馈队列调度算法。其中SJF的平均等待时间、平均周转时间最少。五、死锁所谓死锁是指多个进程因竞争资源而造成的一种僵局（互相等待），若无外力作用，这些进程都将无法向前推进。现实生活中简单的例子：交通阻塞，两股相向而行的车流都想通过已被对方占用的道路，结果双方都不能前进。死锁产生的原因：系统资源的竞争、进程推进顺序非法死锁产生的必要条件：产生死锁必须同时满足以下四个条件，只要其中任一条件不满足，死锁就不会发生。互斥条件：进程要求对所分配的资源进行排他性控制，即在一段时间内某资源仅为一个进程所占有。此时若有其他进程请求该资源，则请求进程只能等待；不剥夺条件：进程所获得的资源在未使用完毕之前，不能被其他资源强行夺走，即只能由获得该资源的进程自己来释放。请求和保持条件：又称为部分分配条件。进程每次申请它所需要的一部分资源，在等待新资源的同时，进程继续占有已分配到的资源。循环等待条件：存在一种进程资源的循环等待链，链中每个进程已获得的资源同时被链中下一个进程所请求。即存在一个处于等待状态的进程集合$\{P_1,P_2,….P_n\}$其中$P_i$等待的资源被$P_{i+1}(i=0,1,2,…n-1)$占有，$P_n$等待资源被$P_0$占有。死锁处理策略预防死锁：设置某些限制条件，破坏产生死锁的四个必要条件中的一个或几个避免死锁：在资源的动态分配过程中，用某种方法防止系统进入不安全状态。银行家算法是著名的死锁避免算法。死锁的检测及解除：无需采取任何限制性措施，允许进程在运行过程中发生死锁，通过系统的检测机制及时地检测出死锁的发生，然后采取某种措施解除死锁。死锁可利用资源分配图来描述。死锁的解除主要方法：资源剥夺法、撤销进程法、进程回退法]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>操作系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（10）：QA]]></title>
    <url>%2F2017%2F08%2F30%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%8810%EF%BC%89%EF%BC%9AQA%2F</url>
    <content type="text"><![CDATA[一、什么是Java虚拟机？为什么Java被称作是“平台无关的编程语言”？Java虚拟机是一个可以执行Java字节码的虚拟机进程。Java源文件被编译成能被Java虚拟机执行的字节码文件。二、JDK和JRE的区别是什么？Java运行时环境(JRE)是将要执行Java程序的Java虚拟机。它同时也包含了执行applet需要的浏览器插件。Java开发工具包(JDK)是完整的Java软件开发包，包含了JRE，编译器和其他的工具(比如：JavaDoc，Java调试器)，可以让开发者开发、编译、执行Java应用程序。三、static关键字是什么意思？Java中是否可以覆盖(override)一个private或者是static的方法？第一小问：static 修饰符能够与变量、方法一起使用，表示是“静态”的。静态变量和静态方法能够通过类名来访问，不需要创建一个类的对象来访问该类的静态成员，所以static修饰的成员又称作类变量和类方法。静态变量与实例变量不同，实例变量总是通过对象来访问，因为它们的值在对象和对象之间有所不同。静态变量属于类，不属于任何独立的对象，所以无需创建类的实例就可以访问静态变量。之所以会产生这样的结果，是因为编译器只为整个类创建了一个静态变量的副本，也就是只分配一个内存空间，虽然有多个实例，但这些实例共享该内存。实例变量则不同，每创建一个对象，都会分配一次内存空间，不同变量的内存相互独立，互不影响，改变 a 对象的实例变量不会影响 b 对象。static 的变量是在类装载的时候就会被初始化。也就是说，只要类被装载，不管你是否使用了这个static 变量，它都会被初始化。小结：类变量(class variables)用关键字 static 修饰，在类加载的时候，分配类变量的内存，以后再生成类的实例对象时，将共享这块内存（类变量），任何一个对象对类变量的修改，都会影响其它对象。外部有两种访问方式：通过对象来访问或通过类名来访问。123456789101112131415public class Demo &#123; static int i; int j; public static void main(String[] args) &#123; Demo obj1 = new Demo(); obj1.i = 10; obj1.j = 20; Demo obj2 = new Demo(); System.out.println("obj1.i=" + obj1.i + ", obj1.j=" + obj1.j); System.out.println("obj2.i=" + obj2.i + ", obj2.j=" + obj2.j); &#125;&#125;运行结果：12obj1.i=10, obj1.j=20obj2.i=10, obj2.j=0第二小问：被覆盖的方法不能为static。如果父类中的方法为静态的，而子类中的方法不是静态的，但是两个方法除了这一点外其他都满足覆盖条件，那么会发生编译错误；反之亦然。即使父类和子类中的方法都是静态的，并且满足覆盖条件，但是仍然不会发生覆盖，因为方法覆盖是基于运行时动态绑定的，而static方法是编译时静态绑定的。static方法跟类的任何实例都不相关，所以概念上不适用。四、是否可以在static环境中访问非static变量？非静态的既可以访问静态的，也可以访问非静态的，而静态的只能访问静态的。不可以在静态环境中访问非静态。因为静态的成员属于类，随着类的加载而加载到静态方法区内存，当类加载时，此时不一定有实例创建，没有实例，就不可以访问非静态的成员。类的加载先于实例的创建，因此静态环境中，不可以访问非静态！static变量在Java中是属于类的，它在所有的实例中的值是一样的。当类被Java虚拟机载入的时候，会对static变量进行初始化。如果你的代码尝试不用实例来访问非static的变量，编译器会报错，因为这些变量还没有被创建出来，还没有跟任何实例关联上。五、Java支持的数据类型有哪些？什么是自动拆装箱？基本数据类型对应的包装类byteByteshortShortintIntegerlongLongcharCharacterfloatFloatdoubleDoublebooleanBoolean虽然 Java 语言是典型的面向对象编程语言，但其中的八种基本数据类型并不支持面向对象编程，基本类型的数据不具备“对象”的特性——不携带属性、没有方法可调用。 沿用它们只是为了迎合人类根深蒂固的习惯，并的确能简单、有效地进行常规数据处理。这种借助于非面向对象技术的做法有时也会带来不便，比如引用类型数据均继承了 Object 类的特性，要转换为 String 类型（经常有这种需要）时只要简单调用 Object 类中定义的toString()即可，而基本数据类型转换为 String 类型则要麻烦得多。为解决此类问题 ，Java为每种基本数据类型分别设计了对应的类，称之为包装类(Wrapper Classes)，也有教材称为外覆类或数据类型类。每个包装类的对象可以封装一个相应的基本类型的数据，并提供了其它一些有用的方法。包装类对象一经创建，其内容（所封装的基本类型数据值）不可改变。基本类型和对应的包装类可以相互装换：由基本类型向对应的包装类转换称为装箱，例如把 int 包装成 Integer 类的对象；包装类向对应的基本类型转换称为拆箱，例如把 Integer 类的对象重新简化为 int。八个包装类的使用比较相似，下面是常见的应用场景。1）实现 int 和 Integer 的相互转换可以通过 Integer 类的构造方法将 int 装箱，通过 Integer 类的 intValue 方法将 Integer 拆箱。例如：1234567891011public class Demo &#123; public static void main(String[] args) &#123; int m = 500; Integer obj = new Integer(m); // 手动装箱 int n = obj.intValue(); // 手动拆箱 System.out.println(&quot;n = &quot; + n); Integer obj1 = new Integer(500); System.out.println(&quot;obj 等价于 obj1？&quot; + obj.equals(obj1)); &#125;&#125;运行结果：12n = 500obj 等价于 obj1？true2）将字符串转换为整数Integer 类有一个静态的 paseInt() 方法，可以将字符串转换为整数，语法为：1parseInt(String s, int radix);s 为要转换的字符串，radix 为进制，可选，默认为十进制。下面的代码将会告诉你什么样的字符串可以转换为整数：1234567891011121314public class Demo &#123; public static void main(String[] args) &#123; String str[] = &#123;&quot;123&quot;, &quot;123abc&quot;, &quot;abc123&quot;, &quot;abcxyz&quot;&#125;; for(String str1 : str)&#123; try&#123; int m = Integer.parseInt(str1, 10); System.out.println(str1 + &quot; 可以转换为整数 &quot; + m); &#125;catch(Exception e)&#123; System.out.println(str1 + &quot; 无法转换为整数&quot;); &#125; &#125; &#125;&#125;1234123 可以转换为整数 123123abc 无法转换为整数abc123 无法转换为整数abcxyz 无法转换为整数3）将整数转换为字符串Integer 类有一个静态的 toString() 方法，可以将整数转换为字符串。例如：1234567public class Demo &#123; public static void main(String[] args) &#123; int m = 500; String s = Integer.toString(m); System.out.println(&quot;s = &quot; + s); &#125;&#125;运行结果：1s = 500上面的例子都需要手动实例化一个包装类，称为手动拆箱装箱。Java 1.5(5.0) 之前必须手动拆箱装箱。Java 1.5 之后可以自动拆箱装箱，也就是在进行基本数据类型和对应的包装类转换时，系统将自动进行，这将大大方便程序员的代码书写。例如：1234567891011public class Demo &#123; public static void main(String[] args) &#123; int m = 500; Integer obj = m; // 自动装箱 int n = obj; // 自动拆箱 System.out.println("n = " + n); Integer obj1 = 500; System.out.println("obj 等价于 obj1？" + obj.equals(obj1)); &#125;&#125;运行结果：12n = 500obj 等价于 obj1？true自动拆箱装箱是常用的一个功能，需要重点掌握。六、Java中的方法覆盖(Overriding)和方法重载(Overloading)是什么意思？1、方法覆盖（Overriding）：在类继承中，子类可以修改从父类继承来的方法，也就是说子类能创建一个与父类方法有不同功能的方法，但具有相同的名称、返回值类型、参数列表。如果在新类中定义一个方法，其名称、返回值类型和参数列表正好与父类中的相同，那么，新方法被称做覆盖旧方法。参数列表又叫参数签名，包括参数的类型、参数的个数和参数的顺序，只要有一个不同就叫做参数列表不同。被覆盖的方法在子类中只能通过super调用。注意：覆盖不会删除父类中的方法，而是对子类的实例隐藏，暂时不使用。123456789101112131415161718192021222324252627282930public class Demo&#123; public static void main(String[] args) &#123; Dog myDog = new Dog("花花"); myDog.say(); // 子类的实例调用子类中的方法 Animal myAnmial = new Animal("贝贝"); myAnmial.say(); // 父类的实例调用父类中的方法 &#125;&#125;class Animal&#123; String name; public Animal(String name)&#123; this.name = name; &#125; public void say()&#123; System.out.println("我是一只小动物，我的名字叫" + name + "，我会发出叫声"); &#125;&#125;class Dog extends Animal&#123; // 构造方法不能被继承，通过super()调用 public Dog(String name)&#123; super(name); &#125; // 覆盖say() 方法 public void say()&#123; System.out.println("我是一只小狗，我的名字叫" + name + "，我会发出汪汪的叫声"); &#125;&#125;运行结果：12我是一只小狗，我的名字叫花花，我会发出汪汪的叫声我是一只小动物，我的名字叫贝贝，我会发出叫声方法覆盖的原则：覆盖方法的返回类型、方法名称、参数列表必须与原方法的相同。覆盖方法不能比原方法访问性差（即访问权限不允许缩小）。覆盖方法不能比原方法抛出更多的异常。被覆盖的方法不能是final类型，因为final修饰的方法是无法覆盖的。被覆盖的方法不能为private，否则在其子类中只是新定义了一个方法，并没有对其进行覆盖。被覆盖的方法不能为static。如果父类中的方法为静态的，而子类中的方法不是静态的，但是两个方法除了这一点外其他都满足覆盖条件，那么会发生编译错误；反之亦然。即使父类和子类中的方法都是静态的，并且满足覆盖条件，但是仍然不会发生覆盖，因为静态方法是在编译的时候把静态方法和类的引用类型进行匹配。2、方法重载(Overloading)在Java中，同一个类中的多个方法可以有相同的名字，只要它们的参数列表不同就可以，这被称为方法重载(method overloading)。参数列表又叫参数签名，包括参数的类型、参数的个数和参数的顺序，只要有一个不同就叫做参数列表不同。重载是面向对象的一个基本特性。下面看一个详细的实例。123456789101112131415161718192021222324252627public class Demo&#123; // 一个普通的方法，不带参数 void test()&#123; System.out.println(&quot;No parameters&quot;); &#125; // 重载上面的方法，并且带了一个整型参数 void test(int a)&#123; System.out.println(&quot;a: &quot; + a); &#125; // 重载上面的方法，并且带了两个参数 void test(int a,int b)&#123; System.out.println(&quot;a and b: &quot; + a + &quot; &quot; + b); &#125; // 重载上面的方法，并且带了一个双精度参数 double test(double a)&#123; System.out.println(&quot;double a: &quot; + a); return a*a; &#125; public static void main(String args[])&#123; Demo obj= new Demo(); obj.test(); obj.test(2); obj.test(2,3); obj.test(2.0); &#125;&#125;运行结果：1234No parametersa: 2a and b: 2 3double a: 2.0通过上面的实例，读者可以看出，重载就是在一个类中，有相同的函数名称，但形参不同的函数。重载的结果，可以让一个程序段尽量减少代码和方法的种类。说明：参数列表不同包括：个数不同、类型不同和顺序不同。仅仅参数变量名称不同是不可以的。跟成员方法一样，构造方法也可以重载。声明为final的方法不能被重载。声明为static的方法不能被重载，但是能够被再次声明。方法的重载的规则：方法名称必须相同。参数列表必须不同（个数不同、或类型不同、参数排列顺序不同等）。方法的返回类型可以相同也可以不相同。仅仅返回类型不同不足以成为方法的重载。方法重载的实现：方法名称相同时，编译器会根据调用方法的参数个数、参数类型等去逐个匹配，以选择对应的方法，如果匹配失败，则编译器报错，这叫做重载分辨。3、覆盖和重载的不同：方法覆盖要求参数列表必须一致，而方法重载要求参数列表必须不一致。方法覆盖要求返回类型必须一致，方法重载对此没有要求。方法覆盖只能用于子类覆盖父类的方法，方法重载用于同一个类中的所有方法（包括从父类中继承而来的方法）。方法覆盖对方法的访问权限和抛出的异常有特殊的要求，而方法重载在这方面没有任何限制。父类的一个方法只能被子类覆盖一次，而一个方法可以在所有的类中可以被重载多次七、Java中，什么是构造方法？什么是构造方法重载？什么是复制构造方法？1、Java中，什么是构造方法？在类实例化的过程中自动执行的方法叫做构造方法，它不需要你手动调用。构造方法可以在类实例化的过程中做一些初始化的工作。构造方法的名称必须与类的名称相同，并且没有返回值。每个类都有构造方法。如果没有显式地为类定义构造方法，Java编译器将会为该类提供一个默认的构造方法。下面是一个构造方法示例：12345678910111213141516171819202122232425public class Dog&#123; String name; int age; // 构造方法，没有返回值 Dog(String name1, int age1)&#123; name = name1; age = age1; System.out.println("感谢主人领养了我"); &#125; // 普通方法，必须有返回值 void bark()&#123; System.out.println("汪汪，不要过来"); &#125; void hungry()&#123; System.out.println("主人，我饿了"); &#125; public static void main(String arg[])&#123; // 创建对象时传递的参数要与构造方法参数列表对应 Dog myDog = new Dog("花花", 3); &#125;&#125;运行结果：1感谢主人领养了我说明：构造方法不能被显示调用。构造方法不能有返回值，因为没有变量来接收返回值。2、什么是构造方法重载？跟成员方法一样，构造方法也可以重载。Java中构造函数重载和方法重载很相似。可以为一个类创建多个构造函数。每一个构造函数必须有它自己唯一的参数列表。3、什么是复制构造方法？Java不支持像C++中那样的复制构造函数，这个不同点是因为如果你不自己写构造函数的情况下，Java不会创建默认的复制构造函数.八、Java支持多继承么？单继承性：Java 允许一个类仅能继承一个其它类，即一个类只能有一个父类，这个限制被称做单继承性。但是java中的接口支持多继承，，即一个子接口可以有多个父接口。（接口的作用是用来扩展对象的功能，一个子接口继承多个父接口，说明子接口扩展了多个功能，当类实现接口时，类就扩展了相应的功能）。九、接口和抽象类的区别是什么？Java提供和支持创建抽象类和接口。它们的实现有共同点，不同点在于：接口中所有的方法隐含的都是抽象的。而抽象类则可以同时包含抽象和非抽象的方法。类可以实现很多个接口，但是只能继承一个抽象类类可以不实现抽象类和接口声明的所有方法，当然，在这种情况下，类也必须得声明成是抽象的。抽象类可以在不提供接口方法实现的情况下实现接口。Java接口中声明的变量默认都是final的。抽象类可以包含非final的变量。Java接口中的成员函数默认是public的。抽象类的成员函数可以是private，protected或者是public。接口是绝对抽象的，不可以被实例化。抽象类也不可以被实例化，但是，如果它包含main方法的话是可以被调用的。也可以参考JDK8中抽象类和接口的区别十、什么是值传递和引用传递?值传递是对基本型变量而言的,传递的是该变量的一个副本,改变副本不影响原变量.引用传递一般是对于对象型变量而言的,传递的是该对象地址的一个副本, 并不是原对象本身 。 所以对引用对象进行操作会同时改变原对象.一般认为,java内的传递都是值传递.十一、进程与线程的区别概述:进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动,进程是系统进行资源分配和调度的一个独立单位.线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源.一个线程可以创建和撤销另一个线程;同一个进程中的多个线程之间可以并发执行.相对进程而言，线程是一个更加接近于执行体的概念，它可以与同进程中的其他线程共享数据，但拥有自己的栈空间，拥有独立的执行序列。在串行程序基础上引入线程和进程是为了提高程序的并发度，从而提高程序运行效率和响应时间。区别:进程和线程的主要差别在于它们是不同的操作系统资源管理方式。进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动，是操作系统进行资源分配和调度的一个独立单位；线程是进程的一个实体，是CPU调度和分派的基本单位，是比进程更小的能独立运行的基本单位。线程的划分尺度小于进程，这使得多线程程序的并发性高；进程在执行时通常拥有独立的内存单元，而线程之间可以共享内存。使用多线程的编程通常能够带来更好的性能和用户体验，但是多线程的程序对于其他程序是不友好的，因为它可能占用了更多的CPU资源。当然，也不是线程越多，程序的性能就越好，因为线程之间的调度和切换也会浪费CPU时间。时下很时髦的Node.js就采用了单线程异步I/O的工作模式。简而言之,一个程序至少有一个进程,一个进程至少有一个线程.线程的划分尺度小于进程，使得多线程程序的并发性高。另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。线程在执行过程中与进程还是有区别的。每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。从逻辑角度来看，多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理以及资源分配。这就是进程和线程的重要区别。优缺点:线程和进程在使用上各有优缺点：线程执行开销小，但不利于资源的管理和保护；而进程正相反。同时，线程适合于在SMP机器上运行，而进程则可以跨机器迁移。十二、为什么集合类没有实现Cloneable和Serializable接口？克隆(cloning)或者是序列化(serialization)的语义和含义是跟具体的实现相关的。因此，应该由集合类的具体实现来决定如何被克隆或者是序列化实现Serializable序列化的作用：将对象的状态保存在存储媒体中以便可以在以后重写创建出完全相同的副本；按值将对象从一个从一个应用程序域发向另一个应用程序域。实现Serializable接口的作用就是可以把对象存到字节流，然后可以恢复。所以你想如果你的对象没有序列化，怎么才能进行网络传输呢？要网络传输就得转为字节流，所以在分布式应用中，你就得实现序列化。如果你不需要分布式应用，那就没必要实现实现序列化。十三、什么是迭代器(Iterator)？Iterator接口提供了很多对集合元素进行迭代的方法。每一个集合类都包含了可以返回迭代器实例的迭代方法。迭代器可以在迭代的过程中删除底层集合的元素,但是不可以直接调用集合的remove(Object Obj)删除，可以通过迭代器的remove()方法删除。十四、 Iterator和ListIterator的区别是什么？Iterator是遍历集合的迭代器（不能遍历Map，只用来遍历Collection），Collection的实现类都实现了iterator()函数，它返回一个Iterator对象，用来遍历集合，ListIterator则专门用来遍历List。Iterator对集合只能是前向遍历，ListIterator既可以前向也可以后向。ListIterator实现了Iterator接口，并包含其他的功能，比如：增加元素，替换元素，获取前一个和后一个元素的索引，等等。十五、快速失败(fail-fast)和安全失败(fail-safe)的区别是什么？Iterator的安全失败是基于对底层集合做拷贝，因此，它不受源集合上修改的影响。java.util包下面的所有的集合类都是快速失败的，而java.util.concurrent包下面的所有的类都是安全失败的。快速失败的迭代器会抛出ConcurrentModificationException异常，而安全失败的迭代器永远不会抛出这样的异常。十六、final的作用在 Java 中，声明类、变量和方法时，可使用关键字 final 来修饰。final 所修饰的数据具有“终态”的特征，表示“最终的”意思。具体规定如下：final 修饰的类不能被继承。final 修饰的方法不能被子类重写。final 修饰的变量（成员变量或局部变量）即成为常量，只能赋值一次。final 修饰的成员变量必须在声明的同时赋值，如果在声明的时候没有赋值，那么只有 一次赋值的机会，而且只能在构造方法中显式赋值，然后才能使用。final 修饰的局部变量可以只声明不赋值，然后再进行一次性的赋值。final 一般用于修饰那些通用性的功能、实现方式或取值不能随意被改变的数据，以避免被误用，例如实现数学三角方法、幂运算等功能的方法，以及数学常量π=3.141593、e=2.71828 等。事实上，为确保终态性，提供了上述方法和常量的 java.lang.Math 类也已被定义为final 的。需要注意的是，如果将引用类型（任何类的类型）的变量标记为 final，那么该变量不能指向任何其它对象。但可以改变对象的内容，因为只有引用本身是 final 的。如果变量被标记为 final，其结果是使它成为常数。想改变 final 变量的值会导致一个编译错误。下面是一个正确定义 final 变量的例子：1public final int MAX_ARRAY_SIZE = 25; // 常量名一般大写常量因为有 final 修饰，所以不能被继承。请看下面的代码：12345678910111213141516public final class Demo&#123; public static final int TOTAL_NUMBER = 5; public int id; public Demo() &#123; // 非法，对final变量TOTAL_NUMBER进行二次赋值了 // 因为++TOTAL_NUMBER相当于 TOTAL_NUMBER=TOTAL_NUMBER+1 id = ++TOTAL_NUMBER; &#125; public static void main(String[] args) &#123; final Demo t = new Demo(); final int i = 10; final int j; j = 20; j = 30; // 非法，对final变量进行二次赋值 &#125;&#125;final 也可以用来修饰类（放在 class 关键字前面），阻止该类再派生出子类，例如 Java.lang.String 就是一个 final 类。这样做是出于安全原因，因为要保证一旦有字符串的引用，就必须是类 String 的字符串，而不是某个其它类的字符串（String 类可能被恶意继承并篡改）。方法也可以被 final 修饰，被 final 修饰的方法不能被覆盖；变量也可以被 final 修饰，被 final 修饰的变量在创建对象以后就不允许改变它们的值了。一旦将一个类声明为 final，那么该类包含的方法也将被隐式地声明为 final，但是变量不是。被 final 修饰的方法为静态绑定，不会产生多态（动态绑定），程序在运行时不需要再检索方法表，能够提高代码的执行效率。在Java中，被 static 或 private 修饰的方法会被隐式的声明为 final，因为动态绑定没有意义。由于动态绑定会消耗资源并且很多时候没有必要，所以有一些程序员认为：除非有足够的理由使用多态性，否则应该将所有的方法都用 final 修饰。这样的认识未免有些偏激，因为 JVM 中的即时编译器能够实时监控程序的运行信息，可以准确的知道类之间的继承关系。如果一个方法没有被覆盖并且很短，编译器就能够对它进行优化处理，这个过程为称为内联(inlining)。例如，内联调用 e.getName() 将被替换为访问 e.name 变量。这是一项很有意义的改进，这是由于CPU在处理调用方法的指令时，使用的分支转移会扰乱预取指令的策略，所以，这被视为不受欢迎的。然而，如果 getName() 在另外一个类中被覆盖，那么编译器就无法知道覆盖的代码将会做什么操作，因此也就不能对它进行内联处理了。十七、String 与StringBuffer的区别，有什么好处？String 的值是不可变的，每次对String的操作都会生成新的String对象，不仅效率低，而且耗费大量内存空间。StringBuffer类和String类一样，也用来表示字符串，但是StringBuffer的内部实现方式和String不同，在进行字符串处理时，不生成新的对象，在内存使用上要优于String。StringBuffer 默认分配16字节长度的缓冲区，当字符串超过该大小时，会自动增加缓冲区长度，而不是生成新的对象。StringBuffer不像String，只能通过 new 来创建对象，不支持简写方式，例如：1234StringBuffer str1 = new StringBuffer(); // 分配16个字节长度的缓冲区StringBuffer str2 = =new StringBuffer(512); // 分配512个字节长度的缓冲区// 在缓冲区中存放了字符串，并在后面预留了16个字节长度的空缓冲区StringBuffer str3 = new StringBuffer(&quot;www.baidu.com&quot;);17.1 StringBuffer类的主要方法StringBuffer类中的方法主要偏重于对于字符串的操作，例如追加、插入和删除等，这个也是StringBuffer类和String类的主要区别。实际开发中，如果需要对一个字符串进行频繁的修改，建议使用 StringBuffer。1) append() 方法append() 方法用于向当前字符串的末尾追加内容，类似于字符串的连接。调用该方法以后，StringBuffer对象的内容也发生改变，例如：12StringBuffer str = new StringBuffer(“biancheng100”);str.append(true);则对象str的值将变成”biancheng100true”。注意是str指向的内容变了，不是str的指向变了。字符串的”+“操作实际上也是先创建一个StringBuffer对象，然后调用append()方法将字符串片段拼接起来，最后调用toString()方法转换为字符串。这样看来，String的连接操作就比StringBuffer多出了一些附加操作，效率上必然会打折扣。但是，对于长度较小的字符串，”+“操作更加直观，更具可读性，有些时候可以稍微牺牲一下效率。2) deleteCharAt()deleteCharAt() 方法用来删除指定位置的字符，并将剩余的字符形成新的字符串。例如：12StringBuffer str = new StringBuffer(&quot;abcdef&quot;);str. deleteCharAt(3);该代码将会删除索引值为3的字符，即”d“字符。你也可以通过delete()方法一次性删除多个字符，例如：12StingBuffer str = new StringBuffer(&quot;abcdef&quot;);str.delete(1, 4);该代码会删除索引值为1~4之间的字符，包括索引值1，但不包括4。3) insert()方法insert() 用来在指定位置插入字符串，可以认为是append()的升级版。例如：12StringBuffer str = new StringBuffer(&quot;abcdef&quot;);str.insert(3, &quot;xyz&quot;);最后str所指向的字符串为 abcdxyzef。4) setCharAt() 方法setCharAt() 方法用来修改指定位置的字符。例如：12StringBuffer str = new StringBuffer(&quot;abcdef&quot;);str.setCharAt(3, &apos;z&apos;);该代码将把索引值为3的字符修改为 z，最后str所指向的字符串为 abczef。以上仅仅是部分常用方法的简单说明，更多方法和解释请查阅API文档。17.2 String和StringBuffer的效率对比为了更加明显地看出它们的执行效率，下面的代码，将26个英文字母加了10000次。123456789101112131415161718192021222324public class Demo &#123; public static void main(String[] args)&#123; String fragment = "abcdefghijklmnopqrstuvwxyz"; int times = 10000; // 通过String对象 long timeStart1 = System.currentTimeMillis(); String str1 = ""; for (int i=0; i&lt;times; i++) &#123; str1 += fragment; &#125; long timeEnd1 = System.currentTimeMillis(); System.out.println("String: " + (timeEnd1 - timeStart1) + "ms"); // 通过StringBuffer long timeStart2 = System.currentTimeMillis(); StringBuffer str2 = new StringBuffer(); for (int i=0; i&lt;times; i++) &#123; str2.append(fragment); &#125; long timeEnd2 = System.currentTimeMillis(); System.out.println("StringBuffer: " + (timeEnd2 - timeStart2) + "ms"); &#125;&#125;运行结果：12String: 5287msStringBuffer: 3ms结论很明显，StringBuffer的执行效率比String快上千倍，这个差异随着叠加次数的增加越来越明显，当叠加次数达到30000次的时候，运行结果为：12String: 35923msStringBuffer: 8ms所以，强烈建议在涉及大量字符串操作时使用StringBuffer。17.3 StringBuilder类StringBuilder类和StringBuffer类功能基本相似，方法也差不多，主要区别在于StringBuffer类的方法是多线程安全的，而StringBuilder不是线程安全的，相比而言，StringBuilder类会略微快一点。StringBuffer、StringBuilder、String中都实现了CharSequence接口。CharSequence是一个定义字符串操作的接口，它只包括length()、charAt(int index)、subSequence(int start, int end) 这几个API。StringBuffer、StringBuilder、String对CharSequence接口的实现过程不一样，如下图所示：可见，String直接实现了CharSequence接口；StringBuilder 和 StringBuffer都是可变的字符序列，它们都继承于AbstractStringBuilder，实现了CharSequence接口。总结一下：线程安全：StringBuffer：线程安全StringBuilder：线程不安全速度：一般情况下，速度从快到慢为 StringBuilder &gt; StringBuffer &gt; String，当然这是相对的，不是绝对的。使用环境：操作少量的数据使用 String；单线程操作大量数据使用 StringBuilder；多线程操作大量数据使用 StringBuffer。十九、异常处理机制，，try和finally里面都有return的时候，会不会执行finally的return。二十、 synchronized和lock的区别动态绑定与静态绑定：volatile是什么，threadlocal是什么volatile和synchronized的区别垃圾回收机制1.进程与线程的区别，线程的同步问题，两个线程访问一个临界资源该怎么做？线程什么时候终止？（还问到一个daemon函数，我当时完全不知道这是什么）1.什么是面向对象？JAVA与C相比有什么区别？JAVA的对象与c的结构体有什么区别？2.JAVA的IO有哪些类？接口？关系是啥？（谁继承谁之类的）3.你用过哪些JAVA的库？（java.io, java.util, 等等）你什么时候开始用JAVA的？（这个问题是最开始问的，我说14年开始的。。后面又问了一遍。。）然后问了高并发问题，讲讲CurrentHashMap原理，可是我并不了解，就大概说了下自己想法]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>QA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（9）：内部类、抽象类、接口]]></title>
    <url>%2F2017%2F08%2F28%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%889%EF%BC%89%EF%BC%9A%E5%86%85%E9%83%A8%E7%B1%BB%E3%80%81%E6%8A%BD%E8%B1%A1%E7%B1%BB%E3%80%81%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[一、Java内部类及其实例化在 Java 中，允许在一个类（或方法、语句块）的内部定义另一个类，称为内部类(Inner Class)，有时也称为嵌套类(Nested Class)。 内部类和外层封装它的类之间存在逻辑上的所属关系，一般只用在定义它的类或语句块之内，实现一些没有通用意义的功能逻辑，在外部引用它时必须给出完整的名称。 使用内部类的主要原因有： 内部类可以访问外部类中的数据，包括私有的数据。 内部类可以对同一个包中的其他类隐藏起来。 当想要定义一个回调函数且不想编写大量代码时，使用匿名(anonymous)内部类比较便捷。 减少类的命名冲突。 请看下面的例子： 123456789101112131415161718public class Demo &#123; private int size; public class Inner &#123; private int counter = 10; public void doStuff() &#123; size++; &#125; &#125; public static void main(String args[]) &#123; Demo outer = new Demo(); Inner inner = outer.new Inner(); inner.doStuff(); System.out.println(outer.size); System.out.println(inner.counter); // 编译错误，外部类不能访问内部类的变量// System.out.println(counter); &#125;&#125; 这段代码定义了一个外部类 Outer，它包含了一个内部类 Inner。将错误语句注释掉，编译，会生成两个 .class 文件：Outer.class 和 Outer`Inner.class。也就是说，内部类会被编译成独立的字节码文件。内部类是一种编译器现象，与虚拟机无关。编译器将会把内部类翻译成用` 符号分隔外部类名与内部类名的常规类文件，而虚拟机则对此一无所知。注意：必须先有外部类的对象才能生成内部类的对象，因为内部类需要访问外部类中的成员变量，成员变量必须实例化才有意义。 内部类是 Java 1.1 的新增特性，有些程序员认为这是一个值得称赞的进步，但是内部类的语法很复杂，严重破坏了良好的代码结构， 违背了Java要比C++更加简单的设计理念。 内部类看似增加了—些优美有趣，实属没必要的特性，这是不是也让Java开始走上了许多语言饱受折磨的毁灭性道路呢？本教程并不打算就这个问题给予一个肯定的答案。 二、内部类的分类内部类可以是静态(static)的，可以使用 public、protected 和 private 访问控制符，而外部类只能使用 public，或者默认。 2.1 成员式内部类在外部类内部直接定义（不在方法内部或代码块内部）的类就是成员式内部类，它可以直接使用外部类的所有变量和方法，即使是 private 的。外部类要想访问内部类的成员变量和方法，则需要通过内部类的对象来获取。 请看下面的代码： 三、抽象类的概念和使用在自上而下的继承层次结构中，位于上层的类更具有通用性，甚至可能更加抽象。从某种角度看，祖先类更加通用，它只包含一些最基本的成员，人们只将它作为派生其他类的基类，而不会用来创建对象。甚至，你可以只给出方法的定义而不实现，由子类根据具体需求来具体实现。 这种只给出方法定义而不具体实现的方法被称为抽象方法，抽象方法是没有方法体的，在代码的表达上就是没有“{}”。包含一个或多个抽象方法的类也必须被声明为抽象类。 使用 abstract 修饰符来表示抽象方法和抽象类。 抽象类除了包含抽象方法外，还可以包含具体的变量和具体的方法。类即使不包含抽象方法，也可以被声明为抽象类，防止被实例化。 抽象类不能被实例化，抽象方法必须在子类中被实现。请看下面的代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243import static java.lang.System.*;public class Demo&#123; public static void main(String[] args) &#123; Teacher t = new Teacher(); t.setName(&quot;王明&quot;); t.work(); Driver d = new Driver(); d.setName(&quot;小陈&quot;); d.work(); &#125;&#125;// 定义一个抽象类abstract class People&#123; private String name; // 实例变量 // 共有的 setter 和 getter 方法 public void setName(String name)&#123; this.name = name; &#125; public String getName()&#123; return this.name; &#125; // 抽象方法 public abstract void work();&#125;class Teacher extends People&#123; // 必须实现该方法 public void work()&#123; out.println(&quot;我的名字叫&quot; + this.getName() + &quot;，我正在讲课，请大家不要东张西望...&quot;); &#125;&#125;class Driver extends People&#123; // 必须实现该方法 public void work()&#123; out.println(&quot;我的名字叫&quot; + this.getName() + &quot;，我正在开车，不能接听电话...&quot;); &#125;&#125; 运行结果：我的名字叫王明，我正在讲课，请大家不要东张西望…我的名字叫小陈，我正在开车，不能接听电话… 关于抽象类的几点说明： 抽象类不能直接使用，必须用子类去实现抽象类，然后使用其子类的实例。然而可以创建一个变量，其类型是一个抽象类，并让它指向具体子类的一个实例，也就是可以使用抽象类来充当形参，实际实现类作为实参，也就是多态的应用。 不能有抽象构造方法或抽象静态方法。 在下列情况下，一个类将成为抽象类： 当一个类的一个或多个方法是抽象方法时； 当类是一个抽象类的子类，并且不能为任何抽象方法提供任何实现细节或方法主体时； 当一个类实现一个接口，并且不能为任何抽象方法提供实现细节或方法主体时；注意： 这里说的是这些情况下一个类将成为抽象类，没有说抽象类一定会有这些情况。一个典型的错误：抽象类一定包含抽象方法。 但是反过来说“包含抽象方法的类一定是抽象类”就是正确的。事实上，抽象类可以是一个完全正常实现的类 四、接口的概念和使用4.1 接口的概念在抽象类中，可以包含一个或多个抽象方法；但在接口(interface)中，所有的方法必须都是抽象的，不能有方法体，它比抽象类更加“抽象”。 接口使用 interface 关键字来声明，可以看做是一种特殊的抽象类，可以指定一个类必须做什么，而不是规定它如何去做。 现实中也有很多接口的实例，比如说串口电脑硬盘，Serial ATA委员会指定了Serial ATA 2.0规范，这种规范就是接口。Serial ATA委员会不负责生产硬盘，只是指定通用的规范。 希捷、日立、三星等生产厂家会按照规范生产符合接口的硬盘，这些硬盘就可以实现通用化，如果正在用一块160G日立的串口硬盘，现在要升级了，可以购买一块320G的希捷串口硬盘，安装上去就可以继续使用了。 下面的代码可以模拟Serial ATA委员会定义以下串口硬盘接口： 12345678public interface Demo&#123; //连接线的数量 public static final int CONNECT_LINE=4; //写数据 public void writeData(String data); //读数据 public String readData();&#125; 注意：接口中声明的成员变量默认都是 public static final 的，必须显式地初始化。因而在常量声明时可以省略这些修饰符。 接口是若干常量和抽象方法的集合，目前看来和抽象类差不多。确实如此，接口本就是从抽象类中演化而来的，因而除特别规定，接口享有和类同样的“待遇”。比如，源程序中可以定义多个类或接口，但最多只能有一个public 的类或接口，如果有则源文件必须取和public的类和接口相同的名字。和类的继承格式一样，接口之间也可以继承，子接口可以继承父接口中的常量和抽象方法并添加新的抽象方法等。 但接口有其自身的一些特性，归纳如下。 1) 接口中只能定义抽象方法，这些方法默认为 public abstract 的，因而在声明方法时可以省略这些修饰符。试图在接口中定义实例变量、非抽象的实例方法及静态方法，都是非法的。例如： 12345678910public interface SataHdd&#123; //连接线的数量 public int connectLine; //编译出错，connectLine被看做静态常量，必须显式初始化 //写数据 protected void writeData(String data); //编译出错，必须是public类型 //读数据 public static String readData()&#123; //编译出错，接口中不能包含静态方法 return &quot;数据&quot;; //编译出错，接口中只能包含抽象方法， &#125;&#125; 2) 接口中没有构造方法，不能被实例化。 3) 一个接口不实现另一个接口，但可以继承多个其他接口。接口的多继承特点弥补了类的单继承。例如： 123456789101112131415//串行硬盘接口public interface SataHdd extends A,B&#123; // 连接线的数量 public static final int CONNECT_LINE = 4; // 写数据 public void writeData(String data); // 读数据 public String readData();&#125;interface A&#123; public void a();&#125;interface B&#123; public void b();&#125; 4.2 为什么使用接口大型项目开发中，可能需要从继承链的中间插入一个类，让它的子类具备某些功能而不影响它们的父类。例如 A -&gt; B -&gt; C -&gt; D -&gt; E，A 是祖先类，如果需要为C、D、E类添加某些通用的功能，最简单的方法是让C类再继承另外一个类。但是问题来了，Java 是一种单继承的语言，不能再让C继承另外一个父类了，只到移动到继承链的最顶端，让A再继承一个父类。这样一来，对C、D、E类的修改，影响到了整个继承链，不具备可插入性的设计。 接口是可插入性的保证。在一个继承链中的任何一个类都可以实现一个接口，这个接口会影响到此类的所有子类，但不会影响到此类的任何父类。此类将不得不实现这个接口所规定的方法，而子类可以从此类自动继承这些方法，这时候，这些子类具有了可插入性。 我们关心的不是哪一个具体的类，而是这个类是否实现了我们需要的接口。 接口提供了关联以及方法调用上的可插入性，软件系统的规模越大，生命周期越长，接口使得软件系统的灵活性和可扩展性，可插入性方面得到保证。 接口在面向对象的 Java 程序设计中占有举足轻重的地位。事实上在设计阶段最重要的任务之一就是设计出各部分的接口，然后通过接口的组合，形成程序的基本框架结构。 4.3 接口的使用接口的使用与类的使用有些不同。在需要使用类的地方，会直接使用new关键字来构建一个类的实例，但接口不可以这样使用，因为接口不能直接使用 new 关键字来构建实例。 接口必须通过类来实现(implements)它的抽象方法，然后再实例化类。类实现接口的关键字为implements。 如果一个类不能实现该接口的所有抽象方法，那么这个类必须被定义为抽象方法。 不允许创建接口的实例，但允许定义接口类型的引用变量，该变量指向了实现接口的类的实例。 一个类只能继承一个父类，但却可以实现多个接口。 实现接口的格式如下：修饰符 class 类名 extends 父类 implements 多个接口 {实现方法} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import static java.lang.System.*;public class Demo&#123; public static void main(String[] args) &#123; SataHdd sh1=new SeagateHdd(); //初始化希捷硬盘 SataHdd sh2=new SamsungHdd(); //初始化三星硬盘 &#125;&#125;//串行硬盘接口interface SataHdd&#123; //连接线的数量 public static final int CONNECT_LINE=4; //写数据 public void writeData(String data); //读数据 public String readData();&#125;// 维修硬盘接口interface fixHdd&#123; // 维修地址 String address = &quot;北京市海淀区&quot;; // 开始维修 boolean doFix();&#125;//捷硬盘class SeagateHdd implements SataHdd, fixHdd&#123; //希捷硬盘读取数据 public String readData()&#123; return &quot;数据&quot;; &#125; //希捷硬盘写入数据 public void writeData(String data) &#123; out.println(&quot;写入成功&quot;); &#125; // 维修希捷硬盘 public boolean doFix()&#123; return true; &#125;&#125;//三星硬盘class SamsungHdd implements SataHdd&#123; //三星硬盘读取数据 public String readData()&#123; return &quot;数据&quot;; &#125; //三星硬盘写入数据 public void writeData(String data)&#123; out.println(&quot;写入成功&quot;); &#125;&#125;//某劣质硬盘，不能写数据abstract class XXHdd implements SataHdd&#123; //硬盘读取数据 public String readData() &#123; return &quot;数据&quot;; &#125;&#125; 4.4 接口作为类型使用接口作为引用类型来使用，任何实现该接口的类的实例都可以存储在该接口类型的变量中，通过这些变量可以访问类中所实现的接口中的方法，Java 运行时系统会动态地确定应该使用哪个类中的方法，实际上是调用相应的实现类的方法。 示例如下： 12345678910111213141516171819public class Demo&#123; public void test1(A a) &#123; a.doSth(); &#125; public static void main(String[] args) &#123; Demo d = new Demo(); A a = new B(); d.test1(a); &#125;&#125;interface A &#123; public int doSth();&#125;class B implements A &#123; public int doSth() &#123; System.out.println("now in B"); return 123; &#125;&#125; 运行结果：now in B 大家看到接口可以作为一个类型来使用，把接口作为方法的参数和返回类型。 五、接口和抽象类的区别类是对象的模板，抽象类和接口可以看做是具体的类的模板。 由于从某种角度讲，接口是一种特殊的抽象类，它们的渊源颇深，有很大的相似之处，所以在选择使用谁的问题上很容易迷糊。我们首先分析它们具有的相同点。 都代表类树形结构的抽象层。在使用引用变量时，尽量使用类结构的抽象层，使方法的定义和实现分离，这样做对于代码有松散耦合的好处。 都不能被实例化。 都能包含抽象方法。抽象方法用来描述系统提供哪些功能，而不必关心具体的实现。 下面说一下抽象类和接口的主要区别。 1) 抽象类可以为部分方法提供实现，避免了在子类中重复实现这些方法，提高了代码的可重用性，这是抽象类的优势；而接口中只能包含抽象方法，不能包含任何实现。 12345678910111213141516public abstract class A&#123; public abstract void method1(); public void method2()&#123; //A method2 &#125;&#125;public class B extends A&#123; public void method1()&#123; //B method1 &#125;&#125;public class C extends A&#123; public void method1()&#123; //C method1 &#125;&#125; 抽象类A有两个子类B、C，由于A中有方法method2的实现，子类B、C中不需要重写method2方法，我们就说A为子类提供了公共的功能，或A约束了子类的行为。method2就是代码可重用的例子。A 并没有定义 method1的实现，也就是说B、C 可以根据自己的特点实现method1方法，这又体现了松散耦合的特性。 再换成接口看看： 1234567891011121314151617181920public interface A&#123; public void method1(); public void method2();&#125;public class B implements A&#123; public void method1()&#123; //B method1 &#125; public void method2()&#123; //B method2 &#125;&#125;public class C implements A&#123; public void method1()&#123; //C method1 &#125; public void method2()&#123; //C method2 &#125;&#125; 接口A无法为实现类B、C提供公共的功能，也就是说A无法约束B、C的行为。B、C可以自由地发挥自己的特点现实 method1和 method2方法，接口A毫无掌控能力。 2) 一个类只能继承一个直接的父类（可能是抽象类），但一个类可以实现多个接口，这个就是接口的优势。 12345678910111213141516171819202122232425262728293031interface A&#123; public void method2();&#125;interface B&#123; public void method1();&#125;class C implements A,B&#123; public void method1()&#123; //C method1 &#125; public void method2()&#123; //C method2 &#125;&#125;//可以如此灵活的使用C，并且C还有机会进行扩展，实现其他接口A a=new C();B b=new C();abstract class A&#123; public abstract void method1();&#125;abstract class B extends A&#123; public abstract void method2();&#125;class C extends B&#123; public void method1()&#123; //C method1 &#125; public void method2() &#123; //C method2 &#125;&#125; 对于C类，将没有机会继承其他父类了。 综上所述，接口和抽象类各有优缺点，在接口和抽象类的选择上，必须遵守这样一个原则： 行为模型应该总是通过接口而不是抽象类定义，所以通常是优先选用接口，尽量少用抽象类。 选择抽象类的时候通常是如下情况：需要定义子类的行为，又要为子类提供通用的功能。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>内部类</tag>
        <tag>抽象类</tag>
        <tag>接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（8）：常用库类、向量与哈希]]></title>
    <url>%2F2017%2F08%2F27%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%888%EF%BC%89%EF%BC%9A%E5%B8%B8%E7%94%A8%E5%BA%93%E7%B1%BB%E3%80%81%E5%90%91%E9%87%8F%E4%B8%8E%E5%93%88%E5%B8%8C%2F</url>
    <content type="text"><![CDATA[一、Java基础类库Java 的类库是 Java 语言提供的已经实现的标准类的集合，是 Java 编程的 API（Application Program Interface），它可以帮助开发者方便、快捷地开发 Java 程序。这些类根据实现的功能不同，可以划分为不同的集合，每个集合组成一个包，称为类库。Java 类库中大部分都是由Sun 公司提供的，这些类库称为基础类库。 Java 语言中提供了大量的类库共程序开发者来使用，了解类库的结构可以帮助开发者节省大量的编程时间，而且能够使编写的程序更简单更实用。Java 中丰富的类库资源也是 Java 语言的一大特色，是 Java 程序设计的基础。 Java 常用包的简单介绍如下： java.lang 包：主要含有与语言相关的类。java.lang 包由解释程序自动加载，不需要显示说明。 java.io 包：主要含有与输入/输出相关的类，这些类提供了对不同的输入和输出设备读写数据的支持，这些输入和输出设备包括键盘、显示器、打印机、磁盘文件等。 java.util 包：包括许多具有特定功能的类，有日期、向量、哈希表、堆栈等，其中 Date类支持与时间有关的操作。 java.swing 包和 java.awt 包：提供了创建图形用户界面元素的类。通过这些元素，编程者可以控制所写的 Applet 或 Application 的外观界面。包中包含了窗口、对话框、菜单等类。 java.net 包：含有与网络操作相关的类，如 TCP Scokets、URL 等工具。 java.applet 包：含有控制 HTML 文档格式、应用程序中的声音等资源的类，其中 Applet类是用来创建包含于 HTML 的 Applet 必不可少的类。 java.beans 包：定义了应用程序编程接口（API），Java Beans 是 Java 应用程序环境的中性平台组件结构。 二、Java语言包(java.lang)简介Java语言包（java.lang）定义了Java中的大多数基本类，由Java语言自动调用，不需要显示声明。该包中包含了Object类，Object类是整个类层次结构的根结点，同时还定义了基本数据类型的类，如：String、Boolean、Byter、Short等。这些类支持数字类型的转换和字符串的操作等，下面将进行简单介绍。 Math类提供了常用的数学运算方法以及Math.PI和Math.E两个数学常量。该类是final的，不能被继承，类中的方法和属性全部是静态，不允许在类的外部创建Math类的对象。因此，只能使用Math类的方法而不能对其作任何更改。表8-1列出了Math类的主要方法。 2.1 Math类 Math类的主要方法 方法 功能 int abs(int i) 求整数的绝对值（另有针对long、float、double的方法） double ceil(double d) 不小于d的最小整数（返回值为double型） double floor(double d) 不大于d的最大整数（返回值为double型） int max(int i1,int i2) 求两个整数中最大数（另有针对long、float、double的方法） int min(int i1,int i2) 求两个整数中最小数（另有针对long、float、double的方法） double random() 产生0~1之间的随机数 int round(float f) 求最靠近f的整数 long round(double d) 求最靠近d的长整数 double sqrt(double a) 求平方根 double sin(double d) 求d的sin值（另有求其他三角函数的方法如cos，tan，atan） double log(double x) 求自然对数 double exp(double x) 求e的x次幂（ex） double pow(double a, double b) 求a的b次幂 例如：产生10个10~100之间的随机整数。 123456789101112//********** ep8_2.java **********class Demo&#123; public static void main(String args[])&#123; int a; System.out.print(&quot;随机数为：&quot;); for(int i=1;i&lt;=10;i++)&#123; a=(int)((100-10+1)*Math.random()+10); System.out.print(&quot; &quot;+a); &#125; System.out.println(); &#125;&#125; 2.2 字符串类字符串是字符的序列。在 Java 中，字符串无论是常量还是变量都是用类的对象来实现的。java.lang 提供了两种字符串类：String 类和 StringBuffer 类。 1.String 类 按照 Java 语言的规定，String 类是 immutable 的 Unicode 字符序列，其作用是实现一种不能改变的静态字符串。例如，把两个字符串连接起来的结果是生成一个新的字符串，而不会使原来的字符串改变。实际上，所有改变字符串的结果都是生成新的字符串，而不是改变原来字符串。 字符串与数组的实现很相似，也是通过 index 编号来指出字符在字符串中的位置的，编号从0 开始，第 2 个字符的编号为 1，以此类推。如果要访问的编号不在合法的范围内，系统会产生 StringIndexOutOfBoundsExecption 异常。如果 index 的值不是整数，则会产生编译错误。 String 类提供了如下表所示的几种字符串创建方法。 方法 功能 String s=”Hello!” 用字符串常量自动创建 String 实例。 String s=new String(String s) 通过 String 对象或字符串常量传递给构造方法。 public String(char value[]) 将整个字符数组赋给 String 构造方法。 public String(char value[], int offset, int count) 将字符数组的一部分赋给 String 构造方法，offset 为起始下标，count为子数组长度。 2.StringBuffer 类 String 类不能改变字符串对象中的内容，只能通过建立一个新串来实现字符串的变化。如果字符串需要动态改变，就需要用 StringBuffer 类。StringBuffer 类主要用来实现字符串内容的添加、修改、删除，也就是说该类对象实体的内存空间可以自动改变大小，以便于存放一个可变的字符序列。 StringBuffer 类提供的三种构造方法： 构造方法 说明 StringBuffer() 使用该无参数的构造方法创建的 StringBuffer 对象，初始容量为 16 个字符，当对象存放的字符序列大于 16 个字符时，对象的容量自动增加。该对象可以通过 length()方法获取实体中存放的字符序列的长度，通过 capacity()方法获取当前对象的实际容量。 StringBuffer(int length) 使用该构造方法创建的 StringBuffer 对象，其初始容量为参数 length 指定的字符个数，当对象存放的字符序列的长度大于 length 时，对象的容量自动增加，以便存放所增加的字符。 StringBuffer(Strin str) 使用该构造方法创建的 StringBuffer 对象，其初始容量为参数字符串 str 的长度再加上 16 个字符。 几种 StringBuffer 类常用的方法 方法 说明 append() 使用 append() 方法可以将其他 Java 类型数据转化为字符串后再追加到 StringBuffer 的对象中。 insert(int index, String str) insert() 方法将一个字符串插入对象的字符序列中的某个位置。 setCharAt(int n, char ch) 将当前 StringBuffer 对象中的字符序列 n 处的字符用参数 ch 指定的字符替换，n 的值必须是非负的，并且小于当前对象中字符串序列的长度。 reverse() 使用 reverse()方法可以将对象中的字符序列翻转。 delete(int n, int m) 从当前 StringBuffer 对象中的字符序列删除一个子字符序列。这里的 n 指定了需要删除的第一个字符的下标，m 指定了需要删除的最后一个字符的下一个字符的下标，因此删除的子字符串从 n~m-1。 replace(int n, int m, String str) 用 str 替换对象中的字符序列，被替换的子字符序列由下标 n 和 m 指定。 三、日期和时间类Java 的日期和时间类位于 java.util 包中。利用日期时间类提供的方法，可以获取当前的日期和时间，创建日期和时间参数，计算和比较时间。 3.1 Date 类Date 类是 Java 中的日期时间类，其构造方法比较多，下面是常用的两个： Date()：使用当前的日期和时间初始化一个对象。 Date(long millisec)：从1970年01月01日00时（格林威治时间）开始以毫秒计算时间，计算 millisec 毫秒。如果运行 Java 程序的本地时区是北京时区（与格林威治时间相差 8 小时），Date dt1=new Date(1000);，那么对象 dt1 就是1970年01月01日08时00分01秒。 请看一个显示日期时间的例子： 123456789import java.util.Date;public class Demo&#123; public static void main(String args[])&#123; Date da=new Date(); //创建时间对象 System.out.println(da); //显示时间和日期 long msec=da.getTime(); System.out.println(&quot;从1970年1月1日0时到现在共有：&quot; + msec + &quot;毫秒&quot;); &#125;&#125; 运行结果： 12Mon Feb 05 22:50:05 CST 2007从1970年1月1日0时到现在共有：1170687005390 毫秒 一些比较常用的 Date 类方法： 方法 功能 boolean after(Date date) 若调用 Date 对象所包含的日期比 date 指定的对象所包含的日期晚，返回 true，否则返回 false。 boolean before(Date date) 若调用 Date 对象所包含的日期比 date 指定的对象所包含的日期早，返回 true，否则返回 false。 Object clone() 复制调用 Date 对象。 int compareTo(Date date) 比较调用对象所包含的日期和指定的对象包含的日期，若相等返回 0；若前者比后者早，返回负值；否则返回正值。 long getTime() 以毫秒数返回从 1970 年 01 月 01 日 00 时到目前的时间。 int hashCode() 返回调用对象的散列值。 void setTime(long time) 根据 time 的值，设置时间和日期。time 值从 1970 年 01 月 01 日 00 时开始计算。 String toString() 把调用的 Date 对象转换成字符串并返回结果。 public Static String valueOf(type variable) 把 variable 转换为字符串。 Date 对象表示时间的默认顺序是星期、月、日、小时、分、秒、年。若需要修改时间显示的格式可以使用“SimpleDateFormat(String pattern)”方法。 例如，用不同的格式输出时间： 123456789101112import java.util.Date;import java.text.SimpleDateFormat;public class Demo&#123; public static void main(String args[])&#123; Date da=new Date(); System.out.println(da); SimpleDateFormat ma1=new SimpleDateFormat(&quot;yyyy 年 MM 月 dd 日 E 北京时间&quot;); System.out.println(ma1.format(da)); SimpleDateFormat ma2=new SimpleDateFormat(&quot;北京时间：yyyy 年 MM 月 dd 日 HH 时 mm 分 ss 秒&quot;); System.out.println(ma2.format(-1000)); &#125;&#125; 运行结果： 123Sun Jan 04 17:31:36 CST 20152015 年 01 月 04 日 星期日 北京时间北京时间：1970 年 01 月 01 日 07 时 59 分 59 秒 3.2 Calendar 类抽象类 Calendar 提供了一组方法，允许把以毫秒为单位的时间转换成一些有用的时间组成部分。Calendar 不能直接创建对象，但可以使用静态方法 getInstance() 获得代表当前日期的日历对象，如： 1Calendar calendar=Calendar.getInstance(); 该对象可以调用下面的方法将日历翻到指定的一个时间： 123void set(int year,int month,int date);void set(int year,int month,int date,int hour,int minute);void set(int year,int month,int date,int hour,int minute,int second); 若要调用有关年份、月份、小时、星期等信息，可以通过调用下面的方法实现： 1int get(int field); 其中，参数 field 的值由 Calendar 类的静态常量决定。其中：YEAR 代表年，MONTH 代表月，HOUR 代表小时，MINUTE 代表分，如： 1calendar.get(Calendar.MONTH); 如果返回值为 0 代表当前日历是一月份，如果返回 1 代表二月份，依此类推。 由 Calendar 定义的一些常用方法如下表所示： 方法 功能 abstract void add(int which,int val) 将 val 加到 which 所指定的时间或者日期中，如果需要实现减的功能，可以加一个负数。which 必须是 Calendar 类定义的字段之一，如 Calendar.HOUR boolean after(Object calendarObj) 如果调用 Calendar 对象所包含的日期比 calendarObj 指定的对象所包含的日期晚，返回 true，否则返回 false boolean before(Object calendarObj) 如果调用 Calendar 对象所包含的日期比 calendarObj 指定的对象所包含的日期早，返回 true，否则返回 false final void clear() 对调用对象包含的所有时间组成部分清零 final void clear(int which) 对调用对象包含的 which 所指定的时间组成部分清零 boolean equals(Object calendarObj) 如果调用 Calendar 对象所包含的日期和 calendarObj 指定的对象所包含的日期相等，返回 true，否则返回 false int get(int calendarField) 返回调用 Calendar 对象的一个时间组成部分的值，这个组成部分由 calendarField指定，可以被返回的组成部分如：Calendar.YEAR，Calendar.MONTH 等 static Calendar getInstance() 返回使用默认地域和时区的一个 Calendar 对象 final Date getTime() 返回一个和调用对象时间相等的 Date 对象 final boolean isSet(int which) 如果调用对象所包含的 which 指定的时间部分被设置了，返回 true，否则返回 false final void set(int year,int month) 设置调用对象的各种日期和时间部分 final void setTime(Date d) 从 Date 对象 d 中获得日期和时间部分 void setTimeZone(TimeZone t) 设置调用对象的时区为 t 指定的那个时区 3.3 GregorianCalendar 类GregorianCalendar 是一个具体实现 Calendar 类的类，该类实现了公历日历。Calendar 类的 getInstance() 方法返回一个 GregorianCalendar，它被初始化为默认的地域和时区下的当前日期和时间。 GregorianCalendar 类定义了两个字段：AD 和 BC，分别代表公元前和公元后。其默认的构造方法 GregorianCalendar() 以默认的地域和时区的当前日期和时间初始化对象，另外也可以指定地域和时区来建立一个 GregorianCalendar 对象，例如： 123GregorianCalendar(Locale locale);GregorianCalendar(TimeZone timeZone);GregorianCalendar(TimeZone timeZone,Locale locale); GregorianCalendar 类提供了 Calendar 类中所有的抽象方法的实现，同时还提供了一些附加的方法，其中用来判断闰年的方法为： 1Boolean isLeapYear(int year); 如果 year 是闰年，该方法返回 true，否则返回 false。 四、哈希表及其应用哈希表也称为散列表，是用来存储群体对象的集合类结构。 4.1 什么是哈希表数组和向量都可以存储对象，但对象的存储位置是随机的，也就是说对象本身与其存储位置之间没有必然的联系。当要查找一个对象时，只能以某种顺序（如顺序查找或二分查找）与各个元素进行比较，当数组或向量中的元素数量很多时，查找的效率会明显的降低。 一种有效的存储方式，是不与其他元素进行比较，一次存取便能得到所需要的记录。这就需要在对象的存储位置和对象的关键属性（设为 k）之间建立一个特定的对应关系（设为 f），使每个对象与一个唯一的存储位置相对应。在查找时，只要根据待查对象的关键属性 k 计算f(k)的值即可。如果此对象在集合中，则必定在存储位置 f(k)上，因此不需要与集合中的其他元素进行比较。称这种对应关系 f 为哈希（hash）方法，按照这种思想建立的表为哈希表。 Java 使用哈希表类（Hashtable）来实现哈希表，以下是与哈希表相关的一些概念： 容量（Capacity）：Hashtable 的容量不是固定的，随对象的加入其容量也可以自动增长。 关键字（Key）：每个存储的对象都需要有一个关键字，key 可以是对象本身，也可以是对象的一部分（如某个属性）。要求在一个 Hashtable 中的所有关键字都是唯一的。 哈希码（Hash Code）：若要将对象存储到 Hashtable 上，就需要将其关键字 key 映射到一个整型数据，成为 key 的哈希码。 项（Item）：Hashtable 中的每一项都有两个域，分别是关键字域 key 和值域 value（存储的对象）。Key 和 value 都可以是任意的 Object 类型的对象，但不能为空。 装填因子（Load Factor）：装填因子表示为哈希表的装满程度，其值等于元素数比上哈希表的长度。 4.2 哈希表的使用哈希表类主要有三种形式的构造方法： Hashtable(); //默认构造函数，初始容量为 101，最大填充因子 0.75 Hashtable(int capacity); Hashtable(int capacity,float loadFactor) 哈希表类的主要方法如下表所示。 方法 功能 void clear() 重新设置并清空哈希表 boolean contains(Object value) 确定哈希表内是否包含了给定的对象，若有返回 true，否则返回 false boolean containsKey(Object key) 确定哈希表内是否包含了给定的关键字，若有返回 true，否则返回 false boolean isEmpty() 确认哈希表是否为空，若是返回 true，否则返回 false Object get(Object key) 获取对应关键字的对象，若不存在返回 null void rehash() 再哈希，扩充哈希表使之可以保存更多的元素，当哈希表达到饱和时，系统自动调用此方法 Object put(Object key,Object value) 用给定的关键字把对象保存到哈希表中，此处的关键字和元素均不可为空 Object remove(Object key) 从哈希表中删除与给定关键字相对应的对象，若该对象不存在返回 null int size() 返回哈希表的大小 String toString() 将哈希表内容转换为字符串 哈希表的创建也可以通过 new 操作符实现。其语句为： 1HashTable has=new HashTable(); 哈希表的遍历 123456789101112131415//*** ep8_12.java **********import java.util.*;class Demo&#123; public static void main(String args[])&#123; Hashtable has=new Hashtable(); has.put(&quot;one&quot;,new Integer(1)); has.put(&quot;two&quot;,new Integer(2)); has.put(&quot;three&quot;,new Integer(3)); has.put(&quot;four&quot;,new Double(12.3)); Set s=has.keySet(); for(Iterator&lt;String&gt; i=s.iterator();i.hasNext();)&#123; System.out.println(has.get(i.next())); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>常用库类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（7）：深入理解java异常处理机制]]></title>
    <url>%2F2017%2F08%2F26%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%887%EF%BC%89%EF%BC%9A%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3java%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[一、java异常异常指不期而至的各种状况，如：文件找不到、网络连接失败、非法参数等。异常是一个事件，它发生在程序运行期间，干扰了正常的指令流程。Java通 过API中Throwable类的众多子类描述各种不同的异常。因而，Java异常都是对象，是Throwable子类的实例，描述了出现在一段编码中的 错误条件。当条件生成时，错误将引发异常。 Java异常类层次结构图： 在 Java 中，所有的异常都有一个共同的祖先 Throwable（可抛出）。Throwable 指定代码中可用异常传播机制通过 Java 应用程序传输的任何问题的共性。 1.1 Exception（异常）和 Error（错误） Throwable： 有两个重要的子类：Exception（异常）和 Error（错误），二者都是 Java 异常处理的重要子类，各自都包含大量子类。 Error（错误）:是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。这些错误表示故障发生于虚拟机自身、或者发生在虚拟机试图执行应用时，如Java虚拟机运行错误（Virtual MachineError）、类定义错误（NoClassDefFoundError）等。这些错误是不可查的，因为它们在应用程序的控制和处理能力之 外，而且绝大多数是程序运行时不允许出现的状况。对于设计合理的应用程序来说，即使确实发生了错误，本质上也不应该试图去处理它所引起的异常状况。在 Java中，错误通过Error的子类描述。 Exception（异常）:是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 类及其子类表示“JVM 常用操作”引发的错误。例如，若试图使用空值对象引用、除数为零或数组越界，则分别引发运行时异常（NullPointerException、ArithmeticException）和 ArrayIndexOutOfBoundException。 注意：异常和错误的区别：异常能被程序本身可以处理，错误是无法处理。 1.2 可查异常（checked exceptions）和不可查异常（unchecked exceptions）Java的异常(包括Exception和Error)分为可查的异常（checked exceptions）和不可查的异常（unchecked exceptions）。 可查异常（编译器要求必须处置的异常）： 正确的程序在运行中，很容易出现的、情理可容的异常状况。可查异常虽然是异常状况，但在一定程度上它的发生是可以预计的，而且一旦发生这种异常状况，就必须采取某种方式进行处理。 除了RuntimeException及其子类以外，其他的Exception类及其子类都属于可查异常。这种异常的特点是Java编译器会检查它，也就是说，当程序中可能出现这类异常，要么用try-catch语句捕获它，要么用throws子句声明抛出它，否则编译不会通过。 不可查异常(编译器不要求强制处置的异常): 包括运行时异常（RuntimeException与其子类）和错误（Error）。 1.3 运行时异常和非运行时异常Exception 这种异常分两大类运行时异常和非运行时异常(编译异常)。程序中应当尽可能去处理这些异常。 运行时异常： 都是RuntimeException类及其子类异常，如NullPointerException(空指针异常)、IndexOutOfBoundsException(下标越界异常)等，这些异常是不检查异常，程序中可以选择捕获处理，也可以不处理。这些异常一般是由程序逻辑错误引起的，程序应该从逻辑角度尽可能避免这类异常的发生。运行时异常的特点是Java编译器不会检查它，也就是说，当程序中可能出现这类异常，即使没有用try-catch语句捕获它，也没有用throws子句声明抛出它，也会编译通过。 非运行时异常 （编译异常）： 是RuntimeException以外的异常，类型上都属于Exception类及其子类。从程序语法角度讲是必须进行处理的异常，如果不处理，程序就不能编译通过。如IOException、SQLException等以及用户自定义的Exception异常，一般情况下不自定义检查异常。 二、处理异常机制在Java应用程序中，异常处理机制为：抛出异常，捕捉异常。 抛出异常： 当一个方法出现错误引发异常时，方法创建异常对象并交付运行时系统，异常对象中包含了异常类型和异常出现时的程序状态等异常信息。运行时系统负责寻找处置异常的代码并执行。 捕获异常： 在方法抛出异常之后，运行时系统将转为寻找合适的异常处理器（exception handler）。潜在的异常处理器是异常发生时依次存留在调用栈中的方法的集合。当异常处理器所能处理的异常类型与方法抛出的异常类型相符时，即为合适 的异常处理器。运行时系统从发生异常的方法开始，依次回查调用栈中的方法，直至找到含有合适异常处理器的方法并执行。当运行时系统遍历调用栈而未找到合适 的异常处理器，则运行时系统终止。同时，意味着Java程序的终止。 对于运行时异常、错误或可查异常，Java技术所要求的异常处理方式有所不同。 由于运行时异常的不可查性，为了更合理、更容易地实现应用程序，Java规定，运行时异常将由Java运行时系统自动抛出，允许应用程序忽略运行时异常。 对于方法运行中可能出现的Error，当运行方法不欲捕捉时，Java允许该方法不做任何抛出声明。因为，大多数Error异常属于永远不能被允许发生的状况，也属于合理的应用程序不该捕捉的异常。 对于所有的可查异常，Java规定：一个方法必须捕捉，或者声明抛出方法之外。也就是说，当一个方法选择不捕捉可查异常时，它必须声明将抛出异常。 能够捕捉异常的方法，需要提供相符类型的异常处理器。所捕捉的异常，可能是由于自身语句所引发并抛出的异常，也可能是由某个调用的方法或者Java运行时 系统等抛出的异常。也就是说，一个方法所能捕捉的异常，一定是Java代码在某处所抛出的异常。简单地说，异常总是先被抛出，后被捕捉的。 任何Java代码都可以抛出异常，如：自己编写的代码、来自Java开发环境包中代码，或者Java运行时系统。无论是谁，都可以通过Java的throw语句抛出异常。 从方法中抛出的任何异常都必须使用throws子句。 捕捉异常通过try-catch语句或者try-catch-finally语句实现。 总体来说，Java规定，对于可查异常必须捕捉、或者声明抛出。允许忽略不可查的RuntimeException和Error。 2.1 捕获异常：try、catch和finally2.1.1 try-catch语句在Java中，异常通过try-catch语句捕获。其一般语法形式为： 12345678try &#123; // 可能会发生异常的程序代码 &#125; catch (Type1 id1)&#123; // 捕获并处置try抛出的异常类型Type1 &#125; catch (Type2 id2)&#123; //捕获并处置try抛出的异常类型Type2 &#125; 关键词try后的一对大括号将一块可能发生异常的代码包起来，称为监控区域。Java方法在运行过程中出现异常，则创建异常对象。将异常抛出监控区域之 外，由Java运行时系统试图寻找匹配的catch子句以捕获异常。若有匹配的catch子句，则运行其异常处理代码，try-catch语句结束。 匹配的原则是：如果抛出的异常对象属于catch子句的异常类，或者属于该异常类的子类，则认为生成的异常对象与catch块捕获的异常类型相匹配。 例1 捕捉throw语句抛出的“除数为0”异常。 123456789101112131415public class TestException &#123; public static void main(String[] args) &#123; int a = 6; int b = 0; try &#123; // try监控区域 if (b == 0) throw new ArithmeticException(); // 通过throw语句抛出异常 System.out.println(&quot;a/b的值是：&quot; + a / b); &#125; catch (ArithmeticException e) &#123; // catch捕捉异常 System.out.println(&quot;程序出现异常，变量b不能为0。&quot;); &#125; System.out.println(&quot;程序正常结束。&quot;); &#125; &#125; 运行结果： 12程序出现异常，变量b不能为0。程序正常结束。 在try监控区域通过if语句进行判断，当“除数为0”的错误条件成立时引发ArithmeticException异常，创建 ArithmeticException异常对象，并由throw语句将异常抛给Java运行时系统，由系统寻找匹配的异常处理器catch并运行相应异 常处理代码，打印输出“程序出现异常，变量b不能为0。”try-catch语句结束，继续程序流程。 事实上，“除数为0”等ArithmeticException，是RuntimException的子类。而运行时异常将由运行时系统自动抛出，不需要使用throw语句。 例2 捕捉运行时系统自动抛出“除数为0”引发的ArithmeticException异常。 1234567891011public static void main(String[] args) &#123; int a = 6; int b = 0; try &#123; System.out.println(&quot;a/b的值是：&quot; + a / b); &#125; catch (ArithmeticException e) &#123; System.out.println(&quot;程序出现异常，变量b不能为0。&quot;); &#125; System.out.println(&quot;程序正常结束。&quot;); &#125; &#125; 运行结果： 12程序出现异常，变量b不能为0。 程序正常结束。 例2中的语句System.out.println(“a/b的值是：” + a/b);在运行中出现“除数为0”错误，引发ArithmeticException异常。运行时系统创建异常对象并抛出监控区域，转而匹配合适的异常处理器catch，并执行相应的异常处理代码。由于检查运行时异常的代价远大于捕捉异常所带来的益处，运行时异常不可查。Java编译器允许忽略运行时异常，一个方法可以既不捕捉，也不声明抛出运行时异常。 例3 不捕捉、也不声明抛出运行时异常。 12345678public class TestException &#123; public static void main(String[] args) &#123; int a, b; a = 6; b = 0; // 除数b 的值为0 System.out.println(a / b); &#125; &#125; 运行结果： 12Exception in thread &quot;main&quot; java.lang.ArithmeticException: / by zeroat Test.TestException.main(TestException.java:8) 例4 程序可能存在除数为0异常和数组下标越界异常。 123456789101112131415161718public class TestException &#123; public static void main(String[] args) &#123; int[] intArray = new int[3]; try &#123; for (int i = 0; i &lt;= intArray.length; i++) &#123; intArray[i] = i; System.out.println(&quot;intArray[&quot; + i + &quot;] = &quot; + intArray[i]); System.out.println(&quot;intArray[&quot; + i + &quot;]模 &quot; + (i - 2) + &quot;的值: &quot; + intArray[i] % (i - 2)); &#125; &#125; catch (ArrayIndexOutOfBoundsException e) &#123; System.out.println(&quot;intArray数组下标越界异常。&quot;); &#125; catch (ArithmeticException e) &#123; System.out.println(&quot;除数为0异常。&quot;); &#125; System.out.println(&quot;程序正常结束。&quot;); &#125; &#125; 运行结果： 1234567intArray[0] = 0intArray[0]模 -2的值: 0intArray[1] = 1intArray[1]模 -1的值: 0intArray[2] = 2除数为0异常。程序正常结束。 例4中程序可能会出现除数为0异常，还可能会出现数组下标越界异常。程序运行过程中ArithmeticException异常类型是先行匹配的，因此执行相匹配的catch语句： (ArithmeticException e)&#123; 12 System.out.println(&quot;除数为0异常。&quot;); &#125; 需要注意的是，一旦某个catch捕获到匹配的异常类型，将进入异常处理代码。一经处理结束，就意味着整个try-catch语句结束。其他的catch子句不再有匹配和捕获异常类型的机会。 Java通过异常类描述异常类型，异常类的层次结构如图1所示。对于有多个catch子句的异常程序而言，应该尽量将捕获底层异常类的catch子 句放在前面，同时尽量将捕获相对高层的异常类的catch子句放在后面。否则，捕获底层异常类的catch子句将可能会被屏蔽。 RuntimeException异常类包括运行时各种常见的异常，ArithmeticException类和ArrayIndexOutOfBoundsException类都是它的子类。因此，RuntimeException异常类的catch子句应该放在 最后面，否则可能会屏蔽其后的特定异常处理或引起编译错误。 2.1.2 try-catch-finally语句 try-catch语句还可以包括第三部分，就是finally子句。它表示无论是否出现异常，都应当执行的内容。try-catch-finally语句的一般语法形式为：123456789try &#123; // 可能会发生异常的程序代码 &#125; catch (Type1 id1) &#123; // 捕获并处理try抛出的异常类型Type1 &#125; catch (Type2 id2) &#123; // 捕获并处理try抛出的异常类型Type2 &#125; finally &#123; // 无论是否发生异常，都将执行的语句块 &#125; 例5 带finally子句的异常处理程序。 1234567891011121314151617public class TestException &#123; public static void main(String args[]) &#123; int i = 0; String greetings[] = &#123; &quot; Hello world !&quot;, &quot; Hello World !! &quot;, &quot; HELLO WORLD !!!&quot; &#125;; while (i &lt; 4) &#123; try &#123; // 特别注意循环控制变量i的设计，避免造成无限循环 System.out.println(greetings[i++]); &#125; catch (ArrayIndexOutOfBoundsException e) &#123; System.out.println(&quot;数组下标越界异常&quot;); &#125; finally &#123; System.out.println(&quot;--------------------------&quot;); &#125; &#125; &#125; &#125; 运行结果：12345678Hello world !--------------------------Hello World !!--------------------------HELLO WORLD !!!--------------------------数组下标越界异常-------------------------- 在例5中，请特别注意try子句中语句块的设计，如果设计为如下，将会出现死循环。如果设计为： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849try &#123; System.out.println (greetings[i]); i++; &#125; ``` 小结：try 块：用于捕获异常。其后可接零个或多个catch块，如果没有catch块，则必须跟一个finally块。catch 块：用于处理try捕获到的异常。finally 块：无论是否捕获或处理异常，finally块里的语句都会被执行。当在try块或catch块中遇到return语句时，finally语句块将在方法返回之前被执行。在以下4种特殊情况下，finally块不会被执行：1. 在finally语句块中发生了异常。2. 在前面的代码中用了System.exit()退出程序。3. 程序所在的线程死亡。4. 关闭CPU。#### 2.1.3 try-catch-finally 规则1. 必须在 try 之后添加 catch 或 finally 块。try 块后可同时接 catch 和 finally 块，但至少有一个块。2. 必须遵循块顺序：若代码同时使用 catch 和 finally 块，则必须将 catch 块放在 try 块之后。3. catch 块与相应的异常类的类型相关。4. 一个 try 块可能有多个 catch 块。若如此，则执行第一个匹配块。即Java虚拟机会把实际抛出的异常对象依次和各个catch代码块声明的异常类型匹配，如果异常对象为某个异常类型或其子类的实例，就执行这个catch代码块，不会再执行其他的 catch代码块5. 可嵌套 try-catch-finally 结构。6. 在 try-catch-finally 结构中，可重新抛出异常。7. 除了下列情况，总将执行 finally 做为结束：JVM 过早终止（调用 System.exit(int)）；在 finally 块中抛出一个未处理的异常；计算机断电、失火、或遭遇病毒攻击。#### 2.1.4 try、catch、finally语句块的执行顺序1. 当try没有捕获到异常时：try语句块中的语句逐一被执行，程序将跳过catch语句块，执行finally语句块和其后的语句；2. 当try捕获到异常，catch语句块里没有处理此异常的情况：当try语句块里的某条语句出现异常时，而没有处理此异常的catch语句块时，此异常将会抛给JVM处理，finally语句块里的语句还是会被执行，但finally语句块后的语句不会被执行；3. 当try捕获到异常，catch语句块里有处理此异常的情况：在try语句块中是按照顺序来执行的，当执行到某一条语句出现异常时，程序将跳到catch语句块，并与catch语句块逐一匹配，找到与之对应的处理程序，其他的catch语句块将不会被执行，而try语句块中，出现异常之后的语句也不会被执行，catch语句块执行完后，执行finally语句块里的语句，最后执行finally语句块后的语句；如下图所示：![](http://omu7tit09.bkt.clouddn.com/15032852537063.jpg)### 2.2 抛出异常任何Java代码都可以抛出异常，如：自己编写的代码、来自Java开发环境包中代码，或者Java运行时系统。无论是谁，都可以通过Java的throw语句抛出异常。从方法中抛出的任何异常都必须使用throws子句。#### 2.2.1 throws抛出异常如果一个方法可能会出现异常，但没有能力处理这种异常，可以在方法声明处用throws子句来声明抛出异常。例如汽车在运行时可能会出现故障，汽车本身没办法处理这个故障，那就让开车的人来处理。throws语句用在方法定义时声明该方法要抛出的异常类型，如果抛出的是Exception异常类型，则该方法被声明为抛出所有的异常。多个异常可使用逗号分割。throws语句的语法格式为： methodname throws Exception1,Exception2,..,ExceptionN {}12方法名后的throws Exception1,Exception2,...,ExceptionN 为声明要抛出的异常列表。当方法抛出异常列表的异常时，方法将不对这些类型及其子类类型的异常作处理，而抛向调用该方法的方法，由他去处理。例如： import java.lang.Exception;public class TestException { static void pop() throws NegativeArraySizeException { // 定义方法并抛出NegativeArraySizeException异常 int[] arr = new int[-3]; // 创建数组 } public static void main(String[] args) { // 主方法 try { // try语句处理异常信息 pop(); // 调用pop()方法 } catch (NegativeArraySizeException e) { System.out.println(&quot;pop()方法抛出的异常&quot;);// 输出异常信息 } } }12345678910使用throws关键字将异常抛给调用者后，如果调用者不想处理该异常，可以继续向上抛出，但最终要有能够处理该异常的调用者。 pop方法没有处理异常NegativeArraySizeException，而是由main函数来处理。 Throws抛出异常的规则：1. 如果是不可查异常（unchecked exception），即Error、RuntimeException或它们的子类，那么可以不使用throws关键字来声明要抛出的异常，编译仍能顺利通过，但在运行时会被系统抛出。2. 必须声明方法可抛出的任何可查异常（checked exception）。即如果一个方法可能出现受可查异常，要么用try-catch语句捕获，要么用throws子句声明将它抛出，否则会导致编译错误3. 仅当抛出了异常，该方法的调用者才必须处理或者重新抛出该异常。当方法的调用者无力处理该异常的时候，应该继续抛出，而不是囫囵吞枣。4. 调用方法必须遵循任何可查异常的处理和声明规则。若覆盖一个方法，则不能声明与覆盖方法不同的异常。声明的任何异常必须是被覆盖方法所声明异常的同类或子类。例如： void method1() throws IOException{} //合法 //编译错误，必须捕获或声明抛出IOExceptionvoid method2(){ method1();} //合法，声明抛出IOExceptionvoid method3()throws IOException { method1();} //合法，声明抛出Exception，IOException是Exception的子类void method4()throws Exception { method1();} //合法，捕获IOExceptionvoid method5(){ try{ method1(); }catch(IOException e){…}} //编译错误，必须捕获或声明抛出Exceptionvoid method6(){ try{ method1(); }catch(IOException e){throw new Exception();}} //合法，声明抛出Exceptionvoid method7()throws Exception{ try{ method1(); }catch(IOException e){throw new Exception();}}1234567891011判断一个方法可能会出现异常的依据如下：1. 方法中有throw语句。例如，以上method7()方法的catch代码块有throw语句。2. 调用了其他方法，其他方法用throws子句声明抛出某种异常。例如，method3()方法调用了method1()方法，method1()方法声明抛出IOException，因此，在method3()方法中可能会出现IOException。#### 2.2.2 throw抛出异常throw总是出现在函数体中，用来抛出一个Throwable类型的异常。程序会在throw语句后立即终止，它后面的语句执行不到，然后在包含它的所有try块中（可能在上层调用函数中）从里向外寻找含有与其匹配的catch子句的try块。我们知道，异常是异常类的实例对象，我们可以创建异常类的实例对象通过throw语句抛出。该语句的语法格式为： throw new exceptionname;1例如抛出一个IOException类的异常对象： throw new IOException;1要注意的是，throw 抛出的只能够是可抛出类Throwable 或者其子类的实例对象。下面的操作是错误的： throw new String(“exception”);12345这是因为String 不是Throwable 类的子类。如果抛出了检查异常，则还应该在方法头部声明方法可能抛出的异常类型。该方法的调用者也必须检查处理抛出的异常。如果所有方法都层层上抛获取的异常，最终JVM会进行处理，处理也很简单，就是打印异常消息和堆栈信息。如果抛出的是Error或RuntimeException，则该方法的调用者可选择处理该异常。 package Test;import java.lang.Exception;public class TestException { static int quotient(int x, int y) throws MyException { // 定义方法抛出异常 if (y &lt; 0) { // 判断参数是否小于0 throw new MyException(“除数不能是负数”); // 异常信息 } return x/y; // 返回值 } public static void main(String args[]) { // 主方法 int a =3; int b =0; try { // try语句包含可能发生异常的语句 int result = quotient(a, b); // 调用方法quotient() } catch (MyException e) { // 处理自定义异常 System.out.println(e.getMessage()); // 输出异常信息 } catch (ArithmeticException e) { // 处理ArithmeticException异常 System.out.println(“除数不能为0”); // 输出提示信息 } catch (Exception e) { // 处理其他异常 System.out.println(“程序发生了其他的异常”); // 输出提示信息 } } }class MyException extends Exception { // 创建自定义异常类 String message; // 定义String类型变量 public MyException(String ErrorMessagr) { // 父类方法 message = ErrorMessagr; } public String getMessage() { // 覆盖getMessage()方法 return message; } }12345678### 2.3 异常链&gt; 如果调用quotient(3,-1)将发生MyException异常，程序调转到catch (MyException e)代码块中执行；&gt; 如果调用quotient(5,0)将会因“除数为0”错误引发ArithmeticException异常，属于运行时异常类，由Java运行时系统自动抛出。quotient（）方法没有捕捉ArithmeticException异常，Java运行时系统将沿方法调用栈查到main方法，将抛出的异常上传至quotient（）方法的调用者： int result = quotient(a, b); // 调用方法quotient()由于该语句在try监控区域内，因此传回的“除数为0”的ArithmeticException异常1由Java运行时系统抛出，并匹配catch子句： catch (ArithmeticException e) { // 处理ArithmeticException异常System.out.println(“除数不能为0”); // 输出提示信息}``` 处理结果是输出“除数不能为0”。Java这种向上传递异常信息的处理机制，形成异常链。 Java方法抛出的可查异常将依据调用栈、沿着方法调用的层次结构一直传递到具备处理能力的调用方法，最高层次到main方法为止。如果异常传递到main方法，而main不具备处理能力，也没有通过throws声明抛出该异常，将可能出现编译错误。 如还有其他异常发生 将使用catch (Exception e)捕捉异常。由于Exception是所有异常类的父类，如果将catch (Exception e)代码块放在其他两个代码块的前面，后面的代码块将永远得不到执行，就没有什么意义了，所以catch语句的顺序不可掉换。 2.4 Throwable类中的常用方法注意：catch关键字后面括号中的Exception类型的参数e。Exception就是try代码块传递给catch代码块的变量类型，e就是变量名。catch代码块中语句”e.getMessage();”用于输出错误性质。通常异常处理常用3个函数来获取异常的有关信息: getCause()：返回抛出异常的原因。如果 cause 不存在或未知，则返回 null。 getMeage()：返回异常的消息信息。 printStackTrace()：对象的堆栈跟踪输出至错误输出流，作为字段 System.err 的值。 有时为了简单会忽略掉catch语句后的代码，这样try-catch语句就成了一种摆设，一旦程序在运行过程中出现了异常，就会忽略处理异常，而错误发生的原因很难查找。 三、Java常见异常 在Java中提供了一些异常用来描述经常发生的错误，对于这些异常，有的需要程序员进行捕获处理或声明抛出，有的是由Java虚拟机自动进行捕获处理。Java中常见的异常类: 3.1 runtimeException子类: java.lang.ArrayIndexOutOfBoundsException：数组索引越界异常。当对数组的索引值为负数或大于等于数组大小时抛出。 java.lang.ArithmeticException：算术条件异常。譬如：整数除零等。 java.lang.NullPointerException：空指针异常。当应用试图在要求使用对象的地方使用了null时，抛出该异常。譬如：调用null对象的实例方法、访问null对象的属性、计算null对象的长度、使用throw语句抛出null等等 java.lang.ClassNotFoundException：找不到类异常。当应用试图根据字符串形式的类名构造类，而在遍历CLASSPAH之后找不到对应名称的class文件时，抛出该异常。 java.lang.NegativeArraySizeException：数组长度为负异常 java.lang.ArrayStoreException：数组中包含不兼容的值抛出的异常 java.lang.SecurityException：安全性异常 java.lang.IllegalArgumentException：非法参数异常 3.2 IOException IOException：操作输入流和输出流时可能出现的异常。 EOFException：文件已结束异常 FileNotFoundException ：文件未找到异常3.3 其他 ClassCastException：类型转换异常类 ArrayStoreException：数组中包含不兼容的值抛出的异常 SQLException：操作数据库异常类 NoSuchFieldException：字段未找到异常 NoSuchMethodException：方法未找到抛出的异常 NumberFormatException：字符串转换为数字抛出的异常 StringIndexOutOfBoundsException：字符串索引超出范围抛出的异常 IllegalAccessException：不允许访问某类异常 InstantiationException：当应用程序试图使用Class类中的newInstance()方法创建一个类的实例，而指定的类对象无法被实例化时，抛出该异常 四、自定义异常使用Java内置的异常类可以描述在编程时出现的大部分异常情况。除此之外，用户还可以自定义异常。用户自定义异常类，只需继承Exception类即可。 在程序中使用自定义异常类，大体可分为以下几个步骤。 创建自定义异常类。 在方法中通过throw关键字抛出异常对象。 如果在当前抛出异常的方法中处理异常，可以使用try-catch语句捕获并处理；否则在方法的声明处通过throws关键字指明要抛出给方法调用者的异常，继续进行下一步操作。 在出现异常方法的调用者中捕获并处理异常。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>异常处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（6）：异常处理]]></title>
    <url>%2F2017%2F08%2F25%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%886%EF%BC%89%EF%BC%9A%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[一、异常处理基础Java异常是一个描述在代码段中发生的异常（也就是出错）情况的对象。当异常情况发生，一个代表该异常的对象被创建并且在导致该错误的方法中被抛出（throw）。该方法可以选择自己处理异常或传递该异常。两种情况下，该异常被捕获（catch）并处理。异常可能是由Java运行时系统产生，或者是由你的手工代码产生。被Java抛出的异常与违反语言规范或超出Java执行环境限制的基本错误有关。手工编码产生的异常基本上用于报告方法调用程序的出错状况。 Java异常处理通过5个关键字控制：try、catch、throw、throws和 finally。下面讲述它们如何工作的。程序声明了你想要的异常监控包含在一个try块中。如果在try块中发生异常，它被抛出。你的代码可以捕捉这个异常（用catch）并且用某种合理的方法处理该异常。系统产生的异常被Java运行时系统自动抛出。手动抛出一个异常，用关键字throw。任何被抛出方法的异常都必须通过throws子句定义。任何在方法返回前绝对被执行的代码被放置在finally块中。 下面是一个异常处理块的通常形式： 12345678910111213try&#123; // block of code to monitor for errors&#125;catch (ExceptionType1 exOb) &#123; // exception handler for ExceptionType1&#125;catch (ExceptionType2 exOb) &#123; // exception handler for ExceptionType2&#125;// ...finally &#123; // block of code to be executed before try block ends&#125; 这里，ExceptionType 是发生异常的类型。下面将介绍怎样应用这个框架。 二、异常类型所有异常类型都是内置类Throwable的子类。因此，Throwable在异常类层次结构的顶层。紧接着Throwable下面的是两个把异常分成两个不同分支的子类。一个分支是Exception。该类用于用户程序可能捕捉的异常情况。它也是你可以用来创建你自己用户异常类型子类的类。在Exception分支中有一个重要子类RuntimeException。该类型的异常自动为你所编写的程序定义并且包括被零除和非法数组索引这样的错误。 另一类分支由Error作为顶层，Error定义了在通常环境下不希望被程序捕获的异常。Error类型的异常用于Java运行时系统来显示与运行时系统本身有关的错误。堆栈溢出是这种错误的一例。本章将不讨论关于Error类型的异常处理，因为它们通常是灾难性的致命错误，不是你的程序可以控制的。 三、未被捕获的异常在你学习在程序中处理异常之前，看一看如果你不处理它们会有什么情况发生是很有好处的。下面的小程序包括一个故意导致被零除错误的表达式。 123456class Exc0 &#123; public static void main(String args[]) &#123; int d = 0; int a = 42 / d; &#125;&#125; 当Java运行时系统检查到被零除的情况，它构造一个新的异常对象然后抛出该异常。这导致Exc0的执行停止，因为一旦一个异常被抛出，它必须被一个异常处理程序捕获并且被立即处理。该例中，我们没有提供任何我们自己的异常处理程序，所以异常被Java运行时系统的默认处理程序捕获。任何不是被你程序捕获的异常最终都会被该默认处理程序处理。默认处理程序显示一个描述异常的字符串，打印异常发生处的堆栈轨迹并且终止程序。 下面是由标准javaJDK运行时解释器执行该程序所产生的输出： 12java.lang.ArithmeticException: / by zeroat Exc0.main(Exc0.java:4) 注意，类名Exc0，方法名main，文件名Exc0.java和行数4是怎样被包括在一个简单的堆栈使用轨迹中的。还有，注意抛出的异常类型是Exception的一个名为ArithmeticException的子类，该子类更明确的描述了何种类型的错误方法。本章后面部分将讨论，Java提供多个内置的与可能产生的不同种类运行时错误相匹配的异常类型。 堆栈轨迹将显示导致错误产生的方法调用序列。例如，下面是前面程序的另一个版本，它介绍了相同的错误，但是错误是在main( )方法之外的另一个方法中产生的： 123456789class Demo &#123; static void subroutine() &#123; int d = 0; int a = 10 / d; &#125; public static void main(String args[]) &#123; Exc1.subroutine(); &#125;&#125; 默认异常处理器的堆栈轨迹结果表明了整个调用栈是怎样显示的： 123java.lang.ArithmeticException: / by zeroat Exc1.subroutine(Exc1.java:4)at Exc1.main(Exc1.java:7) 如你所见，栈底是main的第7行，该行调用了subroutine( )方法。该方法在第4行导致了异常。调用堆栈对于调试来说是很重要的，因为它查明了导致错误的精确的步骤。 四、try和catch的使用尽管由Java运行时系统提供的默认异常处理程序对于调试是很有用的，但通常你希望自己处理异常。这样做有两个好处。第一，它允许你修正错误。第二，它防止程序自动终止。大多数用户对于在程序终止运行和在无论何时错误发生都会打印堆栈轨迹感到很烦恼（至少可以这么说）。幸运的是，这很容易避免。为防止和处理一个运行时错误，只需要把你所要监控的代码放进一个try块就可以了。紧跟着try块的，包括一个说明你希望捕获的错误类型的catch子句。完成这个任务很简单，下面的程序包含一个处理因为被零除而产生的ArithmeticException 异常的try块和一个catch子句。 12345678910111213class Demo &#123; public static void main(String args[]) &#123; int d, a; try &#123; // monitor a block of code. d = 0; a = 42 / d; System.out.println(&quot;This will not be printed.&quot;); &#125; catch (ArithmeticException e) &#123; // catch divide-by-zero error System.out.println(&quot;Division by zero.&quot;); &#125; System.out.println(&quot;After catch statement.&quot;); &#125;&#125; 该程序输出如下： 12Division by zero.After catch statement. 注意在try块中的对println( )的调用是永远不会执行的。一旦异常被引发，程序控制由try块转到catch块。执行永远不会从catch块“返回”到try块。因此，“This will not be printed。”将不会被显示。一旦执行了catch语句，程序控制从整个try/catch机制的下面一行继续。 一个try和它的catch语句形成了一个单元。catch子句的范围限制于try语句前面所定义的语句。一个catch语句不能捕获另一个try声明所引发的异常（除非是嵌套的try语句情况）。 被try保护的语句声明必须在一个大括号之内（也就是说，它们必须在一个块中）。你不能单独使用try。 构造catch子句的目的是解决异常情况并且像错误没有发生一样继续运行。例如，下面的程序中，每一个for循环的反复得到两个随机整数。这两个整数分别被对方除，结果用来除12345。最后的结果存在a中。如果一个除法操作导致被零除错误，它将被捕获，a的值设为零，程序继续运行。 123456789101112131415161718192021//Handle an exception and move on.import java.util.Random;class Demo &#123; public static void main(String args[]) &#123; int a=0, b=0, c=0; Random r = new Random(); for(int i=0; i&lt;30; i++) &#123; try &#123; b = r.nextInt(); c = r.nextInt(); a = 12345 / (b/c); &#125; catch (ArithmeticException e) &#123; System.out.println(&quot;Division by zero.&quot;); a = 0; // set a to zero and continue &#125; System.out.println(&quot;a: &quot; + a); &#125; &#125;&#125; Throwable重载toString( )方法（由Object定义），所以它返回一个包含异常描述的字符串。你可以通过在println( )中传给异常一个参数来显示该异常的描述。例如，前面程序的catch块可以被重写成 123456789101112131415161718192021//Handle an exception and move on.import java.util.Random;class Demo &#123; public static void main(String args[]) &#123; int a=0, b=0, c=0; Random r = new Random(); for(int i=0; i&lt;30; i++) &#123; try &#123; b = r.nextInt(); c = r.nextInt(); a = 12345 / (b/c); &#125; catch (ArithmeticException e) &#123; System.out.println(&quot;Exception: &quot; + e); a = 0; // set a to zero and continue &#125; System.out.println(&quot;a: &quot; + a); &#125; &#125;&#125; 当这个版本代替原程序中的版本，程序在标准javaJDK解释器下运行，每一个被零除错误显示下面的消息： 1Exception: java.lang.ArithmeticException: / by zero 尽管在上下文中没有特殊的值，显示一个异常描述的能力在其他情况下是很有价值的——特别是当你对异常进行实验和调试时。 五、多重catch语句的使用某些情况，由单个代码段可能引起多个异常。处理这种情况，你可以定义两个或更多的catch子句，每个子句捕获一种类型的异常。当异常被引发时，每一个catch子句被依次检查，第一个匹配异常类型的子句执行。当一个catch语句执行以后，其他的子句被旁路，执行从try/catch块以后的代码开始继续。下面的例子设计了两种不同的异常类型： 1234567891011121314151617//Demonstrate multiple catch statements.class Demo &#123; public static void main(String args[]) &#123; try &#123; int a = args.length; System.out.println(&quot;a = &quot; + a); int b = 42 / a; int c[] = &#123; 1 &#125;; c[42] = 99; &#125; catch(ArithmeticException e) &#123; System.out.println(&quot;Divide by 0: &quot; + e); &#125; catch(ArrayIndexOutOfBoundsException e) &#123; System.out.println(&quot;Array index oob: &quot; + e); &#125; System.out.println(&quot;After try/catch blocks.&quot;); &#125;&#125; 该程序在没有命令行参数的起始条件下运行导致被零除异常，因为a为0。如果你提供一个命令行参数，它将幸免于难，把a设成大于零的数值。但是它将导致ArrayIndexOutOf BoundsException异常，因为整型数组c的长度为1，而程序试图给c[42]赋值。下面是运行在两种不同情况下程序的输出： 123456C:\&gt;java MultiCatcha = 0Divide by 0: java.lang.ArithmeticException: / by zero After try/catch blocks.C:\&gt;java MultiCatch TestArga = 1Array index oob: java.lang.ArrayIndexOutOfBoundsException After try/catch blocks. 当你用多catch语句时，记住异常子类必须在它们任何父类之前使用是很重要的。这是因为运用父类的catch语句将捕获该类型及其所有子类类型的异常。这样，如果子类在父类后面，子类将永远不会到达。而且，Java中不能到达的代码是一个错误。例如，考虑下面的程序： 123456789101112131415161718//This program contains an error.//A subclass must come before its superclass in a series of catch statements. If not,unreachable code will be created and acompile-time error will result.//*/class Demo &#123; public static void main(String args[]) &#123; try &#123; int a = 0; int b = 42 / a; &#125; catch(Exception e) &#123; System.out.println(&quot;Generic Exception catch.&quot;); &#125; /* This catch is never reached because ArithmeticException is a subclass of Exception. */ catch(ArithmeticException e) &#123; // ERROR - unreachable System.out.println(&quot;This is never reached.&quot;); &#125; &#125;&#125; 如果你试着编译该程序，你会收到一个错误消息，该错误消息说明第二个catch语句不会到达，因为该异常已经被捕获。因为ArithmeticException 是Exception的子类，第一个catch语句将处理所有的面向Exception的错误，包括ArithmeticException。这意味着第二个catch语句永远不会执行。为修改程序，颠倒两个catch语句的次序。 六、try语句的嵌套Try语句可以被嵌套。也就是说，一个try语句可以在另一个try块内部。每次进入try语句，异常的前后关系都会被推入堆栈。如果一个内部的try语句不含特殊异常的catch处理程序，堆栈将弹出，下一个try语句的catch处理程序将检查是否与之匹配。这个过程将继续直到一个catch语句匹配成功，或者是直到所有的嵌套try语句被检查耗尽。如果没有catch语句匹配，Java的运行时系统将处理这个异常。下面是运用嵌套try语句的一个例子： 123456789101112131415161718192021222324//An example of nested try statements.class Demo &#123; public static void main(String args[]) &#123; try &#123; int a = args.length; /* If no command-line args are present,the following statement will generate a divide-by-zero exception. */ int b = 42 / a; System.out.println(&quot;a = &quot; + a); try &#123; // nested try block /* If one command-line arg is used,then a divide-by-zero exception will be generated by the following code. */ if(a==1) a = a/(a-a); // division by zero /* If two command-line args are used,then generate an out-of-bounds exception. */ if(a==2) &#123; int c[] = &#123; 1 &#125;; c[42] = 99; // generate an out-of-bounds exception &#125; &#125; catch(ArrayIndexOutOfBoundsException e) &#123; System.out.println(&quot;Array index out-of-bounds: &quot; + e); &#125; &#125; catch(ArithmeticException e) &#123; System.out.println(&quot;Divide by 0: &quot; + e); &#125; &#125;&#125; 如你所见，该程序在一个try块中嵌套了另一个try块。程序工作如下：当你在没有命令行参数的情况下执行该程序，外面的try块将产生一个被零除的异常。程序在有一个命令行参数条件下执行，由嵌套的try块产生一个被零除的错误。因为内部的块不匹配这个异常，它将把异常传给外部的try块，在那里异常被处理。如果你在具有两个命令行参数的条件下执行该程序，由内部try块产生一个数组边界异常。下面的结果阐述了每一种情况： 12345678C:\&gt;java NestTryDivide by 0: java.lang.ArithmeticException: / by zeroC:\&gt;java NestTry Onea = 1Divide by 0: java.lang.ArithmeticException: / by zeroC:\&gt;java NestTry One Twoa = 2Array index out-of-bounds: java.lang.ArrayIndexOutOfBoundsException 当有方法调用时，try语句的嵌套可以很隐蔽的发生。例如，你可以把对方法的调用放在一个try块中。在该方法内部，有另一个try语句。这种情况下，方法内部的try仍然是嵌套在外部调用该方法的try块中的。下面是前面例子的修改，嵌套的try块移到了方法nesttry( )的内部： 12345678910111213141516171819202122232425262728//Try statements can be implicitly nested via calls to methods. */class Demo &#123; static void nesttry(int a) &#123; try &#123; // nested try block /* If one command-line arg is used,then a divide-by-zero exception will be generated by the following code. */ if(a==1) a = a/(a-a); // division by zero /* If two command-line args are used,then generate an out-of-bounds exception. */ if(a==2) &#123; int c[] = &#123; 1 &#125;; c[42] = 99; // generate an out-of-bounds exception &#125; &#125; catch(ArrayIndexOutOfBoundsException e) &#123; System.out.println(&quot;Array index out-of-bounds: &quot; + e); &#125; &#125; public static void main(String args[]) &#123; try &#123; int a = args.length; /* If no command-line args are present,the following statement will generate a divide-by-zero exception. */ int b = 42 / a; System.out.println(&quot;a = &quot; + a); nesttry(a); &#125; catch(ArithmeticException e) &#123; System.out.println(&quot;Divide by 0: &quot; + e); &#125; &#125;&#125; 该程序的输出与前面的例子相同。 八、throws子句如果一个方法可以导致一个异常但不处理它，它必须指定这种行为以使方法的调用者可以保护它们自己而不发生异常。做到这点你可以在方法声明中包含一个throws子句。一个 throws 子句列举了一个方法可能抛出的所有异常类型。这对于除Error或RuntimeException及它们子类以外类型的所有异常是必要的。一个方法可以抛出的所有其他类型的异常必须在throws子句中声明。如果不这样做，将会导致编译错误。 下面是包含一个throws子句的方法声明的通用形式： 123type method-name(parameter-list) throws exception-list&#123; // body of method&#125; 这里，exception-list是该方法可以抛出的以有逗号分割的异常列表。 下面是一个不正确的例子。该例试图抛出一个它不能捕获的异常。因为程序没有指定一个throws子句来声明这一事实，程序将不会编译。12345678910//this program contains an error and will not compile.class ThrowsDemo &#123; static void throwOne() &#123; System.out.println(&quot;Inside throwOne.&quot;); throw new IllegalAccessException(&quot;demo&quot;); &#125; public static void main(String args[]) &#123; throwOne(); &#125;&#125; 为编译该程序，需要改变两个地方。第一，需要声明throwOne( )引发IllegalAccess Exception异常。第二，main( )必须定义一个try/catch 语句来捕获该异常。正确的例子如下：1234567891011121314//This is now correct.class Demo &#123; static void throwOne() throws IllegalAccessException &#123; System.out.println(&quot;Inside throwOne.&quot;); throw new IllegalAccessException(&quot;demo&quot;); &#125; public static void main(String args[]) &#123; try &#123; throwOne(); &#125; catch (IllegalAccessException e) &#123; System.out.println(&quot;Caught &quot; + e); &#125; &#125;&#125; 下面是例题的输出结果： 12inside throwOnecaught java.lang.IllegalAccessException: demo 九、finally块当异常被抛出，通常方法的执行将作一个陡峭的非线性的转向。依赖于方法是怎样编码的，异常甚至可以导致方法过早返回。这在一些方法中是一个问题。例如，如果一个方法打开一个文件项并关闭，然后退出，你不希望关闭文件的代码被异常处理机制旁路。finally关键字为处理这种意外而设计。finally创建一个代码块。该代码块在一个try/catch 块完成之后另一个try/catch出现之前执行。finally块无论有没有异常抛出都会执行。如果异常被抛出，finally甚至是在没有与该异常相匹配的catch子句情况下也将执行。一个方法将从一个try/catch块返回到调用程序的任何时候，经过一个未捕获的异常或者是一个明确的返回语句，finally子句在方法返回之前仍将执行。这在关闭文件句柄和释放任何在方法开始时被分配的其他资源是很有用的。finally子句是可选项，可以有也可以无。然而每一个try语句至少需要一个catch或finally子句。 下面的例子显示了3种不同的退出方法。每一个都执行了finally子句： 12345678910111213141516171819202122232425262728293031323334353637383940//Demonstrate finally.class Demo &#123; // Through an exception out of the method. static void procA() &#123; try &#123; System.out.println("inside procA"); throw new RuntimeException("demo"); &#125; finally &#123; System.out.println("procA's finally"); &#125; &#125; // Return from within a try block. static void procB() &#123; try &#123; System.out.println("inside procB"); return; &#125; finally &#123; System.out.println("procB's finally"); &#125; &#125; // Execute a try block normally. static void procC() &#123; try &#123; System.out.println("inside procC"); &#125; finally &#123; System.out.println("procC's finally"); &#125; &#125; public static void main(String args[]) &#123; try &#123; procA(); &#125; catch (Exception e) &#123; System.out.println("Exception caught"); &#125; procB(); procC(); &#125;&#125; 该例中，procA( )过早地通过抛出一个异常中断了try。Finally子句在退出时执行。procB( )的try语句通过一个return语句退出。在procB( )返回之前finally子句执行。在procC（）中，try语句正常执行，没有错误。然而，finally块仍将执行。 注意：如果finally块与一个try联合使用，finally块将在try结束之前执行。 下面是上述程序产生的输出： 1234567inside procAprocA’s finallyException caughtinside procBprocB’s finallyinside procCprocC’s finally 十、内置异常在标准包java.lang中，Java定义了若干个异常类。前面的例子曾用到其中一些。这些异常一般是标准类RuntimeException的子类。因为java.lang实际上被所有的Java程序引入，多数从RuntimeException派生的异常都自动可用。而且，它们不需要被包含在任何方法的throws列表中。Java语言中，这被叫做未经检查的异常（unchecked exceptions ）。因为编译器不检查它来看一个方法是否处理或抛出了这些异常。 java.lang中定义的未经检查的异常列于表10-1。表10-2列出了由 java.lang定义的必须在方法的throws列表中包括的异常，如果这些方法能产生其中的某个异常但是不能自己处理它。这些叫做受检查的异常（checked exceptions）。Java定义了几种与不同类库相关的其他的异常类型。 Java 的 java.lang 中定义的未检查异常子类 异常 说明 ArithmeticException 算术错误，如被0除 ArrayIndexOutOfBoundsException 数组下标出界 ArrayStoreException 数组元素赋值类型不兼容 ClassCastException 非法强制转换类型 IllegalArgumentException 调用方法的参数非法 IllegalMonitorStateException 非法监控操作，如等待一个未锁定线程 IllegalStateException 环境或应用状态不正确 IllegalThreadStateException 请求操作与当前线程状态不兼容 IndexOutOfBoundsException 某些类型索引越界 NullPointerException 非法使用空引用 NumberFormatException 字符串到数字格式非法转换 SecurityException 试图违反安全性 StringIndexOutOfBounds 试图在字符串边界之外索引 UnsupportedOperationException 遇到不支持的操作 java.lang 中定义的检查异常 异常 意义 ClassNotFoundException 找不到类 CloneNotSupportedException 试图克隆一个不能实现Cloneable接口的对象 IllegalAccessException 对一个类的访问被拒绝 InstantiationException 试图创建一个抽象类或者抽象接口的对象 InterruptedException 一个线程被另一个线程中断 NoSuchFieldException 请求的字段不存在 NoSuchMethodException 请求的方法不存在 十一、创建自己的异常子类尽管Java的内置异常处理大多数常见错误，你也许希望建立你自己的异常类型来处理你所应用的特殊情况。这是非常简单的：只要定义Exception的一个子类就可以了（Exception当然是Throwable的一个子类）。你的子类不需要实际执行什么——它们在类型系统中的存在允许你把它们当成异常使用。Exception类自己没有定义任何方法。当然，它继承了Throwable提供的一些方法。因此，所有异常，包括你创建的，都可以获得Throwable定义的方法。这些方法显示在表10-3中。你还可以在你创建的异常类中覆盖一个或多个这样的方法。 Throwable 定义的方法 方法 描述 Throwable fillInStackTrace( ) 返回一个包含完整堆栈轨迹的Throwable对象，该对象可能被再次引发。 String getLocalizedMessage( ) 返回一个异常的局部描述 String getMessage( ) 返回一个异常的描述 void printStackTrace( ) 显示堆栈轨迹 void printStackTrace(PrintStreamstream) 把堆栈轨迹送到指定的流 void printStackTrace(PrintWriterstream) 把堆栈轨迹送到指定的流 String toString( ) 返回一个包含异常描述的String对象。当输出一个Throwable对象时，该方法被println( )调用 下面的例子声明了Exception的一个新子类，然后该子类当作方法中出错情形的信号。它重载了toString( )方法，这样可以用println( )显示异常的描述。 1234567891011121314151617181920212223242526272829//This program creates a custom exception type.class MyException extends Exception &#123; private int detail; MyException(int a) &#123; detail = a; &#125; public String toString() &#123; return &quot;MyException[&quot; + detail + &quot;]&quot;; &#125;&#125;class ExceptionDemo &#123; static void compute(int a) throws MyException &#123; System.out.println(&quot;Called compute(&quot; + a + &quot;)&quot;); if(a &gt; 10) throw new MyException(a); System.out.println(&quot;Normal exit&quot;); &#125; public static void main(String args[]) &#123; try &#123; compute(1); compute(20); &#125; catch (MyException e) &#123; System.out.println(&quot;Caught &quot; + e); &#125; &#125;&#125; 该例题定义了Exception的一个子类MyException。该子类非常简单：它只含有一个构造函数和一个重载的显示异常值的toString( )方法。ExceptionDemo类定义了一个compute( )方法。该方法抛出一个MyException对象。当compute( )的整型参数比10大时该异常被引发。 main( )方法为MyException设立了一个异常处理程序，然后用一个合法的值和不合法的值调用compute( )来显示执行经过代码的不同路径。下面是结果： 1234Called compute(1)Normal exitCalled compute(20)Caught MyException[20] 十二、断言断言用于证明和测试程序的假设，比如“这里的值大于 5”。断言可以在运行时从代码中完全删除，所以对代码的运行速度没有影响。 断言有两种方法： 一种是 assert&lt;&lt;布尔表达式&gt;&gt; ； 另一种是 assert&lt;&lt;布尔表达式&gt;&gt; ：&lt;&lt;细节描述&gt;&gt;。 如果布尔表达式的值为false ， 将抛出AssertionError 异常； 细节描述是AssertionError异常的描述文本使用 javac –source 1.4 MyClass.java 的方式进行编译示例如下： 123456789101112131415public class Demo &#123; public static void main(String[] args) &#123; int x = 10; if (args.length &gt; 0) &#123; try &#123; x = Integer.parseInt(args[0]); &#125; catch (NumberFormatException nfe) &#123; /* Ignore */ &#125; &#125; System.out.println("Testing assertion that x == 10"); assert x == 10 : "Our assertion failed"; System.out.println("Test passed"); &#125;&#125; 由于引入了一个新的关键字，所以在编译的时候就需要增加额外的参数，要编译成功，必须使用 JDK1.4 的 javac 并加上参数’-source 1.4′,例如可以使用以下的命令编译上面的代码： 1javac -source 1.4 AssertExample.java 以上程序运行使用断言功能也需要使用额外的参数（并且需要一个数字的命令行参数），例如： 1java -ea AssertExample 1 程序的输出为： 123Testing assertion that x == 10Exception in thread “main” java.lang.AssertionError:Our assertion failedat AssertExample.main(AssertExample.java:20) 由于输入的参数不等于 10，因此断言功能使得程序运行时抛出断言错误，注意是错误， 这意味着程序发生严重错误并且将强制退出。断言使用 boolean 值，如果其值不为 true 则 抛出 AssertionError 并终止程序的运行。 断言推荐使用方法 用于验证方法中的内部逻辑，包括： 内在不变式 控制流程不变式 后置条件和类不变式 注意：不推荐用于公有方法内的前置条件的检查。 运行时要屏蔽断言，可以用如下方法： 1java –disableassertions 或 java –da 类名 运行时要允许断言，可以用如下方法： 1java –enableassertions 或 java –ea类名]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>异常处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（5）：static、final关键字和Object类]]></title>
    <url>%2F2017%2F08%2F24%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89%EF%BC%9Astatic%E3%80%81final%E5%85%B3%E9%94%AE%E5%AD%97%E5%92%8CObject%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[一、static关键字static 修饰符能够与变量、方法一起使用，表示是“静态”的。静态变量和静态方法能够通过类名来访问，不需要创建一个类的对象来访问该类的静态成员，所以static修饰的成员又称作类变量和类方法。静态变量与实例变量不同，实例变量总是通过对象来访问，因为它们的值在对象和对象之间有所不同。请看下面的例子： 1234567891011121314public class Demo &#123; static int i = 10; int j; Demo() &#123; this.j = 20; &#125; public static void main(String[] args) &#123; System.out.println(&quot;类变量 i=&quot; + Demo.i); Demo obj = new Demo(); System.out.println(&quot;实例变量 j=&quot; + obj.j); &#125;&#125; 运行结果： 12类变量 i=10实例变量 j=20 1.1 static的内存分配静态变量属于类，不属于任何独立的对象，所以无需创建类的实例就可以访问静态变量。之所以会产生这样的结果，是因为编译器只为整个类创建了一个静态变量的副本，也就是只分配一个内存空间，虽然有多个实例，但这些实例共享该内存。实例变量则不同，每创建一个对象，都会分配一次内存空间，不同变量的内存相互独立，互不影响，改变 a 对象的实例变量不会影响 b 对象。 123456789101112131415public class Demo &#123; static int i; int j; public static void main(String[] args) &#123; Demo obj1 = new Demo(); obj1.i = 10; obj1.j = 20; Demo obj2 = new Demo(); System.out.println("obj1.i=" + obj1.i + ", obj1.j=" + obj1.j); System.out.println("obj2.i=" + obj2.i + ", obj2.j=" + obj2.j); &#125;&#125; 运行结果： 12obj1.i=10, obj1.j=20obj2.i=10, obj2.j=0 注意：静态变量虽然也可以通过对象来访问，但是不被提倡，编译器也会产生警告。 上面的代码中，i 是静态变量，通过 obj1 改变 i 的值，会影响到 obj2；j 是实例变量，通过 obj1 改变 j 的值，不会影响到 obj2。这是因为 obj1.i 和 obj2.i 指向同一个内存空间，而 obj1.j 和 obj2.j 指向不同的内存空间，请看下图： 注意：static 的变量是在类装载的时候就会被初始化。也就是说，只要类被装载，不管你是否使用了这个static 变量，它都会被初始化。 小结：类变量(class variables)用关键字 static 修饰，在类加载的时候，分配类变量的内存，以后再生成类的实例对象时，将共享这块内存（类变量），任何一个对象对类变量的修改，都会影响其它对象。外部有两种访问方式：通过对象来访问或通过类名来访问。 1.2 静态方法静态方法是一种不能向对象实施操作的方法。例如，Math 类的 pow() 方法就是一个静态方法，语法为 Math.pow(x, a)，用来计算 x 的 a 次幂，在使用时无需创建任何 Math 对象。 因为静态方法不能操作对象，所以不能在静态方法中访问实例变量，只能访问自身类的静态变量。 以下情形可以使用静态方法： 一个方法不需要访问对象状态，其所需参数都是通过显式参数提供（例如 Math.pow()）。 一个方法只需要访问类的静态变量。 读者肯定注意到，main() 也是一个静态方法，不对任何对象进行操作。实际上，在程序启动时还没有任何对象，main() 方法是程序的入口，将被执行并创建程序所需的对象。 关于静态变量和静态方法的总结： 一个类的静态方法只能访问静态变量； 一个类的静态方法不能够直接调用非静态方法； 如访问控制权限允许，静态变量和静态方法也可以通过对象来访问，但是不被推荐； 静态方法中不存在当前对象，因而不能使用 this，当然也不能使用 super； 静态方法不能被非静态方法覆盖； 构造方法不允许声明为 static 的； 局部变量不能使用static修饰。 静态方法举例： 12345678910public class Demo &#123; static int sum(int x, int y)&#123; return x + y; &#125; public static void main(String[] args) &#123; int sum = Demo.sum(10, 10); System.out.println(&quot;10+10=&quot; + sum); &#125;&#125; 运行结果： 110+10=20 static 方法不需它所属的类的任何实例就会被调用，因此没有 this 值，不能访问实例变量，否则会引起编译错误。 注意：实例变量只能通过对象来访问，不能通过类访问。 1.3 静态初始器（静态块）块是由大括号包围的一段代码。静态初始器(Static Initializer)是一个存在于类中、方法外面的静态块。静态初始器仅仅在类装载的时候（第一次使用类的时候）执行一次，往往用来初始化静态变量。 示例代码： 123456789101112131415public class Demo &#123; public static int i; static&#123; i = 10; System.out.println(&quot;Now in static block.&quot;); &#125; public void test() &#123; System.out.println(&quot;test method: i=&quot; + i); &#125; public static void main(String[] args) &#123; System.out.println(&quot;Demo.i=&quot; + Demo.i); new Demo().test(); &#125;&#125; 运行结果是： 123Now in static block.Demo.i=10test method: i=10 1.4 静态导入静态导入是 Java 5 的新增特性，用来导入类的静态变量和静态方法。 一般我们导入类都这样写： 1import packageName.className; // 导入某个特定的类 或者 1import packageName.*; // 导入包中的所有类 而静态导入可以这样写： 1import static packageName.className.methonName; // 导入某个特定的静态方法 或者 1import static packageName.className.*; // 导入类中的所有静态成员 导入后，可以在当前类中直接用方法名调用静态方法，不必再用 className.methodName 来访问。 对于使用频繁的静态变量和静态方法，可以将其静态导入。静态导入的好处是可以简化一些操作，例如输出语句 System.out.println(); 中的 out 就是 System 类的静态变量，可以通过 import static java.lang.System.*; 将其导入，下次直接调用 out.println() 就可以了。 1234567import static java.lang.System.*;import static java.lang.Math.random;public class Demo &#123; public static void main(String[] args) &#123; out.println(&quot;产生的一个随机数：&quot; + random()); &#125;&#125; 运行结果： 1产生的一个随机数：0.05800891549018705 二、final关键字在 Java 中，声明类、变量和方法时，可使用关键字 final 来修饰。final 所修饰的数据具有“终态”的特征，表示“最终的”意思。具体规定如下： final 修饰的类不能被继承。 final 修饰的方法不能被子类重写。 final 修饰的变量（成员变量或局部变量）即成为常量，只能赋值一次。 final 修饰的成员变量必须在声明的同时赋值，如果在声明的时候没有赋值，那么只有 一次赋值的机会，而且只能在构造方法中显式赋值，然后才能使用。 final 修饰的局部变量可以只声明不赋值，然后再进行一次性的赋值。 final 一般用于修饰那些通用性的功能、实现方式或取值不能随意被改变的数据，以避免被误用，例如实现数学三角方法、幂运算等功能的方法，以及数学常量π=3.141593、e=2.71828 等。 事实上，为确保终态性，提供了上述方法和常量的 java.lang.Math 类也已被定义为final 的。 需要注意的是，如果将引用类型（任何类的类型）的变量标记为 final，那么该变量不能指向任何其它对象。但可以改变对象的内容，因为只有引用本身是 final 的。 如果变量被标记为 final，其结果是使它成为常数。想改变 final 变量的值会导致一个编译错误。下面是一个正确定义 final 变量的例子： 1public final int MAX_ARRAY_SIZE = 25; // 常量名一般大写 常量因为有 final 修饰，所以不能被继承。请看下面的代码： 12345678910111213141516public final class Demo&#123; public static final int TOTAL_NUMBER = 5; public int id; public Demo() &#123; // 非法，对final变量TOTAL_NUMBER进行二次赋值了 // 因为++TOTAL_NUMBER相当于 TOTAL_NUMBER=TOTAL_NUMBER+1 id = ++TOTAL_NUMBER; &#125; public static void main(String[] args) &#123; final Demo t = new Demo(); final int i = 10; final int j; j = 20; j = 30; // 非法，对final变量进行二次赋值 &#125;&#125; final 也可以用来修饰类（放在 class 关键字前面），阻止该类再派生出子类，例如 Java.lang.String 就是一个 final 类。这样做是出于安全原因，因为要保证一旦有字符串的引用，就必须是类 String 的字符串，而不是某个其它类的字符串（String 类可能被恶意继承并篡改）。 方法也可以被 final 修饰，被 final 修饰的方法不能被覆盖；变量也可以被 final 修饰，被 final 修饰的变量在创建对象以后就不允许改变它们的值了。一旦将一个类声明为 final，那么该类包含的方法也将被隐式地声明为 final，但是变量不是。 被 final 修饰的方法为静态绑定，不会产生多态（动态绑定），程序在运行时不需要再检索方法表，能够提高代码的执行效率。在Java中，被 static 或 private 修饰的方法会被隐式的声明为 final，因为动态绑定没有意义。 由于动态绑定会消耗资源并且很多时候没有必要，所以有一些程序员认为：除非有足够的理由使用多态性，否则应该将所有的方法都用 final 修饰。 这样的认识未免有些偏激，因为 JVM 中的即时编译器能够实时监控程序的运行信息，可以准确的知道类之间的继承关系。如果一个方法没有被覆盖并且很短，编译器就能够对它进行优化处理，这个过程为称为内联(inlining)。例如，内联调用 e.getName() 将被替换为访问 e.name 变量。这是一项很有意义的改进，这是由于CPU在处理调用方法的指令时，使用的分支转移会扰乱预取指令的策略，所以，这被视为不受欢迎的。然而，如果 getName() 在另外一个类中被覆盖，那么编译器就无法知道覆盖的代码将会做什么操作，因此也就不能对它进行内联处理了。 三、Object类Object 类位于 java.lang 包中，是所有 Java 类的祖先，Java 中的每个类都由它扩展而来。定义Java类时如果没有显示的指明父类，那么就默认继承了 Object 类。例如： 123public class Demo&#123; // ...&#125; 实际上是下面代码的简写形式： 123public class Demo extends Object&#123; // ...&#125; 在Java中，只有基本类型不是对象，例如数值、字符和布尔型的值都不是对象，所有的数组类型，不管是对象数组还是基本类型数组都是继承自 Object 类。 Object 类定义了一些有用的方法，由于是根类，这些方法在其他类中都存在，一般是进行了重载或覆盖，实现了各自的具体功能。 3.1 equals() 方法Object 类中的 equals() 方法用来检测一个对象是否等价于另外一个对象，语法为： 1public boolean equals(Object obj) 例如： 1obj1.equals(obj2); 在Java中，数据等价的基本含义是指两个数据的值相等。在通过 equals() 和“==”进行比较的时候，引用类型数据比较的是引用，即内存地址，基本数据类型比较的是值。 在Java中，数据等价的基本含义是指两个数据的值相等。在通过 equals() 和“==”进行比较的时候，引用类型数据比较的是引用，即内存地址，基本数据类型比较的是值。 注意： equals()方法只能比较引用类型，“==”可以比较引用类型及基本类型。 当用 equals() 方法进行比较时，对类 File、String、Date 及包装类来说，是比较类型及内容而不考虑引用的是否是同一个实例。 用“==”进行比较时，符号两边的数据类型必须一致（可自动转换的数据类型除外），否则编译出错，而用 equals 方法比较的两个数据只要都是引用类型即可。 3.2 hashCode()方法散列码(hashCode)是按照一定的算法由对象得到的一个数值，散列码没有规律。如果 x 和 y 是不同的对象，x.hashCode() 与 y.hashCode() 基本上不会相同。 hashCode() 方法主要用来在集合中实现快速查找等操作，也可以用于对象的比较。 在 Java 中，对 hashCode 的规定如下： 在同一个应用程序执行期间，对同一个对象调用 hashCode()，必须返回相同的整数结果——前提是 equals() 所比较的信息都不曾被改动过。至于同一个应用程序在不同执行期所得的调用结果，无需一致。 如果两个对象被 equals() 方法视为相等，那么对这两个对象调用 hashCode() 必须获得相同的整数结果。 如果两个对象被 equals() 方法视为不相等，那么对这两个对象调用 hashCode() 不必产生不同的整数结果。然而程序员应该意识到，对不同对象产生不同的整数结果，有可能提升hashTable的效率。 简单地说：如果两个对象相同，那么它们的 hashCode 值一定要相同；如果两个对象的 hashCode 值相同，它们并不一定相同。在 Java 规范里面规定，一般是覆盖 equals() 方法应该连带覆盖 hashCode() 方法。 3.3 toString()方法toString() 方法是 Object 类中定义的另一个重要方法，是对象的字符串表现形式，语法为： 1public String toString() 返回值是 String 类型，用于描述当前对象的有关信息。Object 类中实现的 toString() 方法是返回当前对象的类型和内存地址信息，但在一些子类（如 String、Date 等）中进行了 重写，也可以根据需要在用户自定义类型中重写 toString() 方法，以返回更适用的信息。 除显式调用对象的 toString() 方法外，在进行 String 与其它类型数据的连接操作时，会自动调用 toString() 方法。 以上几种方法，在Java中是经常用到的，这里仅作简单介绍，让大家对Object类和其他类有所了解，详细说明请参考 Java API 文档。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>static</tag>
        <tag>final</tag>
        <tag>Object</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（4）：多态]]></title>
    <url>%2F2017%2F08%2F23%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89%EF%BC%9A%E5%A4%9A%E6%80%81%2F</url>
    <content type="text"><![CDATA[一、多态在Java中，父类的变量可以引用父类的实例，也可以引用子类的实例。请读者先看一段代码：12345678910111213141516171819202122232425262728293031323334public class Demo &#123; public static void main(String[] args)&#123; Animal obj = new Animal(); obj.cry(); obj = new Cat(); obj.cry(); obj = new Dog(); obj.cry(); &#125;&#125;class Animal&#123; // 动物的叫声 public void cry()&#123; System.out.println(&quot;不知道怎么叫&quot;); &#125; &#125;class Cat extends Animal&#123; // 猫的叫声 public void cry()&#123; System.out.println(&quot;喵喵~&quot;); &#125;&#125;class Dog extends Animal&#123; // 狗的叫声 public void cry()&#123; System.out.println(&quot;汪汪~&quot;); &#125;&#125;运行结果：123不知道怎么叫喵喵~汪汪~上面的代码，定义了三个类，分别是 Animal、Cat 和 Dog，Cat 和 Dog 类都继承自 Animal 类。obj 变量的类型为 Animal，它既可以指向 Animal 类的实例，也可以指向 Cat 和 Dog 类的实例，这是正确的。也就是说，父类的变量可以引用父类的实例，也可以引用子类的实例。注意反过来是错误的，因为所有的猫都是动物，但不是所有的动物都是猫。可以看出，obj 既可以是人类，也可以是猫、狗，它有不同的表现形式，这就被称为多态。多态是指一个事物有不同的表现形式或形态。再比如“人类”，也有很多不同的表达或实现，TA 可以是司机、教师、医生等，你憎恨自己的时候会说“下辈子重新做人”，那么你下辈子成为司机、教师、医生都可以，我们就说“人类”具备了多态性。多态存在的三个必要条件：要有继承、要有重写、父类变量引用子类对象。1234567891011121314151617181920212223242526272829303132333435363738394041public class Demo &#123; public static void main(String[] args)&#123; Animal obj = new Animal(); obj.cry(); obj = new Cat(); obj.bark(); obj = new Dog(); obj.cry(); &#125;&#125;class Animal&#123; // 动物的叫声 public void cry()&#123; System.out.println(&quot;不知道怎么叫&quot;); &#125; public void bark() &#123; System.out.println(&quot;哈哈！！&quot;); &#125; &#125;class Cat extends Animal&#123; // 猫的叫声 public void cry()&#123; System.out.println(&quot;喵喵~&quot;); &#125; public void bark()&#123; System.out.println(&quot;丫丫！！&quot;); &#125; &#125;class Dog extends Animal&#123; // 狗的叫声 public void cry()&#123; System.out.println(&quot;汪汪~&quot;); &#125;&#125;从上面的例子可以看出，多态的一个好处是：当子类比较多时，也不需要定义多个变量，可以只定义一个父类类型的变量来引用不同子类的实例。请再看下面的一个例子：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class Demo &#123; public static void main(String[] args)&#123; // 借助多态，主人可以给很多动物喂食 Master ma = new Master(); ma.feed(new Animal(), new Food()); ma.feed(new Cat(), new Fish()); ma.feed(new Dog(), new Bone()); &#125;&#125;// Animal类及其子类class Animal&#123; public void eat(Food f)&#123; System.out.println(&quot;我是一个小动物，正在吃&quot; + f.getFood()); &#125;&#125;class Cat extends Animal&#123; public void eat(Food f)&#123; System.out.println(&quot;我是一只小猫咪，正在吃&quot; + f.getFood()); &#125;&#125;class Dog extends Animal&#123; public void eat(Food f)&#123; System.out.println(&quot;我是一只狗狗，正在吃&quot; + f.getFood()); &#125;&#125;// Food及其子类class Food&#123; public String getFood()&#123; return &quot;食物&quot;; &#125;&#125;class Fish extends Food&#123; public String getFood()&#123; return &quot;鱼&quot;; &#125;&#125;class Bone extends Food&#123; public String getFood()&#123; return &quot;骨头&quot;; &#125;&#125;// Master类class Master&#123; public void feed(Animal an, Food f)&#123; an.eat(f); &#125;&#125;运行结果：123我是一个小动物，正在吃事物我是一只小猫咪，正在吃鱼我是一只狗狗，正在吃骨头Master 类的 feed 方法有两个参数，分别是 Animal 类型和 Food 类型，因为是父类，所以可以将子类的实例传递给它，这样 Master 类就不需要多个方法来给不同的动物喂食。二、动态绑定为了理解多态的本质，下面讲一下Java调用方法的详细流程。1) 编译器查看对象的声明类型和方法名。假设调用 obj.func(param)，obj 为 Cat 类的对象。需要注意的是，有可能存在多个名字为func但参数签名不一样的方法。例如，可能存在方法 func(int) 和 func(String)。编译器将会一一列举所有 Cat 类中名为func的方法和其父类 Animal 中访问属性为 public 且名为func的方法。这样，编译器就获得了所有可能被调用的候选方法列表。2) 接下来，编泽器将检查调用方法时提供的参数签名。如果在所有名为func的方法中存在一个与提供的参数签名完全匹配的方法，那么就选择这个方法。这个过程被称为重载解析(overloading resolution)。例如，如果调用 func(“hello”)，编译器会选择 func(String)，而不是 func(int)。由于自动类型转换的存在，例如 int 可以转换为 double，如果没有找到与调用方法参数签名相同的方法，就进行类型转换后再继续查找，如果最终没有匹配的类型或者有多个方法与之匹配，那么编译错误。这样，编译器就获得了需要调用的方法名字和参数签名。3) 如果方法的修饰符是private、static、final（static和final将在后续讲解），或者是构造方法，那么编译器将可以准确地知道应该调用哪个方法，我们将这种调用方式 称为静态绑定(static binding)。与此对应的是，调用的方法依赖于对象的实际类型， 并在运行时实现动态绑。例如调用 func(“hello”)，编泽器将采用动态绑定的方式生成一条调用 func(String) 的指令。4)当程序运行，并且釆用动态绑定调用方法时，JVM一定会调用与 obj 所引用对象的实际类型最合适的那个类的方法。我们已经假设 obj 的实际类型是 Cat，它是 Animal 的子类，如果 Cat 中定义了 func(String)，就调用它，否则将在 Animal 类及其父类中寻找。每次调用方法都要进行搜索，时间开销相当大，因此，JVM预先为每个类创建了一个方法表(method lable)，其中列出了所有方法的名称、参数签名和所属的类。这样一来，在真正调用方法的时候，虚拟机仅查找这个表就行了。在上面的例子中，JVM 搜索 Cat 类的方法表，以便寻找与调用 func(“hello”) 相匹配的方法。这个方法既有可能是 Cat.func(String)，也有可能是 Animal.func(String)。注意，如果调用super.func(“hello”)，编译器将对父类的方法表迸行搜索。假设 Animal 类包含cry()、getName()、getAge() 三个方法，那么它的方法表如下：cry() -&gt; Animal.cry()getName() -&gt; Animal.getName()getAge() -&gt; Animal.getAge()实际上，Animal 也有默认的父类 Object（后续会讲解），会继承 Object 的方法，所以上面列举的方法并不完整。假设 Cat 类覆盖了 Animal 类中的 cry() 方法，并且新增了一个方法 climbTree()，那么它的参数列表为：cry() -&gt; Cat.cry()getName() -&gt; Animal.getName()getAge() -&gt; Animal.getAge()climbTree() -&gt; Cat.climbTree()在运行的时候，调用 obj.cry() 方法的过程如下：JVM 首先访问 obj 的实际类型的方法表，可能是 Animal 类的方法表，也可能是 Cat 类及其子类的方法表。JVM 在方法表中搜索与 cry() 匹配的方法，找到后，就知道它属于哪个类了。JVM 调用该方法。三、instanceof运算符多态性带来了一个问题，就是如何判断一个变量所实际引用的对象的类型 。 C++使用runtime-type information(RTTI)，Java 使用 instanceof 操作符。instanceof 运算符用来判断一个变量所引用的对象的实际类型，注意是它引用的对象的类型，不是变量的类型。请看下面的代码：123456789101112131415161718192021222324252627282930313233343536373839public final class Demo&#123; public static void main(String[] args) &#123; // 引用 People 类的实例 People obj = new People(); if(obj instanceof Object)&#123; System.out.println(&quot;我是一个对象&quot;); &#125; if(obj instanceof People)&#123; System.out.println(&quot;我是人类&quot;); &#125; if(obj instanceof Teacher)&#123; System.out.println(&quot;我是一名教师&quot;); &#125; if(obj instanceof President)&#123; System.out.println(&quot;我是校长&quot;); &#125; System.out.println(&quot;-----------&quot;); // 分界线 // 引用 Teacher 类的实例 obj = new Teacher(); if(obj instanceof Object)&#123; System.out.println(&quot;我是一个对象&quot;); &#125; if(obj instanceof People)&#123; System.out.println(&quot;我是人类&quot;); &#125; if(obj instanceof Teacher)&#123; System.out.println(&quot;我是一名教师&quot;); &#125; if(obj instanceof President)&#123; System.out.println(&quot;我是校长&quot;); &#125; &#125;&#125;class People&#123; &#125;class Teacher extends People&#123; &#125;class President extends Teacher&#123; &#125;运行结果：123456我是一个对象我是人类———–我是一个对象我是人类我是一名教师可以看出，如果变量引用的是当前类或它的子类的实例，instanceof 返回 true，否则返回 false。四、多态对象的类型转换这里所说的对象类型转换，是指存在继承关系的对象，不是任意类型的对象。当对不存在继承关系的对象进行强制类型转换时，java 运行时将抛出 java.lang.ClassCastException 异常。在继承链中，我们将子类向父类转换称为“向上转型”，将父类向子类转换称为“向下转型”。很多时候，我们会将变量定义为父类的类型，却引用子类的对象，这个过程就是向上转型。程序运行时通过动态绑定来实现对子类方法的调用，也就是多态性。然而有些时候为了完成某些父类没有的功能，我们需要将向上转型后的子类对象再转成子类，调用子类的方法，这就是向下转型。注意：不能直接将父类的对象强制转换为子类类型，只能将向上转型后的子类对象再次转换为子类类型。也就是说，子类对象必须向上转型后，才能再向下转型。请看下面的代码：12345678910111213141516class Demo &#123; public static void main(String args[]) &#123; SuperClass superObj = new SuperClass(); SonClass sonObj = new SonClass(); // 下面的代码运行时会抛出异常，不能将父类对象直接转换为子类类型 // SonClass sonObj2 = (SonClass)superObj; // 先向上转型，再向下转型 superObj = sonObj; SonClass sonObj1 = (SonClass)superObj; &#125;&#125;class SuperClass&#123; &#125;class SonClass extends SuperClass&#123; &#125;将第7行的注释去掉，运行时会抛出异常，但是编译可以通过。因为向下转型存在风险，所以在接收到父类的一个引用时，请务必使用 instanceof 运算符来判断该对象是否是你所要的子类，请看下面的代码：123456789101112131415161718192021222324public class Demo &#123; public static void main(String args[]) &#123; SuperClass superObj = new SuperClass(); SonClass sonObj = new SonClass(); // superObj 不是 SonClass 类的实例 if(superObj instanceof SonClass)&#123; SonClass sonObj1 = (SonClass)superObj; &#125;else&#123; System.out.println(&quot;①不能转换&quot;); &#125; superObj = sonObj; // superObj 是 SonClass 类的实例 if(superObj instanceof SonClass)&#123; SonClass sonObj2 = (SonClass)superObj; &#125;else&#123; System.out.println(&quot;②不能转换&quot;); &#125; &#125;&#125;class SuperClass&#123; &#125;class SonClass extends SuperClass&#123; &#125;运行结果：①不能转换总结：对象的类型转换在程序运行时检查，向上转型会自动进行，向下转型的对象必须是当前引用类型的子类。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多态</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（3）：继承、覆盖、重载]]></title>
    <url>%2F2017%2F08%2F22%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%883%EF%BC%89%EF%BC%9A%E7%BB%A7%E6%89%BF%2F</url>
    <content type="text"><![CDATA[一、继承的概念与实现继承是类与类之间的关系，是一个很简单很直观的概念，与现实世界中的继承（例如儿子继承父亲财产）类似。继承可以理解为一个类从另一个类获取方法和属性的过程。如果类B继承于类A，那么B就拥有A的方法和属性。继承使用 extends 关键字。例如我们已经定义了一个类 People：123456789class People&#123; String name; int age; int height; void say()&#123; System.out.println(&quot;我的名字是 &quot; + name + &quot;，年龄是 &quot; + age + &quot;，身高是 &quot; + height); &#125;&#125;如果现在需要定义一个类 Teacher，它也有 name、age、height 属性和 say() 方法，另外还需要增加 school、seniority、subject 属性和 lecturing() 方法，怎么办呢？我们要重新定义一个类吗？完全没必要，可以先继承 People 类的成员，再增加自己的成员即可，例如：1234567891011121314class Teacher extends People&#123; String school; // 所在学校 String subject; // 学科 int seniority; // 教龄 // 覆盖 People 类中的 say() 方法 void say()&#123; System.out.println(&quot;我叫&quot; + name + &quot;，在&quot; + school + &quot;教&quot; + subject + &quot;，有&quot; + seniority + &quot;年教龄&quot;); &#125; void lecturing()&#123; System.out.println(&quot;我已经&quot; + age + &quot;岁了，依然站在讲台上讲课&quot;); &#125;&#125;对程序的说明name 和 age 变量虽然没有在 Teacher 中定义，但是已在 People 中定义，可以直接拿来用。Teacher 是 People 的子类，People 是Teacher 类的父类。子类可以覆盖父类的方法。子类可以继承父类除private以为的所有的成员。构造方法不能被继承。继承是在维护和可靠性方面的一个伟大进步。如果在 People 类中进行修改，那么 Teacher 类就会自动修改，而不需要程序员做任何工作，除了对它进行编译。单继承性：Java 允许一个类仅能继承一个其它类，即一个类只能有一个父类，这个限制被称做单继承性。后面将会学到接口(interface)的概念，接口允许多继承。最后对上面的代码进行整理：12345678910111213141516171819202122232425262728293031323334353637public class Demo &#123; public static void main(String[] args) &#123; Teacher t = new Teacher(); t.name = &quot;小布&quot;; t.age = 70; t.school = &quot;清华大学&quot;; t.subject = &quot;Java&quot;; t.seniority = 12; t.say(); t.lecturing(); &#125;&#125;class People&#123; String name; int age; int height; void say()&#123; System.out.println(&quot;我的名字是 &quot; + name + &quot;，年龄是 &quot; + age + &quot;，身高是 &quot; + height); &#125;&#125;class Teacher extends People&#123; String school; // 所在学校 String subject; // 学科 int seniority; // 教龄 // 覆盖 People 类中的 say() 方法 void say()&#123; System.out.println(&quot;我叫&quot; + name + &quot;，在&quot; + school + &quot;教&quot; + subject + &quot;，有&quot; + seniority + &quot;年教龄&quot;); &#125; void lecturing()&#123; System.out.println(&quot;我已经&quot; + age + &quot;岁了，依然站在讲台上讲课&quot;); &#125;&#125;运行结果：12我叫小布，在清华大学教Java，有12年教龄我已经70岁了，依然站在讲台上讲课注意：构造方法不能被继承，掌握这一点很重要。 一个类能得到构造方法，只有两个办法：编写构造方法，或者根本没有构造方法，类有一个默认的构造方法。二、super关键字super 关键字与 this 类似，this 用来表示当前类的实例，super 用来表示父类。super 可以用在子类中，通过点号(.)来获取父类的成员变量和方法。super 也可以用在子类的子类中，Java 能自动向上层类追溯。父类行为被调用，就好象该行为是本类的行为一样，而且调用行为不必发生在父类中，它能自动向上层类追溯。super 关键字的功能：调用父类中声明为 private 的变量。点取已经覆盖了的方法。作为方法名表示父类构造方法。2.1 调用隐藏变量和被覆盖的方法1234567891011121314151617181920212223public class Demo&#123; public static void main(String[] args) &#123; Dog obj = new Dog(); obj.move(); &#125;&#125;class Animal&#123; private String desc = &quot;Animals are human&apos;s good friends&quot;; // 必须要声明一个 getter 方法 public String getDesc() &#123; return desc; &#125; public void move()&#123; System.out.println(&quot;Animals can move&quot;); &#125;&#125;class Dog extends Animal&#123; public void move()&#123; super.move(); // 调用父类的方法 System.out.println(&quot;Dogs can walk and run&quot;); // 通过 getter 方法调用父类隐藏变量 System.out.println(&quot;Please remember: &quot; + super.getDesc()); &#125;&#125;运行结果：123Animals can moveDogs can walk and runPlease remember: Animals are human’s good friendsmove() 方法也可以定义在某些祖先类中，比如父类的父类，Java 具有追溯性，会一直向上找，直到找到该方法为止。通过 super 调用父类的隐藏变量，必须要在父类中声明 getter 方法，因为声明为 private 的数据成员对子类是不可见的。2.2 调用父类的构造方法在许多情况下，使用默认构造方法来对父类对象进行初始化。当然也可以使用 super 来显示调用父类的构造方法。1234567891011121314151617181920212223public class Demo&#123; public static void main(String[] args) &#123; Dog obj = new Dog(&quot;花花&quot;, 3); obj.say(); &#125;&#125;class Animal&#123; String name; public Animal(String name)&#123; this.name = name; &#125;&#125;class Dog extends Animal&#123; int age; public Dog(String name, int age)&#123; super(name); this.age = age; &#125; public void say()&#123; System.out.println(&quot;我是一只可爱的小狗，我的名字叫&quot; + name + &quot;，我&quot; + age + &quot;岁了&quot;); &#125;&#125;运行结果：1我是一只可爱的小狗，我的名字叫花花，我3岁了注意：无论是 super() 还是 this()，都必须放在构造方法的第一行。值得注意的是：在构造方法中调用另一个构造方法，调用动作必须置于最起始的位置。不能在构造方法以外的任何方法内调用构造方法。在一个构造方法内只能调用一个构造方法。如果编写一个构造方法，既没有调用 super() 也没有调用 this()，编译器会自动插入一个调用到父类构造方法中，而且不带参数。最后注意 super 与 this 的区别：super 不是一个对象的引用，不能将 super 赋值给另一个对象变量，它只是一个指示编译器调用父类方法的特殊关键字。三、继承中的方法的覆盖和重载3.1 覆盖在类继承中，子类可以修改从父类继承来的方法，也就是说子类能创建一个与父类方法有不同功能的方法，但具有相同的名称、返回值类型、参数列表。如果在新类中定义一个方法，其名称、返回值类型和参数列表正好与父类中的相同，那么，新方法被称做覆盖旧方法。参数列表又叫参数签名，包括参数的类型、参数的个数和参数的顺序，只要有一个不同就叫做参数列表不同。被覆盖的方法在子类中只能通过super调用。注意：覆盖不会删除父类中的方法，而是对子类的实例隐藏，暂时不使用。123456789101112131415161718192021222324252627282930public class Demo&#123; public static void main(String[] args) &#123; Dog myDog = new Dog("花花"); myDog.say(); // 子类的实例调用子类中的方法 Animal myAnmial = new Animal("贝贝"); myAnmial.say(); // 父类的实例调用父类中的方法 &#125;&#125;class Animal&#123; String name; public Animal(String name)&#123; this.name = name; &#125; public void say()&#123; System.out.println("我是一只小动物，我的名字叫" + name + "，我会发出叫声"); &#125;&#125;class Dog extends Animal&#123; // 构造方法不能被继承，通过super()调用 public Dog(String name)&#123; super(name); &#125; // 覆盖say() 方法 public void say()&#123; System.out.println("我是一只小狗，我的名字叫" + name + "，我会发出汪汪的叫声"); &#125;&#125;运行结果：12我是一只小狗，我的名字叫花花，我会发出汪汪的叫声我是一只小动物，我的名字叫贝贝，我会发出叫声方法覆盖的原则：覆盖方法的返回类型、方法名称、参数列表必须与原方法的相同。覆盖方法不能比原方法访问性差（即访问权限不允许缩小）。覆盖方法不能比原方法抛出更多的异常。被覆盖的方法不能是final类型，因为final修饰的方法是无法覆盖的。被覆盖的方法不能为private，否则在其子类中只是新定义了一个方法，并没有对其进行覆盖。被覆盖的方法不能为static。如果父类中的方法为静态的，而子类中的方法不是静态的，但是两个方法除了这一点外其他都满足覆盖条件，那么会发生编译错误；反之亦然。即使父类和子类中的方法都是静态的，并且满足覆盖条件，但是仍然不会发生覆盖，因为静态方法是在编译的时候把静态方法和类的引用类型进行匹配。3.2 重载前面已经对Java方法重载进行了说明，这里再强调一下，Java父类和子类中的方法都会参与重载，例如，父类中有一个方法是 func(){ … }，子类中有一个方法是 func(int i){ … }，就构成了方法的重载。覆盖和重载的不同：方法覆盖要求参数列表必须一致，而方法重载要求参数列表必须不一致。方法覆盖要求返回类型必须一致，方法重载对此没有要求。方法覆盖只能用于子类覆盖父类的方法，方法重载用于同一个类中的所有方法（包括从父类中继承而来的方法）。方法覆盖对方法的访问权限和抛出的异常有特殊的要求，而方法重载在这方面没有任何限制。父类的一个方法只能被子类覆盖一次，而一个方法可以在所有的类中可以被重载多次]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>继承</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（2）：类与对象]]></title>
    <url>%2F2017%2F08%2F21%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89%EF%BC%9A%E7%B1%BB%E4%B8%8E%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[一、类的定义及其实例化类必须先定义才能使用。类是创建对象的模板，创建对象也叫类的实例化。下面通过一个简单的例子来理解Java中类的定义：123456789101112public class Dog&#123; String name; int age; void bark()&#123; // 汪汪叫 System.out.println("汪汪，不要过来"); &#125; void hungry()&#123; // 饥饿 System.out.println("主人，我饿了"); &#125;&#125;对示例的说明：public 是类的修饰符，表明该类是公共类，可以被其他类访问。修饰符将在下节讲解。class 是定义类的关键字。Dog 是类名称。name、age是类的成员变量，也叫属性；bark()、hungry() 是类中的函数，也叫方法。一个类可以包含以下类型变量：局部变量：在方法或者语句块中定义的变量被称为局部变量。变量声明和初始化都是在方法中，方法结束后，变量就会自动销毁。成员变量：成员变量是定义在类中、方法体之外的变量。这种变量在创建对象的时候实例化（分配内存）。成员变量可以被类中的方法和特定类的语句访问。类变量：类变量也声明在类中，方法体之外，但必须声明为static类型。static 也是修饰符的一种，将在下节讲解。1.1 构造方法在类实例化的过程中自动执行的方法叫做构造方法，它不需要你手动调用。构造方法可以在类实例化的过程中做一些初始化的工作。构造方法的名称必须与类的名称相同，并且没有返回值。每个类都有构造方法。如果没有显式地为类定义构造方法，Java编译器将会为该类提供一个默认的构造方法。下面是一个构造方法示例：12345678910111213141516171819202122232425public class Dog&#123; String name; int age; // 构造方法，没有返回值 Dog(String name1, int age1)&#123; name = name1; age = age1; System.out.println("感谢主人领养了我"); &#125; // 普通方法，必须有返回值 void bark()&#123; System.out.println("汪汪，不要过来"); &#125; void hungry()&#123; System.out.println("主人，我饿了"); &#125; public static void main(String arg[])&#123; // 创建对象时传递的参数要与构造方法参数列表对应 Dog myDog = new Dog("花花", 3); &#125;&#125;运行结果：1感谢主人领养了我说明：构造方法不能被显示调用。构造方法不能有返回值，因为没有变量来接收返回值。1.2 创建对象对象是类的一个实例，创建对象的过程也叫类的实例化。对象是以类为模板来创建的。在Java中，使用new关键字来创建对象，一般有以下三个步骤：声明：声明一个对象，包括对象名称和对象类型。实例化：使用关键字new来创建一个对象。初始化：使用new创建对象时，会调用构造方法初始化对象。例如：12Dog myDog; // 声明一个对象myDog = new Dog(&quot;花花&quot;, 3); // 实例化也可以在声明的同时进行初始化：1Dog myDog = new Dog(&quot;花花&quot;, 3);1.3 访问成员变量和方法通过已创建的对象来访问成员变量和成员方法，例如：123456//实例化Dog myDog = new Dog(&quot;花花&quot;, 3);// 通过点号访问成员变量myDog.name;// 通过点号访问成员方法myDog.bark();下面的例子演示了如何访问成员变量和方法：1234567891011121314151617181920212223242526272829public class Dog&#123; String name; int age; Dog(String name1, int age1)&#123; name = name1; age = age1; System.out.println("感谢主人领养了我"); &#125; void bark()&#123; System.out.println("汪汪，不要过来"); &#125; void hungry()&#123; System.out.println("主人，我饿了"); &#125; public static void main(String arg[])&#123; Dog myDog = new Dog("花花", 3); // 访问成员变量 String name = myDog.name; int age = myDog.age; System.out.println("我是一只小狗，我名字叫" + name + "，我" + age + "岁了"); // 访问方法 myDog.bark(); myDog.hungry(); &#125;&#125;运行结果：1234感谢主人领养了我我是一只小狗，我名字叫花花，我3岁了汪汪，不要过来主人，我饿了二、Java访问修饰符Java 通过修饰符来控制类、属性和方法的访问权限和其他功能，通常放在语句的最前端。例如：123456789public class className &#123; // body of class&#125;private boolean myFlag;static final double weeks = 9.5;protected static final int BOXWIDTH = 42;public static void main(String[] arguments) &#123; // body of method&#125;Java 的修饰符很多，分为访问修饰符和非访问修饰符。本节仅介绍访问修饰符，非访问修饰符会在后续介绍。访问修饰符也叫访问控制符，是指能够控制类、成员变量、方法的使用权限的关键字。在面向对象编程中，访问控制符是一个很重要的概念，可以使用它来保护对类、变量、方法和构造方法的访问。Java支持四种不同的访问权限：修饰符说明public公有的，对所有类可见protected受保护的，对同一包内的类和所有子类可见private私有的，在同一类内可见默认的在同一包内可见。默认不使用任何修饰符2.1 public：公有的被声明为public的类、方法、构造方法和接口能够被任何其他类访问。如果几个相互访问的public类分布在不同的包中，则需要导入相应public类所在的包。由于类的继承性，类所有的公有方法和变量都能被其子类继承。下面的方法使用了公有访问控制：123public static void main(String[] arguments) &#123; // body of method&#125;Java程序的main() 方法必须设置成公有的，否则，Java解释器将不能运行该类。2.2 protected：受保护的被声明为protected的变量、方法和构造方法不能被同一个包中的任何其他类访问，也不能够被不同包中的子类访问。protected访问修饰符不能修饰类和接口，方法和成员变量能够声明为protected，但是接口的成员变量和成员方法不能声明为protected。子类能访问protected修饰符声明的方法和变量，这样就能保护不相关的类使用这些方法和变量。下面的父类使用了protected访问修饰符，子类重载了父类的bark()方法。1234567891011public class Dog&#123; protected void bark() &#123; System.out.println("汪汪，不要过来"); &#125;&#125;class Teddy extends Dog&#123; // 泰迪 void bark() &#123; System.out.println("汪汪，我好怕，不要跟着我"); &#125;&#125;如果把bark()方法声明为private，那么除了Dog之外的类将不能访问该方法。如果把bark()声明为public，那么所有的类都能够访问该方法。如果我们只想让该方法对其所在类的子类可见，则将该方法声明为protected。2.3 private：私有的私有访问修饰符是最严格的访问级别，所以被声明为private的方法、变量和构造方法只能被所属类访问，并且类和接口不能声明为private。声明为私有访问类型的变量只能通过类中公共的Getter/Setter方法被外部类访问。private访问修饰符的使用主要用来隐藏类的实现细节和保护类的数据。下面的类使用了私有访问修饰符：12345678910111213141516public class Dog&#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125;例子中，Dog类中的name、age变量为私有变量，所以其他类不能直接得到和设置该变量的值。为了使其他类能够操作该变量，定义了两对public方法，getName()/setName() 和 getAge()/setAge()，它们用来获取和设置私有变量的值。this 是Java中的一个关键字，接下来会讲到。在类中定义访问私有变量的方法，习惯上是这样命名的：在变量名称前面加“get”或“set”，并将变量的首字母大写。例如，获取私有变量 name 的方法为 getName()，设置 name 的方法为 setName()。这些方法经常使用，也有了特定的称呼，称为 Getter 和 Setter 方法。2.4 默认的：不使用任何关键字不使用任何修饰符声明的属性和方法，对同一个包内的类是可见的。接口里的变量都隐式声明为public static final，而接口里的方法默认情况下访问权限为public。如下例所示，类、变量和方法的定义没有使用任何修饰符：123456789101112class Dog&#123; String name; int age; void bark()&#123; // 汪汪叫 System.out.println(&quot;汪汪，不要过来&quot;); &#125; void hungry()&#123; // 饥饿 System.out.println(&quot;主人，我饿了&quot;); &#125;&#125;2.5 访问控制和继承请注意以下方法继承的规则：父类中声明为public的方法在子类中也必须为public。父类中声明为protected的方法在子类中要么声明为protected，要么声明为public。不能声明为private。父类中默认修饰符声明的方法，能够在子类中声明为private。父类中声明为private的方法，不能够被继承。2.6 如何使用访问控制符访问控制符可以让我们很方便的控制代码的权限：当需要让自己编写的类被所有的其他类访问时，就可以将类的访问控制符声明为 public。当需要让自己的类只能被自己的包中的类访问时，就可以省略访问控制符。当需要控制一个类中的成员数据时，可以将这个类中的成员数据访问控制符设置为 public、protected，或者省略。三、Java变量的作用域在Java中，变量的作用域分为四个级别：类级、对象实例级、方法级、块级。类级变量：又称全局级变量或静态变量，需要使用static关键字修饰，你可以与 C/C++ 中的 static 变量对比学习。类级变量在类定义后就已经存在，占用内存空间，可以通过类名来访问，不需要实例化。对象实例级变量：就是成员变量，实例化后才会分配内存空间，才能访问。方法级变量：就是在方法内部定义的变量，就是局部变量。块级变量：就是定义在一个块内部的变量，变量的生存周期就是这个块，出了这个块就消失了，比如 if、for 语句的块。块是指由大括号包围的代码，例如：12345678&#123;int age = 3; String name = &quot;www.yq1012.com&quot;; // 正确，在块内部可以访问 age 和 name 变量 System.out.println( name + &quot;已经&quot; + age + &quot;岁了&quot;);&#125;// 错误，在块外部无法访问 age 和 name 变量System.out.println( name + &quot;已经&quot; + age + &quot;岁了&quot;);说明：方法内部除了能访问方法级的变量，还可以访问类级和实例级的变量。块内部能够访问类级、实例级变量，如果块被包含在方法内部，它还可以访问方法级的变量。方法级和块级的变量必须被显示地初始化，否则不能访问。演示代码：123456789101112131415161718192021222324252627lic class Demo&#123; public static String name = &quot;程序员&quot;; // 类级变量 public int i; // 对象实例级变量 // 属性块，在类初始化属性时候运行 &#123; int j = 2;// 块级变量 &#125; public void test1() &#123; int j = 3; // 方法级变量 if(j == 3) &#123; int k = 5; // 块级变量 &#125; // 这里不能访问块级变量，块级变量只能在块内部访问 System.out.println(&quot;name=&quot; + name + &quot;, i=&quot; + i + &quot;, j=&quot; + j); &#125; public static void main(String[] args) &#123; // 不创建对象，直接通过类名访问类级变量 System.out.println(Demo.name); // 创建对象并访问它的方法 Demo t = new Demo(); t.test1(); &#125;&#125;运行结果：12程序员name=程序员, i=0, j=3四、this关键字this 关键字用来表示当前对象本身，或当前类的一个实例，通过 this 可以调用本对象的所有方法和属性。例如：123456789101112131415public class Demo&#123; public int x = 10; public int y = 15; public void sum()&#123; // 通过 this 点取成员变量 int z = this.x + this.y; System.out.println("x + y = " + z); &#125; public static void main(String[] args) &#123; Demo obj = new Demo(); obj.sum(); &#125;&#125;运行结果：1x + y = 25上面的程序中，obj 是 Demo 类的一个实例，this 与 obj 等价，执行 int z = this.x + this.y;，就相当于执行 int z = obj.x + obj.y;。注意：this 只有在类实例化后才有意义。4.1 使用this区分同名变量成员变量与方法内部的变量重名时，希望在方法内部调用成员变量，怎么办呢？这时候只能使用this，例如：123456789101112131415161718public class Demo &#123; public String name; public int age; //构造方法 Demo(String name, int age)&#123; this.name = name; this.age = age; &#125; //普通方法 public void say()&#123; System.out.println("网站的名字是" + name + "，已经成立了" + age + "年"); &#125; //main方法 public static void main(String[] args) &#123; Demo obj = new Demo("程序员", 3); obj.say(); &#125;&#125;运行结果：1网站的名字是程序员，已经成立了3年形参的作用域是整个方法体，是局部变量。在Demo()中，形参和成员变量重名，如果不使用this，访问到的就是局部变量name和age，而不是成员变量。在 say() 中，我们没有使用 this，因为成员变量的作用域是整个实例，当然也可以加上 this：Java 默认将所有成员变量和成员方法与 this 关联在一起，因此使用 this 在某些情况下是多余的。4.2 作为方法名来初始化对象也就是相当于调用本类的其它构造方法，它必须作为构造方法的第一句。示例如下：12345678910111213141516171819202122public class Demo&#123; public String name; public int age; public Demo()&#123; this(&quot;程序员&quot;, 3); &#125; public Demo(String name, int age)&#123; this.name = name; this.age = age; &#125; public void say()&#123; System.out.println(&quot;网站的名字是&quot; + name + &quot;，已经成立了&quot; + age + &quot;年&quot;); &#125; public static void main(String[] args) &#123; Demo obj = new Demo(); obj.say(); &#125;&#125;运行结果：1网站的名字是程序员，已经成立了3年值得注意的是：在构造方法中调用另一个构造方法，调用动作必须置于最起始的位置。不能在构造方法以外的任何方法内调用构造方法。在一个构造方法内只能调用一个构造方法。上述代码涉及到方法重载，即Java允许出现多个同名方法，只要参数不同就可以。后续章节会讲解。4.3 作为参数传递需要在某些完全分离的类中调用一个方法，并将当前对象的一个引用作为参数传递时。例如：12345678910111213141516171819202122232425public class Demo&#123; public static void main(String[] args)&#123; B b = new B(new A()); &#125;&#125;class A&#123; public A()&#123; new B(this).print(); // 匿名对象 &#125; public void print()&#123; System.out.println(&quot;Hello from A!&quot;); &#125;&#125;class B&#123; A a; public B(A a)&#123; this.a = a; &#125; public void print() &#123; a.print(); System.out.println(&quot;Hello from B!&quot;); &#125;&#125;运行结果：12Hello from A!Hello from B!匿名对象就是没有名字的对象。如果对象只使用一次，就可以作为匿名对象，代码中 new B(this).print(); 等价于 ( new B(this) ).print();，先通过 new B(this) 创建一个没有名字的对象，再调用它的方法。五、方法重载在Java中，同一个类中的多个方法可以有相同的名字，只要它们的参数列表不同就可以，这被称为方法重载(method overloading)。参数列表又叫参数签名，包括参数的类型、参数的个数和参数的顺序，只要有一个不同就叫做参数列表不同。重载是面向对象的一个基本特性。下面看一个详细的实例。123456789101112131415161718192021222324252627public class Demo&#123; // 一个普通的方法，不带参数 void test()&#123; System.out.println(&quot;No parameters&quot;); &#125; // 重载上面的方法，并且带了一个整型参数 void test(int a)&#123; System.out.println(&quot;a: &quot; + a); &#125; // 重载上面的方法，并且带了两个参数 void test(int a,int b)&#123; System.out.println(&quot;a and b: &quot; + a + &quot; &quot; + b); &#125; // 重载上面的方法，并且带了一个双精度参数 double test(double a)&#123; System.out.println(&quot;double a: &quot; + a); return a*a; &#125; public static void main(String args[])&#123; Demo obj= new Demo(); obj.test(); obj.test(2); obj.test(2,3); obj.test(2.0); &#125;&#125;运行结果：1234No parametersa: 2a and b: 2 3double a: 2.0通过上面的实例，读者可以看出，重载就是在一个类中，有相同的函数名称，但形参不同的函数。重载的结果，可以让一个程序段尽量减少代码和方法的种类。说明：参数列表不同包括：个数不同、类型不同和顺序不同。仅仅参数变量名称不同是不可以的。跟成员方法一样，构造方法也可以重载。声明为final的方法不能被重载。声明为static的方法不能被重载，但是能够被再次声明。方法的重载的规则：方法名称必须相同。参数列表必须不同（个数不同、或类型不同、参数排列顺序不同等）。方法的返回类型可以相同也可以不相同。仅仅返回类型不同不足以成为方法的重载。方法重载的实现：方法名称相同时，编译器会根据调用方法的参数个数、参数类型等去逐个匹配，以选择对应的方法，如果匹配失败，则编译器报错，这叫做重载分辨。六、基本运行顺序我们以下面的类来说明一个基本的 Java 类的运行顺序：12345678910111213public class Demo&#123; private String name; private int age; public Demo()&#123; name = "java学习"; age = 3; &#125; public static void main(String[] args)&#123; Demo obj = new Demo(); System.out.println(obj.name + "的年龄是" + obj.age); &#125;&#125;基本运行顺序是：先运行到第 9 行，这是程序的入口。然后运行到第 10 行，这里要 new 一个Demo，就要调用 Demo 的构造方法。就运行到第 5 行，注意：可能很多人觉得接下来就应该运行第 6 行了，错！初始化一个类，必须先初始化它的属性。因此运行到第 2 行，然后是第 3 行。属性初始化完过后，才回到构造方法，执行里面的代码，也就是第 6 行、第 7 行。然后是第8行，表示 new 一个Demo实例完成。然后回到 main 方法中执行第 11 行。然后是第 12 行，main方法执行完毕。作为程序员，应该清楚程序的基本运行过程，否则糊里糊涂的，不利于编写代码，也不利于技术上的发展。七、包装类、拆箱和装箱虽然 Java 语言是典型的面向对象编程语言，但其中的八种基本数据类型并不支持面向对象编程，基本类型的数据不具备“对象”的特性——不携带属性、没有方法可调用。 沿用它们只是为了迎合人类根深蒂固的习惯，并的确能简单、有效地进行常规数据处理。这种借助于非面向对象技术的做法有时也会带来不便，比如引用类型数据均继承了 Object 类的特性，要转换为 String 类型（经常有这种需要）时只要简单调用 Object 类中定义的toString()即可，而基本数据类型转换为 String 类型则要麻烦得多。为解决此类问题 ，Java为每种基本数据类型分别设计了对应的类，称之为包装类(Wrapper Classes)，也有教材称为外覆类或数据类型类。基本数据类型对应的包装类byteByteshortShortintIntegerlongLongcharCharacterfloatFloatdoubleDoublebooleanBoolean每个包装类的对象可以封装一个相应的基本类型的数据，并提供了其它一些有用的方法。包装类对象一经创建，其内容（所封装的基本类型数据值）不可改变。基本类型和对应的包装类可以相互装换：由基本类型向对应的包装类转换称为装箱，例如把 int 包装成 Integer 类的对象；包装类向对应的基本类型转换称为拆箱，例如把 Integer 类的对象重新简化为 int。7.1 包装类的应用八个包装类的使用比较相似，下面是常见的应用场景。1）实现 int 和 Integer 的相互转换可以通过 Integer 类的构造方法将 int 装箱，通过 Integer 类的 intValue 方法将 Integer 拆箱。例如：1234567891011public class Demo &#123; public static void main(String[] args) &#123; int m = 500; Integer obj = new Integer(m); // 手动装箱 int n = obj.intValue(); // 手动拆箱 System.out.println(&quot;n = &quot; + n); Integer obj1 = new Integer(500); System.out.println(&quot;obj 等价于 obj1？&quot; + obj.equals(obj1)); &#125;&#125;运行结果：12n = 500obj 等价于 obj1？true2）将字符串转换为整数Integer 类有一个静态的 paseInt() 方法，可以将字符串转换为整数，语法为：1parseInt(String s, int radix);s 为要转换的字符串，radix 为进制，可选，默认为十进制。下面的代码将会告诉你什么样的字符串可以转换为整数：1234567891011121314public class Demo &#123; public static void main(String[] args) &#123; String str[] = &#123;&quot;123&quot;, &quot;123abc&quot;, &quot;abc123&quot;, &quot;abcxyz&quot;&#125;; for(String str1 : str)&#123; try&#123; int m = Integer.parseInt(str1, 10); System.out.println(str1 + &quot; 可以转换为整数 &quot; + m); &#125;catch(Exception e)&#123; System.out.println(str1 + &quot; 无法转换为整数&quot;); &#125; &#125; &#125;&#125;1234123 可以转换为整数 123123abc 无法转换为整数abc123 无法转换为整数abcxyz 无法转换为整数3）将整数转换为字符串Integer 类有一个静态的 toString() 方法，可以将整数转换为字符串。例如：1234567public class Demo &#123; public static void main(String[] args) &#123; int m = 500; String s = Integer.toString(m); System.out.println(&quot;s = &quot; + s); &#125;&#125;运行结果：1s = 5007.2 自动拆箱和装箱上面的例子都需要手动实例化一个包装类，称为手动拆箱装箱。Java 1.5(5.0) 之前必须手动拆箱装箱。Java 1.5 之后可以自动拆箱装箱，也就是在进行基本数据类型和对应的包装类转换时，系统将自动进行，这将大大方便程序员的代码书写。例如：1234567891011public class Demo &#123; public static void main(String[] args) &#123; int m = 500; Integer obj = m; // 自动装箱 int n = obj; // 自动拆箱 System.out.println("n = " + n); Integer obj1 = 500; System.out.println("obj 等价于 obj1？" + obj.equals(obj1)); &#125;&#125;运行结果：12n = 500obj 等价于 obj1？true自动拆箱装箱是常用的一个功能，需要重点掌握。八、源文件的声明规则当在一个源文件中定义多个类，并且还有import语句和package语句时，要特别注意这些规则：一个源文件中只能有一个public类。一个源文件可以有多个非public类。源文件的名称应该和public类的类名保持一致。例如：源文件中public类的类名是Employee，那么源文件应该命名为Employee.java。如果一个类定义在某个包中，那么package语句应该在源文件的首行。如果源文件包含import语句，那么应该放在package语句和类定义之间。如果没有package语句，那么import语句应该在源文件中最前面。import语句和package语句对源文件中定义的所有类都有效。在同一源文件中，不能给不同的类不同的包声明。类有若干种访问级别，并且类也分不同的类型：抽象类和final类等。这些将在后续章节介绍。除了上面提到的几种类型，Java还有一些特殊的类，如内部类、匿名类。8.1 一个简单的例子在该例子中，我们创建两个类 Employee 和 EmployeeTest，分别放在包 p1 和 p2 中。Employee类有四个成员变量，分别是 name、age、designation和salary。该类显式声明了一个构造方法，该方法只有一个参数。在Eclipse中，创建一个包，命名为 p1，在该包中创建一个类，命名为 Employee，将下面的代码复制到源文件中：12345678910111213141516171819202122232425262728293031package p1;public class Employee&#123; String name; int age; String designation; double salary; // Employee 类的构造方法 public Employee(String name)&#123; this.name = name; &#125; // 设置age的值 public void empAge(int empAge)&#123; age = empAge; &#125; // 设置designation的值 public void empDesignation(String empDesig)&#123; designation = empDesig; &#125; // 设置salary的值 public void empSalary(double empSalary)&#123; salary = empSalary; &#125; // 输出信息 public void printEmployee()&#123; System.out.println("Name:"+ name ); System.out.println("Age:" + age ); System.out.println("Designation:" + designation ); System.out.println("Salary:" + salary); &#125;&#125;程序都是从main方法开始执行。为了能运行这个程序，必须包含main方法并且创建一个对象。下面给出EmployeeTest类，该类创建两个Employee对象，并调用方法设置变量的值。在Eclipse中再创建一个包，命名为 p2，在该包中创建一个类，命名为 EmployeeTest，将下面的代码复制到源文件中：123456789101112131415161718192021package p2;import p1.*;public class EmployeeTest&#123; public static void main(String args[])&#123; // 创建两个对象 Employee empOne = new Employee("James Smith"); Employee empTwo = new Employee("Mary Anne"); // 调用这两个对象的成员方法 empOne.empAge(26); empOne.empDesignation("Senior Software Engineer"); empOne.empSalary(1000); empOne.printEmployee(); empTwo.empAge(21); empTwo.empDesignation("Software Engineer"); empTwo.empSalary(500); empTwo.printEmployee(); &#125;&#125;编译并运行 EmployeeTest 类，可以看到如下的输出结果：12345678Name:James SmithAge:26Designation:Senior Software EngineerSalary:1000.0Name:Mary AnneAge:21Designation:Software EngineerSalary:500.0]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习笔记（1）：语法基础]]></title>
    <url>%2F2017%2F08%2F20%2FJava%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89%EF%BC%9A%E8%AF%AD%E6%B3%95%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[Java是完全面向对象的语言。通过虚拟机的运行机制，实现“跨平台”的理念。一次编译，永久使用。先来看一个 HelloWorld.java 程序。这个程序在屏幕上打印出一串字符”Hello World!”:1234567public class HelloWorld&#123; public static void main(String[] args) &#123; System.out.println("Hello World!"); &#125;&#125;程序中包括Java的一些基本特征：类(class)：上面程序定义了一个 类 HelloWorld，该类的名字与.java文件的名字相同。方法(method)：类的内部定义了该类的一个 方法 main。语句(statement)：真正的“打印”功能由一个语句实现，即: System.out.println(“Hello World!”);下面两点有关Java的书写方式：Java中的语句要以 ; 结尾 (与C/C++相同)。用花括号 {} 来整合语句，形成程序块。通过程序块，我们可以知道程序的不同部分的范围，比如类从哪里开始，到哪里结束。一、编译与运行Java程序要经过编译器编译才能执行。在Linux或Mac下，可以下载安装 Java JDK使用 javac 来编译。在命令行中输入下面语句 编译 :1$javac HelloWorld.java当前路径下，将有一个名为HelloWorld.class的文件生成。使用 java 命令来 运行 。1$java HelloWorldJava会搜寻该类中的main方法，并执行。二、变量计算机语言通常需要在内存中存放数据，比如C语言中的变量，Java也有类似的变量。Java和C语言都是静态类型的语言。在使用变量之前，要声明变量的类型。变量(variable) 占据一定的内存空间。不同类型的变量占据不同的大小。Java中的变量类型如下：变量类型存储大小例值注释byte1byte3字节int4bytes3整数short2bytes3短整数long8bytes3长整数float4bytes1.2单精度浮点数double8bytes1.2双精度浮点数char2bytes‘a’字符boolean1bittrue布尔值在Java中，变量需要先 声明(declare)才能使用。在声明中，我说明变量的类型，赋予变量以特别名字，以便在后面的程序中调用它。你可以在程序中的任意位置声明变量。 比如:12345678910public class Test&#123; public static void main(String[] args) &#123; System.out.println("Declare in the middle:"); int a; a = 5; System.out.println(a); // print an integer &#125;&#125;上面a是变量名。可以在声明变量的同时，给变量赋值，比如 int a = 5;“变量”的概念实际上来自于面向过程的编程语言。在Java中，所谓的变量实际上是 “基本类型” (premitive type) 。我们将在类的讲解中更多深入。上面的程序还可以看到，Java中，可用 // 引领注释。二、数组Java中有 数组(array) 。数组包含相同类型的多个数据。我用下面方法来声明一个整数数组:1int[] a;在声明数组时，数组所需的空间并没有真正分配给数组。我可以在声明的同时，用new来创建数组所需空间:1int[] a = new int[100];这里创建了可以容纳100个整数的数组。相应的内存分配也完成了。我还可以在声明的同时，给数组赋值。数组的大小也同时确定。1int[] a = new int[] &#123;1, 3, 5, 7, 9&#125;;2.1 数组初始化你可以在声明数组的同时进行初始化（静态初始化），也可以在声明以后进行初始化（动态初始化）。例如：12345678910// 静态初始化// 静态初始化的同时就为数组元素分配空间并赋值int intArray[] = &#123;1,2,3,4&#125;;String stringArray[] = &#123;&quot;程序员&quot;, &quot;http://www.baidu.com&quot;, &quot;一切编程语言都是纸老虎&quot;&#125;;// 动态初始化float floatArray[] = new float[3];floatArray[0] = 1.0f;floatArray[1] = 132.63f;floatArray[2] = 100F;2.2 数组引用可以通过下标来引用数组：1arrayName[index];index从0开始。与C、C++不同，Java对数组元素要进行越界检查以保证安全性。每个数组都有一个length属性来指明它的长度，例如 intArray.length 指明数组 intArray 的长度。其他类型的数组与整数数组相似。2.3 数组的遍历实际开发中，经常需要遍历数组以获取数组中的每一个元素。最容易想到的方法是for循环，例如：1234int arrayDemo[] = &#123;1, 2, 4, 7, 9, 192, 100&#125;;for(int i=0,len=arrayDemo.length; i&lt;len; i++)&#123; System.out.println(arrayDemo[i] + &quot;, &quot;);&#125;不过，Java提供了”增强版“的for循环，专门用来遍历数组，语法为：123for( arrayType varName: arrayName )&#123; // Some Code&#125;arrayType 为数组类型（也是数组元素的类型）；varName 是用来保存当前元素的变量，每次循环它的值都会改变；arrayName 为数组名称。每循环一次，就会获取数组中下一个元素的值，保存到 varName 变量，直到数组结束。即，第一次循环 varName 的值为第0个元素，第二次循环为第1个元素……例如：1234int arrayDemo[] = &#123;1, 2, 4, 7, 9, 192, 100&#125;;for(int x: arrayDemo)&#123; System.out.println(x + &quot;, &quot;);&#125;这种增强版的for循环也被称为”foreach循环“，它是普通for循环语句的特殊简化版。所有的foreach循环都可以被改写成for循环。但是，如果你希望使用数组的索引，那么增强版的 for 循环无法做到。2.4 二维数组二维数组的声明、初始化和引用与一维数组相似：123456int intArray[ ][ ] = &#123; &#123;1,2&#125;, &#123;2,3&#125;, &#123;4,5&#125; &#125;;int a[ ][ ] = new int[2][3];a[0][0] = 12;a[0][1] = 34;// ......a[1][2] = 93;Java语言中，由于把二维数组看作是数组的数组，数组空间不是连续分配的，所以不要求二维数组每一维的大小相同。例如：1234int intArray[ ][ ] = &#123; &#123;1,2&#125;, &#123;2,3&#125;, &#123;3,4,5&#125; &#125;;int a[ ][ ] = new int[2][ ];a[0] = new int[3];a[1] = new int[5];【示例】通过二维数组计算两个矩阵的乘积。12345678910111213141516171819202122232425262728293031public class Demo &#123; public static void main(String[] args)&#123; // 第一个矩阵（动态初始化一个二维数组） int a[][] = new int[2][3]; // 第二个矩阵（静态初始化一个二维数组） int b[][] = &#123; &#123;1,5,2,8&#125;, &#123;5,9,10,-3&#125;, &#123;2,7,-5,-18&#125; &#125;; // 结果矩阵 int c[][] = new int[2][4]; // 初始化第一个矩阵 for(int i=0; i&lt;2; i++) for(int j=0; j&lt;3 ;j++) a[i][j] = (i+1) * (j+2); // 计算矩阵乘积 for (int i=0; i&lt;2; i++)&#123; for (int j=0; j&lt;4; j++)&#123; c[i][j]=0; for(int k=0; k&lt;3; k++) c[i][j] += a[i][k] * b[k][j]; &#125; &#125; // 输出结算结果 for(int i=0; i&lt;2; i++)&#123; for (int j=0; j&lt;4; j++) System.out.printf("%-5d", c[i][j]); System.out.println(); &#125; &#125;&#125;123运行结果：25 65 14 -6550 130 28 -130几点说明上面讲的是静态数组。静态数组一旦被声明，它的容量就固定了，不容改变。所以在声明数组时，一定要考虑数组的最大容量，防止容量不够的现象。如果想在运行程序时改变容量，就需要用到数组列表(ArrayList，也称动态数组)或向量(Vector)。正是由于静态数组容量固定的缺点，实际开发中使用频率不高，被 ArrayList 或 Vector 代替，因为实际开发中经常需要向数组中添加或删除元素，而它的容量不好预估。三、表达式表达式 是变量、常量和运算符的组合，它表示一个数据。 1 + 1 是常见的表达式。再比如:12345678910public class Test&#123; public static void main(String[] args) &#123; System.out.println("Declare in the middle:"); int a; a = 5 + 1; System.out.println(a); // print an integer &#125;&#125;上面的5 + 1也是一个表达式，等于6。3.1 数学表达式数学运算，结果为一个数值1 + 2加法4 - 3.4减法7 * 1.5乘法3.5 / 7除法7 % 2求余数3.2 关系表达式判断表达式是否成立。即一个boolean值，真假a &gt; 4.2大于3.4 &gt;= b大于等于1.5 &lt; 9小于6 &lt;= 1小于等于2 == 2等于2 != 2不等于3.3 布林表达式两个boolean值的与、或、非的逻辑关系true &amp;&amp; falseand(3 &gt; 1)(2 == 1)or! truenot3.4 位运算对整数的二进制形式逐位进行逻辑运算，得到一个整数&amp;andor^xor~not5 &lt;&lt; 30b101 left shift 3 bits6 &gt;&gt; 10b110 right shift 1 bit还有下列在C中常见的运算符，我会在用到的时候进一步解释:m ++变量m加1n —变量n减1condition ? x1 : x2condition为一个boolean值。根据condition，取x1或x2的值四、控制结构Java中控制结构(control flow)的语法与C类似。它们都使用{}来表达隶属关系。4.1 选择（if）condition是一个表示真假值的表达式。statements;是语句。123456789101112if (conditon1) &#123; statements; ...&#125;else if (condition2) &#123; statements; ...&#125;else &#123; statements; ...&#125;4.2 循环（while）12345while (condition) &#123;statements;&#125;4.2 循环（do…while）123do &#123;statements;&#125;while(condition); // 注意结尾的;4.3 循环（for）123for (initial; condition; update) &#123;statements;&#125;4.4 跳出或跳出循环在循环中，可以使用12break; // 跳出循环continue; // 直接进入下一循环4.5 选择（switch）123456789101112switch(expression) &#123;case 1: statements; break; case 2: statements; break; ...default: statements; break; &#125;五、字符串从表面上看，字符串就是双引号之间的数据，例如“java”等。在Java中，可以使用下面的方法定义字符串：String stringName = “string content”;1String webName = &quot;java学习&quot;;字符串可以通过“+”连接，基本数据类型与字符串进行“+”操作一般也会自动转换为字符串，例如：12345678910public class Demo &#123; public static void main(String[] args)&#123; String stuName = "小明"; int stuAge = 17; float stuScore = 92.5f; String info = stuName + "的年龄是 " + stuAge + "，成绩是 " + stuScore; System.out.println(info); &#125;&#125;String字符串与数组有一个共同点，就是它们被初始化后，长度是不变的，并且内容也不变。如果要改变它的值，就会产生一个新的字符串，如下所示：12String str = &quot;Hello &quot;;str += &quot;World!&quot;;这个赋值表达式看起来有点像简单的接龙，在str后面直接加上一个“World!”字符串，形成最后的字符串“Hello World!”。其运行原理是这样的：程序首先产生了str1字符串，并在内存中申请了一段空间。此时要追加新的字符串是不可能的，因为字符串被初始化后，长度是固定的。如果要改变它，只有放弃原来的空间，重新申请能够容纳“Hello World!”字符串的内存空间，然后将“Hello World!”字符串放到内存中。实际上，String 是java.lang包下的一个类，按照标准的面向对象的语法，其格式应该为：1String stringName = new String(&quot;string content&quot;);例如1String url = new String(&quot;http://www.baidu.com&quot;);但是由于String特别常用，所以Java提供了一种简化的语法。使用简化语法的另外一个原因是，按照标准的面向对象的语法，在内存使用上存在比较大的浪费。例如String str = new String(“abc”);实际上创建了两个String对象，一个是”abc”对象，存储在常量空间中，一个是使用new关键字为对象str申请的空间。5.1 常用的String对象方法5.1.2 length()方法length() 返回字符串的长度，例如：1234String str1 = &quot;微学苑&quot;;String str2 = &quot;weixueyuan&quot;;System.out.println(&quot;The lenght of str1 is &quot; + str1.length());System.out.println(&quot;The lenght of str2 is &quot; + str2.length());输出结果为：12The lenght of str1 is 3The lenght of str2 is 105.1.2 charAt()方法charAt() 方法的作用是按照索引值获得字符串中的指定字符。Java规定，字符串中第一个字符的索引值是0，第二个字符的索引值是1，依次类推。例如：12String str = &quot;123456789&quot;;System.out.println(str.charAt(0) + &quot; &quot; + str.charAt(5) + &quot; &quot; + str.charAt(8))输出结果为：11 6 95.1.3 contain()方法contains() 方法用来检测字符串是否包含某个子串，例如：12String str = &quot;baidu&quot;;System.out.println(str.contains(&quot;bai&quot;));输出结果：1true5.1.4 replace()方法字符串替换，用来替换字符串中所有指定的子串，例如：1234ring str1 = &quot;The url of baidu is www.google.com!&quot;;String str2 = str1.replace(&quot;baidu&quot;, &quot;google&quot;);System.out.println(str1);System.out.println(str2);输出结果：12The url of baidu is www.google.com!The url of google is www.google.com!注意：replace() 方法不会改变原来的字符串，而是生成一个新的字符串。5.1.5 split()方法以指定字符串作为分隔符，对当前字符串进行分割，分割的结果是一个数组，例如：12345678import java.util.*;public class Demo &#123; public static void main(String[] args)&#123; String str = &quot;wei_xue_yuan_is_good&quot;; String strArr[] = str.split(&quot;_&quot;); System.out.println(Arrays.toString(strArr)); &#125;&#125;运行结果：1[wei, xue, yuan, is, good]以上仅仅列举了几个常用的String对象的方法，更多方法和详细解释请参考API文档。5.2 StringBuffer与StringBuiderString 的值是不可变的，每次对String的操作都会生成新的String对象，不仅效率低，而且耗费大量内存空间。StringBuffer类和String类一样，也用来表示字符串，但是StringBuffer的内部实现方式和String不同，在进行字符串处理时，不生成新的对象，在内存使用上要优于String。StringBuffer 默认分配16字节长度的缓冲区，当字符串超过该大小时，会自动增加缓冲区长度，而不是生成新的对象。StringBuffer不像String，只能通过 new 来创建对象，不支持简写方式，例如：1234StringBuffer str1 = new StringBuffer(); // 分配16个字节长度的缓冲区StringBuffer str2 = =new StringBuffer(512); // 分配512个字节长度的缓冲区// 在缓冲区中存放了字符串，并在后面预留了16个字节长度的空缓冲区StringBuffer str3 = new StringBuffer(&quot;www.baidu.com&quot;);5.2.1 StringBuffer类的主要方法StringBuffer类中的方法主要偏重于对于字符串的操作，例如追加、插入和删除等，这个也是StringBuffer类和String类的主要区别。实际开发中，如果需要对一个字符串进行频繁的修改，建议使用 StringBuffer。1) append() 方法append() 方法用于向当前字符串的末尾追加内容，类似于字符串的连接。调用该方法以后，StringBuffer对象的内容也发生改变，例如：12StringBuffer str = new StringBuffer(“biancheng100”);str.append(true);则对象str的值将变成”biancheng100true”。注意是str指向的内容变了，不是str的指向变了。字符串的”+“操作实际上也是先创建一个StringBuffer对象，然后调用append()方法将字符串片段拼接起来，最后调用toString()方法转换为字符串。这样看来，String的连接操作就比StringBuffer多出了一些附加操作，效率上必然会打折扣。但是，对于长度较小的字符串，”+“操作更加直观，更具可读性，有些时候可以稍微牺牲一下效率。2) deleteCharAt()deleteCharAt() 方法用来删除指定位置的字符，并将剩余的字符形成新的字符串。例如：12StringBuffer str = new StringBuffer(&quot;abcdef&quot;);str. deleteCharAt(3);该代码将会删除索引值为3的字符，即”d“字符。你也可以通过delete()方法一次性删除多个字符，例如：12StingBuffer str = new StringBuffer(&quot;abcdef&quot;);str.delete(1, 4);该代码会删除索引值为1~4之间的字符，包括索引值1，但不包括4。3) insert()方法insert() 用来在指定位置插入字符串，可以认为是append()的升级版。例如：12StringBuffer str = new StringBuffer(&quot;abcdef&quot;);str.insert(3, &quot;xyz&quot;);最后str所指向的字符串为 abcdxyzef。4) setCharAt() 方法setCharAt() 方法用来修改指定位置的字符。例如：12StringBuffer str = new StringBuffer(&quot;abcdef&quot;);str.setCharAt(3, &apos;z&apos;);该代码将把索引值为3的字符修改为 z，最后str所指向的字符串为 abczef。以上仅仅是部分常用方法的简单说明，更多方法和解释请查阅API文档。5.2.2 String和StringBuffer的效率对比为了更加明显地看出它们的执行效率，下面的代码，将26个英文字母加了10000次。123456789101112131415161718192021222324public class Demo &#123; public static void main(String[] args)&#123; String fragment = "abcdefghijklmnopqrstuvwxyz"; int times = 10000; // 通过String对象 long timeStart1 = System.currentTimeMillis(); String str1 = ""; for (int i=0; i&lt;times; i++) &#123; str1 += fragment; &#125; long timeEnd1 = System.currentTimeMillis(); System.out.println("String: " + (timeEnd1 - timeStart1) + "ms"); // 通过StringBuffer long timeStart2 = System.currentTimeMillis(); StringBuffer str2 = new StringBuffer(); for (int i=0; i&lt;times; i++) &#123; str2.append(fragment); &#125; long timeEnd2 = System.currentTimeMillis(); System.out.println("StringBuffer: " + (timeEnd2 - timeStart2) + "ms"); &#125;&#125;运行结果：12String: 5287msStringBuffer: 3ms结论很明显，StringBuffer的执行效率比String快上千倍，这个差异随着叠加次数的增加越来越明显，当叠加次数达到30000次的时候，运行结果为：12String: 35923msStringBuffer: 8ms所以，强烈建议在涉及大量字符串操作时使用StringBuffer。5.2.3 StringBuilder类StringBuilder类和StringBuffer类功能基本相似，方法也差不多，主要区别在于StringBuffer类的方法是多线程安全的，而StringBuilder不是线程安全的，相比而言，StringBuilder类会略微快一点。StringBuffer、StringBuilder、String中都实现了CharSequence接口。CharSequence是一个定义字符串操作的接口，它只包括length()、charAt(int index)、subSequence(int start, int end) 这几个API。StringBuffer、StringBuilder、String对CharSequence接口的实现过程不一样，如下图所示：可见，String直接实现了CharSequence接口；StringBuilder 和 StringBuffer都是可变的字符序列，它们都继承于AbstractStringBuilder，实现了CharSequence接口。总结一下：线程安全：StringBuffer：线程安全StringBuilder：线程不安全速度：一般情况下，速度从快到慢为 StringBuilder &gt; StringBuffer &gt; String，当然这是相对的，不是绝对的。使用环境：操作少量的数据使用 String；单线程操作大量数据使用 StringBuilder；多线程操作大量数据使用 StringBuffer。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（11）：Java HashMap源码全剖析]]></title>
    <url>%2F2017%2F08%2F19%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%8811%EF%BC%89%EF%BC%9AJava%20HashMap%E6%BA%90%E7%A0%81%E5%85%A8%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[HashMap简介 HashMap是基于哈希表实现的，每一个元素都是一个key-value对，其内部通过单链表解决冲突问题，容量不足（超过了阈值）时，同样会自动增长。 HashMap是非线程安全的，只是用于单线程环境下，多线程环境下可以采用concurrent并发包下的concurrentHashMap。 HashMap实现了Serializable接口，因此它支持序列化，实现了Cloneable接口，能被克隆。 一、HashMap源码剖析HashMap的源码如下（加入了比较详细的注释）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754package java.util; import java.io.*; public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; // 默认的初始容量（容量为HashMap中槽的数目）是16，且实际容量必须是2的整数次幂。 static final int DEFAULT_INITIAL_CAPACITY = 16; // 最大容量（必须是2的幂且小于2的30次方，传入容量过大将被这个值替换） static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 默认加载因子为0.75 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 存储数据的Entry数组，长度是2的幂。 // HashMap采用链表法解决冲突，每一个Entry本质上是一个单向链表 transient Entry[] table; // HashMap的底层数组中已用槽的数量 transient int size; // HashMap的阈值，用于判断是否需要调整HashMap的容量（threshold = 容量*加载因子） int threshold; // 加载因子实际大小 final float loadFactor; // HashMap被改变的次数 transient volatile int modCount; // 指定“容量大小”和“加载因子”的构造函数 public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); // HashMap的最大容量只能是MAXIMUM_CAPACITY if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //加载因此不能小于0 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); // 找出“大于initialCapacity”的最小的2的幂 int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; // 设置“加载因子” this.loadFactor = loadFactor; // 设置“HashMap阈值”，当HashMap中存储数据的数量达到threshold时，就需要将HashMap的容量加倍。 threshold = (int)(capacity * loadFactor); // 创建Entry数组，用来保存数据 table = new Entry[capacity]; init(); &#125; // 指定“容量大小”的构造函数 public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; // 默认构造函数。 public HashMap() &#123; // 设置“加载因子”为默认加载因子0.75 this.loadFactor = DEFAULT_LOAD_FACTOR; // 设置“HashMap阈值”，当HashMap中存储数据的数量达到threshold时，就需要将HashMap的容量加倍。 threshold = (int)(DEFAULT_INITIAL_CAPACITY * DEFAULT_LOAD_FACTOR); // 创建Entry数组，用来保存数据 table = new Entry[DEFAULT_INITIAL_CAPACITY]; init(); &#125; // 包含“子Map”的构造函数 public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR); // 将m中的全部元素逐个添加到HashMap中 putAllForCreate(m); &#125; //求hash值的方法，重新计算hash值 static int hash(int h) &#123; h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); &#125; // 返回h在数组中的索引值，这里用&amp;代替取模，旨在提升效率 // h &amp; (length-1)保证返回值的小于length static int indexFor(int h, int length) &#123; return h &amp; (length-1); &#125; public int size() &#123; return size; &#125; public boolean isEmpty() &#123; return size == 0; &#125; // 获取key对应的value public V get(Object key) &#123; if (key == null) return getForNullKey(); // 获取key的hash值 int hash = hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; //判断key是否相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125; //没找到则返回null return null; &#125; // 获取“key为null”的元素的值 // HashMap将“key为null”的元素存储在table[0]位置，但不一定是该链表的第一个位置！ private V getForNullKey() &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) return e.value; &#125; return null; &#125; // HashMap是否包含key public boolean containsKey(Object key) &#123; return getEntry(key) != null; &#125; // 返回“键为key”的键值对 final Entry&lt;K,V&gt; getEntry(Object key) &#123; // 获取哈希值 // HashMap将“key为null”的元素存储在table[0]位置，“key不为null”的则调用hash()计算哈希值 int hash = (key == null) ? 0 : hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; return null; &#125; // 将“key-value”添加到HashMap中 public V put(K key, V value) &#123; // 若“key为null”，则将该键值对添加到table[0]中。 if (key == null) return putForNullKey(value); // 若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中。 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; // 若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出！ if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 若“该key”对应的键值对不存在，则将“key-value”添加到table中 modCount++; //将key-value添加到table[i]处 addEntry(hash, key, value, i); return null; &#125; // putForNullKey()的作用是将“key为null”键值对添加到table[0]位置 private V putForNullKey(V value) &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 如果没有存在key为null的键值对，则直接题阿见到table[0]处! modCount++; addEntry(0, null, value, 0); return null; &#125; // 创建HashMap对应的“添加方法”， // 它和put()不同。putForCreate()是内部方法，它被构造函数等调用，用来创建HashMap // 而put()是对外提供的往HashMap中添加元素的方法。 private void putForCreate(K key, V value) &#123; int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); // 若该HashMap表中存在“键值等于key”的元素，则替换该元素的value值 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; e.value = value; return; &#125; &#125; // 若该HashMap表中不存在“键值等于key”的元素，则将该key-value添加到HashMap中 createEntry(hash, key, value, i); &#125; // 将“m”中的全部元素都添加到HashMap中。 // 该方法被内部的构造HashMap的方法所调用。 private void putAllForCreate(Map&lt;? extends K, ? extends V&gt; m) &#123; // 利用迭代器将元素逐个添加到HashMap中 for (Iterator&lt;? extends Map.Entry&lt;? extends K, ? extends V&gt;&gt; i = m.entrySet().iterator(); i.hasNext(); ) &#123; Map.Entry&lt;? extends K, ? extends V&gt; e = i.next(); putForCreate(e.getKey(), e.getValue()); &#125; &#125; // 重新调整HashMap的大小，newCapacity是调整后的容量 void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; //如果就容量已经达到了最大值，则不能再扩容，直接返回 if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 新建一个HashMap，将“旧HashMap”的全部元素添加到“新HashMap”中， // 然后，将“新HashMap”赋值给“旧HashMap”。 Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); &#125; // 将HashMap中的全部元素都添加到newTable中 void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125; &#125; // 将&quot;m&quot;的全部元素都添加到HashMap中 public void putAll(Map&lt;? extends K, ? extends V&gt; m) &#123; // 有效性判断 int numKeysToBeAdded = m.size(); if (numKeysToBeAdded == 0) return; // 计算容量是否足够， // 若“当前阀值容量 &lt; 需要的容量”，则将容量x2。 if (numKeysToBeAdded &gt; threshold) &#123; int targetCapacity = (int)(numKeysToBeAdded / loadFactor + 1); if (targetCapacity &gt; MAXIMUM_CAPACITY) targetCapacity = MAXIMUM_CAPACITY; int newCapacity = table.length; while (newCapacity &lt; targetCapacity) newCapacity &lt;&lt;= 1; if (newCapacity &gt; table.length) resize(newCapacity); &#125; // 通过迭代器，将“m”中的元素逐个添加到HashMap中。 for (Iterator&lt;? extends Map.Entry&lt;? extends K, ? extends V&gt;&gt; i = m.entrySet().iterator(); i.hasNext(); ) &#123; Map.Entry&lt;? extends K, ? extends V&gt; e = i.next(); put(e.getKey(), e.getValue()); &#125; &#125; // 删除“键为key”元素 public V remove(Object key) &#123; Entry&lt;K,V&gt; e = removeEntryForKey(key); return (e == null ? null : e.value); &#125; // 删除“键为key”的元素 final Entry&lt;K,V&gt; removeEntryForKey(Object key) &#123; // 获取哈希值。若key为null，则哈希值为0；否则调用hash()进行计算 int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; e = prev; // 删除链表中“键为key”的元素 // 本质是“删除单向链表中的节点” while (e != null) &#123; Entry&lt;K,V&gt; next = e.next; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; modCount++; size--; if (prev == e) table[i] = next; else prev.next = next; e.recordRemoval(this); return e; &#125; prev = e; e = next; &#125; return e; &#125; // 删除“键值对” final Entry&lt;K,V&gt; removeMapping(Object o) &#123; if (!(o instanceof Map.Entry)) return null; Map.Entry&lt;K,V&gt; entry = (Map.Entry&lt;K,V&gt;) o; Object key = entry.getKey(); int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; e = prev; // 删除链表中的“键值对e” // 本质是“删除单向链表中的节点” while (e != null) &#123; Entry&lt;K,V&gt; next = e.next; if (e.hash == hash &amp;&amp; e.equals(entry)) &#123; modCount++; size--; if (prev == e) table[i] = next; else prev.next = next; e.recordRemoval(this); return e; &#125; prev = e; e = next; &#125; return e; &#125; // 清空HashMap，将所有的元素设为null public void clear() &#123; modCount++; Entry[] tab = table; for (int i = 0; i &lt; tab.length; i++) tab[i] = null; size = 0; &#125; // 是否包含“值为value”的元素 public boolean containsValue(Object value) &#123; // 若“value为null”，则调用containsNullValue()查找 if (value == null) return containsNullValue(); // 若“value不为null”，则查找HashMap中是否有值为value的节点。 Entry[] tab = table; for (int i = 0; i &lt; tab.length ; i++) for (Entry e = tab[i] ; e != null ; e = e.next) if (value.equals(e.value)) return true; return false; &#125; // 是否包含null值 private boolean containsNullValue() &#123; Entry[] tab = table; for (int i = 0; i &lt; tab.length ; i++) for (Entry e = tab[i] ; e != null ; e = e.next) if (e.value == null) return true; return false; &#125; // 克隆一个HashMap，并返回Object对象 public Object clone() &#123; HashMap&lt;K,V&gt; result = null; try &#123; result = (HashMap&lt;K,V&gt;)super.clone(); &#125; catch (CloneNotSupportedException e) &#123; // assert false; &#125; result.table = new Entry[table.length]; result.entrySet = null; result.modCount = 0; result.size = 0; result.init(); // 调用putAllForCreate()将全部元素添加到HashMap中 result.putAllForCreate(this); return result; &#125; // Entry是单向链表。 // 它是 “HashMap链式存储法”对应的链表。 // 它实现了Map.Entry 接口，即实现getKey(), getValue(), setValue(V value), equals(Object o), hashCode()这些函数 static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; // 指向下一个节点 Entry&lt;K,V&gt; next; final int hash; // 构造函数。 // 输入参数包括&quot;哈希值(h)&quot;, &quot;键(k)&quot;, &quot;值(v)&quot;, &quot;下一节点(n)&quot; Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; // 判断两个Entry是否相等 // 若两个Entry的“key”和“value”都相等，则返回true。 // 否则，返回false public final boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) &#123; Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; &#125; return false; &#125; // 实现hashCode() public final int hashCode() &#123; return (key==null ? 0 : key.hashCode()) ^ (value==null ? 0 : value.hashCode()); &#125; public final String toString() &#123; return getKey() + &quot;=&quot; + getValue(); &#125; // 当向HashMap中添加元素时，绘调用recordAccess()。 // 这里不做任何处理 void recordAccess(HashMap&lt;K,V&gt; m) &#123; &#125; // 当从HashMap中删除元素时，绘调用recordRemoval()。 // 这里不做任何处理 void recordRemoval(HashMap&lt;K,V&gt; m) &#123; &#125; &#125; // 新增Entry。将“key-value”插入指定位置，bucketIndex是位置索引。 void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); // 若HashMap的实际大小 不小于 “阈值”，则调整HashMap的大小 if (size++ &gt;= threshold) resize(2 * table.length); &#125; // 创建Entry。将“key-value”插入指定位置。 void createEntry(int hash, K key, V value, int bucketIndex) &#123; // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); size++; &#125; // HashIterator是HashMap迭代器的抽象出来的父类，实现了公共了函数。 // 它包含“key迭代器(KeyIterator)”、“Value迭代器(ValueIterator)”和“Entry迭代器(EntryIterator)”3个子类。 private abstract class HashIterator&lt;E&gt; implements Iterator&lt;E&gt; &#123; // 下一个元素 Entry&lt;K,V&gt; next; // expectedModCount用于实现fast-fail机制。 int expectedModCount; // 当前索引 int index; // 当前元素 Entry&lt;K,V&gt; current; HashIterator() &#123; expectedModCount = modCount; if (size &gt; 0) &#123; // advance to first entry Entry[] t = table; // 将next指向table中第一个不为null的元素。 // 这里利用了index的初始值为0，从0开始依次向后遍历，直到找到不为null的元素就退出循环。 while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) ; &#125; &#125; public final boolean hasNext() &#123; return next != null; &#125; // 获取下一个元素 final Entry&lt;K,V&gt; nextEntry() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); Entry&lt;K,V&gt; e = next; if (e == null) throw new NoSuchElementException(); // 注意！！！ // 一个Entry就是一个单向链表 // 若该Entry的下一个节点不为空，就将next指向下一个节点; // 否则，将next指向下一个链表(也是下一个Entry)的不为null的节点。 if ((next = e.next) == null) &#123; Entry[] t = table; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) ; &#125; current = e; return e; &#125; // 删除当前元素 public void remove() &#123; if (current == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); Object k = current.key; current = null; HashMap.this.removeEntryForKey(k); expectedModCount = modCount; &#125; &#125; // value的迭代器 private final class ValueIterator extends HashIterator&lt;V&gt; &#123; public V next() &#123; return nextEntry().value; &#125; &#125; // key的迭代器 private final class KeyIterator extends HashIterator&lt;K&gt; &#123; public K next() &#123; return nextEntry().getKey(); &#125; &#125; // Entry的迭代器 private final class EntryIterator extends HashIterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public Map.Entry&lt;K,V&gt; next() &#123; return nextEntry(); &#125; &#125; // 返回一个“key迭代器” Iterator&lt;K&gt; newKeyIterator() &#123; return new KeyIterator(); &#125; // 返回一个“value迭代器” Iterator&lt;V&gt; newValueIterator() &#123; return new ValueIterator(); &#125; // 返回一个“entry迭代器” Iterator&lt;Map.Entry&lt;K,V&gt;&gt; newEntryIterator() &#123; return new EntryIterator(); &#125; // HashMap的Entry对应的集合 private transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet = null; // 返回“key的集合”，实际上返回一个“KeySet对象” public Set&lt;K&gt; keySet() &#123; Set&lt;K&gt; ks = keySet; return (ks != null ? ks : (keySet = new KeySet())); &#125; // Key对应的集合 // KeySet继承于AbstractSet，说明该集合中没有重复的Key。 private final class KeySet extends AbstractSet&lt;K&gt; &#123; public Iterator&lt;K&gt; iterator() &#123; return newKeyIterator(); &#125; public int size() &#123; return size; &#125; public boolean contains(Object o) &#123; return containsKey(o); &#125; public boolean remove(Object o) &#123; return HashMap.this.removeEntryForKey(o) != null; &#125; public void clear() &#123; HashMap.this.clear(); &#125; &#125; // 返回“value集合”，实际上返回的是一个Values对象 public Collection&lt;V&gt; values() &#123; Collection&lt;V&gt; vs = values; return (vs != null ? vs : (values = new Values())); &#125; // “value集合” // Values继承于AbstractCollection，不同于“KeySet继承于AbstractSet”， // Values中的元素能够重复。因为不同的key可以指向相同的value。 private final class Values extends AbstractCollection&lt;V&gt; &#123; public Iterator&lt;V&gt; iterator() &#123; return newValueIterator(); &#125; public int size() &#123; return size; &#125; public boolean contains(Object o) &#123; return containsValue(o); &#125; public void clear() &#123; HashMap.this.clear(); &#125; &#125; // 返回“HashMap的Entry集合” public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() &#123; return entrySet0(); &#125; // 返回“HashMap的Entry集合”，它实际是返回一个EntrySet对象 private Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet0() &#123; Set&lt;Map.Entry&lt;K,V&gt;&gt; es = entrySet; return es != null ? es : (entrySet = new EntrySet()); &#125; // EntrySet对应的集合 // EntrySet继承于AbstractSet，说明该集合中没有重复的EntrySet。 private final class EntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() &#123; return newEntryIterator(); &#125; public boolean contains(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;K,V&gt; e = (Map.Entry&lt;K,V&gt;) o; Entry&lt;K,V&gt; candidate = getEntry(e.getKey()); return candidate != null &amp;&amp; candidate.equals(e); &#125; public boolean remove(Object o) &#123; return removeMapping(o) != null; &#125; public int size() &#123; return size; &#125; public void clear() &#123; HashMap.this.clear(); &#125; &#125; // java.io.Serializable的写入函数 // 将HashMap的“总的容量，实际容量，所有的Entry”都写入到输出流中 private void writeObject(java.io.ObjectOutputStream s) throws IOException &#123; Iterator&lt;Map.Entry&lt;K,V&gt;&gt; i = (size &gt; 0) ? entrySet0().iterator() : null; // Write out the threshold, loadfactor, and any hidden stuff s.defaultWriteObject(); // Write out number of buckets s.writeInt(table.length); // Write out size (number of Mappings) s.writeInt(size); // Write out keys and values (alternating) if (i != null) &#123; while (i.hasNext()) &#123; Map.Entry&lt;K,V&gt; e = i.next(); s.writeObject(e.getKey()); s.writeObject(e.getValue()); &#125; &#125; &#125; private static final long serialVersionUID = 362498820763181265L; // java.io.Serializable的读取函数：根据写入方式读出 // 将HashMap的“总的容量，实际容量，所有的Entry”依次读出 private void readObject(java.io.ObjectInputStream s) throws IOException, ClassNotFoundException &#123; // Read in the threshold, loadfactor, and any hidden stuff s.defaultReadObject(); // Read in number of buckets and allocate the bucket array; int numBuckets = s.readInt(); table = new Entry[numBuckets]; init(); // Give subclass a chance to do its thing. // Read in size (number of Mappings) int size = s.readInt(); // Read the keys and values, and put the mappings in the HashMap for (int i=0; i&lt;size; i++) &#123; K key = (K) s.readObject(); V value = (V) s.readObject(); putForCreate(key, value); &#125; &#125; // 返回“HashMap总的容量” int capacity() &#123; return table.length; &#125; // 返回“HashMap的加载因子” float loadFactor() &#123; return loadFactor; &#125; &#125; 二、HashMap细节剖析2.1 存储结构 首先要清楚HashMap的存储结构，如下图所示： 图中，紫色部分即代表哈希表，也称为哈希数组，数组的每个元素都是一个单链表的头节点，链表是用来解决冲突的，如果不同的key映射到了数组的同一位置处，就将其放入单链表中。 2.2 链表节点的数据结构1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// Entry是单向链表。 // 它是 “HashMap链式存储法”对应的链表。 // 它实现了Map.Entry 接口，即实现getKey(), getValue(), setValue(V value), equals(Object o), hashCode()这些函数 static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; // 指向下一个节点 Entry&lt;K,V&gt; next; final int hash; // 构造函数。 // 输入参数包括&quot;哈希值(h)&quot;, &quot;键(k)&quot;, &quot;值(v)&quot;, &quot;下一节点(n)&quot; Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; // 判断两个Entry是否相等 // 若两个Entry的“key”和“value”都相等，则返回true。 // 否则，返回false public final boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) &#123; Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; &#125; return false; &#125; // 实现hashCode() public final int hashCode() &#123; return (key==null ? 0 : key.hashCode()) ^ (value==null ? 0 : value.hashCode()); &#125; public final String toString() &#123; return getKey() + &quot;=&quot; + getValue(); &#125; // 当向HashMap中添加元素时，绘调用recordAccess()。 // 这里不做任何处理 void recordAccess(HashMap&lt;K,V&gt; m) &#123; &#125; // 当从HashMap中删除元素时，会调用recordRemoval()。 // 这里不做任何处理 void recordRemoval(HashMap&lt;K,V&gt; m) &#123; &#125; &#125; 它的结构元素除了key、value、hash外，还有next，next指向下一个节点。另外，这里覆写了equals和hashCode方法来保证键值对的独一无二。 2.3 构造方法HashMap共有四个构造方法。构造方法中提到了两个很重要的参数：初始容量和加载因子。这两个参数是影响HashMap性能的重要参数，其中容量表示哈希表中槽的数量（即哈希数组的长度），初始容量是创建哈希表时的容量（从构造函数中可以看出，如果不指明，则默认为16），加载因子是哈希表在其容量自动增加之前可以达到多满的一种尺度，当哈希表中的条目数超出了加载因子与当前容量的乘积时，则要对该哈希表进行 resize 操作（即扩容）。 下面说下加载因子，如果加载因子越大，对空间的利用更充分，但是查找效率会降低（链表长度会越来越长）；如果加载因子太小，那么表中的数据将过于稀疏（很多空间还没用，就开始扩容了），对空间造成严重浪费。如果我们在构造方法中不指定，则系统默认加载因子为0.75，这是一个比较理想的值，一般情况下我们是无需修改的。 另外，无论我们指定的容量为多少，构造方法都会将实际容量设为不小于指定容量的2的次方的一个数，且最大值不能超过2的30次方 2.4 HashMap中key和value都允许为null。2.5 重点分析put和get要重点分析下HashMap中用的最多的两个方法put和get。先从比较简单的get方法着手，源码如下： 12345678910111213141516171819202122232425262728// 获取key对应的value public V get(Object key) &#123; if (key == null) return getForNullKey(); // 获取key的hash值 int hash = hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; /判断key是否相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125; 没找到则返回null return null; &#125; // 获取“key为null”的元素的值 // HashMap将“key为null”的元素存储在table[0]位置，但不一定是该链表的第一个位置！ private V getForNullKey() &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) return e.value; &#125; return null; &#125; 首先，如果key为null，则直接从哈希表的第一个位置table[0]对应的链表上查找。记住，key为null的键值对永远都放在以table[0]为头结点的链表中，当然不一定是存放在头结点table[0]中。 如果key不为null，则先求的key的hash值，根据hash值找到在table中的索引，在该索引对应的单链表中查找是否有键值对的key与目标key相等，有就返回对应的value，没有则返回null。 put方法稍微复杂些，代码如下： 12345678910111213141516171819202122232425 // 将“key-value”添加到HashMap中 public V put(K key, V value) &#123; // 若“key为null”，则将该键值对添加到table[0]中。 if (key == null) return putForNullKey(value); // 若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中。 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; // 若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出！ if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 若“该key”对应的键值对不存在，则将“key-value”添加到table中 modCount++; //将key-value添加到table[i]处 addEntry(hash, key, value, i); return null; &#125; 如果key为null，则将其添加到table[0]对应的链表中，putForNullKey的源码如下： 123456789101112131415// putForNullKey()的作用是将“key为null”键值对添加到table[0]位置 private V putForNullKey(V value) &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 如果没有存在key为null的键值对，则直接题阿见到table[0]处! modCount++; addEntry(0, null, value, 0); return null; &#125; 如果key不为null，则同样先求出key的hash值，根据hash值得出在table中的索引，而后遍历对应的单链表，如果单链表中存在与目标key相等的键值对，则将新的value覆盖旧的value，比将旧的value返回，如果找不到与目标key相等的键值对，或者该单链表为空，则将该键值对插入到改单链表的头结点位置（每次新插入的节点都是放在头结点的位置），该操作是有addEntry方法实现的，它的源码如下： 1234567891011// 新增Entry。将“key-value”插入指定位置，bucketIndex是位置索引。 void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); // 若HashMap的实际大小 不小于 “阈值”，则调整HashMap的大小 if (size++ &gt;= threshold) resize(2 * table.length); &#125; 注意这里倒数第三行的构造方法，将key-value键值对赋给table[bucketIndex]，并将其next指向元素e，这便将key-value放到了头结点中，并将之前的头结点接在了它的后面。该方法也说明，每次put键值对的时候，总是将新的该键值对放在table[bucketIndex]处（即头结点处）。 两外注意最后两行代码，每次加入键值对时，都要判断当前已用的槽的数目是否大于等于阀值（容量*加载因子），如果大于等于，则进行扩容，将容量扩为原来容量的2倍。 2.6 关于扩容上面我们看到了扩容的方法，resize方法，它的源码如下： 123456789101112131415161718// 重新调整HashMap的大小，newCapacity是调整后的单位 void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 新建一个HashMap，将“旧HashMap”的全部元素添加到“新HashMap”中， // 然后，将“新HashMap”赋值给“旧HashMap”。 Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); &#125; ``` 很明显，是新建了一个HashMap的底层数组，而后调用transfer方法，将就HashMap的全部元素添加到新的HashMap中（要重新计算元素在新的数组中的索引位置）。transfer方法的源码如下： // 将HashMap中的全部元素都添加到newTable中void transfer(Entry[] newTable) { Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) { Entry e = src[j]; if (e != null) { src[j] = null; do { Entry next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } while (e != null); } }}123456789101112很明显，扩容是一个相当耗时的操作，因为它需要重新计算这些元素在新的数组中的位置并进行复制处理。因此，我们在用HashMap的时，最好能提前预估下HashMap中元素的个数，这样有助于提高HashMap的性能。### 2..7 containsKey方法和containsValue方法。注意containsKey方法和containsValue方法。前者直接可以通过key的哈希值将搜索范围定位到指定索引对应的链表，而后者要对哈希数组的每个链表进行搜索。### 2.8 求hash值和索引值的方法我们重点来分析下求hash值和索引值的方法，这两个方法便是HashMap设计的最为核心的部分，二者结合能保证哈希表中的元素尽可能均匀地散列。计算哈希值的方法如下： static int hash(int h) { h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); } 123它只是一个数学公式，IDK这样设计对hash值的计算，自然有它的好处，至于为什么这样设计，我们这里不去追究，只要明白一点，用的位的操作使hash值的计算效率很高。由hash值找到对应索引的方法如下： static int indexFor(int h, int length) { return h &amp; (length-1); }```这个我们要重点说下，我们一般对哈希表的散列很自然地会想到用hash值对length取模（即除法散列法），Hashtable中也是这样实现的，这种方法基本能保证元素在哈希表中散列的比较均匀，但取模会用到除法运算，效率很低，HashMap中则通过h&amp;(length-1)的方法来代替取模，同样实现了均匀的散列，但效率要高很多，这也是HashMap对Hashtable的一个改进。 接下来，我们分析下为什么哈希表的容量一定要是2的整数次幂。首先，length为2的整数次幂的话，h&amp;(length-1)就相当于对length取模，这样便保证了散列的均匀，同时也提升了效率；其次，length为2的整数次幂的话，为偶数，这样length-1为奇数，奇数的最后一位是1，这样便保证了h&amp;(length-1)的最后一位可能为0，也可能为1（这取决于h的值），即与后的结果可能为偶数，也可能为奇数，这样便可以保证散列的均匀性，而如果length为奇数的话，很明显length-1为偶数，它的最后一位是0，这样h&amp;(length-1)的最后一位肯定为0，即只能为偶数，这样任何hash值都只会被散列到数组的偶数下标位置上，这便浪费了近一半的空间，因此，length取2的整数次幂，是为了使不同hash值发生碰撞的概率较小，这样就能使元素在哈希表中均匀地散列。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>数据结构</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（10）：hashCode方法与equal方法]]></title>
    <url>%2F2017%2F08%2F18%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%8810%EF%BC%89%EF%BC%9AhashCode%E6%96%B9%E6%B3%95%E4%B8%8Eequal%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[哈希表这个数据结构想必大多数人都不陌生，而且在很多地方都会利用到hash表来提高查找效率。在Java的Object类中有一个方法: 1public native int hashCode(); 根据这个方法的声明可知，该方法返回一个int类型的数值，并且是本地方法，因此在Object类中并没有给出具体的实现。 为何Object类需要这样一个方法？它有什么作用呢？今天我们就来具体探讨一下hashCode方法。 一、hashCode()方法的作用对于包含容器类型的程序设计语言来说，基本上都会涉及到hashCode。在Java中也一样，hashCode方法的主要作用是为了配合基于散列的集合一起正常运行，这样的散列集合包括HashSet、HashMap以及HashTable。 为什么这么说呢？考虑一种情况，当向集合中插入对象时，如何判别在集合中是否已经存在该对象了？（注意：集合中不允许重复的元素存在） 也许大多数人都会想到调用equals方法来逐个进行比较，这个方法确实可行。但是如果集合中已经存在一万条数据或者更多的数据，如果采用equals方法去逐一比较，效率必然是一个问题。此时hashCode方法的作用就体现出来了，当集合要添加新的对象时，先调用这个对象的hashCode方法，得到对应的hashcode值，实际上在HashMap的具体实现中会用一个table保存已经存进去的对象的hashcode值，如果table中没有该hashcode值，它就可以直接存进去，不用再进行任何比较了；如果存在该hashcode值， 就调用它的equals方法与新元素进行比较，相同的话就不存了，不相同就散列其它的地址，所以这里存在一个冲突解决的问题，这样一来实际调用equals方法的次数就大大降低了，说通俗一点：Java中的hashCode方法就是根据一定的规则将与对象相关的信息（比如对象的存储地址，对象的字段等）映射成一个数值，这个数值称作为散列值。下面这段代码是java.util.HashMap的中put方法的具体实现： 12345678910111213141516171819public V put(K key, V value) &#123; if (key == null) return putForNullKey(value); int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); return null; &#125; put方法是用来向HashMap中添加新的元素，从put方法的具体实现可知，会先调用hashCode方法得到该元素的hashCode值，然后查看table中是否存在该hashCode值，如果存在则调用equals方法重新确定是否存在该元素，如果存在，则更新value值，否则将新的元素添加到HashMap中。从这里可以看出，hashCode方法的存在是为了减少equals方法的调用次数，从而提高程序效率。 有些朋友误以为默认情况下，hashCode返回的就是对象的存储地址，事实上这种看法是不全面的，确实有些JVM在实现时是直接返回对象的存储地址，但是大多时候并不是这样，只能说可能存储地址有一定关联。下面是HotSpot JVM中生成hash散列值的实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445static inline intptr_t get_next_hash(Thread * Self, oop obj) &#123; intptr_t value = 0 ; if (hashCode == 0) &#123; // This form uses an unguarded global Park-Miller RNG, // so it&apos;s possible for two threads to race and generate the same RNG. // On MP system we&apos;ll have lots of RW access to a global, so the // mechanism induces lots of coherency traffic. value = os::random() ; &#125; else if (hashCode == 1) &#123; // This variation has the property of being stable (idempotent) // between STW operations. This can be useful in some of the 1-0 // synchronization schemes. intptr_t addrBits = intptr_t(obj) &gt;&gt; 3 ; value = addrBits ^ (addrBits &gt;&gt; 5) ^ GVars.stwRandom ; &#125; else if (hashCode == 2) &#123; value = 1 ; // for sensitivity testing &#125; else if (hashCode == 3) &#123; value = ++GVars.hcSequence ; &#125; else if (hashCode == 4) &#123; value = intptr_t(obj) ; &#125; else &#123; // Marsaglia&apos;s xor-shift scheme with thread-specific state // This is probably the best overall implementation -- we&apos;ll // likely make this the default in future releases. unsigned t = Self-&gt;_hashStateX ; t ^= (t &lt;&lt; 11) ; Self-&gt;_hashStateX = Self-&gt;_hashStateY ; Self-&gt;_hashStateY = Self-&gt;_hashStateZ ; Self-&gt;_hashStateZ = Self-&gt;_hashStateW ; unsigned v = Self-&gt;_hashStateW ; v = (v ^ (v &gt;&gt; 19)) ^ (t ^ (t &gt;&gt; 8)) ; Self-&gt;_hashStateW = v ; value = v ; &#125; value &amp;= markOopDesc::hash_mask; if (value == 0) value = 0xBAD ; assert (value != markOopDesc::no_hash, &quot;invariant&quot;) ; TEVENT (hashCode: GENERATE) ; return value;&#125; 因此有人会说，可以直接根据hashcode值判断两个对象是否相等吗？肯定是不可以的，因为不同的对象可能会生成相同的hashcode值。虽然不能根据hashcode值判断两个对象是否相等，但是可以直接根据hashcode值判断两个对象不等，如果两个对象的hashcode值不等，则必定是两个不同的对象。如果要判断两个对象是否真正相等，必须通过equals方法。也就是说对于两个对象: 如果调用equals方法得到的结果为true，则两个对象的hashcode值必定相等； 如果equals方法得到的结果为false，则两个对象的hashcode值不一定不同； 如果两个对象的hashcode值不等，则equals方法得到的结果必定为false； 如果两个对象的hashcode值相等，则equals方法得到的结果未知。 二、equal方法和hashCode方法在有些情况下，程序设计者在设计一个类的时候为需要重写equals方法，比如String类，但是千万要注意，在重写equals方法的同时，必须重写hashCode方法。为什么这么说呢？ 下面看一个例子： 12345678910111213141516171819202122232425262728293031323334353637import java.util.HashMap;import java.util.HashSet;import java.util.Set; class People&#123; private String name; private int age; public People(String name,int age) &#123; this.name = name; this.age = age; &#125; public void setAge(int age)&#123; this.age = age; &#125; @Override public boolean equals(Object obj) &#123; // TODO Auto-generated method stub return this.name.equals(((People)obj).name) &amp;&amp; this.age== ((People)obj).age; &#125;&#125; public class Demo &#123; public static void main(String[] args) &#123; People p1 = new People(&quot;Jack&quot;, 12); System.out.println(p1.hashCode()); HashMap&lt;People, Integer&gt; hashMap = new HashMap&lt;People, Integer&gt;(); hashMap.put(p1, 1); System.out.println(hashMap.get(new People(&quot;Jack&quot;, 12))); &#125;&#125; 在这里我只重写了equals方法，也就说如果两个People对象，如果它的姓名和年龄相等，则认为是同一个人。 这段代码本来的意愿是想这段代码输出结果为“1”，但是事实上它输出的是“null”。为什么呢？原因就在于重写equals方法的同时忘记重写hashCode方法。 虽然通过重写equals方法使得逻辑上姓名和年龄相同的两个对象被判定为相等的对象（跟String类类似），但是要知道默认情况下，hashCode方法是将对象的存储地址进行映射。那么上述代码的输出结果为“null”就不足为奇了。原因很简单，p1指向的对象和System.out.println(hashMap.get(new People(“Jack”, 12)));这句中的new People(“Jack”, 12)生成的是两个对象，它们的存储地址肯定不同。下面是HashMap的get方法的具体实现： 12345678910111213public V get(Object key) &#123; if (key == null) return getForNullKey(); int hash = hash(key.hashCode()); for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125; return null; &#125; 所以在hashmap进行get操作时，因为得到的hashCode值不同（注意，上述代码也许在某些情况下会得到相同的hashcode值，不过这种概率比较小，因为虽然两个对象的存储地址不同也有可能得到相同的hashcode值），所以导致在get方法中for循环不会执行，直接返回null。 因此如果想上述代码输出结果为“1”，很简单，只需要重写hashCode方法，让equals方法和hashCode方法始终在逻辑上保持一致性。 1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.HashMap;import java.util.HashSet;import java.util.Set; class People&#123; private String name; private int age; public People(String name,int age) &#123; this.name = name; this.age = age; &#125; public void setAge(int age)&#123; this.age = age; &#125; @Override public int hashCode() &#123; // TODO Auto-generated method stub return name.hashCode()+age; &#125; @Override public boolean equals(Object obj) &#123; // TODO Auto-generated method stub return this.name.equals(((People)obj).name) &amp;&amp; this.age== ((People)obj).age; &#125;&#125; public class Demo &#123; public static void main(String[] args) &#123; People p1 = new People(&quot;Jack&quot;, 12); System.out.println(p1.hashCode()); HashMap&lt;People, Integer&gt; hashMap = new HashMap&lt;People, Integer&gt;(); hashMap.put(p1, 1); System.out.println(hashMap.get(new People(&quot;Jack&quot;, 12))); &#125;&#125; 这样一来的话，输出结果就为“1”了。 下面这段话摘自Effective Java一书： 在程序执行期间，只要equals方法的比较操作用到的信息没有被修改，那么对这同一个对象调用多次，hashCode方法必须始终如一地返回同一个整数。 如果两个对象根据equals方法比较是相等的，那么调用两个对象的hashCode方法必须返回相同的整数结果。 如果两个对象根据equals方法比较是不等的，则hashCode方法不一定得返回不同的整数。 对于第二条和第三条很好理解，但是第一条，很多时候就会忽略。在《Java编程思想》一书中的P495页也有同第一条类似的一段话： “设计hashCode()时最重要的因素就是：无论何时，对同一个对象调用hashCode()都应该产生同样的值。如果在将一个对象用put()添加进HashMap时产生一个hashCdoe值，而用get()取出时却产生了另一个hashCode值，那么就无法获取该对象了。所以如果你的hashCode方法依赖于对象中易变的数据，用户就要当心了，因为此数据发生变化时，hashCode()方法就会生成一个不同的散列码”。 下面举个例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.util.HashMap;import java.util.HashSet;import java.util.Set; class People&#123; private String name; private int age; public People(String name,int age) &#123; this.name = name; this.age = age; &#125; public void setAge(int age)&#123; this.age = age; &#125; @Override public int hashCode() &#123; // TODO Auto-generated method stub return name.hashCode()*37+age; &#125; @Override public boolean equals(Object obj) &#123; // TODO Auto-generated method stub return this.name.equals(((People)obj).name) &amp;&amp; this.age== ((People)obj).age; &#125;&#125; public class Demo &#123; public static void main(String[] args) &#123; People p1 = new People(&quot;Jack&quot;, 12); System.out.println(p1.hashCode()); HashMap&lt;People, Integer&gt; hashMap = new HashMap&lt;People, Integer&gt;(); hashMap.put(p1, 2); p1.setAge(13); System.out.println(hashMap.get(p1)); &#125;&#125; 这段代码输出的结果为“null”，想必其中的原因大家应该都清楚了。 因此，在设计hashCode方法和equals方法的时候，如果对象中的数据易变，则最好在equals方法和hashCode方法中不要依赖于该字段。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java集合</tag>
        <tag>HashCode()</tag>
        <tag>equal()</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（9）：Java 集合对比]]></title>
    <url>%2F2017%2F08%2F17%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%889%EF%BC%89%EF%BC%9AJava%20%E9%9B%86%E5%90%88%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[一、HashMap与HashTable的区别HashMap和Hashtable的比较是Java面试中的常见问题，用来考验程序员是否能够正确使用集合类以及是否可以随机应变使用多种思路解决问题。HashMap的工作原理、ArrayList与Vector的比较以及这个问题是有关Java 集合框架的最经典的问题。 Hashtable是个过时的集合类，存在于Java API中很久了。在Java 4中被重写了，实现了Map接口，所以自此以后也成了Java集合框架中的一部分。Hashtable和HashMap在Java面试中相当容易被问到，甚至成为了集合框架面试题中最常被考的问题，所以在参加任何Java面试之前，都不要忘了准备这一题。 HashMap和Hashtable都实现了Map接口，但决定用哪一个之前先要弄清楚它们之间的分别。主要的区别有： HashMap HashTable 非线程安全（非线程同步） 线程安全（线程同步） 更适合于单线程 更适合于多线程 允许null值 不允许null值 迭代器Iterator是fail-fast迭代器 迭代器enumerator不是fail-fast的 初始容量为16 初始容量为11 两者最主要的区别在于Hashtable是线程安全，而HashMap则非线程安全 HashMap是非synchronized，而Hashtable是synchronized，这意味着Hashtable是线程安全的，多个线程可以共享一个Hashtable；而如果没有正确的同步的话，多个线程是不能共享HashMap的 由于Hashtable的实现方法里面都添加了synchronized关键字来确保线程同步，所以在单线程环境下它比HashMap要慢。如果你不需要同步，只需要单一线程，那么使用HashMap性能要好过Hashtable。仅在你需要完全的线程安全的时候使用Hashtable，而如果你使用Java 5或以上的话，请使用ConcurrentHashMap吧。 线程安全的实现原理：jvm有一个main memory，而每个线程有自己的working memory，一个线程对一个变量进行操作时，都要在自己的working memory里面建立一个copy，操作完之后再写入main memory。多个线程同时操作同一个变量，就可能会出现不可预知的结果。用synchronized的关键是建立一个镜像，这个镜像可以是要修改的变量也可以其他你认为合适的对象比如方法和类，然后通过给这个镜像加锁来实现线程安全，每个线程在获得这个锁之后，要执行完才会释放它得到的锁。这样就实现了所谓的线程安全。sychronized意味着在一次仅有一个线程能够更改Hashtable。就是说任何线程要更新Hashtable时要首先获得同步锁，其它线程要等到同步锁被释放之后才能再次获得同步锁更新Hashtable。 我们平时使用时若无特殊需求建议使用HashMap，在多线程环境下若使用HashMap需要使用Collections.synchronizedMap()方法来获取一个线程安全的集合（Collections.synchronizedMap()实现原理是Collections定义了一个SynchronizedMap的内部类，这个类实现了Map接口，在调用方法时使用synchronized来保证线程同步,当然了实际上操作的还是我们传入的HashMap实例，简单的说就是Collections.synchronizedMap()方法帮我们在操作HashMap时自动添加了synchronized来实现线程同步，类似的其它Collections.synchronizedXX方法也是类似原理） HashMap的迭代器Iterator是fail-fast迭代器，而Hashtable的enumerator迭代器不是fail-fast的。 当有其它线程改变了HashMap的结构（增加或者移除元素），将会抛出ConcurrentModificationException，但迭代器本身的remove()方法移除元素则不会抛出ConcurrentModificationException异常。但这并不是一个一定发生的行为，要看JVM。这条同样也是Enumeration和Iterator的区别。 Fail-safe和iterator迭代器相关。如果某个集合对象创建了Iterator或者ListIterator，然后其它的线程试图“结构上”更改集合对象，将会抛出ConcurrentModificationException异常。但其它线程可以通过set()方法更改集合对象是允许的，因为这并没有从“结构上”更改集合。但是假如已经从结构上进行了更改，再调用set()方法，将会抛出IllegalArgumentException异常。 结构上的更改指的是删除或者插入一个元素，这样会影响到map的结构。 HashMap可以使用null作为key，而Hashtable则不允许null作为key 虽说HashMap支持null值作为key，不过建议还是尽量避免这样使用，因为一旦不小心使用了，若因此引发一些问题，排查起来很是费事。HashMap以null作为key时，总是存储在table数组的第一个节点上 HashMap是对Map接口的实现，HashTable实现了Map接口和Dictionary抽象类 HashMap的初始容量为16，Hashtable初始容量为11，两者的填充因子默认都是0.75 HashMap扩容时是当前容量翻倍即:$capacity2$，Hashtable扩容时是容量翻倍+1即:$capacity2+1$ 两者计算hash的方法不同 Hashtable计算hash是直接使用key的hashcode对table数组的长度直接进行取模 12int hash = key.hashCode();int index = (hash &amp; 0x7FFFFFFF) % tab.length; HashMap计算hash对key的hashcode进行了二次hash，以获得更好的散列值，然后对table数组长度取摸 12345678static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;static int indexFor(int h, int n) &#123; return h &amp; (n-1); &#125; 二、HashSet与HashMap的区别HashMap和HashSet的区别是Java面试中最常被问到的问题。如果没有涉及到Collection框架以及多线程的面试，可以说是不完整。而Collection框架的问题不涉及到HashSet和HashMap，也可以说是不完整。HashMap和HashSet都是collection框架的一部分，它们让我们能够使用对象的集合。collection框架有自己的接口和实现，主要分为Set接口，List接口和Queue接口。它们有各自的特点，Set的集合里不允许对象有重复的值，List允许有重复，它对集合中的对象进行索引，Queue的工作原理是FCFS算法(First Come, First Serve)。 首先让我们来看看什么是HashMap和HashSet，然后再来比较它们之间的分别。 2.1 什么是HashSetHashSet实现了Set接口，它不允许集合中有重复的值，当我们提到HashSet时，第一件事情就是在将对象存储在HashSet之前，要先确保对象重写equals()和hashCode()方法，这样才能比较对象的值是否相等，以确保set中没有储存相等的对象。如果我们没有重写这两个方法，将会使用这个方法的默认实现。 public boolean add(Object o)方法用来在Set中添加元素，当元素值重复时则会立即返回false，如果成功添加的话会返回true。 HashSet不是key value结构，仅仅是存储不重复的元素，相当于简化版的HashMap，只是仅仅包含HashMap中的key而已。通过查看源码也证实了这一点，HashSet内部就是使用HashMap实现，只不过HashSet里面的HashMap所有的value都是同一个Object而已，因此HashSet也是非线程安全的，至于HashSet和Hashtable的区别，HashSet就是个简化的HashMap的。 下面是HashSet几个主要方法的实现，更具体的可以参考【Java学习手册：Java HashSet】 1234567891011121314151617181920212223private transient HashMap&lt;E,Object&gt; map; private static final Object PRESENT = new Object(); public HashSet() &#123; map = new HashMap&lt;E,Object&gt;(); &#125;public boolean contains(Object o) &#123; return map.containsKey(o); &#125;public boolean add(E e) &#123; return map.put(e, PRESENT)==null; &#125;public boolean add(E e) &#123; return map.put(e, PRESENT)==null; &#125;public boolean remove(Object o) &#123; return map.remove(o)==PRESENT; &#125;public void clear() &#123; map.clear(); &#125; 2.2 什么是HashMapHashMap实现了Map接口，Map接口对键值对进行映射。Map中不允许重复的键。Map接口有两个基本的实现，HashMap和TreeMap。TreeMap保存了对象的排列次序，而HashMap则不能。HashMap允许键和值为null。HashMap是非synchronized的，但collection框架提供方法能保证HashMap synchronized，这样多个线程同时访问HashMap时，能保证只有一个线程更改Map。 public Object put(Object Key,Object value)方法用来将元素添加到map中。 2.3 HashSet和HashMap的区别 HashMap HashSet HashMap实现了Map接口 HashSet实现了Set接口 HashMap储存键值对 HashSet仅仅存储对象 使用put()方法将元素放入map中 使用add()方法将元素放入set中 HashMap中使用键对象来计算hashcode值 HashSet使用成员对象来计算hashcode值，equals()方法判断对象相等性，不同返回false HashMap比较快，因为是使用唯一的键来获取对象 HashSet较HashMap来说比较慢 三、HashSet和TreeSet的区别 Hashset 的底层是由hashTable实现的，add()，remove()，contains()方法的时间复杂度是O(1).可以放入null，但只能放入一个null。Treeset 底层是由红黑树实现的,add()，remove()，contains()方法的时间复杂度是O(logn)。不允许放入null值 如果需要在Treeset 中插入对象，需要实现Comparable 接口，为其指定比较策略： 123456public class TreeSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements SortedSet&lt;E&gt;, Cloneable, java.io.Serializablepublic class HashSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable 其中SortedSet中组合了一个：Comparator&lt;? super E&gt; comparator(); HashSet是基于Hash算法实现的,其性能通常优于TreeSet,我们通常都应该使用HashSet,在我们需要排序的功能时,我门才使用TreeSet 四、ArrayList、LinkedList、Vector的底层实现和区别4.1 ArrayListArrayList是一个可以处理变长数组的类型，这里不局限于“数”组，ArrayList是一个泛型类，可以存放任意类型的对象。顾名思义，ArrayList是一个数组列表，因此其内部是使用一个数组来存放对象的，因为Object是一切类型的父类，因而ArrayList内部是有一个Object类型的数组类存放对象。ArrayList类常用的方法有add()、clear()、get()、indexOf()、remove()、sort()、toArray()、toString()等等，同时ArrayList内部有一个私有类实现Iterator接口，因此可以使用iterator()方法得到ArrayList的迭代器，同时，还有一个私有类实现了ListIterator接口，因此ArrayList也可以调用listIterator()方法得到ListIterator迭代器。 由于ArrayList是依靠数组来存放对象的，只不过封装起来了而已，因此其一些查找方法的效率都是O(n)，跟普通的数组效率差不多，只不过这个ArrayList是一个可变”数组“，并且可以存放一切指定的对象。 另外，由于ArrayList的所有方法都是默认在单一线程下进行的，因此ArrayList不具有线程安全性。若想在多线程下使用，应该使用Colletions类中的静态方法synchronizedList()对ArrayList进行调用即可。 4.2 LinkedListLinkedList可以看做为一个双向链表，所有的操作都可以认为是一个双向链表的操作，因为它实现了Deque接口和List接口。同样，LinkedList也是线程不安全的，如果在并发环境下使用它，同样用Colletions类中的静态方法synchronizedList()对LinkedList进行调用即可。 在LinkedList的内部实现中，并不是用普通的数组来存放数据的，而是使用结点来存放数据的，有一个指向链表头的结点first和一个指向链表尾的结点last。不同于ArrayList只能在数组末尾添加数据，LinkList可以很方便在链表头或者链表尾插入数据，或者在指定结点前后插入数据，还提供了取走链表头或链表尾的结点，或取走中间某个结点，还可以查询某个结点是否存在。add()方法默认在链表尾部插入数据。总之，LinkedList提供了大量方便的操作方法，并且它的插入或增加等方法的效率明显高于ArrayList类型，但是查询的效率要低一点，因为它是一个双向链表。 因此，LinkedList与ArrayList最大的区别是LinkedList更加灵活，并且部分方法的效率比ArrayList对应方法的效率要高很多，对于数据频繁出入的情况下，并且要求操作要足够灵活，建议使用LinkedList；对于数组变动不大，主要是用来查询的情况下，可以使用ArrayList。 4.3 VectorVector也是一个类似于ArrayList的可变长度的数组类型，它的内部也是使用数组来存放数据对象的。值得注意的是Vector与ArrayList唯一的区别是，Vector是线程安全的，即它的大部分方法都包含有关键字synchronized，因此，若对于单一线程的应用来说，最好使用ArrayList代替Vector，因为这样效率会快很多（类似的情况有StringBuffer与StringBuilder）；而在多线程程序中，为了保证数据的同步和一致性，可以使用Vector代替ArrayList实现同样的功能。 五、数组(Array)和列表(ArrayList)的区别 Array可以包含基本类型和对象类型，ArrayList只能包含对象类型。 Array大小是固定的，ArrayList的大小是动态变化的。 ArrayList提供了更多的方法和特性，比如：addAll()，removeAll()，iterator()等等。 ArrayList可以存任何Object，如String等。 ArrayList与数组的区别主要就是由于动态增容的效率问题了。对于基本类型数据，集合使用自动装箱来减少编码工作量。但是，当处理固定大小的基本数据类型的时候，这种方式相对比较慢。因此基本类型用Array，动态变化用ArrayList。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（8）：Java 集合框架]]></title>
    <url>%2F2017%2F08%2F16%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%888%EF%BC%89%EF%BC%9AJava%20%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[一、概念1.1 什么是集合？Java官方的入门文档是这样描述集合的： Collection(有时候也叫container)是一个简单的对象，它把多个元素组织成一个单元。集合可以用来存储、检索、操作、通信。通常情况下，集合代表了一个自然数据项，比如一组手牌(牌的集合)、邮件文件夹(邮件的集合)、电话目录(姓名到电话的映射)。如果你使用过Java或者其他语言，你应该很熟悉集合。 1.2 什么是集合框架？Collections Framework是一个用来表示和操作集合的统一的架构。集合的框架包括了： Interfaces:这些是表示集合的抽象数据类型，接口允许集合完成操作，独立与其详细的实现。在面向对象的语言中，接口构成了体系架构； Implementations:这些是接口的具体实现。本质上，是一些可复用的数据结构； Algorithms:这些方法可以对接口实现的对象进行有用的计算，比如搜索、排序。这些算法是具有多态性的：也就是说，同样的方法可以用在合适的接口的不同实现。本质上，是一些可复用的函数。 除了Java的集合框架，还有一些著名的集合框架的例子：比如C++的STL和Smalltalk的集合架构。从历史上来看，集合框架可能比较复杂，也可能有一些很陡峭的学习曲线。不过我们相信Java的集合框架会突破这样的传统，在这章你就可以自己学会。 1.3 使用集合框架有什么好处？Java的集合框架提供了一下优点： 减少编程的工作量：通过提供有用的数据结构和算法，集合框架能让你更专注的实现程序的核心功能，而不是去做一个底层的“管道工”。Java框架通过促进无关API的互操作性，使得你不用自己去实现不同API的适配 提高程序的速度与质量：集合框架提供了一些有用数据结构和算法的高性能、高质量的实现。每个接口的不同的实现也是可以互换的，所以程序可以通过切换集合来做一些调整。正因为你从实现数据结构的那些苦差事中脱离出来，你才可以有更多的实现去改善你自己程序的性能和质量 允许无关APIs的互操作：集合接口是API之间传递集合的一个“方言”，比如我的网络管理API有一个节点名的集合，而GUI工具需要一个列标题的集合，即使是分开实现它们，我们的APIs也可以无缝的接合。 省力地学习和使用新API：这是另一个领先的优势，设计者和实现者没必要在每次都重新设计API的时候都“推倒重来”地实现集合，而是直接使用标准的集合接口就好了。 促进软件的复用：符合标准集合接口的新数据结构本质上是可以复用的。对于操作这些新数据结构算法也是一样可以复用的。 二、集合框架Java集合工具包位于Java.util包下，包含了很多常用的数据结构，如数组、链表、栈、队列、集合、哈希表等。学习Java集合框架下大致可以分为如下五个部分：List列表、Set集合、Map映射、迭代器（Iterator、Enumeration）、工具类（Arrays、Collections）。 Java集合类的整体框架如下： 从上图中可以看出，集合类主要分为两大类：Collection和Map。 Collection是List、Set等集合高度抽象出来的接口，它包含了这些集合的基本操作，它主要又分为两大部分：List和Set。 List接口通常表示一个列表（数组、队列、链表、栈等），其中的元素可以重复，常用实现类为ArrayList和LinkedList，另外还有不常用的Vector。 另外，LinkedList还是实现了Queue接口，因此也可以作为队列使用。 Set接口通常表示一个集合，其中的元素不允许重复（通过hashcode和equals函数保证），常用实现类有HashSet和TreeSet，HashSet是通过Map中的HashMap实现的，而TreeSet是通过Map中的TreeMap实现的。另外，TreeSet还实现了SortedSet接口，因此是有序的集合（集合中的元素要实现Comparable接口，并覆写Compartor函数才行）。我们看到，抽象类AbstractCollection、AbstractList和AbstractSet分别实现了Collection、List和Set接口，这就是在Java集合框架中用的很多的适配器设计模式，用这些抽象类去实现接口，在抽象类中实现接口中的若干或全部方法，这样下面的一些类只需直接继承该抽象类，并实现自己需要的方法即可，而不用实现接口中的全部抽象方法。 Map是一个映射接口，其中的每个元素都是一个key-value键值对，同样抽象类AbstractMap通过适配器模式实现了Map接口中的大部分函数，TreeMap、HashMap、WeakHashMap等实现类都通过继承AbstractMap来实现，另外，不常用的HashTable直接实现了Map接口，它和Vector都是JDK1.0就引入的集合类。 Iterator是遍历集合的迭代器（不能遍历Map，只用来遍历Collection），Collection的实现类都实现了iterator()函数，它返回一个Iterator对象，用来遍历集合，ListIterator则专门用来遍历List。而Enumeration则是JDK1.0时引入的，作用与Iterator相同，但它的功能比Iterator要少，它只能再Hashtable、Vector和Stack中使用。 Arrays和Collections是用来操作数组、集合的两个工具类，例如在ArrayList和Vector中大量调用了Arrays.Copyof()方法，而Collections中有很多静态方法可以返回各集合类的synchronized版本，即线程安全的版本，当然了，如果要用线程安全的结合类，首选Concurrent并发包下的对应的集合类。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>LinkedList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（7）：Java LinkedList]]></title>
    <url>%2F2017%2F08%2F15%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%887%EF%BC%89%EF%BC%9AJava%20LinkedList%2F</url>
    <content type="text"><![CDATA[一、概述LinkedList和ArrayList一样，都实现了List接口，但其内部的数据结构有本质的不同。LinkedList是基于链表实现的（通过名字也能区分开来），所以它的插入和删除操作比ArrayList更加高效。但也是由于其为基于链表的，所以随机访问的效率要比ArrayList差。 看一下LinkedList的类的定义： 1234public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable&#123;&#125; LinkedList继承自AbstractSequenceList，实现了List、Deque、Cloneable、java.io.Serializable接口。AbstractSequenceList提供了List接口骨干性的实现以减少实现List接口的复杂度，Deque接口定义了双端队列的操作。 在LinkedList中除了本身自己的方法外，还提供了一些可以使其作为栈、队列或者双端队列的方法。这些方法可能彼此之间只是名字不同，以使得这些名字在特定的环境中显得更加合适。 1234LinkedList&lt;String&gt; list = new LinkedList&lt;String&gt;();list.add(&quot;语文: 1&quot;);list.add(&quot;数学: 2&quot;);list.add(&quot;英语: 3&quot;); 结构也相对简单一些，如下图所示： 二、数据结构LinkedList是基于链表结构实现，所以在类中包含了first和last两个指针(Node)。Node中包含了上一个节点和下一个节点的引用，这样就构成了双向的链表。每个Node只能知道自己的前一个节点和后一个节点，但对于链表来说，这已经足够了。 123456789101112131415transient int size = 0;transient Node&lt;E&gt; first; //链表的头指针transient Node&lt;E&gt; last; //尾指针//存储对象的结构 Node, LinkedList的内部类private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; // 指向下一个节点 Node&lt;E&gt; prev; //指向上一个节点 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; 三、存储3.1 add(E e)该方法是在链表的end添加元素，其调用了自己的方法linkLast(E e)。 该方法首先将last的Node引用指向了一个新的Node(l)，然后根据l新建了一个newNode，其中的元素就为要添加的e；而后，我们让last指向了newNode。接下来是自身进行维护该链表。 1234567891011121314151617181920212223242526/** * Appends the specified element to the end of this list. * * &lt;p&gt;This method is equivalent to &#123;@link #addLast&#125;. * * @param e element to be appended to this list * @return &#123;@code true&#125; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; linkLast(e); return true;&#125;/*** Links e as last element.*/void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; 3.2 add(int index, E element)该方法是在指定index位置插入元素。如果index位置正好等于size，则调用linkLast(element)将其插入末尾；否则调用 linkBefore(element, node(index))方法进行插入。该方法的实现在下面，大家可以自己仔细的分析一下。（分析链表的时候最好能够边画图边分析） 1234567891011121314151617181920212223242526272829303132/** * Inserts the specified element at the specified position in this list. * Shifts the element currently at that position (if any) and any * subsequent elements to the right (adds one to their indices). * * @param index index at which the specified element is to be inserted * @param element element to be inserted * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ public void add(int index, E element) &#123; checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index)); &#125; /** * Inserts element e before non-null Node succ. */ void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++; &#125; LinkedList的方法实在是太多，在这没法一一举例分析。但很多方法其实都只是在调用别的方法而已，所以建议大家将其几个最核心的添加的方法搞懂就可以了，比如linkBefore、linkLast。其本质也就是链表之间的删除添加等。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>LinkedList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（6）：Java ArrayList]]></title>
    <url>%2F2017%2F08%2F14%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%886%EF%BC%89%EF%BC%9AJava%20ArrayList%2F</url>
    <content type="text"><![CDATA[一、概述ArrayList可以理解为动态数组，就是Array的复杂版本。与Java中的数组相比，它的容量能动态增长。ArrayList是List接口的可变数组的实现。实现了所有可选列表操作，并允许包括 null 在内的所有元素。除了实现 List 接口外，此类还提供一些方法来操作内部用来存储列表的数组的大小。（此类大致上等同于 Vector 类，除了此类是不同步的。） 每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。 注意，此实现不是同步的。如果多个线程同时访问一个ArrayList实例，而其中至少一个线程从结构上修改了列表，那么它必须保持外部同步。（结构上的修改是指任何添加或删除一个或多个元素的操作，或者显式调整底层数组的大小；仅仅设置元素的值不是结构上的修改。） 我们先学习了解其内部的实现原理，才能更好的理解其应用。 二、ArrayList的实现对于ArrayList而言，它实现List接口、底层使用数组保存所有元素。其操作基本上是对数组的操作。下面我们来分析ArrayList的源代码： 2.1 List接口1234public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123;&#125; ArrayList继承了AbstractList，实现了List。它是一个数组队列，提供了相关的添加、删除、修改、遍历等功能。 ArrayList实现了RandmoAccess接口，即提供了随机访问功能。RandmoAccess是java中用来被List实现，为List提供快速访问功能的。在ArrayList中，我们即可以通过元素的序号快速获取元素对象；这就是快速随机访问。 ArrayList实现了Cloneable接口，即覆盖了函数clone()，能被克隆。 ArrayList实现java.io.Serializable接口，这意味着ArrayList支持序列化，能通过序列化去传输。 2.2 底层使用数组实现12345/*** The array buffer into which the elements of the ArrayList are stored.* The capacity of the ArrayList is the length of this array buffer.*/private transient Object[] elementData; 2.3 构造方法123456789101112131415161718192021222324252627282930313233343536/** * Constructs an empty list with an initial capacity of ten. */public ArrayList() &#123; this(10);&#125;/** * Constructs an empty list with the specified initial capacity. * * @param initialCapacity the initial capacity of the list * @throws IllegalArgumentException if the specified initial capacity * is negative */public ArrayList(int initialCapacity) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); this.elementData = new Object[initialCapacity];&#125;/** * Constructs a list containing the elements of the specified * collection, in the order they are returned by the collection's * iterator. * * @param c the collection whose elements are to be placed into this list * @throws NullPointerException if the specified collection is null */public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); size = elementData.length; // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class);&#125; ArrayList提供了三种方式的构造器： public ArrayList()：可以构造一个默认初始容量为10的空列表； public ArrayList(int initialCapacity)：构造一个指定初始容量的空列表； public ArrayList(Collection&lt;? extends E&gt; c)：构造一个包含指定collection的元素的列表，这些元素按照该collection的迭代器返回它们的顺序排列的。 2.4 存储ArrayList提供了set(int index, E element)、add(E e)、add(int index, E element)、addAll(Collection&lt;? extends E&gt; c)、addAll(int index, Collection&lt;? extends E&gt; c)这些添加元素的方法。下面我们一一讲解： set(int index, E element)：该方法首先调用rangeCheck(index)来校验index变量是否超出数组范围，超出则抛出异常。而后，取出原index位置的值，并且将新的element放入Index位置，返回oldValue。 123456789101112131415161718192021222324252627/** * Replaces the element at the specified position in this list with * the specified element. * * @param index index of the element to replace * @param element element to be stored at the specified position * @return the element previously at the specified position * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ // 用指定的元素替代此列表中指定位置上的元素，并返回以前位于该位置上的元素。public E set(int index, E element) &#123; rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125;/** * Checks if the given index is in range. If not, throws an appropriate * runtime exception. This method does *not* check if the index is * negative: It is always used immediately prior to an array access, * which throws an ArrayIndexOutOfBoundsException if index is negative. */ private void rangeCheck(int index) &#123; if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); &#125; add(E e)：该方法是将指定的元素添加到列表的尾部。当容量不足时，会调用grow方法增长容量。 1234567891011121314151617181920212223242526272829/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &lt;tt&gt;true&lt;/tt&gt; (as specified by &#123;@link Collection#add&#125;) */ // 将指定的元素添加到此列表的尾部。public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; add(int index, E element)：在index位置插入element。 1234567891011121314151617181920212223/** * Inserts the specified element at the specified position in this * list. Shifts the element currently at that position (if any) and * any subsequent elements to the right (adds one to their indices). * * @param index index at which the specified element is to be inserted * @param element element to be inserted * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ // 将指定的元素插入此列表中的指定位置。 // 如果当前位置有元素，则向右移动当前位于该位置的元素以及所有后续元素（将其索引加1）。public void add(int index, E element) &#123; rangeCheckForAdd(index); // 如果数组长度不足，将进行扩容。 ensureCapacityInternal(size + 1); // Increments modCount!! // 将 elementData中从Index位置开始、长度为size-index的元素， // 拷贝到从下标为index+1位置开始的新的elementData数组中。 // 即将当前位于该位置的元素以及所有后续元素右移一个位置。 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; addAll(Collection&lt;? extends E&gt; c)和addAll(int index, Collection&lt;? extends E&gt; c)：将特定Collection中的元素添加到Arraylist末尾。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Appends all of the elements in the specified collection to the end of * this list, in the order that they are returned by the * specified collection&apos;s Iterator. The behavior of this operation is * undefined if the specified collection is modified while the operation * is in progress. (This implies that the behavior of this call is * undefined if the specified collection is this list, and this * list is nonempty.) * * @param c collection containing elements to be added to this list * @return &lt;tt&gt;true&lt;/tt&gt; if this list changed as a result of the call * @throws NullPointerException if the specified collection is null */ // 按照指定collection的迭代器所返回的元素顺序，将该collection中的所有元素添加到此列表的尾部。 public boolean addAll(Collection&lt;? extends E&gt; c) &#123; Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0; &#125; /** * Inserts all of the elements in the specified collection into this * list, starting at the specified position. Shifts the element * currently at that position (if any) and any subsequent elements to * the right (increases their indices). The new elements will appear * in the list in the order that they are returned by the * specified collection&apos;s iterator. * * @param index index at which to insert the first element from the * specified collection * @param c collection containing elements to be added to this list * @return &lt;tt&gt;true&lt;/tt&gt; if this list changed as a result of the call * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; * @throws NullPointerException if the specified collection is null */ // 从指定的位置开始，将指定collection中的所有元素插入到此列表中。 public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; rangeCheckForAdd(index); Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount int numMoved = size - index; if (numMoved &gt; 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0; &#125; 在ArrayList的存储方法，其核心本质是在数组的某个位置将元素添加进入。但其中又会涉及到关于数组容量不够而增长等因素。 2.5 读取这个方法就比较简单了，ArrayList能够支持随机访问的原因也是很显然的，因为它内部的数据结构是数组，而数组本身就是支持随机访问。该方法首先会判断输入的index值是否越界，然后将数组的index位置的元素返回即可。 12345678910111213141516/*** Returns the element at the specified position in this list.** @param index index of the element to return* @return the element at the specified position in this list* @throws IndexOutOfBoundsException &#123;@inheritDoc&#125;*/// 返回此列表中指定位置上的元素。public E get(int index) &#123; rangeCheck(index); return (E) elementData[index];&#125;private void rangeCheck(int index) &#123; if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125; 2.6 删除ArrayList提供了根据下标或者指定对象两种方式的删除功能。需要注意的是该方法的返回值并不相同，如下： 根据下标删除： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// 移除此列表中指定位置上的元素。 /** * Removes the element at the specified position in this list. * Shifts any subsequent elements to the left (subtracts one from their * indices). * * @param index the index of the element to be removed * @return the element that was removed from the list * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // Let gc do its work return oldValue; &#125;``` - 指定对象删除：```java/** * Removes the first occurrence of the specified element from this list, * if it is present. If the list does not contain the element, it is * unchanged. More formally, removes the element with the lowest index * &lt;tt&gt;i&lt;/tt&gt; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;get(i)==null&amp;nbsp;:&amp;nbsp;o.equals(get(i)))&lt;/tt&gt; * (if such an element exists). Returns &lt;tt&gt;true&lt;/tt&gt; if this list * contained the specified element (or equivalently, if this list * changed as a result of the call). * * @param o element to be removed from this list, if present * @return &lt;tt&gt;true&lt;/tt&gt; if this list contained the specified element */ // 移除此列表中首次出现的指定元素（如果存在）。这是应为ArrayList中允许存放重复的元素。 public boolean remove(Object o) &#123; // 由于ArrayList中允许存放null，因此下面通过两种情况来分别处理。 if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; // 类似remove(int index)，移除列表中指定位置上的元素。 fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false; &#125; 注意：从数组中移除元素的操作，也会导致被移除的元素以后的所有元素的向左移动一个位置。 2.7 调整数组容量从上面介绍的向ArrayList中存储元素的代码中，我们看到，每当向数组中添加元素时，都要去检查添加后元素的个数是否会超出当前数组的长度，如果超出，数组将会进行扩容，以满足添加数据的需求。数组扩容有两个方法，其中开发者可以通过一个public的方法ensureCapacity(int minCapacity)来增加ArrayList的容量，而在存储元素等操作过程中，如果遇到容量不足，会调用priavte方法private void ensureCapacityInternal(int minCapacity)实现。 12345678910111213141516171819202122232425262728public void ensureCapacity(int minCapacity) &#123; if (minCapacity &gt; 0) ensureCapacityInternal(minCapacity);&#125;private void ensureCapacityInternal(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;/** * Increases the capacity to ensure that it can hold at least the * number of elements specified by the minimum capacity argument. * * @param minCapacity the desired minimum capacity */private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 从上述代码中可以看出，数组进行扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量的增长大约是其原容量的1.5倍（从int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1)这行代码得出）。这种操作的代价是很高的，因此在实际使用时，我们应该尽量避免数组容量的扩张。当我们可预知要保存的元素的多少时，要在构造ArrayList实例时，就指定其容量，以避免数组扩容的发生。或者根据实际需求，通过调用ensureCapacity方法来手动增加ArrayList实例的容量。 ArrayList还给我们提供了将底层数组的容量调整为当前列表保存的实际元素的大小的功能。它可以通过trimToSize方法来实现。代码如下： 1234567public void trimToSize() &#123; modCount++; int oldCapacity = elementData.length; if (size &lt; oldCapacity) &#123; elementData = Arrays.copyOf(elementData, size); &#125; &#125; 2.8 Fail-Fast机制ArrayList也采用了快速失败的机制，通过记录modCount参数来实现。在面对并发的修改时，迭代器很快就会完全失败，而不是冒着在将来某个不确定时间发生任意不确定行为的风险。关于Fail-Fast的更详细的介绍，在之前HashMap中已经提到。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>ArrayList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（5）：Java LinkedHashSet]]></title>
    <url>%2F2017%2F08%2F13%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%885%EF%BC%89%EF%BC%9AJava%20LinkedHashSet%2F</url>
    <content type="text"><![CDATA[一、概述首先我们需要知道的是它是一个Set的实现，所以它其中存的肯定不是键值对，而是值。此实现与HashSet的不同之处在于，LinkedHashSet维护着一个运行于所有条目的双重链接列表。此链接列表定义了迭代顺序，该迭代顺序可为插入顺序或是访问顺序。 看到上面的介绍，是不是感觉其与HashMap和LinkedHashMap的关系很像？ 注意，此实现不是同步的。如果多个线程同时访问链接的哈希Set，而其中至少一个线程修改了该Set，则它必须保持外部同步。 在【Java学习手册：LinkedHashMap】中，通过例子演示了HashMap和LinkedHashMap的区别。举一反三，我们现在学习的LinkedHashSet与之前的很相同，只不过之前存的是键值对，而现在存的只有值。 LinkedHashSet是可以按照插入顺序或者访问顺序进行迭代。 二、LinkedHashSet的实现对于LinkedHashSet而言，它继承与HashSet、又基于LinkedHashMap来实现的。LinkedHashSet底层使用LinkedHashMap来保存所有元素，它继承与HashSet，其所有的方法操作上又与HashSet相同，因此LinkedHashSet的实现上非常简单，只提供了四个构造方法，并通过传递一个标识参数，调用父类的构造器，底层构造一个LinkedHashMap来实现，在相关操作上与父类HashSet的操作相同，直接调用父类HashSet的方法即可。LinkedHashSet的源代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable &#123; private static final long serialVersionUID = -2851667679971038690L; /** * 构造一个带有指定初始容量和加载因子的新空链接哈希set。 * * 底层会调用父类的构造方法，构造一个有指定初始容量和加载因子的LinkedHashMap实例。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 */ public LinkedHashSet(int initialCapacity, float loadFactor) &#123; super(initialCapacity, loadFactor, true); &#125; /** * 构造一个带指定初始容量和默认加载因子0.75的新空链接哈希set。 * * 底层会调用父类的构造方法，构造一个带指定初始容量和默认加载因子0.75的LinkedHashMap实例。 * @param initialCapacity 初始容量。 */ public LinkedHashSet(int initialCapacity) &#123; super(initialCapacity, .75f, true); &#125; /** * 构造一个带默认初始容量16和加载因子0.75的新空链接哈希set。 * * 底层会调用父类的构造方法，构造一个带默认初始容量16和加载因子0.75的LinkedHashMap实例。 */ public LinkedHashSet() &#123; super(16, .75f, true); &#125; /** * 构造一个与指定collection中的元素相同的新链接哈希set。 * * 底层会调用父类的构造方法，构造一个足以包含指定collection * 中所有元素的初始容量和加载因子为0.75的LinkedHashMap实例。 * @param c 其中的元素将存放在此set中的collection。 */ public LinkedHashSet(Collection&lt;? extends E&gt; c) &#123; super(Math.max(2*c.size(), 11), .75f, true); addAll(c); &#125;&#125; 以上几乎就是LinkedHashSet的全部代码了，那么读者可能就会怀疑了，不是说LinkedHashSet是基于LinkedHashMap实现的吗？那我为什么在源码中甚至都没有看到出现过LinkedHashMap。不要着急，我们可以看到在LinkedHashSet的构造方法中，其调用了父类的构造方法。我们可以进去看一下： 123456789101112/** * 以指定的initialCapacity和loadFactor构造一个新的空链接哈希集合。 * 此构造函数为包访问权限，不对外公开，实际只是是对LinkedHashSet的支持。 * * 实际底层会以指定的参数构造一个空LinkedHashMap实例来实现。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 * @param dummy 标记。 */HashSet(int initialCapacity, float loadFactor, boolean dummy) &#123; map = new LinkedHashMap&lt;E,Object&gt;(initialCapacity, loadFactor);&#125; 在父类HashSet中，专为LinkedHashSet提供的构造方法如下，该方法为包访问权限，并未对外公开。由上述源代码可见，LinkedHashSet通过继承HashSet，底层使用LinkedHashMap，以很简单明了的方式来实现了其自身的所有功能。 三、总结以上就是关于LinkedHashSet的内容，我们只是从概述上以及构造方法这几个方面介绍了，并不是我们不想去深入其读取或者写入方法，而是其本身没有实现，只是继承于父类HashSet的方法。 所以我们需要注意的点是： LinkedHashSet是Set的一个具体实现，其维护着一个运行于所有条目的双重链接列表。此链接列表定义了迭代顺序，该迭代顺序可为插入顺序或是访问顺序。 LinkedHashSet继承与HashSet，并且其内部是通过LinkedHashMap来实现的。有点类似于我们之前说的LinkedHashMap其内部是基于Hashmap实现一样，不过还是有一点点区别的（具体的区别大家可以自己去思考一下）。 如果我们需要迭代的顺序为插入顺序或者访问顺序，那么LinkedHashSet是需要你首先考虑的。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>LinkedHashSet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（4）：Java LinkedHashMap]]></title>
    <url>%2F2017%2F08%2F12%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%884%EF%BC%89%EF%BC%9AJava%20LinkedHashMap%2F</url>
    <content type="text"><![CDATA[一、概述HashMap是无序的，HashMap在put的时候是根据key的hashcode进行hash然后放入对应的地方。所以在按照一定顺序put进HashMap中，然后遍历出HashMap的顺序跟put的顺序不同（除非在put的时候key已经按照hashcode排序好了，这种几率非常小） JAVA在JDK1.4以后提供了LinkedHashMap来帮助我们实现了有序的HashMap。 LinkedHashMap是HashMap的一个子类，它保留插入的顺序， 如果需要输出的顺序和输入时的相同，那么就选用LinkedHashMap。 LinkedHashMap是Map接口的哈希表和链接列表实现，具有可预知的迭代顺序。此实现提供所有可选的映射操作，并允许使用null值和null键。此类不保证映射的顺序，特别是它不保证该顺序恒久不变。 LinkedHashMap实现与HashMap的不同之处在于，LinkedHashMap维护着一个运行于所有条目的双重链接列表。此链接列表定义了迭代顺序，该迭代顺序可以是插入顺序或者是访问顺序。 注意，此实现不是同步的。如果多个线程同时访问链接的哈希映射，而其中至少一个线程从结构上修改了该映射，则它必须保持外部同步。 根据链表中元素的顺序可以分为：按插入顺序的链表和按访问顺序(调用get方法)的链表。默认是按插入顺序排序，如果指定按访问顺序排序，那么调用get方法后，会将这次访问的元素移至链表尾部，不断访问可以形成按访问顺序排序的链表。 我们写一个简单的LinkedHashMap的程序： 123456789101112LinkedHashMap&lt;String, Integer&gt; lmap = new LinkedHashMap&lt;String, Integer&gt;();lmap.put("语文", 1);lmap.put("数学", 2);lmap.put("英语", 3);lmap.put("历史", 4);lmap.put("政治", 5);lmap.put("地理", 6);lmap.put("生物", 7);lmap.put("化学", 8);for(Entry&lt;String, Integer&gt; entry : lmap.entrySet()) &#123; System.out.println(entry.getKey() + ": " + entry.getValue());&#125; 运行结果是： 12345678语文: 1数学: 2英语: 3历史: 4政治: 5地理: 6生物: 7化学: 8 我们可以观察到，和HashMap的运行结果不同，LinkedHashMap的迭代输出的结果保持了插入顺序。是什么样的结构使得LinkedHashMap具有如此特性呢？我们还是一样的看看LinkedHashMap的内部结构，对它有一个感性的认识： Hash table and linked list implementation of the Map interface, with predictable iteration order. This implementation differs from HashMap in that it maintains a doubly-linked list running through all of its entries. This linked list defines the iteration ordering, which is normally the order in which keys were inserted into the map (insertion-order). 没错，正如官方文档所说：LinkedHashMap是Hash表和链表的实现，并且依靠着双向链表保证了迭代顺序是插入的顺序。 二、插入顺序、访问顺序的演示先做几个demo来演示一下LinkedHashMap的使用。看懂了其效果，然后再来研究其原理。 2.1 HashMap看下面这个代码： 12345678910111213public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put(&quot;apple&quot;, &quot;苹果&quot;); map.put(&quot;watermelon&quot;, &quot;西瓜&quot;); map.put(&quot;banana&quot;, &quot;香蕉&quot;); map.put(&quot;peach&quot;, &quot;桃子&quot;); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); System.out.println(entry.getKey() + &quot;=&quot; + entry.getValue()); &#125;&#125; 一个比较简单的测试HashMap的代码，通过控制台的输出，我们可以看到HashMap是没有顺序的。 1234banana=香蕉apple=苹果peach=桃子watermelon=西瓜 2.2 LinkedHashMap我们现在将map的实现换成LinkedHashMap，其他代码不变：Map&lt;String, String&gt; map = new LinkedHashMap&lt;String, String&gt;();看一下控制台的输出： 1234apple=苹果watermelon=西瓜banana=香蕉peach=桃子 我们可以看到，其输出顺序是完成按照插入顺序的！也就是我们上面所说的保留了插入的顺序。我们不是在上面还提到过其可以按照访问顺序进行排序么？好的，我们还是通过一个例子来验证一下： 12345678910111213141516public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new LinkedHashMap&lt;String, String&gt;(16,0.75f,true); map.put(&quot;apple&quot;, &quot;苹果&quot;); map.put(&quot;watermelon&quot;, &quot;西瓜&quot;); map.put(&quot;banana&quot;, &quot;香蕉&quot;); map.put(&quot;peach&quot;, &quot;桃子&quot;); map.get(&quot;banana&quot;); map.get(&quot;apple&quot;); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); System.out.println(entry.getKey() + &quot;=&quot; + entry.getValue()); &#125;&#125; 代码与之前的都差不多，但我们多了两行代码，并且初始化LinkedHashMap的时候，用的构造函数也不相同，看一下控制台的输出结果： 1234watermelon=西瓜peach=桃子banana=香蕉apple=苹果 这也就是我们之前提到过的，LinkedHashMap可以选择按照访问顺序进行排序。 三、LinkedHashMap的实现对于LinkedHashMap而言，它继承于HashMap、底层使用哈希表与双向链表来保存所有元素。其基本操作与父类HashMap相似，它通过重写父类相关的方法，来实现自己的链接列表特性。下面我们来分析LinkedHashMap的源代码： 3.1 成员变量LinkedHashMap采用的hash算法和HashMap相同，但是它重新定义了数组中保存的元素Entry，该Entry除了保存当前对象的引用外，还保存了其上一个元素before和下一个元素after的引用，从而在哈希表的基础上又构成了双向链接列表。看源代码： 12345678910111213141516171819/*** The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt;* for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order.* 如果为true，则按照访问顺序；如果为false，则按照插入顺序。*/private final boolean accessOrder;/*** 双向链表的表头元素。 */private transient Entry&lt;K,V&gt; header;/*** LinkedHashMap的Entry元素。* 继承HashMap的Entry元素，又保存了其上一个元素before和下一个元素after的引用。 */private static class Entry&lt;K,V&gt; extends HashMap.Entry&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; ……&#125; LinkedHashMap中的Entry集成与HashMap的Entry，但是其增加了before和after的引用，指的是上一个元素和下一个元素的引用。 3.2 初始化通过源代码可以看出，在LinkedHashMap的构造方法中，实际调用了父类HashMap的相关构造方法来构造一个底层存放的table数组，但额外可以增加accessOrder这个参数，如果不设置，默认为false，代表按照插入顺序进行迭代；当然可以显式设置为true，代表以访问顺序进行迭代。如： 1234public LinkedHashMap(int initialCapacity, float loadFactor,boolean accessOrder) &#123; super(initialCapacity, loadFactor); this.accessOrder = accessOrder;&#125; HashMap中的相关构造方法： 1234567891011121314151617181920public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); // Find a power of 2 &gt;= initialCapacity int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; this.loadFactor = loadFactor; threshold = (int)(capacity * loadFactor); table = new Entry[capacity]; init(); &#125; 我们已经知道LinkedHashMap的Entry元素继承HashMap的Entry，提供了双向链表的功能。在上述HashMap的构造器中，最后会调用init()方法，进行相关的初始化，这个方法在HashMap的实现中并无意义，只是提供给子类实现相关的初始化调用。 LinkedHashMap重写了init()方法，在调用父类的构造方法完成构造后，进一步实现了对其元素Entry的初始化操作。 12345678910/*** Called by superclass constructors and pseudoconstructors (clone,* readObject) before any entries are inserted into the map. Initializes* the chain.*/@Overridevoid init() &#123; header = new Entry&lt;&gt;(-1, null, null, null); header.before = header.after = header;&#125; 3.3 存储LinkedHashMap并未重写父类HashMap的put方法，而是重写了父类HashMap的put方法调用的子方法void recordAccess(HashMap m) ，void addEntry(int hash, K key, V value, int bucketIndex) 和void createEntry(int hash, K key, V value, int bucketIndex)，提供了自己特有的双向链接列表的实现。我们在之前的文章中已经讲解了HashMap的put方法，我们在这里重新贴一下HashMap的put方法的源代码： 12345678910111213141516171819public V put(K key, V value) &#123; if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); return null;&#125; 重写方法： 1234567891011121314151617181920212223242526272829303132333435363738void recordAccess(HashMap&lt;K,V&gt; m) &#123; LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m; if (lm.accessOrder) &#123; lm.modCount++; remove(); addBefore(lm.header); &#125;&#125;void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 调用create方法，将新元素以双向链表的的形式加入到映射中。 createEntry(hash, key, value, bucketIndex); // 删除最近最少使用元素的策略定义 Entry&lt;K,V&gt; eldest = header.after; if (removeEldestEntry(eldest)) &#123; removeEntryForKey(eldest.key); &#125; else &#123; if (size &gt;= threshold) resize(2 * table.length); &#125;&#125;void createEntry(int hash, K key, V value, int bucketIndex) &#123; HashMap.Entry&lt;K,V&gt; old = table[bucketIndex]; Entry&lt;K,V&gt; e = new Entry&lt;K,V&gt;(hash, key, value, old); table[bucketIndex] = e; // 调用元素的addBrefore方法，将元素加入到哈希、双向链接列表。 e.addBefore(header); size++;&#125;private void addBefore(Entry&lt;K,V&gt; existingEntry) &#123; after = existingEntry; before = existingEntry.before; before.after = this; after.before = this;&#125; 3.4 读取LinkedHashMap重写了父类HashMap的get方法，实际在调用父类getEntry()方法取得查找的元素后，再判断当排序模式accessOrder为true时，记录访问顺序，将最新访问的元素添加到双向链表的表头，并从原来的位置删除。由于的链表的增加、删除操作是常量级的，故并不会带来性能的损失。 12345678910111213141516171819202122232425262728293031323334public V get(Object key) &#123; // 调用父类HashMap的getEntry()方法，取得要查找的元素。 Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)getEntry(key); if (e == null) return null; // 记录访问顺序。 e.recordAccess(this); return e.value;&#125;void recordAccess(HashMap&lt;K,V&gt; m) &#123; LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m; // 如果定义了LinkedHashMap的迭代顺序为访问顺序， // 则删除以前位置上的元素，并将最新访问的元素添加到链表表头。 if (lm.accessOrder) &#123; lm.modCount++; remove(); addBefore(lm.header); &#125;&#125;/*** Removes this entry from the linked list.*/private void remove() &#123; before.after = after; after.before = before;&#125;/**clear链表，设置header为初始状态*/public void clear() &#123; super.clear(); header.before = header.after = header;&#125; LinkedHashMap的这些额外操作基本上都是为了维护好那个具有访问顺序的双向链表，目的就是保持双向链表中节点的顺序要从eldest到youngest。 3.4 排序模式LinkedHashMap定义了排序模式accessOrder，该属性为boolean型变量，对于访问顺序，为true；对于插入顺序，则为false。 1private final boolean accessOrder; 一般情况下，不必指定排序模式，其迭代顺序即为默认为插入顺序。看LinkedHashMap的构造方法，如： 1234public LinkedHashMap(int initialCapacity, float loadFactor) &#123; super(initialCapacity, loadFactor); accessOrder = false; &#125; 这些构造方法都会默认指定排序模式为插入顺序。如果你想构造一个LinkedHashMap，并打算按从近期访问最少到近期访问最多的顺序（即访问顺序）来保存元素，那么请使用下面的构造方法构造LinkedHashMap： 123456public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) &#123; super(initialCapacity, loadFactor); this.accessOrder = accessOrder; &#125; 该哈希映射的迭代顺序就是最后访问其条目的顺序，这种映射很适合构建LRU缓存。LinkedHashMap提供了removeEldestEntry(Map.Entry eldest)方法，在将新条目插入到映射后，put和 putAll将调用此方法。该方法可以提供在每次添加新条目时移除最旧条目的实现程序，默认返回false，这样，此映射的行为将类似于正常映射，即永远不能移除最旧的元素。 123protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; return false; &#125; 此方法通常不以任何方式修改映射，相反允许映射在其返回值的指引下进行自我修改。如果用此映射构建LRU缓存，则非常方便，它允许映射通过删除旧条目来减少内存损耗。 例如：重写此方法，维持此映射只保存100个条目的稳定状态，在每次添加新条目时删除最旧的条目。 1234private static final int MAX_ENTRIES = 100; protected boolean removeEldestEntry(Map.Entry eldest) &#123; return size() &gt; MAX_ENTRIES; &#125; 四、总结其实LinkedHashMap几乎和HashMap一样：从技术上来说，不同的是它定义了一个Entry header，这个header不是放在Table里，它是额外独立出来的。LinkedHashMap通过继承hashMap中的Entry,并添加两个属性Entry before,after,和header结合起来组成一个双向链表，来实现按插入顺序或访问顺序排序。 在写关于LinkedHashMap的过程中，记起来之前面试的过程中遇到的一个问题，也是问我Map的哪种实现可以做到按照插入顺序进行迭代？当时脑子是突然短路的，但现在想想，也只能怪自己对这个知识点还是掌握的不够扎实，所以又从头认真的把代码看了一遍。 不过，我的建议是，大家首先首先需要记住的是：LinkedHashMap能够做到按照插入顺序或者访问顺序进行迭代，这样在我们以后的开发中遇到相似的问题，才能想到用LinkedHashMap来解决，否则就算对其内部结构非常了解，不去使用也是没有什么用的。我们学习的目的是为了更好的应用。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>LinkedHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（3）：Java HashTable]]></title>
    <url>%2F2017%2F08%2F11%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%883%EF%BC%89%EF%BC%9AJava%20HashTable%2F</url>
    <content type="text"><![CDATA[一、概述和HashMap一样，Hashtable也是一个散列表，它存储的内容是键值对。 Hashtable在Java中的定义为： 123public class Hashtable&lt;K,V&gt; extends Dictionary&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, java.io.Serializable&#123;&#125; 从源码中，我们可以看出，Hashtable继承于Dictionary类，实现了Map, Cloneable, java.io.Serializable接口。其中Dictionary类是任何可将键映射到相应值的类（如 Hashtable）的抽象父类，每个键和值都是对象（源码注释为：The Dictionary class is the abstract parent of any class, such as Hashtable, which maps keys to values. Every key and every value is an object.）。但其Dictionary源码注释是这样的：NOTE: This class is obsolete. New implementations should implement the Map interface, rather than extending this class. 该话指出Dictionary这个类过时了，新的实现类应该实现Map接口。 二、成员变量Hashtable是通过”拉链法”实现的哈希表。它包括几个重要的成员变量：table, count, threshold, loadFactor, modCount。 table是一个Entry[]数组类型，而Entry（在HashMap中有讲解过）实际上就是一个单向链表。哈希表的”key-value键值对”都是存储在Entry数组中的。 count是Hashtable的大小，它是Hashtable保存的键值对的数量。 threshold是Hashtable的阈值，用于判断是否需要调整Hashtable的容量。threshold的值=”容量*加载因子”。 loadFactor就是加载因子。 modCount是用来实现fail-fast机制的。 变量的解释在源码注释中如下： 123456789101112131415161718192021222324252627282930313233/** * The hash table data. */ private transient Entry&lt;K,V&gt;[] table; /** * The total number of entries in the hash table. */ private transient int count; /** * The table is rehashed when its size exceeds this threshold. (The * value of this field is (int)(capacity * loadFactor).) * * @serial */ private int threshold; /** * The load factor for the hashtable. * * @serial */ private float loadFactor; /** * The number of times this Hashtable has been structurally modified * Structural modifications are those that change the number of entries in * the Hashtable or otherwise modify its internal structure (e.g., * rehash). This field is used to make iterators on Collection-views of * the Hashtable fail-fast. (See ConcurrentModificationException). */ private transient int modCount = 0; 三、构造方法Hashtable一共提供了4个构造方法： public Hashtable(int initialCapacity, float loadFactor)： 用指定初始容量和指定加载因子构造一个新的空哈希表。useAltHashing为boolean，其如果为真，则执行另一散列的字符串键，以减少由于弱哈希计算导致的哈希冲突的发生。 public Hashtable(int initialCapacity)：用指定初始容量和默认的加载因子 (0.75) 构造一个新的空哈希表。 public Hashtable()：默认构造函数，容量为11，加载因子为0.75。 public Hashtable(Map&lt;? extends K, ? extends V&gt; t)：构造一个与给定的 Map 具有相同映射关系的新哈希表。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * Constructs a new, empty hashtable with the specified initial * capacity and the specified load factor. * * @param initialCapacity the initial capacity of the hashtable. * @param loadFactor the load factor of the hashtable. * @exception IllegalArgumentException if the initial capacity is less * than zero, or if the load factor is nonpositive. */ public Hashtable(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal Load: &quot;+loadFactor); if (initialCapacity==0) initialCapacity = 1; this.loadFactor = loadFactor; table = new Entry[initialCapacity]; threshold = (int)Math.min(initialCapacity * loadFactor, MAX_ARRAY_SIZE + 1); useAltHashing = sun.misc.VM.isBooted() &amp;&amp; (initialCapacity &gt;= Holder.ALTERNATIVE_HASHING_THRESHOLD); &#125; /** * Constructs a new, empty hashtable with the specified initial capacity * and default load factor (0.75). * * @param initialCapacity the initial capacity of the hashtable. * @exception IllegalArgumentException if the initial capacity is less * than zero. */ public Hashtable(int initialCapacity) &#123; this(initialCapacity, 0.75f); &#125; /** * Constructs a new, empty hashtable with a default initial capacity (11) * and load factor (0.75). */ public Hashtable() &#123; this(11, 0.75f); &#125; /** * Constructs a new hashtable with the same mappings as the given * Map. The hashtable is created with an initial capacity sufficient to * hold the mappings in the given Map and a default load factor (0.75). * * @param t the map whose mappings are to be placed in this map. * @throws NullPointerException if the specified map is null. * @since 1.2 */ public Hashtable(Map&lt;? extends K, ? extends V&gt; t) &#123; this(Math.max(2*t.size(), 11), 0.75f); putAll(t); &#125; 四、put方法put方法的整个流程为： 判断value是否为空，为空则抛出异常； 计算key的hash值，并根据hash值获得key在table数组中的位置index，如果table[index]元素不为空，则进行迭代，如果遇到相同的key，则直接替换，并返回旧value； 否则，我们可以将其插入到table[index]位置。 123456789101112131415161718192021222324252627282930313233343536373839public synchronized V put(K key, V value) &#123; // Make sure the value is not null确保value不为null if (value == null) &#123; throw new NullPointerException(); &#125; // Makes sure the key is not already in the hashtable. //确保key不在hashtable中 //首先，通过hash方法计算key的哈希值，并计算得出index值，确定其在table[]中的位置 //其次，迭代index索引位置的链表，如果该位置处的链表存在相同的key，则替换value，返回旧的value Entry tab[] = table; int hash = hash(key); int index = (hash &amp; 0x7FFFFFFF) % tab.length; for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; V old = e.value; e.value = value; return old; &#125; &#125; modCount++; if (count &gt;= threshold) &#123; // Rehash the table if the threshold is exceeded //如果超过阀值，就进行rehash操作 rehash(); tab = table; hash = hash(key); index = (hash &amp; 0x7FFFFFFF) % tab.length; &#125; // Creates the new entry. //将值插入，返回的为null Entry&lt;K,V&gt; e = tab[index]; // 创建新的Entry节点，并将新的Entry插入Hashtable的index位置，并设置e为新的Entry的下一个元素 tab[index] = new Entry&lt;&gt;(hash, key, value, e); count++; return null; &#125; 通过一个实际的例子来演示一下这个过程：假设我们现在Hashtable的容量为5，已经存在了(5,5)，(13,13)，(16,16)，(17,17)，(21,21)这5个键值对，目前他们在Hashtable中的位置如下： 现在，我们插入一个新的键值对，put(16,22)，假设key=16的索引为1.但现在索引1的位置有两个Entry了，所以程序会对链表进行迭代。迭代的过程中，发现其中有一个Entry的key和我们要插入的键值对的key相同，所以现在会做的工作就是将newValue=22替换oldValue=16，然后返回oldValue=16. 然后我们现在再插入一个，put(33,33)，key=33的索引为3，并且在链表中也不存在key=33的Entry，所以将该节点插入链表的第一个位置。 五、get方法相比较于put方法，get方法则简单很多。其过程就是首先通过hash()方法求得key的哈希值，然后根据hash值得到index索引（上述两步所用的算法与put方法都相同）。然后迭代链表，返回匹配的key的对应的value；找不到则返回null。 1234567891011public synchronized V get(Object key) &#123; Entry tab[] = table; int hash = hash(key); int index = (hash &amp; 0x7FFFFFFF) % tab.length; for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; return e.value; &#125; &#125; return null; &#125; 六、遍历方式Hashtable有多种遍历方式： 1234567891011121314151617181920212223//1、使用keys()Enumeration&lt;String&gt; en1 = table.keys(); while(en1.hasMoreElements()) &#123; en1.nextElement();&#125;//2、使用elements()Enumeration&lt;String&gt; en2 = table.elements(); while(en2.hasMoreElements()) &#123; en2.nextElement();&#125;//3、使用keySet()Iterator&lt;String&gt; it1 = table.keySet().iterator(); while(it1.hasNext()) &#123; it1.next();&#125;//4、使用entrySet()Iterator&lt;Entry&lt;String, String&gt;&gt; it2 = table.entrySet().iterator(); while(it2.hasNext()) &#123; it2.next();&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>HashTable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（2）：Java HashSet]]></title>
    <url>%2F2017%2F08%2F10%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%882%EF%BC%89%EF%BC%9AJava%20HashSet%2F</url>
    <content type="text"><![CDATA[一、概述 This class implements the Set interface, backed by a hash table (actually a HashMap instance). It makes no guarantees as to the iteration order of the set; in particular, it does not guarantee that the order will remain constant over time. This class permits the null element. HashSet实现Set接口，由哈希表（实际上是一个HashMap实例）支持。它不保证set 的迭代顺序；特别是它不保证该顺序恒久不变。此类允许使用null元素。 HashSet是基于HashMap来实现的，操作很简单，更像是对HashMap做了一次“封装”，而且只使用了HashMap的key来实现各种特性，我们先来感性的认识一下这个结构： 123456789HashSet&lt;String&gt; set = new HashSet&lt;String&gt;();set.add(&quot;语文&quot;);set.add(&quot;数学&quot;);set.add(&quot;英语&quot;);set.add(&quot;历史&quot;);set.add(&quot;政治&quot;);set.add(&quot;地理&quot;);set.add(&quot;生物&quot;);set.add(&quot;化学&quot;); 通过HashSet最简单的构造函数和几个成员变量来看一下，证明咱们上边说的，其底层是HashMap： 123456789101112private transient HashMap&lt;E,Object&gt; map;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object();/** * Constructs a new, empty set; the backing &lt;tt&gt;HashMap&lt;/tt&gt; instance has * default initial capacity (16) and load factor (0.75). */public HashSet() &#123; map = new HashMap&lt;&gt;();&#125; 其实在英文注释中已经说的比较明确了。首先有一个HashMap的成员变量，我们在HashSet的构造函数中将其初始化，默认情况下采用的是initial capacity为16，load factor为0.75。 二、HashSet的实现 对于HashSet而言，它是基于HashMap实现的，HashSet底层使用HashMap来保存所有元素，因此HashSet 的实现比较简单，相关HashSet的操作，基本上都是直接调用底层HashMap的相关方法来完成，只不过HashSet里面的HashMap所有的value都是同一个Object而已。HashSet的源代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169public class HashSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable &#123; static final long serialVersionUID = -5024744406713321676L; // 底层使用HashMap来保存HashSet中所有元素。 private transient HashMap&lt;E,Object&gt; map; // 定义一个虚拟的Object对象作为HashMap的value，将此对象定义为static final。 private static final Object PRESENT = new Object(); /** * 默认的无参构造器，构造一个空的HashSet。 * * 实际底层会初始化一个空的HashMap，并使用默认初始容量为16和加载因子0.75。 */ public HashSet() &#123; map = new HashMap&lt;E,Object&gt;(); &#125; /** * 构造一个包含指定collection中的元素的新set。 * * 实际底层使用默认的加载因子0.75和足以包含指定 * collection中所有元素的初始容量来创建一个HashMap。 * @param c 其中的元素将存放在此set中的collection。 */ public HashSet(Collection&lt;? extends E&gt; c) &#123; map = new HashMap&lt;E,Object&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); addAll(c); &#125; /** * 以指定的initialCapacity和loadFactor构造一个空的HashSet。 * * 实际底层以相应的参数构造一个空的HashMap。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 */ public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;E,Object&gt;(initialCapacity, loadFactor); &#125; /** * 以指定的initialCapacity构造一个空的HashSet。 * * 实际底层以相应的参数及加载因子loadFactor为0.75构造一个空的HashMap。 * @param initialCapacity 初始容量。 */ public HashSet(int initialCapacity) &#123; map = new HashMap&lt;E,Object&gt;(initialCapacity); &#125; /** * 以指定的initialCapacity和loadFactor构造一个新的空链接哈希集合。 * 此构造函数为包访问权限，不对外公开，实际只是是对LinkedHashSet的支持。 * * 实际底层会以指定的参数构造一个空LinkedHashMap实例来实现。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 * @param dummy 标记。 */ HashSet(int initialCapacity, float loadFactor, boolean dummy) &#123; map = new LinkedHashMap&lt;E,Object&gt;(initialCapacity, loadFactor); &#125; /** * 返回对此set中元素进行迭代的迭代器。返回元素的顺序并不是特定的。 * * 底层实际调用底层HashMap的keySet来返回所有的key。 * 可见HashSet中的元素，只是存放在了底层HashMap的key上， * value使用一个static final的Object对象标识。 * @return 对此set中元素进行迭代的Iterator。 */ public Iterator&lt;E&gt; iterator() &#123; return map.keySet().iterator(); &#125; /** * 返回此set中的元素的数量（set的容量）。 * * 底层实际调用HashMap的size()方法返回Entry的数量，就得到该Set中元素的个数。 * @return 此set中的元素的数量（set的容量）。 */ public int size() &#123; return map.size(); &#125; /** * 如果此set不包含任何元素，则返回true。 * * 底层实际调用HashMap的isEmpty()判断该HashSet是否为空。 * @return 如果此set不包含任何元素，则返回true。 */ public boolean isEmpty() &#123; return map.isEmpty(); &#125; /** * 如果此set包含指定元素，则返回true。 * 更确切地讲，当且仅当此set包含一个满足(o==null ? e==null : o.equals(e)) * 的e元素时，返回true。 * * 底层实际调用HashMap的containsKey判断是否包含指定key。 * @param o 在此set中的存在已得到测试的元素。 * @return 如果此set包含指定元素，则返回true。 */ public boolean contains(Object o) &#123; return map.containsKey(o); &#125; /** * 如果此set中尚未包含指定元素，则添加指定元素。 * 更确切地讲，如果此 set 没有包含满足(e==null ? e2==null : e.equals(e2)) * 的元素e2，则向此set 添加指定的元素e。 * 如果此set已包含该元素，则该调用不更改set并返回false。 * * 底层实际将将该元素作为key放入HashMap。 * 由于HashMap的put()方法添加key-value对时，当新放入HashMap的Entry中key * 与集合中原有Entry的key相同（hashCode()返回值相等，通过equals比较也返回true）， * 新添加的Entry的value会将覆盖原来Entry的value，但key不会有任何改变， * 因此如果向HashSet中添加一个已经存在的元素时，新添加的集合元素将不会被放入HashMap中， * 原来的元素也不会有任何改变，这也就满足了Set中元素不重复的特性。 * @param e 将添加到此set中的元素。 * @return 如果此set尚未包含指定元素，则返回true。 */ public boolean add(E e) &#123; return map.put(e, PRESENT)==null; &#125; /** * 如果指定元素存在于此set中，则将其移除。 * 更确切地讲，如果此set包含一个满足(o==null ? e==null : o.equals(e))的元素e， * 则将其移除。如果此set已包含该元素，则返回true * （或者：如果此set因调用而发生更改，则返回true）。（一旦调用返回，则此set不再包含该元素）。 * * 底层实际调用HashMap的remove方法删除指定Entry。 * @param o 如果存在于此set中则需要将其移除的对象。 * @return 如果set包含指定元素，则返回true。 */ public boolean remove(Object o) &#123; return map.remove(o)==PRESENT; &#125; /** * 从此set中移除所有元素。此调用返回后，该set将为空。 * * 底层实际调用HashMap的clear方法清空Entry中所有元素。 */ public void clear() &#123; map.clear(); &#125; /** * 返回此HashSet实例的浅表副本：并没有复制这些元素本身。 * * 底层实际调用HashMap的clone()方法，获取HashMap的浅表副本，并设置到HashSet中。 */ public Object clone() &#123; try &#123; HashSet&lt;E&gt; newSet = (HashSet&lt;E&gt;) super.clone(); newSet.map = (HashMap&lt;E, Object&gt;) map.clone(); return newSet; &#125; catch (CloneNotSupportedException e) &#123; throw new InternalError(); &#125; &#125; &#125; 对于HashSet中保存的对象，请注意正确重写其equals和hashCode方法，以保证放入的对象的唯一性。这两个方法是比较重要的，希望大家在以后的开发过程中需要注意一下。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>HashSet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合学习手册（1）：Java HashMap]]></title>
    <url>%2F2017%2F08%2F09%2FJava%E9%9B%86%E5%90%88%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%EF%BC%881%EF%BC%89%EF%BC%9AJava%20HashMap%2F</url>
    <content type="text"><![CDATA[一、概述从本文你可以学习到： 什么时候会使用HashMap？他有什么特点？ 你知道HashMap的工作原理吗？ 你知道get和put的原理吗？equals()和hashCode()的都有什么作用？ 你知道hash的实现吗？为什么要这样实现？ 如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？ 当我们执行下面的操作时： 123456789101112HashMap&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;();map.put(&quot;语文&quot;, 1);map.put(&quot;数学&quot;, 2);map.put(&quot;英语&quot;, 3);map.put(&quot;历史&quot;, 4);map.put(&quot;政治&quot;, 5);map.put(&quot;地理&quot;, 6);map.put(&quot;生物&quot;, 7);map.put(&quot;化学&quot;, 8);for(Entry&lt;String, Integer&gt; entry : map.entrySet()) &#123; System.out.println(entry.getKey() + &quot;: &quot; + entry.getValue());&#125; 运行结果是： 12345678政治: 5生物: 7历史: 4数学: 2化学: 8语文: 1英语: 3地理: 6 发生了什么呢？下面是一个大致的结构，希望我们对HashMap的结构有一个感性的认识： 在官方文档中是这样描述HashMap的：&gt;Hash table based implementation of the Map interface. This implementation provides all of the optional map operations, and permits null values and the null key. (The HashMap class is roughly equivalent to Hashtable, except that it is unsynchronized and permits nulls.) This class makes no guarantees as to the order of the map; in particular, it does not guarantee that the order will remain constant over time. 几个关键的信息：基于Map接口实现、允许null键/值、非同步、不保证有序(比如插入的顺序)、也不保证序不随时间变化。 在HashMap中有两个很重要的参数，容量(Capacity)和负载因子(Load factor) Initial capacity： The capacity is the number of buckets in the hash table, The initial capacity is simply the capacity at the time the hash table is created. Load factor： The load factor is a measure of how full the hash table is allowed to get before its capacity is automatically increased. 简单的说，Capacity就是bucket的大小，Load factor就是bucket填满程度的最大比例。如果对迭代性能要求很高的话不要把capacity设置过大，也不要把load factor设置过小。当bucket中的entries的数目大于capacity*load factor时就需要调整bucket的大小为当前的2倍。 二、构造函数HashMap底层维护一个数组，当新建一个HashMap的时候，就会初始化一个数组。我们看一下JDK源码中的HashMap构造函数： 12345678910111213141516171819202122public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); // Find a power of 2 &gt;= initialCapacity int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; this.loadFactor = loadFactor; threshold = (int)Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); table = new Entry[capacity]; useAltHashing = sun.misc.VM.isBooted() &amp;&amp; (capacity &gt;= Holder.ALTERNATIVE_HASHING_THRESHOLD); init();&#125; 可以看到其中一行为table = new Entry[capacity];。在构造函数中，其创建了一个Entry的数组，其大小为capacity，那么Entry又是什么结构呢？看一下源码： 12345678transient Entry&lt;K,V&gt;[] table;static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; Entry&lt;K,V&gt; next; final int hash; ……&#125; HashMap中的是通过transient Entry[] table来存储数据，该变量是通过transient进行修饰的。 Entry是一个static class，其中包含了key和value，也就是键值对，另外还包含了一个next的Entry指针。我们可以总结出：Entry就是数组中的元素，每个Entry其实就是一个key-value对，它持有一个指向下一个元素的引用，这就构成了链表。 三、put()方法和get()方法3.1 put()方法put函数大致的思路为： 对key的hashCode()做hash，然后再计算index; 如果没碰撞直接放到bucket里； 如果碰撞了，以链表的形式存在buckets后； 如果碰撞导致链表过长(大于等于TREEIFY_THRESHOLD)，就把链表转换成红黑树； 如果节点已经存在就替换old value(保证key的唯一性) 如果bucket满了(超过load factor*current capacity)，就要resize。 具体代码的实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //如果当前map中无数据，执行resize方法。并且返回n if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; ///如果要插入的键值对要存放的这个位置刚好没有元素，那么把他封装成Node对象，放在这个位置上就完事了 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); //否则的话，说明这上面有元素 else &#123; Node&lt;K,V&gt; e; K k; //如果这个元素的key与要插入的一样，那么就替换一下，也完事。 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //1.如果当前节点是TreeNode类型的数据，执行putTreeVal方法 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); //还是遍历这条链子上的数据，跟jdk7没什么区别 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); //2.完成了操作后多做了一件事情，判断，并且可能执行treeifyBin方法，treeifyBin()就是将链表转换成红黑树。 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 写入 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; //判断阈值，决定是否扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 一直到JDK7为止，HashMap的结构基于一个数组以及多个链表的实现，hash值冲突的时候，就将对应节点以链表的形式存储。 这样子的HashMap性能上就抱有一定疑问，如果说成百上千个节点在hash时发生碰撞，存储一个链表中，那么如果要查找其中一个节点，那就不可避免的花费O(N)的查找时间，这将是多么大的性能损失。这个问题终于在JDK8中得到了解决。再最坏的情况下，链表查找的时间复杂度为O(n),而红黑树一直是O(logn),这样会提高HashMap的效率。JDK7中HashMap采用的是位桶+链表的方式，即我们常说的散列链表的方式，而JDK8中采用的是位桶+链表/红黑树（有关红黑树请查看红黑树）的方式，也是非线程安全的。当某个位桶的链表的长度达到某个阀值的时候，这个链表就将转换成红黑树。 JDK8中，当同一个hash值的节点数不小于8时，将不再以单链表的形式存储了，会被调整成一颗红黑树。这就是JDK7与JDK8中HashMap实现的最大区别。JDK中Entry的名字变成了Node，原因是和红黑树的实现TreeNode相关联。 1transient Node&lt;K,V&gt;[] table; 当冲突节点数不小于8-1时，转换成红黑树。 1static final int TREEIFY_THRESHOLD = 8; 总结下put的过程： 当程序试图将一个key-value对放入HashMap中时，程序首先根据该 key 的 hashCode() 返回值决定该 Entry 的存储位置， 该位置就是此对象准备往数组中存放的位置。 如果该位置没有对象存在，就将此对象直接放进数组当中；如果该位置已经有对象存在了，则顺着此存在的对象的链开始寻找(为了判断是否是否值相同，map不允许键值对重复)， 使用 equals方法进行比较，如果对此链上的 key 通过 equals 比较有一个返回 true，新添加 Entry 的 value 将覆盖集合中原有 Entry 的 value，但key不会覆盖；如果对此链上的每个对象的 equals 方法比较都为 false，则将该对象放到数组当中，然后将数组中该位置以前存在的那个对象链接到此对象的后面，即新值存放在数组中，旧值在新值的链表上。 3.2 get()方法在理解了put之后，get就很简单了。大致思路如下： bucket里的第一个节点，直接命中； 如果有冲突，则通过key.equals(k)去查找对应的entry 若为树，则在树中通过key.equals(k)查找，O(logn)； 若为链表，则在链表中通过key.equals(k)查找，O(n)。 具体代码的实现如下： 123456789101112131415161718192021222324252627public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 直接命中 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 未命中 if ((e = first.next) != null) &#123; // 在树中get if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 在链表中get do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 3.3 null key存取对于put方法来说，HashMap会对null值key进行特殊处理，总是放到table[0]位置对于get方法来说，同样当key为null时会进行特殊处理，在table[0]的链表上查找key为null的元素 12345678910111213private V putForNullKey(V value) &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(0, null, value, 0); return null; &#125; 四、hash()与indexFor()4.1 hash()方法在get和put的过程中，计算下标时，先对hashCode进行hash操作，然后再通过hash值进一步计算下标，如下图所示： 在对hashCode()计算hash时具体实现是这样的： 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 按位取并，作用上相当于取模mod或者取余%。这意味着数组下标相同，并不表示hashCode相同。 可以看到这个函数大概的作用就是：高16bit不变，低16bit和高16bit做了一个异或。其中代码注释是这样写的： &gt;Computes key.hashCode() and spreads (XORs) higher bits of hash to lower. Because the table uses power-of-two masking, sets of hashes that vary only in bits above the current mask will always collide. (Among known examples are sets of Float keys holding consecutive whole numbers in small tables.) So we apply a transform that spreads the impact of higher bits downward. There is a tradeoff between speed, utility, and quality of bit-spreading. Because many common sets of hashes are already reasonably distributed (so don’t benefit from spreading), and because we use trees to handle large sets of collisions in bins, we just XOR some shifted bits in the cheapest possible way to reduce systematic lossage, as well as to incorporate impact of the highest bits that would otherwise never be used in index calculations because of table bounds. 4.2 indexFor()方法在设计hash函数时，因为目前的table长度length n为2的幂，而计算下标的时候，是这样实现的(使用&amp;位操作，而非%求余)： 123static int indexFor(int h, int n) &#123; return h &amp; (n-1); &#125; 设计者认为这方法很容易发生碰撞。为什么这么说呢？不妨思考一下，在n - 1为15(0x1111)时，其实散列真正生效的只是低4bit的有效位，当然容易碰撞了。 因此，设计者想了一个顾全大局的方法(综合考虑了速度、作用、质量)，就是把高16bit和低16bit异或了一下。设计者还解释到因为现在大多数的hashCode的分布已经很不错了，就算是发生了碰撞也用O(logn)的tree去做了。仅仅异或一下，既减少了系统的开销，也不会造成的因为高位没有参与下标的计算(table长度比较小时)，从而引起的碰撞。 如果还是产生了频繁的碰撞，会发生什么问题呢？作者注释说，他们使用树来处理频繁的碰撞(we use trees to handle large sets of collisions in bins)，在JEP-180中，描述了这个问题：&gt;Improve the performance of java.util.HashMap under high hash-collision conditions by using balanced trees rather than linked lists to store map entries. Implement the same improvement in the LinkedHashMap class. 之前已经提过，在获取HashMap的元素时，基本分两步： 首先根据hashCode()做hash，然后确定bucket的index； 如果bucket的节点的key不是我们需要的，则通过keys.equals()在链中找。 在Java 8之前的实现中是用链表解决冲突的，在产生碰撞的情况下，进行get时，两步的时间复杂度是O(1)+O(n)。因此，当碰撞很厉害的时候n很大，O(n)的速度显然是影响速度的。 因此在Java 8中，利用红黑树替换链表，这样复杂度就变成了O(1)+O(logn)了，这样在n很大的时候，能够比较理想的解决这个问题，在Java 8：HashMap的性能提升一文中有性能测试的结果。 五、resize()方法当put时，如果发现目前的bucket占用程度已经超过了Load Factor所希望的比例，那么就会发生resize。在resize的过程，简单的说就是把bucket扩充为2倍，之后重新计算index，把节点再放到新的bucket中。resize的注释是这样描述的： &gt;Initializes or doubles table size. If null, allocates in accord with initial capacity target held in field threshold. Otherwise, because we are using power-of-two expansion, the elements from each bin must either stay at same index, or move with a power of two offset in the new table. 大致意思就是说，当超过限制的时候会resize，然而又因为我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。 怎么理解呢？例如我们从16扩展为32时，具体的变化如下所示： 因此元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。 下面是代码的具体实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 六、remove()、clear()、containsKey()和containsValue()6.1 remove()方法remove方法和put get类似，计算hash，计算index，然后遍历查找，将找到的元素从table[index]链表移除123456789101112131415161718192021222324252627282930public V remove(Object key) &#123; Entry&lt;K,V&gt; e = removeEntryForKey(key); return (e == null ? null : e.value); &#125; final Entry&lt;K,V&gt; removeEntryForKey(Object key) &#123; int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; e = prev; while (e != null) &#123; Entry&lt;K,V&gt; next = e.next; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; modCount++; size--; if (prev == e) table[i] = next; else prev.next = next; e.recordRemoval(this); return e; &#125; prev = e; e = next; &#125; return e; &#125; 6.2 clear()方法clear方法非常简单，就是遍历table然后把每个位置置为null，同时修改元素个数为0需要注意的是clear方法只会清楚里面的元素，并不会重置capactiy 1234567public void clear() &#123; modCount++; Entry[] tab = table; for (int i = 0; i &lt; tab.length; i++) tab[i] = null; size = 0; &#125; 6.3 containsKey()方法containsKey方法是先计算hash然后使用hash和table.length取摸得到index值，遍历table[index]元素查找是否包含key相同的值 123456789101112131415public boolean containsKey(Object key) &#123; return getEntry(key) != null; &#125;final Entry&lt;K,V&gt; getEntry(Object key) &#123; int hash = (key == null) ? 0 : hash(key.hashCode()); for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; return null; &#125; 6.4 containsValue()方法containsValue方法就比较粗暴了，就是直接遍历所有元素直到找到value，由此可见HashMap的containsValue方法本质上和普通数组和list的contains方法没什么区别，你别指望它会像containsKey那么高效 1234567891011public boolean containsValue(Object value) &#123; if (value == null) return containsNullValue(); Entry[] tab = table; for (int i = 0; i &lt; tab.length ; i++) for (Entry e = tab[i] ; e != null ; e = e.next) if (value.equals(e.value)) return true; return false; &#125; 6.5 Fail-Fast机制我们知道java.util.HashMap不是线程安全的，因此如果在使用迭代器的过程中有其他线程修改了map，那么将抛出ConcurrentModificationException，这就是所谓fail-fast策略。 fail-fast 机制是java集合(Collection)中的一种错误机制。 当多个线程对同一个集合的内容进行操作时，就可能会产生 fail-fast 事件。 例如：当某一个线程A通过 iterator去遍历某集合的过程中，若该集合的内容被其他线程所改变了；那么线程A访问集合时，就会抛出 ConcurrentModificationException异常，产生 fail-fast 事件。 这一策略在源码中的实现是通过modCount域，modCount顾名思义就是修改次数，对HashMap内容（当然不仅仅是HashMap才会有，其他例如ArrayList也会）的修改都将增加这个值（大家可以再回头看一下其源码，在很多操作中都有modCount++这句），那么在迭代器初始化过程中会将这个值赋给迭代器的expectedModCount。 12345678HashIterator() &#123; expectedModCount = modCount; if (size &gt; 0) &#123; // advance to first entry Entry[] t = table; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) ; &#125;&#125; 在迭代过程中，判断modCount跟expectedModCount是否相等，如果不相等就表示已经有其他线程修改了Map： 注意到modCount声明为volatile，保证线程之间修改的可见性。 123final Entry&lt;K,V&gt; nextEntry() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); 在HashMap的API中指出： 由所有HashMap类的“collection 视图方法”所返回的迭代器都是快速失败的：在迭代器创建之后，如果从结构上对映射进行修改，除非通过迭代器本身的 remove 方法，其他任何时间任何方式的修改，迭代器都将抛出 ConcurrentModificationException。因此，面对并发的修改，迭代器很快就会完全失败，而不冒在将来不确定的时间发生任意不确定行为的风险。 注意，迭代器的快速失败行为不能得到保证，一般来说，存在非同步的并发修改时，不可能作出任何坚决的保证。快速失败迭代器尽最大努力抛出ConcurrentModificationException。因此，编写依赖于此异常的程序的做法是错误的，正确做法是：迭代器的快速失败行为应该仅用于检测程序错误。 在上文中也提到，fail-fast机制，是一种错误检测机制。它只能被用来检测错误，因为JDK并不保证fail-fast机制一定会发生。若在多线程环境下使用 fail-fast机制的集合，建议使用“java.util.concurrent包下的类”去取代“java.util包下的类”。 6.6 两种遍历方式第一种效率高,以后一定要使用此种方式！ 1234567 Map map = new HashMap(); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); Object key = entry.getKey(); Object val = entry.getValue(); &#125; 第二种效率低,以后尽量少使用！ 123456 Map map = new HashMap(); Iterator iter = map.keySet().iterator(); while (iter.hasNext()) &#123; Object key = iter.next(); Object val = map.get(key); &#125; 七、一些问题我们现在可以回答开始的几个问题，加深对HashMap的理解： 什么时候会使用HashMap？他有什么特点？是基于Map接口的实现，存储键值对时，它可以接收null的键值，是非同步的，HashMap存储着Entry(hash, key, value, next)对象。 你知道HashMap的工作原理吗？通过hash的方法，通过put和get存储和获取对象。存储对象时，我们将K/V传给put方法时，它调用hashCode计算hash从而得到bucket位置，进一步存储，HashMap会根据当前bucket的占用情况自动调整容量(超过Load Facotr则resize为原来的2倍)。获取对象时，我们将K传给get，它调用hashCode计算hash从而得到bucket位置，并进一步调用equals()方法确定键值对。如果发生碰撞的时候，Hashmap通过链表将产生碰撞冲突的元素组织起来，在Java 8中，如果一个bucket中碰撞冲突的元素超过某个限制(默认是8)，则使用红黑树来替换链表，从而提高速度。 你知道get和put的原理吗？equals()和hashCode()的都有什么作用？通过对key的hashCode()进行hashing，并计算下标( n-1 &amp; hash)，从而获得buckets的位置。如果产生碰撞，则利用key.equals()方法去链表或树中去查找对应的节点。 你知道hash的实现吗？为什么要这样实现？在Java 1.8的实现中，是通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在bucket的n比较小的时候，也能保证考虑到高低bit都参与到hash的计算中，同时不会有太大的开销。 如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？如果超过了负载因子(默认0.75)，则会重新resize一个原来长度两倍的HashMap，并且重新调用hash方法。 Java集合小抄以Entry[]数组实现的哈希桶数组，用Key的哈希值取模桶数组的大小可得到数组下标。插入元素时，如果两条Key落在同一个桶(比如哈希值1和17取模16后都属于第一个哈希桶)，Entry用一个next属性实现多个Entry以单向链表存放，后入桶的Entry将next指向桶当前的Entry。查找哈希值为17的key时，先定位到第一个哈希桶，然后以链表遍历桶里所有元素，逐个比较其key值。当Entry数量达到桶数量的75%时(很多文章说使用的桶数量达到了75%，但看代码不是)，会成倍扩容桶数组，并重新分配所有原来的Entry，所以这里也最好有个预估值。取模用位运算(hash &amp; (arrayLength-1))会比较快，所以数组的大小永远是2的N次方， 你随便给一个初始值比如17会转为32。默认第一次放入元素时的初始值是16。iterator()时顺着哈希桶数组来遍历，看起来是个乱序。在JDK8里，新增默认为8的閥值，当一个桶里的Entry超过閥值，就不以单向链表而以红黑树来存放以加快Key的查找速度。 一个很棒的面试题 HashMap的工作原理是近年来常见的Java面试题。几乎每个Java程序员都知道HashMap，都知道哪里要用HashMap，知道Hashtable和HashMap之间的区别，那么为何这道面试题如此特殊呢？是因为这道题考察的深度很深。这题经常出现在高级或中高级面试中。投资银行更喜欢问这个问题，甚至会要求你实现HashMap来考察你的编程能力。ConcurrentHashMap和其它同步集合的引入让这道题变得更加复杂。让我们开始探索的旅程吧！ 先来些简单的问题 “你用过HashMap吗？” “什么是HashMap？你为什么用到它？” 几乎每个人都会回答“是的”，然后回答HashMap的一些特性，譬如HashMap可以接受null键值和值，而Hashtable则不能；HashMap是非synchronized;HashMap很快；以及HashMap储存的是键值对等等。这显示出你已经用过HashMap，而且对它相当的熟悉。但是面试官来个急转直下，从此刻开始问出一些刁钻的问题，关于HashMap的更多基础的细节。面试官可能会问出下面的问题： “你知道HashMap的工作原理吗？” “你知道HashMap的get()方法的工作原理吗？” 你也许会回答“我没有详查标准的Java API，你可以看看Java源代码或者Open JDK。”“我可以用Google找到答案。” 但一些面试者可能可以给出答案，“HashMap是基于hashing的原理，我们使用put(key, value)存储对象到HashMap中，使用get(key)从HashMap中获取对象。当我们给put()方法传递键和值时，我们先对键调用hashCode()方法，返回的hashCode用于找到bucket位置来储存Entry对象。”这里关键点在于指出，HashMap是在bucket中储存键对象和值对象，作为Map.Entry。这一点有助于理解获取对象的逻辑。如果你没有意识到这一点，或者错误的认为仅仅只在bucket中存储值的话，你将不会回答如何从HashMap中获取对象的逻辑。这个答案相当的正确，也显示出面试者确实知道hashing以及HashMap的工作原理。但是这仅仅是故事的开始，当面试官加入一些Java程序员每天要碰到的实际场景的时候，错误的答案频现。下个问题可能是关于HashMap中的碰撞探测(collision detection)以及碰撞的解决方法： “当两个对象的hashcode相同会发生什么？” 从这里开始，真正的困惑开始了，一些面试者会回答因为hashcode相同，所以两个对象是相等的，HashMap将会抛出异常，或者不会存储它们。然后面试官可能会提醒他们有equals()和hashCode()两个方法，并告诉他们两个对象就算hashcode相同，但是它们可能并不相等。一些面试者可能就此放弃，而另外一些还能继续挺进，他们回答“因为hashcode相同，所以它们的bucket位置相同，‘碰撞’会发生。因为HashMap使用链表存储对象，这个Entry(包含有键值对的Map.Entry对象)会存储在链表中。”这个答案非常的合理，虽然有很多种处理碰撞的方法，这种方法是最简单的，也正是HashMap的处理方法。但故事还没有完结，面试官会继续问： “如果两个键的hashcode相同，你如何获取值对象？” 面试者会回答：当我们调用get()方法，HashMap会使用键对象的hashcode找到bucket位置，然后获取值对象。面试官提醒他如果有两个值对象储存在同一个bucket，他给出答案:将会遍历链表直到找到值对象。面试官会问因为你并没有值对象去比较，你是如何确定确定找到值对象的？除非面试者直到HashMap在链表中存储的是键值对，否则他们不可能回答出这一题。 其中一些记得这个重要知识点的面试者会说，找到bucket位置之后，会调用keys.equals()方法去找到链表中正确的节点，最终找到要找的值对象。完美的答案！ 许多情况下，面试者会在这个环节中出错，因为他们混淆了hashCode()和equals()方法。因为在此之前hashCode()屡屡出现，而equals()方法仅仅在获取值对象的时候才出现。一些优秀的开发者会指出使用不可变的、声明作final的对象，并且采用合适的equals()和hashCode()方法的话，将会减少碰撞的发生，提高效率。不可变性使得能够缓存不同键的hashcode，这将提高整个获取对象的速度，使用String，Interger这样的wrapper类作为键是非常好的选择。 如果你认为到这里已经完结了，那么听到下面这个问题的时候，你会大吃一惊。“如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？”除非你真正知道HashMap的工作原理，否则你将回答不出这道题。默认的负载因子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其它集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程叫作rehashing，因为它调用hash方法找到新的bucket位置。 如果你能够回答这道问题，下面的问题来了：“你了解重新调整HashMap大小存在什么问题吗？”你可能回答不上来，这时面试官会提醒你当多线程的情况下，可能产生条件竞争(race condition)。 当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。这个时候，你可以质问面试官，为什么这么奇怪，要在多线程的环境下使用HashMap呢？：） 热心的读者贡献了更多的关于HashMap的问题： 为什么String, Interger这样的wrapper类适合作为键？ String, Interger这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的，也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点。不可变性是必要的，因为为了要计算hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性还有其他的优点如线程安全。如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法是非常重要的。如果两个不相等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。 我们可以使用自定义的对象作为键吗？ 这是前一个问题的延伸。当然你可能使用任何对象作为键，只要它遵守了equals()和hashCode()方法的定义规则，并且当对象插入到Map中之后将不会再改变了。如果这个自定义对象时不可变的，那么它已经满足了作为键的条件，因为当它创建之后就已经不能改变了。 我们可以使用CocurrentHashMap来代替Hashtable吗？这是另外一个很热门的面试题，因为ConcurrentHashMap越来越多人用了。我们知道Hashtable是synchronized的，但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。看看这篇博客查看Hashtable和ConcurrentHashMap的区别。 我个人很喜欢这个问题，因为这个问题的深度和广度，也不直接的涉及到不同的概念。让我们再来看看这些问题设计哪些知识点： hashing的概念 HashMap中解决碰撞的方法 equals()和hashCode()的应用，以及它们在HashMap中的重要性 不可变对象的好处 HashMap多线程的条件竞争 重新调整HashMap的大小 总结 HashMap的工作原理HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，让后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap在每个链表节点中储存键值对对象。 当两个不同的键对象的hashcode相同时会发生什么？ 它们会储存在同一个bucket位置的链表中。键对象的equals()方法用来找到键值对。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java集合</tag>
        <tag>数据结构</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（10）：0-1背包问题与部分背包问题]]></title>
    <url>%2F2017%2F08%2F09%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%8810%EF%BC%89%EF%BC%9A0-1%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%E4%B8%8E%E9%83%A8%E5%88%86%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[假设我们有n件物品，分别编号为1, 2…n。其中编号为i的物品价值为$v_i$，它的重量为$w_i$。为了简化问题，假定价值和重量都是整数值。现在，假设我们有一个背包，它能够承载的重量是W。现在，我们希望往包里装这些物品，使得包里装的物品价值最大化，那么我们该如何来选择装的东西呢？问题结构如下图所示： 这个问题其实根据不同的情况可以归结为不同的解决方法。假定我们这里选取的物品每个都是独立的，不能选取部分。也就是说我们要么选取某个物品，要么不能选取，不能只选取一个物品的一部分。这种情况，我们称之为0-1背包问题。而如果我们可以使用部分的物品的话，这个问题则成为部分背包(fractional knapsack)问题。下面我们针对每种情况具体分析一下。 一、0-1背包1.1 初步分析对于这个问题，一开始确实有点不太好入手。一堆的物品，每一个都有一定的质量和价值，我们能够装入的总重量有限制，该怎么来装使得价值最大呢？对于这n个物品，每个物品我们可能会选，也可能不选，那么我们总共就可能有2^n种组合选择方式。如果我们采用这种办法来硬算的话，则整体的时间复杂度就达到指数级别的，肯定不可行。 现在我们换一种思路。既然每一种物品都有价格和重量，我们优先挑选那些单位价格最高的是否可行呢？比如在下图中，我们有3种物品，他们的重量和价格分别是10, 20, 30 kg和60, 100, 120。 那么按照单位价格来算的话，我们最先应该挑选的是价格为60的元素，选择它之后，背包还剩下50 - 10 = 40kg。再继续前面的选择，我们应该挑选价格为100的元素，这样背包里的总价值为60 + 100 = 160。所占用的重量为30, 剩下20kg。因为后面需要挑选的物品为30kg已经超出背包的容量了。我们按照这种思路能选择到的最多就是前面两个物品。如下图： 按照我们前面的期望，这样选择得到的价值应该是最大的。可是由于有一个背包重量的限制，这里只用了30kg，还有剩下20kg浪费了。这会是最优的选择吗？我们看看所有的选择情况： 很遗憾，在这几种选择情况中，我们前面的选择反而是带来价值最低的。而选择重量分别为20kg和30kg的物品带来了最大的价值。看来，我们刚才这种选择最佳单位价格的方式也行不通。 1.2 动态规划既然前面两种办法都不可行，我们再来看看有没有别的方法。我们再来看这个问题。我们需要选择n个元素中的若干个来形成最优解，假定为k个。那么对于这k个元素a1, a2, …ak来说，它们组成的物品组合必然满足总重量&lt;=背包重量限制，而且它们的价值必然是最大的。因为它们是我们假定的最优选择嘛，肯定价值应该是最大的。假定$a_k$是我们按照前面顺序放入的最后一个物品。它的重量为$w_k$，它的价值为$v_k$。既然我们前面选择的这k个元素构成了最优选择，如果我们把这个$a_k$物品拿走，对应于$k-1$个物品来说，它们所涵盖的重量范围为$0-(W-w_k)$。假定W为背包允许承重的量。假定最终的价值是V，剩下的物品所构成的价值为$V-v_k$。这剩下的$k-1$个元素是不是构成了一个这种$W-w_k$的最优解呢？ 我们可以用反证法来推导。假定拿走$a_k$这个物品后，剩下的这些物品没有构成$W-w_k$重量范围的最佳价值选择。那么我们肯定有另外$k-1$个元素，他们在$W-w_k$重量范围内构成的价值更大。如果这样的话，我们用这$k-1$个物品再加上第k个，他们构成的最终W重量范围内的价值就是最优的。这岂不是和我们前面假设的k个元素构成最佳矛盾了吗？所以我们可以肯定，在这k个元素里拿掉最后那个元素，前面剩下的元素依然构成一个最佳解。 现在我们经过前面的推理已经得到了一个基本的递推关系，就是一个最优解的子解集也是最优的。可是，我们该怎么来求得这个最优解呢？我们这样来看。假定我们定义一个函数$c[i, w]$表示到第i个元素为止，在限制总重量为w的情况下我们所能选择到的最优解。那么这个最优解要么包含有i这个物品，要么不包含，肯定是这两种情况中的一种。如果我们选择了第i个物品，那么实际上这个最优解是c[i - 1, w-wi] + vi。而如果我们没有选择第i个物品，这个最优解是c[i-1, w]。这样，实际上对于到底要不要取第i个物品，我们只要比较这两种情况，哪个的结果值更大不就是最优的么？ 在前面讨论的关系里，还有一个情况我们需要考虑的就是，我们这个最优解是基于选择物品i时总重量还是在w范围内的，如果超出了呢？我们肯定不能选择它，这就和c[i-1, w]一样。 另外，对于初始的情况呢？很明显c[0, w]里不管w是多少，肯定为0。因为它表示我们一个物品都不选择的情况。c[i, 0]也一样，当我们总重量限制为0时，肯定价值为0。 这样，基于我们前面讨论的这3个部分，我们可以得到一个如下的递推公式： 有了这个关系，我们可以更进一步的来考虑代码实现了。我们有这么一个递归的关系，其中，后面的函数结果其实是依赖于前面的结果的。我们只要按照前面求出来最基础的最优条件，然后往后面一步步递推，就可以找到结果了。 我们再来考虑一下具体实现的细节。这一组物品分别有价值和重量，我们可以定义两个数组int[] v, int[] w。v[i]表示第i个物品的价值，w[i]表示第i个物品的重量。为了表示c[i, w]，我们可以使用一个int[i][w]的矩阵。其中i的最大值为物品的数量，而w表示最大的重量限制。按照前面的递推关系，c[i][0]和c[0][w]都是0。而我们所要求的最终结果是c[n][w]。所以我们实际中创建的矩阵是(n + 1) x (w + 1)的规格。下面是该过程的一个代码参考实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class DynamicKnapSack &#123; private int[] v; private int[] w; private int[][] c; private int weight; public DynamicKnapSack(int length, int weight, int[] vin, int[] win) &#123; v = new int[length + 1]; w = new int[length + 1]; c = new int[length + 1][weight + 1]; this.weight = weight; for(int i = 0; i &lt; length + 1; i++) &#123; v[i] = vin[i]; w[i] = win[i]; &#125; &#125; public void solve() &#123; for(int i = 1; i &lt; v.length; i++) &#123; for(int k = 1; k &lt;= weight; k++) &#123; if(w[i] &lt;= k) &#123; if(v[i] + c[i - 1][k - w[i]] &gt; c[i - 1][k]) c[i][k] = v[i] + c[i - 1][k - w[i]]; else c[i][k] = c[i - 1][k]; &#125; else c[i][k] = c[i - 1][k]; &#125; &#125; &#125; public void printResult() &#123; for(int i = 0; i &lt; v. length; i++) &#123; for(int j = 0; j &lt;= weight; j++) System.out.print(c[i][j] + &quot; &quot;); System.out.println(); &#125; &#125; public static void main(String[] args) &#123; int[] v = &#123;0, 60, 100, 120&#125;; int[] w = &#123;0, 10, 20, 30&#125;; int weight = 50; DynamicKnapSack knapsack = new DynamicKnapSack(3, weight, v, w); knapsack.solve(); knapsack.printResult(); &#125;&#125; 这部分代码里关键的就是solve方法。里面两个遍历循环，i表示从1到n的范围，对应于我们递归方法里描述的c(i, w)中到第i位。而k表示的是当前的重量限制。下面是程序运行的输出结果： 12340 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 0 0 0 0 0 0 0 0 0 0 60 60 60 60 60 60 60 60 60 60 100 100 100 100 100 100 100 100 100 100 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 160 0 0 0 0 0 0 0 0 0 0 60 60 60 60 60 60 60 60 60 60 100 100 100 100 100 100 100 100 100 100 160 160 160 160 160 160 160 160 160 160 180 180 180 180 180 180 180 180 180 180 220 最右下角的数值220就是c[3, 50]的解。 至此，我们对于这种问题的解决方法已经分析出来了。它的总体时间复杂度为O(nw) ，其中w是设定的一个重量范围，因此也可以说它的时间复杂度为O(n)。 二、部分背包问题和前面使用动态规划方法解决问题不一样。因为这里是部分背包问题，我们可以采用前面讨论过的一个思路。就是每次选择最优单位价格的物品，直到达到背包重量限制要求。 以前面的示例来看，我们按照这种方式选择的物品结果应该如下图： 现在，我们从实现的角度再来考虑一下。我们这里的最优解是每次挑选性价比最高的物品。对于这一组物品来说，我们需要将他们按照性价比从最高到最低的顺序来取。我们可能需要将他们进行排序。然后再依次取出来放入背包中。假定我们已经有数组v，w，他们已经按照性价比排好序了。一个参考代码的实现如下： 12345678910111213141516public double selectMax() &#123; double maxValue = 0.0; int sum = 0; int i; for(i = 0; i &lt; v.length; i++) &#123; if(sum + w[i] &lt; weight) &#123; sum += w[i]; maxValue += v[i]; &#125; else break; &#125; if(i &lt; v.length &amp;&amp; sum &lt; weight) &#123; maxValue += (double)(weight - sum) / w[i] * v[i]; &#125; return maxValue; &#125; 这里省略了对数组v, w的定义。关键点在于我们选择了若干了物品后要判断是否装满了背包重量。如果没有，还要从后面的里面挑选一部分。所以有一个if(i &lt; v.length &amp;&amp; sum &lt; weight)的判断。 在实现后我们来看该问题这种解法的时间复杂度，因为需要将数组排序，我们的时间复杂度为O(nlgn)。 在前面我们挑选按照性价比排好序的物品时，排序消耗了主要的时间。在这里，我们是否真的需要去把这些物品排序呢？在某些情况下，我们只要选择一堆物品，保证他们物品重量在指定范围内。如果我们一次挑出来一批这样的物品，而且他们满足这样的条件是不是更好呢？这一种思路是借鉴快速排序里对元素进行划分的思路。主要过程如下： 求每个元素的单位价值，pi = vi /wi。然后数组按照pi进行划分，这样会被分成3个部分，L, M, N。其中L &lt; M &lt; N。这里L表示单位价值小于某个指定值的集合，M是等于这个值的集合，而N是大于这个值的集合。 我们可以首先看N的集合，因为这里都是单位价值高的集合。我们将他们的重量累加，如果WN的重量等于我们期望的值W，则N中间的结果就是我们找到的结果。 如果WN的重量大于W，我们需要在N集合里做进一步划分。 如果WN的重量小于W，我们需要在N的基础上再去L的集合里划分，找里面大的一部分。 这样重复步骤1到4. 这里和快速排序的思路基本上差不多，只是需要将一个分割的集合给记录下来。其时间复杂度也更好一点，为O(N)。这里就简单的描述下思路，等后续再将具体的实现代码给补上。 三、总结我们这里讨论的两种背包问题因为问题的不同其本质解决方法也不同。对于0-1背包来说，他们构成了一个最优解问题的基础。我们可以通过从最小的结果集递推出最终最优结果。他们之间构成了一个递归的关系。而对于部分背包问题来说，我们可以考虑用贪婪算法，每次选择当前看来最优的结果。最终也构成了一个最优的结果。一个小小的前提变化，问题解决的思路却大不同。里面的思想值得反复体会。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>0-1背包</tag>
        <tag>部分背包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（11）：最长回文子串]]></title>
    <url>%2F2017%2F08%2F09%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%8811%EF%BC%89%EF%BC%9A%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[给定字符串S，求它的最长回文子串。假设S的最长长度为1000，并且仅有唯一的最长回文子串。 一、暴力法（ Brute Force）最简便，但同时时间复杂度也是最高的肯定是暴力解法，就是遍历字符串的“所有子串”，并判断每个子串是否为对称回文。因为字符串所有子串的复杂度为$O(n^2)$，在判断回文，总体的复杂度达到$O(n^3)$。 复杂度：时间 $O(n^3) $空间 $O(1)$可以做一些简单的优化： 从最长的子串开始遍历，一旦找到一个回文，就终止迭代。 判断回文采用收缩法，从最外一对字符往中心推进。 12345678910111213141516171819202122public class Solution &#123; public String longestPalindrome(String s) &#123; for (int size = s.length(); size &gt; 0; size--) &#123; for (int low = 0, high = low+size-1; high &lt; s.length(); low++, high++) &#123; if (shrinkCheckPalindrome(s,low,high)) &#123; return s.substring(low,high+1); &#125; &#125; &#125; return s.substring(0,1); &#125; public boolean shrinkCheckPalindrome(String s, int low, int high) &#123; while (low &lt;= high) &#123; if (s.charAt(low) == s.charAt(high)) &#123; low++; high--; &#125; else &#123; return false; &#125; &#125; return true; &#125;&#125; 二、动态规划根据回文的特性，一个大回文按比例缩小后的字符串也必定是回文，比如ABCCBA，那BCCB肯定也是回文。所以我们可以根据动态规划的两个特点：第一大问题拆解为小问题，第二重复利用之前的计算结果，来解答这道题。那如何划分小问题呢，我们可以先把所有长度最短为1的子字符串计算出来，根据起始位置从左向右，这些必定是回文。然后计算所有长度为2的子字符串，再根据起始位置从左向右。到长度为3的时候，我们就可以利用上次的计算结果：如果中心对称的短字符串不是回文，那长字符串也不是，如果短字符串是回文，那就要看长字符串两头是否一样。这样，一直到长度最大的子字符串，我们就把整个字符串集穷举完了，但是由于使用动态规划，使计算时间从$O(N^3)$减少到$O(n^2)$。 复杂度：时间 $O(n^2) $空间 $O(n^2)$ 12345678910111213141516class Solution &#123; public String longestPalindrome(String s) &#123; int n = s.length(); String res = null; boolean[][] dp = new boolean[n][n]; for (int i = n - 1; i &gt;= 0; i--) &#123; for (int j = i; j &lt; n; j++) &#123; dp[i][j] = s.charAt(i) == s.charAt(j) &amp;&amp; (j - i &lt; 3 || dp[i + 1][j - 1]); if (dp[i][j] &amp;&amp; (res == null || j - i + 1 &gt; res.length())) &#123; res = s.substring(i, j + 1); &#125; &#125; &#125; return res; &#125;&#125; 三、中心扩散法动态规划虽然优化了时间，但也浪费了空间。实际上我们并不需要一直存储所有子字符串的回文情况，我们需要知道的只是中心对称的较小一层是否是回文。所以如果我们从小到大连续以某点为个中心的所有子字符串进行计算，就能省略这个空间。 这种解法中，外层循环遍历的是子字符串的中心点，内层循环则是从中心扩散，一旦不是回文就不再计算其他以此为中心的较大的字符串。由于中心对称有两种情况，一是奇数个字母以某个字母对称，而是偶数个字母以两个字母中间为对称，所以我们要分别计算这两种对称情况。 复杂度：时间 O(n^2) 空间 O(1) 12345678910111213141516171819202122232425public class Solution &#123; private int max = 0; private String res = &quot;&quot;; public String longestPalindrome(String s) &#123; if (s.length() == 1) &#123; return s; &#125; for (int i = 0; i &lt; s.length()-1; i++) &#123; checkPalindromeExpand(s,i,i); checkPalindromeExpand(s,i,i+1); &#125; return res; &#125; public void checkPalindromeExpand(String s, int low, int high) &#123; while (low &gt;= 0 &amp;&amp; high &lt; s.length()) &#123; if (s.charAt(low) == s.charAt(high)) &#123; if (high - low + 1 &gt; max) &#123; max = high - low + 1; res = s.substring(low,high+1); &#125; low--; high++; &#125; else &#123; return; &#125; &#125; &#125;&#125; 四、马拉车算法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Solution &#123; public String longestPalindrome(String s) &#123; if(s.length()&lt;=1)&#123; return s; &#125; // 预处理字符串，避免奇偶问题 String str = preProcess(s); // idx是当前能够向右延伸的最远的回文串中心点，随着迭代而更新 // max是当前最长回文串在总字符串中所能延伸到的最右端的位置 // maxIdx是当前已知的最长回文串中心点 // maxSpan是当前已知的最长回文串向左或向右能延伸的长度 int idx = 0, max = 0; int maxIdx = 0; int maxSpan = 0; int[] p = new int[str.length()]; for(int curr = 1; curr &lt; str.length(); curr++)&#123; // 找出当前下标相对于idx的对称点 int symmetryOfCurr = 2 * idx - curr; // 如果当前已知延伸的最右端大于当前下标，我们可以用对称点的P值，否则记为1等待检查 p[curr] = max &gt; curr? Math.min(p[symmetryOfCurr], max - curr):1; // 检查并更新当前下标为中心的回文串最远延伸的长度 while((curr+p[curr])&lt;str.length() &amp;&amp; str.charAt(curr+p[curr])==str.charAt(curr-p[curr]))&#123; p[curr]++; &#125; // 检查并更新当前已知能够延伸最远的回文串信息 if(curr+p[curr]&gt;max)&#123; max = p[curr] + curr; idx = curr; &#125; // 检查并更新当前已知的最长回文串信息 if(p[curr]&gt;maxSpan)&#123; maxSpan = p[curr]; maxIdx = curr; &#125; &#125; //去除占位符 return s.substring((maxIdx-maxSpan)/2,(maxSpan+maxIdx)/2-1); &#125; // 预处理，如ABC,变为$#A#B#C# private String preProcess(String s)&#123; StringBuilder sb = new StringBuilder(); sb.append("$"); for(int i = 0; i &lt; s.length(); i++)&#123; sb.append("#"); sb.append(s.charAt(i)); &#125; sb.append("#"); return sb.toString(); &#125;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>最长回文子串</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（9）：最长公共子序列和最长公共子串]]></title>
    <url>%2F2017%2F08%2F08%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%889%EF%BC%89%EF%BC%9A%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97%E5%92%8C%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[一、最长公共子序列（LCS）求最长公共子序列的数目，注意这里的子序列可以不是连续序列，务必问清楚题意。求『最长』类的题目往往与动态规划有点关系，这里是两个字符串，故应为双序列动态规划。 这道题的状态很容易找，不妨先试试以$f[i][j]$表示字符串 A 的前 i 位和字符串 B 的前 j 位的最长公共子序列数目，那么接下来试试寻找其状态转移方程。 从实际例子ABCD和EDCA出发，首先初始化f的长度为字符串长度加1，那么有$f[0][0] = 0, f[0][] = 0, f[][0] = 0$,最后应该返回$f[lenA][lenB]$. 即 f 中索引与字符串索引对应(字符串索引从1开始算起)，那么在A 的第一个字符与 B 的第一个字符相等时，$f[1][1] = 1 + f[0][0]$, 否则$f[1][1] = max(f[0][1], f[1][0])$。 推而广之，也就意味着若$A[i] == B[j]$, 则分别去掉这两个字符后，原 LCS 数目减一，那为什么一定是1而不是0或者2呢？因为不管公共子序列是以哪个字符结尾，在$A[i] == B[j]$时 LCS 最多只能增加1. 而在$A[i] != B[j]$时，由于A[i] 或者 B[j] 不可能同时出现在最终的 LCS 中，故这个问题可进一步缩小，$f[i][j] = max(f[i - 1][j], f[i][j - 1])$. 需要注意的是这种状态转移方程只依赖最终的 LCS 数目，而不依赖于公共子序列到底是以第几个索引结束。 123456789101112131415161718192021222324252627public class Demo &#123; public static int longestCommonSubsequence(String A, String B) &#123; if (A == null || A.length() == 0) return 0; if (B == null || B.length() == 0) return 0; int lenA = A.length(); int lenB = B.length(); int[][] lcs = new int[1 + lenA][1 + lenB]; for (int i = 1; i &lt; 1 + lenA; i++) &#123; for (int j = 1; j &lt; 1 + lenB; j++) &#123; if (A.charAt(i - 1) == B.charAt(j - 1)) &#123; lcs[i][j] = 1 + lcs[i - 1][j - 1]; &#125; else &#123; lcs[i][j] = Math.max(lcs[i - 1][j], lcs[i][j - 1]); &#125; &#125; &#125; return lcs[lenA][lenB]; &#125; public static void main(String[] args) &#123; String source = "zhanghua"; String target = "zhanghau"; System.out.println("longestCommonSubsequence=" + longestCommonSubsequence(source, target)); &#125;&#125; 二、最长公共子串2.1 简单考虑可以使用两根指针索引分别指向两个字符串的当前遍历位置，若遇到相等的字符时则同时向后移动一位。 1234567891011121314151617181920212223242526272829public class Demo &#123; public static int longestCommonSubstring(String A, String B) &#123; if (A == null || A.length() == 0) return 0; if (B == null || B.length() == 0) return 0; int lenA = A.length(); int lenB = B.length(); int lcs = 0, lcs_temp = 0; for (int i = 0; i &lt; lenA; ++i) &#123; for (int j = 0; j &lt; lenB; ++j) &#123; lcs_temp = 0; while ((i + lcs_temp &lt; lenA) &amp;&amp; (j + lcs_temp &lt; lenB) &amp;&amp; (A.charAt(i + lcs_temp) == B.charAt(j + lcs_temp))) &#123; ++lcs_temp; &#125; if (lcs_temp &gt; lcs) &#123; lcs = lcs_temp; &#125; &#125; &#125; return lcs; &#125; public static void main(String[] args) &#123; String source = "zhanghua"; String target = "zhanghau"; System.out.println("longestCommonString=" + longestCommonSubstring(source, target)); &#125;&#125; 2.2 动态规划把$D[i][j] $定义为：两个string的前i个和前j个字符串，尾部连到最后的最长子串。 然后$D[i][j] = $ $i = 0 || j = 0 : 0$ $s1.char[i - 1] = s2.char[j - 1] ? D[i-1][j-1] + 1 : 0;$ 另外，创建一个max的缓存，不段更新即可。 1234567891011121314151617181920212223242526272829303132public class Demo &#123; public static int longestCommonSubstring(String A, String B) &#123; if (A == null || A.length() == 0) return 0; if (B == null || B.length() == 0) return 0; int lenA = A.length(); int lenB = B.length(); int[][] D = new int[lenA + 1][lenB + 1]; int max = 0; for (int i = 0; i &lt;= lenA; i++) &#123; for (int j = 0; j &lt;= lenB; j++) &#123; if (i == 0 || j == 0) &#123; D[i][j] = 0; &#125; else &#123; if (A.charAt(i - 1) == B.charAt(j - 1)) &#123; D[i][j] = D[i - 1][j - 1] + 1; &#125; else &#123; D[i][j] = 0; &#125; &#125; max = Math.max(max, D[i][j]); &#125; &#125; return max; &#125; public static void main(String[] args) &#123; String source = "zhanghua"; String target = "zhanghau"; System.out.println("longestCommonString=" + longestCommonSubstring(source, target)); &#125;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>最长公共子序列</tag>
        <tag>最长公共子串</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（8）：KMP算法]]></title>
    <url>%2F2017%2F08%2F08%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%888%EF%BC%89%EF%BC%9AKMP%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[KMP算法是三位大牛：D.E.Knuth、J.H.Morris和V.R.Pratt同时发现的。其中第一位就是《计算机程序设计艺术》的作者！ KMP算法要解决的问题就是在字符串（也叫主串）中的模式（pattern）定位问题。说简单点就是我们平时常说的关键字搜索。模式串就是关键字（接下来称它为P），如果它在一个主串（接下来称为T）中出现，就返回它的具体位置，否则返回-1（常用手段）。 首先，对于这个问题有一个很单纯的想法：从左到右一个个匹配，如果这个过程中有某个字符不匹配，就跳回去，将模式串向右移动一位。这有什么难的？ 我们可以这样初始化： 之后我们只需要比较i指针指向的字符和j指针指向的字符是否一致。如果一致就都向后移动，如果不一致，如下图： A和E不相等，那就把i指针移回第1位（假设下标从0开始），j移动到模式串的第0位，然后又重新开始这个步骤： 12345678910111213141516171819202122public class Demo&#123; public static int bf(String ts,String ps) &#123; char[] t = ts.toCharArray(); char[] p = ps.toCharArray(); int i = 0; // 主串的位置 int j = 0; // 模式串的位置 while (i &lt; t.length &amp;&amp; j &lt; p.length) &#123; if (t[i] == p[j]) &#123; // 当两个字符相同，就比较下一个 i++; j++; &#125;else &#123; i = i - j + 1; // 一旦不匹配，i后退 j = 0; // j归0 &#125; &#125; if (j == p.length) &#123; return i - j; &#125;else &#123; return -1; &#125; &#125;&#125; 上面的程序是没有问题的，但不够好！如果是人为来寻找的话，肯定不会再把i移动回第1位，因为主串匹配失败的位置前面除了第一个A之外再也没有A了，我们为什么能知道主串前面只有一个A？因为我们已经知道前面三个字符都是匹配的！（这很重要）。移动过去肯定也是不匹配的！有一个想法，i可以不动，我们只需要移动j即可，如下图： 上面的这种情况还是比较理想的情况，我们最多也就多比较了再次。但假如是在主串“SSSSSSSSSSSSSA”中查找“SSSSB”，比较到最后一个才知道不匹配，然后i回溯，这个的效率是显然是最低的。 大牛们是无法忍受“暴力破解”这种低效的手段的，于是他们三个研究出了KMP算法。其思想就如同我们上边所看到的一样：“利用已经部分匹配这个有效信息，保持i指针不回溯，通过修改j指针，让模式串尽量地移动到有效的位置。” 所以，整个KMP的重点就在于当某一个字符与主串不匹配时，我们应该知道j指针要移动到哪？ 接下来我们自己来发现j的移动规律： 如图：C和D不匹配了，我们要把j移动到哪？显然是第1位。为什么？因为前面有一个A相同啊： 如下图也是一样的情况： 可以把j指针移动到第2位，因为前面有两个字母是一样的： 至此我们可以大概看出一点端倪，当匹配失败时，j要移动的下一个位置k。存在着这样的性质：最前面的k个字符和j之前的最后k个字符是一样的。 如果用数学公式来表示是这样的 P[0，k-1] == P[j-k， j-1]这个相当重要，如果觉得不好记的话，可以通过下图来理解： 弄明白了这个就应该可能明白为什么可以直接将j移动到k位置了。 因为: 1234567当T[i] != P[j]时有T[i-j ~ i-1] == P[0 ~ j-1]由P[0 ~ k-1] == P[j-k ~ j-1]必然：T[i-k ~ i-1] == P[0 ~ k-1] 公式很无聊，能看明白就行了，不需要记住。 这一段只是为了证明我们为什么可以直接将j移动到k而无须再比较前面的k个字符。 好，接下来就是重点了，怎么求这个（这些）k呢？因为在P的每一个位置都可能发生不匹配，也就是说我们要计算每一个位置j对应的k，所以用一个数组next来保存，next[j] = k，表示当T[i] != P[j]时，j指针的下一个位置。 很多教材或博文在这个地方都是讲得比较含糊或是根本就一笔带过，甚至就是贴一段代码上来，为什么是这样求？怎么可以这样求？根本就没有说清楚。而这里恰恰是整个算法最关键的地方。 1234567891011121314151617public class Demo&#123; public static int[] getNext(String ps) &#123; char[] p = ps.toCharArray(); int[] next = new int[p.length]; next[0] = -1; int j = 0; int k = -1; while (j &lt; p.length - 1) &#123; if (k == -1 || p[j] == p[k]) &#123; next[++j] = ++k; &#125; else &#123; k = next[k]; &#125; &#125; return next; &#125;&#125; 这个版本的求next数组的算法应该是流传最广泛的，代码是很简洁。可是真的很让人摸不到头脑，它这样计算的依据到底是什么？好，先把这个放一边，我们自己来推导思路，现在要始终记住一点，next[j]的值（也就是k）表示，当P[j] != T[i]时，j指针的下一步移动位置。先来看第一个：当j为0时，如果这时候不匹配，怎么办？像上图这种情况，j已经在最左边了，不可能再移动了，这时候要应该是i指针后移。所以在代码中才会有next[0] = -1;这个初始化。如果是当j为1的时候呢？ 显然，j指针一定是后移到0位置的。因为它前面也就只有这一个位置了~~~下面这个是最重要的，请看如下图： 请仔细对比这两个图。我们发现一个规律： 12当P[k] == P[j]时，有next[j+1] == next[j] + 1 其实这个是可以证明的： 123因为在P[j]之前已经有P[0 ~ k-1] == p[j-k ~ j-1]。（next[j] == k）这时候现有P[k] == P[j]，我们是不是可以得到P[0 ~ k-1] + P[k] == p[j-k ~ j-1] + P[j]。即：P[0 ~ k] == P[j-k ~ j]，即next[j+1] == k + 1 == next[j] + 1。 这里的公式不是很好懂，还是看图会容易理解些。那如果P[k] != P[j]呢？比如下图所示：像这种情况，如果你从代码上看应该是这一句：k = next[k];为什么是这样子？你看下面应该就明白了。 现在你应该知道为什么要k = next[k]了吧！像上边的例子，我们已经不可能找到[ A，B，A，B ]这个最长的后缀串了，但我们还是可能找到[ A，B ]、[ B ]这样的前缀串的。所以这个过程像不像在定位[ A，B，A，C ]这个串，当C和主串不一样了（也就是k位置不一样了），那当然是把指针移动到next[k]啦。 在P[K]!=P[j]时，我们已经知道（0，k-1）串和（j-k,j-1）串是相等的，所以可以把（0，k-1）串当做一个新的模式串，发现在新模式串中（0，next[k]-1）串与（k-next[k],k-1）串相等，所以（j-k,j-1）中存在与（0，next[k]-1）相等的串，所以可以把j移动到next[k],继续比较和移动。 有了next数组之后就一切好办了，我们可以动手写KMP算法了： 12345678910111213141516171819202122public static int KMP(String ts, String ps) &#123; char[] t = ts.toCharArray(); char[] p = ps.toCharArray(); int i = 0; // 主串的位置 int j = 0; // 模式串的位置 int[] next = getNext(ps); while (i &lt; t.length &amp;&amp; j &lt; p.length) &#123; if (j == -1 || t[i] == p[j]) &#123; // 当j为-1时，要移动的是i，当然j也要归0 i++; j++; &#125; else &#123; // i不需要回溯了 // i = i - j + 1; j = next[j]; // j回到指定位置 &#125; &#125; if (j == p.length) &#123; return i - j; &#125; else &#123; return -1; &#125;&#125; 和暴力破解相比，就改动了4个地方。其中最主要的一点就是，i不需要回溯了。 最后，来看一下上边的算法存在的缺陷。来看第一个例子： 显然，当我们上边的算法得到的next数组应该是[ -1，0，0，1 ] 所以下一步我们应该是把j移动到第1个元素咯：不难发现，这一步是完全没有意义的。因为后面的B已经不匹配了，那前面的B也一定是不匹配的，同样的情况其实还发生在第2个元素A上。显然，发生问题的原因在于P[j] == P[next[j]]。所以我们也只需要添加一个判断条件即可： 12345678910111213141516171819public static int[] getNext(String ps) &#123; char[] p = ps.toCharArray(); int[] next = new int[p.length]; next[0] = -1; int j = 0; int k = -1; while (j &lt; p.length - 1) &#123; if (k == -1 || p[j] == p[k]) &#123; if (p[++j] == p[++k]) &#123; // 当两个字符相等时要跳过 next[++j] = ++k; &#125; else &#123; next[j] = k;; &#125; &#125;else &#123; k = next[k]; &#125; &#125; return next;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>KMP算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（7）：最短编辑距离]]></title>
    <url>%2F2017%2F08%2F07%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%887%EF%BC%89%EF%BC%9A%E6%9C%80%E7%9F%AD%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[现代搜索技术的发展很多以提供优质、高效的服务作为目标。比如说：baidu、google、sousou等知名全文搜索系统。当我们输入一个错误的query=”Jave” 的时候，返回中有大量包含正确的拼写 “Java”的网页。是怎么做到的呢？这其中，字符串的相似度计算是做到这一点的方法之一。 一、字符串编辑距离是一种字符串之间相似度计算的方法。给定两个字符串S、T，将S转换成T所需要的删除，插入，替换操作的数量就叫做S到T的编辑路径。而最短的编辑路径就叫做字符串S和T的编辑距离。 举个例子：S=“eeba” T=”abac” 我们可以按照这样的步骤转变：(1) 将S中的第一个e变成a;(2) 删除S中的第二个e;(3)在S中最后添加一个c; 那么S到T的编辑路径就等于3。当然，这种变换并不是唯一的，但如果3是所有变换中最小值的话。那么我们就可以说S和T的编辑距离等于3了。 二、动态规划解决编辑距离动态规划(dynamic programming)是一种解决复杂问题最优解的策略。它的基本思路就是：将一个复杂的最优解问题分解成一系列较为简单的最优解问题，再将较为简单的的最优解问题进一步分解，直到可以一眼看出最优解为止。 动态规划算法是解决复杂问题最优解的重要算法。其算法的难度并不在于算法本身的递归难以实现，而主要是编程者对问题本身的认识是否符合动态规划的思想。现在我们就来看看动态规划是如何解决编辑距离的。 假设$dp[i-1][j-1]$表示一个长为$i-1$的字符串$str1$变为长为$j-1$的字符串str2的最短距离，如果我们此时想要把$str1a$这个字符串变成$str2b$这个字符串，我们有如下几种选择： 替换： 在str1变成str2的步骤后，我们将str1a中的a替换为b，就得到str2b (如果a和b相等，就不用操作) 增加： 在str1a变成str2的步骤后，我们再在末尾添加一个b，就得到str2b (str1a先根据已知距离变成str2，再加个b) 删除： 在str1变成str2b的步骤后，对于str1a，我们将末尾的a删去，就得到str2b (str1a将a删去得到str1，而str1到str2b的编辑距离已知) 根据这三种操作，我们可以得到递推式 若a和b相等： 1dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]) 若a和b不相等： 1dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+1) 因为将一个非空字符串变成空字符串的最小操作数是字母个数（全删），反之亦然，所以： 1dp[0][j]=j, dp[i][0]=i 最后我们只要返回dp[m][n]即可，其中m是word1的长度，n是word2的长度 三、代码12345678910111213141516171819202122232425262728293031public class Demo &#123; public static int minDistance(String word1, String word2) &#123; int m = word1.length(), n = word2.length(); int[][] dp = new int[m + 1][n + 1]; // 初始化空字符串的情况 for(int i = 1; i &lt;= m; i++)&#123; dp[i][0] = i; &#125; for(int i = 1; i &lt;= n; i++)&#123; dp[0][i] = i; &#125; for(int i = 1; i &lt;= m; i++)&#123; for(int j = 1; j &lt;= n; j++)&#123; // 增加操作：str1a变成str2后再加上b，得到str2b int insertion = dp[i][j-1] + 1; // 删除操作：str1a删除a后，再由str1变为str2b int deletion = dp[i-1][j] + 1; // 替换操作：先由str1变为str2，然后str1a的a替换为b，得到str2b int replace = dp[i-1][j-1] + (word1.charAt(i - 1) == word2.charAt(j - 1) ? 0 : 1); // 三者取最小 dp[i][j] = Math.min(replace, Math.min(insertion, deletion)); &#125; &#125; return dp[m][n]; &#125; public static void main(String[] args) &#123; String source = &quot;zhanghua&quot;; String target = &quot;zhanghau&quot;; System.out.println(&quot;minDistance=&quot; + minDistance(source, target)); &#125;&#125; 测试结果为： 1minDistance=2]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>最短编辑距离</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（6）：重点掌握]]></title>
    <url>%2F2017%2F08%2F06%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%886%EF%BC%89%EF%BC%9A%E9%87%8D%E7%82%B9%E6%8E%8C%E6%8F%A1%2F</url>
    <content type="text"><![CDATA[最基础的数据结构与算法java实现。 一、排序排序面试题： 实现快速排序以及时空复杂度分析 实现归并排序以及时空复杂度分析 实现堆排序以及时空复杂度分析 1.1 归并排序归并排序是典型的二路合并排序，将原始数据集分成两部分(不一定能够均分)，分别对它们进行排序，然后将排序后的子数据集进行合并，典型的分治法策略。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class MergeSortTest &#123; public static void main(String[] args) &#123; int[] data = new int[] &#123; 5, 3, 6, 2, 1, 9, 4, 8, 7 &#125;; print(data); mergesort(data); System.out.println("排序后的数组："); print(data); &#125; public static void mergesort(int[] arr)&#123; sort(arr, 0, arr.length-1); &#125; private static void sort(int[] a, int left, int right)&#123; //当left==right的时，已经不需要再划分了 if (left&lt;right)&#123; int middle = (left+right)/2; sort(a, left, middle); //左子数组 sort(a, middle+1, right); //右子数组 merge(a, left, middle, right); //合并两个子数组 &#125; &#125; // 合并两个有序子序列 arr[left, ..., middle] 和 arr[middle+1, ..., right]。temp是辅助数组。 private static void merge(int arr[], int left, int middle, int right)&#123; int[] temp = new int[right - left + 1]; int i=left; int j=middle+1; int k=0; //将记录由小到大地放进temp数组 while ( i&lt;=middle &amp;&amp; j&lt;=right)&#123; if (arr[i] &lt;=arr[j])&#123; temp[k++] = arr[i++]; &#125; else&#123; temp[k++] = arr[j++]; &#125; &#125; while (i &lt;=middle)&#123; temp[k++] = arr[i++]; &#125; while ( j&lt;=right)&#123; temp[k++] = arr[j++]; &#125; //把数据复制回原数组 for (i=0; i&lt;k; ++i)&#123; arr[left+i] = temp[i]; &#125; &#125; public static void print(int[] data) &#123; for (int i = 0; i &lt; data.length; i++) &#123; System.out.print(data[i] + "\t"); &#125; System.out.println(); &#125;&#125; 在合并数组的时候需要一个temp数组。我们当然有足够的理由在每次调用的时候重新new一个数组（例如，减少一个参数），但是，注意到多次的创建数组对象会造成额外的开销，我们可以在开始就创建一个足够大的数组（等于原数组长度就行），以后都使用这个数组。实际上，上面的代码就是这么写的。 时间复杂度：在归并排序中，进行一趟归并需要的关键字比较次数和数据元素移动次数最多为$n$，需要归并的趟数$log n$，故归并排序的时间复杂度为$O(nlog n)$。并且由于归并算法是固定的，不受输入数据影响，所以它在最好、最坏、平均情况下表现几乎相同，均为$O(log n)$。 空间复杂度：归并排序需要长度等于序列长度为$n$的辅助存储单元，故归并排序的空间复杂度为$O(n)$。归并排序最大的缺陷在于其空间复杂度。可不可以省略这个数组呢？不行!如果取消辅助数组而又要保证原来的数组中数据不被覆盖，那就必须要在数组中花费大量时间来移动数据。不仅容易出错，还降低了效率。因此这个辅助空间是少不掉的。 稳定性：因为我们在遇到相等的数据的时候必然是按顺序“抄写”到辅助数组上的，所以，归并排序是稳定的排序算法。 1.2 快速排序快速排序是图灵奖得主C.R.A Hoare于1960年提出的一种划分交换排序。它采用了一种分治的策略，通常称其为分治法（Divide-and-Conquer Method） 分治法的基本思想是：将原问题分解为若干个规模更小但结构与原问题相似的子问题。递归地解这些子问题，然后将这些子问题组合为原问题的解。 利用分治法可将快速排序分为三步： 从数列中挑出一个元素作为“基准”（pivot）。 分区过程，将比基准数大的放到右边，小于或等于它的数都放到左边。这个操作称为“分区操作”，分区操作结束后，基准元素所处的位置就是最终排序后它的位置 再对“基准”左右两边的子集不断重复第一步和第二步，直到所有子集只剩下一个元素为止。 1234567891011121314151617181920212223242526272829303132333435363738public class quickSortTest &#123; public static void main(String[] args) &#123; int[] data = new int[] &#123; 5, 3, 6, 2, 1, 9, 4, 8, 7 &#125;; print(data); quickSort(data); System.out.println("排序后的数组："); print(data); &#125; public static void quickSort(int[] arr)&#123; qsort(arr, 0, arr.length-1); &#125; private static void qsort(int[] arr, int left, int right)&#123; if (left &lt; right)&#123; int pivot=partition(arr, left, right); //将数组分为两部分 qsort(arr, left, pivot-1); //递归排序左子数组 qsort(arr, pivot+1, right); //递归排序右子数组 &#125; &#125; private static int partition(int[] arr, int left, int right)&#123; int pivot = arr[left]; //基准记录 while (left&lt;right)&#123; while (left&lt;right &amp;&amp; arr[right]&gt;=pivot) --right; arr[left]=arr[right]; //交换比基准小的记录到左端 while (left&lt;right &amp;&amp; arr[left]&lt;=pivot) ++left; arr[right] = arr[left]; //交换比基准大的记录到右端 &#125; //扫描完成，基准到位 arr[left] = pivot; //返回的是基准的位置 return left; &#125; public static void print(int[] data) &#123; for (int i = 0; i &lt; data.length; i++) &#123; System.out.print(data[i] + "\t"); &#125; System.out.println(); &#125;&#125; 二、查找2.1 二分查找 1234567891011121314151617181920int binary_search(int array[],int n,int value) &#123; int left=0; int right=n-1; while (left&lt;=right) &#123; int middle=left + ((right-left)&gt;&gt;1); if (array[middle]&gt;value) &#123; right =middle-1; //right赋值，适时而变 &#125; else if(array[middle]&lt;value) &#123; left=middle+1; &#125; else return middle; &#125; return -1; &#125; 三、二叉树这块内容讨论二叉树的常见遍历方式的代码（java）实现，包括前序（preorder）、中序（inorder）、后序（postorder）、层序（levelorder），进一步考虑递归和非递归的实现方式。 递归的实现方法相对简单，但由于递归的执行方式每次都会产生一个新的方法调用栈，如果递归层级较深，会造成较大的内存开销，相比之下，非递归的方式则可以避免这个问题。递归遍历容易实现，非递归则没那么简单，非递归调用本质上是通过维护一个栈，模拟递归调用的方法调用栈的行为。 在此之前，先简单定义节点的数据结构： 二叉树节点最多只有两个儿子，并保存一个节点的值，为了实验的方便，假定它为 int。同时，我们直接使用 Java 的 System.out.print 方法来输出节点值，以显示遍历结果。 1234567891011class Node&#123; public int value; public Node left; public Node right; public Node(int v)&#123; this.value=v; this.left=null; this.right=null; &#125; &#125; 3.1 前序遍历3.1.1 递归实现递归实现很简单，在每次访问到某个节点时，先输出节点值，然后再依次递归的对左儿子、右儿子调用遍历的方法。代码如下 java 1234567public void preOrder(Node root)&#123; if(root!=null)&#123; System.out.print(root.value); preOrder(root.left); preOrder(root.right); &#125;&#125; 3.1.2 非递归实现利用栈实现循环先序遍历二叉树，维护一个栈，将根节点入栈，只要栈不为空，出栈并访问，接着依次将访问节点的右节点、左节点入栈。这种方式是对先序遍历的一种特殊实现，简洁明了，但是不具备很好地扩展性，在中序和后序方式中不适用。 1234567891011public void preOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; stack = new Stack&lt;Node&gt;(); stack.push(root); while(!stack.isEmpty)&#123; Node temp = stack.pop(); System.out.print(temp.value); if(temp.right!=null)stack.push(temp.right); if(temp.left!=null)stack.push(temp.left); &#125;&#125; 还有一种方式就是利用栈模拟递归过程实现循环先序遍历二叉树。这种方式具备扩展性，它模拟了递归的过程，将左子树不断的压入栈，直到null，然后处理栈顶节点的右子树。 java 12345678910111213public void preOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; s = new Stack&lt;Node&gt;(); while(root!=null||!s.isEmtpy())&#123; while(root!=null)&#123; System.out.print(root.value);、//先访问 s.push(root);//再入栈 root = root.left; &#125; root = s.pop(); root = root.right;//如果是null，出栈并处理右子树 &#125;&#125; 3.2 中序遍历3.2.1 递归实现1234567public void inOrder(Node root)&#123; if(root!=null)&#123; preOrder(root.left); System.out.print(root.value); preOrder(root.right); &#125;&#125; 3.2.2 非递归实现利用栈模拟递归过程实现循环中序遍历二叉树。跟前序遍历的非递归实现方法二很类似。唯一的不同是访问当前节点的时机：前序遍历在入栈前访问，而中序遍历在出栈后访问。 java 12345678910111213public void inOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; s = Stack&lt;Node&gt;(); while(root!=null||s.isEmpty())&#123; while(root!=null)&#123; s.push(root); root=root.left; &#125; root = s.pop(root); System.out.print(root.value); root = root.right; &#125;&#125; 3.3 后序遍历3.3.1 递归实现1234567public void inOrder(Node root)&#123; if(root!=null)&#123; preOrder(root.left); preOrder(root.right); System.out.print(root.value); &#125;&#125; 3.3.2 非递归实现1234567891011121314151617public void postOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; s1 = new Stack&lt;Node&gt;(); Stack&lt;Node&gt; s2 = new Stack&lt;Node&gt;(); Node node = root; s1.push(node); while(s1!=null)&#123;//这个while循环的功能是找出后序遍历的逆序，存在s2里面 node = s1.pop(); if(node.left!=null) s1.push(node.left); if(node.right!=null)s1.push(node.right); s2.push(node); &#125; while(s2!=null)&#123;//将s2中的元素出栈，即为后序遍历次序 node = s2.pop(); System.out.print(node.value); &#125;&#125; 3.4 层序遍历1234567891011public static void levelTravel(Node root)&#123; if(root==null)return; Queue&lt;Node&gt; q=new LinkedList&lt;Node&gt;(); q.add(root); while(!q.isEmpty())&#123; Node temp = q.poll(); System.out.println(temp.value); if(temp.left!=null)q.add(temp.left); if(temp.right!=null)q.add(temp.right); &#125; &#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（5）：剑指offer解题报告]]></title>
    <url>%2F2017%2F08%2F05%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%885%EF%BC%89%EF%BC%9A%E5%89%91%E6%8C%87offer%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[剑指offer编程题java实现整理。 3. 二维数组中的查找（数组）在一个二维数组中，每一行都按照从左到右的递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一维数组和一个整数，判断数组中是否含有该整数。 首先选取数组中右上角的数字，如果该数字等于我们要查找的数组，查找过程结束；如果该数字大于要查找的数组，剔除这个数字所在的列；如果该数字小于要查找的数组，剔除这个数字所在的行。也就是说如果要查找的数字不再数组的右上角，则每一次都在数组的查找范围中剔除一行或者一列，这样每一步都可以缩小查找的范围，直到找到要查找的数字，或者查找范围为空。 java 12345678910111213141516171819public class Solution &#123; public boolean Find(int target, int [][] array) &#123; int row = 0; int col = array[0].length - 1; while(row&lt;=array.length-1&amp;&amp;col&gt;=0)&#123; if(target == array[row][col])&#123; return true; &#125; else if(target&gt;array[row][col])&#123; row++; &#125; else&#123; col--; &#125; &#125; return false; &#125;&#125; python 123456789101112131415# -*- coding:utf-8 -*-class Solution: # array 二维列表 def Find(self, target, array): # write code here row = 0 col = len(array[0])-1 while row&lt;=len(array)-1 and col&gt;=0: if target==array[row][col]: return True elif target&gt;array[row][col]: row+=1 else: col-=1 return False 也可以把每一行看做是一个递增的序列，利用二分查找。 java 12345678910111213141516171819public class Solution &#123; public boolean Find(int target, int [][] array) &#123; for(int i=0;i&lt;array.length;i++)&#123; int low =0; int high = array[i].length-1; while(low&lt;=high)&#123; int mid = (low+high)/2; if(array[i][mid]==target) return true; else if(array[i][mid]&gt;target) high =mid-1; else low=mid+1; &#125; &#125; return false; &#125;&#125; 4. 替换空格（字符串）请实现一个函数，把字符串中的每个空格替换成”%20”。例如输入“We are happy.”，则输出“We%20are20happy” 网络编程中，要把特殊符号转换成服务器可识别的字符。转换的规则是在“%”后面跟上ASCII码的两位十六进制的表示。比如空格的ASCII码是32，即十六进制的0X20，因此空格被替换成“%20”。 问题1：替换字符串，是在原来的字符串上做替换，还是新开辟一个字符串做替换！问题2：在当前字符串替换，怎么替换才更有效率（不考虑java里现有的replace方法）。从前往后替换，后面的字符要不断往后移动，要多次移动，所以效率低下；从后往前，先计算需要多少空间，然后从后往前移动，则每个字符只为移动一次，这样效率更高一点。 123456789101112131415161718192021222324public class Solution &#123; public String replaceSpace(StringBuffer str) &#123; int spacenum = 0;//spacenum为计算空格数 for(int i=0;i&lt;str.length();i++)&#123; if(str.charAt(i)==' ') spacenum++; &#125; int indexold = str.length()-1;//indexold为为替换前的str下标 int newlength = str.length()+2*spacenum;//计算空格转换成%20之后的str长度 int indexnew = newlength-1;//indexold为为把空格替换为%20后的str下标 str.setLength(newlength);//使str的长度扩大到转换成%20之后的长度,防止下标越界,setLength方法 for(;indexold&gt;=0&amp;&amp;indexold&lt;newlength;--indexold)&#123; if(str.charAt(indexold)==' ')&#123;//charAt方法 str.setCharAt(indexnew--,'0'); str.setCharAt(indexnew--,'2'); str.setCharAt(indexnew--,'%'); &#125; else&#123; str.setCharAt(indexnew--,str.charAt(indexold)); &#125; &#125; return str.toString(); &#125;&#125; 5. 从尾到头打印链表（链表）输入一个链表的头结点，从尾到头反过来打印每个结点的值（注意不能改变链表的结构）。 解决这个问题肯定要遍历链表。遍历的顺序是从头到尾的顺序，可输出的顺序却是从尾到头。也就是说第一个遍历到的结点最后一个输出，而最后一个遍历到的结点第一个输出。这就是典型的“后进先出”，我们可以用栈实现这种顺序。没经过一个节点的时候，把该结点放到一个栈中。当遍历完整个链表后，再从栈顶开始逐个输出结点的值，此时输出的结点的顺序就翻转过来了。实现代码如下： 12345678910111213141516import java.util.Stack;import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); while (listNode!=null)&#123; stack.push(listNode.val); listNode = listNode.next; &#125; ArrayList&lt;Integer&gt; List = new ArrayList&lt;&gt;(); while(!stack.isEmpty())&#123; List.add(stack.pop()); &#125; return List; &#125;&#125; 既然想到了用栈来实现这个函数，而递归在本质上就是一个栈结构，因此可用递归来实现。要实现反过来输出链表，我们每访问到一个节点的时候，先递归输出它后面的结点，再输出该结点自身，这样链表的输出结果就反过来了。实现代码如下： 12345678910111213141516import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; ArrayList&lt;Integer&gt; list=new ArrayList&lt;Integer&gt;(); ListNode pNode=listNode; if(pNode!=null)&#123; if(pNode.next!=null)&#123; list=printListFromTailToHead(pNode.next); &#125; list.add(pNode.val); &#125; return list; &#125;&#125; 6. 重建二叉树（二叉树）输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。 例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 根据前序遍历的特点，我们知道根结点为1 观察中序遍历。其中root节点G左侧的472必然是root的左子树，G右侧的5386必然是root的右子树。 观察左子树472，左子树的中的根节点必然是大树的root的leftchild。在前序遍历中，大树的root的leftchild位于root之后，所以左子树的根节点为2。 同样的道理，root的右子树节点5386中的根节点也可以通过前序遍历求得。在前序遍历中，一定是先把root和root的所有左子树节点遍历完之后才会遍历右子树，并且遍历的左子树的第一个节点就是左子树的根节点。同理，遍历的右子树的第一个节点就是右子树的根节点。 观察发现，上面的过程是递归的。先找到当前树的根节点，然后划分为左子树，右子树，然后进入左子树重复上面的过程，然后进入右子树重复上面的过程。最后就可以还原一棵树了。 该步递归的过程可以简洁表达如下： 确定根,确定左子树，确定右子树。 在左子树中递归。 在右子树中递归。 打印当前根。 递归代码如下： 123456789101112131415161718192021public class Solution &#123; public TreeNode reConstructBinaryTree(int [] pre,int [] in) &#123; return reConBTree(pre,0,pre.length-1,in,0,in.length-1); &#125; public TreeNode reConBTree(int [] pre,int preleft,int preright,int [] in,int inleft,int inright)&#123; if(preleft &gt; preright || inleft&gt; inright)//当到达边界条件时候返回null return null; //新建一个TreeNode TreeNode root = new TreeNode(pre[preleft]); //对中序数组进行输入边界的遍历 for(int i = inleft; i&lt;= inright; i++)&#123; if(pre[preleft] == in[i])&#123; //重构左子树，注意边界条件 root.left = reConBTree(pre,preleft+1,preleft+i-inleft,in,inleft,i-1); //重构右子树，注意边界条件 root.right = reConBTree(pre,preleft+i+1-inleft,preright,in,i+1,inright); &#125; &#125; return root; &#125;&#125; 7. 用两个栈实现队列（栈与队列）栈是一个非常常见的数据结构，它在计算机领域中被广泛应用，比如操作系统会给每个线程创建一个栈来存储函数调用时各个函数的参数、返回地址及临时变量等。栈的特点是后进先出，即最后被压入（push）栈的元素会第一个被弹出（pop）。 队列是另外一种很重要的数据结构。和栈不同的是，队列的特点是先进先出，即第一个进入队列的元素将会第一个出来。 栈和队列虽然是针锋相对的两个数据结构，但有意思的是他们却相互联系。 通过一个具体的例子来分析往队列插入和删除元素的过程。首先插入一个元素a，不妨先把它插入到stack1，此时stack1中的元素有{a}，stack2为空，再向stack1压入b和c，此时stack1中的元素有{a,b,c}，其中c处于栈顶，而stack2仍然是空的。 因为a是最先进的，最先被删除的元素应该是a，但a位于栈低。我们可以把stack1中的元素逐个弹出并压入stack2，元素在stack2的顺序正好和原来在stack1的顺序相反因此经过三次弹出stack1和压入stack2操作之后，stack1为空，而stack2的元素是{c,b,a}，这时就可以弹出stack2的栈顶a了，随后弹出stack2中的b和c，而这个过程中stack1始终为空. 从上面的分析我们可以总结出删除一个元素的步骤：当stack2中不为空时，在stack2的栈顶元素是最先进入队列的元素，可以弹出。如果stack2为空时，我们把stack1中的元素逐个弹出并压入stack2。由于先进入队列的元素被压到stack1的底端，经过弹出和压入之后就处于stack2的顶端了，又可以直接弹出。 1234567891011121314151617181920import java.util.Stack;public class Solution &#123; Stack&lt;Integer&gt; stack1 = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; stack2 = new Stack&lt;Integer&gt;(); public void push(int node) &#123; stack1.push(node); &#125; public int pop() &#123; while(!stack2.isEmpty())&#123; return stack2.pop(); &#125; while(!stack1.isEmpty())&#123; stack2.push(stack1.pop()); &#125; return stack2.pop(); &#125;&#125; 8. 旋转数组的最小数字(数组)在准备面试的时候，我们应该重点掌握二分查找、归并排序和快速排序，做到能随时正确、完整地写出它们的代码。 若面试题是要求在排序的数组（或部分排序的数组）中查找一个数字或者统计某个数字出现的次数，我们都可以尝试用二分查找算法。 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 可以采用二分法解答这个问题， mid = low + (high - low)/2 ，需要考虑三种情况： array[mid] &gt; array[high]:出现这种情况的array类似[3,4,5,6,0,1,2]，此时最小数字一定在mid的右边。low = mid + 1 array[mid] == array[high]: 出现这种情况的array类似 [1,0,1,1,1] 或者[1,1,1,0,1]，此时最小数字不好判断在mid左边，还是右边,这时只好一个一个试，low = low + 1 或者 high = high - 1 array[mid] &lt; array[high]: 出现这种情况的array类似[2,2,3,4,5,6,6],此时最小数字一定就是array[mid]或者在mid的左边。因为右边必然都是递增的。 high = mid。注意这里有个坑：如果待查询的范围最后只剩两个数，那么mid一定会指向下标靠前的数字，比如 array = [4,6]，array[low] = 4 ;array[mid] = 4 ; array[high] = 6 ; 如果high = mid - 1，就会产生错误， 因此high = mid，但情形(1)中low = mid + 1就不会错误。 代码如下： java 12345678910111213141516171819202122import java.util.ArrayList;public class Solution &#123; public int minNumberInRotateArray(int [] array) &#123; int low = 0; int high = array.length-1; while(low&lt;high)&#123; int mid = low+(high-low)/2; if(array[mid]&gt;array[high])&#123; low=mid+1; &#125; else if(array[mid]==array[high])&#123; high=high-1; &#125; else&#123; high = mid; &#125; &#125; return array[low]; &#125;&#125; 9. 斐波那契数列(数组)9.1 斐波那契数列大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。n&lt;=39。这个题可以说是迭代（Iteration） VS 递归（Recursion），f(n) = f(n-1) + f(n-2)，第一眼看就是递归啊，简直完美的递归环境，递归肯定很爽，这样想着关键代码两三行就搞定了，注意这题的n是从0开始的： 12if(n&lt;=1) return n;else return Fibonacci(n-1)+Fibonacci(n-2); 然而并没有什么用，测试用例里肯定准备着一个超大的n来让Stack Overflow，为什么会溢出？因为重复计算，而且重复的情况还很严重，举个小点的例子，n=4，看看程序怎么跑的： 123Fibonacci(4) = Fibonacci(3) + Fibonacci(2); = Fibonacci(2) + Fibonacci(1) + Fibonacci(1) + Fibonacci(0); = Fibonacci(1) + Fibonacci(0) + Fibonacci(1) + Fibonacci(1) + Fibonacci(0); 由于我们的代码并没有记录Fibonacci(1)和Fibonacci(0)的结果，对于程序来说它每次递归都是未知的，因此光是n=4时f(1)就重复计算了3次之多。 更简单的办法是从下往上计算，首先根据f(0)和f(1)算出f(2)，再根据f(1)和f(2)算出f(3)……依此类推就可以算出第n项了。很容易理解，这种思路的时间复杂度是O(n)。实现代码如下： java 1234567891011121314151617public class Solution &#123; public int Fibonacci(int n) &#123; if(n==0) return 0; if(n==1) return 1; int num1 = 0; int num2 = 1; int fibN=0; for(int i=2;i&lt;=n;++i)&#123; fibN=num1+num2; num1=num2; num2=fibN; &#125; return fibN; &#125;&#125; 9.2 跳台阶一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级台阶总共有多少种跳法。 我们把n级台阶的跳法看成是n的函数，记为f(n)。当n&gt;2时，第一次跳的时候就有两种不同的选择：一是第一次只跳1级，此时跳法数目等于后面剩下的n-1级台阶的跳法数目，即为f(n-1)；另外一种选择是第一次跳2级，此时跳法数目等于后面剩下的n-2级台阶的跳法数目，即为f(n-2)，因此n级台阶的不同跳法的总数f(n)=f(n-1)+f(n-2)。分析到这里，我们不难看出这实际上是斐波那契数列了。代码如下： java 1234567891011121314151617181920public class Solution &#123; public int JumpFloor(int target) &#123; if(target == 0) return 0; if(target == 1) return 1; if(target == 2) return 2; int num1 = 0; int num2 = 1; int jump = 0; for(int i=0;i&lt;target;i++)&#123; jump = num1+num2; num1=num2; num2=jump; &#125; return jump; &#125;&#125; 9.3 变态跳台阶一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 因为n级台阶，第一步有n种跳法：跳1级、跳2级、到跳n级。跳1级，剩下n-1级，则剩下跳法是f(n-1)，跳2级，剩下n-2级，则剩下跳法是f(n-2)。所以f(n)=f(n-1)+f(n-2)+…+f(1)，因为f(n-1)=f(n-2)+f(n-3)+…+f(1)，所以f(n)=2*f(n-1) java 1234567891011public class Solution &#123; public int JumpFloorII(int target) &#123; if(target==0) return 0; if(target==1) return 1; else&#123; return 2*JumpFloorII(target-1); &#125; &#125;&#125; 9.4 矩形覆盖我们可以用$21$的小矩形横着或者竖着去覆盖更大的矩形。请问用n个$21$的小矩形无重叠地覆盖一个$2*n$的大矩形，总共有多少种方法？ 把$28$的覆盖方法记为f(8)。用一个$12$小矩形去覆盖大矩形的最左边有两个选择。竖着放或者横着放。当竖着放时，右边剩下$27$的区域，记为f(7)。横着放时，当$12$的小矩阵横着放在左上角的时候，左下角必须横着放一个$12$的小矩阵，剩下$26$，记为f(6)，因此f(8)=f(7)+f(6)。此时可以看出，仍然是斐波那契数列。 代码如下： java 1234567891011121314151617public class Solution &#123; public int RectCover(int target) &#123; if(target==0) return 0; if(target==1) return 1; int num1=0; int num2=1; int cover =0; for(int i=0;i&lt;target;i++)&#123; cover = num1+num2; num1=num2; num2=cover; &#125; return cover; &#125;&#125; 10. 二进制中1的个数(位运算)输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 如果一个整数不为0，那么这个整数至少有一位是1。如果我们把这个整数减1，那么原来处在整数最右边的1就会变为0，原来在1后面的所有的0都会变成1(如果最右边的1后面还有0的话)。其余所有位将不会受到影响。举个例子：一个二进制数1100，从右边数起第三位是处于最右边的一个1。减去1后，第三位变成0，它后面的两位0变成了1，而前面的1保持不变，因此得到的结果是1011.我们发现减1的结果是把最右边的一个1开始的所有位都取反了。这个时候如果我们再把原来的整数和减去1之后的结果做与运算，从原来整数最右边一个1那一位开始所有位都会变成0。如1100&amp;1011=1000.也就是说，把一个整数减去1，再和原整数做与运算，会把该整数最右边一个1变成0.那么一个整数的二进制有多少个1，就可以进行多少次这样的操作。 java 12345678910public class Solution &#123; public int NumberOf1(int n) &#123; int count =0; while(n!=0)&#123; count++; n=(n-1)&amp;n; &#125; return count; &#125;&#125; 11. 数值的整数次方（位运算）12345678910111213141516171819202122public class Solution &#123; public double Power(double base, int n) &#123; double res = 1,curr = base; int exponent; if(n&gt;0)&#123; exponent = n; &#125;else if(n&lt;0)&#123; if(base==0) throw new RuntimeException(&quot;分母不能为0&quot;); exponent = -n; &#125;else&#123;// n==0 return 1;// 0的0次方 &#125; while(exponent!=0)&#123; if((exponent&amp;1)==1) res*=curr; curr*=curr;// 翻倍 exponent&gt;&gt;=1;// 右移一位 &#125; return n&gt;=0?res:(1/res); &#125;&#125; 12. 打印1到最大的n位数（null）13. 在O(1)时间删除链表结点（链表）给定单向链表的头指针和一个结点指针，定义一个函数在O(1)时间删除该结点。 我们要删除结点i，先把i的下一个结点i.next的内容复制到i，然后在把i的指针指向i.next结点的下一个结点即i.next.next，它的效果刚好是把结点i给删除了。 此外还要考虑删除的结点是头尾结点、链表中只有一个结点、链表为空这几种情况。 java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class DeleteNode &#123; /** * 链表结点 */ public static class ListNode &#123; int value; // 保存链表的值 ListNode next; // 下一个结点 &#125; /** * 给定单向链表的头指针和一个结点指针，定义一个函数在0(1)时间删除该结点, * 【注意1：这个方法和文本上的不一样，书上的没有返回值，这个因为JAVA引用传递的原因， * 如果删除的结点是头结点，如果不采用返回值的方式，那么头结点永远删除不了】 * 【注意2：输入的待删除结点必须是待链表中的结点，否则会引起错误，这个条件由用户进行保证】 * * @param head 链表表的头 * @param toBeDeleted 待删除的结点 * @return 删除后的头结点 */ public static ListNode deleteNode(ListNode head, ListNode toBeDeleted) &#123; // 如果输入参数有空值就返回表头结点 if (head == null || toBeDeleted == null) &#123; return head; &#125; // 如果删除的是头结点，直接返回头结点的下一个结点 if (head == toBeDeleted) &#123; return head.next; &#125; // 下面的情况链表至少有两个结点 // 在多个节点的情况下，如果删除的是最后一个元素 if (toBeDeleted.next == null) &#123; // 找待删除元素的前驱 ListNode tmp = head; while (tmp.next != toBeDeleted) &#123; tmp = tmp.next; &#125; // 删除待结点 tmp.next = null; &#125; // 在多个节点的情况下，如果删除的是某个中间结点 else &#123; // 将下一个结点的值输入当前待删除的结点 toBeDeleted.value = toBeDeleted.next.value; // 待删除的结点的下一个指向原先待删除引号的下下个结点，即将待删除的下一个结点删除 toBeDeleted.next = toBeDeleted.next.next; &#125; // 返回删除节点后的链表头结点 return head; &#125; 14. 调整数组顺序使奇数位于偶数前面（排序） 书上的方法类似于快排，但快排是不稳定的，即其相对位置会发生变化。 java 1234567891011121314151617181920public class Solution &#123; public void reOrderArray(int [] array) &#123; int length = array.length; if(array==null||length==0) return; int left = 0; int right = length-1; while(left&lt;right)&#123; while(left&lt;right&amp;&amp;array[left]%2==1)&#123; left++; &#125; while(left&lt;right&amp;&amp;array[right]%2==0)&#123; right--; &#125; int temp =array[right]; array[right]=array[left]; array[left]=temp; &#125; &#125;&#125; 这里要保证奇数和奇数，偶数和偶数之间的相对位置不变。可以使用插入排序的思想 java 123456789101112131415161718public class Solution &#123; public void reOrderArray(int [] array) &#123; int length = array.length; if(array==null||length==0) return; for(int i=1;i&lt;length;i++)&#123; if(array[i]%2==1)&#123; int curr = array[i]; int j=i-1; while(j&gt;=0&amp;&amp;array[j]%2==0)&#123; array[j+1]=array[j]; j--; &#125; array[j+1]=curr; &#125; &#125; &#125;&#125; 15. 链表中倒数第K个结点（链表）输入一个链表，输出该链表中倒数第k个结点。为了符合大多数人的习惯，本题从1 开始计数，即链表的尾结点是倒数第1个结点。例如一个链表有6个结点，从头结点开始它们的值依次是1、2、3、4、5、6。这个链表的倒数第3个结点的值为4的结点。 很自然的想法是先走到链表尾端，再从尾端回溯k步。可是我们从链表结点的定义可以看出本题中的链表是单向链表，单向链表的结点只有从前向后的指针而没有从后往前的指针，这种思路行不通。 既然不能从尾结点开始遍历链表，我们还是把思路回到头结点上来。假设整个链表有n个结点，那么倒数第k个结点就是从头结点开始往后走n-k+1步就可以了。如何得到结点树n？只需要从头开始遍历链表，每经过一个结点，计数器加1就行了。 也就是说我们需要遍历链表两次，第一次统计出链表中的结点的个数，第二次就能找到倒数第k个结点。但是面试官期待的解法是只需要遍历链表一次。 为了实现只遍历链表一次就能找到倒数第k个结点，我们可以定义两个指针。第一个指针从链表的头指针开始遍历向前走k-1步，第二个指针保持不动；从第k步开始，第二个指针也开始从链表的头指针开始遍历。由于两个指针的距离保持在k-1，当第一个（走在前面的）指针到达链表的尾结点时，第二个指针（走在后边的）指针正好是倒数第k个结点。 但是这样写出来的代码不够鲁棒，面试官可以找出三种办法让这段代码崩溃： 输入的ListHead为空指针。由于代码会试图访问空指针指向的内存，程序崩溃。 输入的以ListHead为头结点的链表的结点总数少于k。由于在for循环中会在链表上向前走k-1步，仍然会由于空指针造成的程序奔溃。 输入的参数k为0.由于k是一个无符号整数，那么在for循环中k-1得到的将不是-1，而是4294967295（无符号的0xFFFFFFFFF），因此for循环执行的次数远远超过我们的预计，同样也会造成程序崩溃。 面试过程中写代码特别要注意鲁棒性，若写出的代码存在多处崩溃的风险，那我们很可能和offer失之交臂。针对前面三个问题，分别处理。若输入的链表头指针为null，那么整个链表为空，此时查找倒数第k个结点自然应该返回null。若输入的k为0，也就是试图查找倒数第0个结点，由于我们计数是从1开始的，因此输入0是没有实际意义，也可以返回null。若链表的结点数少于k，在for循环中遍历链表可能会出现指向null的next，因此我们在for循环中应该加一个if循环。 代码如下： java版本 12345678910111213141516171819202122232425262728293031/*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode FindKthToTail(ListNode head,int k) &#123; if(head==null||k &lt;=0)&#123;return null;&#125; ListNode pAhead = head; ListNode pBehind = head; for(int i=1;i&lt;k;i++)&#123; if(pAhead.next != null) &#123;pAhead = pAhead.next;&#125; else &#123;return null;&#125; &#125; while(pAhead.next!=null) &#123; pAhead = pAhead.next; pBehind = pBehind.next; &#125; return pBehind; &#125;&#125; python版本 12345678910111213141516171819202122# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def FindKthToTail(self, head, k): # write code here if not head or k == 0: return None pAhead = head pBehind = None for i in xrange(0,k-1): if pAhead.next != None: pAhead = pAhead.next else: return None pBehind = head while pAhead.next != None: pAhead = pAhead.next pBehind = pBehind.next return pBehind 16. 反转链表（链表）定义一个函数，输入一个链表的头结点，反转该链表并输出反转后的头结点。链表结点定义如下： 12345678public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125; 解决与链表相关的问题总是有大量的指针操作，而指针操作的代码总是容易出错的。 为了正确地反转一个链表，需要调整链表中指针的方向。为了将调整指针这个复杂的过程分析清楚，可以借助图形来直观分析。在下图所示的链表中，h、i、j是3个相邻的结点。假设经过若干操作，我们已经把结点h之前的指针调整完毕，这些结点的next指向h，此时链表的结果如下所示： 其中（a）为一个链表，（b）把i之前的所有结点的next都指向前一个结点，导致链表在结点i、j之间断裂。 不难注意到，由于结点i的next指向了它的前一个结点，导致我们无法再链表中遍历到结点j。为了避免链表在结点i处断开，我们需要在调整结点i的next之前把结点j保存下来。 也就是说我们在调整结点i的next指针时，除了需要知道结点i本身之外，还需要前一个结点h，因为我们需要把结点i的next指向结点h。同时，我们还事先需要保存i的一个结点j，以防止链表断开。因此相应地我们需要定义3个指针，分别指向当前遍历到的结点、它的前一个结点及后一个结点。 最后我们试着找到反转后链表的头结点。不难分析出反转后链表的头结点是原始链表的尾结点。什么结点是尾结点？自然是next为null的结点。 pre\rightarrow head \rightarrow next先保存next，即$next = head.next$再反转head的指针$head.next=pre $，链表结构变成 pre\leftarrow head \ \ \ next接着向后移动结点$pre=head,head=next$ 实现代码如下： java版本 1234567891011121314151617181920212223242526272829303132public class Solution &#123; public ListNode ReverseList(ListNode head) &#123; if(head==null) return null; //head为当前节点，如果当前节点为空的话，那就什么也不做，直接返回null； ListNode pre = null; ListNode next = null; //当前节点是head，pre为当前节点的前一节点，next为当前节点的下一节点 //需要pre和next的目的是让当前节点从pre-&gt;head-&gt;next1-&gt;next2变成pre&lt;-head next1-&gt;next2 //即pre让节点可以反转所指方向，但反转之后如果不用next节点保存next1节点的话，此单链表就此断开了 //所以需要用到pre和next两个节点 //1-&gt;2-&gt;3-&gt;4-&gt;5 //1&lt;-2&lt;-3 4-&gt;5 while(head!=null)&#123; //做循环，如果当前节点不为空的话，始终执行此循环，此循环的目的就是让当前节点从指向next到指向pre //如此就可以做到反转链表的效果 //先用next保存head的下一个节点的信息，保证单链表不会因为失去head节点的原next节点而就此断裂 next = head.next; //保存完next，就可以让head从指向next变成指向pre了，代码如下 head.next = pre; //head指向pre后，就继续依次反转下一个节点 //让pre，head，next依次向后移动一个节点，继续下一次的指针反转 pre = head; head = next; &#125; //如果head为null的时候，pre就为最后一个节点了，但是链表已经反转完毕，pre就是反转后链表的第一个节点 //直接输出pre就是我们想要得到的反转后的链表 return pre; &#125;&#125; python版本 123456789101112131415161718# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回ListNode def ReverseList(self, pHead): # write code here if not pHead or not pHead.next: return pHead pre = None while pHead: next1 = pHead.next pHead.next = pre pre = pHead pHead = next1 return pre 17. 合并两个排序的链表（链表）输入两个递增排序的链表，合并这两个链表并使新链表中的结点仍然是按照递增排序的。例如下图中的链表1和链表2，则合并之后的升序链表3如下所示： 这是一个经常被各公司采用的面试题。在面试过程中，最容易犯两种错误：一是在写代码之前没有对合并的过程想清楚，最终合并出来的链表要么中间断开了，要么并没有做到递增排序；二是代码在鲁棒性方面存在问题，程序一旦有特殊的输入（如空链表）就会奔溃。首先分析合并两个链表的过程。从合并两个链表的头结点开始。链表1的头结点的值小于链表2的头结点的值，因此链表1的头结点将是合并后链表的头结点。 继续合并剩余的结点。在两个链表中剩下的结点依然是排序的，因此合并这两个链表的步骤和前面的步骤是一样的。依旧比较两个头结点的值。此时链表2的头结点值小于链表1的头结点的值，因此链表2的头结点的值将是合并剩余结点得到的链表的头结点。把这个结点和前面合并链表时得到的链表的尾结点链接起来。 当我们得到两个链表中值较小的头结点并把它链接到已经合并的链表之后，两个链表剩余的结点依然是排序的，因此合并的步骤和之前的步骤是一样的。这是典型的递归过程，我们可以定义递归函数完成这一合并过程。（解决这个问题需要大量的指针操作，如没有透彻地分析问题形成清晰的思路，很难写出正确的代码） 接下来解决鲁棒性问题，每当代码试图访问空指针指向的内存时程序就会奔溃，从而导致鲁棒性问题。本题中一旦输入空的链表就会引入空的指针，因此我们要对空链表单独处理。当第一个链表是空链表，也就是它的头结点是一个空指针时，和第二个链表合并的结果就是第二个链表。同样，当输入的第二个链表的头结点是空指针的时候，和第一个链表合并得到的结果就是第一个链表。如果两个链表都为空，合并得到的是一个空链表。（由于有大量的指针操作，如果稍有不慎就会在代码中遗留很多与鲁棒性相关的隐患。建议应聘者在写代码之前全面分析哪些情况会引入空指针，并考虑清楚怎么处理这些空指针。） 代码如下： java版本 123456789101112131415161718192021222324252627/*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode Merge(ListNode list1,ListNode list2) &#123; if (list1==null) return list2; else if (list2==null) return list1; ListNode MergeHead = null; if (listval&lt;=list2.val)&#123; MergeHead = list1; MergeHead.next = Merge(listnext,list2); &#125; else &#123;MergeHead = list2; MergeHead.next = Merge(list1,list2.next); &#125; return MergeHead; &#125;&#125; python版本 123456789101112131415161718192021# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回合并后列表 def Merge(self, pHead1, pHead2): # write code here if pHead1== None: return pHead2 if pHead2== None: return pHead1 MergeHead = None if pHeadval &lt; pHead2.val: MergeHead = pHead1 MergeHead.next = self.Merge(pHeadnext,pHead2) else: MergeHead = pHead2 MergeHead.next = self.Merge(pHead1,pHead2.next) return MergeHead 18. 树的子结构（二叉树）输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） 要查找树A中是否存在和树B结构一样的子树，我们可以分成两步：第一步在树A中找到和B的根结点的值一样的结点R，第二步再判断树A以R为根结点的子树是不是包含和树B一样的结构。 第一步在树A中查找与根结点的值一样的结点，实际上就是树的遍历。对二叉树这种数据结构熟悉的读者自然知道可以用递归的方法去遍历，也可以用循环的方法去遍历。由于递归的代码实现比较简洁，面试时如果没有特别要求，通常会采用递归的方式。参考代码如下： java第一步 123456789101112131415161718192021public boolean HasSubtree(TreeNode root1,TreeNode root2) &#123; boolean result = false; //一定要注意边界条件的检查，即检查空指针。否则程序容易奔溃，面试时尤其要注意。这里当Tree1和Tree2都不为零的时候，才进行比较。否则直接返回false if(root1!=null&amp;&amp;root2!=null)&#123; ////如果找到了对应Tree2的根节点的点 if(root1.val==root2.val)&#123; //以这个根节点为为起点判断是否包含Tree2 result = DoesTree1HaveTree2(root1,root2); &#125; //如果找不到，那么就再去root的左儿子当作起点，去判断是否包含Tree2 if(!result)&#123; result=HasSubtree(root1.left,root2); &#125; //如果还找不到，那么就再去root的右儿子当作起点，去判断是否包含Tree2 if(!result)&#123; result=HasSubtree(root1.right,root2); &#125; &#125; return result; &#125; 第二步是判断树A中以R为根结点的子树是不是和树B具有相同的结构。同样，我们也可以用递归的思路来考虑：如果结点R的值和树B的根结点不同，则以R为根结点的子树和树B一定不具有相同的结点；如果他们的值相同，则递归地判断它们各自的左右结点的值是不是相同。递归的终止条件是我们达到了树A或者树B的叶结点。 代码如下： java 12345678910111213141516public boolean DoesTree1HaveTree2(TreeNode root1,TreeNode root2)&#123; //如果Tree2已经遍历完了都能对应的上，返回true if(root2==null)&#123; return true; &#125; //如果Tree2还没有遍历完，Tree1却遍历完了。返回false if(root1==null)&#123; return false; &#125; //如果其中有一个点没有对应上，返回false if(root1.val!=root2.val)&#123; return false; &#125; //如果根节点对应的上，那么就分别去左右子节点里面匹配 return DoesTree1HaveTree2(root1.left,root2.left)&amp;&amp;DoesTree1HaveTree2(root1.right,root2.right); &#125; 二叉树相关的代码有大量的指针操作，每一次使用指针的时候，我们都要问自己这个指针有没有可能是NULL，如果是NULL该怎么处理。 19. 二叉树的镜像（二叉树）操作给定的二叉树，将其变换为源二叉树的镜像。 123456789101112131415161718192021public class Solution &#123; public void Mirror(TreeNode root) &#123; //边界 if(root==null) return; if(root.left==null&amp;&amp;root.right==null) return; //交换左右子树 TreeNode temp = root.left; root.left=root.right; root.right=temp; //递归 if(root.left!=null)&#123; Mirror(root.left); &#125; if(root.right!=null)&#123; Mirror(root.right); &#125; &#125;&#125; 20. 顺时针打印矩阵（数组）输入一个矩阵，按照从外向里以顺时针依次打印出每一个数字。例如，输入如下矩阵： java 12345678910111213141516171819202122232425262728import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printMatrix(int [][] matrix) &#123; int row = matrix.length; int col = matrix[0].length; ArrayList&lt;Integer&gt; result = new ArrayList&lt;Integer&gt; (); // 输入的二维数组非法，返回空的数组 if(row==0&amp;&amp;col==0)return result; // 定义四个关键变量，表示左上和右下的打印范围 int left =0,top=0,right=col-1,bottom=row-1; while(left&lt;=right&amp;&amp;top&lt;=bottom)&#123; // left to right for(int i=left;i&lt;=right;i++)&#123;result.add(matrix[top][i]);&#125; // top to bottom for(int i=top+1;i&lt;=bottom;i++)&#123;result.add(matrix[i][right]);&#125; // right to left if(top!=bottom)&#123; for(int i=right-1;i&gt;=left;i--)&#123;result.add(matrix[bottom][i]);&#125;&#125; // bottom to top if(left!=right)&#123; for(int i=bottom-1;i&gt;=top+1;i--)&#123;result.add(matrix[i][left]);&#125;&#125; left++;right--;top++;bottom--; &#125; return result; &#125; &#125; 21.包含min函数的栈（栈）定义栈的数据结构，请在该类型中实现一个能够得到栈最小元素的min函数。在该栈中，调用min、push及pop的时间复杂度都是O(1)。 可以利用一个辅助栈来存放最小值 每入栈一次，就与辅助栈顶比较大小，如果小就入栈，如果大就入栈当前的辅助栈顶 。 当出栈时，辅助栈也要出栈 这种做法可以保证辅助栈顶一定都是最小元素。 12345678910111213141516171819202122232425import java.util.Stack;public class Solution &#123; Stack&lt;Integer&gt; data = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; min = new Stack&lt;Integer&gt;(); public void push(int node) &#123; data.push(node); if(min.empty())&#123;min.push(data.peek());&#125; else if(data.peek()&lt;min.peek())&#123;min.push(data.peek());&#125; else min.push(min.peek()); &#125; public void pop() &#123; data.pop(); min.pop(); &#125; public int top() &#123; return data.peek(); &#125; public int min() &#123; return min.peek(); &#125;&#125; 22. 栈的压入、弹出序列（栈）输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4，5，3，2，1是该压栈序列对应的一个弹出序列，但4，3，5，1，2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 借用一个辅助的栈，遍历压栈顺序，先讲第一个放入栈中，这里是1，然后判断栈顶元素是不是出栈顺序的第一个元素，这里是4，很显然1≠4，所以我们继续压栈，直到相等以后开始出栈，出栈一个元素，则将出栈顺序向后移动一位，直到不相等，这样循环等压栈顺序遍历完成，如果辅助栈还不为空，说明弹出序列不是该栈的弹出顺序。举例：入栈1,2,3,4,5出栈4,5,3,2,1首先1入辅助栈，此时栈顶1≠4，继续入栈2此时栈顶2≠4，继续入栈3此时栈顶3≠4，继续入栈4此时栈顶4＝4，出栈4，弹出序列向后一位，此时为5，,辅助栈里面是1,2,3此时栈顶3≠5，继续入栈5此时栈顶5=5，出栈5,弹出序列向后一位，此时为3，,辅助栈里面是1,2,3….依次执行，最后辅助栈为空。如果不为空说明弹出序列不是该栈的弹出顺序。 java 1234567891011121314151617import java.util.ArrayList;import java.util.Stack;public class Solution &#123; public boolean IsPopOrder(int [] pushA,int [] popA) &#123; if(pushA.length==0||popA.length==0)return false; Stack&lt;Integer&gt; S=new Stack&lt;Integer&gt;(); int popIndex = 0; for(int i=0;i&lt;pushA.length;i++)&#123; S.push(pushA[i]); while(!S.empty()&amp;&amp;popA[popIndex]==S.peek())&#123; S.pop(); popIndex++; &#125; &#125; return S.empty(); &#125;&#125; 23. 从上往下打印二叉树（二叉树）从上往下打印出二叉树的每个节点，同层节点从左至右打印。 每次打印一个结点时，如果该结点有子结点，则把该结点的子结点放到队列的末尾。接下来到队列的头部取出最早进入队列的结点，重复前面的打印操作。 java 12345678910111213141516171819import java.util.ArrayList;import java.util.LinkedList;public class Solution &#123; public ArrayList&lt;Integer&gt; PrintFromTopToBottom(TreeNode root) &#123; ArrayList&lt;Integer&gt; List=new ArrayList&lt;Integer&gt;(); if(root==null)&#123;return List;&#125; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.add(root);//先把根结点加入队列q while(!queue.isEmpty())&#123;//队列非空时 TreeNode treenode=queue.remove();//取出队列头结点 if(treenode.left!=null)&#123;queue.add(treenode.left);&#125;//向队列加入左孩子（若有） if(treenode.right!=null)&#123;queue.add(treenode.right);&#125;//向队列加入右孩子（若有） List.add(treenode.val);//加到打印列表中 &#125; return List; &#125;&#125; 24. 二叉搜索树的后序遍历序列（二叉树）输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。在后序遍历得到的序列中，最后一个数字是树的根结点的值。数组中前面的数字可以分成两部分：第一部分是左子树结点的值，它们都比根结点小；第二部分是右子树结点的值，它们都比根结点大。 java 1234567891011121314151617181920212223242526272829303132import java.util.Arrays;public class Solution &#123; public boolean VerifySquenceOfBST(int [] sequence) &#123; int length = sequence.length; if(sequence==null||length==0)&#123;return false;&#125; int root = sequence[length-1];//根结点 int i=0; //外部初始化 //找到左子树的最后一个结点位置 for(;i&lt;length-1;i++)&#123; if(sequence[i]&gt;root)&#123; break; &#125; &#125; //如果右子树的结点值小于根结点的值，则返回false for(int j=i;j&lt;length-1;j++)&#123; if(sequence[j]&lt;root)&#123; return false; &#125; &#125; //初始化 boolean left=true; boolean right=true; //递归左右子树 if(i&gt;0)&#123; left = VerifySquenceOfBST(Arrays.copyOfRange(sequence,0,i));//Arrays的copyOfRange方法 &#125; if(i&lt;length-1)&#123; right = VerifySquenceOfBST(Arrays.copyOfRange(sequence,i,length-1)); &#125; return left&amp;&amp;right; &#125;&#125; 25. 二叉树中和为某一值的路径（二叉树）输入一颗二叉树和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。 java 12345678910111213141516171819public class Solution &#123; private ArrayList&lt;ArrayList&lt;Integer&gt;&gt; listAll = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); private ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindPath(TreeNode root,int target) &#123; if(root == null) return listAll; list.add(root.val); target -= root.val;//每次减去结点的值 //如果target等于0，则说明这条路径和为target，添加到listAll中 if(target == 0 &amp;&amp; root.left == null &amp;&amp; root.right == null) listAll.add(new ArrayList&lt;Integer&gt;(list));//因为add添加的是引用，如果不new一个的话，后面的操作会更改listAll中list的值 //向左孩子递归 if(root.left!=null)FindPath(root.left, target); //向右孩子递归 if(root.right!=null)FindPath(root.right, target); //如果不满足条件，则回到父节点； list.remove(list.size()-1); return listAll; &#125;&#125; 26. 复杂链表的复制（链表）输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） java 12345678910111213141516171819202122232425262728293031323334public class Solution &#123; public RandomListNode Clone(RandomListNode pHead) &#123; if (pHead == null) return null; //复制next 如原来是A-&gt;B-&gt;C 变成A-&gt;A'-&gt;B-&gt;B'-&gt;C-&gt;C' RandomListNode pCur = pHead; while (pCur != null) &#123; RandomListNode node = new RandomListNode(pCur.label); node.next = pCur.next; pCur.next = node; pCur = node.next; &#125; //复制random pCur是原来链表的结点 pCur.next是复制pCur的结点 pCur = pHead; while (pCur!=null) &#123; if (pCur.random!=null) pCur.next.random = pCur.random.next; pCur = pCur.next.next; &#125; //拆分链表 RandomListNode head = pHead.next; RandomListNode tmp = head; pCur = pHead; while(pCur.next!=null) &#123; tmp = pCur.next; pCur.next = tmp.next; pCur = tmp; &#125; return head; &#125;&#125; 27. 二叉搜素树与双向链表（二叉树）输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 java 12345678910111213141516171819202122232425public class Solution &#123; TreeNode head = null; TreeNode realHead = null; public TreeNode Convert(TreeNode pRootOfTree) &#123; ConvertSub(pRootOfTree); return realHead//realHead是每个子树排序后的第一个结点，head是排序后的最后一个结点; &#125; private void ConvertSub(TreeNode pRootOfTree) &#123; //递归中序遍历 if(pRootOfTree==null) return; ConvertSub(pRootOfTree.left); if (head == null) &#123; //初始处 head = pRootOfTree; realHead = pRootOfTree; &#125; else &#123; //前两句实现双向，第三句跳到下一个节点。 head.right = pRootOfTree; pRootOfTree.left = head; head = pRootOfTree; &#125; ConvertSub(pRootOfTree.right); &#125;&#125; 28. 字符串的排列（字符串）输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。 首先我要打印abc的全排列，就是第一步把a 和bc交换（得到bac,cab），这需要一个for循环，循环里面有一个swap，交换之后就相当于不管第一步了，进入下一步递归，所以跟一个递归函数， 完成递归之后把交换的换回来，变成原来的字串 12345678abc 为例子：1. 固定a, 求后面bc的全排列： abc, acb。 求完后，a 和 b交换； 得到bac,开始第二轮2. 固定b, 求后面ac的全排列： bac, bca。 求完后，b 和 c交换； 得到cab,开始第三轮3. 固定c, 求后面ba的全排列： cab, cba 即递归树： str: a b c ab ac ba bc ca cb result: abc acb bac bca cab cba java 12345678910111213141516171819202122232425262728293031import java.util.ArrayList;import java.util.*;public class Solution &#123; public ArrayList&lt;String&gt; Permutation(String str) &#123; ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); if(str.length()==0) return list; char[] array = str.toCharArray(); permutation(array,0,list); Collections.sort(list); return list; &#125; public void permutation(char[] array,int begin,ArrayList&lt;String&gt; list) &#123; if(begin == array.length-1) &#123; list.add(String.valueOf(array)); &#125;else &#123; for(int i=begin;i&lt;array.length;++i) &#123; if(i==begin || array[i]!=array[begin]) &#123; swap(array,begin,i); permutation(array,begin+1,list); swap(array,begin,i); &#125; &#125; &#125; &#125; public void swap(char[] array,int i,int j) &#123; char temp = array[i]; array[i] = array[j]; array[j] = temp; &#125;&#125; 29. 数组中出现次数超过一半的数字（数组）数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 数组中有一个数字出现的次数超过数组长度的一半，也就是说它出现的次数比其他所有数字出现的次数的和还要多。因此我们可以考虑在遍历数组的时候保存两个值：一个是数组的一个数字，一个是次数。当我们遍历到下一个数字的时候，如果下一个数字和我们之前保存的数字相同，则次数加1；如果不同，则次数减1；如果次数为0，则保存下一个数字，并把次数设为1。 还要判断这个数字是否超过数组长度的一半，如果不存在输出0。 123456789101112131415161718192021222324252627282930313233public class Solution &#123; public int MoreThanHalfNum_Solution(int [] array) &#123; if(array==null||array.length==0)&#123; return 0; &#125; int result=array[0]; int count=1; for(int i=1;i&lt;array.length;i++)&#123; if(result==array[i])&#123; count++; &#125; else if(result!=array[i])&#123; count--; &#125; if(count==0)&#123; result=array[i]; count=1; &#125; &#125; int times=0; for(int i=0;i&lt;array.length;i++)&#123; if(array[i]==result)&#123; times++; &#125; &#125; if(times*2&lt;=array.length)&#123; System.out.println(times); return 0; &#125; else return result; &#125;&#125; 30. 最小的K个数（数组）输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 第一种方法，借用partition函数 java 1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.ArrayList; public class Solution &#123; public ArrayList&lt;Integer&gt; GetLeastNumbers_Solution(int[] input, int k) &#123; ArrayList&lt;Integer&gt; output = new ArrayList&lt;Integer&gt;(); int length = input.length; if (input == null || length &lt;= 0 || length &lt; k || k&lt;= 0) &#123; return output; &#125; int left = 0; int right = length - 1; int index = partition(input,left,right); while(index != k -1) &#123; if(index &lt; k - 1) &#123; left = index + 1; //不够的话往右边走走 index = partition(input,left,right); &#125; else &#123; right = index - 1; //太多的话往左边走走 index = partition(input,left,right); &#125; &#125; for (int i = 0;i &lt; k;i++) &#123; output.add(input[i]); &#125; return (ArrayList&lt;Integer&gt;) output; &#125; //基准左右分区 private int partition(int[] input,int left,int right) &#123; int pivot = input[left]; while(left &lt; right) &#123; while(input[right] &gt;= pivot &amp;&amp; left &lt; right) &#123; right--; &#125; input[left] = input[right]; while(input[left] &lt;= pivot &amp;&amp; left &lt;right) &#123; left++; &#125; input[right] = input[left]; &#125; input[left] = pivot; return left; &#125; &#125; 第二种方法 用最大堆保存这k个数，每次只和堆顶比，如果比堆顶小，删除堆顶，新数入堆。 java 123456789101112131415161718192021222324252627282930313233343536import java.util.ArrayList;import java.util.PriorityQueue;import java.util.Comparator;public class Solution &#123; public ArrayList&lt;Integer&gt; GetLeastNumbers_Solution(int[] input, int k) &#123; ArrayList&lt;Integer&gt; result = new ArrayList&lt;Integer&gt;(); int length = input.length; if(k &gt; length || k == 0)&#123; return result; &#125; PriorityQueue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;Integer&gt;(k, new Comparator&lt;Integer&gt;() &#123; @Override//PriorityQueue默认是小顶堆，实现大顶堆，需要反转默认排序器 public int compare(Integer o1, Integer o2) &#123; return o2.compareTo(o1); &#125; &#125;); for (int i = 0; i &lt; length; i++) &#123; //如果最大堆中已有的数字少于k个，直接读入 if (maxHeap.size() != k) &#123; maxHeap.offer(input[i]); &#125; //如果最大堆中已有k个数字了，即容器已满，且大顶堆顶大于待插入数字，将待插入数字替换进大顶堆 else if (maxHeap.peek() &gt; input[i]) &#123; Integer temp = maxHeap.poll(); temp = null; maxHeap.offer(input[i]); &#125; &#125; //输出大顶堆中的数 for (Integer integer : maxHeap) &#123; result.add(integer); &#125; return result; &#125;&#125; 31. 连续子数组的最大和（数组）输入一个整型数组，数组中有正数也有负数。数组中一个或连续的多个整数组成一个子数组。求所有子数组的和的最大值。要求时间复杂度为O(n) 第一种方法 java 12345678910111213141516171819public class Solution &#123; public int FindGreatestSumOfSubArray(int[] array) &#123; if(array.length==0||array==null) return 0; int cSum = 0; int result = array[0];// result存储最大和，不能初始为0，存在负数 for(int i=0;i&lt;array.length;i++)&#123; if(cSum&lt;0)&#123; cSum=array[i];// 当前和&lt;0，抛弃不要 &#125;else&#123; cSum += array[i];//否则累加上去 &#125; if(cSum&gt;result)&#123; result = cSum;// 存储当前的最大结果 &#125; &#125; return result; &#125;&#125; 第二种方法：动态规划 1234567891011121314151617181920212223F（i）：以array[i]为末尾元素的子数组的和的最大值，子数组的元素的相对位置不变 F（i）=max（F（i-1）+array[i] ， array[i]） res：所有子数组的和的最大值 res=max（res，F（i）） 如数组[6, -3, -2, 7, -15, 1, 2, 2] 初始状态： F（0）=6 res=6 i=1： F（1）=max（F（0）-3，-3）=max（6-3，3）=3 res=max（F（1），res）=max（3，6）=6 i=2： F（2）=max（F（1）-2，-2）=max（3-2，-2）=1 res=max（F（2），res）=max（1，6）=6 i=3： F（3）=max（F（2）+7，7）=max（1+7，7）=8 res=max（F（2），res）=max（8，6）=8 i=4： F（4）=max（F（3）-15，-15）=max（8-15，-15）=-7 res=max（F（4），res）=max（-7，8）=8 以此类推 最终res的值为8 java 1234567891011public class Solution &#123; public int FindGreatestSumOfSubArray(int[] array) &#123; int res = array[0]; int max = array[0]; for(int i=1;i&lt;array.length;i++)&#123; max=Math.max(max+array[i],array[i]); res = Math.max(max,res); &#125; return res; &#125;&#125; 32.从1到n整数中1出现的次数（数组）输入一个整数n，求1到n这n个整数的十进制表示中1出现的次数。例如输入12，从1到12这些整数中包含1的数字有1、10、11、12，1一共出现了5次。 一、1的数目 编程之美上给出的规律： 如果第i位（自右至左，从1开始标号）上的数字为0，则第i位可能出现1的次数由更高位决定（若没有高位，视高位为0），等于更高位数字X当前位数的权重$10^{i-1}$。 如果第i位上的数字为1，则第i位上可能出现1的次数不仅受更高位影响，还受低位影响（若没有低位，视低位为0），等于更高位数字X当前位数的权重$10^{i-1}+$（低位数字+1）。 如果第i位上的数字大于1，则第i位上可能出现1的次数仅由更高位决定（若没有高位，视高位为0），等于（更高位数字+1）X当前位数的权重$10^{i-1}$。 二、X的数目这里的 X∈[1,9] ，因为 X=0 不符合下列规律，需要单独计算。首先要知道以下的规律： 从 1 至 10，在它们的个位数中，任意的 X 都出现了 1 次。 从 1 至 100，在它们的十位数中，任意的 X 都出现了 10 次。 从 1 至 1000，在它们的百位数中，任意的 X 都出现了 100 次。 依此类推，从 1 至 $10^i$ ，在它们的左数第二位（右数第 i 位）中，任意的 X 都出现了 $10^{i−1}$ 次。 这个规律很容易验证，这里不再多做说明。 接下来以 n=2593,X=5 为例来解释如何得到数学公式。从 1 至 2593 中，数字 5 总计出现了 813 次，其中有 259 次出现在个位，260 次出现在十位，294 次出现在百位，0 次出现在千位。 现在依次分析这些数据，首先是个位。从 1 至 2590 中，包含了 259 个 10，因此任意的 X 都出现了 259 次。最后剩余的三个数 2591, 2592 和 2593，因为它们最大的个位数字 3 &lt; X，因此不会包含任何 5。（也可以这么看，3&lt;X，则个位上可能出现的X的次数仅由更高位决定，等于更高位数字$（259）\times 10^{1-1}=259$）。 然后是十位。从 1 至 2500 中，包含了 25 个 100，因此任意的 X 都出现了 25×10=250 次。剩下的数字是从 2501 至 2593，它们最大的十位数字9&gt;X，因此会包含全部10个5。最后总计250 + 10 = 260。（也可以这么看，9&gt;X，则十位上可能出现的X的次数仅由更高位决定，等于更高位数字$（25+1）\times 10^{2-1}=260$）。 接下来是百位。从 1 至 2000 中，包含了 2 个 1000，因此任意的 X 都出现了 2×100=200 次。剩下的数字是从 2001 至 2593，它们最大的百位数字 5 == X，这时情况就略微复杂，它们的百位肯定是包含 5 的，但不会包含全部 100 个。如果把百位是 5 的数字列出来，是从 2500 至 2593，数字的个数与百位和十位数字相关，是 93+1 = 94。最后总计 200 + 94 = 294。（也可以这么看，5==X，则百位上可能出现X的次数不仅受更高位影响，还受低位影响，等于更高位数字$（2）\times 10^{3-1}+（93+1）=294$）。 最后是千位。现在已经没有更高位，因此直接看最大的千位数字 2 &lt; X，所以不会包含任何 5。（也可以这么看，2&lt;X，则千位上可能出现的X的次数仅由更高位决定，等于更高位数字$（0）\times 10^{4-1}=0$）。 到此为止，已经计算出全部数字 5 的出现次数。总结一下以上的算法，可以看到，当计算右数第 i 位包含的 X 的个数时： 取第 i 位左边（高位）的数字，乘以$10^{i−1}$ ，得到基础值a 。 取第 i 位数字，计算修正值： 如果大于 X，则结果为 $a+ 10^{i−1}$ 。 如果小于 X，则结果为 a 。 如果等 X，则取第 i 位右边（低位）数字，设为 b ，最后结果为 a+b+1 。 相应的代码非常简单，效率也非常高，时间复杂度只有 $ O( log_ {10} n) $。 代码如下： 12345678910111213141516171819202122public int NumberOfXBetween1AndN_Solution(int n,int x) &#123; if(n&lt;0||x&lt;1||x&gt;9) return 0; int high,low,curr,tmp,i = 1; high = n; int total = 0; while(high!=0)&#123; high = n/(int)Math.pow(10, i);// 获取第i位的高位 tmp = n%(int)Math.pow(10, i); curr = tmp/(int)Math.pow(10, i-1);// 获取第i位 low = tmp%(int)Math.pow(10, i-1);// 获取第i位的低位 if(curr==x)&#123; total+= high*(int)Math.pow(10, i-1)+low+1; &#125;else if(curr&lt;x)&#123; total+=high*(int)Math.pow(10, i-1); &#125;else&#123; total+=(high+1)*(int)Math.pow(10, i-1); &#125; i++; &#125; return total; &#125; 33. 把数组排成最小的数(数组)输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 java 123456789101112131415161718192021222324252627282930import java.util.ArrayList;import java.util.Collections;import java.util.Comparator;public class Solution &#123; public String PrintMinNumber(int [] numbers) &#123; int n; String s=""; ArrayList&lt;Integer&gt; list=new ArrayList&lt;Integer&gt;(); n=numbers.length; for(int i=0;i&lt;n;i++)&#123; list.add(numbers[i]);//将数组放入arrayList中 &#125; //实现了Comparator接口的compare方法，将集合元素按照compare方法的规则进行排序 Collections.sort(list,new Comparator&lt;Integer&gt;()&#123; @Override public int compare(Integer str1, Integer str2) &#123; // TODO Auto-generated method stub String s1=str1+""+str2; String s2=str2+""+str1; return s1.compareTo(s2); &#125; &#125;); for(int j:list)&#123; s+=j; &#125; return s; &#125;&#125; 34. 丑数（数组）把只包含因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 java 12345678910111213141516171819import java.util.*;public class Solution &#123; public int GetUglyNumber_Solution(int index) &#123; if(index&lt;7)return index; int[] res = new int[index]; res[0] = 1; int t2 = 0, t3 = 0, t5 = 0, i; for(i=1;i&lt;index;i++)&#123; res[i] = min(res[t2]*2,min(res[t3]*3,res[t5]*5)); if(res[i] == res[t2]*2)t2++; if(res[i] == res[t3]*3)t3++; if(res[i] == res[t5]*5)t5++; &#125; return res[index-1]; &#125; private int min(int a,int b)&#123; return (a&gt;b)? b:a; &#125;&#125; 35. 第一次只出现一次的字符（字符串）在一个字符串(1&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置. 我们可以使用一个容器来存放每个字符的出现次数。在这个数据容器中可以根据字符来查找出现的次数，也就是这个容器的作用是把一个字符映射成一个数字。在常用的数据容器中，哈希表正是这个用途。 为了解决这个问题，我们可以定义哈希表的键值（Key）是字符，而值（Value）是该字符出现的次数。同时我们还需要从头开始扫描字符串两次。第一次扫面字符串时，每扫到一个字符就在哈希表的对应项把次数加1.接下来第二次扫描时，每扫描到一个字符就能在哈希表中得到该字符出现的次数，这样第一个只出现一次的字符就是符合要求的输出。 需要涉及到Java中HashMap工作原理及实现，资料链接 java 1234567891011121314151617181920212223import java.util.HashMap;public class Solution &#123; public int FirstNotRepeatingChar(String str) &#123; HashMap&lt;Character,Integer&gt; map = new HashMap&lt;Character,Integer&gt;(); for(int i=0;i&lt;str.length();i++)&#123; char c = str.charAt(i);//charAt方法，获得位置i的串 if(map.containsKey(c))&#123;//HashMap的containKey方法； int time = map.get(c);//HashMap的get方法，得到Key c的Value； time++; map.put(c,time);//HashMap的put方法，将Key c的Value置为time； &#125;else&#123; map.put(c,1); &#125; &#125; for(int i=0;i&lt;str.length();i++)&#123; char c = str.charAt(i); if(map.get(c)==1)&#123; return i; &#125; &#125; return -1; &#125;&#125; 36. 数组中的逆序对（数组）在数组中的两个数字如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组，求出这个数组中的逆序对的总数。例如在数组{7，5，6，4}中，一共存在5个逆序对，分别是（7，6）、（7，5）、（7，4）、（5，4）和（6，4）。 可以按照归并排序的思路，先把数组分隔成子数组，先统计出子数组内部的逆序对的数目，然后再统计出两个相邻子数组之间的逆序对的数目。在统计逆序对的过程中，还需要对数组进行排序。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Solution &#123; int cnt; public int InversePairs(int[] array) &#123; cnt = 0; if (array != null) mergeSortUp2Down(array, 0, array.length - 1); return cnt; &#125; /* * 归并排序(从上往下) */ public void mergeSortUp2Down(int[] a, int start, int end) &#123; if (start &gt;= end) return; int mid = (start + end) &gt;&gt; 1; mergeSortUp2Down(a, start, mid); mergeSortUp2Down(a, mid + 1, end); merge(a, start, mid, end); &#125; /* * 将一个数组中的两个相邻有序区间合并成一个 */ public void merge(int[] a, int start, int mid, int end) &#123; int[] tmp = new int[end - start + 1]; int i = start, j = mid + 1, k = 0; while (i &lt;= mid &amp;&amp; j &lt;= end) &#123; if (a[i] &lt;= a[j]) tmp[k++] = a[i++]; else &#123; tmp[k++] = a[j++]; cnt += mid - i + 1; //关键的一步，统计逆序对.......... cnt%=1000000007; &#125; &#125; while (i &lt;= mid) tmp[k++] = a[i++]; while (j &lt;= end) tmp[k++] = a[j++]; for (k = 0; k &lt; tmp.length; k++) a[start + k] = tmp[k]; &#125;&#125; 37. 两个链表的第一个公共结点（链表）输入两个链表找出他们的第一个公共结点。 面试的时候碰到这道题，很多应聘者的第一个想法就是蛮力法：在第一个链表上顺序遍历每个结点，每遍历到一个结点的时候，在第二个链表上顺序遍历每个结点。若第二个链表上有一个结点和第一个链表上的结点一样，说明两个链表在这个结点上重合，于是就找到了它们的公共结点。如果第一个链表的长度为m，第二个链表的长度为n，显然该方法的时间复杂度是O(mn)。 通常蛮力法不会是最好的办法，我们接下来试着分析有公共结点的两个链表有哪些特点。从链表结构的定义看出，这两个链表是单向链表。如果他们有公共的结点，那么这两个链表从某一结点开始，他们的next指向同一个结点。但由于是单向链表的结点，每个结点只有一个next，因此从第一个公共结点开始，之后的结点都是重合的，不可能再出现分叉。所以两个有公共结点而部分重合的链表，拓扑形状看起来像一个Y，而不是X。 经过我们的分析发现，若两个链表有公共结点，那么公共结点出现在两个链表的尾部。如果我们从两个链表的尾部开始往前比较，最后一个相同的结点就是我们要找的结点。我们想到用栈的特点来解决这个问题：分别把两个链表的结点放入两个栈中，这样两个链表的尾结点就位于两个栈的栈顶，接下来比较两个栈顶的结点是否相同。若果相同，则把栈顶弹出接着比较下一个栈顶，直到找到最后一个相同的结点。 上面需要用到两个辅助栈。若链表的长度分别为m和n，那么空间复杂度是O(m+n)。这种思路的时间复杂度也是O(m+n)。和最开始的蛮力法相比，时间效率得到了提升，相当于是用空间换取时间效率。 之所以需要用到栈，是因为我们想同时遍历到达两个栈的尾结点。当两个链表的长度不相同时，如果我们从头开始遍历到达尾结点的时间就不一致。其实解决这个问题还有一个更简单的办法：首先遍历两个链表得到他们的长度，就能知道哪个链表比较长，以及长的链表比短的链表多几个结点。在第二次遍历的时候，在较长的链表上先走若干步，接着再同时在两个链表上遍历，找到的第一个相同的结点就是他们的第一个公共结点。 第三种思路和第二种思路相比，时间复杂度都是O(m+n)，但我们不再需要辅助的栈，因此提高了空间效率。实现代码如下： java版本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode FindFirstCommonNode(ListNode pHead1, ListNode pHead2) &#123; ListNode current1 = pHead1;//链表1 ListNode current2 = pHead2;//链表2 if(pHead1 ==null||pHead2==null)&#123;return null;&#125;// int len1 = getlistlength(pHead1);//链表1的长度 int len2 = getlistlength(pHead2);//链表2的长度 //若链表1长度大于链表2 if(len1&gt;=len2)&#123; int len=len1-len2; //遍历链表1，遍历长度为两链表长度差 while (len&gt;0)&#123; current1 = currentnext; len--; &#125; &#125; //若链表2长度大于链表1 else if(len1&lt;len2)&#123; int len=len2-len1; //遍历链表2，遍历长度为两链表长度差 while (len&gt;0)&#123; current2=current2.next; len--; &#125; &#125; //开始齐头并进，直到找到第一个公共结点 while(current1!=current2)&#123; current1 = currentnext; current2 = current2.next; &#125; return current1; &#125; //求指定链表的长度 public static int getlistlength(ListNode pHead)&#123; int length = 0; ListNode current = pHead; while(current!=null)&#123; length++; current = current.next; &#125; return length; &#125;&#125; python版本 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def FindFirstCommonNode(self, pHead1, pHead2): # write code here current1=pHead1 current2=pHead2 len1 = self.getlistlength(current1) len2 = self.getlistlength(current2) if len1&gt;=len2: length = len1-len2 while length&gt;0: current1 = currentnext length=length-1 elif len1&lt;len2: length = len2-len1 while length&gt;0: current2 = current2.next length=length-1 while current1!=current2: current1=currentnext current2=current2.next return current1 def getlistlength(self,pHead): length =0 current =pHead while current!=None: length=length+1 current = current.next return length 38. 数字在排序数组中出现的次数（数组）统计一个数字在排序数组中出现的次数。 利用二分查找直接找到第一个K和最后一个K。以下代码使用递归方法找到第一个K，使用循环方法最后一个K。 java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class Solution &#123; public int GetNumberOfK(int [] array , int k) &#123; int length = array.length; if(length == 0)&#123; return 0; &#125; int firstK = getFirstK(array, k, 0, length-1); int lastK = getLastK(array, k, 0, length-1); if(firstK != -1 &amp;&amp; lastK != -1)&#123; return lastK - firstK + 1; &#125; return 0; &#125; //递归写法 private int getFirstK(int [] array , int k, int start, int end)&#123; if(start &gt; end)&#123; return -1; &#125; int mid = (start + end) &gt;&gt; 1; if(array[mid] &gt; k)&#123; return getFirstK(array, k, start, mid-1); &#125; else if (array[mid] &lt; k)&#123; return getFirstK(array, k, mid+1, end); &#125; else if(mid-1 &gt;=0 &amp;&amp; array[mid-1] == k)&#123; return getFirstK(array, k, start, mid-1); &#125; else&#123; return mid; &#125; &#125; //循环写法 private int getLastK(int [] array , int k, int start, int end)&#123; int length = array.length; int mid = (start + end) &gt;&gt; 1; while(start &lt;= end)&#123; if(array[mid] &gt; k)&#123; end = mid-1; &#125; else if(array[mid] &lt; k)&#123; start = mid+1; &#125; else if(mid+1 &lt;= length-1 &amp;&amp; array[mid+1] == k)&#123; start = mid+1; &#125; else&#123; return mid; &#125; mid = (start + end) &gt;&gt; 1; &#125; return -1; &#125;&#125; 39. 二叉树的深度（二叉树）39.1 二叉树的深度输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 Java 经典的求二叉树深度 递归写法 java 12345678public class Solution &#123; public int TreeDepth(TreeNode root) &#123; if(root==null)return 0; int nleft = TreeDepth(root.left); int nright = TreeDepth(root.right); return nleft&gt;nright?(nleft+1):(nright+1); &#125;&#125; 39.2 平衡二叉树输入一棵二叉树，判断该二叉树是否是平衡二叉树。 有了求二叉树的深度的经验之后，我们就很容易想到一个思路：在遍历树的每个结点的时候，调用函数TreeDepth得到它的左右子树的深度。如果每个结点的左右子树的深度相差都不超过1，按照定义它就是一颗平衡的二叉树。 java 123456789101112131415161718public class Solution &#123; public boolean IsBalanced_Solution(TreeNode root) &#123; if(root==null)return true; int left = TreeDepth(root.left); int right = TreeDepth(root.right); int diff = left-right; if(diff&gt;1||diff&lt;-1) return false; return IsBalanced_Solution(root.left)&amp;&amp;IsBalanced_Solution(root.right); &#125; public int TreeDepth(TreeNode root) &#123; if(root==null)return 0; int nleft = TreeDepth(root.left); int nright = TreeDepth(root.right); return nleft&gt;nright?(nleft+1):(nright+1); &#125;&#125; 40. 数组中只出现一次的数字（数组）一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。要求时间复杂度是$O(n)$，空间复杂度是$O(1)$。 首先我们考虑这个问题的一个简单版本：一个数组里除了一个数字之外，其他的数字都出现了两次。请写程序找出这个只出现一次的数字。 这个题目的突破口在哪里？题目为什么要强调有一个数字出现一次，其他的出现两次？我们想到了异或运算的性质：任何一个数字异或它自己都等于0 。也就是说，如果我们从头到尾依次异或数组中的每一个数字，那么最终的结果刚好是那个只出现一次的数字，因为那些出现两次的数字全部在异或中抵消掉了。 有了上面简单问题的解决方案之后，我们回到原始的问题。如果能够把原数组分为两个子数组。在每个子数组中，包含一个只出现一次的数字，而其它数字都出现两次。如果能够这样拆分原数组，按照前面的办法就是分别求出这两个只出现一次的数字了。 我们还是从头到尾依次异或数组中的每一个数字，那么最终得到的结果就是两个只出现一次的数字的异或结果。因为其它数字都出现了两次，在异或中全部抵消掉了。由于这两个数字肯定不一样，那么这个异或结果肯定不为0 ，也就是说在这个结果数字的二进制表示中至少就有一位为1 。我们在结果数字中找到第一个为1 的位的位置，记为第N 位。现在我们以第N 位是不是1 为标准把原数组中的数字分成两个子数组，第一个子数组中每个数字的第N 位都为1 ，而第二个子数组的每个数字的第N 位都为0 。 现在我们已经把原数组分成了两个子数组，每个子数组都包含一个只出现一次的数字，而其它数字都出现了两次。因此到此为止，所有的问题我们都已经解决。 java 123456789101112131415161718192021222324252627282930313233//num1,num2分别为长度为1的数组。传出参数//将num1[0],num2[0]设置为返回结果public class Solution &#123; public void FindNumsAppearOnce(int [] array,int num1[] , int num2[]) &#123; if(array==null ||array.length&lt;2) return ; int temp = 0; for(int i=0;i&lt;array.length;i++) temp ^= array[i]; int indexOf1 = findFirstBitIs(temp); for(int i=0;i&lt;array.length;i++)&#123; if(isBit(array[i], indexOf1)) num1[0]^=array[i]; else num2[0]^=array[i]; &#125; &#125; //在正数num的二进制表示中找到最右边是1的位 public int findFirstBitIs(int num)&#123; int indexBit = 0; while(((num &amp; 1)==0) &amp;&amp; (indexBit)&lt;8*4)&#123; num = num &gt;&gt; 1; ++indexBit; &#125; return indexBit; &#125; //判断在num的二进制表示中从右边数起的indexBit位是不是1. public boolean isBit(int num,int indexBit)&#123; num = num &gt;&gt; indexBit; return (num &amp; 1) == 1; &#125;&#125; 41.和为S的两个数字VS和为s的连续正数序列（数组）41.1 和为s的两个数字一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。 数列满足递增，设两个头尾两个指针i和j， 若ai + aj == sum，就是答案（相差越远乘积越小） 若ai + aj &gt; sum，aj肯定不是答案之一（前面已得出 i 前面的数已是不可能），j -= 1 若ai + aj &lt; sum，ai肯定不是答案之一（前面已得出 j 后面的数已是不可能），i += 1 时间复杂度为O(n)。 1234567891011121314151617181920212223import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; FindNumbersWithSum(int [] array,int sum) &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); if(array==null||array.length&lt;2)&#123; return list; &#125; int i=0,j=array.length-1; while(i&lt;j)&#123; if(array[i]+array[j]==sum)&#123; list.add(array[i]); list.add(array[j]); break; &#125; else if(array[i]+array[j]&gt;sum)&#123; j--; &#125; else i++; &#125; return list; &#125;&#125; 41.2 和为s的连续正数序列输入一个正数s，打印出所有和为s的连续正数序列（至少含有两个数）。例如输入15，由于1+2+3+4+5=4+5+6=7+8=15，所以结果打印出三个连续序列1~5、4~6和7~8。 考虑用两个数small和big分别表示序列的最小值和最大值。首先把small初始化为1，big初始化为2，如果从small到big的序列和大于s，我们可以从序列中去掉较小的值，也就是增大small的值。如果从small到big的序列和小于s，我们可以增大big，让这个序列包含更多的数字。因为这个序列至少要有两个数字，我们一直增加small到（1+s）/2为止。 java 12345678910111213141516171819202122232425262728293031323334353637import java.util.ArrayList;/**初始化small=1，big=2;*small到big序列和小于sum，big++;大于sum，small++;*当small增加到(1+sum)/2是停止*/public class Solution &#123; public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindContinuousSequence(int sum) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; lists=new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if(sum&lt;=1)&#123;return lists;&#125; int small=1; int big=2; while(small!=(1+sum)/2)&#123; //当small==(1+sum)/2的时候停止 int curSum=sumOfList(small,big); if(curSum==sum)&#123; ArrayList&lt;Integer&gt; list=new ArrayList&lt;Integer&gt;(); for(int i=small;i&lt;=big;i++)&#123; list.add(i); &#125; lists.add(list); small++;big++; &#125;else if(curSum&lt;sum)&#123; big++; &#125;else&#123; small++; &#125; &#125; return lists; &#125; public int sumOfList(int head,int leap)&#123; //计算当前序列的和 int sum=head; for(int i=head+1;i&lt;=leap;i++)&#123; sum+=i; &#125; return sum; &#125;&#125; 42. 翻转单词顺序VS左旋转字符串（字符串）42.1 翻转单词顺序输入一个英文句子，翻转句子中单词的顺序，但单词内字符的顺序不变。为简单起见，标点符号和普通字母一样处理。例如输入字符串“I am a student”，则输出“student. a am I”。可以先翻转整个句子，然后，依次翻转每个单词。依据空格来确定单词的起始和终止位置 java 1234567891011121314151617181920212223242526public class Solution &#123; public String ReverseSentence(String str) &#123; char[] chars = str.toCharArray(); reverse(chars,0,chars.length-1); int blank = -1; for(int i =0;i&lt;chars.length-1;i++)&#123; if(chars[i]==' ')&#123; int nextblank = i; reverse(chars,blank+1,nextblank-1); blank = nextblank; &#125; &#125; reverse(chars,blank+1,chars.length-1);//单独翻转最后一个单词 return new String(chars); &#125; public void reverse(char[] chars,int low,int high)&#123; while(low&lt;high)&#123; char temp = chars[low]; chars[low]=chars[high]; chars[high]=temp; low++; high--; &#125; &#125;&#125; 42.2 左旋转字符串汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。 以“abcdefg”为例，我们可以把它分为两部分。由于想把它的前两个字符移到后面，我们就把钱两个字符分到第一部分，把后面的所有字符都分到第二部分。然后先翻转这两部分，于是就得到“bagfedc”。接下来在翻转整个字符串，得到的”cdefgab”刚好就是把原始字符串左旋转2位的结果。 123456789101112131415161718192021public class Solution &#123; public String LeftRotateString(String str,int n) &#123; char[] chars = str.toCharArray(); if(chars.length &lt; n) return &quot;&quot;; reverse(chars, 0, n-1); reverse(chars, n, chars.length-1); reverse(chars, 0, chars.length-1); return new String(chars); &#125; public void reverse(char[] chars,int low,int high)&#123; char temp; while(low&lt;high)&#123; temp = chars[low]; chars[low] = chars[high]; chars[high] = temp; low++; high--; &#125; &#125;&#125; 43. N个骰子的点数（null）44. 扑克牌的顺子（数组）从扑克牌中随机抽5张牌，判断是不是顺子，即这5张牌是不是连续的。2~10为数字本身，A为1，J为11，Q为12，K为13，而大小王可以看做是任意数字，这里定为0. java 12345678910111213141516171819202122232425262728293031import java.util.*;public class Solution &#123; public boolean isContinuous(int [] numbers) &#123; int length = numbers.length; if(numbers==null||length==0)return false;//特殊情况 Arrays.sort(numbers);//排序 //统计数组中0的个数 int numberOfZero = 0; for(int i =0;i&lt;length&amp;&amp;numbers[i]==0;i++)&#123; ++numberOfZero; &#125; int numberOfGap = 0; int small = numberOfZero; int big = small+1; while(big&lt;length)&#123; //含有对子，不可能是顺子 if(numbers[small]==numbers[big])&#123; return false; &#125; //统计数组中的间隔数目 numberOfGap += numbers[big]-numbers[small]-1; small=big; big++; &#125; //如果间隔数小于等于零的数量则可以组成顺子，否则不行。 if(numberOfGap&lt;=numberOfZero)&#123; return true; &#125;else&#123;return false;&#125; &#125;&#125; 45. 圆圈中最后剩下的数字（链表）0、…..，n-1这n个数字排成一个圆圈，从数字0开始每次从这个圆圈里删除第m个数字。求出这个圆圈里剩下的最后一个数字。约瑟夫环问题，用环形链表模拟圆圈的经典解法， java 12345678910111213141516171819202122232425262728public class Solution&#123; public int LastRemaining_Solution(int n, int m)&#123; if(m&lt;=0||n&lt;=0)return -1; //先构造循环链表 ListNode head= new ListNode(0);//头结点, 值为0 ListNode pre = head; ListNode temp = null; for(int i=1;i&lt;n;i++)&#123; temp = new ListNode(i); pre.next = temp; pre = temp; &#125; temp.next = head;//将第n-1个结点(也就是尾结点)指向头结点 ListNode temp2 = null; while(n&gt;=1)&#123; //每次都当前头结点找到第m个结点的前驱 temp2=head; for(int i =1;i&lt;m-1;i++)&#123; temp2 = temp2.next; &#125; temp2.next = temp2.next.next; head = temp2.next;//设置当前头结点 n--; &#125; return head.val; &#125;&#125; java 12345678910111213public class Solution&#123; public int LastRemaining_Solution(int n, int m) &#123; if(n==0||m==0)return -1; int last=0; for(int i=2;i&lt;=n;i++) &#123; last=(last+m)%i; &#125; return last ; &#125;&#125; 46. 求1+2+…..+n（逻辑）求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 需利用逻辑与的短路特性实现递归终止。 当n==0时，(n&gt;0)&amp;&amp;((sum+=Sum_Solution(n-1))&gt;0)只执行前面的判断，为false，然后直接返回0； 当n&gt;0时，执行sum+=Sum_Solution(n-1)，实现递归计算Sum_Solution(n)。 java 12345678public class Solution &#123; public int Sum_Solution(int n) &#123; int sum=n; boolean ans = (n&gt;0)&amp;&amp;((sum+=Sum_Solution(n-1))&gt;0); return sum; &#125;&#125; 47. 不用加减乘除做加法（位运算）首先看十进制是如何做的： 5+7=12，三步走 第一步：相加各位的值，不算进位，得到2。 第二步：计算进位值，得到10. 如果这一步的进位值为0，那么第一步得到的值就是最终结果。 第三步：重复上述两步，只是相加的值变成上述两步的得到的结果2和10，得到12。 同样我们可以用三步走的方式计算二进制值相加： 5-101，7-111 第一步：相加各位的值，不算进位，得到010，二进制每位相加就相当于各位做异或操作，101^111。 第二步：计算进位值，得到1010，相当于各位做与操作得到101，再向左移一位得到1010，(101&amp;111)&lt;&lt;1。 第三步重复上述两步， 各位相加 010^1010=1000，进位值为100=(010&amp;1010)&lt;&lt;1。继续重复上述两步：1000^100 = 1100，进位值为0，跳出循环，1100为最终结果。 12345678910public class Solution &#123; public int Add(int num1,int num2) &#123; while (num2!=0) &#123; int temp = num1^num2; num2 = (num1&amp;num2)&lt;&lt;1; num1 = temp; &#125; return num1; &#125;&#125; 49. 把字符串转换成整数（字符串）将一个字符串转换成一个整数，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。问题不难，但是要把很多特殊情况都考虑进去，却并不容易。需要考虑的特殊情况有以下几个： 空指针null 字符串为空 正负号 上下溢出 Integer.MAX_VALUE (2^31-1) Integer.MIN_VALUE(-2^31) java 1234567891011121314151617181920212223242526272829303132333435363738394041public class Solution &#123; public int StrToInt(String str) &#123; if(str==null||str.length()==0)&#123;return 0;&#125;//空指针或空字符串 char[] c = str.toCharArray(); boolean minus=false; int i=0; //正负号 if(c[i]=='+')&#123; i++; &#125;else if(c[i]=='-')&#123; i++; minus=true; &#125; int num=0; if(i&lt;c.length)&#123; num = StrToIntCore(c,minus,i); &#125;else&#123; return num; &#125; return num; &#125; int StrToIntCore(char[] str,boolean minus,int i)&#123; int num=0; for(int j=i;j&lt;str.length;j++)&#123; if(str[j]&gt;='0'&amp;&amp;str[j]&lt;='9')&#123; int flag = minus?-1:1; num = num*10+flag*(str[j]-'0'); if((!minus&amp;&amp;num&gt;Integer.MAX_VALUE)||minus&amp;&amp;num&lt;Integer.MIN_VALUE)&#123;//上下溢出 num=0; break; &#125; &#125;else&#123;//非法数值 num=0; break; &#125; &#125; return num; &#125;&#125; 50.树中两个结点的最低公共祖先（二叉树）50.1 二叉搜索树的最低公共祖先二叉搜索树是经过排序的，位于左子树的节点都比父节点小，位于右子树的节点都比父节点大。既然要找最低的公共祖先节点，我们可以从根节点开始进行比较。若当前节点的值比两个节点的值都大，那么最低的祖先节点一定在当前节点的左子树中，则遍历当前节点的左子节点；反之，若当前节点的值比两个节点的值都小，那么最低的祖先节点一定在当前节点的右子树中，则遍历当前节点的右子节点；这样，直到找到一个节点，位于两个节点值的中间，则找到了最低的公共祖先节点。 java 1234567891011public class Solution &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if(root==null||root==q||root==p)return root; if(root.val&gt;p.val&amp;&amp;root.val&gt;q.val)&#123; return lowestCommonAncestor(root.left,p,q); &#125;else if(root.val&lt;p.val&amp;&amp;root.val&lt;q.val)&#123; return lowestCommonAncestor(root.right,p,q); &#125;else return root; &#125;&#125; 50.2 普通二叉树的最低公共祖先一种简单的方法是DFS分别寻找到两个节点p和q的路径，然后对比路径，查看他们的第一个分岔口，则为LCA。这个思路比较简单，代码写起来不如下面这种方法优雅： 我们仍然可以用递归来解决，递归寻找两个带查询LCA的节点p和q，当找到后，返回给它们的父亲。如果某个节点的左右子树分别包括这两个节点，那么这个节点必然是所求的解，返回该节点。否则，返回左或者右子树（哪个包含p或者q的就返回哪个）。复杂度O(n) java 12345678910public class Solution &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if(root==null||root==p||root==q)&#123;return root;&#125; TreeNode left = lowestCommonAncestor(root.left,p,q); TreeNode right = lowestCommonAncestor(root.right,p,q); if(left!=null&amp;&amp;right!=null)return root; return left!=null? left:right; &#125;&#125; 51. 数组中重复的数字（数组）在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是重复的数字2或者3。 java 12345678910111213141516171819202122232425public class Solution &#123; public boolean duplicate(int numbers[],int length,int [] duplication) &#123; if(numbers==null||length==0)&#123;return false;&#125;//空指针或空数组 // 判断数组是否合法,即每个数都在0~n-1之间 for(int i=0;i&lt;length;i++)&#123; if(numbers[i]&gt;length-1||numbers[i]&lt;0)&#123; return false; &#125; &#125; //若数值与下标不同，则调换位置； //比较位置下标为数值(numbers[i])的数值(numbers[numbers[i]])与该数值(numbers[i])是否一致，若一致，则说明有重复数字 for(int i=0;i&lt;length;i++)&#123; while(numbers[i]!=i)&#123; if(numbers[i]==numbers[numbers[i]])&#123; duplication[0] = numbers[i]; return true; &#125; int temp=numbers[i]; numbers[i]=numbers[temp]; numbers[temp]=temp; &#125; &#125; return false; &#125;&#125; 52. 构建乘积数组（数组）给定一个数组A[0,1,…,n-1],请构建一个数组$B[0,1,…,n-1]$,其中B中的元素$B[i]=A[0]A[1]…A[i-1]A[i+1]…A[n-1]$。不能使用除法。 java 12345678910111213141516171819import java.util.ArrayList;public class Solution &#123; public int[] multiply(int[] A) &#123; int length = A.length; int[] B = new int[length]; if(length!=0)&#123; B[0]=1; for(int i=1;i&lt;length;i++)&#123; B[i]=B[i-1]*A[i-1]; &#125; int temp=1; for(int j=length-2;j&gt;=0;j--)&#123; temp = temp*A[j+1]; B[j]=temp*B[j]; &#125; &#125; return B; &#125;&#125; 53. 正则表达式匹配（字符串）当模式中的第二个字符不是“*”时： 如果字符串第一个字符和模式中的第一个字符相匹配，那么字符串和模式都后移一个字符，然后匹配剩余的。 如果字符串第一个字符和模式中的第一个字符相不匹配，直接返回false。 而当模式中的第二个字符是“*”时： 如果字符串第一个字符跟模式第一个字符不匹配，则模式后移2个字符，继续匹配。如果字符串第一个字符跟模式第一个字符匹配，可以有3种匹配方式： 模式后移2字符，相当于$x*$被忽略； 字符串后移1字符，模式后移2字符； 字符串后移1字符，模式不变，即继续匹配字符下一位，因为*可以匹配多位； java 123456789101112131415161718192021222324252627282930313233343536public class Solution &#123; public boolean match(char[] str, char[] pattern) &#123; if (str == null || pattern == null) &#123; return false; &#125; int strIndex = 0; int patternIndex = 0; return matchCore(str, strIndex, pattern, patternIndex);&#125; public boolean matchCore(char[] str, int strIndex, char[] pattern, int patternIndex) &#123; //有效性检验：str到尾，pattern到尾，匹配成功 if (strIndex == str.length &amp;&amp; patternIndex == pattern.length) &#123; return true; &#125; //pattern先到尾，匹配失败 if (strIndex != str.length &amp;&amp; patternIndex == pattern.length) &#123; return false; &#125; //模式第2个是*，且字符串第1个跟模式第1个匹配,分3种匹配模式；如不匹配，模式后移2位 if (patternIndex + 1 &lt; pattern.length &amp;&amp; pattern[patternIndex + 1] == '*') &#123; if ((strIndex != str.length &amp;&amp; pattern[patternIndex] == str[strIndex]) || (pattern[patternIndex] == '.' &amp;&amp; strIndex != str.length)) &#123; return matchCore(str, strIndex, pattern, patternIndex + 2)//模式后移2，视为x*匹配0个字符 || matchCore(str, strIndex + 1, pattern, patternIndex + 2)//视为模式匹配1个字符 || matchCore(str, strIndex + 1, pattern, patternIndex);//*匹配1个，再匹配str中的下一个 &#125; else &#123; return matchCore(str, strIndex, pattern, patternIndex + 2); &#125; &#125; //模式第2个不是*，且字符串第1个跟模式第1个匹配，则都后移1位，否则直接返回false if ((strIndex != str.length &amp;&amp; pattern[patternIndex] == str[strIndex]) || (pattern[patternIndex] == '.' &amp;&amp; strIndex != str.length)) &#123; return matchCore(str, strIndex + 1, pattern, patternIndex + 1); &#125; return false; &#125;&#125; 54. 表示数值的字符串（字符串）请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。 java 12345678910111213141516171819202122232425262728public class Solution &#123; boolean isNumeric(char[] s) &#123; if(s.length==0) return false; if((s.length==1)&amp;&amp;(s[0]&lt;'0'||s[0]&gt;'9')) return false; if(s[0]=='+'||s[0]=='-')&#123; if(s.length==2&amp;&amp;(s[1]=='.')) return false; &#125;else if((s[0]&lt;'0'||s[0]&gt;'9')&amp;&amp;s[0]!='.') return false;//首位既不是符号也不是数字还不是小数点，当然是false int i = 1; while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; if(i&lt;s.length&amp;&amp;s[i]=='.')&#123; i++; //if(i&gt;=s.length) return false; while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; &#125; if(i&lt;s.length&amp;&amp;(s[i]=='e'||s[i]=='E'))&#123; i++; if((i&lt;s.length)&amp;&amp;(s[i]=='+'||s[i]=='-'))&#123; i++; if(i&lt;s.length) while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; else return false; &#125;else if(i&lt;s.length)&#123; while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; &#125;else return false; &#125; if(i&lt;s.length) return false; return true; &#125;&#125; 55. 字符流中第一个不重复的数组（字符串）使用一个HashMap来统计字符出现的次数，同时用一个ArrayList来记录输入流，每次返回第一个出现一次的字符都是在这个ArrayList（输入流）中的字符作为key去map中查找。 java 123456789101112131415161718192021222324252627282930313233import java.util.*;public class Solution &#123; //HashMap来统计字符出现的次数 HashMap&lt;Character, Integer&gt; map=new HashMap(); //ArrayList来记录输入流 ArrayList&lt;Character&gt; list=new ArrayList&lt;Character&gt;(); //Insert one char from stringstream public void Insert(char ch) &#123; if(map.containsKey(ch))&#123; int time = map.get(ch); time++; map.put(ch,time); &#125;else&#123; map.put(ch,1); &#125; list.add(ch); &#125; //return the first appearence once char in current stringstream public char FirstAppearingOnce() &#123; char ch='#'; for(char k : list)&#123;//list迭代 if(map.get(k)==1)&#123; ch=k; break;//得到第一个结果即可break &#125; &#125; return ch; &#125;&#125; 56. 链表中环的入口结点（链表）一个链表中包含环，如何找到环的入口结点？例如在下图的链表中，环的入口结点是结点3。 以3为例分析两个指针的移动规律。指针$P_1$和$P_2$在初始化时都指向链表的头结点。由于环中有4个结点，指针$P_1$先在链表上向前移动4步。接下来两个指针以相同的速度在链表上向前移动，直到它们相遇。它们相遇的结点正好是还的入口结点。 剩下的问题就是如何得到环中结点的数目。我们可以使用一快一慢两个指针。若两个指针相遇，说明链表中有环。两个指针相遇的结点一定是在环中的。可以从这个结点出发，一边继续向前移动一边计数，当再次回到这个结点时，就可以得到环中结点数了实现代码如下： java版本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; //找到一快一满指针相遇处的节点，相遇的节点一定是在环中 public static ListNode meetingNode(ListNode pHead) &#123; if(pHead == null)&#123;return null;&#125;//空链表处理 ListNode pslow = pHead.next; if(pslow == null)&#123;return null;&#125;//无环链表处理 ListNode pfast = pslow.next; while(pfast!=null &amp;&amp; pslow!=null)&#123; if(pslow==pfast)&#123;return pfast;&#125; pslow = pslow.next;//慢指针 pfast = pfast.next; if(pfast!=null)&#123; pfast = pfast.next; &#125;//块指针 &#125; return null; &#125; public ListNode EntryNodeOfLoop(ListNode pHead)&#123; ListNode meetingNode=meetingNode(pHead);//相遇结点 //环的结点个数 if(meetingNode==null)&#123;return null;&#125;//是否有环 int nodesInLoop = 1; ListNode p1=meetingNode; while(pnext!=meetingNode)&#123; p1=pnext; ++nodesInLoop; &#125; //p1慢指针,先往前走 p1=pHead; for(int i=0;i&lt;nodesInLoop;i++)&#123; p1=pnext; &#125; //p1,p2同步走，相遇的地方即为环入口 ListNode p2=pHead; while(p1!=p2)&#123; p1=pnext; p2=p2.next; &#125; return p1; &#125; &#125; python 版本 123456789101112131415161718192021222324252627282930313233343536373839# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def meetingNode(self,pHead): if not pHead: return None pslow =pHead.next if not pslow: return None pfast = pslow.next while pfast and pslow: if pslow==pfast: return pfast pslow = pslow.next pfast = pfast.next if pfast: pfast=pfast.next return None def EntryNodeOfLoop(self, pHead): meetingNode = self.meetingNode(pHead) if not meetingNode: return None nodesInLoop = 1 p1 = meetingNode while pnext!=meetingNode: p1=pnext nodesInLoop +=1 p1 = pHead for i in xrange(0,nodesInLoop): p1=pnext p2=pHead while p1!=p2: p1=pnext p2=p2.next return p1 57. 删除链表中重复的结点（链表）在一个排序的链表中，如何删除重复的结点？如在下图中重复结点被删除之后，链表如下图所示： 从头遍历整个链表。如果当前结点的值与下一个节点的值相同，那么它们就是重复的结点，都可以被删除。为了保证删除之后的链表仍然是相连的而没有中间断开，我们要把当前结点的前一个结点preNode和后面值比当前结点的值要大的结点相连。要确保preNode要始终与下一个没有重复的结点连接在一起。 实现代码如下： java递归版 12345678910111213141516171819202122232425262728/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode deleteDuplication(ListNode pHead) &#123; if (pHead == null || pHead.next == null) &#123; // 只有0个或1个结点，则返回 return pHead; &#125; if (pHead.val == pHead.next.val) &#123; // 当前结点是重复结点 ListNode pNode = pHead.next; while (pNode != null &amp;&amp; pNode.val == pHead.val) &#123; // 跳过值与当前结点相同的全部结点,找到第一个与当前结点不同的结点 pNode = pNode.next; &#125; return deleteDuplication(pNode); // 从第一个与当前结点不同的结点开始递归 &#125; else &#123; // 当前结点不是重复结点 pHead.next = deleteDuplication(pHead.next); // 保留当前结点，从下一个结点开始递归 return pHead; &#125; &#125;&#125; python版本 1234567891011121314151617# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def deleteDuplication(self, pHead): if not pHead or not pHead.next: return pHead if pHead.val==pHead.next.val: pNode = pHead.next while pNode and pNode.val == pHead.val: pNode = pNode.next return self.deleteDuplication(pNode) else: pHead.next = self.deleteDuplication(pHead.next) return pHead java非递归 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode deleteDuplication(ListNode pHead) &#123; if(pHead==null)return null; ListNode preNode = null; ListNode node = pHead; while(node!=null)&#123; ListNode nextNode = node.next; boolean needDelete = false; //需要删除重复节点的情况 if(nextNode!=null&amp;&amp;nextNode.val==node.val)&#123; needDelete = true; &#125; //不重复结点不删除 if(!needDelete)&#123; preNode = node; node = node.next; &#125; //重复节点删除 else&#123; int value = node.val; ListNode toBeDel = node; //连续重复结点 while(toBeDel != null &amp;&amp; toBeDel.val == value)&#123; nextNode = toBeDel.next; toBeDel = nextNode; if(preNode==null) pHead = nextNode; else preNode.next = nextNode; node = nextNode; &#125; &#125; &#125; return pHead; &#125;&#125; 58. 二叉树的下一个结点（二叉树）给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。我们可发现分成两大类： 有右子树的，那么下个结点就是右子树最左边的点；（eg：D，B，E，A，F，C，G） 没有右子树的，也可以分成两类，a)是父节点左孩子（eg：N，I，L） ，那么父节点就是下一个节点 ； b)是父节点的右孩子（eg：H，J，K，M）找他的父节点的父节点的父节点…直到当前结点是其父节点的左孩子位置。如果没有eg：M，那么他就是尾节点。 java 123456789101112131415161718public class Solution &#123; public TreeLinkNode GetNext(TreeLinkNode pNode) &#123; if(pNode==null)&#123;return null;&#125; if(pNode.right!=null)&#123; pNode = pNode.right; while(pNode.left!=null)&#123; pNode = pNode.left; &#125; return pNode; &#125; while(pNode.next!=null)&#123; if(pNode.next.left==pNode)return pNode.next; pNode = pNode.next; &#125; return null; &#125;&#125; 59. 对称的二叉树（二叉树）请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 如果先序遍历的顺序分为两种先左后右和先右后左两种顺序遍历，如果两者相等说明二叉树是对称的二叉树 java 12345678910111213public class Solution &#123; boolean isSymmetrical(TreeNode pRoot)&#123; return isSymmetrical(pRoot,pRoot); &#125; boolean isSymmetrical(TreeNode pRoot1,TreeNode pRoot2)&#123; if(pRoot1==null&amp;&amp;pRoot2==null)return true; if(pRoot1==null||pRoot2==null)return false; if(pRoot1.val==pRoot2.val)&#123;return isSymmetrical(pRoot1.left,pRoot2.right)&amp;&amp;isSymmetrical(pRoot1.right,pRoot2.left); &#125;else return false; &#125;&#125; 60. 把二叉树打印成多行（二叉树）从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。用end记录每层结点数目，start记录每层已经打印的数目，当start=end，重新建立list，开始下一层打印。 java 123456789101112131415161718192021222324public class Solution &#123; ArrayList&lt;ArrayList&lt;Integer&gt; &gt; Print(TreeNode pRoot) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if(pRoot==null)&#123;return result;&#125; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); queue.add(pRoot); int start = 0,end = 1; while(!queue.isEmpty())&#123; TreeNode treenode = queue.remove(); list.add(treenode.val); start++; if(treenode.left!=null)&#123;queue.add(treenode.left);&#125; if(treenode.right!=null)&#123;queue.add(treenode.right);&#125; if(start==end)&#123; end = queue.size(); start = 0; result.add(list); list = new ArrayList&lt;Integer&gt;(); &#125; &#125; return result; &#125; &#125; 61. 按S型打印二叉树（二叉树）请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。 java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.util.ArrayList;import java.util.Stack;public class Solution &#123; public ArrayList&lt;ArrayList&lt;Integer&gt; &gt; Print(TreeNode pRoot) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; alist =new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if(pRoot==null)return alist; Stack&lt;TreeNode&gt; stack1 = new Stack&lt;TreeNode&gt;(); stack1.add(pRoot); Stack&lt;TreeNode&gt; stack2 = new Stack&lt;TreeNode&gt;(); while(!stack1.isEmpty()||!stack2.isEmpty())&#123; if(!stack1.isEmpty())&#123; ArrayList&lt;Integer&gt; alist2 = new ArrayList&lt;Integer&gt;(); while(!stack1.isEmpty())&#123; TreeNode treenode=stack1.pop(); alist2.add(treenode.val); if(treenode.left!=null)&#123; stack2.add(treenode.left); &#125; if(treenode.right!=null)&#123; stack2.add(treenode.right); &#125; &#125; alist.add(alist2); &#125; else&#123; ArrayList&lt;Integer&gt; alist2 = new ArrayList&lt;Integer&gt;(); while(!stack2.isEmpty())&#123; TreeNode treenode = stack2.pop(); alist2.add(treenode.val); if(treenode.right!=null)&#123; stack1.add(treenode.right); &#125; if(treenode.left!=null)&#123; stack1.add(treenode.left); &#125; &#125; alist.add(alist2); &#125; &#125; return alist; &#125;&#125; 62. 序列化与反序列化二叉树（二叉树）请实现两个函数，分别用来序列化和反序列化二叉树算法思想：根据前序遍历规则完成序列化与反序列化。所谓序列化指的是遍历二叉树为字符串；所谓反序列化指的是依据字符串重新构造成二叉树。 依据前序遍历序列来序列化二叉树，因为前序遍历序列是从根结点开始的。当在遍历二叉树时碰到Null指针时，这些Null指针被序列化为一个特殊的字符“#”。另外，结点之间的数值用逗号隔开。 java 12345678910111213141516171819202122232425262728293031public class Solution &#123; int index = -1; //计数变量 String Serialize(TreeNode root) &#123; StringBuilder sb = new StringBuilder();//新建字符串 if(root == null)&#123; sb.append("#,"); return sb.toString(); &#125; //递归 sb.append(root.val + ","); sb.append(Serialize(root.left)); sb.append(Serialize(root.right)); return sb.toString(); &#125; TreeNode Deserialize(String str) &#123; index++; //int len = str.length(); //if(index &gt;= len)&#123; // return null; // &#125; String[] strr = str.split(","); TreeNode node = null; if(!strr[index].equals("#"))&#123; node = new TreeNode(Integer.valueOf(strr[index])); node.left = Deserialize(str); node.right = Deserialize(str); &#125; return node; &#125;&#125; 63. 二叉搜索树的第K个结点（二叉树）给定一颗二叉搜索树，请找出其中的第k大的结点。例如在下图二叉搜索树里，按结点数值大小顺序第三个结点的值是4.如果按照中序遍历的顺序遍历一颗二叉搜索树，遍历序列的数值是递增排序的，只需要用中序遍历算法遍历一颗二叉搜索树，就很容易找出它的第K大的结点。 java 123456789101112131415161718public class Solution &#123; int index = 0; //计数器 TreeNode KthNode(TreeNode root, int k) &#123; if(root != null)&#123; //中序遍历寻找第k个 TreeNode node = KthNode(root.left,k); if(node != null) return node; index ++; if(index == k) return root; node = KthNode(root.right,k); if(node != null) return node; &#125; return null; &#125;&#125; 64. 数据流中的中位数（二叉树）Java的PriorityQueue是从JDK1.5开始提供的新的数据结构接口，默认内部是自然排序，结果为小顶堆，也可以自定义排序器，比如下面反转比较，完成大顶堆。 为了保证插入新数据和取中位数的时间效率都高效，这里使用大顶堆+小顶堆的容器，并且满足： 两个堆中的数据数目差不能超过1，这样可以使中位数只会出现在两个堆的交接处； 大顶堆的所有数据都小于小顶堆，这样就满足了排序要求。 java 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.Comparator;import java.util.PriorityQueue; public class Solution &#123; int count=0; PriorityQueue&lt;Integer&gt; minHeap = new PriorityQueue&lt;Integer&gt;(); PriorityQueue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;Integer&gt;(11, new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; //PriorityQueue默认是小顶堆，实现大顶堆，需要反转默认排序器 return o2.compareTo(o1); &#125; &#125;); public void Insert(Integer num) &#123; if (count %2 == 0) &#123;//当数据总数为偶数时，新加入的元素，应当进入小根堆 //（注意不是直接进入小根堆，而是经大根堆筛选后取大根堆中最大元素进入小根堆） //1.新加入的元素先入到大根堆，由大根堆筛选出堆中最大的元素 maxHeap.offer(num); int filteredMaxNum = maxHeap.poll(); //2.筛选后的【大根堆中的最大元素】进入小根堆 minHeap.offer(filteredMaxNum); &#125; else &#123;//当数据总数为奇数时，新加入的元素，应当进入大根堆 //（注意不是直接进入大根堆，而是经小根堆筛选后取小根堆中最大元素进入大根堆） //1.新加入的元素先入到小根堆，由小根堆筛选出堆中最小的元素 minHeap.offer(num); int filteredMinNum = minHeap.poll(); //2.筛选后的【小根堆中的最小元素】进入大根堆 maxHeap.offer(filteredMinNum); &#125; count++;&#125; public Double GetMedian() &#123; if (count %2 == 0) &#123; return new Double((minHeap.peek() + maxHeap.peek())) / 2; &#125; else &#123; return new Double(minHeap.peek()); &#125;&#125;&#125; 65. 滑动窗口的最大值（数组）给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 java 123456789101112131415161718192021222324252627282930313233343536373839import java.util.ArrayList;import java.util.LinkedList;public class Solution &#123; public ArrayList&lt;Integer&gt; maxInWindows(int [] num, int size) &#123; ArrayList&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); if (num == null) &#123; return ret; &#125; if (num.length &lt; size || size &lt; 1) &#123; return ret; &#125; LinkedList&lt;Integer&gt; indexDeque = new LinkedList&lt;&gt;(); //前size-1个中，前面比num[i]小的，对应下标从下标队列移除； for (int i = 0; i &lt; size - 1; i++) &#123; if (!indexDeque.isEmpty() &amp;&amp; num[i] &gt; num[indexDeque.getLast()]) &#123; indexDeque.removeLast(); &#125; indexDeque.addLast(i); &#125; //从第size-1个开始；前面比num[i]小的，对应下标从下标队列移除； for (int i = size - 1; i &lt; num.length; i++) &#123; while(!indexDeque.isEmpty() &amp;&amp; num[i] &gt; num[indexDeque.getLast()]) &#123; indexDeque.removeLast(); &#125; //把下一个下标加入队列中 indexDeque.addLast(i); //当第一个数字的下标与当前处理的数字的下标之差大于或者等于滑动窗口的大小时，这个数字已经从窗口划出，可以移除了； if (i - indexDeque.getFirst() + 1 &gt; size) &#123; indexDeque.removeFirst(); &#125; //下标队列的第一个是滑动窗口最大值对应的下标； ret.add(num[indexDeque.getFirst()]); &#125; return ret; &#125;&#125; 66. 矩阵中的路径（数组）请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则该路径不能再进入该格子。 例如下面的矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 java 12345678910111213141516171819202122232425262728293031public class Solution &#123; public int movingCount(int threshold, int rows, int cols) &#123; boolean[] visited=new boolean[rows*cols]; return movingCountCore(threshold, rows, cols, 0,0,visited); &#125; private int movingCountCore(int threshold, int rows, int cols, int row,int col,boolean[] visited) &#123; if(row&lt;0||row&gt;=rows||col&lt;0||col&gt;=cols) return 0; int i=row*cols+col; if(visited[i]||!checkSum(threshold,row,col)) return 0; visited[i]=true; return 1+movingCountCore(threshold, rows, cols,row,col+1,visited) +movingCountCore(threshold, rows, cols,row,col-1,visited) +movingCountCore(threshold, rows, cols,row+1,col,visited) +movingCountCore(threshold, rows, cols,row-1,col,visited); &#125; private boolean checkSum(int threshold, int row, int col) &#123; int sum=0; while(row!=0)&#123; sum+=row%10; row=row/10; &#125; while(col!=0)&#123; sum+=col%10; col=col/10; &#125; if(sum&gt;threshold) return false; return true; &#125;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（4）：二叉树题解]]></title>
    <url>%2F2017%2F08%2F04%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%884%EF%BC%89%EF%BC%9A%E4%BA%8C%E5%8F%89%E6%A0%91%E9%A2%98%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[二叉树相关题解java实现。 一、重建二叉树（剑6）输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。 例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 根据前序遍历的特点，我们知道根结点为1 观察中序遍历。其中root节点G左侧的472必然是root的左子树，G右侧的5386必然是root的右子树。 观察左子树472，左子树的中的根节点必然是大树的root的leftchild。在前序遍历中，大树的root的leftchild位于root之后，所以左子树的根节点为2。 同样的道理，root的右子树节点5386中的根节点也可以通过前序遍历求得。在前序遍历中，一定是先把root和root的所有左子树节点遍历完之后才会遍历右子树，并且遍历的左子树的第一个节点就是左子树的根节点。同理，遍历的右子树的第一个节点就是右子树的根节点。 观察发现，上面的过程是递归的。先找到当前树的根节点，然后划分为左子树，右子树，然后进入左子树重复上面的过程，然后进入右子树重复上面的过程。最后就可以还原一棵树了。 该步递归的过程可以简洁表达如下： 确定根,确定左子树，确定右子树。 在左子树中递归。 在右子树中递归。 打印当前根。 递归代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class Solution &#123; public TreeNode reConstructBinaryTree(int [] pre,int [] in) &#123; return reConBTree(pre,0,pre.length-1,in,0,in.length-1); &#125; public TreeNode reConBTree(int [] pre,int preleft,int preright,int [] in,int inleft,int inright)&#123; if(preleft &gt; preright || inleft&gt; inright)//当到达边界条件时候返回null return null; //新建一个TreeNode TreeNode root = new TreeNode(pre[preleft]); //对中序数组进行输入边界的遍历 for(int i = inleft; i&lt;= inright; i++)&#123; if(pre[preleft] == in[i])&#123; //重构左子树，注意边界条件 root.left = reConBTree(pre,preleft+1,preleft+i-inleft,in,inleft,i-1); //重构右子树，注意边界条件 root.right = reConBTree(pre,preleft+i+1-inleft,preright,in,i+1,inright); &#125; &#125; return root; &#125;&#125;``` ## 二、树的子结构（剑18）输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构）要查找树A中是否存在和树B结构一样的子树，我们可以分成两步：第一步在树A中找到和B的根结点的值一样的结点R，第二步再判断树A以R为根结点的子树是不是包含和树B一样的结构。第一步在树A中查找与根结点的值一样的结点，实际上就是树的遍历。对二叉树这种数据结构熟悉的读者自然知道可以用递归的方法去遍历，也可以用循环的方法去遍历。由于递归的代码实现比较简洁，面试时如果没有特别要求，通常会采用递归的方式。参考代码如下：&gt;java第一步```javapublic boolean HasSubtree(TreeNode root1,TreeNode root2) &#123; boolean result = false; //一定要注意边界条件的检查，即检查空指针。否则程序容易奔溃，面试时尤其要注意。这里当Tree1和Tree2都不为零的时候，才进行比较。否则直接返回false if(root1!=null&amp;&amp;root2!=null)&#123; ////如果找到了对应Tree2的根节点的点 if(root1.val==root2.val)&#123; //以这个根节点为为起点判断是否包含Tree2 result = DoesTree1HaveTree2(root1,root2); &#125; //如果找不到，那么就再去root的左儿子当作起点，去判断是否包含Tree2 if(!result)&#123; result=HasSubtree(root1.left,root2); &#125; //如果还找不到，那么就再去root的右儿子当作起点，去判断是否包含Tree2 if(!result)&#123; result=HasSubtree(root1.right,root2); &#125; &#125; return result; &#125; 第二步是判断树A中以R为根结点的子树是不是和树B具有相同的结构。同样，我们也可以用递归的思路来考虑：如果结点R的值和树B的根结点不同，则以R为根结点的子树和树B一定不具有相同的结点；如果他们的值相同，则递归地判断它们各自的左右结点的值是不是相同。递归的终止条件是我们达到了树A或者树B的叶结点。 代码如下： java 12345678910111213141516public boolean DoesTree1HaveTree2(TreeNode root1,TreeNode root2)&#123; //如果Tree2已经遍历完了都能对应的上，返回true if(root2==null)&#123; return true; &#125; //如果Tree2还没有遍历完，Tree1却遍历完了。返回false if(root1==null)&#123; return false; &#125; //如果其中有一个点没有对应上，返回false if(root1.val!=root2.val)&#123; return false; &#125; //如果根节点对应的上，那么就分别去左右子节点里面匹配 return DoesTree1HaveTree2(root1.left,root2.left)&amp;&amp;DoesTree1HaveTree2(root1.right,root2.right); &#125; 二叉树相关的代码有大量的指针操作，每一次使用指针的时候，我们都要问自己这个指针有没有可能是NULL，如果是NULL该怎么处理。 三、二叉树的镜像（剑19）操作给定的二叉树，将其变换为源二叉树的镜像。 123456789101112131415161718192021public class Solution &#123; public void Mirror(TreeNode root) &#123; //边界 if(root==null) return; if(root.left==null&amp;&amp;root.right==null) return; //交换左右子树 TreeNode temp = root.left; root.left=root.right; root.right=temp; //递归 if(root.left!=null)&#123; Mirror(root.left); &#125; if(root.right!=null)&#123; Mirror(root.right); &#125; &#125;&#125; 四、从上往下打印二叉树（剑23）从上往下打印出二叉树的每个节点，同层节点从左至右打印。 每次打印一个结点时，如果该结点有子结点，则把该结点的子结点放到队列的末尾。接下来到队列的头部取出最早进入队列的结点，重复前面的打印操作。 java 12345678910111213141516171819import java.util.ArrayList;import java.util.LinkedList;public class Solution &#123; public ArrayList&lt;Integer&gt; PrintFromTopToBottom(TreeNode root) &#123; ArrayList&lt;Integer&gt; List=new ArrayList&lt;Integer&gt;(); if(root==null)&#123;return List;&#125; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.add(root);//先把根结点加入队列q while(!queue.isEmpty())&#123;//队列非空时 TreeNode treenode=queue.remove();//取出队列头结点 if(treenode.left!=null)&#123;queue.add(treenode.left);&#125;//向队列加入左孩子（若有） if(treenode.right!=null)&#123;queue.add(treenode.right);&#125;//向队列加入右孩子（若有） List.add(treenode.val);//加到打印列表中 &#125; return List; &#125;&#125; 五、二叉搜索树的后序遍历序列（剑24）输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。在后序遍历得到的序列中，最后一个数字是树的根结点的值。数组中前面的数字可以分成两部分：第一部分是左子树结点的值，它们都比根结点小；第二部分是右子树结点的值，它们都比根结点大。 java 1234567891011121314151617181920212223242526272829303132import java.util.Arrays;public class Solution &#123; public boolean VerifySquenceOfBST(int [] sequence) &#123; int length = sequence.length; if(sequence==null||length==0)&#123;return false;&#125; int root = sequence[length-1];//根结点 int i=0; //外部初始化 //找到左子树的最后一个结点位置 for(;i&lt;length-1;i++)&#123; if(sequence[i]&gt;root)&#123; break; &#125; &#125; //如果右子树的结点值小于根结点的值，则返回false for(int j=i;j&lt;length-1;j++)&#123; if(sequence[j]&lt;root)&#123; return false; &#125; &#125; //初始化 boolean left=true; boolean right=true; //递归左右子树 if(i&gt;0)&#123; left = VerifySquenceOfBST(Arrays.copyOfRange(sequence,0,i));//Arrays的copyOfRange方法 &#125; if(i&lt;length-1)&#123; right = VerifySquenceOfBST(Arrays.copyOfRange(sequence,i,length-1)); &#125; return left&amp;&amp;right; &#125;&#125; 六、二叉树中和为某一值的路径（剑25）输入一颗二叉树和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。 java 12345678910111213141516171819public class Solution &#123; private ArrayList&lt;ArrayList&lt;Integer&gt;&gt; listAll = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); private ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindPath(TreeNode root,int target) &#123; if(root == null) return listAll; list.add(root.val); target -= root.val;//每次减去结点的值 //如果target等于0，则说明这条路径和为target，添加到listAll中 if(target == 0 &amp;&amp; root.left == null &amp;&amp; root.right == null) listAll.add(new ArrayList&lt;Integer&gt;(list));//因为add添加的是引用，如果不new一个的话，后面的操作会更改listAll中list的值 //向左孩子递归 if(root.left!=null)FindPath(root.left, target); //向右孩子递归 if(root.right!=null)FindPath(root.right, target); //如果不满足条件，则回到父节点； list.remove(list.size()-1); return listAll; &#125;&#125; 七、二叉搜索树与双向链表（剑27）输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 java 12345678910111213141516171819202122232425public class Solution &#123; TreeNode head = null; TreeNode realHead = null; public TreeNode Convert(TreeNode pRootOfTree) &#123; ConvertSub(pRootOfTree); return realHead//realHead是每个子树排序后的第一个结点，head是排序后的最后一个结点; &#125; private void ConvertSub(TreeNode pRootOfTree) &#123; //递归中序遍历 if(pRootOfTree==null) return; ConvertSub(pRootOfTree.left); if (head == null) &#123; //初始处 head = pRootOfTree; realHead = pRootOfTree; &#125; else &#123; //前两句实现双向，第三句跳到下一个节点。 head.right = pRootOfTree; pRootOfTree.left = head; head = pRootOfTree; &#125; ConvertSub(pRootOfTree.right); &#125;&#125; 八、二叉树的深度（剑39.1）输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 Java 经典的求二叉树深度 递归写法 java 12345678public class Solution &#123; public int TreeDepth(TreeNode root) &#123; if(root==null)return 0; int nleft = TreeDepth(root.left); int nright = TreeDepth(root.right); return nleft&gt;nright?(nleft+1):(nright+1); &#125;&#125; 九、平衡二叉树（剑39.2）输入一棵二叉树，判断该二叉树是否是平衡二叉树。 有了求二叉树的深度的经验之后，我们就很容易想到一个思路：在遍历树的每个结点的时候，调用函数TreeDepth得到它的左右子树的深度。如果每个结点的左右子树的深度相差都不超过1，按照定义它就是一颗平衡的二叉树。 java 123456789101112131415161718public class Solution &#123; public boolean IsBalanced_Solution(TreeNode root) &#123; if(root==null)return true; int left = TreeDepth(root.left); int right = TreeDepth(root.right); int diff = left-right; if(diff&gt;1||diff&lt;-1) return false; return IsBalanced_Solution(root.left)&amp;&amp;IsBalanced_Solution(root.right); &#125; public int TreeDepth(TreeNode root) &#123; if(root==null)return 0; int nleft = TreeDepth(root.left); int nright = TreeDepth(root.right); return nleft&gt;nright?(nleft+1):(nright+1); &#125;&#125; 十、二叉搜索树的最低公共祖先（剑50.1）二叉搜索树是经过排序的，位于左子树的节点都比父节点小，位于右子树的节点都比父节点大。既然要找最低的公共祖先节点，我们可以从根节点开始进行比较。若当前节点的值比两个节点的值都大，那么最低的祖先节点一定在当前节点的左子树中，则遍历当前节点的左子节点；反之，若当前节点的值比两个节点的值都小，那么最低的祖先节点一定在当前节点的右子树中，则遍历当前节点的右子节点；这样，直到找到一个节点，位于两个节点值的中间，则找到了最低的公共祖先节点。 java 1234567891011public class Solution &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if(root==null||root==q||root==p)return root; if(root.val&gt;p.val&amp;&amp;root.val&gt;q.val)&#123; return lowestCommonAncestor(root.left,p,q); &#125;else if(root.val&lt;p.val&amp;&amp;root.val&lt;q.val)&#123; return lowestCommonAncestor(root.right,p,q); &#125;else return root; &#125;&#125; 十一、普通二叉树的最低公共祖先（剑50.2）一种简单的方法是DFS分别寻找到两个节点p和q的路径，然后对比路径，查看他们的第一个分岔口，则为LCA。这个思路比较简单，代码写起来不如下面这种方法优雅： 我们仍然可以用递归来解决，递归寻找两个带查询LCA的节点p和q，当找到后，返回给它们的父亲。如果某个节点的左右子树分别包括这两个节点，那么这个节点必然是所求的解，返回该节点。否则，返回左或者右子树（哪个包含p或者q的就返回哪个）。复杂度O(n) java 12345678910public class Solution &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if(root==null||root==p||root==q)&#123;return root;&#125; TreeNode left = lowestCommonAncestor(root.left,p,q); TreeNode right = lowestCommonAncestor(root.right,p,q); if(left!=null&amp;&amp;right!=null)return root; return left!=null? left:right; &#125;&#125; 十二、二叉树的下一个结点（剑58）给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。我们可发现分成两大类： 有右子树的，那么下个结点就是右子树最左边的点；（eg：D，B，E，A，F，C，G） 没有右子树的，也可以分成两类，a)是父节点左孩子（eg：N，I，L） ，那么父节点就是下一个节点 ； b)是父节点的右孩子（eg：H，J，K，M）找他的父节点的父节点的父节点…直到当前结点是其父节点的左孩子位置。如果没有eg：M，那么他就是尾节点。 java 123456789101112131415161718public class Solution &#123; public TreeLinkNode GetNext(TreeLinkNode pNode) &#123; if(pNode==null)&#123;return null;&#125; if(pNode.right!=null)&#123; pNode = pNode.right; while(pNode.left!=null)&#123; pNode = pNode.left; &#125; return pNode; &#125; while(pNode.next!=null)&#123; if(pNode.next.left==pNode)return pNode.next; pNode = pNode.next; &#125; return null; &#125;&#125; 十三、对称的二叉树（剑59）请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 如果先序遍历的顺序分为两种先左后右和先右后左两种顺序遍历，如果两者相等说明二叉树是对称的二叉树 java 12345678910111213public class Solution &#123; boolean isSymmetrical(TreeNode pRoot)&#123; return isSymmetrical(pRoot,pRoot); &#125; boolean isSymmetrical(TreeNode pRoot1,TreeNode pRoot2)&#123; if(pRoot1==null&amp;&amp;pRoot2==null)return true; if(pRoot1==null||pRoot2==null)return false; if(pRoot1.val==pRoot2.val)&#123;return isSymmetrical(pRoot1.left,pRoot2.right)&amp;&amp;isSymmetrical(pRoot1.right,pRoot2.left); &#125;else return false; &#125;&#125; 十四、把二叉树打印成多行（剑60）从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。用end记录每层结点数目，start记录每层已经打印的数目，当start=end，重新建立list，开始下一层打印。 java 123456789101112131415161718192021222324public class Solution &#123; ArrayList&lt;ArrayList&lt;Integer&gt; &gt; Print(TreeNode pRoot) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if(pRoot==null)&#123;return result;&#125; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); queue.add(pRoot); int start = 0,end = 1; while(!queue.isEmpty())&#123; TreeNode treenode = queue.remove(); list.add(treenode.val); start++; if(treenode.left!=null)&#123;queue.add(treenode.left);&#125; if(treenode.right!=null)&#123;queue.add(treenode.right);&#125; if(start==end)&#123; end = queue.size(); start = 0; result.add(list); list = new ArrayList&lt;Integer&gt;(); &#125; &#125; return result; &#125; &#125; 十五、按S型打印二叉树（剑61）请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。 java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.util.ArrayList;import java.util.Stack;public class Solution &#123; public ArrayList&lt;ArrayList&lt;Integer&gt; &gt; Print(TreeNode pRoot) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; alist =new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if(pRoot==null)return alist; Stack&lt;TreeNode&gt; stack1 = new Stack&lt;TreeNode&gt;(); stack1.add(pRoot); Stack&lt;TreeNode&gt; stack2 = new Stack&lt;TreeNode&gt;(); while(!stack1.isEmpty()||!stack2.isEmpty())&#123; if(!stack1.isEmpty())&#123; ArrayList&lt;Integer&gt; alist2 = new ArrayList&lt;Integer&gt;(); while(!stack1.isEmpty())&#123; TreeNode treenode=stack1.pop(); alist2.add(treenode.val); if(treenode.left!=null)&#123; stack2.add(treenode.left); &#125; if(treenode.right!=null)&#123; stack2.add(treenode.right); &#125; &#125; alist.add(alist2); &#125; else&#123; ArrayList&lt;Integer&gt; alist2 = new ArrayList&lt;Integer&gt;(); while(!stack2.isEmpty())&#123; TreeNode treenode = stack2.pop(); alist2.add(treenode.val); if(treenode.right!=null)&#123; stack1.add(treenode.right); &#125; if(treenode.left!=null)&#123; stack1.add(treenode.left); &#125; &#125; alist.add(alist2); &#125; &#125; return alist; &#125;&#125; 十六、序列化与反序列化二叉树（剑62）请实现两个函数，分别用来序列化和反序列化二叉树算法思想：根据前序遍历规则完成序列化与反序列化。所谓序列化指的是遍历二叉树为字符串；所谓反序列化指的是依据字符串重新构造成二叉树。 依据前序遍历序列来序列化二叉树，因为前序遍历序列是从根结点开始的。当在遍历二叉树时碰到Null指针时，这些Null指针被序列化为一个特殊的字符“#”。另外，结点之间的数值用逗号隔开。 java 12345678910111213141516171819202122232425262728293031public class Solution &#123; int index = -1; //计数变量 String Serialize(TreeNode root) &#123; StringBuilder sb = new StringBuilder();//新建字符串 if(root == null)&#123; sb.append("#,"); return sb.toString(); &#125; //递归 sb.append(root.val + ","); sb.append(Serialize(root.left)); sb.append(Serialize(root.right)); return sb.toString(); &#125; TreeNode Deserialize(String str) &#123; index++; //int len = str.length(); //if(index &gt;= len)&#123; // return null; // &#125; String[] strr = str.split(","); TreeNode node = null; if(!strr[index].equals("#"))&#123; node = new TreeNode(Integer.valueOf(strr[index])); node.left = Deserialize(str); node.right = Deserialize(str); &#125; return node; &#125;&#125; 十七、二叉搜索树的第K个结点（剑63）给定一颗二叉搜索树，请找出其中的第k大的结点。例如在下图二叉搜索树里，按结点数值大小顺序第三个结点的值是4.如果按照中序遍历的顺序遍历一颗二叉搜索树，遍历序列的数值是递增排序的，只需要用中序遍历算法遍历一颗二叉搜索树，就很容易找出它的第K大的结点。 java 123456789101112131415161718public class Solution &#123; int index = 0; //计数器 TreeNode KthNode(TreeNode root, int k) &#123; if(root != null)&#123; //中序遍历寻找第k个 TreeNode node = KthNode(root.left,k); if(node != null) return node; index ++; if(index == k) return root; node = KthNode(root.right,k); if(node != null) return node; &#125; return null; &#125;&#125; 十八、数据流中的中位数（剑64）Java的PriorityQueue是从JDK1.5开始提供的新的数据结构接口，默认内部是自然排序，结果为小顶堆，也可以自定义排序器，比如下面反转比较，完成大顶堆。 为了保证插入新数据和取中位数的时间效率都高效，这里使用大顶堆+小顶堆的容器，并且满足： 两个堆中的数据数目差不能超过1，这样可以使中位数只会出现在两个堆的交接处； 大顶堆的所有数据都小于小顶堆，这样就满足了排序要求。 java 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.Comparator;import java.util.PriorityQueue; public class Solution &#123; int count=0; PriorityQueue&lt;Integer&gt; minHeap = new PriorityQueue&lt;Integer&gt;(); PriorityQueue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;Integer&gt;(11, new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; //PriorityQueue默认是小顶堆，实现大顶堆，需要反转默认排序器 return o2.compareTo(o1); &#125; &#125;); public void Insert(Integer num) &#123; if (count %2 == 0) &#123;//当数据总数为偶数时，新加入的元素，应当进入小根堆 //（注意不是直接进入小根堆，而是经大根堆筛选后取大根堆中最大元素进入小根堆） //1.新加入的元素先入到大根堆，由大根堆筛选出堆中最大的元素 maxHeap.offer(num); int filteredMaxNum = maxHeap.poll(); //2.筛选后的【大根堆中的最大元素】进入小根堆 minHeap.offer(filteredMaxNum); &#125; else &#123;//当数据总数为奇数时，新加入的元素，应当进入大根堆 //（注意不是直接进入大根堆，而是经小根堆筛选后取小根堆中最大元素进入大根堆） //1.新加入的元素先入到小根堆，由小根堆筛选出堆中最小的元素 minHeap.offer(num); int filteredMinNum = minHeap.poll(); //2.筛选后的【小根堆中的最小元素】进入大根堆 maxHeap.offer(filteredMinNum); &#125; count++;&#125; public Double GetMedian() &#123; if (count %2 == 0) &#123; return new Double((minHeap.peek() + maxHeap.peek())) / 2; &#125; else &#123; return new Double(minHeap.peek()); &#125;&#125;&#125; 十九、 二叉树最大路径和（leetcode 124）一个很有意思的问题，一个社区，所有的房子构成一棵二叉树，每个房子里有一定价值的财物，这棵二叉树有一个根节点root。如果相邻的两座房子同时被进入，就会触发警报。一个小偷，最初只能访问root节点，并可以通过二叉树的边访问房子（注：访问不意味着进入），请问不触发警报的前提下他能偷到的财物的最大价值是多少？ 123456以下面这棵二叉树为例，最多能偷走3+3+1=7的财物 3 / \ 2 3 \ \ 3 1 分析：这个问题乍一看上去可能没什么思路，但是如果是用递归，可以很优雅的解决这个问题，这需要读者对递归有比较深刻的理解。下面给出解决这个问题的java代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.util.HashMap;import java.util.Map;import common.TreeNode;public class Solution &#123; //使用一个cache 缓存以每个节点为根节点的rob方法返回值，减少计算量 Map&lt;TreeNode, Integer&gt; cache = new HashMap&lt;TreeNode, Integer&gt;(); public int rob(TreeNode root) &#123; //如果当前节点为空 直接返回0 if(null == root)&#123; return 0; &#125; //首先查看缓存中有没有这个节点的rob方法返回值 if(null != cache.get(root))&#123; return cache.get(root); &#125; //计算当前节点左孩子的rob方法返回值 int maxLeft = rob(root.left); //计算当前节点右孩子的rob方法返回值 int maxRight = rob(root.right); int maxLeftLeft = 0; int maxLeftRight = 0; //如果当前节点有左孩子 if(null != root.left)&#123; //计算其左孩子的左孩子的rob值 maxLeftLeft = rob(root.left.left); //计算其左孩子的右孩子的rob值 maxLeftRight = rob(root.left.right); &#125; int maxRightLeft = 0; int maxRightRight = 0; //如果当前节点有右孩子 if(null != root.right)&#123; //计算其右孩子的左孩子的rob值 maxRightLeft = rob(root.right.left); //计算其右孩子的右孩子的rob值 maxRightRight = rob(root.right.right); &#125; //不偷当前节点能偷到的财物的最大值 int notIncludeCurrentNodeMax = maxLeft + maxRight; //偷当前节点能偷到的财物的最大值 int includeCurrentNodeMax = maxLeftLeft + maxLeftRight + maxRightLeft + maxRightRight + root.val; //以其中的较大值作为当前节点的rob方法返回值 int res = notIncludeCurrentNodeMax &gt; includeCurrentNodeMax ? notIncludeCurrentNodeMax : includeCurrentNodeMax; //缓存当前节点的rob方法返回值 cache.put(root, res); return res; &#125;&#125; 面经中出现过的二叉树题：已整理 一、给定二叉树的先序跟后序遍历，能不能将二叉树重建（不能，因为先序：父节点-左节点-右节点，后序：左节点-右节点-父节点，两者的拓扑序列是一样的，所以无法建立），如果给出一个二叉搜索树的后续能不能建立（可以，因为只要将遍历结果排序就可以得到中序结果）。 二、判断一棵树是否是另一棵的子树。 三、 翻转二叉树 四、二叉树打印路径 六、输入一颗二叉树和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。二叉树路径和等于某个值的所有路径输出。 七、排序二叉树转双向链表 十、输入根节点和两个子节点，找到最小公共父节点，2叉树只有孩子节点；查找二叉树某两个节点的最近公共祖先 十二、输入二叉树节点 P, 找到二叉树中序遍历 P 的下一个节点。 十四、把二叉树打印成多行；中序遍历二叉树，利用O(1)空间统计遍历的每个节点的层次 十五、手写S型遍历二叉树，如何优化，最后说了个空间优化的方法 十九、leetcode 124 二叉树最大路径和 未整理求完全二叉树的节点个数，要求最优解法，我写的是递归，复杂度O(logn*logn)]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>二叉树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（3）：字符串题解]]></title>
    <url>%2F2017%2F08%2F04%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%883%EF%BC%89%EF%BC%9A%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%A2%98%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[字符串相关题解java实现 一、替换空格（剑4）请实现一个函数，把字符串中的每个空格替换成”%20”。例如输入“We are happy.”，则输出“We%20are%20happy” 网络编程中，要把特殊符号转换成服务器可识别的字符。转换的规则是在“%”后面跟上ASCII码的两位十六进制的表示。比如空格的ASCII码是32，即十六进制的0X20，因此空格被替换成“%20”。 问题1：替换字符串，是在原来的字符串上做替换，还是新开辟一个字符串做替换！问题2：在当前字符串替换，怎么替换才更有效率（不考虑java里现有的replace方法）。从前往后替换，后面的字符要不断往后移动，要多次移动，所以效率低下；从后往前，先计算需要多少空间，然后从后往前移动，则每个字符只为移动一次，这样效率更高一点。 123456789101112131415161718192021222324public class Solution &#123; public String replaceSpace(StringBuffer str) &#123; int spacenum = 0;//spacenum为计算空格数 for(int i=0;i&lt;str.length();i++)&#123; if(str.charAt(i)==' ') spacenum++; &#125; int indexold = str.length()-1;//indexold为为替换前的str下标 int newlength = str.length()+2*spacenum;//计算空格转换成%20之后的str长度 int indexnew = newlength-1;//indexold为为把空格替换为%20后的str下标 str.setLength(newlength);//使str的长度扩大到转换成%20之后的长度,防止下标越界,setLength方法 for(;indexold&gt;=0&amp;&amp;indexold&lt;newlength;--indexold)&#123; if(str.charAt(indexold)==' ')&#123;//charAt方法 str.setCharAt(indexnew--,'0'); str.setCharAt(indexnew--,'2'); str.setCharAt(indexnew--,'%'); &#125; else&#123; str.setCharAt(indexnew--,str.charAt(indexold)); &#125; &#125; return str.toString(); &#125;&#125; 二、字符串的排列（剑28）输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。 首先我要打印abc的全排列，就是第一步把a 和bc交换（得到bac,cab），这需要一个for循环，循环里面有一个swap，交换之后就相当于不管第一步了，进入下一步递归，所以跟一个递归函数， 完成递归之后把交换的换回来，变成原来的字串 12345678abc 为例子：1. 固定a, 求后面bc的全排列： abc, acb。 求完后，a 和 b交换； 得到bac,开始第二轮2. 固定b, 求后面ac的全排列： bac, bca。 求完后，b 和 c交换； 得到cab,开始第三轮3. 固定c, 求后面ba的全排列： cab, cba 即递归树： str: a b c ab ac ba bc ca cb result: abc acb bac bca cab cba java 12345678910111213141516171819202122232425262728293031import java.util.ArrayList;import java.util.*;public class Solution &#123; public ArrayList&lt;String&gt; Permutation(String str) &#123; ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); if(str.length()==0) return list; char[] array = str.toCharArray(); permutation(array,0,list); Collections.sort(list); return list; &#125; public void permutation(char[] array,int begin,ArrayList&lt;String&gt; list) &#123; if(begin == array.length-1) &#123; list.add(String.valueOf(array)); &#125;else &#123; for(int i=begin;i&lt;array.length;++i) &#123; if(i==begin || array[i]!=array[begin]) &#123; swap(array,begin,i); permutation(array,begin+1,list); swap(array,begin,i); &#125; &#125; &#125; &#125; public void swap(char[] array,int i,int j) &#123; char temp = array[i]; array[i] = array[j]; array[j] = temp; &#125;&#125; 三、第一次只出现一次的字符（字符串）在一个字符串(1&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置. 我们可以使用一个容器来存放每个字符的出现次数。在这个数据容器中可以根据字符来查找出现的次数，也就是这个容器的作用是把一个字符映射成一个数字。在常用的数据容器中，哈希表正是这个用途。 为了解决这个问题，我们可以定义哈希表的键值（Key）是字符，而值（Value）是该字符出现的次数。同时我们还需要从头开始扫描字符串两次。第一次扫面字符串时，每扫到一个字符就在哈希表的对应项把次数加1.接下来第二次扫描时，每扫描到一个字符就能在哈希表中得到该字符出现的次数，这样第一个只出现一次的字符就是符合要求的输出。 需要涉及到Java中HashMap工作原理及实现，资料链接 java 1234567891011121314151617181920212223import java.util.HashMap;public class Solution &#123; public int FirstNotRepeatingChar(String str) &#123; HashMap&lt;Character,Integer&gt; map = new HashMap&lt;Character,Integer&gt;(); for(int i=0;i&lt;str.length();i++)&#123; char c = str.charAt(i);//charAt方法，获得位置i的串 if(map.containsKey(c))&#123;//HashMap的containKey方法； int time = map.get(c);//HashMap的get方法，得到Key c的Value； time++; map.put(c,time);//HashMap的put方法，将Key c的Value置为time； &#125;else&#123; map.put(c,1); &#125; &#125; for(int i=0;i&lt;str.length();i++)&#123; char c = str.charAt(i); if(map.get(c)==1)&#123; return i; &#125; &#125; return -1; &#125;&#125; 四、翻转单词顺序（剑42.1）输入一个英文句子，翻转句子中单词的顺序，但单词内字符的顺序不变。为简单起见，标点符号和普通字母一样处理。例如输入字符串“I am a student.”，则输出“student. a am I”。可以先翻转整个句子，然后，依次翻转每个单词。依据空格来确定单词的起始和终止位置 java 1234567891011121314151617181920212223242526public class Solution &#123; public String ReverseSentence(String str) &#123; char[] chars = str.toCharArray(); reverse(chars,0,chars.length-1); int blank = -1; for(int i =0;i&lt;chars.length-1;i++)&#123; if(chars[i]==' ')&#123; int nextblank = i; reverse(chars,blank+1,nextblank-1); blank = nextblank; &#125; &#125; reverse(chars,blank+1,chars.length-1);//单独翻转最后一个单词 return new String(chars); &#125; public void reverse(char[] chars,int low,int high)&#123; while(low&lt;high)&#123; char temp = chars[low]; chars[low]=chars[high]; chars[high]=temp; low++; high--; &#125; &#125;&#125; 五、左旋转字符串（剑42.2）汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。 以“abcdefg”为例，我们可以把它分为两部分。由于想把它的前两个字符移到后面，我们就把钱两个字符分到第一部分，把后面的所有字符都分到第二部分。然后先翻转这两部分，于是就得到“bagfedc”。接下来在翻转整个字符串，得到的”cdefgab”刚好就是把原始字符串左旋转2位的结果。123456789101112131415161718192021public class Solution &#123; public String LeftRotateString(String str,int n) &#123; char[] chars = str.toCharArray(); if(chars.length &lt; n) return &quot;&quot;; reverse(chars, 0, n-1); reverse(chars, n, chars.length-1); reverse(chars, 0, chars.length-1); return new String(chars); &#125; public void reverse(char[] chars,int low,int high)&#123; char temp; while(low&lt;high)&#123; temp = chars[low]; chars[low] = chars[high]; chars[high] = temp; low++; high--; &#125; &#125;&#125; 六、 把字符串转换成整数（剑49）将一个字符串转换成一个整数，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。问题不难，但是要把很多特殊情况都考虑进去，却并不容易。需要考虑的特殊情况有以下几个： 空指针null 字符串为空 正负号 上下溢出 Integer.MAX_VALUE (2^31-1) Integer.MIN_VALUE(-2^31) java 1234567891011121314151617181920212223242526272829303132333435363738394041public class Solution &#123; public int StrToInt(String str) &#123; if(str==null||str.length()==0)&#123;return 0;&#125;//空指针或空字符串 char[] c = str.toCharArray(); boolean minus=false; int i=0; //正负号 if(c[i]=='+')&#123; i++; &#125;else if(c[i]=='-')&#123; i++; minus=true; &#125; int num=0; if(i&lt;c.length)&#123; num = StrToIntCore(c,minus,i); &#125;else&#123; return num; &#125; return num; &#125; int StrToIntCore(char[] str,boolean minus,int i)&#123; int num=0; for(int j=i;j&lt;str.length;j++)&#123; if(str[j]&gt;='0'&amp;&amp;str[j]&lt;='9')&#123; int flag = minus?-1:1; num = num*10+flag*(str[j]-'0'); if((!minus&amp;&amp;num&gt;Integer.MAX_VALUE)||minus&amp;&amp;num&lt;Integer.MIN_VALUE)&#123;//上下溢出 num=0; break; &#125; &#125;else&#123;//非法数值 num=0; break; &#125; &#125; return num; &#125;&#125; 七、正则表达式匹配（剑53）当模式中的第二个字符不是“*”时： 如果字符串第一个字符和模式中的第一个字符相匹配，那么字符串和模式都后移一个字符，然后匹配剩余的。 如果字符串第一个字符和模式中的第一个字符相不匹配，直接返回false。 而当模式中的第二个字符是“*”时： 如果字符串第一个字符跟模式第一个字符不匹配，则模式后移2个字符，继续匹配。如果字符串第一个字符跟模式第一个字符匹配，可以有3种匹配方式： 模式后移2字符，相当于$x*$被忽略； 字符串后移1字符，模式后移2字符； 字符串后移1字符，模式不变，即继续匹配字符下一位，因为*可以匹配多位； java 123456789101112131415161718192021222324252627282930313233343536public class Solution &#123; public boolean match(char[] str, char[] pattern) &#123; if (str == null || pattern == null) &#123; return false; &#125; int strIndex = 0; int patternIndex = 0; return matchCore(str, strIndex, pattern, patternIndex);&#125; public boolean matchCore(char[] str, int strIndex, char[] pattern, int patternIndex) &#123; //有效性检验：str到尾，pattern到尾，匹配成功 if (strIndex == str.length &amp;&amp; patternIndex == pattern.length) &#123; return true; &#125; //pattern先到尾，匹配失败 if (strIndex != str.length &amp;&amp; patternIndex == pattern.length) &#123; return false; &#125; //模式第2个是*，且字符串第1个跟模式第1个匹配,分3种匹配模式；如不匹配，模式后移2位 if (patternIndex + 1 &lt; pattern.length &amp;&amp; pattern[patternIndex + 1] == '*') &#123; if ((strIndex != str.length &amp;&amp; pattern[patternIndex] == str[strIndex]) || (pattern[patternIndex] == '.' &amp;&amp; strIndex != str.length)) &#123; return matchCore(str, strIndex, pattern, patternIndex + 2)//模式后移2，视为x*匹配0个字符 || matchCore(str, strIndex + 1, pattern, patternIndex + 2)//视为模式匹配1个字符 || matchCore(str, strIndex + 1, pattern, patternIndex);//*匹配1个，再匹配str中的下一个 &#125; else &#123; return matchCore(str, strIndex, pattern, patternIndex + 2); &#125; &#125; //模式第2个不是*，且字符串第1个跟模式第1个匹配，则都后移1位，否则直接返回false if ((strIndex != str.length &amp;&amp; pattern[patternIndex] == str[strIndex]) || (pattern[patternIndex] == '.' &amp;&amp; strIndex != str.length)) &#123; return matchCore(str, strIndex + 1, pattern, patternIndex + 1); &#125; return false; &#125;&#125; 八、表示数值的字符串（剑54）请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。 java 12345678910111213141516171819202122232425262728public class Solution &#123; boolean isNumeric(char[] s) &#123; if(s.length==0) return false; if((s.length==1)&amp;&amp;(s[0]&lt;'0'||s[0]&gt;'9')) return false; if(s[0]=='+'||s[0]=='-')&#123; if(s.length==2&amp;&amp;(s[1]=='.')) return false; &#125;else if((s[0]&lt;'0'||s[0]&gt;'9')&amp;&amp;s[0]!='.') return false;//首位既不是符号也不是数字还不是小数点，当然是false int i = 1; while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; if(i&lt;s.length&amp;&amp;s[i]=='.')&#123; i++; //if(i&gt;=s.length) return false; while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; &#125; if(i&lt;s.length&amp;&amp;(s[i]=='e'||s[i]=='E'))&#123; i++; if((i&lt;s.length)&amp;&amp;(s[i]=='+'||s[i]=='-'))&#123; i++; if(i&lt;s.length) while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; else return false; &#125;else if(i&lt;s.length)&#123; while((i&lt;s.length)&amp;&amp;(s[i]&gt;='0'&amp;&amp;s[i]&lt;='9')) i++; &#125;else return false; &#125; if(i&lt;s.length) return false; return true; &#125;&#125; 九、字符流中第一个不重复的数组（剑55）使用一个HashMap来统计字符出现的次数，同时用一个ArrayList来记录输入流，每次返回第一个出现一次的字符都是在这个ArrayList（输入流）中的字符作为key去map中查找。 java 123456789101112131415161718192021222324252627282930313233import java.util.*;public class Solution &#123; //HashMap来统计字符出现的次数 HashMap&lt;Character, Integer&gt; map=new HashMap(); //ArrayList来记录输入流 ArrayList&lt;Character&gt; list=new ArrayList&lt;Character&gt;(); //Insert one char from stringstream public void Insert(char ch) &#123; if(map.containsKey(ch))&#123; int time = map.get(ch); time++; map.put(ch,time); &#125;else&#123; map.put(ch,1); &#125; list.add(ch); &#125; //return the first appearence once char in current stringstream public char FirstAppearingOnce() &#123; char ch='#'; for(char k : list)&#123;//list迭代 if(map.get(k)==1)&#123; ch=k; break;//得到第一个结果即可break &#125; &#125; return ch; &#125;&#125; 十、最长无重复字符子串给定一个字符串，找字符中的最大非重复子串 。 基本思路是维护一个窗口，每次关注窗口中的字符串，在每次判断中，左窗口和右窗口选择其一向前移动。同样是维护一个HashSet, 正常情况下移动右窗口，如果没有出现重复则继续移动右窗口，如果发现重复字符，则说明当前窗口中的串已经不满足要求，继续移动有窗口不可能得到更好的结果，此时移动左窗口，直到不再有重复字符为止，中间跳过的这些串中不会有更好的结果，因为他们不是重复就是更短。因为左窗口和右窗口都只向前，所以两个窗口都对每个元素访问不超过一遍，因此时间复杂度为O(2*n)=O(n),是线性算法。空间复杂度为HashSet的size,也是O(n). 用start记录当前处理的开始位置历遍字符串，当当前字符从开始位置start开始已经出现过的时候，子串开始位置+1，否则更新map中的hash值为当前位置 。代码如下： java 123456789101112131415161718import java.util.HashMap;public class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; if (s.length()==0) return 0; HashMap&lt;Character, Integer&gt; map = new HashMap&lt;Character, Integer&gt;(); int max=0; int lens=s.length(); for (int i=0, start=0; i&lt;lens; ++i)&#123; char ch = s.charAt(i); if (map.containsKey(ch))&#123; start = Math.max(start,map.get(ch)+1); &#125; map.put(ch,i); max = Math.max(max,i-start+1); &#125; return max; &#125;&#125; 十一、最长回文字符串已整理 十二、KMP算法已整理 面试出现的字符串题已整理 三、寻找字符串中第一个只出现一次的字符；寻找一个字符串中第一个只出现一次的字符 五、左旋转字符串；手写算法:字符串反转；翻转一个英文字符串中的单词位置，单词间以空格分隔，但不改变每个单词本身的顺序。如输入“Ha Mo”，输出“Mo Ha”；字符串反转；请实现一个函数将“I am a student”转为“student a am I”。 六、实现atoi函数，即字符串转整型；就是那个字符串转换成整数，题目不难，但考虑的细节特别多。。。没写出来；写程序 str2Int 七、写 find 函数，在目标串中匹配模式串（要考虑中文字符的情况）。 十一、最长回文子串：判断一个数字是否为回文数（此处需注意，面试官一直问我有没有更优的方法，我当时已经说出了2-3个方法，囧）， 十二、KMP算法 未整理字符串由大小写字母组成，要求去重，只允许使用几个int临时变量，要求时间复杂度尽可能少。左右括号组成的字符串，去除最少使得剩余的字符串是合法的统计一个字符串中英文字母、空格、数字的个数，考察代码风格是否规范然后要求手写纯C字符串拼接，当时笔者想到了三个细节（1：const char* str 2: 空串判断 3：返回新串还是原有串），写完代码之后面试就结束了。在出门的那一刹那，我想起了代码中一个问题，空间申请啊，内心是崩溃的。。。字符串分割字符串排序字符串中字符替换两个字符串的复制（除了字符串地址重叠的情况，也要注意判断字符串本身的空间足够不足够，对于异常情况要考虑全面）写code去除字符串S1中的字符使得最终的字符串S2不包含’ab’和’c’]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>字符串</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（2）：数组题解]]></title>
    <url>%2F2017%2F08%2F03%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%882%EF%BC%89%EF%BC%9A%E6%95%B0%E7%BB%84%E9%A2%98%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[数组相关题解java实现。 一、二维数组中的查找（剑3）在一个二维数组中，每一行都按照从左到右的递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一维数组和一个整数，判断数组中是否含有该整数。 首先选取数组中右上角的数字，如果该数字等于我们要查找的数组，查找过程结束；如果该数字大于要查找的数组，剔除这个数字所在的列；如果该数字小于要查找的数组，剔除这个数字所在的行。也就是说如果要查找的数字不再数组的右上角，则每一次都在数组的查找范围中剔除一行或者一列，这样每一步都可以缩小查找的范围，直到找到要查找的数字，或者查找范围为空。 java 12345678910111213141516171819public class Solution &#123; public boolean Find(int target, int [][] array) &#123; int row = 0; int col = array[0].length - 1; while(row&lt;=array.length-1&amp;&amp;col&gt;=0)&#123; if(target == array[row][col])&#123; return true; &#125; else if(target&gt;array[row][col])&#123; row++; &#125; else&#123; col--; &#125; &#125; return false; &#125;&#125; python 123456789101112131415# -*- coding:utf-8 -*-class Solution: # array 二维列表 def Find(self, target, array): # write code here row = 0 col = len(array[0])-1 while row&lt;=len(array)-1 and col&gt;=0: if target==array[row][col]: return True elif target&gt;array[row][col]: row+=1 else: col-=1 return False 也可以把每一行看做是一个递增的序列，利用二分查找。 java 12345678910111213141516171819public class Solution &#123; public boolean Find(int target, int [][] array) &#123; for(int i=0;i&lt;array.length;i++)&#123; int low =0; int high = array[i].length-1; while(low&lt;=high)&#123; int mid = (low+high)/2; if(array[i][mid]==target) return true; else if(array[i][mid]&gt;target) high =mid-1; else low=mid+1; &#125; &#125; return false; &#125;&#125; 二、用两个栈实现队列（剑7）栈是一个非常常见的数据结构，它在计算机领域中被广泛应用，比如操作系统会给每个线程创建一个栈来存储函数调用时各个函数的参数、返回地址及临时变量等。栈的特点是后进先出，即最后被压入（push）栈的元素会第一个被弹出（pop）。 队列是另外一种很重要的数据结构。和栈不同的是，队列的特点是先进先出，即第一个进入队列的元素将会第一个出来。 栈和队列虽然是针锋相对的两个数据结构，但有意思的是他们却相互联系。 通过一个具体的例子来分析往队列插入和删除元素的过程。首先插入一个元素a，不妨先把它插入到stack1，此时stack1中的元素有{a}，stack2为空，再向stack1压入b和c，此时stack1中的元素有{a,b,c}，其中c处于栈顶，而stack2仍然是空的。 因为a是最先进的，最先被删除的元素应该是a，但a位于栈低。我们可以把stack1中的元素逐个弹出并压入stack2，元素在stack2的顺序正好和原来在stack1的顺序相反因此经过三次弹出stack1和压入stack2操作之后，stack1为空，而stack2的元素是{c,b,a}，这时就可以弹出stack2的栈顶a了，随后弹出stack2中的b和c，而这个过程中stack1始终为空. 从上面的分析我们可以总结出删除一个元素的步骤：当stack2中不为空时，在stack2的栈顶元素是最先进入队列的元素，可以弹出。如果stack2为空时，我们把stack1中的元素逐个弹出并压入stack2。由于先进入队列的元素被压到stack1的底端，经过弹出和压入之后就处于stack2的顶端了，又可以直接弹出。 1234567891011121314151617181920import java.util.Stack;public class Solution &#123; Stack&lt;Integer&gt; stack1 = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; stack2 = new Stack&lt;Integer&gt;(); public void push(int node) &#123; stack1.push(node); &#125; public int pop() &#123; while(!stack2.isEmpty())&#123; return stack2.pop(); &#125; while(!stack1.isEmpty())&#123; stack2.push(stack1.pop()); &#125; return stack2.pop(); &#125;&#125; 三、旋转数组的最小数字（剑8）在准备面试的时候，我们应该重点掌握二分查找、归并排序和快速排序，做到能随时正确、完整地写出它们的代码。 若面试题是要求在排序的数组（或部分排序的数组）中查找一个数字或者统计某个数字出现的次数，我们都可以尝试用二分查找算法。 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 可以采用二分法解答这个问题， mid = low + (high - low)/2 ，需要考虑三种情况： array[mid] &gt; array[high]:出现这种情况的array类似[3,4,5,6,0,1,2]，此时最小数字一定在mid的右边。low = mid + 1 array[mid] == array[high]: 出现这种情况的array类似 [1,0,1,1,1] 或者[1,1,1,0,1]，此时最小数字不好判断在mid左边，还是右边,这时只好一个一个试，low = low + 1 或者 high = high - 1 array[mid] &lt; array[high]: 出现这种情况的array类似[2,2,3,4,5,6,6],此时最小数字一定就是array[mid]或者在mid的左边。因为右边必然都是递增的。 high = mid。注意这里有个坑：如果待查询的范围最后只剩两个数，那么mid一定会指向下标靠前的数字，比如 array = [4,6]，array[low] = 4 ;array[mid] = 4 ; array[high] = 6 ; 如果high = mid - 1，就会产生错误， 因此high = mid，但情形(1)中low = mid + 1就不会错误。 代码如下： java 12345678910111213141516171819202122import java.util.ArrayList;public class Solution &#123; public int minNumberInRotateArray(int [] array) &#123; int low = 0; int high = array.length-1; while(low&lt;high)&#123; int mid = low+(high-low)/2; if(array[mid]&gt;array[high])&#123; low=mid+1; &#125; else if(array[mid]==array[high])&#123; high=high-1; &#125; else&#123; high = mid; &#125; &#125; return array[low]; &#125;&#125; 四、斐波那契数列（剑9）4.1 斐波那契数列大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。n&lt;=39。这个题可以说是迭代（Iteration） VS 递归（Recursion），f(n) = f(n-1) + f(n-2)，第一眼看就是递归啊，简直完美的递归环境，递归肯定很爽，这样想着关键代码两三行就搞定了，注意这题的n是从0开始的： 12if(n&lt;=1) return n;else return Fibonacci(n-1)+Fibonacci(n-2); 然而并没有什么用，测试用例里肯定准备着一个超大的n来让Stack Overflow，为什么会溢出？因为重复计算，而且重复的情况还很严重，举个小点的例子，n=4，看看程序怎么跑的： 123Fibonacci(4) = Fibonacci(3) + Fibonacci(2); = Fibonacci(2) + Fibonacci(1) + Fibonacci(1) + Fibonacci(0); = Fibonacci(1) + Fibonacci(0) + Fibonacci(1) + Fibonacci(1) + Fibonacci(0); 由于我们的代码并没有记录Fibonacci(1)和Fibonacci(0)的结果，对于程序来说它每次递归都是未知的，因此光是n=4时f(1)就重复计算了3次之多。 更简单的办法是从下往上计算，首先根据f(0)和f(1)算出f(2)，再根据f(1)和f(2)算出f(3)……依此类推就可以算出第n项了。很容易理解，这种思路的时间复杂度是O(n)。实现代码如下： java 1234567891011121314151617public class Solution &#123; public int Fibonacci(int n) &#123; if(n==0) return 0; if(n==1) return 1; int num1 = 0; int num2 = 1; int fibN=0; for(int i=2;i&lt;=n;++i)&#123; fibN=num1+num2; num1=num2; num2=fibN; &#125; return fibN; &#125;&#125; 4.2 跳台阶一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级台阶总共有多少种跳法。 我们把n级台阶的跳法看成是n的函数，记为f(n)。当n&gt;2时，第一次跳的时候就有两种不同的选择：一是第一次只跳1级，此时跳法数目等于后面剩下的n-1级台阶的跳法数目，即为f(n-1)；另外一种选择是第一次跳2级，此时跳法数目等于后面剩下的n-2级台阶的跳法数目，即为f(n-2)，因此n级台阶的不同跳法的总数f(n)=f(n-1)+f(n-2)。分析到这里，我们不难看出这实际上是斐波那契数列了。代码如下： java 1234567891011121314151617181920public class Solution &#123; public int JumpFloor(int target) &#123; if(target == 0) return 0; if(target == 1) return 1; if(target == 2) return 2; int num1 = 0; int num2 = 1; int jump = 0; for(int i=0;i&lt;target;i++)&#123; jump = num1+num2; num1=num2; num2=jump; &#125; return jump; &#125;&#125; 4.3 变态跳台阶一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 因为n级台阶，第一步有n种跳法：跳1级、跳2级、到跳n级。跳1级，剩下n-1级，则剩下跳法是f(n-1)，跳2级，剩下n-2级，则剩下跳法是f(n-2)。所以f(n)=f(n-1)+f(n-2)+…+f(1)，因为f(n-1)=f(n-2)+f(n-3)+…+f(1)，所以f(n)=2*f(n-1) java 1234567891011public class Solution &#123; public int JumpFloorII(int target) &#123; if(target==0) return 0; if(target==1) return 1; else&#123; return 2*JumpFloorII(target-1); &#125; &#125;&#125; 4.4 矩形覆盖我们可以用$21$的小矩形横着或者竖着去覆盖更大的矩形。请问用n个$21$的小矩形无重叠地覆盖一个$2*n$的大矩形，总共有多少种方法？ 把$28$的覆盖方法记为f(8)。用一个$12$小矩形去覆盖大矩形的最左边有两个选择。竖着放或者横着放。当竖着放时，右边剩下$27$的区域，记为f(7)。横着放时，当$12$的小矩阵横着放在左上角的时候，左下角必须横着放一个$12$的小矩阵，剩下$26$，记为f(6)，因此f(8)=f(7)+f(6)。此时可以看出，仍然是斐波那契数列。 代码如下： java 1234567891011121314151617public class Solution &#123; public int RectCover(int target) &#123; if(target==0) return 0; if(target==1) return 1; int num1=0; int num2=1; int cover =0; for(int i=0;i&lt;target;i++)&#123; cover = num1+num2; num1=num2; num2=cover; &#125; return cover; &#125;&#125; 五、调整数组顺序使奇数位于偶数前面（剑14） 书上的方法类似于快排，但快排是不稳定的，即其相对位置会发生变化。 java 1234567891011121314151617181920public class Solution &#123; public void reOrderArray(int [] array) &#123; int length = array.length; if(array==null||length==0) return; int left = 0; int right = length-1; while(left&lt;right)&#123; while(left&lt;right&amp;&amp;array[left]%2==1)&#123; left++; &#125; while(left&lt;right&amp;&amp;array[right]%2==0)&#123; right--; &#125; int temp =array[right]; array[right]=array[left]; array[left]=temp; &#125; &#125;&#125; 这里要保证奇数和奇数，偶数和偶数之间的相对位置不变。可以使用插入排序的思想 java 123456789101112131415161718public class Solution &#123; public void reOrderArray(int [] array) &#123; int length = array.length; if(array==null||length==0) return; for(int i=1;i&lt;length;i++)&#123; if(array[i]%2==1)&#123; int curr = array[i]; int j=i-1; while(j&gt;=0&amp;&amp;array[j]%2==0)&#123; array[j+1]=array[j]; j--; &#125; array[j+1]=curr; &#125; &#125; &#125;&#125; 六、顺时针打印矩阵（剑20）输入一个矩阵，按照从外向里以顺时针依次打印出每一个数字。例如，输入如下矩阵： java 12345678910111213141516171819202122232425262728import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printMatrix(int [][] matrix) &#123; int row = matrix.length; int col = matrix[0].length; ArrayList&lt;Integer&gt; result = new ArrayList&lt;Integer&gt; (); // 输入的二维数组非法，返回空的数组 if(row==0&amp;&amp;col==0)return result; // 定义四个关键变量，表示左上和右下的打印范围 int left =0,top=0,right=col-1,bottom=row-1; while(left&lt;=right&amp;&amp;top&lt;=bottom)&#123; // left to right for(int i=left;i&lt;=right;i++)&#123;result.add(matrix[top][i]);&#125; // top to bottom for(int i=top+1;i&lt;=bottom;i++)&#123;result.add(matrix[i][right]);&#125; // right to left if(top!=bottom)&#123; for(int i=right-1;i&gt;=left;i--)&#123;result.add(matrix[bottom][i]);&#125;&#125; // bottom to top if(left!=right)&#123; for(int i=bottom-1;i&gt;=top+1;i--)&#123;result.add(matrix[i][left]);&#125;&#125; left++;right--;top++;bottom--; &#125; return result; &#125; &#125; 七、包含min函数的栈（剑21）定义栈的数据结构，请在该类型中实现一个能够得到栈最小元素的min函数。在该栈中，调用min、push及pop的时间复杂度都是O(1)。 可以利用一个辅助栈来存放最小值 每入栈一次，就与辅助栈顶比较大小，如果小就入栈，如果大就入栈当前的辅助栈顶 。 当出栈时，辅助栈也要出栈 这种做法可以保证辅助栈顶一定都是最小元素。 12345678910111213141516171819202122232425import java.util.Stack;public class Solution &#123; Stack&lt;Integer&gt; data = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; min = new Stack&lt;Integer&gt;(); public void push(int node) &#123; data.push(node); if(min.empty())&#123;min.push(data.peek());&#125; else if(data.peek()&lt;min.peek())&#123;min.push(data.peek());&#125; else min.push(min.peek()); &#125; public void pop() &#123; data.pop(); min.pop(); &#125; public int top() &#123; return data.peek(); &#125; public int min() &#123; return min.peek(); &#125;&#125; 八、栈的压入、弹出序列（剑22）输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4，5，3，2，1是该压栈序列对应的一个弹出序列，但4，3，5，1，2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 借用一个辅助的栈，遍历压栈顺序，先讲第一个放入栈中，这里是1，然后判断栈顶元素是不是出栈顺序的第一个元素，这里是4，很显然1≠4，所以我们继续压栈，直到相等以后开始出栈，出栈一个元素，则将出栈顺序向后移动一位，直到不相等，这样循环等压栈顺序遍历完成，如果辅助栈还不为空，说明弹出序列不是该栈的弹出顺序。举例：入栈1,2,3,4,5出栈4,5,3,2,1首先1入辅助栈，此时栈顶1≠4，继续入栈2此时栈顶2≠4，继续入栈3此时栈顶3≠4，继续入栈4此时栈顶4＝4，出栈4，弹出序列向后一位，此时为5，,辅助栈里面是1,2,3此时栈顶3≠5，继续入栈5此时栈顶5=5，出栈5,弹出序列向后一位，此时为3，,辅助栈里面是1,2,3….依次执行，最后辅助栈为空。如果不为空说明弹出序列不是该栈的弹出顺序。 java 1234567891011121314151617import java.util.ArrayList;import java.util.Stack;public class Solution &#123; public boolean IsPopOrder(int [] pushA,int [] popA) &#123; if(pushA.length==0||popA.length==0)return false; Stack&lt;Integer&gt; S=new Stack&lt;Integer&gt;(); int popIndex = 0; for(int i=0;i&lt;pushA.length;i++)&#123; S.push(pushA[i]); while(!S.empty()&amp;&amp;popA[popIndex]==S.peek())&#123; S.pop(); popIndex++; &#125; &#125; return S.empty(); &#125;&#125; 九、数组中出现次数超过一半的数字（剑29）数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 数组中有一个数字出现的次数超过数组长度的一半，也就是说它出现的次数比其他所有数字出现的次数的和还要多。因此我们可以考虑在遍历数组的时候保存两个值：一个是数组的一个数字，一个是次数。当我们遍历到下一个数字的时候，如果下一个数字和我们之前保存的数字相同，则次数加1；如果不同，则次数减1；如果次数为0，则保存下一个数字，并把次数设为1。 还要判断这个数字是否超过数组长度的一半，如果不存在输出0。 123456789101112131415161718192021222324252627282930313233public class Solution &#123; public int MoreThanHalfNum_Solution(int [] array) &#123; if(array==null||array.length==0)&#123; return 0; &#125; int result=array[0]; int count=1; for(int i=1;i&lt;array.length;i++)&#123; if(result==array[i])&#123; count++; &#125; else if(result!=array[i])&#123; count--; &#125; if(count==0)&#123; result=array[i]; count=1; &#125; &#125; int times=0; for(int i=0;i&lt;array.length;i++)&#123; if(array[i]==result)&#123; times++; &#125; &#125; if(times*2&lt;=array.length)&#123; System.out.println(times); return 0; &#125; else return result; &#125;&#125; 十、最小的K个数（剑30）输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 第一种方法，借用partition函数 java 1234567891011121314151617181920212223242526272829303132333435363738import java.util.ArrayList; public class Solution &#123; public ArrayList&lt;Integer&gt; GetLeastNumbers_Solution(int[] input, int k) &#123; ArrayList&lt;Integer&gt; output = new ArrayList&lt;Integer&gt;(); int length = input.length; if (input == null || length &lt;= 0 || length &lt; k || k&lt;= 0) &#123; return output; &#125; int low = 0; int high = length - 1; while(low&lt;high) &#123; int pivotloc = partition(input,low,high); if(pivotloc==k-1)break; else if(pivotloc &lt; k - 1) &#123; low = pivotloc + 1; //在右边 &#125; else &#123; high = pivotloc - 1; //在左边 &#125; &#125; for (int i = 0;i &lt; k;i++) &#123; output.add(input[i]); &#125; return output; &#125; //基准左右分区 private int partition(int[] input,int low,int high) &#123; int pivot = input[low]; while(low &lt; high) &#123; while(input[high] &gt;= pivot &amp;&amp; low &lt; high) high--; input[low] = input[high]; while(input[low] &lt;= pivot &amp;&amp; low &lt;high) low++; input[high] = input[low]; &#125; input[low] = pivot; return low; &#125; &#125; 第二种方法 用最大堆保存这k个数，每次只和堆顶比，如果比堆顶小，删除堆顶，新数入堆。时间 $O(NlogK)$ 空间 $O(K)$ java 1234567891011121314151617181920212223242526272829import java.util.ArrayList;import java.util.PriorityQueue;import java.util.Comparator;public class Solution &#123; public ArrayList&lt;Integer&gt; GetLeastNumbers_Solution(int[] input, int k) &#123; ArrayList&lt;Integer&gt; result = new ArrayList&lt;Integer&gt;(); int length = input.length; if(k &gt; length || k == 0)&#123; return result; &#125; PriorityQueue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;Integer&gt;(k, new Comparator&lt;Integer&gt;() &#123; @Override//PriorityQueue默认是小顶堆，实现大顶堆，需要反转默认排序器 public int compare(Integer o1, Integer o2) &#123; return o2.compareTo(o1); &#125; &#125;); //遍历数组时将数字加入优先队列（堆），一旦堆的大小大于k就将堆顶元素去除，确保堆的大小为k。遍历完后堆中的数就是最小的K个数。 for (int i:input) &#123; maxHeap.offer(i); if(maxHeap.size()&gt;k) maxHeap.poll(); &#125; //输出大顶堆中的数 for (int integer : maxHeap) &#123; result.add(integer); &#125; return result; &#125;&#125; 十一、连续子数组的最大和（剑31）输入一个整型数组，数组中有正数也有负数。数组中一个或连续的多个整数组成一个子数组。求所有子数组的和的最大值。要求时间复杂度为O(n) 第一种方法 java 12345678910111213141516171819public class Solution &#123; public int FindGreatestSumOfSubArray(int[] array) &#123; if(array.length==0||array==null) return 0; int cSum = 0; int result = array[0];// result存储最大和，不能初始为0，存在负数 for(int i=0;i&lt;array.length;i++)&#123; if(cSum&lt;0)&#123; cSum=array[i];// 当前和&lt;0，抛弃不要 &#125;else&#123; cSum += array[i];//否则累加上去 &#125; if(cSum&gt;result)&#123; result = cSum;// 存储当前的最大结果 &#125; &#125; return result; &#125;&#125; 第二种方法：动态规划 1234567891011121314151617181920212223F（i）：以array[i]为末尾元素的子数组的和的最大值，子数组的元素的相对位置不变 F（i）=max（F（i-1）+array[i] ， array[i]） res：所有子数组的和的最大值 res=max（res，F（i）） 如数组[6, -3, -2, 7, -15, 1, 2, 2] 初始状态： F（0）=6 res=6 i=1： F（1）=max（F（0）-3，-3）=max（6-3，3）=3 res=max（F（1），res）=max（3，6）=6 i=2： F（2）=max（F（1）-2，-2）=max（3-2，-2）=1 res=max（F（2），res）=max（1，6）=6 i=3： F（3）=max（F（2）+7，7）=max（1+7，7）=8 res=max（F（2），res）=max（8，6）=8 i=4： F（4）=max（F（3）-15，-15）=max（8-15，-15）=-7 res=max（F（4），res）=max（-7，8）=8 以此类推 最终res的值为8 java 1234567891011public class Solution &#123; public int FindGreatestSumOfSubArray(int[] array) &#123; int res = array[0]; int max = array[0]; for(int i=1;i&lt;array.length;i++)&#123; max=Math.max(max+array[i],array[i]); res = Math.max(max,res); &#125; return res; &#125;&#125; 十二、从1到n整数中1出现的次数（剑32）输入一个整数n，求1到n这n个整数的十进制表示中1出现的次数。例如输入12，从1到12这些整数中包含1的数字有1、10、11、12，1一共出现了5次。 一、1的数目 编程之美上给出的规律： 如果第i位（自右至左，从1开始标号）上的数字为0，则第i位可能出现1的次数由更高位决定（若没有高位，视高位为0），等于更高位数字X当前位数的权重$10^{i-1}$。 如果第i位上的数字为1，则第i位上可能出现1的次数不仅受更高位影响，还受低位影响（若没有低位，视低位为0），等于更高位数字X当前位数的权重$10^{i-1}+$（低位数字+1）。 如果第i位上的数字大于1，则第i位上可能出现1的次数仅由更高位决定（若没有高位，视高位为0），等于（更高位数字+1）X当前位数的权重$10^{i-1}$。 二、X的数目这里的 X∈[1,9] ，因为 X=0 不符合下列规律，需要单独计算。首先要知道以下的规律： 从 1 至 10，在它们的个位数中，任意的 X 都出现了 1 次。 从 1 至 100，在它们的十位数中，任意的 X 都出现了 10 次。 从 1 至 1000，在它们的百位数中，任意的 X 都出现了 100 次。 依此类推，从 1 至 $10^i$ ，在它们的左数第二位（右数第 i 位）中，任意的 X 都出现了 $10^{i−1}$ 次。 这个规律很容易验证，这里不再多做说明。 接下来以 n=2593,X=5 为例来解释如何得到数学公式。从 1 至 2593 中，数字 5 总计出现了 813 次，其中有 259 次出现在个位，260 次出现在十位，294 次出现在百位，0 次出现在千位。 现在依次分析这些数据，首先是个位。从 1 至 2590 中，包含了 259 个 10，因此任意的 X 都出现了 259 次。最后剩余的三个数 2591, 2592 和 2593，因为它们最大的个位数字 3 &lt; X，因此不会包含任何 5。（也可以这么看，3&lt;X，则个位上可能出现的X的次数仅由更高位决定，等于更高位数字$（259）\times 10^{1-1}=259$）。 然后是十位。从 1 至 2500 中，包含了 25 个 100，因此任意的 X 都出现了 25×10=250 次。剩下的数字是从 2501 至 2593，它们最大的十位数字9&gt;X，因此会包含全部10个5。最后总计250 + 10 = 260。（也可以这么看，9&gt;X，则十位上可能出现的X的次数仅由更高位决定，等于更高位数字$（25+1）\times 10^{2-1}=260$）。 接下来是百位。从 1 至 2000 中，包含了 2 个 1000，因此任意的 X 都出现了 2×100=200 次。剩下的数字是从 2001 至 2593，它们最大的百位数字 5 == X，这时情况就略微复杂，它们的百位肯定是包含 5 的，但不会包含全部 100 个。如果把百位是 5 的数字列出来，是从 2500 至 2593，数字的个数与百位和十位数字相关，是 93+1 = 94。最后总计 200 + 94 = 294。（也可以这么看，5==X，则百位上可能出现X的次数不仅受更高位影响，还受低位影响，等于更高位数字$（2）\times 10^{3-1}+（93+1）=294$）。 最后是千位。现在已经没有更高位，因此直接看最大的千位数字 2 &lt; X，所以不会包含任何 5。（也可以这么看，2&lt;X，则千位上可能出现的X的次数仅由更高位决定，等于更高位数字$（0）\times 10^{4-1}=0$）。 到此为止，已经计算出全部数字 5 的出现次数。总结一下以上的算法，可以看到，当计算右数第 i 位包含的 X 的个数时： 取第 i 位左边（高位）的数字，乘以$10^{i−1}$ ，得到基础值a 。 取第 i 位数字，计算修正值： 如果大于 X，则结果为 $a+ 10^{i−1}$ 。 如果小于 X，则结果为 a 。 如果等 X，则取第 i 位右边（低位）数字，设为 b ，最后结果为 a+b+1 。 相应的代码非常简单，效率也非常高，时间复杂度只有 $ O( log_ {10} n) $。 代码如下： 1234567891011121314151617181920212223public class Solution &#123; public int NumberOf1Between1AndN_Solution(int n) &#123; if (n &lt; 1) return 0; int len = getLenOfNum(n); if (len == 1) return 1; int tmp = (int) Math.pow(10, len - 1); int first = n / tmp; int firstOneNum = first == 1 ? n % tmp + 1 : tmp; int otherOneNUm = first * (len - 1) * (tmp / 10); return firstOneNum + otherOneNUm + NumberOf1Between1AndN_Solution(n % tmp); &#125; private int getLenOfNum(int n) &#123; int len = 0; while (n != 0) &#123; len++; n /= 10; &#125; return len; &#125;&#125; 十三、把数组排成最小的数（剑33）输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 java 123456789101112131415161718192021222324252627282930import java.util.ArrayList;import java.util.Collections;import java.util.Comparator;public class Solution &#123; public String PrintMinNumber(int [] numbers) &#123; int n; String s=""; ArrayList&lt;Integer&gt; list=new ArrayList&lt;Integer&gt;(); n=numbers.length; for(int i=0;i&lt;n;i++)&#123; list.add(numbers[i]);//将数组放入arrayList中 &#125; //实现了Comparator接口的compare方法，将集合元素按照compare方法的规则进行排序 Collections.sort(list,new Comparator&lt;Integer&gt;()&#123; @Override public int compare(Integer str1, Integer str2) &#123; // TODO Auto-generated method stub String s1=str1+""+str2; String s2=str2+""+str1; return s1.compareTo(s2); &#125; &#125;); for(int j:list)&#123; s+=j; &#125; return s; &#125;&#125; 十四、丑数（剑34）把只包含因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 java 12345678910111213141516171819import java.util.*;public class Solution &#123; public int GetUglyNumber_Solution(int index) &#123; if(index&lt;7)return index; int[] res = new int[index]; res[0] = 1; int t2 = 0, t3 = 0, t5 = 0, i; for(i=1;i&lt;index;i++)&#123; res[i] = min(res[t2]*2,min(res[t3]*3,res[t5]*5)); if(res[i] == res[t2]*2)t2++; if(res[i] == res[t3]*3)t3++; if(res[i] == res[t5]*5)t5++; &#125; return res[index-1]; &#125; private int min(int a,int b)&#123; return (a&gt;b)? b:a; &#125;&#125; 十五、数组中的逆序对（剑36）在数组中的两个数字如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组，求出这个数组中的逆序对的总数。例如在数组{7，5，6，4}中，一共存在5个逆序对，分别是（7，6）、（7，5）、（7，4）、（5，4）和（6，4）。 可以按照归并排序的思路，先把数组分隔成子数组，先统计出子数组内部的逆序对的数目，然后再统计出两个相邻子数组之间的逆序对的数目。在统计逆序对的过程中，还需要对数组进行排序。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Solution &#123; int cnt; public int InversePairs(int[] array) &#123; cnt = 0; if (array != null) mergeSortUp2Down(array, 0, array.length - 1); return cnt; &#125; /* * 归并排序(从上往下) */ public void mergeSortUp2Down(int[] a, int start, int end) &#123; if (start &gt;= end) return; int mid = (start + end) &gt;&gt; 1; mergeSortUp2Down(a, start, mid); mergeSortUp2Down(a, mid + 1, end); merge(a, start, mid, end); &#125; /* * 将一个数组中的两个相邻有序区间合并成一个 */ public void merge(int[] a, int start, int mid, int end) &#123; int[] tmp = new int[end - start + 1]; int i = start, j = mid + 1, k = 0; while (i &lt;= mid &amp;&amp; j &lt;= end) &#123; if (a[i] &lt;= a[j]) tmp[k++] = a[i++]; else &#123; tmp[k++] = a[j++]; cnt += mid - i + 1; //关键的一步，统计逆序对.......... cnt%=1000000007; &#125; &#125; while (i &lt;= mid) tmp[k++] = a[i++]; while (j &lt;= end) tmp[k++] = a[j++]; for (k = 0; k &lt; tmp.length; k++) a[start + k] = tmp[k]; &#125;&#125; 十六、数字在排序数组中出现的次数（剑38）统计一个数字在排序数组中出现的次数。 利用二分查找直接找到第一个K和最后一个K。以下代码使用递归方法找到第一个K，使用循环方法最后一个K。 java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class Solution &#123; public int GetNumberOfK(int [] array , int k) &#123; int length = array.length; if(length == 0)&#123; return 0; &#125; int firstK = getFirstK(array, k, 0, length-1); int lastK = getLastK(array, k, 0, length-1); if(firstK != -1 &amp;&amp; lastK != -1)&#123; return lastK - firstK + 1; &#125; return 0; &#125; //递归写法 private int getFirstK(int [] array , int k, int start, int end)&#123; if(start &gt; end)&#123; return -1; &#125; int mid = (start + end) &gt;&gt; 1; if(array[mid] &gt; k)&#123; return getFirstK(array, k, start, mid-1); &#125; else if (array[mid] &lt; k)&#123; return getFirstK(array, k, mid+1, end); &#125; else if(mid-1 &gt;=0 &amp;&amp; array[mid-1] == k)&#123; return getFirstK(array, k, start, mid-1); &#125; else&#123; return mid; &#125; &#125; //循环写法 private int getLastK(int [] array , int k, int start, int end)&#123; int length = array.length; int mid = (start + end) &gt;&gt; 1; while(start &lt;= end)&#123; if(array[mid] &gt; k)&#123; end = mid-1; &#125; else if(array[mid] &lt; k)&#123; start = mid+1; &#125; else if(mid+1 &lt;= length-1 &amp;&amp; array[mid+1] == k)&#123; start = mid+1; &#125; else&#123; return mid; &#125; mid = (start + end) &gt;&gt; 1; &#125; return -1; &#125;&#125; 十七、数组中只出现一次的数字（剑40）一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。要求时间复杂度是$O(n)$，空间复杂度是$O(1)$。 首先我们考虑这个问题的一个简单版本：一个数组里除了一个数字之外，其他的数字都出现了两次。请写程序找出这个只出现一次的数字。 这个题目的突破口在哪里？题目为什么要强调有一个数字出现一次，其他的出现两次？我们想到了异或运算的性质：任何一个数字异或它自己都等于0 。也就是说，如果我们从头到尾依次异或数组中的每一个数字，那么最终的结果刚好是那个只出现一次的数字，因为那些出现两次的数字全部在异或中抵消掉了。 有了上面简单问题的解决方案之后，我们回到原始的问题。如果能够把原数组分为两个子数组。在每个子数组中，包含一个只出现一次的数字，而其它数字都出现两次。如果能够这样拆分原数组，按照前面的办法就是分别求出这两个只出现一次的数字了。 我们还是从头到尾依次异或数组中的每一个数字，那么最终得到的结果就是两个只出现一次的数字的异或结果。因为其它数字都出现了两次，在异或中全部抵消掉了。由于这两个数字肯定不一样，那么这个异或结果肯定不为0 ，也就是说在这个结果数字的二进制表示中至少就有一位为1 。我们在结果数字中找到第一个为1 的位的位置，记为第N 位。现在我们以第N 位是不是1 为标准把原数组中的数字分成两个子数组，第一个子数组中每个数字的第N 位都为1 ，而第二个子数组的每个数字的第N 位都为0 。 现在我们已经把原数组分成了两个子数组，每个子数组都包含一个只出现一次的数字，而其它数字都出现了两次。因此到此为止，所有的问题我们都已经解决。 java 123456789101112131415161718192021222324252627282930313233//num1,num2分别为长度为1的数组。传出参数//将num1[0],num2[0]设置为返回结果public class Solution &#123; public void FindNumsAppearOnce(int [] array,int num1[] , int num2[]) &#123; if(array==null ||array.length&lt;2) return ; int temp = 0; for(int i=0;i&lt;array.length;i++) temp ^= array[i]; int indexOf1 = findFirstBitIs(temp); for(int i=0;i&lt;array.length;i++)&#123; if(isBit(array[i], indexOf1)) num1[0]^=array[i]; else num2[0]^=array[i]; &#125; &#125; //在正数num的二进制表示中找到最右边是1的位 public int findFirstBitIs(int num)&#123; int indexBit = 0; while(((num &amp; 1)==0) &amp;&amp; (indexBit)&lt;8*4)&#123; num = num &gt;&gt; 1; ++indexBit; &#125; return indexBit; &#125; //判断在num的二进制表示中从右边数起的indexBit位是不是1. public boolean isBit(int num,int indexBit)&#123; num = num &gt;&gt; indexBit; return (num &amp; 1) == 1; &#125;&#125; 十八、 和为s的两个数字（剑41.1）一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。 数列满足递增，设两个头尾两个指针i和j， 若ai + aj == sum，就是答案（相差越远乘积越小） 若ai + aj &gt; sum，aj肯定不是答案之一（前面已得出 i 前面的数已是不可能），j -= 1 若ai + aj &lt; sum，ai肯定不是答案之一（前面已得出 j 后面的数已是不可能），i += 1 时间复杂度为O(n)。 1234567891011121314151617181920212223import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; FindNumbersWithSum(int [] array,int sum) &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); if(array==null||array.length&lt;2)&#123; return list; &#125; int i=0,j=array.length-1; while(i&lt;j)&#123; if(array[i]+array[j]==sum)&#123; list.add(array[i]); list.add(array[j]); break; &#125; else if(array[i]+array[j]&gt;sum)&#123; j--; &#125; else i++; &#125; return list; &#125;&#125; 十九、和为s的连续正数序列（剑41.2）输入一个正数s，打印出所有和为s的连续正数序列（至少含有两个数）。例如输入15，由于1+2+3+4+5=4+5+6=7+8=15，所以结果打印出三个连续序列1~5、4~6和7~8。 考虑用两个数small和big分别表示序列的最小值和最大值。首先把small初始化为1，big初始化为2，如果从small到big的序列和大于s，我们可以从序列中去掉较小的值，也就是增大small的值。如果从small到big的序列和小于s，我们可以增大big，让这个序列包含更多的数字。因为这个序列至少要有两个数字，我们一直增加small到（1+s）/2为止。 java 12345678910111213141516171819202122232425262728293031323334353637import java.util.ArrayList;/**初始化small=1，big=2;*small到big序列和小于sum，big++;大于sum，small++;*当small增加到(1+sum)/2是停止*/public class Solution &#123; public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindContinuousSequence(int sum) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; lists=new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if(sum&lt;=1)&#123;return lists;&#125; int small=1; int big=2; while(small!=(1+sum)/2)&#123; //当small==(1+sum)/2的时候停止 int curSum=sumOfList(small,big); if(curSum==sum)&#123; ArrayList&lt;Integer&gt; list=new ArrayList&lt;Integer&gt;(); for(int i=small;i&lt;=big;i++)&#123; list.add(i); &#125; lists.add(list); small++;big++; &#125;else if(curSum&lt;sum)&#123; big++; &#125;else&#123; small++; &#125; &#125; return lists; &#125; public int sumOfList(int head,int leap)&#123; //计算当前序列的和 int sum=head; for(int i=head+1;i&lt;=leap;i++)&#123; sum+=i; &#125; return sum; &#125;&#125; 二十、扑克牌的顺子（剑44）从扑克牌中随机抽5张牌，判断是不是顺子，即这5张牌是不是连续的。2~10为数字本身，A为1，J为11，Q为12，K为13，而大小王可以看做是任意数字，这里定为0. java 12345678910111213141516171819202122232425262728293031import java.util.*;public class Solution &#123; public boolean isContinuous(int [] numbers) &#123; int length = numbers.length; if(numbers==null||length==0)return false;//特殊情况 Arrays.sort(numbers);//排序 //统计数组中0的个数 int numberOfZero = 0; for(int i =0;i&lt;length&amp;&amp;numbers[i]==0;i++)&#123; ++numberOfZero; &#125; int numberOfGap = 0; int small = numberOfZero; int big = small+1; while(big&lt;length)&#123; //含有对子，不可能是顺子 if(numbers[small]==numbers[big])&#123; return false; &#125; //统计数组中的间隔数目 numberOfGap += numbers[big]-numbers[small]-1; small=big; big++; &#125; //如果间隔数小于等于零的数量则可以组成顺子，否则不行。 if(numberOfGap&lt;=numberOfZero)&#123; return true; &#125;else&#123;return false;&#125; &#125;&#125; 二十一、求1+2+…..+n（剑46）求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 需利用逻辑与的短路特性实现递归终止。 当n==0时，(n&gt;0)&amp;&amp;((sum+=Sum_Solution(n-1))&gt;0)只执行前面的判断，为false，然后直接返回0； 当n&gt;0时，执行sum+=Sum_Solution(n-1)，实现递归计算Sum_Solution(n)。 java 12345678public class Solution &#123; public int Sum_Solution(int n) &#123; int sum=n; boolean ans = (n&gt;0)&amp;&amp;((sum+=Sum_Solution(n-1))&gt;0); return sum; &#125;&#125; 二十二、数组中重复的数字（剑51）在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是重复的数字2或者3。 java 12345678910111213141516171819202122232425public class Solution &#123; public boolean duplicate(int numbers[],int length,int [] duplication) &#123; if(numbers==null||length==0)&#123;return false;&#125;//空指针或空数组 // 判断数组是否合法,即每个数都在0~n-1之间 for(int i=0;i&lt;length;i++)&#123; if(numbers[i]&gt;length-1||numbers[i]&lt;0)&#123; return false; &#125; &#125; //若数值与下标不同，则调换位置； //比较位置下标为数值(numbers[i])的数值(numbers[numbers[i]])与该数值(numbers[i])是否一致，若一致，则说明有重复数字 for(int i=0;i&lt;length;i++)&#123; while(numbers[i]!=i)&#123; if(numbers[i]==numbers[numbers[i]])&#123; duplication[0] = numbers[i]; return true; &#125; int temp=numbers[i]; numbers[i]=numbers[temp]; numbers[temp]=temp; &#125; &#125; return false; &#125;&#125; 二十三、构建乘积数组（剑52） 给定一个数组A[0,1,…,n-1],请构建一个数组$B[0,1,…,n-1]$,其中B中的元素$B[i]=A[0]A[1]…A[i-1]A[i+1]…A[n-1]$。不能使用除法。 java 12345678910111213141516171819import java.util.ArrayList;public class Solution &#123; public int[] multiply(int[] A) &#123; int length = A.length; int[] B = new int[length]; if(length!=0)&#123; B[0]=1; for(int i=1;i&lt;length;i++)&#123; B[i]=B[i-1]*A[i-1]; &#125; int temp=1; for(int j=length-2;j&gt;=0;j--)&#123; temp = temp*A[j+1]; B[j]=temp*B[j]; &#125; &#125; return B; &#125;&#125; 二十四、 滑动窗口的最大值（剑65）给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 java 123456789101112131415161718192021222324252627282930313233343536373839import java.util.ArrayList;import java.util.LinkedList;public class Solution &#123; public ArrayList&lt;Integer&gt; maxInWindows(int [] num, int size) &#123; ArrayList&lt;Integer&gt; ret = new ArrayList&lt;&gt;(); if (num == null) &#123; return ret; &#125; if (num.length &lt; size || size &lt; 1) &#123; return ret; &#125; LinkedList&lt;Integer&gt; indexDeque = new LinkedList&lt;&gt;(); //前size-1个中，前面比num[i]小的，对应下标从下标队列移除； for (int i = 0; i &lt; size - 1; i++) &#123; if (!indexDeque.isEmpty() &amp;&amp; num[i] &gt; num[indexDeque.getLast()]) &#123; indexDeque.removeLast(); &#125; indexDeque.addLast(i); &#125; //从第size-1个开始；前面比num[i]小的，对应下标从下标队列移除； for (int i = size - 1; i &lt; num.length; i++) &#123; while(!indexDeque.isEmpty() &amp;&amp; num[i] &gt; num[indexDeque.getLast()]) &#123; indexDeque.removeLast(); &#125; //把下一个下标加入队列中 indexDeque.addLast(i); //当第一个数字的下标与当前处理的数字的下标之差大于或者等于滑动窗口的大小时，这个数字已经从窗口划出，可以移除了； if (i - indexDeque.getFirst() + 1 &gt; size) &#123; indexDeque.removeFirst(); &#125; //下标队列的第一个是滑动窗口最大值对应的下标； ret.add(num[indexDeque.getFirst()]); &#125; return ret; &#125;&#125; 二十五、矩阵中的路径（剑66）请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则该路径不能再进入该格子。 例如下面的矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 java 12345678910111213141516171819202122232425262728293031public class Solution &#123; public int movingCount(int threshold, int rows, int cols) &#123; boolean[] visited=new boolean[rows*cols]; return movingCountCore(threshold, rows, cols, 0,0,visited); &#125; private int movingCountCore(int threshold, int rows, int cols, int row,int col,boolean[] visited) &#123; if(row&lt;0||row&gt;=rows||col&lt;0||col&gt;=cols) return 0; int i=row*cols+col; if(visited[i]||!checkSum(threshold,row,col)) return 0; visited[i]=true; return 1+movingCountCore(threshold, rows, cols,row,col+1,visited) +movingCountCore(threshold, rows, cols,row,col-1,visited) +movingCountCore(threshold, rows, cols,row+1,col,visited) +movingCountCore(threshold, rows, cols,row-1,col,visited); &#125; private boolean checkSum(int threshold, int row, int col) &#123; int sum=0; while(row!=0)&#123; sum+=row%10; row=row/10; &#125; while(col!=0)&#123; sum+=col%10; col=col/10; &#125; if(sum&gt;threshold) return false; return true; &#125;&#125; 二十六、寻找某个值的区间（leetcode 34 Search for a Range）主要考查二分查找，如果读者对二分查找不是很熟悉，这里推荐一篇博客：你真的会二分查找吗？上面讲得非常详细。 这题要求在一个排好序可能有重复元素的数组里面找到包含某个值的区间范围。要求使用O(log n)的时间，所以我们采用两次二分查找。 主要实现两个方法： searchRightIndex：查找并返回target出现在nums数组最右边的index。注意一点，如果target比数组最小值还小，那么返回-1 searchLeftIndex：查找并返回target出现在nums数组最左边的index。如果target比数组最大值还大，那么返回nums.length 1234567891011121314151617181920212223242526272829public class Solution &#123; public int[] searchRange(int[] nums, int target) &#123; int[] result = &#123;-1, -1&#125;; int index = searchRightIndex(nums, 0, nums.length - 1, target); if (index &lt; 0 || nums[index] != target) return result; result[0] = searchLeftIndex(nums, 0, index, target); result[1] = index; return result; &#125; //查找并返回target出现在nums数组最右边的index public int searchRightIndex(int[] nums, int left, int right, int target) &#123; while (left &lt;= right) &#123; int mid = (left + right) / 2; if (nums[mid] &gt; target) right = mid - 1; else left = mid + 1; &#125; return right; &#125; //查找并返回target出现在nums数组最左边的index public int searchLeftIndex(int[] nums, int left, int right, int target) &#123; while (left &lt;= right) &#123; int mid = (left + right) / 2; if (nums[mid] &lt; target) left = mid + 1; else right = mid - 1; &#125; return left; &#125; &#125; 二十七、第K个数的问题27.1 无序数组寻找第K大的数这题是一道很好的面试题目，首先题目短小，很快就能说清题意而且有很多种解法。从简单到复杂的解法都有，梯度均匀。解决它不需要预先知道特殊领域知识。 这题有很多思路： 按从大到小全排序，然后取第k个元素，时间复杂度O(nlogn)，空间复杂度O(1) 利用堆进行部分排序。维护一个大根堆，将数组元素全部压入堆，然后弹出k次，第k个就是答案。时间复杂度$O(klogn)$，空间复杂度$O(n)$ 选择排序，第k次选择后即可得到第k大的数，时间复杂度O(nk)，空间复杂度O(1) 以上三种方法时间复杂度太高。下面介绍两种更好的方法： 第一种 利用快速排序中的partition思想，从数组中随机选择一个基准pivot，把数组划分为左右两部分，左边部分元素小于pivot，右边部分元素大于或等于pivot。 1234567891011121314151617181920212223242526272829public class Solution &#123; public static int findKthLargest(int[] nums, int k) &#123; k = nums.length-k;//找前K大的 int low = 0; int high = nums.length-1; while(low &lt; high)&#123; int pivotloc = partition(nums,low,high); if(pivotloc==k) break; else if(k&lt;pivotloc)&#123;//左边 high = pivotloc-1; &#125;else if(k&gt;pivotloc)&#123; low = pivotloc+1;//在右边 &#125; &#125; return nums[k]; &#125; public static int partition(int[] nums, int low, int high)&#123; int pivot = nums[low]; while(low&lt;high)&#123; while(low&lt;high &amp;&amp; nums[high]&gt;=pivot) high--; nums[low]=nums[high]; while(low&lt;high &amp;&amp; nums[low]&lt;=pivot) low++; nums[high] =nums[low]; &#125; nums[low]=pivot; return low; &#125;&#125; 第二种方法： 遍历数组时将数字加入优先队列（堆），一旦堆的大小大于k就将堆顶元素去除，确保堆的大小为k。遍历完后堆顶就是返回值。 12345678910public class Solution &#123; public int findKthLargest(int[] nums, int k) &#123; PriorityQueue&lt;Integer&gt; p = new PriorityQueue&lt;Integer&gt;(); for(int i = 0 ; i &lt; nums.length; i++)&#123; p.add(nums[i]); if(p.size()&gt;k) p.poll(); &#125; return p.poll(); &#125;&#125; 27.2 两个有序数组的第K大的数两个有序数组，寻找归并排序后数组的中位数/第 k 大数字（与二分有关）； 27.3 N个有序数组求最大的K个数27.4 求两个有序数组的中位数二十八、求根算法（ LeetCode 69）这道题要找x的平方根，x的平方根肯定小于x/2。要在[1,x/2]有序序列当中找一个数，用二分法： 1234567891011121314public int mySqrt(int x) &#123; long high = (x / 2) + 1; long low = 0; while (high &gt;= low) &#123; long mid = (high + low) / 2; if (mid * mid == x) return (int)mid; else if (mid * mid &gt; x) high = mid - 1; else low = mid + 1; &#125; return (int)high; &#125; 二分法在数值计算中非常常见，还是得熟练掌握。其实这个题目还有另一种方法，称为牛顿法。不过他更多的是一种数学方法，算法在这里没有太多体现，不过牛顿法是数值计算中非常重要的方法，所以我还是介绍一下。不太了解牛顿法基本思想的朋友，可以参考一下牛顿法-维基百科。一般牛顿法是用数值方法来解一个f(y)=0的方程（为什么变量在这里用y是因为我们要求的开方是x，避免歧义）。对于这个问题我们构造f(y)=y^2-x，其中x是我们要求平方根的数，那么当f(y)=0时，即y^2-x=0,所以y=sqrt(x),即是我们要求的平方根。f(y)的导数f’(y)=2*y，根据牛顿法的迭代公式我们可以得到y_(n+1)=y_n-f(n)/f’(n)=(y_n+x/y_n)/2。最后根据以上公式来迭代解以上方程。 1234567891011public int sqrt(int x) &#123; if (x == 0) return 0; double lastY = 0; double y = 1; while (y != lastY) &#123; lastY = y; y = (y + x / y) / 2; &#125; return (int)(y); &#125; 面经中出现过的数组题已整理 一、二维数组，每行递增，每列递增,实现查找；二维数组，每行递增，每列递增，求第 k 大的数； 三、旋转数组的查找；有序数组 从中间某点隔开，右边的放到左边，然后问在这个数组中怎么进行二分查找。讲了思路后手写代码；一维有序数组，经过循环位移后，最小的数出现在数列中间，如果原数组严格递增，如何找这个最小数；如果原数组严格递增或递减，如何找这个最小数；如果原数组非严格递增或递减，如何找这个最小数； 五、调整数组顺序使奇数位于偶数前面；让一个数组中的所有奇数在前，偶数在后。 六、顺时针打印矩阵 九、数组中超过一半的数字；找出数组中出现次数超过一半的数，现在有一个数组，已知一个数出现的次数超过了一半，请用O(n)的复杂度的算法找出这个数我说了一个最简答的，直接遍历数组，用map存储《数，出现的次数》这个键-值对，然后找出超过一半的即可。继续优化，，，，没答上来 十、最小的K个数；数列中找第 k 大的数字（与快排或堆排序有关）；编程题最少时间复杂度求数组中第k大的数， 十一、连续子数组的最大和 十二、从1到n整数中1出现的次数；给你一个数N，问1-N这N个整数里面，每个位上一共出现多少次数字1. 这个是编程之美上的原题。当时看的时候觉得好复杂，最后我也没写出来，当然，面试的技巧就是，无论这道题你会还是不会，尽量把你的思考过程说出来，一方面防止冷场，另一方面可以让他知道你的思考过程。最后面试官让我不用最优的，我就写了个最笨的。 十五、数组中逆序对计算，打印逆序对儿，如输入数组[1,4,2,5,3]，输出(4,2),(4,3),(5,3)；统计数列中的逆序对（归并排序有关）； 十八、和为S的两个数字VS和为s的连续正数序列；有序数组寻找和为某数的一对数字；寻找和为定值的多个数；寻找和为定值的两个数 二十四、滑动窗口的最大值；coding：数组滑动窗口得到最大的窗口，O（n）复杂度、扩展：这个滑动窗口中必须有m个不同才可以，目标不变，如果改，并分析时间复杂度； 二十六、寻找某个值的区间（leetcode 34 Search for a Range） 未整理数组中后面的数减前面的数的差的最大值，要求时间、空间复杂度尽可能底多个有序数组的归并多个有序数组求交集两个有序数组求差集两个数组，求差集两个集合如何求并集，交集； leetcode 89 Gray Codeleetcode 42 trapping rain water 墙里能装多少水 有一个数组，让找到两个不重复的连续子序列A,B ，求Max(Sum(A)-Sum(B)。 3分钟解出，10分钟写完代码 LeetCode原题： 有一个集合A包含了一些数，输入N，求元素个数最小的集合B，使得A并B后内的数组合相加能够组成1到N中的所有数 给定一个数组，长度已知为N（中等），求中位数，要求时间复杂度尽可能小：快排+丢弃长度小于N/2的部分 给定数组，要求以最小的时间复杂度求得最大最小值：维护一个小数组，只存放两个数，预定义为min和max，后将剩余N-2个数和它们依次比较，大于max的覆盖max，小于min的覆盖min，其余情形的不更新数组。时间复杂度o(n)。 然后考了个数据结构，给个数组如何建堆http://xfhnever.com/categories/算法与数据结构/page/2/ 一维数组，swap 其中的几对数字（每个数字只属于一次 swap 操作），实现查找（与二分有关）；一个有序数组，其中一个数字发生变异，但不知道变异后会不会影响整体序，如何实现查找；任意交换其中的两数，发现并恢复； 给定数组，寻找 next big（堆排序有关）； 数组可能是递增、递减、递减后递增、递增后递减四种情况，递增递减都是非严格的，如果有转折点，返回转折点的值，否则返回-1； 给出一个数组，返回数组中满足类似a &gt; b &lt; c ＞ｄ这种情况的连续子数组的最大长度。时间复杂度当然是Ｏ（ｎ），比较有意思的题目。 同样是一个数组，去掉其中一个数，得到与这个数相邻中比较小的那个数的值（如果是最左侧或者最右侧则返回０）。不断重复上面的操作，直到数组为空，返回累加的最大值。 已知一个数组，有n+2个不同的数，其中n个数出现了偶数次，2个数出现了奇数次，设计算法找出这2个数又只想出一个简单的，用栈，偶数的进出进出，最后在栈中没有了，奇数的进出进，最后会留在栈中。就找到了（这个空间复杂度为O（n）继续优化，，又没想出来。 写递推公式给定整数n和m，问能不能找出整数x，使得x以后的所有整数都可以由整数n和m组合而成 区间查询最大值，要求查询复杂度为O(1)，正解为st表，我敲的线段树，也过了]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>数组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题解（1）：链表题解]]></title>
    <url>%2F2017%2F08%2F03%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3%EF%BC%881%EF%BC%89%EF%BC%9A%E9%93%BE%E8%A1%A8%E9%A2%98%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[链表相关题解java实现。 一、从尾到头打印链表（剑5）输入一个链表的头结点，从尾到头反过来打印每个结点的值（注意不能改变链表的结构）。 解决这个问题肯定要遍历链表。遍历的顺序是从头到尾的顺序，可输出的顺序却是从尾到头。也就是说第一个遍历到的结点最后一个输出，而最后一个遍历到的结点第一个输出。这就是典型的“后进先出”，我们可以用栈实现这种顺序。没经过一个节点的时候，把该结点放到一个栈中。当遍历完整个链表后，再从栈顶开始逐个输出结点的值，此时输出的结点的顺序就翻转过来了。实现代码如下： 12345678910111213141516import java.util.Stack;import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); while (listNode!=null)&#123; stack.push(listNode.val); listNode = listNode.next; &#125; ArrayList&lt;Integer&gt; List = new ArrayList&lt;&gt;(); while(!stack.isEmpty())&#123; List.add(stack.pop()); &#125; return List; &#125;&#125; 既然想到了用栈来实现这个函数，而递归在本质上就是一个栈结构，因此可用递归来实现。要实现反过来输出链表，我们每访问到一个节点的时候，先递归输出它后面的结点，再输出该结点自身，这样链表的输出结果就反过来了。实现代码如下： 12345678910111213141516import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; ArrayList&lt;Integer&gt; list=new ArrayList&lt;Integer&gt;(); ListNode pNode=listNode; if(pNode!=null)&#123; if(pNode.next!=null)&#123; list=printListFromTailToHead(pNode.next); &#125; list.add(pNode.val); &#125; return list; &#125;&#125; 二、在O(1)时间删除链表结点（剑13）给定单向链表的头指针和一个结点指针，定义一个函数在O(1)时间删除该结点。 我们要删除结点i，先把i的下一个结点i.next的内容复制到i，然后在把i的指针指向i.next结点的下一个结点即i.next.next，它的效果刚好是把结点i给删除了。 此外还要考虑删除的结点是头尾结点、链表中只有一个结点、链表为空这几种情况。 java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class DeleteNode &#123; /** * 链表结点 */ public static class ListNode &#123; int value; // 保存链表的值 ListNode next; // 下一个结点 &#125; /** * 给定单向链表的头指针和一个结点指针，定义一个函数在0(1)时间删除该结点, * 【注意1：这个方法和文本上的不一样，书上的没有返回值，这个因为JAVA引用传递的原因， * 如果删除的结点是头结点，如果不采用返回值的方式，那么头结点永远删除不了】 * 【注意2：输入的待删除结点必须是待链表中的结点，否则会引起错误，这个条件由用户进行保证】 * * @param head 链表表的头 * @param toBeDeleted 待删除的结点 * @return 删除后的头结点 */ public static ListNode deleteNode(ListNode head, ListNode toBeDeleted) &#123; // 如果输入参数有空值就返回表头结点 if (head == null || toBeDeleted == null) &#123; return head; &#125; // 如果删除的是头结点，直接返回头结点的下一个结点 if (head == toBeDeleted) &#123; return head.next; &#125; // 下面的情况链表至少有两个结点 // 在多个节点的情况下，如果删除的是最后一个元素 if (toBeDeleted.next == null) &#123; // 找待删除元素的前驱 ListNode tmp = head; while (tmp.next != toBeDeleted) &#123; tmp = tmp.next; &#125; // 删除待结点 tmp.next = null; &#125; // 在多个节点的情况下，如果删除的是某个中间结点 else &#123; // 将下一个结点的值输入当前待删除的结点 toBeDeleted.value = toBeDeleted.next.value; // 待删除的结点的下一个指向原先待删除引号的下下个结点，即将待删除的下一个结点删除 toBeDeleted.next = toBeDeleted.next.next; &#125; // 返回删除节点后的链表头结点 return head; &#125; 三、链表中倒数第K个结点（剑15）输入一个链表，输出该链表中倒数第k个结点。为了符合大多数人的习惯，本题从1 开始计数，即链表的尾结点是倒数第1个结点。例如一个链表有6个结点，从头结点开始它们的值依次是1、2、3、4、5、6。这个链表的倒数第3个结点的值为4的结点。 很自然的想法是先走到链表尾端，再从尾端回溯k步。可是我们从链表结点的定义可以看出本题中的链表是单向链表，单向链表的结点只有从前向后的指针而没有从后往前的指针，这种思路行不通。 既然不能从尾结点开始遍历链表，我们还是把思路回到头结点上来。假设整个链表有n个结点，那么倒数第k个结点就是从头结点开始往后走n-k+1步就可以了。如何得到结点树n？只需要从头开始遍历链表，每经过一个结点，计数器加1就行了。 也就是说我们需要遍历链表两次，第一次统计出链表中的结点的个数，第二次就能找到倒数第k个结点。但是面试官期待的解法是只需要遍历链表一次。 为了实现只遍历链表一次就能找到倒数第k个结点，我们可以定义两个指针。第一个指针从链表的头指针开始遍历向前走k-1步，第二个指针保持不动；从第k步开始，第二个指针也开始从链表的头指针开始遍历。由于两个指针的距离保持在k-1，当第一个（走在前面的）指针到达链表的尾结点时，第二个指针（走在后边的）指针正好是倒数第k个结点。 但是这样写出来的代码不够鲁棒，面试官可以找出三种办法让这段代码崩溃： 输入的ListHead为空指针。由于代码会试图访问空指针指向的内存，程序崩溃。 输入的以ListHead为头结点的链表的结点总数少于k。由于在for循环中会在链表上向前走k-1步，仍然会由于空指针造成的程序奔溃。 输入的参数k为0.由于k是一个无符号整数，那么在for循环中k-1得到的将不是-1，而是4294967295（无符号的0xFFFFFFFFF），因此for循环执行的次数远远超过我们的预计，同样也会造成程序崩溃。 面试过程中写代码特别要注意鲁棒性，若写出的代码存在多处崩溃的风险，那我们很可能和offer失之交臂。针对前面三个问题，分别处理。若输入的链表头指针为null，那么整个链表为空，此时查找倒数第k个结点自然应该返回null。若输入的k为0，也就是试图查找倒数第0个结点，由于我们计数是从1开始的，因此输入0是没有实际意义，也可以返回null。若链表的结点数少于k，在for循环中遍历链表可能会出现指向null的next，因此我们在for循环中应该加一个if循环。 代码如下： java版本 12345678910111213141516171819202122232425262728293031/*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode FindKthToTail(ListNode head,int k) &#123; if(head==null||k &lt;=0)&#123;return null;&#125; ListNode pAhead = head; ListNode pBehind = head; for(int i=1;i&lt;k;i++)&#123; if(pAhead.next != null) &#123;pAhead = pAhead.next;&#125; else &#123;return null;&#125; &#125; while(pAhead.next!=null) &#123; pAhead = pAhead.next; pBehind = pBehind.next; &#125; return pBehind; &#125;&#125; python版本 12345678910111213141516171819202122# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def FindKthToTail(self, head, k): # write code here if not head or k == 0: return None pAhead = head pBehind = None for i in xrange(0,k-1): if pAhead.next != None: pAhead = pAhead.next else: return None pBehind = head while pAhead.next != None: pAhead = pAhead.next pBehind = pBehind.next return pBehind 四、反转链表（剑16）定义一个函数，输入一个链表的头结点，反转该链表并输出反转后的头结点。链表结点定义如下： 12345678public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125; 解决与链表相关的问题总是有大量的指针操作，而指针操作的代码总是容易出错的。 为了正确地反转一个链表，需要调整链表中指针的方向。为了将调整指针这个复杂的过程分析清楚，可以借助图形来直观分析。在下图所示的链表中，h、i、j是3个相邻的结点。假设经过若干操作，我们已经把结点h之前的指针调整完毕，这些结点的next指向h，此时链表的结果如下所示： 其中（a）为一个链表，（b）把i之前的所有结点的next都指向前一个结点，导致链表在结点i、j之间断裂。 不难注意到，由于结点i的next指向了它的前一个结点，导致我们无法再链表中遍历到结点j。为了避免链表在结点i处断开，我们需要在调整结点i的next之前把结点j保存下来。 也就是说我们在调整结点i的next指针时，除了需要知道结点i本身之外，还需要前一个结点h，因为我们需要把结点i的next指向结点h。同时，我们还事先需要保存i的一个结点j，以防止链表断开。因此相应地我们需要定义3个指针，分别指向当前遍历到的结点、它的前一个结点及后一个结点。 最后我们试着找到反转后链表的头结点。不难分析出反转后链表的头结点是原始链表的尾结点。什么结点是尾结点？自然是next为null的结点。 pre\rightarrow head \rightarrow next先保存next，即$next = head.next$再反转head的指针$head.next=pre $，链表结构变成 pre\leftarrow head \ \ \ next接着向后移动结点$pre=head,head=next$ 实现代码如下： java版本 1234567891011121314151617181920212223242526272829303132public class Solution &#123; public ListNode ReverseList(ListNode head) &#123; if(head==null) return null; //head为当前节点，如果当前节点为空的话，那就什么也不做，直接返回null； ListNode pre = null; ListNode next = null; //当前节点是head，pre为当前节点的前一节点，next为当前节点的下一节点 //需要pre和next的目的是让当前节点从pre-&gt;head-&gt;next1-&gt;next2变成pre&lt;-head next1-&gt;next2 //即pre让节点可以反转所指方向，但反转之后如果不用next节点保存next1节点的话，此单链表就此断开了 //所以需要用到pre和next两个节点 //1-&gt;2-&gt;3-&gt;4-&gt;5 //1&lt;-2&lt;-3 4-&gt;5 while(head!=null)&#123; //做循环，如果当前节点不为空的话，始终执行此循环，此循环的目的就是让当前节点从指向next到指向pre //如此就可以做到反转链表的效果 //先用next保存head的下一个节点的信息，保证单链表不会因为失去head节点的原next节点而就此断裂 next = head.next; //保存完next，就可以让head从指向next变成指向pre了，代码如下 head.next = pre; //head指向pre后，就继续依次反转下一个节点 //让pre，head，next依次向后移动一个节点，继续下一次的指针反转 pre = head; head = next; &#125; //如果head为null的时候，pre就为最后一个节点了，但是链表已经反转完毕，pre就是反转后链表的第一个节点 //直接输出pre就是我们想要得到的反转后的链表 return pre; &#125;&#125; python版本 123456789101112131415161718# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回ListNode def ReverseList(self, pHead): # write code here if not pHead or not pHead.next: return pHead pre = None while pHead: next1 = pHead.next pHead.next = pre pre = pHead pHead = next1 return pre 五、合并两个排序的链表（剑17）输入两个递增排序的链表，合并这两个链表并使新链表中的结点仍然是按照递增排序的。例如下图中的链表1和链表2，则合并之后的升序链表3如下所示： 这是一个经常被各公司采用的面试题。在面试过程中，最容易犯两种错误：一是在写代码之前没有对合并的过程想清楚，最终合并出来的链表要么中间断开了，要么并没有做到递增排序；二是代码在鲁棒性方面存在问题，程序一旦有特殊的输入（如空链表）就会奔溃。首先分析合并两个链表的过程。从合并两个链表的头结点开始。链表1的头结点的值小于链表2的头结点的值，因此链表1的头结点将是合并后链表的头结点。 继续合并剩余的结点。在两个链表中剩下的结点依然是排序的，因此合并这两个链表的步骤和前面的步骤是一样的。依旧比较两个头结点的值。此时链表2的头结点值小于链表1的头结点的值，因此链表2的头结点的值将是合并剩余结点得到的链表的头结点。把这个结点和前面合并链表时得到的链表的尾结点链接起来。 当我们得到两个链表中值较小的头结点并把它链接到已经合并的链表之后，两个链表剩余的结点依然是排序的，因此合并的步骤和之前的步骤是一样的。这是典型的递归过程，我们可以定义递归函数完成这一合并过程。（解决这个问题需要大量的指针操作，如没有透彻地分析问题形成清晰的思路，很难写出正确的代码） 接下来解决鲁棒性问题，每当代码试图访问空指针指向的内存时程序就会奔溃，从而导致鲁棒性问题。本题中一旦输入空的链表就会引入空的指针，因此我们要对空链表单独处理。当第一个链表是空链表，也就是它的头结点是一个空指针时，和第二个链表合并的结果就是第二个链表。同样，当输入的第二个链表的头结点是空指针的时候，和第一个链表合并得到的结果就是第一个链表。如果两个链表都为空，合并得到的是一个空链表。（由于有大量的指针操作，如果稍有不慎就会在代码中遗留很多与鲁棒性相关的隐患。建议应聘者在写代码之前全面分析哪些情况会引入空指针，并考虑清楚怎么处理这些空指针。） 代码如下： java 1234567891011121314151617181920212223242526public class Solution &#123; public ListNode Merge(ListNode list1,ListNode list2) &#123; //新建一个头节点，用来存合并的链表。 ListNode head=new ListNode(-1); head.next=null; ListNode root=head; while(list1!=null&amp;&amp;list2!=null)&#123; if(list1.val&lt;list2.val)&#123; head.next=list1; list1=list1.next; &#125;else&#123; head.next=list2; list2=list2.next; &#125; head = head.next; &#125; //把未结束的链表连接到合并后的链表尾部 if(list1!=null)&#123; head.next=list1; &#125; if(list2!=null)&#123; head.next=list2; &#125; return root.next; &#125;&#125; java递归写法 123456789101112131415161718public class Solution &#123; public ListNode Merge(ListNode list1,ListNode list2) &#123; if (list1==null) return list2; else if (list2==null) return list1; ListNode MergeHead = null; if (list.val&lt;=list2.val)&#123; MergeHead = list1; MergeHead.next = Merge(list1.next,list2); &#125; else &#123;MergeHead = list2; MergeHead.next = Merge(list1,list2.next); &#125; return MergeHead; &#125;&#125; 六、复杂链表的复制（剑26）输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） java 12345678910111213141516171819202122232425262728293031323334public class Solution &#123; public RandomListNode Clone(RandomListNode pHead) &#123; if (pHead == null) return null; //复制next 如原来是A-&gt;B-&gt;C 变成A-&gt;A'-&gt;B-&gt;B'-&gt;C-&gt;C' RandomListNode pCur = pHead; while (pCur != null) &#123; RandomListNode node = new RandomListNode(pCur.label); node.next = pCur.next; pCur.next = node; pCur = node.next; &#125; //复制random pCur是原来链表的结点 pCur.next是复制pCur的结点 pCur = pHead; while (pCur!=null) &#123; if (pCur.random!=null) pCur.next.random = pCur.random.next; pCur = pCur.next.next; &#125; //拆分链表 RandomListNode head = pHead.next; RandomListNode tmp = head; pCur = pHead; while(pCur.next!=null) &#123; tmp = pCur.next; pCur.next = tmp.next; pCur = tmp; &#125; return head; &#125;&#125; 七、二叉搜索树与双向链表（剑27）输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 java 12345678910111213141516171819202122232425public class Solution &#123; TreeNode head = null; TreeNode realHead = null; public TreeNode Convert(TreeNode pRootOfTree) &#123; ConvertSub(pRootOfTree); return realHead//realHead是每个子树排序后的第一个结点，head是排序后的最后一个结点; &#125; private void ConvertSub(TreeNode pRootOfTree) &#123; //递归中序遍历 if(pRootOfTree==null) return; ConvertSub(pRootOfTree.left); if (head == null) &#123; //初始处 head = pRootOfTree; realHead = pRootOfTree; &#125; else &#123; //前两句实现双向，第三句跳到下一个节点。 head.right = pRootOfTree; pRootOfTree.left = head; head = pRootOfTree; &#125; ConvertSub(pRootOfTree.right); &#125;&#125; 八、两个链表的第一个公共结点（剑37）输入两个链表找出他们的第一个公共结点。 面试的时候碰到这道题，很多应聘者的第一个想法就是蛮力法：在第一个链表上顺序遍历每个结点，每遍历到一个结点的时候，在第二个链表上顺序遍历每个结点。若第二个链表上有一个结点和第一个链表上的结点一样，说明两个链表在这个结点上重合，于是就找到了它们的公共结点。如果第一个链表的长度为m，第二个链表的长度为n，显然该方法的时间复杂度是O(mn)。 通常蛮力法不会是最好的办法，我们接下来试着分析有公共结点的两个链表有哪些特点。从链表结构的定义看出，这两个链表是单向链表。如果他们有公共的结点，那么这两个链表从某一结点开始，他们的next指向同一个结点。但由于是单向链表的结点，每个结点只有一个next，因此从第一个公共结点开始，之后的结点都是重合的，不可能再出现分叉。所以两个有公共结点而部分重合的链表，拓扑形状看起来像一个Y，而不是X。 经过我们的分析发现，若两个链表有公共结点，那么公共结点出现在两个链表的尾部。如果我们从两个链表的尾部开始往前比较，最后一个相同的结点就是我们要找的结点。我们想到用栈的特点来解决这个问题：分别把两个链表的结点放入两个栈中，这样两个链表的尾结点就位于两个栈的栈顶，接下来比较两个栈顶的结点是否相同。若果相同，则把栈顶弹出接着比较下一个栈顶，直到找到最后一个相同的结点。 上面需要用到两个辅助栈。若链表的长度分别为m和n，那么空间复杂度是O(m+n)。这种思路的时间复杂度也是O(m+n)。和最开始的蛮力法相比，时间效率得到了提升，相当于是用空间换取时间效率。 之所以需要用到栈，是因为我们想同时遍历到达两个栈的尾结点。当两个链表的长度不相同时，如果我们从头开始遍历到达尾结点的时间就不一致。其实解决这个问题还有一个更简单的办法：首先遍历两个链表得到他们的长度，就能知道哪个链表比较长，以及长的链表比短的链表多几个结点。在第二次遍历的时候，在较长的链表上先走若干步，接着再同时在两个链表上遍历，找到的第一个相同的结点就是他们的第一个公共结点。 第三种思路和第二种思路相比，时间复杂度都是O(m+n)，但我们不再需要辅助的栈，因此提高了空间效率。实现代码如下： java版本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode FindFirstCommonNode(ListNode pHead1, ListNode pHead2) &#123; ListNode current1 = pHead1;//链表1 ListNode current2 = pHead2;//链表2 if(pHead1 ==null||pHead2==null)&#123;return null;&#125;// int len1 = getlistlength(pHead1);//链表1的长度 int len2 = getlistlength(pHead2);//链表2的长度 //若链表1长度大于链表2 if(len1&gt;=len2)&#123; int len=len1-len2; //遍历链表1，遍历长度为两链表长度差 while (len&gt;0)&#123; current1 = currentnext; len--; &#125; &#125; //若链表2长度大于链表1 else if(len1&lt;len2)&#123; int len=len2-len1; //遍历链表2，遍历长度为两链表长度差 while (len&gt;0)&#123; current2=current2.next; len--; &#125; &#125; //开始齐头并进，直到找到第一个公共结点 while(current1!=current2)&#123; current1 = currentnext; current2 = current2.next; &#125; return current1; &#125; //求指定链表的长度 public static int getlistlength(ListNode pHead)&#123; int length = 0; ListNode current = pHead; while(current!=null)&#123; length++; current = current.next; &#125; return length; &#125;&#125; python版本 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def FindFirstCommonNode(self, pHead1, pHead2): # write code here current1=pHead1 current2=pHead2 len1 = self.getlistlength(current1) len2 = self.getlistlength(current2) if len1&gt;=len2: length = len1-len2 while length&gt;0: current1 = currentnext length=length-1 elif len1&lt;len2: length = len2-len1 while length&gt;0: current2 = current2.next length=length-1 while current1!=current2: current1=currentnext current2=current2.next return current1 def getlistlength(self,pHead): length =0 current =pHead while current!=None: length=length+1 current = current.next return length 九、圆圈中最后剩下的的数字（剑45）0、…..，n-1这n个数字排成一个圆圈，从数字0开始每次从这个圆圈里删除第m个数字。求出这个圆圈里剩下的最后一个数字。约瑟夫环问题，用环形链表模拟圆圈的经典解法， java 12345678910111213141516171819202122232425262728public class Solution&#123; public int LastRemaining_Solution(int n, int m)&#123; if(m&lt;=0||n&lt;=0)return -1; //先构造循环链表 ListNode head= new ListNode(0);//头结点, 值为0 ListNode pre = head; ListNode temp = null; for(int i=1;i&lt;n;i++)&#123; temp = new ListNode(i); pre.next = temp; pre = temp; &#125; temp.next = head;//将第n-1个结点(也就是尾结点)指向头结点 ListNode temp2 = null; while(n&gt;=1)&#123; //每次都当前头结点找到第m个结点的前驱 temp2=head; for(int i =1;i&lt;m-1;i++)&#123; temp2 = temp2.next; &#125; temp2.next = temp2.next.next; head = temp2.next;//设置当前头结点 n--; &#125; return head.val; &#125;&#125; java 12345678910111213public class Solution&#123; public int LastRemaining_Solution(int n, int m) &#123; if(n==0||m==0)return -1; int last=0; for(int i=2;i&lt;=n;i++) &#123; last=(last+m)%i; &#125; return last ; &#125;&#125; 十、链表中环的入口结点（剑56）一个链表中包含环，如何找到环的入口结点？例如在下图的链表中，环的入口结点是结点3。 以3为例分析两个指针的移动规律。指针$P_1$和$P_2$在初始化时都指向链表的头结点。由于环中有4个结点，指针$P_1$先在链表上向前移动4步。接下来两个指针以相同的速度在链表上向前移动，直到它们相遇。它们相遇的结点正好是还的入口结点。 剩下的问题就是如何得到环中结点的数目。我们可以使用一快一慢两个指针。若两个指针相遇，说明链表中有环。两个指针相遇的结点一定是在环中的。可以从这个结点出发，一边继续向前移动一边计数，当再次回到这个结点时，就可以得到环中结点数了实现代码如下： java版本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; //找到一快一满指针相遇处的节点，相遇的节点一定是在环中 public static ListNode meetingNode(ListNode pHead) &#123; if(pHead == null)&#123;return null;&#125;//空链表处理 ListNode pslow = pHead.next; if(pslow == null)&#123;return null;&#125;//无环链表处理 ListNode pfast = pslow.next; while(pfast!=null &amp;&amp; pslow!=null)&#123; if(pslow==pfast)&#123;return pfast;&#125; pslow = pslow.next;//慢指针 pfast = pfast.next; if(pfast!=null)&#123; pfast = pfast.next; &#125;//块指针 &#125; return null; &#125; public ListNode EntryNodeOfLoop(ListNode pHead)&#123; ListNode meetingNode=meetingNode(pHead);//相遇结点 //环的结点个数 if(meetingNode==null)&#123;return null;&#125;//是否有环 int nodesInLoop = 1; ListNode p1=meetingNode; while(pnext!=meetingNode)&#123; p1=pnext; ++nodesInLoop; &#125; //p1慢指针,先往前走 p1=pHead; for(int i=0;i&lt;nodesInLoop;i++)&#123; p1=pnext; &#125; //p1,p2同步走，相遇的地方即为环入口 ListNode p2=pHead; while(p1!=p2)&#123; p1=pnext; p2=p2.next; &#125; return p1; &#125; &#125; python 版本 123456789101112131415161718192021222324252627282930313233343536373839# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def meetingNode(self,pHead): if not pHead: return None pslow =pHead.next if not pslow: return None pfast = pslow.next while pfast and pslow: if pslow==pfast: return pfast pslow = pslow.next pfast = pfast.next if pfast: pfast=pfast.next return None def EntryNodeOfLoop(self, pHead): meetingNode = self.meetingNode(pHead) if not meetingNode: return None nodesInLoop = 1 p1 = meetingNode while pnext!=meetingNode: p1=pnext nodesInLoop +=1 p1 = pHead for i in xrange(0,nodesInLoop): p1=pnext p2=pHead while p1!=p2: p1=pnext p2=p2.next return p1 十一、删除链表中重复的结点（剑57）在一个排序的链表中，如何删除重复的结点？如在下图中重复结点被删除之后，链表如下图所示： 从头遍历整个链表。如果当前结点的值与下一个节点的值相同，那么它们就是重复的结点，都可以被删除。为了保证删除之后的链表仍然是相连的而没有中间断开，我们要把当前结点的前一个结点preNode和后面值比当前结点的值要大的结点相连。要确保preNode要始终与下一个没有重复的结点连接在一起。 实现代码如下： java递归版 12345678910111213141516171819202122232425262728/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode deleteDuplication(ListNode pHead) &#123; if (pHead == null || pHead.next == null) &#123; // 只有0个或1个结点，则返回 return pHead; &#125; if (pHead.val == pHead.next.val) &#123; // 当前结点是重复结点 ListNode pNode = pHead.next; while (pNode != null &amp;&amp; pNode.val == pHead.val) &#123; // 跳过值与当前结点相同的全部结点,找到第一个与当前结点不同的结点 pNode = pNode.next; &#125; return deleteDuplication(pNode); // 从第一个与当前结点不同的结点开始递归 &#125; else &#123; // 当前结点不是重复结点 pHead.next = deleteDuplication(pHead.next); // 保留当前结点，从下一个结点开始递归 return pHead; &#125; &#125;&#125; python版本 1234567891011121314151617# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def deleteDuplication(self, pHead): if not pHead or not pHead.next: return pHead if pHead.val==pHead.next.val: pNode = pHead.next while pNode and pNode.val == pHead.val: pNode = pNode.next return self.deleteDuplication(pNode) else: pHead.next = self.deleteDuplication(pHead.next) return pHead java非递归 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode deleteDuplication(ListNode pHead) &#123; if(pHead==null)return null; ListNode preNode = null; ListNode node = pHead; while(node!=null)&#123; ListNode nextNode = node.next; boolean needDelete = false; //需要删除重复节点的情况 if(nextNode!=null&amp;&amp;nextNode.val==node.val)&#123; needDelete = true; &#125; //不重复结点不删除 if(!needDelete)&#123; preNode = node; node = node.next; &#125; //重复节点删除 else&#123; int value = node.val; ListNode toBeDel = node; //连续重复结点 while(toBeDel != null &amp;&amp; toBeDel.val == value)&#123; nextNode = toBeDel.next; toBeDel = nextNode; if(preNode==null) pHead = nextNode; else preNode.next = nextNode; node = nextNode; &#125; &#125; &#125; return pHead; &#125;&#125; 十二、翻转部分链表(leetcode 92 Reverse Linked List II)给了一个链表，第1个结点标号为1，把链表中标号在M到N区间的部分反转 （我写的很慢，面试官看不下去了，让我只说思路） 这道题是比较常见的链表反转操作，不过不是反转整个链表，而是从m到n的一部分。分为两个步骤，第一步是找到m结点所在位置，第二步就是进行反转直到n结点。反转的方法就是每读到一个结点，把它插入到m结点前面位置，然后m结点接到读到结点的下一个。总共只需要一次扫描，所以时间是O(n)，只需要几个辅助指针，空间是O(1)。代码如下： java 123456789101112131415161718192021222324252627public class Solution &#123; public ListNode reverseBetween(ListNode head, int m, int n) &#123; if(head == null) return null; ListNode dummy = new ListNode(0); dummy.next = head; ListNode preNode = dummy; int i=1; while(preNode.next!=null &amp;&amp; i&lt;m) &#123; preNode = preNode.next; i++; &#125; ListNode mNode = preNode.next; ListNode cur = mNode.next; while(cur!=null &amp;&amp; i&lt;n) &#123; ListNode next = cur.next; cur.next = preNode.next; preNode.next = cur; mNode.next = next; cur = next; i++; &#125; return dummy.next; &#125; &#125; 十三、链表插入排序(leetcode 147 Insertion Sort List)这道题跟Sort List类似，要求在链表上实现一种排序算法，这道题是指定实现插入排序。插入排序是一种$O(n^2)$复杂度的算法，基本想法相信大家都比较了解，就是每次循环找到一个元素在当前排好的结果中相对应的位置，然后插进去，经过n次迭代之后就得到排好序的结果了。了解了思路之后就是链表的基本操作了，搜索并进行相应的插入。时间复杂度是排序算法的$O(n^2)$，空间复杂度是O(1)。代码如下： java 123456789101112131415161718192021222324252627public class Solution &#123; public ListNode insertionSortList(ListNode head) &#123; if( head == null )&#123; return head; &#125; ListNode helper = new ListNode(0); //new starter of the sorted list ListNode cur = head; //the node will be inserted ListNode pre = helper; //insert node between pre and pre.next ListNode next = null; //the next node will be inserted //not the end of input list while( cur != null )&#123; next = cur.next; //find the right place to insert while( pre.next != null &amp;&amp; pre.next.val &lt; cur.val )&#123; pre = pre.next; &#125; //insert between pre and pre.next cur.next = pre.next; pre.next = cur; pre = helper; cur = next; &#125; return helper.next; &#125;&#125; 十四、链表归并排序(leetcode 148 Sort List)链表排序，不允许直接交换节点的值，敲的有点bug，指针初始化有问题 这道题跟Insertion Sort List类似，要求我们用O(nlogn)算法对链表进行排序，但是并没有要求用哪一种排序算法，我们可以使用归并排序，快速排序，堆排序等满足要求的方法来实现。对于这道题比较容易想到的是归并排序，因为我们已经做过Merge Two Sorted Lists，这是归并排序的一个subroutine。剩下我们需要做的就是每次找到中点，然后对于左右进行递归，最后用Merge Two Sorted Lists把他们合并起来。代码如下： java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class Solution &#123; public ListNode sortList(ListNode head) &#123; if (head == null || head.next == null) return head; // step 1. cut the list to two halves ListNode prev = null, slow = head, fast = head; while (fast != null &amp;&amp; fast.next != null) &#123; prev = slow; slow = slow.next; fast = fast.next.next; &#125; prev.next = null; // step 2. sort each half ListNode l1 = sortList(head); ListNode l2 = sortList(slow); // step 3. merge l1 and l2 return merge(l1, l2); &#125; ListNode merge(ListNode l1, ListNode l2) &#123; ListNode l = new ListNode(0), p = l; while (l1 != null &amp;&amp; l2 != null) &#123; if (l1.val &lt; l2.val) &#123; p.next = l1; l1 = l1.next; &#125; else &#123; p.next = l2; l2 = l2.next; &#125; p = p.next; &#125; if (l1 != null) p.next = l1; if (l2 != null) p.next = l2; return l.next; &#125;&#125; 十五、实现稀疏矩阵相乘给定两个稀疏矩阵A和B，求AB。A的列数和B的行数相等。 面经：（链表，即每行的非零值存进单链表里。从面试官表情判断，应该说对了） http://www.chongchonggou.com/g_5929864.html例如： 1234567891011121314A = [ [ 1, 0, 0], [-1, 0, 3]]B = [ [ 7, 0, 0 ], [ 0, 0, 0 ], [ 0, 0, 1 ]] | 1 0 0 | | 7 0 0 | | 7 0 0 |AB = | -1 0 3 | x | 0 0 0 | = | -7 0 3 | | 0 0 1 | 这道题让我们实现稀疏矩阵相乘，稀疏矩阵的特点是矩阵中绝大多数的元素为0，而相乘的结果是还应该是稀疏矩阵，即还是大多数元素为0，那么我们使用传统的矩阵相乘的算法肯定会处理大量的0乘0的无用功，所以我们需要适当的优化算法，使其可以顺利通过OJ，我们知道一个$ i x k$ 的矩阵A乘以一个 $k x j$ 的矩阵B会得到一个$i x j$ 大小的矩阵C，那么我们来看结果矩阵中的某个元素$C[i][j]$是怎么来的。起始是$A[i][0]B[0][j] + A[i][1]B[1][j] + … + A[i][k]B[k][j]$，那么为了不重复计算0乘0，我们首先遍历A数组，要确保$A[i][k]$不为0，才继续计算，然后我们遍历B矩阵的第k行，如果$B[K][J]$不为0，我们累加结果矩阵$res[i][j] += A[i][k] B[k][j]$; 这样我们就能高效的算出稀疏矩阵的乘法，参见代码如下： java 1234567891011121314151617public int[][] multiply(int[][] A, int[][] B) &#123; int rowA = A.length; int colA = A[0].length; int colB = B[0].length; int[][] res = new int[rowA][colB]; for (int i = 0; i &lt; rowA; i++) &#123; for (int k = 0; k &lt; colA; j++) &#123; if (A[i][k] != 0) &#123;// Check whether A[i][j]==0 to spare much time, because a sparse matrix has more than 95% zero elements for (int j = 0; j&lt; colB; j++) &#123; if (B[k][j] != 0) res[i][j] += A[i][k] * B[k][j]; &#125; &#125; &#125; &#125; return res; &#125; 再来看另一种方法，这种方法其实核心思想跟上面那种方法相同，稍有不同的是我们用一个链表来记录每一行中，各个位置中不为0的列数和其对应的值，这样就得到i个链表，然后我们遍历链表，取出每行中不为零的列数和值，然后遍历B中对应行进行累加相乘，参见代码如下： java 123456789101112131415161718192021222324252627public int[][] multiply(int[][] A, int[][] B) &#123; int rowA = A.length; colA = A[0].length; colB = B[0].length; int[][] res = new int[rowA][colB]; LinkedList&lt;Point&gt;[] rowsA = new LinkedList[rowA]; for (int i = 0; i &lt; rowA; i++) &#123;// Create non-zero array for each row in A rowsA[i] = new LinkedList(); for (int j = 0; j &lt; colA; j++) &#123; if (A[i][j] != 0) &#123; rowsA[i].add(new Point(j, A[i][j])); &#125; &#125; &#125; for (int i = 0; i &lt; rowA; i++) &#123;// Only deal with non-zero elements in the above arrays for (int j = 0; j &lt; rowsA[i].size(); j++) &#123; int col = rowsA[i].get(j).x; int val = rowsA[i].get(j).y; for (int k = 0; k &lt; colB; k++) &#123; res[i][k] += val * B[col][k]; &#125; &#125; &#125; return res; &#125; 十六、排序链表中找出和为给定值的一段链表排序链表中找出和为给定值的一段链表 面经中出现过的链表题： 二、如何在O(1)时间删除链表节点； 三、第一题是链表倒数第 k 节点；如何找链表倒数第K个结点（联系这个题目，两链表找交点的题就可以在O(1)空间解决了）；单链表如何判断有环；链表中倒数第K个结点 四、反转链表递归、非递归；链表反转；链表逆序。。正中我下怀~；写程序 翻转链表；给了个单链表逆置，写代码；用C/C++实现单链表的反转。 六、复杂链表的复制 七、二叉搜索树转换成一个排好序的双向链表；上网搜搜有怎样将二叉排序树变成双向链表 八、两个相交链表如何找交点（我说了用栈保存每个链表节点的方法，他问有没有O(1)空间解法，一时没想到）；判断两条链表是否交叉 十、确定链表中环的起始位置 十二、翻转部分链表(Reverse Linked List II) 十三、链表插入排序(Insertion Sort List) 十四、链表归并排序(Sort List) 十六、排序链表中找出和为给定值的一段链表]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（19）：海量数据处理]]></title>
    <url>%2F2017%2F08%2F02%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8819%EF%BC%89%EF%BC%9A%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[一、何谓海量数据处理？所谓海量数据处理，其实很简单，海量，海量，何谓海量，就是数据量太大，所以导致要么是无法再较短时间内迅速解决，要么是数据太大，导致无法一次性装入内存。 那解决办法呢？针对时间，我们可以采用巧妙的算法搭配合适的数据结构，如Bloom Filter、Hash、Bit-map、堆、数据库或倒排索引、trie，针对空间，无非就一个办法：大而化小、分而治之、hash映射，你不是说规模太大嘛，那简单啊，就把规模大化为规模小的，各个击破不就完了嘛。 至于所谓的单机及集群问题，通俗点来讲，单机就是处理装载数据的机器有限（只要考虑CPU、内存、硬盘的数据交互），而集群，机器有多辆，适合分布式处理，并行计算（更多考虑节点和节点间的数据交互）。 再者，通过本blog内的有关海量数据处理的文章，我们已经大致知道，处理海量数据问题无非就是： 分而治之、hash映射+hash统计+堆、快速、归并排序； 双层桶划分 Bloom Filter、bitmap trie树、数据库、倒排索引； 外排序 分布式处理之Hadoop、Mapreduce 下面，本文第一部分、从Set、Map谈到hashtable、hash_map、hash_set，简要介绍下Set、Map、Multiset、Multimap，及hash_set、hash_map、hash_multiset、hash_multimap之区别，而第二部分，则针对上述6种方法模式结合对应的海量数据处理面试题分别具体阐述。 二、从Set、Map谈到hashtable、hash_map/hash_set稍后本文第二部分将多次提到hash_map、hash_set，下面稍稍介绍下这些容器，以作为基础准备。一般来说，STL容器分为两种 序列式容器：vector、list、deque、stack、queue、heap 关联式容器：关联式容器又分为set（集合）和map（映射表）两大类，以及这两大类的衍生体（多键集合）和multimap（多键映射表），这些容器均以RB-tree完成。此外，还有第三类关联容器，如hashtable（散列表），以及以hashtable为底层机制完成的hash_set（散列集合）、hash_map（散列映射表）、hash_multiset(散列多键集合)、hash_multimap（散列多键映射表）。也就是说，set、map、multiset、multimap都内含一个RB-tree，而hash_set、hash_map、hash_multiset、hash_multimap都内含一个hashtable。 所谓关联式容器，类似关联式数据库，每笔数据或每个元素都有一个键值（key）和一个实值（value），即所谓的Key-Value（键-值对）。当元素被插入到关联式容器中时，容器内结构（RB-Tree、hashtable）便依照其键值大小，以某种特定规则将这个元素放置于适当位置。 包括在非关联式数据库中，比如，在MongoDB内，文档（document）是最基本的数据组织形式，每个文档也是以Key-Value(键-值对)的方式组织起来。一个文档可以有多个Key-Value组合，每个Value可以是不同的类型，比如String、Integer、List等等。${“name”:”July”,”Sex”:”male”,”age”:23}$ 2.1 关联式容器之集合：set/map/multimap/multimapset，同map一样，所有元素都会根据元素的键值自动被排序，因为set、map两者的所有操作，都是调用RB-tree的操作行为，不过，值得注意的是，两者都不允许两个元素有相同的键值。 不同的是，set的元素不像map那样可以同时拥有实值（value）和键值（key），set元素的键值就是实值，实值就是键值，而map的所有元素都是pair，同时拥有实值（value）和键值（key），pair的第一个元素被视为键值，第二个元素被视为实值。 至于multiset、multimap，他们的特性及用法和set、map完全相同，唯一的差别就在于它们允许键值重复，即所有的插入操作基于RB-tree的insert——equal()而非insert_unique()。 2.2 关联式容器之映射表：hash_set/hash_map/hash_multiset/hash_multimaphash_set/hash_map，两者的一切操作都是基于hashtable之上。hash_set同set一样，同时拥有实值和键值，且实质就是键值，键值就是实值，而hash_map同map一样，每一个元素同时拥有一个实值(value)和一个键值(key)，所以其使用方式，和上面的map基本相同。但由于hash_set/hash_map都是基于hashtable之上，所以不具备自动排序功能。为什么?因为hashtable没有自动排序功能。 至于hash_multiset/hash_multimap的特性与上面的multiset/multimap完全相同，唯一的差别就是它们hash_multiset/hash_multimap的底层实现机制是hashtable（而multiset/multimap，上面说了，底层实现机制是RB-tree），所以它们的元素都不会被自动排序，不过也都允许键值重复。 所以，综上，说白了，什么样的结构决定其什么样的性质，因为set/map/multiset/multimap都是基于RB-tree之上，所以有自动排序功能，而hash_set/hash_map/hash_multiset/hash_multimap都是基于hashtable之上，所以不含有自动排序功能，至于加个前缀multi_无非就是允许键值重复而已。 三、处理海量数据问题之六把密钥密钥一：分而治之/Hash映射+Hash_map统计+堆/快/归并排序这一部分介绍处理海量数据问题的第一把密钥：分而治之、哈希映射+Hash统计+堆、快速、归并排序 （1）海量日志数据，提取出某日访问 既然是海量数据处理，那么可想而知，给我们的数据那就一定是海量的。针对这个数据的海量，我们如何着手呢?对的，无非就是分而治之/hash映射 + hash统计 + 堆/快速/归并排序，说白了，就是先映射，而后统计，最后排序： 分而治之/hash映射：针对数据太大，内存受限，只能是：把大文件化成(取模映射)小文件，即16字方针：大而化小，各个击破，缩小规模，逐个解决 hash统计：当大文件转化了小文件，那么我们便可以采用常规的Hashmap(ip，value)来进行频率统计。 堆/快速排序：统计完了之后，便进行排序(可采取堆排序)，得到次数最多的IP 具体而论，“首先是这一天，并且是访问百度的日志的IP取出来，逐个写到一个大文件中。注意到IP是32位的，最多有个$2^{32}$个IP。同样可以采用映射的方法，比如%1000，把整个大文件映射为1000个小文件，再找出每个小文件中出现频率最大的IP（可以采用Hash_map对那1000个文件中的所以IP进行频率统计，然后依次找出各个文件中频率最大的那个IP）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。” 关于本题，还有几个问题： Hash取模是一种等价映射，不会存在同一个元素分散到不同小文件中的情况，即这里采用的是mod1000算法，那么相同的IP在hash取模后，只可能落在同一个文件中，不可能被分散的。因为如果两个IP相等，那么经过Hash（IP）之后的哈希值是相同的，将此哈希值取模，必定仍然相等。 那到底什么是hash映射呢？简单来说，就是为了便于计算机在有限的内存中处理大数据，从而通过一种映射散列的方式让数据均匀分布在对应的内存位置（如大数据通过取余的方式映射城小数存放在内存中，或者大文件映射成多个小文件），而这个映射散列方式便是我们通常所说的hash函数，设计的好的hash函数能让数据均匀分布而减少冲突。尽管数据映射到了另外一些不同的位置，但数据还是原来的数据，只是代替和表示这些原始数据的形式发生了变化而已。 （2）寻找热门查询，300万个查询字符串中统计最热门的10个查询 原题：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1~255字节。假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门），请你统计最热门的10个查询串，要求使用的内存不能超过1G。 由上面那题可知，数据大则划分为小的，比如一亿个IP求Top10，可先%1000将IP分到1000个小文件中去，并保证一种IP只出现在一个文件中，再对每个小文件中的IP进行Hashmap计数统计并按数量排序，最后归并或者最小堆一次处理每个小文件的top10以得到最后的结。 但如果数据规模比较小，能一次性装入内存呢？比如这第二题，虽然有一千万个Query，但是由于重复度比较高，因此事实上只有300万的Query，每个Query255Byte，因此我们可以考虑把它们都放进内存中去（300万个字符串假设没有重复，都是最大长度，那么最多占用内存3M*1K/4=0.75G。所以可以将所有字符串都存放在内存中进行处理），而现在只需要一个合适的数据结构，在这里，HashTable绝对是我们优先的选择。 所以我们放弃分而治之、Hash映射的步骤，直接上Hash统计，然后排序。所以，针对此类典型的Top K问题，采取的对策往往是：Hashmap+堆。如下所示： Hash_map统计：先对这批海量数据预处理。具体方法是：维护一个Key为Query字符串，Value为该QUery出现次数的HashTable，即Hash_map(Query,Value)，每次读取一个Query，如果该字符串不在table内，则加入该字串，并且将Value值设为1；如果该字串在Table中，那么将该字串的计数加1即可。最终我们在O(N)的时间复杂度内用Hash表完成了统计。 堆排序：借助堆这个数据结构，找出Top K，时间复杂度为NlogK。即借助堆结构，我们可以在log量级的时间内查找和调整。因此，维护一个K（该题目中是10）大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。所以，我们最终的时间复杂度是$O(N)+N·O(logK)$，（N为1000万，N’是300万） 这篇文章中所述的堆排序思路：“维护k个元素的最小堆，即用容量为K的最小堆存储最先遍历到的k个数，并假设它们即是最大的k个数，建堆费时$O(k)$，并调整堆，费时$O(logk)$，这样我们就有$k_1&gt;k_2&gt;…k_{min}$（kmin设为小顶堆中最小元素）。继续遍历数列，每次遍历一个元素x，与堆顶元素比较，若$x&gt;k_{min}$，则更新堆（用时$log_k$），否则不更新堆。这样下来，总费时$O（klogk+（n-k）logk）=O（n*logk）$。此方法得益于在堆中，查找等各项操作时间复杂度均为logk。” 当然，你也可以采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。 （3）有一个1G大小的文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小时1M。返回频数最高的100个词。 由上面那两个例题，分而治之 + hash统计 + 堆/快速排序这个套路，我们已经开始有了屡试不爽的感觉。下面，再拿几道再多多验证下。请看此第3题：又是文件很大，又是内存受限，咋办?还能怎么办呢?无非还是： 分而治之/hash映射：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。 hash统计：对每个小文件，采用trie树、hash——map等统计每个文件中出现的词以及相应的频率。 堆、归并排序：取出出现频率最大的100个词（可以用含100个结点的最小堆），并把100个词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并（类似于归并排序）的过程了。 （4）海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。 如果每个数据元素只出现一次，而且只出现在某一台机器中，那么可以采取以下步骤统计出现次数TOP10的数据元素： 堆排序：在每台电脑上求出TOP10，可以采用包含10个元素的堆完成（TOP10小，用最大堆，TOP10大，用最小堆，比如求TOP10大，我们首先取前10个元素调整成最小堆，如果发现，然后扫描后面的数据，并与堆顶元素比较，如果比堆顶元素大，那么用该元素替换堆顶，然后再调整为最小堆。最后堆中的元素就是TOP10大）。 求出每台电脑上的TOP10后，然后把这100台电脑上的TOP10组合起来，共1000个数据，再利用上面类似的方法求出TOP10就可以了。 但如果同一个元素重复出现在不同的电脑中呢，如下例子所述：就拿2台机器求top2的情况来说，第一台：a(50)、b(50)、c(49)、d(49)、e(0)、e(0)，第二台：a(0)、b(0)、c(49)、d(49)、e(50)、f(50)，这个时候，你可以有两种方法： 遍历一遍所有数据，重新hash取摸，如此使得同一个元素只出现在单独的一台电脑中，然后采用上面所说的方法，统计每台电脑中各个元素的出现次数找出TOP10，继而组合100台电脑上的TOP10，找出最终的TOP10。 暴力求解：直接统计统计每台电脑中各个元素的出现次数，然后把同一个元素在不同机器中的出现次数相加，最终从所有数据中找出TOP10。 （5）有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。 hash映射：顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件（记为）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。 hash统计：找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。注：hash_map(query,query_count)是用来统计每个query的出现次数，不是存储他们的值，出现一次，则count+1。 堆/快速/归并排序：利用快速/堆/归并排序按照出现次数进行排序。将排序好的query和对应的query_cout输出到文件中。这样得到了10个排好序的文件。对这10个文件进行归并排序（内排序与外排序相结合）。 除此之外，此题还有以下两个方法： 一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。 与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。 （6）给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？ 可以估计每个文件的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。 分而治之/hash映射：遍历文件a，对每个url求取，然后根据所取得的值将url分别存储到1000个小文件（记为$a_0,a_1…..,a_{999}$）中。这样每个小文件的大约为300M。遍历文件b，采取和a相同的方式将url分别存储到1000小文件中（记为$b_0,b_1……,b_{999}$）。这样处理后，所有可能相同的url都在对应的小文件（$a_0vsb_0,a_1vsb_1……a_{999}vsb_{999}$）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。 hash_set统计：求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。 （7）怎么在海量数据中找出重复次数最多的一个？ 先做hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求（具体参考前面的题）。 （8）上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据。 上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用hash_map/搜索二叉树/红黑树等来进行统计次数。然后利用堆取出前N个出现次数最多的数据。 （9）一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。 方案1：如果文件比较大，无法一次性读入内存，可以采用hash取模的方法，将大文件分解为多个小文件，对于单个小文件利用hash_map统计出每个小文件中10个最常出现的词，然后再进行归并处理，找出最终的10个最常出现的词。 方案2：通过hash取模将大文件分解为多个小文件后，除了可以用hash_map统计出每个小文件中10个最常出现的词，也可以用trie树统计每个词出现的次数，时间复杂度是$O(nle)$（le表示单词的平准长度），最终同样找出出现最频繁的前10个词（可用堆来实现），时间复杂度是$O(nlg10)$。 密钥二：双层桶划分双层桶划分，其实本质上还是分而治之的思想，重在“分”的技巧上！ 适用范围：第k大，中位数，不重复或重复的数字 基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。可以通过多次缩小，双层只是一个例子。 （1）2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。 有点像鸽巢原理，整数个数为$2^{32}$,也就是，我们可以将这$2^{32}$个数，划分为$2^8$个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用bitmap就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。 （2）5亿个int找它们的中位数。 这个例子比上面那个更明显。首先我们将int划分为$2^{16}$个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。 实际上，如果不是int是int64，我们可以经过3次这样的划分即可降低到可以接受的程度。即可以先将int64分成$2^{24}$个区域，然后确定区域的第几大数，在将该区域分成$2^{20}$个子区域，然后确定是子区域的第几大数，然后子区域里的数的个数只有$2^{20}$，就可以直接利用direct addr table进行统计了。 密钥三：Bloom filter/Bitmap Bloom filter 海量数据处理之Bloom Filter详解 适用范围：可以用来实现数据字典，进行数据的判重，或者集合求交集 基本原理及要点： 对于原理来说很简单，位数组+k个独立hash函数。将hash函数对应的值的位数组置1，查找时如果发现所有hash函数对应位都是1说明存在，很明显这个过程并不保证查找的结果是100%正确的。同时也不支持删除一个已经插入的关键字，因为该关键字对应的位会牵动到其他的关键字。所以一个简单的改进就是 counting Bloom filter，用一个counter数组代替位数组，就可以支持删除了。 还有一个比较重要的问题，如何根据输入元素个数n，确定位数组m的大小及hash函数个数。当hash函数个数$k=(ln2)(m/n)$时错误率最小。在错误率不大于E的情况下，m至少要等于$nlg(1/E)$才能表示任意n个元素的集合。但m还应该更大些，因为还要保证bit数组里至少一半为0，则$m&gt;=nlg(1/E)*lge$ 大概就是$nlg(1/E)1.44$倍(lg表示以2为底的对数)。 举个例子我们假设错误率为0.01，则此时m应大概是n的13倍。这样k大概是8个。 注意这里m与n的单位不同，m是bit为单位，而n则是以元素个数为单位(准确的说是不同元素的个数)。通常单个元素的长度都是有很多bit的。所以使用bloom filter内存上通常都是节省的。 扩展：Bloom filter将集合中的元素映射到位数组中，用k（k为哈希函数个数）个映射位是否全1表示元素在不在这个集合中。Counting bloom filter（CBF）将位数组中的每一位扩展为一个counter，从而支持了元素的删除操作。Spectral Bloom Filter（SBF）将其与集合元素的出现次数关联。SBF采用counter中的最小值来近似表示元素的出现频率。 问题实例： 给你A,B两个文件，各存放50亿条URL，每条URL占用64字节，内存限制是4G，让你找出A,B文件共同的URL。如果是三个乃至n个文件呢？ 根据这个问题我们来计算下内存的占用，$4G=2^{32}$大概是40亿*8大概是340亿，n=50亿，如果按出错率0.01算需要的大概是650亿个bit。现在可用的是340亿，相差并不多，这样可能会使出错率上升些。另外如果这些urlip是一一对应的，就可以转换成ip，则大大简单了。 同时，上文的第5题：给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。 Bitmap介绍 Bit-map详解 所谓的Bit-map就是用一个bit位来标记某个元素对应的Value， 而Key即是该元素。由于采用了Bit为单位来存储数据，因此在存储空间方面，可以大大节省。 如果说了这么多还没明白什么是Bit-map，那么我们来看一个具体的例子，假设我们要对0-7内的5个元素(4,7,2,5,3)排序（这里假设这些元素没有重复）。那么我们就可以采用Bit-map的方法来达到排序的目的。要表示8个数，我们就只需要8个Bit（1Bytes），首先我们开辟1Byte的空间，将这些空间的所有Bit位都置为0，如下图： 然后遍历这5个元素，首先第一个元素是4，那么就把4对应的位置为1（可以这样操作 p+(i/8)|(0×01&lt;&lt;(i%8)) 当然了这里的操作涉及到Big-ending和Little-ending的情况，这里默认为Big-ending）,因为是从零开始的，所以要把第五位置为一，如下图： 然后再处理第二个元素7，将第八位置为1,，接着再处理第三个元素，一直到最后处理完所有的元素，将相应的位置为1，这时候的内存的Bit位的状态如下： 然后我们现在遍历一遍Bit区域，将该位是一的位的编号输出（2，3，4，5，7），这样就达到了排序的目的。下面的代码给出了一个BitMap的用法：排序。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//定义每个Byte中有8个Bit位 #include ＜memory.h＞ #define BYTESIZE 8 void SetBit(char *p, int posi) &#123; for(int i=0; i ＜ (posi/BYTESIZE); i++) &#123; p++; &#125; *p = *p|(0x01＜＜(posi%BYTESIZE));//将该Bit位赋值1 return; &#125; void BitMapSortDemo() &#123; //为了简单起见，我们不考虑负数 int num[] = &#123;3,5,2,10,6,12,8,14,9&#125;; //BufferLen这个值是根据待排序的数据中最大值确定的 //待排序中的最大值是14，因此只需要2个Bytes(16个Bit) //就可以了。 const int BufferLen = 2; char *pBuffer = new char[BufferLen]; //要将所有的Bit位置为0，否则结果不可预知。 memset(pBuffer,0,BufferLen); for(int i=0;i＜9;i++) &#123; //首先将相应Bit位上置为1 SetBit(pBuffer,num[i]); &#125; //输出排序结果 for(int i=0;i＜BufferLen;i++)//每次处理一个字节(Byte) &#123; for(int j=0;j＜BYTESIZE;j++)//处理该字节中的每个Bit位 &#123; //判断该位上是否是1，进行输出，这里的判断比较笨。 //首先得到该第j位的掩码（0x01＜＜j），将内存区中的 //位和此掩码作与操作。最后判断掩码是否和处理后的 //结果相同 if((*pBuffer&amp;(0x01＜＜j)) == (0x01＜＜j)) &#123; printf(&quot;%d &quot;,i*BYTESIZE + j); &#125; &#125; pBuffer++; &#125; &#125; int _tmain(int argc, _TCHAR* argv[]) &#123; BitMapSortDemo(); return 0; &#125; 可进行数据的快速查找，判重，删除，一般来说数据范围是int的10倍以下。问题实例 在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。 方案1：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。 方案2：也可采用与第1题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。” 给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？ 方案1：用位图/Bitmap的方法，申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。 密匙四：Trie树/数据库/倒排索引 Trie树 适用范围：数据量大，重复多，但是数据种类小，可以放入内存 基本原理及要点：实现方式，节点孩子的表示方式 扩展：压缩实现。 问题实例 密钥一（2）：寻找热门查询：查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个，每个不超过255字节。 密钥一（5）：有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。要你按照query的频度排序。 密钥一（8）：一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词。其解决方法是：用trie树统计每个词出现的次数，时间复杂度是O(n*le)（le表示单词的平准长度），然后是找出出现最频繁的前10个词。 1000万字符串，其中有些是相同的(重复),需要把重复的全部去掉，保留没有重复的字符串。请问怎么设计和实现？ 更多有关Trie树的介绍，请参见此文：从Trie树（字典树）谈到后缀树 数据库索引 适用范围：大数据量的增删改查 基本原理及要点：利用数据的设计实现方法，对海量数据的增删改查进行处理。 关于数据库索引及其优化，更多可参见此文：海量数据处理专题（七）——数据库索引及优化。同时，关于MySQL索引背后的数据结构及算法原理，这里还有一篇很好的文章：MySQL索引背后的数据结构及算法原理。 倒排索引(Inverted index) 适用范围：搜索引擎，关键字查询 基本原理及要点：为何叫倒排索引？一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。 以英文为例，下面是要被索引的文本： 123T0 = “it is what it is” T1 = “what is it” T2 = “it is a banana” 我们就能得到下面的反向文件索引： 12345“a”: &#123;2&#125; “banana”: &#123;2&#125; “is”: &#123;0, 1, 2&#125; “it”: &#123;0, 1, 2&#125; “what”: &#123;0, 1&#125; 检索的条件”what”,”is”和”it”将对应集合的交集。 正向索引开发出来用来存储每个文档的单词的列表。正向索引的查询往往满足每个文档有序频繁的全文查询和每个单词在校验文档中的验证这样的查询。在正向索引中，文档占据了中心的位置，每个文档指向了一个它所包含的索引项的序列。也就是说文档指向了它包含的那些单词，而反向索引则是单词指向了包含它的文档，很容易看到这个反向的关系。 问题实例：文档检索系统，查询那些文件包含了某单词，比如常见的学术论文的关键字搜索。 密匙五：外排序适用范围：大数据的排序，去重基本原理及要点：外排序的归并方法，置换选择败者树原理，最优归并树 （1）有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16个字节，内存限制大小是1M。返回频数最高的100个词。 这个数据具有很明显的特点，词的大小为16个字节，但是内存只有1m做hash有些不够，所以可以用来排序。内存可以当输入缓冲区使用。 关于多路归并算法及外排序的具体应用场景，请参见：如何给10000000个数据量的磁盘文件排序。 密匙六：分布式处理之MapreduceMapReduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。这样做的好处是可以在任务被分解后，可以通过大量机器进行并行计算，减少整个操作的时间。但如果你要我再通俗点介绍，那么，说白了，Mapreduce的原理就是一个归并排序。 适用范围：数据量大，但是数据种类小可以放入内存 基本原理及要点：将数据交给不同的机器去处理，数据划分，结果归约。 问题实例 The canonical example application of MapReduce is a process to count the appearances of each different word in a set of documents: 海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。 一共有N个机器，每个机器上有N个数。每个机器最多存O(N)个数并对它们操作。如何找到$N^2$个数的中位数(median)？ 更多具体阐述请参见：从Hadhoop框架与MapReduce模式中谈海量数据处理 MapReduce技术的初步了解与学习 四、海量数据处理题库搜集 如何在海量数据中查找给定部分数据最相似的top200向量，向量的维度也很高，因为之前了解过其他面蚂蚁金服的朋友，也有问到这个题目的，所以反应比较快，直接就说可以用KD树，聚类，hash， 如何在海量数据中查找给定部分数据最相似的top200向量，向量的维度也很高，实现一个分布式的topN算法 最后是一个海量数据处理题，给了个滴滴打车的背景，实质就是如何在海量数据中找到最大值。我按着分治思想说了一个解法，意思到了，他也就没往深处问。 要求手写海量数据topK的问题，手写个最小堆，之前没自己写过，幸亏ＴＴＦ同学临时指导了一下，都准备好了也没面到 海量数据问题，给定１０亿个数，统计出现次数最多的１００个数，如果把数换成字符串呢 给你1000个数，怎样随机抽取10个数 之后就是大数据题目，1KW句子算相似度（还是那套分块+hash/建索引，但是因为本人不是做这个的，文本处理根本说一片空白，所以就不误导大家了），之后就是一直围绕大数据的题目不断深化。 怎么在2G内存里找100TB数据的中位数 从大数据中找出topk 对大小在1-10000的1亿个数进行排序，你会怎么做？（友情提示，不要用基于比较的排序算法哦） 怎么在2G内存里找100TB数据的中位数， 10亿个整数，1G内存，O(n)算法，统计只出现一次的数。 N个数找K大数那个题,堆解释了一遍,比较满意,问还能怎么优化O(Nlogk)的方法，并行方面想 给$10^{10}$个64位数,100M内存的空间排序,感谢队长刚好在去的前一天教过我一个求中位数的方法.用文件操作来做了,像快排一样,二分选个数统计大于那个数的数量和小于那个数的数量,如果能用100M的空间排序就把那些数排了,如果不能继续.直到能排为止. 第k大之类的套路题 如何从很多的query中找出一个query （我开始想到hash，后来经提示我想到了前缀树）若允许有错误，可以再怎么解决（不知道，面试官提示了布隆过滤器） 百度二面那个query找相似的系统设计题，我先后说了前缀树，kd树等解决方案，虽然不对，但面试至少知道我了解这些东西]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（18）：倒排索引]]></title>
    <url>%2F2017%2F08%2F01%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8818%EF%BC%89%EF%BC%9A%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[一、倒排索引倒排索引（inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或一组文档中存储位置的映射。它是文档检索系统中最常用的数据结构。 有两种不同的倒排索引形式： 一条记录的水平反向索引（或者反向档案索引）包含每个引用单词的文档的列表。 一个单词的水平反向索引（或者完全反向索引）又包含每个单词在一个文档中的位置。 后者的形式提供了更多的兼容性（比如短语搜索），但是需要更多的时间和空间来创建。 以英文为例，下面是要被索引的文本： $T_0$=”it is what it is” $T_1$=”what is it” $T_2$=”it is a banana” 我们就能得到下面的反向文件索引： “a”: {2} “banana”: {2} “is”: {0, 1, 2} “it”: {0, 1, 2} “what”: {0, 1} 检索的条件”what”, “is” 和 “it” 将对应这个集合：${\displaystyle \{0,1\}\cap \{0,1,2\}\cap \{0,1,2\}=\{0,1}$ 对相同的文字，我们得到后面这些完全反向索引，有文档数量和当前查询的单词结果组成的的成对数据。 同样，文档数量和当前查询的单词结果都从零开始。所以，”banana”: {(2, 3)} 就是说 “banana”在第三个文档里 $T_{2}$，而且在第三个文档的位置是第四个单词(地址为 3)。 “a”: {(2, 2)} “banana”: {(2, 3)} “is”: {(0, 1), (0, 4), (1, 1), (2, 1)} “it”: {(0, 0), (0, 3), (1, 2), (2, 0)} “what”: {(0, 2), (1, 0)} 如果我们执行短语搜索”what is it” 我们得到这个短语的全部单词各自的结果所在文档为文档0和文档1。但是这个短语检索的连续的条件仅仅在文档1得到。 有了这个索引系统，搜索引擎可以很方便地响应用户的查询，比如用户输入查询词“banana”，搜索系统查找倒排索引，从中可以读出包含这个单词的文档，这些文档就是提供给用户的搜索结果，而利用单词频率信息、文档频率信息即可以对这些候选搜索结果进行排序，计算文档和查询的相似性，按照相似性得分由高到低排序输出，此即为搜索系统的部分内部流程。 二、单词词典单词词典是倒排索引中非常重要的组成部分，它用来维护文档集合中出现过的所有单词的相关信息，同时用来记载某个单词对应的倒排列表在倒排文件中的位置信息。在支持搜索时，根据用户的查询词，去单词词典里查询，就能够获得相应的倒排列表，并以此作为后续排序的基础。 对于一个规模很大的文档集合来说，可能包含几十万甚至上百万的不同单词，能否快速定位某个单词，这直接影响搜索时的响应速度，所以需要高效的数据结构来对单词词典进行构建和查找，常用的数据结构包括哈希加链表结构和树形词典结构。 2.1 哈希加链表 这种词典结构主要由两个部分构成： 主体部分是哈希每个哈希表项保存一个指针，指针指向冲突链表，在冲突链表里，相同哈希值的单词形成链表结构。之所以会有冲突链表，是因为两个不同单词获得相同的哈希值，如果是这样，在哈希方法里被称做是一次冲突，可以将相同哈希值的单词存储在链表里，以供后续查找。 在建立索引的过程中，词典结构也会相应地被构建出来。比如在解析一个新文档的时候，对于某个在文档中出现的单词T，首先利用哈希函数获得其哈希值，之后根据哈希值对应的哈希表项读取其中保存的指针，就找到了对应的冲突链表。如果冲突链表里已经存在这个单词，说明单词在之前解析的文档里已经出现过。如果在冲突链表里没有发现这个单词，说明该单词是首次碰到，则将其加入冲突链表里。通过这种方式，当文档集合内所有文档解析完毕时，相应的词典结构也就建立起来了。 在响应用户查询请求时，其过程与建立词典类似，不同点在于即使词典里没出现过某个单词，也不会添加到词典内。以图1-7为例，假设用户输入的查询请求为单词3，对这个单词进行哈希，定位到哈希表内的2号槽，从其保留的指针可以获得冲突链表，依次将单词3和冲突链表内的单词比较，发现单词3在冲突链表内，于是找到这个单词，之后可以读出这个单词对应的倒排列表来进行后续的工作，如果没有找到这个单词，说明文档集合内没有任何文档包含单词，则搜索结果为空。 2.2 树形结构B树（或者B+树）是另外一种高效查找结构，图1-8是一个 B树结构示意图。B树与哈希方式查找不同，需要字典项能够按照大小排序（数字或者字符序），而哈希方式则无须数据满足此项要求。 B树形成了层级查找结构，中间节点用于指出一定顺序范围的词典项目存储在哪个子树中，起到根据词典项比较大小进行导航的作用，最底层的叶子节点存储单词的地址信息，根据这个地址就可以提取出单词字符串。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>倒排索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（17）：simhash]]></title>
    <url>%2F2017%2F08%2F01%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8817%EF%BC%89%EF%BC%9Asimhash%2F</url>
    <content type="text"><![CDATA[一、引入随着信息爆炸时代的来临，互联网上充斥着着大量的近重复信息，有效地识别它们是一个很有意义的课题。例如，对于搜索引擎的爬虫系统来说，收录重复的网页是毫无意义的，只会造成存储和计算资源的浪费；同时，展示重复的信息对于用户来说也并不是最好的体验。造成网页近重复的可能原因主要包括： 镜像网站 内容复制 嵌入广告 计数改变 少量修改 一个简化的爬虫系统架构如下图所示： 事实上，传统比较两个文本相似性的方法，大多是将文本分词之后，转化为特征向量距离的度量，比如常见的欧氏距离、海明距离或者余弦角度等等。两两比较固然能很好地适应，但这种方法的一个最大的缺点就是，无法将其扩展到海量数据。例如，试想像Google那种收录了数以几十亿互联网信息的大型搜索引擎，每天都会通过爬虫的方式为自己的索引库新增的数百万网页，如果待收录每一条数据都去和网页库里面的每条记录算一下余弦角度，其计算量是相当恐怖的。 我们考虑采用为每一个web文档通过hash的方式生成一个指纹（fingerprint）。传统的加密式hash，比如md5，其设计的目的是为了让整个分布尽可能地均匀，输入内容哪怕只有轻微变化，hash就会发生很大地变化。我们理想当中的哈希函数，需要对几乎相同的输入内容，产生相同或者相近的hashcode，换句话说，hashcode的相似程度要能直接反映输入内容的相似程度。很明显，前面所说的md5等传统hash无法满足我们的需求。 二、simhash的原理simhash是locality sensitive hash（局部敏感哈希）的一种，最早由Moses Charikar在《similarity estimation techniques from rounding algorithms》一文中提出。Google就是基于此算法实现网页文件查重的。simhash算法的主要思想是降维，将高维的特征向量映射成一个f-bit的指纹(fingerprint)，通过比较两篇文章的f-bit指纹的Hamming Distance来确定文章是否重复或者高度近似。我们假设有以下三段文本： the cat sat on the mat the cat sat on a mat we all scream for ice cream 使用传统hash可能会产生如下的结果： 123456789irb(main):006:0&gt; p1 = &apos;the cat sat on the mat&apos; irb(main):005:0&gt; p2 = &apos;the cat sat on a mat&apos; irb(main):007:0&gt; p3 = &apos;we all scream for ice cream&apos; irb(main):007:0&gt; p1.hash =&gt; 415542861 irb(main):007:0&gt; p2.hash =&gt; 668720516 irb(main):007:0&gt; p3.hash =&gt; 767429688 使用simhash会应该产生类似如下的结果: 123456789irb(main):003:0&gt; p1.simhash =&gt; 851459198 00110010110000000011110001111110 irb(main):004:0&gt; p2.simhash =&gt; 847263864 00110010100000000011100001111000 irb(main):002:0&gt; p3.simhash =&gt; 984968088 00111010101101010110101110011000 海明距离的定义，为两个二进制串中不同位的数量。上述三个文本的simhash结果，其两两之间的海明距离为(p1,p2)=4，(p1,p3)=16以及(p2,p3)=12。事实上，这正好符合文本之间的相似度，p1和p2间的相似度要远大于与p3的。 如何实现这种hash算法呢？图解如下： 算法过程大概如下： 将Doc进行关键词抽取(其中包括分词和计算权重)，抽取出n个(关键词，权重)对， 即图中的(feature, weight)们。 记为 feature_weight_pairs = [fw1, fw2 … fwn]，其中 fwn = (feature_n,weight_n)。 hash_weight_pairs = [ (hash(feature), weight) for feature, weight in feature_weight_pairs ]生成图中的(hash,weight)们, 此时假设hash生成的位数bits_count = 6（如图）; 然后对hash_weight_pairs进行位的纵向累加，如果该位是1，则+weight,如果是0，则-weight，最后生成bits_count个数字，如图所示是[13, 108, -22, -5, -32, 55], 这里产生的值和hash函数所用的算法相关。 [13,108,-22,-5,-32,55] -&gt; 110001这个就很简单啦，正1负0。 三、海明距离当我们算出所有doc的simhash值之后，需要计算doc A和doc B之间是否相似的条件是：A和B的海明距离是否小于等于n，这个n值根据经验一般取值为3, 那海明距离怎么计算呢？二进制串A 和 二进制串B 的海明距离 就是 A xor B 后二进制中1的个数。 1234举例如下：A = 100111;B = 101010;hamming_distance(A, B) = count_1(A xor B) = count_1(001101) = 3; simhash本质上是局部敏感性的hash，和md5之类的不一样。 正因为它的局部敏感性，所以我们可以使用海明距离来衡量simhash值的相似度。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>哈希</tag>
        <tag>simhash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（16）：一致性哈希]]></title>
    <url>%2F2017%2F07%2F30%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8816%EF%BC%89%EF%BC%9A%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%2F</url>
    <content type="text"><![CDATA[一致性哈希算法在1997年由麻省理工学院提出的一种分布式哈希（DHT）实现算法，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简单哈希算法带来的问题，使得分布式哈希（DHT）可以在P2P环境中真正得到应用。 一、Hash算法一致性hash算法提出了在动态变化的Cache环境中，判定哈希算法好坏的四个定义： 平衡性(Balance)：平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。 单调性(Monotonicity)：单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应能够保证原有已分配的内容可以被映射到原有的或者新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。 分散性(Spread)：在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。 负载(Load)：负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同 的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。 假设一个简单的场景：有4个cache服务器（后简称cache）组成的集群，当一个对象object传入集群时，这个对象应该存储在哪一个cache里呢？一种简单的方法是使用映射公式： 1Hash(object) % 4 这个算法就可以保证任何object都会尽可能随机落在其中一个cache中。一切运行正常。 然后考虑以下情况： 由于流量增大，需要增加一台cache，共5个cache。这时，映射公式就变成Hash(object) % 5。有一个cache服务器down掉，变成3个cache。这时，映射公式就变成Hash(object) % 3。可见，无论新增还是减少节点，都会改变映射公式，而由于映射公式改变，几乎所有的object都会被映射到新的cache中，这意味着一时间所有的缓存全部失效。 大量的数据请求落在app层甚至是db层上，这样严重的违反了单调性原则,这对服务器的影响当然是灾难性的。 接下来主要讲解一下一致性哈希算法是如何设计的： 二、一致性Hash算法2.1 环形Hash空间按照常用的hash算法来将对应的key哈希到一个具有2^32次方个桶的空间中，即$0至 (2^{32})-1$的数字空间中。现在我们可以将这些数字头尾相连，想象成一个闭合的环形。如下图 2.2 数据映射现在我们将object1、object2、object3、object4四个对象通过特定的Hash函数计算出对应的key值，然后散列到Hash环上。如下图： 1234Hash(object1) = key1；Hash(object2) = key2；Hash(object3) = key3；Hash(object4) = key4； 2.3 机器映射在采用一致性哈希算法的分布式集群中将新的机器加入，其原理是通过使用与对象存储一样的Hash算法将机器也映射到环中（一般情况下对机器的hash计算是采用机器的IP或者机器唯一的别名作为输入值），然后以顺时针的方向计算，将所有对象存储到离自己最近的机器中。 假设现在有NODE1，NODE2，NODE3三台机器，通过Hash算法得到对应的KEY值，映射到环中，其示意图如下： 123Hash(NODE1) = KEY1;Hash(NODE2) = KEY2;Hash(NODE3) = KEY3; 通过上图可以看出对象与机器处于同一哈希空间中，这样按顺时针转动object1存储到了NODE1中，object3存储到了NODE2中，object2、object4存储到了NODE3中。在这样的部署环境中，hash环是不会变更的，因此，通过算出对象的hash值就能快速的定位到对应的机器中，这样就能找到对象真正的存储位置了。 2.4 机器的删除与添加普通hash求余算法最为不妥的地方就是在有机器的添加或者删除之后会照成大量的对象存储位置失效，这样就大大的不满足单调性了。下面来分析一下一致性哈希算法是如何处理的。 2.4.1 节点（机器）的删除以上面的分布为例，如果NODE2出现故障被删除了，那么按照顺时针迁移的方法，object3将会被迁移到NODE3中，这样仅仅是object3的映射位置发生了变化，其它的对象没有任何的改动。如下图： 2.4.2 节点（机器）的添加如果往集群中添加一个新的节点NODE4，通过对应的哈希算法得到KEY4，并映射到环中，如下图： 通过按顺时针迁移的规则，那么object2被迁移到了NODE4中，其它对象还保持这原有的存储位置。通过对节点的添加和删除的分析，一致性哈希算法在保持了单调性的同时，还是数据的迁移达到了最小，这样的算法对分布式集群来说是非常合适的，避免了大量数据迁移，减小了服务器的的压力。 2.5 平衡性根据上面的图解分析，一致性哈希算法满足了单调性和负载均衡的特性以及一般hash算法的分散性，但这还并不能当做其被广泛应用的原由，因为还缺少了平衡性。下面将分析一致性哈希算法是如何满足平衡性的。hash算法是不保证平衡的，如上面只部署了NODE1和NODE3的情况（NODE2被删除的图），object1存储到了NODE1中，而object2、object3、object4都存储到了NODE3中，这样就照成了非常不平衡的状态。 2.6 虚拟节点其实，理论上，只要cache足够多，每个cache在圆环上就会足够分散。但是在真实场景里，cache服务器只会有很少，所以，在一致性哈希算法中，为了尽可能的满足平衡性，其引入了虚拟节点的概念。 “虚拟节点”（ virtual node ）是实际节点（机器）在 hash 空间的复制品（ replica ），一实际个节点（机器）对应了若干个“虚拟节点”，这个对应个数也成为“复制个数”，“虚拟节点”在 hash 空间中以hash值排列。 以上面只部署了NODE1和NODE3的情况（NODE2被删除的图）为例，之前的对象在机器上的分布很不均衡，现在我们以2个副本（复制个数）为例，这样整个hash环中就存在了4个虚拟节点，最后对象映射的关系图如下： 根据上图可知对象的映射关系： 1object1-&gt;NODE1-1，object2-&gt;NODE1-2，object3-&gt;NODE3-2，object4-&gt;NODE3-1 通过虚拟节点的引入，对象的分布就比较均衡了。那么在实际操作中，正真的对象查询是如何工作的呢？对象从hash到虚拟节点到实际节点的转换如下图： “虚拟节点”的hash计算可以采用对应节点的IP地址加数字后缀的方式。例如假设NODE1的IP地址为192.168.1.100。引入“虚拟节点”前，计算 cache A 的 hash 值： 1Hash(“192.168.1.100”); 引入“虚拟节点”后，计算“虚拟节”点NODE1-1和NODE1-2的hash值： 12Hash(“192.168.1.100#1”); // NODE1-1Hash(“192.168.1.100#2”); // NODE1-2]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>哈希</tag>
        <tag>一致性哈希</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（15）：布隆过滤器]]></title>
    <url>%2F2017%2F07%2F29%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8815%EF%BC%89%EF%BC%9A%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%2F</url>
    <content type="text"><![CDATA[一、引入什么情况下需要布隆过滤器？我们先来看几个比较常见的例子： 字处理软件中，需要检查一个英语单词是否拼写正确 在 FBI，一个嫌疑人的名字是否已经在嫌疑名单上 在网络爬虫里，一个网址是否被访问过 yahoo, gmail等邮箱垃圾邮件过滤功能 这几个例子有一个共同的特点： 如何判断一个元素是否存在一个集合中？ 二、常规思路与局限如果想判断一个元素是不是在一个集合里，一般想到的是将集合中所有元素保存起来，然后通过比较确定。链表、树、散列表（又叫哈希表，Hash table）等等数据结构都是这种思路。但是随着集合中元素的增加，我们需要的存储空间越来越大。同时检索速度也越来越慢。 数组 链表 树、平衡二叉树、Trie Map (红黑树) 哈希表 虽然上面描述的这几种数据结构配合常见的排序、二分搜索可以快速高效的处理绝大部分判断元素是否存在集合中的需求。但是当集合里面的元素数量足够大，如果有500万条记录甚至1亿条记录呢？这个时候常规的数据结构的问题就凸显出来了。 数组、链表、树等数据结构会存储元素的内容，一旦数据量过大，消耗的内存也会呈现线性增长，最终达到瓶颈。 有的同学可能会问，哈希表不是效率很高吗？查询效率可以达到O(1)。但是哈希表需要消耗的内存依然很高。使用哈希表存储一亿 个垃圾 email 地址的消耗？哈希表的做法：首先，哈希函数将一个email地址映射成8字节信息指纹；考虑到哈希表存储效率通常小于50%（哈希冲突）；因此消耗的内存：8 2 1亿 字节 = 1.6G 内存，普通计算机是无法提供如此大的内存。这个时候，布隆过滤器（Bloom Filter）就应运而生。在继续介绍布隆过滤器的原理时，先讲解下关于哈希函数的预备知识。 三、哈希函数哈希函数的概念是：将任意大小的数据转换成特定大小的数据的函数，转换后的数据称为哈希值或哈希编码。 一个应用是Hash table（散列表，也叫哈希表），是根据哈希值 (Key value) 而直接进行访问的数据结构。也就是说，它通过把哈希值映射到表中一个位置来访问记录，以加快查找的速度。下面是一个典型的 hash 函数 / 表示意图： 可以明显的看到，原始数据经过哈希函数的映射后称为了一个个的哈希编码，数据得到压缩。哈希函数是实现哈希表和布隆过滤器的基础。 哈希函数有以下两个特点： 如果两个散列值是不相同的（根据同一函数），那么这两个散列值的原始输入也是不相同的。 散列函数的输入和输出不是唯一对应关系的，如果两个散列值相同，两个输入值很可能是相同的。但也可能不同，这种情况称为 “散列碰撞”（或者 “散列冲突”）。 缺点： 引用吴军博士的《数学之美》中所言，哈希表的空间效率还是不够高。如果用哈希表存储一亿个垃圾邮件地址，每个email地址 对应 8bytes, 而哈希表的存储效率一般只有50%，因此一个email地址需要占用16bytes. 因此一亿个email地址占用1.6GB，如果存储几十亿个email address则需要上百GB的内存。除非是超级计算机，一般的服务器是无法存储的。 所以要引入下面的 Bloom Filter。 四、布隆过滤器（Bloom Filter）布隆过滤器（英语：Bloom Filter）是1970年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 4.1 原理布隆过滤器（Bloom Filter）的核心实现是一个超大的位数组和几个哈希函数。假设位数组的长度为m，哈希函数的个数为k 以上图为例，具体的操作流程：假设集合里面有3个元素{x, y, z}，哈希函数的个数为3。首先将位数组进行初始化，将里面每个位都设置位0。 对于集合里面的每一个元素，将元素依次通过3个哈希函数进行映射，每次映射都会产生一个哈希值，这个值对应位数组上面的一个点，然后将位数组对应的位置标记为1。查询W元素是否存在集合中的时候，同样的方法将W通过哈希映射到位数组上的3个点。如果3个点的其中有一个点不为1，则可以判断该元素一定不存在集合中。反之，如果3个点都为1，则该元素可能存在集合中。 注意：此处不能判断该元素是否一定存在集合中，可能存在一定的误判率。可以从图中可以看到：假设某个元素通过映射对应下标为4，5，6这3个点。虽然这3个点都为1，但是很明显这3个点是不同元素经过哈希得到的位置，因此这种情况说明元素虽然不在集合中，也可能对应的都是1，这是误判率存在的原因。 4.2 添加与查询 布隆过滤器添加元素 将要添加的元素给k个哈希函数 得到对应于位数组上的k个位置 将这k个位置设为1 布隆过滤器查询元素 将要查询的元素给k个哈希函数 得到对应于位数组上的k个位置 如果k个位置有一个为0，则肯定不在集合中 如果k个位置全部为1，则可能在集合中 4.3 优点 It tells us that the element either definitely is not in the set or may be in the set. 相比于其它的数据结构，布隆过滤器在空间和时间方面都有巨大的优势。布隆过滤器存储空间和插入/查询时间都是常数（O(k)）。另外，散列函数相互之间没有关系，方便由硬件并行实现。布隆过滤器不需要存储元素本身，在某些对保密要求非常严格的场合有优势。 布隆过滤器可以表示全集，其它任何数据结构都不能； 4.4 缺点但是布隆过滤器的缺点和优点一样明显。误算率是其中之一。随着存入的元素数量增加，误算率随之增加。但是如果元素数量太少，则使用散列表足矣。 误判补救方法是：再建立一个小的白名单，存储那些可能被误判的信息。 另外，一般情况下不能从布隆过滤器中删除元素. 我们很容易想到把位数组变成整数数组，每插入一个元素相应的计数器加 1, 这样删除元素时将计数器减掉就可以了。然而要保证安全地删除元素并非如此简单。首先我们必须保证删除的元素的确在布隆过滤器里面. 这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。 4.5 实例可以快速且空间效率高的判断一个元素是否属于一个集合；用来实现数据字典，或者集合求交集。 Google chrome 浏览器使用bloom filter识别恶意链接（能够用较少的存储空间表示较大的数据集合，简单的想就是把每一个URL都可以映射成为一个bit） 又如： 检测垃圾邮件 假定我们存储一亿个电子邮件地址，我们先建立一个十六亿二进制（比特），即两亿字节的向量，然后将这十六亿个二进制全部设置为零。对于每一个电子邮件地址 X，我们用八个不同的随机数产生器（F1,F2, …,F8） 产生八个信息指纹（f1, f2, …, f8）。再用一个随机数产生器 G 把这八个信息指纹映射到 1 到十六亿中的八个自然数 g1, g2, …,g8。现在我们把这八个位置的二进制全部设置为一。当我们对这一亿个 email 地址都进行这样的处理后。一个针对这些 email 地址的布隆过滤器就建成了。 再如: A,B 两个文件，各存放 50 亿条 URL，每条 URL 占用 64 字节，内存限制是 4G，让你找出 A,B 文件共同的 URL。如果是三个乃至 n 个文件呢？ 分析 ：如果允许有一定的错误率，可以使用 Bloom filter，4G 内存大概可以表示 340 亿 bit。将其中一个文件中的 url 使用 Bloom filter 映射为这 340 亿 bit，然后挨个读取另外一个文件的 url，检查是否与 Bloom filter，如果是，那么该 url 应该是共同的 url（注意会有一定的错误率）。” 4.6 实现下面给出python的简单实现，使用murmurhash算法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import mmh3from bitarray import bitarray# zhihu_crawler.bloom_filter# Implement a simple bloom filter with murmurhash algorithm.# Bloom filter is used to check wether an element exists in a collection, and it has a good performance in big data situation.# It may has positive rate depend on hash functions and elements count.BIT_SIZE = 5000000class BloomFilter: def __init__(self): # Initialize bloom filter, set size and all bits to 0 bit_array = bitarray(BIT_SIZE) bit_array.setall(0) self.bit_array = bit_array def add(self, url): # Add a url, and set points in bitarray to 1 (Points count is equal to hash funcs count.) # Here use 7 hash functions. point_list = self.get_postions(url) for b in point_list: self.bit_array[b] = 1 def contains(self, url): # Check if a url is in a collection point_list = self.get_postions(url) result = True for b in point_list: result = result and self.bit_array[b] return result def get_postions(self, url): # Get points positions in bit vector. point1 = mmh3.hash(url, 41) % BIT_SIZE point2 = mmh3.hash(url, 42) % BIT_SIZE point3 = mmh3.hash(url, 43) % BIT_SIZE point4 = mmh3.hash(url, 44) % BIT_SIZE point5 = mmh3.hash(url, 45) % BIT_SIZE point6 = mmh3.hash(url, 46) % BIT_SIZE point7 = mmh3.hash(url, 47) % BIT_SIZE return [point1, point2, point3, point4, point5, point6, point7]]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>哈希</tag>
        <tag>布隆过滤器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（14）：最短路算法]]></title>
    <url>%2F2017%2F07%2F28%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8814%EF%BC%89%EF%BC%9A%E6%9C%80%E7%9F%AD%E8%B7%AF%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[最短路径问题是图论研究中的一个经典算法问题，旨在寻找图（由结点和路径组成的）中两结点之间的最短路径。算法具体的形式包括： 确定起点的最短路径问题 - 即已知起始结点，求最短路径的问题。适合使用Dijkstra算法。 确定终点的最短路径问题 - 与确定起点的问题相反，该问题是已知终结结点，求最短路径的问题。在无向图中该问题与确定起点的问题完全等同，在有向图中该问题等同于把所有路径方向反转的确定起点的问题。 确定起点终点的最短路径问题 - 即已知起点和终点，求两结点之间的最短路径。 全局最短路径问题 - 求图中所有的最短路径。适合使用Floyd-Warshall算法。 一、Dijkstra算法1.1 算法思想Dijkstra(迪杰斯特拉)算法是典型的单源最短路径算法，用于计算一个节点到其他所有节点的最短路径。主要特点是以起始点为中心向外层层扩展，直到扩展到终点为止。Dijkstra算法是很有代表性的最短路径算法，在很多专业课程中都作为基本内容有详细的介绍，如数据结构，图论，运筹学等等。注意该算法要求图中不存在负权边。 问题描述： 在无向图 G=(V,E) 中，假设每条边 E[i] 的长度为 w[i]，找到由顶点 V0 到其余各点的最短路径。（单源最短路径） 算法思想： 设G=(V,E)是一个带权有向图，把图中顶点集合V分成两组，第一组为已求出最短路径的顶点集合（用S表示，初始时S中只有一个源点，以后每求得一条最短路径 , 就将加入到集合S中，直到全部顶点都加入到S中，算法就结束了），第二组为其余未确定最短路径的顶点集合（用U表示），按最短路径长度的递增次序依次把第二组的顶点加入S中。在加入的过程中，总保持从源点v到S中各顶点的最短路径长度不大于从源点v到U中任何顶点的最短路径长度。此外，每个顶点对应一个距离，S中的顶点的距离就是从v到此顶点的最短路径长度，U中的顶点的距离，是从v到此顶点只包括S中的顶点为中间顶点的当前最短路径长度。 适用条件与限制 有向图和无向图都可以使用本算法，无向图中的每条边可以看成相反的两条边。 用来求最短路的图中不能存在负权边。(可以利用拓扑排序检测) 1.2 算法步骤 初始时，S只包含源点，即S＝{v}，v的距离为0。U包含除v外的其他顶点，即:U={其余顶点}，若v与U中顶点u有边，则正常有权值，若u不是v的出边邻接点，则权值为∞。 从U中选取一个距离v最小的顶点k，把k，加入S中（该选定的距离就是v到k的最短路径长度）。 以k为新考虑的中间点，修改U中各顶点的距离；若从源点v到顶点u的距离（经过顶点k）比原来距离（不经过顶点k）短，则修改顶点u的距离值，修改后的距离值的顶点k的距离加上边上的权。 重复步骤b和c直到所有顶点都包含在S中。 步骤动画如下： 实例如下 用Dijkstra算法找出以A为起点的单源最短路径步骤如下 1.3 代码实现以”邻接矩阵”为例对迪杰斯特拉算法进行说明。 123456789public class MatrixUDG &#123; private int mEdgNum; // 边的数量 private char[] mVexs; // 顶点集合 private int[][] mMatrix; // 邻接矩阵 private static final int INF = Integer.MAX_VALUE; // 最大值 ...&#125; MatrixUDG是邻接矩阵对应的结构体。mVexs用于保存顶点，mEdgNum用于保存边数，mMatrix则是用于保存矩阵信息的二维数组。例如，mMatrix[i][j]=1，则表示”顶点i(即mVexs[i])”和”顶点j(即mVexs[j])”是邻接点；mMatrix[i][j]=0，则表示它们不是邻接点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/* * Dijkstra最短路径。 * 即，统计图中"顶点vs"到其它各个顶点的最短路径。 * * 参数说明： * vs -- 起始顶点(start vertex)。即计算"顶点vs"到其它顶点的最短路径。 * prev -- 前驱顶点数组。即，prev[i]的值是"顶点vs"到"顶点i"的最短路径所经历的全部顶点中，位于"顶点i"之前的那个顶点。 * dist -- 长度数组。即，dist[i]是"顶点vs"到"顶点i"的最短路径的长度。 */public void dijkstra(int vs, int[] prev, int[] dist) &#123; // flag[i]=true表示"顶点vs"到"顶点i"的最短路径已成功获取 boolean[] flag = new boolean[mVexs.length]; // 初始化 for (int i = 0; i &lt; mVexs.length; i++) &#123; flag[i] = false; // 顶点i的最短路径还没获取到。 prev[i] = 0; // 顶点i的前驱顶点为0。 dist[i] = mMatrix[vs][i]; // 顶点i的最短路径为"顶点vs"到"顶点i"的权。 &#125; // 对"顶点vs"自身进行初始化 flag[vs] = true; dist[vs] = 0; // 遍历mVexs.length-1次；每次找出一个顶点的最短路径。 int k=0; for (int i = 1; i &lt; mVexs.length; i++) &#123; // 寻找当前最小的路径； // 即，在未获取最短路径的顶点中，找到离vs最近的顶点(k)。 int min = INF; for (int j = 0; j &lt; mVexs.length; j++) &#123; if (flag[j]==false &amp;&amp; dist[j]&lt;min) &#123; min = dist[j]; k = j; &#125; &#125; // 标记"顶点k"为已经获取到最短路径 flag[k] = true; // 修正当前最短路径和前驱顶点 // 即，当已经"顶点k的最短路径"之后，更新"未获取最短路径的顶点的最短路径和前驱顶点"。 for (int j = 0; j &lt; mVexs.length; j++) &#123; int tmp = (mMatrix[k][j]==INF ? INF : (min + mMatrix[k][j])); if (flag[j]==false &amp;&amp; (tmp&lt;dist[j]) ) &#123; dist[j] = tmp; prev[j] = k; &#125; &#125; &#125; // 打印dijkstra最短路径的结果 System.out.printf("dijkstra(%c): \n", mVexs[vs]); for (int i=0; i &lt; mVexs.length; i++) System.out.printf(" shortest(%c, %c)=%d\n", mVexs[vs], mVexs[i], dist[i]);&#125; 1.4 时间复杂度我们可以用大O符号将该算法的运行时间表示为边数m和顶点数n的函数。 对于基于顶点集Q的实现，算法的运行时间是$O(|E|\cdot dk_{Q}+|V|\cdot em_{Q})$，其中$dk_{Q}和em_{Q}$分别表示完成键的降序排列时间和从Q中提取最小键值的时间。 Dijkstra算法最简单的实现方法是用一个链表或者数组来存储所有顶点的集合Q，所以搜索Q中最小元素的运算（Extract-Min(Q)）只需要线性搜索 Q中的所有元素。这样的话算法的运行时间是$O(n^{2})$。 对于边数少于$n^{2}$的稀疏图来说，我们可以用邻接表来更有效的实现该算法。同时需要将一个二叉堆或者斐波纳契堆用作优先队列来寻找最小的顶点（Extract-Min）。当用到二叉堆的时候，算法所需的时间为${\displaystyle O((m+n)logn)}$，斐波纳契堆能稍微提高一些性能，让算法运行时间达到${\displaystyle O(m+nlogn)}$。然而，使用斐波纳契堆进行编程，常常会由于算法常数过大而导致速度没有显著提高。 二、Floyd算法Dijkstra很优秀，但是使用Dijkstra有一个最大的限制，就是不能有负权边。而Bellman-Ford适用于权值可以为负、无权值为负的回路的图。这比Dijkstra算法的使用范围要广。 2.1 算法思想Floyd算法是一个经典的动态规划算法。用通俗的语言来描述的话，首先我们的目标是寻找从点i到点j的最短路径。从动态规划的角度看问题，我们需要为这个目标重新做一个诠释（这个诠释正是动态规划最富创造力的精华所在） 从任意节点i到任意节点j的最短路径不外乎2种可能，1是直接从i到j，2是从i经过若干个节点k到j。所以，我们假设Dis(i,j)为节点u到节点v的最短路径的距离，对于每一个节点k，我们检查Dis(i,k) + Dis(k,j) &lt; Dis(i,j)是否成立，如果成立，证明从i到k再到j的路径比i直接到j的路径短，我们便设置Dis(i,j) = Dis(i,k) + Dis(k,j)，这样一来，当我们遍历完所有节点k，Dis(i,j)中记录的便是i到j的最短路径的距离。 2.2 算法步骤初始状态：S是记录各个顶点间最短路径的矩阵。 初始化S：矩阵S中顶点a[i][j]的距离为顶点i到顶点j的权值；如果i和j不相邻，则a[i][j]=∞。实际上，就是将图的原始矩阵复制到S中。 注:a[i][j]表示矩阵S中顶点i(第i个顶点)到顶点j(第j个顶点)的距离。 以顶点A(第1个顶点)为中介点，若$a[i][j] &gt; a[i][0]+a[0][j]$，则设置$a[i][j]=a[i][0]+a[0][j]$。 以顶点$a[1]$，上一步操作之后，$a[1][6]=∞$；而将A作为中介点时，(B,A)=12，(A,G)=14，因此B和G之间的距离可以更新为26。 同理，依次将顶点B,C,D,E,F,G作为中介点，并更新a[i][j]的大小。 2.3 代码实现以”邻接矩阵”为例对弗洛伊德算法进行说明，对于”邻接表”实现的图在后面会给出相应的源码。 123456789public class MatrixUDG &#123; private int mEdgNum; // 边的数量 private char[] mVexs; // 顶点集合 private int[][] mMatrix; // 邻接矩阵 private static final int INF = Integer.MAX_VALUE; // 最大值 ...&#125; MatrixUDG是邻接矩阵对应的结构体。mVexs用于保存顶点，mEdgNum用于保存边数，mMatrix则是用于保存矩阵信息的二维数组。例如，mMatrix[i][j]=1，则表示”顶点i(即mVexs[i])”和”顶点j(即mVexs[j])”是邻接点；mMatrix[i][j]=0，则表示它们不是邻接点。 12345678910111213141516171819202122232425262728293031323334353637383940414243/* * floyd最短路径。 * 即，统计图中各个顶点间的最短路径。 * * 参数说明： * path -- 路径。path[i][j]=k表示，"顶点i"到"顶点j"的最短路径会经过顶点k。 * dist -- 长度数组。即，dist[i][j]=sum表示，"顶点i"到"顶点j"的最短路径的长度是sum。 */public void floyd(int[][] path, int[][] dist) &#123; // 初始化 for (int i = 0; i &lt; mVexs.length; i++) &#123; for (int j = 0; j &lt; mVexs.length; j++) &#123; dist[i][j] = mMatrix[i][j]; // "顶点i"到"顶点j"的路径长度为"i到j的权值"。 path[i][j] = j; // "顶点i"到"顶点j"的最短路径是经过顶点j。 &#125; &#125; // 计算最短路径 for (int k = 0; k &lt; mVexs.length; k++) &#123; for (int i = 0; i &lt; mVexs.length; i++) &#123; for (int j = 0; j &lt; mVexs.length; j++) &#123; // 如果经过下标为k顶点路径比原两点间路径更短，则更新dist[i][j]和path[i][j] int tmp = (dist[i][k]==INF || dist[k][j]==INF) ? INF : (dist[i][k] + dist[k][j]); if (dist[i][j] &gt; tmp) &#123; // "i到j最短路径"对应的值设，为更小的一个(即经过k) dist[i][j] = tmp; // "i到j最短路径"对应的路径，经过k path[i][j] = path[i][k]; &#125; &#125; &#125; &#125; // 打印floyd最短路径的结果 System.out.printf("floyd: \n"); for (int i = 0; i &lt; mVexs.length; i++) &#123; for (int j = 0; j &lt; mVexs.length; j++) System.out.printf("%2d ", dist[i][j]); System.out.printf("\n"); &#125;&#125; 2.4 时间复杂度Floyd-Warshall算法的时间复杂度为$O(N^{3})$，空间复杂度为$O(N^{2})$。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>Dijkstra算法</tag>
        <tag>Floyd算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（32）：MapReduce执行流程详解]]></title>
    <url>%2F2017%2F07%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8832%EF%BC%89%EF%BC%9AMapReduce%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[一、简述MapReduce最早是由Google提出的分布式数据处理模型，随后受到了业内的广泛关注，并被大量应用到各种商业场景中。比如： 搜索：网页爬取、倒排索引、PageRank。 Web访问日志分析：分析和挖掘用户在web上的访问、购物行为特征，实现个性化推荐；分析用户访问行为。 文本统计分析：比如莫言小说的WordCount、词频TFIDF分析；学术论文、专利文献的引用分析和统计；维基百科数据分析等。 海量数据挖掘：非结构化数据、时空数据、图像数据的挖掘。 机器学习：监督学习、无监督学习、分类算法如决策树、SVM等。 自然语言处理：基于大数据的训练和预测；基于语料库构建单词同现矩阵，频繁项集数据挖掘、重复文档检测等。 广告推荐：用户点击（CTR)和购买行为（CVR）预测。 一个Map/Reduce作业（job）通常会把输入的数据（input file）且分为若干个独立的数据块（splits），然后由map任务（task）以完全并行的方式处理它们。Map/Reduce框架会对map的输出做一个Shuffle操作，Suffle操作后的结果会输入给reduce任务。整个Map/Reduce框架负责任务的调度和监控，以及重新执行已经失败的任务。 Map/Reduce计算集群由一个单独的JobTracker（master）和每个集群节点一个TaskTracker（slave）共同组成。JobTracker负责调度构成一个作业的所有任务，这些任务会被分派到不同的TaskTracker上去执行，JobTRacker会去监控它们的执行、重新执行已经失败的任务。而TaskTracker仅负责执行由JobTracker指派的任务。 本文将按照map/reduce执行流程中各个任务的时间顺序详细叙述map/reduce的各个任务模块，包括：输入分片（input split）、map阶段、combiner阶段、shuffle阶段和reduce阶段。下图是一个不错的执行流程图： 二、作业的提交与监控JobClient是用户提交的作业与JobTracker交互的主要接口。 JobClient提交作业的过程如下： map/reduce程序通过runJob()方法新建一个JobClient实例; 向JobTracker请求一个新jobID，通过JobTracker的getNewJobId()获取； 检查作业输入输出说明。如果没有指定输出目录或者输出目录已经存在，作业将不会被提交，map/reduce程序； 输入作业划分split，如果划分无法计算（如：输入路径不存在），作业将不会被提交，错误返回给map/reduce程序。 将运行作业所需要的资源（作业的jar文件、配置文件、计算所得的输入划分）复制到一个以作业ID命名的目录中； 通过调用JobTracker的submitJob()方法，告诉JobTracker作业准备提交； JobTracker将提交的作业放到一个内部队列中，交由作业调度器进行调度，并对其进行初始化。 创建Map任务、Reduce任务：一个split对应一个map，有多少split就有多少map; Reduce任务的数量由JobConf的mapred.reduce.tasks属性决定 TaskTracker执行一个简单的循环，定期发送心跳（heartbeat）给JobTracker 三、执行流程3.1 Input fileInput file是map/reduce任务的原始数据，一般存储在HDFS上。应用程序至少应该指明输入/输出的位置（路径），并通过实现合适的接口或抽象类提供map和reduce函数。再加上其他作业的参数，就构成了作业配置（job configuration）。然后，Hadoop的 job client提交作业（jar包/可执行程序等）和配置信息给JobTracker，后者负责分发这些软件和配置信息给slave、调度任务并监控它们的执行，同时提供状态和诊断信息给job-client。 3.1.1 InputFormatInputFormat为Map/Reduce作业输入的细节规范。Map/Reduce框架根据作业的InputFormat来： 检查作业输入的正确性，如格式等。 把输入文件切分成多个逻辑InputSplit实例， 一个InputSplit将会被分配给一个独立的Map任务。 提供RecordReader实现，这个RecordReader从逻辑InputSplit中获得输入记录（”K-V对”），这些记录将由Map任务处理。 InputFormat有如下几种: TextInputFormat: TextInputFormat是默认的INputFormat，输入文件中的每一行就是一个记录，Key是这一行的byte offset，而value是这一行的内容。如果一个作业的Inputformat是TextInputFormat，并且框架检测到输入文件的后缀是 .gz 和 .lzo，就会使用对应的CompressionCodec自动解压缩这些文件。但是需要注意，上述带后缀的压缩文件不会被切分，并且整个压缩文件会分给一个mapper来处理。 KeyValueTextInputFormat 输入文件中每一行就是一个记录，第一个分隔符字符切分每行。在分隔符字符之前的内容为Key，在之后的为Value。分隔符变量通过key.value.separator.in.input.line变量设置，默认为(\t)字符。 NLineInputFormat 与TextInputFormat一样，但每个数据块必须保证有且只有Ｎ行，mapred.line.input.format.linespermap属性，默认为1。 SequenceFileInputFormat 一个用来读取字符流数据的InputFormat，为用户自定义的。字符流数据是Hadoop自定义的压缩的二进制数据格式。它用来优化从一个MapReduce任务的输出到另一个MapReduce任务的输入之间的数据传输过程。 3.2 输入分片（Input files）InputSplit是一个单独的Map任务需要处理的数据块。一般的InputSplit是字节样式输入，然后由RecordReader处理并转化成记录样式。通常一个split就是一个block，这样做的好处是使得Map任务可以在存储有当前数据的节点上运行本地的任务，而不需要通过网络进行跨节点的任务调度。 可以通过设置mapred.min.split.size， mapred.max.split.size, block.size来控制拆分的大小。如果mapred.min.split.size大于block size，则会将两个block合成到一个split，这样有部分block数据需要通过网络读取；如果mapred.max.split.size小于block size，则会将一个block拆成多个split，增加了Map任务数。 在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务，输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组，输入分片（input split）往往和hdfs的block（块）关系很密切，假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别是3mb、65mb和127mb，那么mapreduce会把3mb文件分为一个输入分片（input split），65mb则是两个输入分片（input split）而127mb也是两个输入分片（input split），换句话说我们如果在map计算前做输入分片调整，例如合并小文件，那么就会有5个map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce优化计算的一个关键点。 12输入文件大小 10M 65M 127M分割后的InputSplit大小 10M 64M,1M 64M，63M 在Map任务开始前，会先获取文件在HDFS上的路径和block信息，然后根据splitSize对文件进行切分（splitSize = computeSplitSize(blockSize, minSize, maxSize) ），默认splitSize 就等于blockSize的默认值（64m）。 假设现在我们有两个文本文件，作为我们例子的输入： 1234567File 1 内容：My name is TonyMy company is pivotalFile 2 内容：My name is LisaMy company is EMC 3.2 Map阶段Map是一类将输入记录集转换为中间格式记录集的独立任务，主要是读取InputSplit的每一个Key,Value对并进行处理。 首先我们的输入就是两个文件， 默认情况下就是两个split, 对应前面图中的split 0, split 1 两个split 默认会分给两个Mapper来处理， WordCount例子相当地暴力， 这一步里面就是直接把文件内容分解为单词和 1 （注意， 不是具体数量， 就是数字1）其中的单词就是我们的主健，也称为Key, 后面的数字就是对应的值，也称为value. 那么对应两个Mapper的输出就是： split 0 12345678My 1name 1is 1Tony 1My 1company 1is 1Pivotal 1 split 1 12345678My 1name 1is 1Lisa 1My 1company 1is 1EMC 1 3.3 Shuffle阶段将map的输出作为reduce的输入的过程就是shuffle了，这个是mapreduce优化的重点地方。这里我不讲怎么优化shuffle阶段，讲讲shuffle阶段的原理，因为大部分的书籍里都没讲清楚shuffle阶段。Shuffle一开始就是map阶段做输出操作，一般mapreduce计算的都是海量数据，map输出时候不可能把所有文件都放到内存操作，因此map写入磁盘的过程十分的复杂，更何况map输出时候要对结果进行排序，内存开销是很大的，map在做输出时候会在内存里开启一个环形内存缓冲区，这个缓冲区专门用来输出的，默认大小是100mb，并且在配置文件里为这个缓冲区设定了一个阀值，默认是0.80（这个大小和阀值都是可以在配置文件里进行配置的），同时map还会为输出操作启动一个守护线程，如果缓冲区的内存达到了阀值的80%时候，这个守护线程就会把内容写到磁盘上，这个过程叫spill，另外的20%内存可以继续写入要写进磁盘的数据，写入磁盘和写入内存操作是互不干扰的，如果缓存区被撑满了，那么map就会阻塞写入内存的操作，让写入磁盘操作完成后再继续执行写入内存操作，前面我讲到写入磁盘前会有个排序操作，这个是在写入磁盘操作时候进行，不是在写入内存时候进行的，如果我们定义了combiner函数，那么排序前还会执行combiner操作。每次spill操作也就是写入磁盘操作时候就会写一个溢出文件，也就是说在做map输出有几次spill就会产生多少个溢出文件，等map输出全部做完后，map会合并这些输出文件。这个过程里还会有一个Partitioner操作，对于这个操作很多人都很迷糊，其实Partitioner操作和map阶段的输入分片（Input split）很像，一个Partitioner对应一个reduce作业，如果我们mapreduce操作只有一个reduce操作，那么Partitioner就只有一个，如果我们有多个reduce操作，那么Partitioner对应的就会有多个，Partitioner因此就是reduce的输入分片，这个程序员可以编程控制，主要是根据实际key和value的值，根据实际业务类型或者为了更好的reduce负载均衡要求进行，这是提高reduce效率的一个关键所在。到了reduce阶段就是合并map输出文件了，Partitioner会找到对应的map输出文件，然后进行复制操作，复制操作时reduce会开启几个复制线程，这些线程默认个数是5个，程序员也可以在配置文件更改复制线程的个数，这个复制过程和map写入磁盘过程类似，也有阀值和内存大小，阀值一样可以在配置文件里配置，而内存大小是直接使用reduce的tasktracker的内存大小，复制时候reduce还会进行排序操作和合并文件操作，这些操作完了就会进行reduce计算了。 3.3.1 PartitionPartition 是什么？ Partition 就是分区。 为什么要分区？ 因为有时候会有多个Reducer, Partition就是提前对输入进行处理， 根据将来的Reducer进行分区. 到时候Reducer处理的时候， 只需要处理分给自己的数据就可以了。 如何分区？ 主要的分区方法就是按照Key 的不同，把数据分开，其中很重要的一点就是要保证Key的唯一性， 因为将来做Reduce的时候有可能是在不同的节点上做的， 如果一个Key同时存在于两个节点上， Reduce的结果就会出问题， 所以很常见的Partition方法就是哈希。 结合我们的例子， 我们这里假设有两个Reducer, 前面两个split 做完Partition的结果就会如下： split 0 1234567891011Partition 1:company 1is 1is 1Partition 2:My 1My 1name 1Pivotal 1Tony 1 split 1 1234567891011Partition 1:company 1is 1is 1EMC 1Partition 2:My 1My 1name 1Lisa 1 其中Partition 1 将来是准备给Reducer 1 处理的， Partition 2 是给Reducer 2 的 这里我们可以看到， Partition 只是把所有的条目按照Key 分了一下区， 没有其他任何处理， 每个区里面的Key 都不会出现在另外一个区里面。 3.3.2 SortSort 就是排序喽， 其实这个过程在我来看并不是必须的， 完全可以交给客户自己的程序来处理。 那为什么还要排序呢？ 可能是写MapReduce的大牛们想，“大部分reduce 程序应该都希望输入的是已经按Key排序好的数据， 如果是这样， 那我们就干脆顺手帮你做掉啦， 请叫我雷锋！” ……好吧， 你是雷锋. 那么我们假设对前面的数据再进行排序， 结果如下： split 0 1234567891011Partition 1:company 1is 1is 1Partition 2:My 1My 1name 1Pivotal 1Tony 1 split 1 1234567891011Partition 1:company 1EMC 1is 1is 1Partition 2:Lisa 1My 1My 1name 1 这里可以看到， 每个partition里面的条目都按照Key的顺序做了排序 3.3.3 Combine什么是Combine呢？ Combine 其实可以理解为一个mini Reduce 过程， 它发生在前面Map的输出结果之后， 目的就是在结果送到Reducer之前先对其进行一次计算， 以减少文件的大小， 方便后面的传输。 但这步也不是必须的。 按照前面的输出， 执行Combine: split 0 123456789Partition 1:company 1is 2Partition 2:My 2name 1Pivotal 1Tony 1 split 1 123456789Partition 1:company 1EMC 1is 2Partition 2:Lisa 1My 2name 1 我们可以看到， 针对前面的输出结果， 我们已经局部地统计了is 和My的出现频率， 减少了输出文件的大小。 3.3.4 copy下面就要准备把输出结果传送给Reducer了。 这个阶段被称为Copy, 但事实上雷子认为叫他Download更为合适， 因为实现的时候， 是通过http的方式， 由Reducer节点向各个mapper节点下载属于自己分区的数据。 那么根据前面的Partition, 下载完的结果如下： Reducer 节点 1 共包含两个文件: 123Partition 1:company 1is 2 1234Partition 1:company 1EMC 1is 2 Reducer 节点 2 也是两个文件: 12345 Partition 2:My 2name 1Pivotal 1Tony 1 1234Partition 2:Lisa 1My 2name 1 这里可以看到， 通过Copy, 相同Partition 的数据落到了同一个节点上。 3.3.5 merge如上一步所示， 此时Reducer得到的文件是从不同Mapper那里下载到的， 需要对他们进行合并为一个文件， 所以下面这一步就是Merge, 结果如下： Reducer 节点 1 12345company 1company 1EMC 1is 2is 2 Reducer 节点 2 1234567Lisa 1My 2My 2name 1name 1Pivotal 1Tony 1 3.4 Redeuce阶段reduce阶段对数据进行归约处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。归约后的数据输出到本地文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。 四、实例说明下面将以WordCount为例，解释MapReduce各个阶段的概念。 假设存在一个文本a.txt，文本内每行是一个数字，我们要统计每个数字出现的次数。文本内的数字称为Word，数字出现的次数称为Count。如果MaxCompute Mapreduce完成这一功能，需要经历下图描述的几个步骤： 首先对文本进行分片，将每片内的数据作为单个Map Worker的输入； Map处理输入，每获取一个数字，将数字的Count设置为1，并将此对输出，此时以Word作为输出数据的Key；在Shuffle阶段前期，首先对每个Map Worker的输出，按照Key值，即Word值排序。排序后进行Combine操作，即将Key值(Word值)相同的Count累加，构成一个新的对。此过程被称为合并排序； 在Shuffle阶段后期，数据被发送到Reduce端。Reduce Worker收到数据后依赖Key值再次对数据排序； 每个Reduce Worker对数据进行处理时，采用与Combiner相同的逻辑，将Key值(Word值)相同的Count累加，得到输出结果； 分布式相关mr 方案解决矩阵相乘的代码；hadoop原理，shuffle如何排序，map如何切割数据，如何处理数据倾斜，join的mr代码 MR的shuffle过程？内存不够时涉及大文件排序如何处理？ 答：先hash到不同文件中，每个文件排序，然后每个文件读取行，类似归并排序的思路？ Hadoop,Spark,storm下面的产品，原理，适用场景 spark跟hadoop的区别答：spark有RDD机制，写内存，相对hadoop适合迭代运算 如何用hadoop实现k-means简单介绍 MapReduce 原理，有没有看过源码，说说 Map 阶段怎么实现的,MapReduce 实现统计出现次数最多的前 100 个访问 IP.MapReduce 实现统计不重复用户 ID,MapReduce 实现两个数据集求交集。HBase 行健怎么设计,spark 性能一般优化方法,spark streaming 和 storm 区别.给了一张笔试题， 10 道选择，一道大题。选择题是 java 基础知识，大题一个有三问：根据场景写出 Hive 建表语句； Hsql 从表中查询；用MapReduce写好友推荐，在一堆单词里面找出现次数最多的k个用分布式的方法做采样怎么保证采样结果完全符合预期？后面又问了Hadoop,Spark,storm下面的产品，原理，适用场景，写一个 Hadoop 版本的 wordcount。K-means能否分布式实现？ 答：因为本身是迭代式算法，所以只能半分布式实现，即在计算类的均值、每个样本点属于哪个类的时候还有怎么解决mapreduce数据倾斜]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（13）：深度优先搜索和广度优先搜索]]></title>
    <url>%2F2017%2F07%2F27%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8813%EF%BC%89%EF%BC%9A%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%E5%92%8C%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[BFS和DFS是两种十分重要的搜索算法，BFS适合查找最优解，DFS适合查找是否存在解(或者说能找到任意一个可行解)。用这两种算法即可以解决大部分树和图的问题。 一、深度优先搜索（DFS）1.1 介绍图的深度优先搜索（Depth First Search），和树的先序遍历比较类似。它的思想：假设初始状态是图中所有顶点均未被访问，则从某个顶点V出发，首先访问该顶点，然后依次从它的各个未被访问的邻接点出发深度优先搜索遍历图，直至图中所有和V有路径相通的顶点都被访问到。若此时尚有其他顶点未被访问到，则另选一个未被访问的顶点作起始点，重复上述过程，直至图中所有顶点都被访问到为止。 显然，深度优先搜索是一个递归的过程。 1.2 图解1.2.1 无向图的深度优先搜索下面以“无向图”为例，来对深度优先搜索进行演示。对上面的图G1进行深度优先遍历，从顶点A开始。 第1步：访问A 第2步：访问（A的邻接点）C。在第一步访问A之后，接下来应该访问的是A的邻接点，即“C/D/F”中的一个。但在本文的实现中，顶点ABCDEFG是按照顺序存储，C在“D和F的前面，因此，先访问C。” 第3步：访问（C的邻接点）B。在第2步访问C之后，接下来应该访问C的邻接点，即”B和D”中一个(A已经被访问过，就不算在内)。而由于B在D之前，先访问B。 第4步：访问(C的邻接点)D。 在第3步访问了C的邻接点B之后，B没有未被访问的邻接点；因此，返回到访问C的另一个邻接点D。 第5步：访问(A的邻接点)F。 前面已经访问了A，并且访问完了”A的邻接点B的所有邻接点(包括递归的邻接点在内)”；因此，此时返回到访问A的另一个邻接点F。 第6步：访问(F的邻接点)G。 第7步：访问(G的邻接点)E。 因此访问顺序是：$A=&gt;C=&gt;=&gt;B=&gt;D=&gt;F=&gt;G=&gt;E$ 1.2.2 有向图的深度优先搜索下面以“有向图”为例，来对深度优先搜索进行演示。对上面的图G2进行深度优先遍历，从顶点A开始。 第1步：访问A。 第2步：访问B。在访问了A之后，接下来应该访问的是A的出边的另一个顶点，即顶点B。 第3步：访问C。在访问了B之后，接下来应该访问的是B的出边的另一个顶点，即顶点C,E,F。在本文实现的图中，顶点ABCDEFG按照顺序存储，因此先访问C。 第4步：访问E。接下来访问C的出边的另一个顶点，即顶点E。 第5步：访问D。接下来访问E的出边的另一个顶点，即顶点B,D。顶点B已经被访问过，因此访问顶点D。 第6步：访问F。 接下应该回溯”访问A的出边的另一个顶点F”。 第7步：访问G。 因此访问顺序是：$A =&gt; B =&gt; C =&gt; E =&gt; D =&gt; F =&gt; G$ 二、广度优先搜索（BFS）2.1 介绍广度优先搜索算法（Breadth First Search），又称为“宽度优先搜索”或“横向优先搜索”，简称BFS。 它的思想是：从图中某顶点V出发，在访问了V之后依次访问V的各个未曾访问过的邻接点，然后分别从这些邻接点出发依次访问他们的邻接点，并使得“先被访问的顶点的邻接点先于后被访问的顶点的邻接点被访问”，直至图中所有已被访问的顶点的邻接点都被访问到。如果此时图中尚有顶点未被访问，则需要另选一个未曾被访问的顶点作为新的起始点，重复上述过程，知道图中所有顶点都被访问到为止。 换句话说，广度优先搜索遍历图的过程是以V为起点，由近至远，依次访问和V有路径相同且路径长度为1、2、3……的顶点。 2.2 图解2.2.1 无向图的广度优先搜索下面以“无向图”为例，来对广度优先搜索进行演示。还是以上面的图G1为例进行说明。 第1步：访问A。 第2步：依次访问C,D,F。在访问了A之后，接下来访问A的邻接点。前面已经说过，在本文实现中，顶点ABCDEFG按照顺序存储的，C在”D和F”的前面，因此，先访问C。再访问完C之后，再依次访问D,F。 第3步：依次访问B,G。 在第2步访问完C,D,F之后，再依次访问它们的邻接点。首先访问C的邻接点B，再访问F的邻接点G。 第4步：访问E。在第3步访问完B,G之后，再依次访问它们的邻接点。只有G有邻接点E，因此访问G的邻接点E。 因此访问顺序是：$A =&gt; C =&gt; D =&gt; F =&gt; B =&gt; G =&gt; E$ 2.2.2 有向图的广度优先搜索下面以“有向图”为例，来对广度优先搜索进行演示。还是以上面的图G2为例进行说明。 第1步：访问A。 第2步：访问B。 第3步：依次访问C,E,F。 在访问了B之后，接下来访问B的出边的另一个顶点，即C,E,F。前面已经说过，在本文实现中，顶点ABCDEFG按照顺序存储的，因此会先访问C，再依次访问E,F。 第4步：依次访问D,G。 在访问完C,E,F之后，再依次访问它们的出边的另一个顶点。还是按照C,E,F的顺序访问，C的已经全部访问过了，那么就只剩下E,F；先访问E的邻接点D，再访问F的邻接点G。 三、代码实现无向图和有向图的BFS与DFS 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227/** * Java: 邻接矩阵表示的&quot;无向图(Matrix Undirected Graph)&quot; * * @author skywang * @date 2014/04/19 */import java.io.IOException;import java.util.Scanner;public class Demo &#123; private char[] mVexs; // 顶点集合 private int[][] mMatrix; // 邻接矩阵 /* * 创建图(自己输入数据) */ public Demo() &#123; // 输入&quot;顶点数&quot;和&quot;边数&quot; System.out.printf(&quot;input vertex number: &quot;); int vlen = readInt(); System.out.printf(&quot;input edge number: &quot;); int elen = readInt(); if ( vlen &lt; 1 || elen &lt; 1 || (elen &gt; (vlen*(vlen - 1)))) &#123; System.out.printf(&quot;input error: invalid parameters!\n&quot;); return ; &#125; // 初始化&quot;顶点&quot; mVexs = new char[vlen]; for (int i = 0; i &lt; mVexs.length; i++) &#123; System.out.printf(&quot;vertex(%d): &quot;, i); mVexs[i] = readChar(); &#125; // 初始化&quot;边&quot; mMatrix = new int[vlen][vlen]; for (int i = 0; i &lt; elen; i++) &#123; // 读取边的起始顶点和结束顶点 System.out.printf(&quot;edge(%d):&quot;, i); char c1 = readChar(); char c2 = readChar(); int p1 = getPosition(c1); int p2 = getPosition(c2); if (p1==-1 || p2==-1) &#123; System.out.printf(&quot;input error: invalid edge!\n&quot;); return ; &#125; mMatrix[p1][p2] = 1; mMatrix[p2][p1] = 1; &#125; &#125; /* * 创建图(用已提供的矩阵) * * 参数说明： * vexs -- 顶点数组 * edges -- 边数组 */ public Demo(char[] vexs, char[][] edges) &#123; // 初始化&quot;顶点数&quot;和&quot;边数&quot; int vlen = vexs.length; int elen = edges.length; // 初始化&quot;顶点&quot; mVexs = new char[vlen]; for (int i = 0; i &lt; mVexs.length; i++) mVexs[i] = vexs[i]; // 初始化&quot;边&quot; mMatrix = new int[vlen][vlen]; for (int i = 0; i &lt; elen; i++) &#123; // 读取边的起始顶点和结束顶点 int p1 = getPosition(edges[i][0]); int p2 = getPosition(edges[i][1]); mMatrix[p1][p2] = 1; mMatrix[p2][p1] = 1; &#125; &#125; /* * 返回ch位置 */ private int getPosition(char ch) &#123; for(int i=0; i&lt;mVexs.length; i++) if(mVexs[i]==ch) return i; return -1; &#125; /* * 读取一个输入字符 */ private char readChar() &#123; char ch=&apos;0&apos;; do &#123; try &#123; ch = (char)System.in.read(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; while(!((ch&gt;=&apos;a&apos;&amp;&amp;ch&lt;=&apos;z&apos;) || (ch&gt;=&apos;A&apos;&amp;&amp;ch&lt;=&apos;Z&apos;))); return ch; &#125; /* * 读取一个输入字符 */ private int readInt() &#123; Scanner scanner = new Scanner(System.in); return scanner.nextInt(); &#125; /* * 返回顶点v的第一个邻接顶点的索引，失败则返回-1 */ private int firstVertex(int v) &#123; if (v&lt;0 || v&gt;(mVexs.length-1)) return -1; for (int i = 0; i &lt; mVexs.length; i++) if (mMatrix[v][i] == 1) return i; return -1; &#125; /* * 返回顶点v相对于w的下一个邻接顶点的索引，失败则返回-1 */ private int nextVertex(int v, int w) &#123; if (v&lt;0 || v&gt;(mVexs.length-1) || w&lt;0 || w&gt;(mVexs.length-1)) return -1; for (int i = w + 1; i &lt; mVexs.length; i++) if (mMatrix[v][i] == 1) return i; return -1; &#125; /* * 深度优先搜索遍历图的递归实现 */ private void DFS(int i, boolean[] visited) &#123; visited[i] = true; System.out.printf(&quot;%c &quot;, mVexs[i]); // 遍历该顶点的所有邻接顶点。若是没有访问过，那么继续往下走 for (int w = firstVertex(i); w &gt;= 0; w = nextVertex(i, w)) &#123; if (!visited[w]) DFS(w, visited); &#125; &#125; /* * 深度优先搜索遍历图 */ public void DFS() &#123; boolean[] visited = new boolean[mVexs.length]; // 顶点访问标记 // 初始化所有顶点都没有被访问 for (int i = 0; i &lt; mVexs.length; i++) visited[i] = false; System.out.printf(&quot;DFS: &quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; if (!visited[i]) DFS(i, visited); &#125; System.out.printf(&quot;\n&quot;); &#125; /* * 广度优先搜索（类似于树的层次遍历） */ public void BFS() &#123; int head = 0; int rear = 0; int[] queue = new int[mVexs.length]; // 辅组队列 boolean[] visited = new boolean[mVexs.length]; // 顶点访问标记 for (int i = 0; i &lt; mVexs.length; i++) visited[i] = false; System.out.printf(&quot;BFS: &quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; if (!visited[i]) &#123; visited[i] = true; System.out.printf(&quot;%c &quot;, mVexs[i]); queue[rear++] = i; // 入队列 &#125; while (head != rear) &#123; int j = queue[head++]; // 出队列 for (int k = firstVertex(j); k &gt;= 0; k = nextVertex(j, k)) &#123; //k是为访问的邻接顶点 if (!visited[k]) &#123; visited[k] = true; System.out.printf(&quot;%c &quot;, mVexs[k]); queue[rear++] = k; &#125; &#125; &#125; &#125; System.out.printf(&quot;\n&quot;); &#125; /* * 打印矩阵队列图 */ public void print() &#123; System.out.printf(&quot;Martix Graph:\n&quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; for (int j = 0; j &lt; mVexs.length; j++) System.out.printf(&quot;%d &quot;, mMatrix[i][j]); System.out.printf(&quot;\n&quot;); &#125; &#125; public static void main(String[] args) &#123; char[] vexs = &#123;&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;D&apos;, &apos;E&apos;, &apos;F&apos;, &apos;G&apos;&#125;; char[][] edges = new char[][]&#123; &#123;&apos;A&apos;, &apos;C&apos;&#125;, &#123;&apos;A&apos;, &apos;D&apos;&#125;, &#123;&apos;A&apos;, &apos;F&apos;&#125;, &#123;&apos;B&apos;, &apos;C&apos;&#125;, &#123;&apos;C&apos;, &apos;D&apos;&#125;, &#123;&apos;E&apos;, &apos;G&apos;&#125;, &#123;&apos;F&apos;, &apos;G&apos;&#125;&#125;; Demo pG; // 自定义&quot;图&quot;(输入矩阵队列) //pG = new MatrixUDG(); // 采用已有的&quot;图&quot; pG = new Demo(vexs, edges); pG.print(); // 打印图 pG.DFS(); // 深度优先遍历 pG.BFS(); // 广度优先遍历 &#125;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>图</tag>
        <tag>DFS</tag>
        <tag>BFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（31）：在线最优化求解（online Optimization）]]></title>
    <url>%2F2017%2F07%2F26%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8831%EF%BC%89%EF%BC%9A%E5%9C%A8%E7%BA%BF%E6%9C%80%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3%EF%BC%88online%20Optimization%EF%BC%89%2F</url>
    <content type="text"><![CDATA[最优化求解问题可能是我们在工作中遇到的最多的一类问题了：从已有的数据中提炼出最适合的模型参数，从而对未知的数据进行预测。当我们面对高维高数据量的场景时，常见的批量处理的方式已经显得力不从心，需要有在线处理的方法来解决此类问题。本文以模型的稀疏性作为主线，逐一介绍几个在线最优化求解算法，并进行推导，力求讲清楚算法的来龙去脉，以及不同算法之间的区别和联系，达到融会贯通。在各个算法原理介绍之后，都给出该算法的工程实现伪代码，可以用于实际工作的参考。 一、动机与目的在实际工作中，无论是工程师、项目经理、产品同学都会经常讨论一类话题：“从线上对比的效果来看，某某特征或因素对xx产品的最终效果有很大的影响。”这类话题本质上说的是通过已有的数据反映出某些特定的因素对结果有很强的正（或负）相关性。而如何定量计算这种相关性？如何得到一套模型参数能够使得效果达到最好？这就是最优化计算要做的事情。 举一类典型点的例子：在推荐和广告计算中，我们经常会需要对某些值进行预测，例如在一条推荐或广告在曝光之前先预测用户是否会点击（CTR预估），或者是否会由此产生某些转换（RPM预估）。这列问题可以表示为：针对一个输入$X=[x_1,x_2,……x_N]\in R^N$，通过某个函数$h(X)$计算（预测）输出$y\in R$。根据$y$值为连续的还是离散的，预测问题被划分成回归问题（Regression）和分类问题（Classification）。而利用已有的样本数据$\{(X_j,y_j) | j=1,2,3….,M\}$训练$h(X)$的过程往往转换成一个最优化求解的过程。 无论是线性回归（Linear Regression）、逻辑回归（Logistic Regression）、支持向量机（SVM）、深度学习（Deep Learning）中，最优化求解都是基本的步骤。常见的梯度下降、牛顿法、拟牛顿法等属于批量处理的方法（Batch），每次更新都需要对已经训练过的样本重新训练一遍。当我们面对高维高数据量的时候，批量处理的方式就显得笨重和不够高效，因此需要在线处理的方法来解决相同的问题。关于在线最优化问题（Online Optimization）的论文比较多，注意查找阅读费时费力，那么本文就以高维高数据量的应用场景中比较看重的稀疏性作为主线，来介绍一些在线最优化的方法。 本文的预期读者大概有如下几类： 具有很深的机器学习经验和背景的高阶人员：就拿这篇文章当做一个关于在线最优化算法的回归材料好了，如有错误和不足欢迎指正。 具有一定机器学习经验的中级读者：可以将本文作为一个理论资料进行阅读，略过“预备知识”部分，直接进入主题，将之前对于在线最优化算法的理解串联起来，希望对将来的工作提供帮助。 对机器学习有认识但是时间经验较少的初级读者：从预备知识看起，注意理解相关概念和方法，从而达到融会贯通的目的。 仅仅对算法的工程实现感兴趣的读者：大致浏览下预备知识的2.3节，了解我们要讨论什么，然后直奔各算法的算法逻辑（伪代码），照着实现就好了。 高富帅和白富美：只需要知道本文讨论的是一堆好用的求最优解的方法，可以用于分类回归预测的一系列问题，然后吩咐工程师去实践就好了。还可以拿这篇文章嘲笑机器学习的屌丝：看你们弄些啥，累死累活的，挣那么几个钢镚。 二、预备知识2.1 凸函数如果$f(X)$是定义在N为向量空间上的实值函数，对于在$f(X)$的定义域$C$上的任意两个点$X_1$和$X_2$，以及任意$[0,1]$之间的值$t$都有： f(tX_1+(1-t)X_2)≤tf(X_1)+(1-t)f(X_2) \ \ \ ∀𝑋_1,𝑋_2∈𝐶, \ \ 0≤𝑡≤1则$f(X)$是严格凸函数（Strict Convex），如图一所示，（a）为严格凸函数，（b）为凸函数。 2.2 拉格朗日乘数法及KKT条件通常我们需要求解的最优化问题有如下三类： 1.无约束优化问题： X=arg\underset{X}{min} \ f(X)含义是求解$X$，使得目标函数$f(X)$最小；2.有等式约束的优化问题： X=arg\underset{X}{min} \ f(X) \\ s.t. \ \ h_k(X)=0; \ \ k=1,2...n含义是在n个等式约束$h_k(X)=0$的条件下，求解$X$，使得目标函数$f(X)$最小；3.有不等式约束的优化问题： X=arg\underset{X}{min} \ f(X) \\ s.t. \ \ h_k(X)=0; \ \ k=1,2...n \\ g_l(X)≤0；l=1,2.....m含义是在n个等式约束$h_k(X)$以及m各不等式约束$g_l(X)$的条件下，求解$X$使得目标函数$f(X)$最小。 针对无约束最优化问题，通常做法就是对$f(X)$求导，并令$\frac{\partial}{\partial X}f(X)=0$，求解可以得到最优值。如果$f(X)$为凸函数，则可以保证结果为全局最优解。 针对有等式约束的最优化问题，采用拉格朗日乘数法（Lagrange Multiplier）进行求解：通过拉格朗日系数$A=[a_1,a_2…a_n]^T\in R^n$把等式约束和目标函数组合成为一个式子，对该式子进行最优化求解： X=arg\underset{X}{min}\ [f(X)+A^TH(X)]其中,$H(X)=[h_1(X),h_2(X)…h_n(X)]^T\in R^n$，相当于将有等式约束的最优化问题转换成了无约束最优化求解问题，解决方法依旧是对$f(X)+A^TH(X)$的各个参数$(X,A)$求偏导，并令其为0，联立等式求解。 针对有不等式约束的最优化问题，常用的方法是KKT条件（Karush-Kuhn-Tucker Conditions）：同样地，把所有的不等式约束、等是约束和目标函数全部写为一个式子： L(X,A,B)=f(X)+A^TH(X)+B^TG(X)KKT条件是说最优值必须满足以下条件： \frac{\partial}{\partial X}L(X,A,B)=0 \\ H(X)=0 \\ B^TG(X)=0其中，$B=[b_1,b_2…b_m]^T\in R^m，G(X)=[g_1(X),g_2(X)…g_m(X)]^T \in R^m$。KKT条件是求解最优值$X^*$的必要条件，要使其成为充分必要条件，还需要$f(X)$为凸函数才行。 在KKT条件中，$B^TG(X)=0$这个条件最有趣，因为$g_l(X)≤0$,如果要满足这个等式，需要$b_l=0$或者$g_l(X)=0$。在我们后面的推导中会用到这个性质。 2.3 从Batch到Online我们面对的最优化问题都是无约束的最优化问题（有约束最优化问题可以利用拉格朗日乘数法或KKT条件转换成无约束最优化问题），因此我们通常可以将它们描述成： W=𝑎𝑟𝑔\underset{w}{𝑚𝑖𝑛}\ l(𝑊, 𝑍) \\ 𝑍 = \{(𝑋_𝑗,𝑦_𝑗)|𝑗 = 1,2,...𝑀\}\\𝑦_𝑗 =h(𝑊,𝑋_𝑗)这里$Z$为观测样本集合（训练集）；$X_j$是第$j$条阉割版的特征向量；$y_j=h(W,X_j)$为预测值；$h(W,X_j)$为特征向量到预测值的映射函数；$l (𝑊,𝑍) $为最优化求解的目标函数，也称作损失函数，损失函数通常可以分解为各样本损失函数的累加，即$l(𝑊,𝑍) =\sum_{j=1}^{M}l(𝑊,𝑍_𝑗)$;$W$为特征权重，也就是我们需要求解的参数。以线性回归和逻辑回归为例，它们的映射函数和损失函数分别为： 在2.1中我们给出了无约束最优化问题解析解的求法。而在我们实际的数值计算中，通常做法是随机给定一个初始的$W^{(0)}$，通过迭代，在每次迭代中计算损失函数在当前$W^{(t)}$的下降方向，并更新$W$,直到损失函数稳定在最小的点。例如著名的梯度下降法（Gradient Descent）就是通过计算损失函数的在当前$W^{(t)}$处的梯度（Gradient），以梯度$\nabla_Wl(W^{(t)},Z)$的反方向作为下降方向更新$W$，如果损失函数是一个非平滑的凸函数（Non-Smooth Convex），在不可导处用次梯度（Subgradient）方向的反方向作为下降方向。算法如下：GD是一种批量处理的方式（Batch），每次更新$W$的时候都要扫描所有的样本以计算一个全局的梯度$\nabla_Wl(W,Z)$ 考虑另一种权重更新策略：在算法2中，每次迭代仅仅根据单个样本更新权重$W$，这种算法称作随机梯度下降（SGD，Stochastic Gradient Descent） 与 GD 每次扫所有的样本以计算一个全局的梯度相比，SGD 则每次只针对一 个观测到的样本进行更新。通常情况下，SGD 能够比 GD“更快”地令𝑊逼近最优值。当样 本数𝑀特别大的时候，SGD 的优势更加明显，并且由于 SGD 针对观测到的“一条”样本更新 𝑊，很适合进行增量计算，实现梯度下降的 Online 模式(OGD, Online Gradient Descent)。 2.4 正则化正则化(Regularization)的意义本质上是为了避免训练得到的模型过度拟合(overfitting) 训练数据。我们用图 2 来说明什么是过拟合。图 2 是一个局部加权线性回归(Locally weighted linear regression)的训练结果，当学习度为 1 时，相当于进行线性回归，这时候模型与训练样本以及实际曲线拟合得都不够好，模型处于 欠拟合(underfitting)状态;当学习度逐渐增加到 4 的过程中，模型逐渐与实际曲线吻合; 随着学习度继续增加，越来越多的样本直接落到模型曲线上(模型拟合训练数据)，但是模 型却与实际曲线相差越来越大，出现了过拟合。过拟合体现出来的现象就是特征权重𝑊的各个维度的绝对值非常大:一些大正数，一些大负数。这种模型虽然能够很好匹配样本(如图 2 中 Degree = 20 的情况)，但是对新样本做 预测的时候会使得预测值与真实值相差很远。 为了避免过拟合的情况，我们通常在损失函数的基础上加一个关于特征权重𝑊的限制， 限制它的模不要太大，如果用𝜓(𝑊)表示特征权重𝑊的一种求模计算，那么(2-3-1)转换成: 为了避免过拟合的情况，我们通常在损失函数的基础上加一个关于特征权重$𝑊$的限制， 限制它的模不要太大，如果用$𝜓(𝑊)$表示特征权重$𝑊$的一种求模计算，那么(2-3-1)转换成: W=arg \underset {W}{min }l(W,Z) \\ s.t. 𝜓(𝑊)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>online learning</tag>
        <tag>FTRL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（12）：排序]]></title>
    <url>%2F2017%2F07%2F26%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8812%EF%BC%89%EF%BC%9A%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[经典排序算法在面试中占有很大的比重，也是基础，在这里整理并用Java实现了几大经典排序算法，包括冒泡排序、插入排序、选择排序、希尔排序、归并排序、快速排序、堆排序、同排序。我们默认将一个无序数列排序成由小到大。 一、冒泡排序（Bubble Sort）1.1 思想冒泡排序(bubble sort)：每个回合都从第一个元素开始和它后面的元素比较，如果比它后面的元素更大的话就交换，一直重复，直到这个元素到了它能到达的位置。每次遍历都将剩下的元素中最大的那个放到了序列的“最后”(除去了前面已经排好的那些元素)。注意检测是否已经完成了排序，如果已完成就可以退出了。 1.2 代码123456789101112131415161718192021public class Demo&#123; public static void BubbleSort(int[] arr)&#123; int temp = 0; for (int i = arr.length - 1; i &gt; 0; --i) &#123; // 每次需要排序的长度 for (int j = 0; j &lt; i; ++j) &#123; // 从第一个元素到第i个元素 if (arr[j] &gt; arr[j + 1]) &#123; temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125; &#125;//loop j &#125;//loop i &#125;// method bubbleSort public static void main(String[] args)&#123; int[] arr = &#123;10,30,20,60,40,50&#125;; Demo.BubbleSort(arr); for(int i:arr)&#123; System.out.print(i+","); &#125; &#125;&#125; 1.3 时空复杂度冒泡排序的关键字比较次数与数据元素的初始状态无关。第一趟的比较次数为n-1，第i趟的比较次数为n-i，第n-1趟（最后一趟）的比较次数为1，因此冒泡排序总的比较次数为$n(n-1)/2$ 冒泡排序的数据元素移动次数与序列的初始状态有关。在最好的情况下，移动次数为0次；在最坏的情况下，移动次数为$n(n-1)/2$ 冒泡排序的时间复杂度为$O(n^2)$。冒泡排序不需要辅助存储单元，其空间复杂度为$O(1)$。如果关键字相等，则冒泡排序不交换数据元素，他是一种稳定的排序方法。 时间复杂度：最好$O(n)$；最坏$O(n^2)$；平均$O(n^2)$空间复杂度：$O(1)$稳定性：稳定 二、选择排序（Selection Sort）2.1 思想每个回合都选择出剩下的元素中最大的那个，选择的方法是首先默认第一元素是最大的，如果后面的元素比它大的话，那就更新剩下的最大的元素值，找到剩下元素中最大的之后将它放入到合适的位置就行了。和冒泡排序类似，只是找剩下的元素中最大的方式不同而已。 &lt;/center&gt; 2.2 代码1234567891011121314151617181920212223242526public class Demo&#123; public static void selectionSort(int[] arr) &#123; int temp, min = 0; for (int index = 0; index &lt; arr.length - 1; ++index) &#123; min = index; // 循环查找最小值 for (int j = index + 1; j &lt; arr.length; ++j) &#123; if (arr[min] &gt; arr[j]) &#123; min = j; &#125; &#125; if (min != index) &#123; temp = arr[index]; arr[index] = arr[min]; arr[min] = temp; &#125; &#125; &#125; public static void main(String[] args)&#123; int[] arr = &#123;10,30,20,60,40,50&#125;; Demo.selectionSort(arr); for(int i:arr)&#123; System.out.print(i+","); &#125; &#125;&#125; 2.3 时空复杂度对具有$n$个数据元素的序列进行排序时，选择排序需要进行$n-1$趟选择。进行第$i$趟选择时，后面已经有$i-1$个数据元素排好序，第$i$趟从剩下的$n-i+1$个数据元素中选择一个关键字最大的数据元素，并将它与第$i$个数据元素交换，这样即可使后面的$i$个数据元素排好序。 选择排序的关键字比较次数与序列的初始状态无关。对n个数据元素进行排序时，第一趟的比较次数为$n-1$，第$i$趟的比较次数是$n-1$次，第$n-1$趟（最后一趟）的比较次数是1次。因此，总的比较次数为$n(n-1)/2$ 选择排序每一趟都可能移动一次数据元素，其总的移动次数与序列的初始状态有关。当序列已经排好序时，元素的移动次数为0。当每一趟都需要移动数据元素时，总的移动次数为$n-1$ 选择排序的时间复杂度为$O(n^2)$。选择排序不需要辅助的存储单元，其空间复杂度为$O(1)$。选择排序在排序过程中需要在不相邻的数据元素之间进行交换，它是一种不稳定的排序方法。 时间复杂度：$O(n^2)$空间复杂度：$O(1)$稳定性：不稳定 三、插入排序（Insertion Sort）3.1 思想对具有$n$个数据元素的序列进行排序时，插入排序需要进行$n-1$趟插入。进行第$j（1≤j≤n-1）$趟插入时，前面已经有$j$个元素排好序了，第$j$趟将$a_{j+1}$插入到已经排好序的序列中，这样即可使前面的$j+1$个数据排好序。 动图展示 3.2 代码1234567891011121314151617181920public class Demo&#123; public static void insertionSort(int[] arr)&#123; for (int i=1; i&lt;arr.length; ++i)&#123; int value = arr[i]; int position=i; while (position&gt;0 &amp;&amp; arr[position-1]&gt;value)&#123; arr[position] = arr[position-1]; position--; &#125; arr[position] = value; &#125;//loop i &#125; public static void main(String[] args)&#123; int[] arr = &#123;10,30,20,60,40,50&#125;; Demo.insertionSort(arr); for(int i:arr)&#123; System.out.print(i+","); &#125; &#125;&#125; 3.3 时空复杂度直接插入排序关键字比较次数和数据元素移动次数与数据元素的初始状态有关。在最好的情况下，待排序的序列是已经排好序的，每一趟插入，只需要比较一次就可以确定待插入的数据元素的位置，需要移动两次数据元素。因此总的关键字比较次数为$n-1$,总的数据元素移动次数为$2(n-1)$ 在最坏的情况下，待排序的序列是反序的，每一趟中，待插入的数据元素需要与前面已排序序列的每一个数据元素进行比较，移动次数等于比较次数。因此，总的比较次数和移动次数都是$n(n-1)/2$ 直接插入排序的时间复杂度为$O(n^2)$。直接插入排序需要一个单元的辅助存储单元，空间复杂度为$O(1)$。直接插入排序只在相邻的数据元素之间进行交换，它是一种稳定的排序方法。 最好情况$O(n)$；最坏情况$O(n^2)$；平均时间复杂度为：$O(n^2)$空间复杂度：$O(1)$稳定性：稳定 四、希尔排序（Shell Sort）4.1 思想希尔排序，也称递减增量排序算法，实质是分组插入排序。由 Donald Shell 于1959年提出。希尔排序是非稳定排序算法。 希尔排序的基本思想是：将数组列在一个表中并对列分别进行插入排序，重复这过程，不过每次用更长的列（步长更长了，列数更少了）来进行。最后整个表就只有一列了。将数组转换至表是为了更好地理解这算法，算法本身还是使用数组进行排序。 例如，假设有这样一组数[ 13 14 94 33 82 25 59 94 65 23 45 27 73 25 39 10 ]，如果我们以步长为5开始进行排序，我们可以通过将这列表放在有5列的表中来更好地描述算法，这样他们就应该看起来是这样： 123413 14 94 33 8225 59 94 65 2345 27 73 25 3910 然后我们对每列进行排序： 123410 14 73 25 2313 27 94 33 3925 59 94 65 8245 将上述四行数字，依序接在一起时我们得到：[ 10 14 73 25 23 13 27 94 33 39 25 59 94 65 82 45 ]。这时10已经移至正确位置了，然后再以3为步长进行排序： 12345610 14 7325 23 1327 94 3339 25 5994 65 8245 排序之后变为： 12345610 14 1325 23 3327 25 5939 65 7345 94 8294 最后以1步长进行排序（此时就是简单的插入排序了）。 4.2 代码1234567891011121314151617181920public class Demo &#123; public static void main(String[] args) &#123; int[] data = new int[] &#123;10,30,20,60,40,50&#125;; shellSort(data); for(int i:data) &#123; System.out.println(i); &#125; &#125; public static void shellSort(int[] arr)&#123; int temp; for (int delta = arr.length/2; delta&gt;=1; delta/=2)&#123; //对每个增量进行一次排序 for (int i=delta; i&lt;arr.length; i++)&#123; for (int j=i; j&gt;=delta &amp;&amp; arr[j]&lt;arr[j-delta]; j-=delta)&#123; //注意每个地方增量和差值都是delta temp = arr[j-delta]; arr[j-delta] = arr[j]; arr[j] = temp; &#125; &#125;//loop i &#125;//loop delta &#125;&#125; 上面源码的步长的选择是从n/2开始，每次再减半，直至为0。步长的选择直接决定了希尔排序的复杂度。在维基百科上有对于步长串行的详细介绍。 五、归并排序（Merge Sort）5.1 思想典型的是二路合并排序，将原始数据集分成两部分(不一定能够均分)，分别对它们进行排序，然后将排序后的子数据集进行合并，这是典型的分治法策略。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Demo &#123; public static void main(String[] args) &#123; int[] data = new int[] &#123;10,30,20,60,40,50&#125;; mergesort(data); for(int i:data) &#123; System.out.println(i); &#125; &#125; public static void mergesort(int[] arr)&#123; sort(arr, 0, arr.length-1); &#125; private static void sort(int[] a, int left, int right)&#123; //当left==right的时，已经不需要再划分了 if (left&lt;right)&#123; int middle = (left+right)/2; sort(a, left, middle); //左子数组 sort(a, middle+1, right); //右子数组 merge(a, left, middle, right); //合并两个子数组 &#125; &#125; // 合并两个有序子序列 arr[left, ..., middle] 和 arr[middle+1, ..., right]。temp是辅助数组。 private static void merge(int arr[], int left, int middle, int right)&#123; int[] temp = new int[right - left + 1]; int i=left; int j=middle+1; int k=0; //将记录由小到大地放进temp数组 while ( i&lt;=middle &amp;&amp; j&lt;=right)&#123; if (arr[i] &lt;=arr[j])&#123; temp[k++] = arr[i++]; &#125; else&#123; temp[k++] = arr[j++]; &#125; &#125; while (i &lt;=middle)&#123; temp[k++] = arr[i++]; &#125; while ( j&lt;=right)&#123; temp[k++] = arr[j++]; &#125; //把数据复制回原数组 for (i=0; i&lt;k; ++i)&#123; arr[left+i] = temp[i]; &#125; &#125; &#125; 5.3 时空复杂度在归并排序中，进行一趟归并需要的关键字比较次数和数据元素移动次数最多为$n$，需要归并的趟数$log_{2}n$，故归并排序的时间复杂度为$O(nlog_{2}n)$。归并排序需要长度等于序列长度为$n$的辅助存储单元，故归并排序的空间复杂度为$O(n)$。归并排序是稳定的排序算法。 时间复杂度：$O(nlog_{2}n)$空间复杂度：$O(n)$稳定性：稳定 六、快速排序（Quick Sort）6.1 思想快速排序是图灵奖得主C.R.A Hoare于1960年提出的一种划分交换排序。它采用了一种分治的策略，通常称其为分治法（Divide-and-Conquer Method） 分治法的基本思想是：将原问题分解为若干个规模更小但结构与原问题相似的子问题。递归地解这些子问题，然后将这些子问题组合为原问题的解。 利用分治法可将快速排序分为三步： 从数列中挑出一个元素作为“基准”（pivot）。 分区过程，将比基准数大的放到右边，小于或等于它的数都放到左边。这个操作称为“分区操作”，分区操作结束后，基准元素所处的位置就是最终排序后它的位置 再对“基准”左右两边的子集不断重复第一步和第二步，直到所有子集只剩下一个元素为止。 6.2 代码12345678910111213141516171819202122232425262728293031public class Demo &#123; public static void main(String[] args) &#123; int[] data = new int[] &#123;10,30,20,60,40,50&#125;; quickSort(data); for(int i:data) &#123; System.out.println(i); &#125; &#125; public static void quickSort(int[] arr)&#123; qsort(arr, 0, arr.length-1); &#125; private static void qsort(int[] arr, int left, int right)&#123; if (left &lt; right)&#123; int pivot=partition(arr, left, right); //将数组分为两部分 qsort(arr, left, pivot-1); //递归排序左子数组 qsort(arr, pivot+1, right); //递归排序右子数组 &#125; &#125; private static int partition(int[] arr, int left, int right)&#123; int pivot = arr[left]; //基准记录 while (left&lt;right)&#123; while (left&lt;right &amp;&amp; arr[right]&gt;=pivot) --right; arr[left]=arr[right]; //交换比基准小的记录到左端 while (left&lt;right &amp;&amp; arr[left]&lt;=pivot) ++left; arr[right] = arr[left]; //交换比基准大的记录到右端 &#125; //扫描完成，基准到位 arr[left] = pivot; //返回的是基准的位置 return left; &#125;&#125; 6.3 时空复杂度 时间复杂度：最好$O(nlog_2n)$；平均$O(nlog_2n)$，最坏：$O(n^2)$空间复杂度：$O(log_2n)$稳定性：不稳定 七、堆排序（Heap Sort）7.1 思想先上一张堆排序动画演示图： 堆排序在 top K 问题中使用比较频繁。堆排序是采用二叉堆的数据结构来实现的，虽然实质上还是一维数组。堆（二叉堆）可以视为一棵完全的二叉树，完全二叉树的一个“优秀”的性质是，除了最底层之外，每一层都是满的，这使得堆可以利用数组来表示（普通的一般的二叉树通常用链表作为基本容器表示），每一个结点对应数组中的一个元素。 如下图，是一个堆和数组的相互关系 对于给定的某个结点的下标 i，可以很容易的计算出这个结点的父结点、孩子结点的下标： Parent(i) = floor(i/2)，i 的父节点下标 Left(i) = 2i，i 的左子节点下标 Right(i) = 2i + 1，i 的右子节点下标 二叉堆具有以下性质： 父节点的键值总是大于或等于（小于或等于）任何一个子节点的键值。 每个节点的左右子树都是一个二叉堆（都是最大堆或最小堆）。 二叉堆一般分为两种：最大堆和最小堆。 最大堆的定义如下： 最大堆中的最大元素值出现在根结点（堆顶） 堆中每个父节点的元素值都大于等于其孩子结点（如果存在） 最小堆的定义如下： 最小堆中的最小元素值出现在根结点（堆顶） 堆中每个父节点的元素值都小于等于其孩子结点（如果存在） 【提问】堆是怎么调整的，介绍大顶堆和小顶堆； 堆排序的原理如下： 步骤： 构造最大堆（Build_Max_Heap）：若数组下标范围为0~n，考虑到单独一个元素是大根堆，则从下标n/2开始的元素均为大根堆。于是只要从n/2-1开始，向前依次构造大根堆，这样就能保证，构造到某个节点时，它的左右子树都已经是大根堆。 堆排序（HeapSort）：由于堆是用数组模拟的。得到一个大根堆后，数组内部并不是有序的。因此需要将堆化数组有序化。思想是移除根节点，并做最大堆调整的递归运算。第一次将heap[0]与heap[n-1]交换，再对heap[0…n-2]做最大堆调整。第二次将heap[0]与heap[n-2]交换，再对heap[0…n-3]做最大堆调整。重复该操作直至heap[0]和heap[1]交换。由于每次都是将最大的数并入到后面的有序区间，故操作完后整个数组就是有序的了。 最大堆调整（Max_Heapify）：该方法是提供给上述两个过程调用的。目的是将堆的末端子节点作调整，使得子节点永远小于父节点 。 7.2 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class Demo &#123; public static void main(String[] args) &#123; int[] arr = &#123; 50, 10, 90, 30, 70, 40, 80, 60, 20 &#125;; // 堆排序 heapSort(arr); for (int i = 0; i &lt; arr.length; i++) &#123; System.out.print(arr[i] + " "); &#125; &#125; /** * 堆排序 */ private static void heapSort(int[] arr) &#123; // 将待排序的序列构建成一个大顶堆 for (int i = arr.length / 2; i &gt;= 0; i--)&#123; heapAdjust(arr, i, arr.length); &#125; // 逐步将每个最大值的根节点与末尾元素交换，并且再调整二叉树，使其成为大顶堆 for (int i = arr.length - 1; i &gt; 0; i--) &#123; swap(arr, 0, i); // 将堆顶记录和当前未经排序子序列的最后一个记录交换 heapAdjust(arr, 0, i); // 交换之后，需要重新检查堆是否符合大顶堆，不符合则要调整 &#125; &#125; /** * 构建堆的过程 * @param arr 需要排序的数组 * @param i 需要构建堆的根节点的序号 * @param n 数组的长度 */ private static void heapAdjust(int[] arr, int i, int n) &#123; int child; int father; for (father = arr[i]; leftChild(i) &lt; n; i = child) &#123; child = leftChild(i); // 如果左子树小于右子树，则需要比较右子树和父节点 if (child != n - 1 &amp;&amp; arr[child] &lt; arr[child + 1]) &#123; child++; // 序号增1，指向右子树 &#125; // 如果父节点小于孩子结点，则需要交换 if (father &lt; arr[child]) &#123; arr[i] = arr[child]; &#125; else &#123; break; // 大顶堆结构未被破坏，不需要调整 &#125; &#125; arr[i] = father; &#125; // 获取到左孩子结点 private static int leftChild(int i) &#123; return 2 * i + 1; &#125; // 交换元素位置 private static void swap(int[] arr, int index1, int index2) &#123; int tmp = arr[index1]; arr[index1] = arr[index2]; arr[index2] = tmp; &#125; &#125; 7.3 时空复杂度堆排序在建立堆和调整堆的过程中会产生比较大的开销，在元素少的时候并不适用。但是，在元素比较多的情况下，还是不错的一个选择。尤其是在解决诸如“前n大的数”一类问题时，几乎是首选算法。 八、桶排序桶排序 (Bucket sort)或所谓的箱排序的原理是将数组分到有限数量的桶子里，然后对每个桶子再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排序），最后将各个桶中的数据有序的合并起来。 排序过程： 假设待排序的一组数统一的分布在一个范围中，并将这一范围划分成几个子范围，也就是桶 将待排序的一组数，分档规入这些子桶，并将桶中的数据进行排序 将各个桶中的数据有序的合并起来 设有数组 array = [29, 25, 3, 49, 9, 37, 21, 43]，那么数组中最大数为 49，先设置 5 个桶，那么每个桶可存放数的范围为：0~9、10~19、20~29、30~39、40~49，然后分别将这些数放人自己所属的桶，如下图： 然后，分别对每个桶里面的数进行排序，或者在将数放入桶的同时用插入排序进行排序。最后，将各个桶中的数据有序的合并起来，如下图： Data Structure Visualizations 提供了一个桶排序的分步动画演示。 九、各种排序方法的时空复杂度 9.1 时间复杂度 ＂快些以$O(nlog_2n)$的速度归队＂ 即快，希，归，堆都是$O(nlog_2n)$，其他都是$O(n^2)$，基数排序例外，是$O(d(n+rd))$ 9.2 空间复杂度 快排$O(log_2n)$ 归并$O(n)$ 基数$O(r_d)$ 其他$O(1)$ 9.3 稳定性 ＂心情不稳定，快些找一堆朋友聊天吧＂ 即不稳定的有：快，希，堆 9.4 其他性质 直接插入排序，初始基本有序情况下，是$O(n)$ 冒泡排序，初始基本有序情况下，是$O(n)$ 快排在初始状态越差的情况下算法效果越好． 堆排序适合记录数量比较大的时候，从n个记录中选择k个记录． 经过一趟排序，元素可以在它最终的位置的有：交换类的（冒泡，快排），选择类的（简单选择，堆） 比较次数与初始序列无关的是：简单选择与折半插入 排序趟数与原始序列有关的是：交换类的（冒泡和快排） 参考资料维基百科：希尔排序，快速排序，归并排序，堆排序排序算法可视化]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（11）：哈希表]]></title>
    <url>%2F2017%2F07%2F25%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8811%EF%BC%89%EF%BC%9A%E5%93%88%E5%B8%8C%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[一、哈希表的基本概念哈希表（Hash Table）是一种特殊的数据结构，它最大的特点就是可以快速实现查找、插入和删除。因为它独有的特点，Hash表经常被用来解决大数据问题，也因此被广大的程序员所青睐。 我们知道，数组的最大特点就是：寻址容易，插入和删除困难；而链表正好相反，寻址困难，而插入和删除操作容易。那么如果能够结合两者的优点，做出一种寻址、插入和删除操作同样快速容易的数据结构，那该有多好。这就是哈希表创建的基本思想，而实际上哈希表也实现了这样的一个“夙愿”，哈希表就是这样一个集查找、插入和删除操作于一身的数据结构。 哈希表（Hash Table）：也叫散列表，是根据关键码值（key-value）而直接进行访问的数据结构，也就是我们常用到的map。 哈希函数：也称为是散列函数，是Hash表的映射函数，它可以把任意长度的输入变换成固定长度的输出，该输出就是哈希值。哈希函数能使对一个数据序列的访问过程变得更加迅速有效，通过哈希函数，数据元素能够被很快的进行定位。 哈希表和哈希函数的标准定义：若关键字为k，则其值存放在h(k)的存储位置上。由此，不需比较便可直接取得所查记录。称这个对应关系f为哈希函数，按这个思想建立的表为哈希表。 设所有可能出现的关键字集合记为U(简称全集)。实际发生(即实际存储)的关键字集合记为K（|K|比|U|小得多）。 散列方法是使用函数h将U映射到表T[0..m-1]的下标上（m=O(|U|)）。这样以U中关键字为自变量，以h为函数的运算结果就是相应结点的存储地址。从而达到在O(1)时间内就可完成查找。其中： h：U→{0，1，2，…，m-1} ，通常称h为哈希函数(Hash Function)。哈希函数h的作用是压缩待处理的下标范围，使待处理的|U|个值减少到m个值，从而降低空间开销。 T为哈希表(Hash Table)。 $h(K_i)(K_i∈U)$是关键字为Ki结点存储地址(亦称散列值或散列地址)。 将结点按其关键字的哈希地址存储到哈希表中的过程称为散列(Hashing) 设计出一个简单、均匀、存储利用率高的散列函数是散列技术中最关键的问题。但是，一般散列函数都面临着冲突的问题。两个不同的关键字，由于散列函数值相同，因而被映射到同一表位置上。该现象称为冲突(Collision)或碰撞。发生冲突的两个关键字称为该散列函数的同义词(Synonym)。 最理想的解决冲突的方法是安全避免冲突。要做到这一点必须满足两个条件：其一是$|U|≤m$，其二是选择合适的散列函数。这只适用于|U|较小，且关键字均事先已知的情况，此时经过精心设计散列函数h有可能完全避免冲突。 但通常情况下，h是一个压缩映像。虽然$|K|≤m$，但$|U|&gt;m$，故无论怎样设计h，也不可能完全避免冲突。因此，只能在设计h时尽可能使冲突最少。同时还需要确定解决冲突的方法，使发生冲突的同义词能够存储到表中。 冲突的频繁程度除了与h相关外，还与表的填满程度相关。设m和n分别表示表长和表中填入的结点数，则将α=n/m定义为散列表的装填因子(Load Factor)。α越大，表越满，冲突的机会也越大。通常取α≤1。 二、哈希表的实现方法我们之前说了，哈希表是一个集查找、插入和删除操作于一身的数据结构。那这么完美的数据结构到底是怎么实现的呢？哈希表有很多种不同的实现方法，为了实现哈希表的创建，这些所有的方法都离不开两个问题——“定址”和“解决冲突”。 在这里，我们通过详细地介绍哈希表最常用的方法——取余法（定值）+拉链法（解决冲突），来一起窥探一下哈希表强大的优点。 取余法大家一定不会感觉陌生，就是我们经常说的取余数的操作。 拉链法是什么，“拉链”说白了就是“链表数组”。我这么一解释，大家更晕了，啥是“链表数组”啊？为了更好地解释“链表数组”，我们用下图进行解释：图中的主干部分是一个顺序存储结构数组，但是有的数组元素为空，有的对应一个值，有的对应的是一个链表，这就是“链表数组”。比如数组0的位置对应一个链表，链表有两个元素“496”和“896”，这说明元素“496”和“896”有着同样的Hash地址，这就是我们上边介绍的“冲突”或者“碰撞”。但是“链表数组”的存储方式很好地解决了Hash表中的冲突问题，发生冲突的元素会被存在一个对应Hash地址指向的链表中。实际上，“链表数组”就是一个指针数组，每一个指针指向一个链表的头结点，链表可能为空，也可能不为空。 说完这些，大家肯定已经理解了“链表数组”的概念，那我们就一起看看Hash表是如何根据“取余法+拉链法”构建的吧。 将所有关键字为同义词的结点链接在同一个链表中。若选定的散列表长度为m，则可将散列表定义为一个由m个头指针组成的指针数组$T[0..m-1]$。凡是散列地址为i的结点，均插入到以$T[i]$为头指针的单链表中。T中各分量的初值均应为空指针。在拉链法中，装填因子$α$可以大于1，但一般均取$α≤1$。 举例说明拉链法的执行过程，设有一组关键字为(26，36，41，38，44，15，68，12，6，51)，用取余法构造散列函数，初始情况如下图所示：最终结果如下图所示：理解了Hash表的创建，那根据建立的Hash表进行查找就很容易被理解了。 查找操作，如果理解了插入和删除，查找操作就比较简单了，令待查找的关键字是x，也可分为几种情况： 1）x所属的Hash地址未被占用，即不存在与x相同的Hash地址关键字，当然也不存在x了； 2）x所属的Hash地址被占用了，但它所存的关键不属于这个Hash地址，与1）相同，不存在与x相同Hash地址的关键字； 3）x所属的Hash地址被占用了，且它所存的关键属于这个Hash地址，即存在与x相同sHash地址的关键字，只是不知这个关键字是不是x，需要进一步查找。 由此可见，Hash表插入、删除和插入操作的效率都相当的高。 思考一个问题：如果关键字是字符串怎么办？我们怎么根据字符串建立Hash表？ 通常都是将元素的key转换为数字进行散列，如果key本身就是整数，那么散列函数可以采用keymod tablesize（要保证tablesize是质数）。而在实际工作中经常用字符串作为关键字，例如身姓名、职位等等。这个时候需要设计一个好的散列函数进程处理关键字为字符串的元素。参考《数据结构与算法分析》第5章，有以下几种处理方法： 方法1：将字符串的所有的字符的ASCII码值进行相加，将所得和作为元素的关键字。此方法的缺点是不能有效的分布元素，例如假设关键字是有8个字母构成的字符串，散列表的长度为10007。字母最大的ASCII码为127，按照方法1可得到关键字对应的最大数值为127×8=1016，也就是说通过散列函数映射时只能映射到散列表的槽0-1016之间，这样导致大部分槽没有用到，分布不均匀，从而效率低下。 方法2：假设关键字至少有三个字母构成，散列函数只是取前三个字母进行散列。该方法只是取字符串的前三个字符的ASCII码进行散列，最大的得到的数值是2851，如果散列的长度为10007，那么只有28%的空间被用到，大部分空间没有用到。因此如果散列表太大，就不太适用。 方法3：借助Horner’s 规则，构造一个质数（通常是37）的多项式，（非常的巧妙，不知道为何是37）。计算公式为:$key[keysize-i-1]*37^i, 0≤i&lt;keysize$求和。该方法存在的问题是如果字符串关键字比较长，散列函数的计算过程就变长，有可能导致计算的hashVal溢出。针对这种情况可以采取字符串的部分字符进行计算，例如计算偶数或者奇数位的字符。 三、哈希表定址与解决冲突3.1 哈希表“定址的方法”其实常用的“定址”的手法有“五种”： 直接定址法：很容易理解，key=Value+C；这个“C”是常量。Value+C其实就是一个简单的哈希函数。 除法取余法：key=value%C 数字分析法：这种蛮有意思，比如有一组value1=112233，value2=112633，value3=119033，针对这样的数我们分析数中间两个数比较波动，其他数不变。那么我们取key的值就可以是key1=22,key2=26,key3=90。 平方取中法 折叠法：举个例子，比如value=135790，要求key是2位数的散列值。那么我们将value变为13+57+90=160，然后去掉高位“1”,此时key=60，哈哈，这就是他们的哈希关系，这样做的目的就是key与每一位value都相关，来做到“散列地址”尽可能分散的目地。 影响哈希查找效率的一个重要因素是哈希函数本身。当两个不同的数据元素的哈希值相同时，就会发生冲突。为减少发生冲突的可能性，哈希函数应该将数据尽可能分散地映射到哈希表的每一个表项中。 3.2 哈希表“解决冲突”的方法Hash表解决冲突的方法主要有以下几种： 链接地址法：将哈希值相同的数据元素存放在一个链表中，在查找哈希表的过程中，当查找到这个链表时，必须采用线性查找方法。 开放定址法：如果两个数据元素的哈希值相同，则在哈希表中为后插入的数据元素另外选择一个表项。当程序查找哈希表时，如果没有在第一个对应的哈希表项中找到符合查找要求的数据元素，程序就会继续往后查找，直到找到一个符合查找要求的数据元素，或者遇到一个空的表项。线性探测带来的最大问题就是冲突的堆积，你把别人预定的坑占了，别人也就要像你一样去找坑。改进的办法有二次方探测法和随机数探测法。开放地址法包括线性探测、二次探测以及双重散列等方法。其中线性探测法示意图如下：散列过程如下图所示： 再散列函数法：发生冲突时就换一个散列函数计算，总会有一个可以把冲突解决掉，它能够使得关键字不产生聚集，但相应地增加了计算的时间。 公共溢出区法：其实就是为所有的冲突，额外开辟一块存储空间。如果相对基本表而言，冲突的数据很少的时候，使用这种方法比较合适。 3.3 哈希表“定址”和“解决冲突”之间的权衡虽然哈希表是在关键字和存储位置之间建立了对应关系，但是由于冲突的发生，哈希表的查找仍然是一个和关键字比较的过程，不过哈希表平均查找长度比顺序查找要小得多，比二分查找也小。 查找过程中需和给定值进行比较的关键字个数取决于下列三个因素：哈希函数、处理冲突的方法和哈希表的装填因子。 哈希函数的”好坏”首先影响出现冲突的频繁程度，但如果哈希函数是均匀的，则一般不考虑它对平均查找长度的影响。 对同一组关键字，设定相同的哈希函数，但使用不同的冲突处理方法，会得到不同的哈希表，它们的平均查找长度也不同。 一般情况下，处理冲突方法相同的哈希表，其平均查找长度依赖于哈希表的装填因子α。显然，α越小，产生冲突的机会就越大；但α过小，空间的浪费就过多。通过选择一个合适的装填因子α，可以将平均查找长度限定在一个范围内。 总而言之，哈希表“定址”和“解决冲突”之间的权衡决定了哈希表的性能。 四、实现Hashmap假设我们要设计的是一个用来保存某大学所有在校学生个人信息的数据表。因为在校学生数量也不是特别巨大(8W?)，每个学生的学号是唯一的,因此，我们可以简单的应用直接定址法，声明一个10W大小的数组，每个学生的学号作为主键。然后每次要添加或者查找学生，只需要根据需要去操作即可。 但是，显然这样做是很脑残的。这样做系统的可拓展性和复用性就非常差了，比如有一天人数超过10W了？如果是用来保存别的数据呢？或者我只需要保存20条记录呢？声明大小为10W的数组显然是太浪费了的。 如果我们是用来保存大数据量（比如银行的用户数，4大的用户数都应该有3-5亿了吧？），这时候我们计算出来的HashCode就很可能会有冲突了， 我们的系统应该有“处理冲突”的能力，此处我们通过挂链法“处理冲突”。 如果我们的数据量非常巨大，并且还持续在增加，如果我们仅仅只是通过挂链法来处理冲突，可能我们的链上挂了上万个数据后，这个时候再通过静态搜索来查找链表，显然性能也是非常低的。所以我们的系统应该还能实现自动扩容，当容量达到某比例后，即自动扩容，使装载因子保存在一个固定的水平上。 综上所述，我们对这个Hash容器的基本要求应该有如下几点： 满足Hash表的查找要求 能支持从小数据量到大数据量的自动转变（自动扩容） 使用挂链法解决冲突 代码详见hashmap源码剖析]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>哈希表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（10）：查找]]></title>
    <url>%2F2017%2F07%2F24%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8810%EF%BC%89%EF%BC%9A%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[一、基本概念查找（Search）就是根据给定的某个值，在查找表中确定一个其关键字等于给定值的数据元素（或记录）。在计算机应用中，查找是常用的基本运算，例如编译程序中符号表的查找。先说明几个概念： 查找表（Search Table）：由同一类型的数据元素（或记录）构成的集合关键字（Key）：数据元素中某个数据项的值，又称为键值。主键（Primary Key）：可唯一地标识某个数据元素或记录的关键字。 平均查找长度（Average Search Length，ASL）：需和指定key进行比较的关键字的个数的期望值，称为查找算法在查找成功时的平均查找长度。 对于含有n个数据元素的查找表，查找成功的平均查找长度为：$ASL = P_i*C_i$的和。 $P_i$：查找表中第i个数据元素的概率。 $C_i$：找到第i个数据元素时已经比较过的次数。 查找表按照操作方式可分为： 静态查找表（Static Search Table）:只做查找操作的查找表。它的主要操作是： 查询某个“特定的”数据元素是否在表中 检索某个“特定的”数据元素和各种属性 动态查找表（Dynamic Search Table）：在查找的同时进行插入或删除等操作： 查找时插入数据 查找时删除数据 按照查找表是否有序分为无序查找和有序查找： 无序查找：被查找数列有序无序均可； 有序查找：被查找数列必须为有序数列。 二、无序表查找说明：顺序查找适合于存储结构为顺序存储或链接存储的线性表。 基本思想：顺序查找也称为线形查找，属于无序查找算法。从数据结构线形表的一端开始，顺序扫描，依次将扫描到的结点关键字与给定值k相比较，若相等则表示查找成功；若扫描结束仍没有找到关键字等于k的结点，表示查找失败。 算法分析：最好情况是在第一个位置就找到了，此为O(1)；最坏情况是在最后一个位置才找到，此为O(n)；所以平均查找次数为$(n+1)/2$，最终时间复杂度为$O(n)$ 1234567891011121314# 最基础的遍历无序列表的查找算法# 时间复杂度O(n)def sequential_search(lis, key): length = len(lis) for i in range(length): if lis[i] == key: return i else: return Falseif __name__ == '__main__': LIST = [1, 5, 8, 123, 22, 54, 7, 99, 300, 222] result = sequential_search(LIST, 123) print(result) 三、有序表查找查找表中的数据必须按照某个主键进行某种排序！ 3.1 二分查找说明：元素必须是有序的，如果是无序的则要先进行排序操作。基本思想：也称为折半查找，属于有序查找算法。用给定值k先与中间结点的关键字比较，中间结点把线性表分成两个子表，若相等则查找成功；若不相等，再根据k与该中间节点关键字的比较结果确定下一步查找哪个字表，这样递归进行，知道查找到或查找结束发现表中没有这样的结点。 复杂度分析：最坏情况下,关键字比较次数为$log_2(n+1)$，且期望时间复杂度为$O(log_2n)$ 注意：折半查找的前提条件是需要有序表顺序存储，对于静态查找表，一次排序后不再发生变化，折半查找能得到不错的效率。但对于需要频繁执行插入或删除操作的数据集来说，维护有序的排序会带来不小的工作量，那就不建议使用。 12345678910111213141516171819202122232425# 针对有序查找表的二分查找算法# 时间复杂度O(log(n))def binary_search(list, key): low = 0 high = len(list) - 1 time = 0 while low &lt;= high: time += 1 mid = int((low + high) / 2) if key &lt; list[mid]: high = mid - 1 elif key &gt; list[mid]: low = mid + 1 else: # 打印折半的次数 print("times: %s" % time) return mid print("times: %s" % time) return -1if __name__ == '__main__': LIST = [1, 5, 7, 8, 22, 54, 99, 123, 200, 222, 444] result = binary_search(LIST, 99) print(result) 3.2 插值查找在介绍插值查找之前，首先考虑一个新问题，为什么上述算法一定要是折半，而不是折四分之一或者折更多呢？ 打个比方，在英文字典里面查“apple”，你下意识翻开字典是翻前面的书页还是后面的书页呢？如果再让你查“zoo”，你又怎么查？很显然，这里绝对不会是从中农间开始查起，而是有一定目的的往前或往后翻。 同样的，比如要在取值范围1~10000之间100个元素从小到大均匀分布的数组中查找5，我们自然会从数组下标较小的开始查找。 经过上面的分析，折半查找这种查找方式，不是自适应的（也就是说是傻瓜式的）。二分查找中查找点计算如下： mid=(low+high)/2即 mid=low+(high-low)/2通过类比，我们可以将查找的点改进为如下： mid=low+\frac{key -list[low]}{list[high]-list [low]}\times (high-low)也就是将上述的比例参数1/2改进为自适应的，根据关键字在整个有序表中所处的位置，让mid值的变化更靠近关键字key，这样也就间接地减少了比较次数。 基本思想：基于二分查找算法，将查找点的选择改进为自适应选择，可以提高查找效率。当然，插值查找也属于有序查找。 注意：对于表长较大，而关键字分布又比较均匀的查找表来说，插值查找算法的平均性能比折半查找要好的多。反之，数组中如果分布非常不均匀，那么插值查找未必是很合适的选择。 复杂度分析：查找成功或者失败的时间复杂度均为$O(log2(log2n))$。 123456789101112131415161718192021222324252627# 插值查找算法# 时间复杂度O(log(n))def binary_search(lis, key): low = 0 high = len(lis) - 1 time = 0 while low &lt;= high: time += 1 # 计算mid值是插值算法的核心代码 mid = low + int((high - low) * (key - lis[low])/(lis[high] - lis[low])) print("mid=%s, low=%s, high=%s" % (mid, low, high)) if key &lt; lis[mid]: high = mid - 1 elif key &gt; lis[mid]: low = mid + 1 else: # 打印查找的次数 print("times: %s" % time) return mid print("times: %s" % time) return -1if __name__ == '__main__': LIST = [1, 5, 7, 8, 22, 54, 99, 123, 200, 222, 444] result = binary_search(LIST, 444) print(result) 3.3 斐波那契查找在介绍斐波那契查找算法之前，我们先介绍一下和它很紧密相连并且大家都熟知的一个概念——黄金分割。 黄金比例又称为黄金分割，是指事物各部分间一定的数学比例关系，即将整体一分为二，较大部分与较小部分之比等于整体与较大部分之比，其比值约为1：0.618。 0.618倍公认为是最具有审美意义的比例数字，这个数值的作用不仅仅体现在诸如绘画、雕塑、音乐、建筑等艺术领域，而且在管理、工程设计等方面有着不可忽视的作用。因此被称为黄金分割。 大家记不记得斐波那契数列：1，1，2，3，5，8，13，21，34，55，89……（从第三个数开始，后面每一个数都是前两个数的和）。然后我们会发现，随着斐波那契数列的递增，前后两个数的比值会越来越接近0.618，利用这个特性，我们就可以将黄金比例运用到查找技术中。 基本思想：也是二分查找的一种提升算法，通过运用黄金比例的概念在数列中选择查找点进行查找，提高查找效率。同样地，斐波那契查找也属于一种有序查找算法。 相对于折半查找，一般将待比较的key值与第mid=（low+high）/2位置的元素进行比较，比较结果分为三种情况： 相等，mid位置的元素即为所求 ,low=mid+1 &lt;,high=mid-1 斐波那契查找和折半查找很相似，它是根据斐波那契序列的特点对有序表进行分割的。他要求开始表中记录的个数为某个斐波那契小1，即$n=F(k)-1$ 开始将k值与第F(k-1)位置的记录进行比较（即mid=low+F(k-1)-1），比较结果也分为三种 相等，mid位置的元素即为所求 ,low=mid+1,k-=2：说明，low=high+1说明待查找的元素在[mid+1,high]范围内，k-=2说明范围[mid,high]内的元素个数为$n-(F(k-1))=Fk-1-F(k-1)=Fk-F(k-1)-1=f(k-2)-1$个，所以可以递归地应用斐波那契查找。 &lt;,high=mid-1,k-=1:说明，low=mid+1，说明待查找的元素在[mid+1,high]范围内，k-=1说明范围[low,mid-1]内的元素个数为F(k-1)-1个，所以可以递归的应用斐波那契查找 复杂度分析：最坏情况下，时间复杂度为$O(log_2n)$，且其期望复杂度也为$O(log_2n)$。就平均性能，要优于二分查找。但是在最坏情况下，比如这里如果key为1，则始终处于左侧半区查找，此时其效率要低于二分查找。 总结：二分查找的mid运算是加法与除法，插值查找则是复杂的四则运算，而斐波那契查找只是最简单的加减运算。在海量数据的查找中，这种细微的差别可能会影响最终的查找效率。因此，三种有序表的查找方法本质上是分割点的选择不同，各有优劣，应根据实际情况进行选择。 四、线性索引查找对于海量的无序数据，为了提高查找速度，一般会为其构造索引表。索引就是把一个关键字与他相对应的记录进行关联的过程。 一个索引由若干个索引项构成，每个索引项至少包含关键字和其对应的记录在存储器中的位置等信息。 索引按照结构可以分为：线性索引、树形索引和多级索引。线性索引：将索引项的集合通过线性结构来组织，也叫索引表 线性索引可分为：稠密索引、分块索引和倒排索引。 稠密索引： 指的是在线性索引中，为数据集合中的每个记录都建立一个索引项。这其实就相当于给无序的集合，建立了一张有序的线性表，其索引项一定是按照关键码进行有序的排列。这也相当于把查找过程中需要的排序工作给提前做了。 分块索引： 分块查找又称索引顺序查找，是顺序查找的一种改进方法。 算法思想：将n个数据元素“按块有序”划分为m块（m&lt;n）。每一块中的结点不必有序，但块与块之间必须“按块有序”；即第一块中人艺元素的关键字都必须小于第2块中任一元素的关键字；而第二块中任一元素又都必须小于第三块中的任一元素，…… 算法流程：首先选取各块中的最大关键字构成一个索引表；查找分为两个部分：先对索引表进行二分查找或顺序查找，以确定待查记录在哪一块中；然后，在已确定的块中用顺序法进行查找。 这其实是有序查找和无序查找的一种中间状态或者说妥协状态。因为数据量过大，建立完整的稠密索引耗时耗力，占用资源过多；但如果不做任何排序或索引，那么遍历的查找也无法接受，只能这种，做一定程度的排序或索引。分块索引的效率比遍历查找的$O(n)$要高一些，但与二分查找的$O(logn)$还是要差不少。 倒排索引：不是由记录来确定属性值，而是由属性值来确定记录的位置，这种被称为倒排索引。其中记录号表存储具有相同关键字的所有记录的地址或引用（可以是指向记录的指针或该记录的主关键字）。倒排索引是最基础的搜索引擎技术。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>查找算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（30）：Scikit-Learn总结]]></title>
    <url>%2F2017%2F07%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8830%EF%BC%89%EF%BC%9AScikit-Learn%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[http://ff120.github.io/2017/05/14/机器学习专题/机器学习_Scikit-Learn使用技巧/Scikit-learn是一个很受欢迎的机器学习方面的python工具包，它定义的一些范式和处理流程影响深远，所以，了解这个工具包对于机器学习算法的整个流程会有一个整体的了解。它已经实现了很多方法帮助我们便捷的处理数据，例如，划分数据集为训练集和验证集，交叉验证，数据预处理，归一化等等。 一、性能评价指标12345678910111213141516# 计算均方误差from sklearn import metricsimport numpy as nprmse = np.sqrt(metrics.mean_squared_error(y_test,y_pred))# 计算准确率acc = metrics.accuracy_score(y_test,y_pred)#混淆矩阵cm = metrics.confusion_matrix(y_test,y_pred)# classification_reportcr = metrics.classification_report(y_true,y_pred)# ROC AUC 曲线from sklearn.metrics import roc_curve,auc 二、数据集划分123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from sklearn import cross_validationX_train,X_test,y_train,y_test = cross_validation.train_test_split(X,y,test_size=0.3,random_state=0)# K折from sklearn.cross_validation import KFoldkf = KFold(n_samples, n_folds=2)for train, test in kf: print(&quot;%s %s&quot; % (train, test))# 保证不同的类别之间的均衡，这里需要用到标签labelsfrom sklearn.cross_validation import StratifiedKFoldlabels = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]skf = StratifiedKFold(labels, 3)for train, test in skf: print(&quot;%s %s&quot; % (train, test)) # 留一交叉验证from sklearn.cross_validation import LeaveOneOutloo = LeaveOneOut(n_samples)for train, test in loo: print(&quot;%s %s&quot; % (train, test))# 留P交叉验证from sklearn.cross_validation import LeavePOutlpo = LeavePOut(n_samples, p=2)for train, test in lpo: print(&quot;%s %s&quot; % (train, test)) # 按照额外提供的标签留一交叉验证,常用的情况是按照时间序列from sklearn.cross_validation import LeaveOneLabelOutlabels = [1, 1,1, 2, 2]lolo = LeaveOneLabelOut(labels)for train, test in lolo: print(&quot;%s %s&quot; % (train, test)) # 按照额外提供的标签留P交叉验证from sklearn.cross_validation import LeavePLabelOutlabels = [1, 1, 2, 2, 3, 3,3]lplo = LeavePLabelOut(labels, p=2)for train, test in lplo: print(&quot;%s %s&quot; % (train, test))# 随机分组from sklearn.cross_validation import ShuffleSplitss = ShuffleSplit(16, n_iter=3, test_size=0.25,random_state=0)for train_index, test_index in ss: print(&quot;%s %s&quot; % (train_index, test_index))# 考虑类别均衡的随机分组from sklearn.cross_validation import StratifiedShuffleSplitimport numpy as npX = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])y = np.array([0, 0, 1, 1])sss = StratifiedShuffleSplit(y, 3, test_size=0.5, random_state=0)for train, test in sss: print(&quot;%s %s&quot; % (train, test)) 三、特征选择12345678910111213141516171819202122232425262728293031323334353637383940# 基于方差的特征选择from sklearn import feature_selectionvt = feature_selection.VarianceThreshold(threshold=&apos;&apos;)vt.fit(X_train)X_train_transformed = vt.transform(X_train)X_test_transformed = vt.transform(X_test)# 按照某种排序规则 选择前K个特征# 除了使用系统定义好的函数f_classif，还可以自己定义函数sk = SelectKBest(feature_selection.f_classif,k=100)sk.fit(X_train,y_train)X_train_transformed = sk.transform(X_train)X_test_transformed = sk.transform(X_test)# 递归特征消除rfecv = RFECV(estimator=svc, step=step, cv=StratifiedKFold(y, n_folds = n_folds),scoring=&apos;accuracy&apos;)rfecv.fit(X_train, y_train)X_train_transformed = rfecv.transform(X_train)X_test_transformed = rfecv.transform(y_train)# 使用L1做特征选择from sklearn.svm import LinearSVClsvc = LinearSVC(C=1, penalty=&quot;l1&quot;, dual=False)lsvc.fit(X_train,y_train)X_train_transformed = lsvc.transform(X_train)X_test_transformed = lsvc.transform(y_train)# 基于树的特征选择from sklearn.ensemble import ExtraTreesClassifieretc = ExtraTreesClassifier()etc.fit(X_train, y_train)X_train_transformed = etc.transform(X_train)X_test_transformed = etc.transform(X_test)# 基于线性判别分析做特征选择from sklearn.discriminant_analysis import LinearDiscriminantAnalysislda = LinearDiscriminantAnalysis(solver=&apos;lsqr&apos;,shrinkage=&apos;auto&apos;)lda.fit(X_train, y_train)X_train_transformed = lda.transform(X_train)X_test_transformed = lda.transform(X_test) 基于方差的特征选择 123456789101112from sklearn.feature_selection import VarianceThresholdX = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]sel = VarianceThreshold(threshold=(.8 * (1 - .8)))X2 = sel.fit_transform(X)X2Out[5]: array([[0, 1], [1, 0], [0, 0], [1, 1], [1, 0], [1, 1]]) 可以看到，方差小于0.16的只有第一维特征，所以X2保留下来的是原来的第二维和第三维特征。这应该是最简单的特征选择方法了：假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用，而且实际当中，一般不太会有95%以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。 Univariate feature selection （单变量特征选择） 主要使用统计的方法计算各个统计值，再根据一定的阈值筛选出符合要求的特征，去掉不符合要求的特征。 主要的统计方法有 F值分类： f_classif 值回归：f_regression 卡方统计：chi2 (适用于非负特征值和稀疏特征值) 主要的选择策略 选择排名前K的特征：SelectKbest 选择前百分之几的特征：SelectPercentile SelectFpr：Select features based on a false positive rate test. SelectFdr：Select features based on an estimated false discovery rate. SelectFwe：Select features based on family-wise error rate. GenericUnivariateSelect：Univariate feature selector with configurable mode. 其中 false positive rate：FP / (FP + TP) 假设类别为0，1；记0为negative,1为positive, FPR就是实际的类别是0，但是分类器错误的预测为1的个数 与 分类器预测的类别为1的样本的总数（包括正确的预测为1和错误的预测为1） 的比值。 estimated false discovery rate: 错误的拒绝原假设的概率； family-wise error rate: 至少有一个检验犯第一类错误的概率；假设检验的两类错误： &gt; - 第一类错误：原假设是正确的，但是却被拒绝了。(用α表示） &gt; - 第二类错误：原假设是错误的，但是却被接受了。(用β表示)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Scikit-Learn</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（9）：Trie树]]></title>
    <url>%2F2017%2F07%2F23%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%889%EF%BC%89%EF%BC%9ATrie%E6%A0%91%2F</url>
    <content type="text"><![CDATA[Trie树是一种非常重要的数据结构，它在信息检索，字符串匹配等领域有广泛的应用，同时，它也是很多算法和复杂数据结构的基础，如后缀树，AC自动机等，因此，掌握Trie树这种数据结构，对于一名IT人员，显得非常基础且必要！ 一、什么是Trie树Trie树，又叫字典树、前缀树（Prefix Tree）、单词查找树或键树，是一种多叉树结构。 字典树（Trie）可以保存一些字符串-&gt;值的对应关系。基本上，它跟 Java 的 HashMap 功能相同，都是 key-value 映射，只不过 Trie 的 key 只能是字符串。是一种哈希树的变种 如下图： 上图是一棵Trie树，表示了关键字集合{“a”,”to”,”tea”,”ted”,”ten”,”i”,”in”,”inn”}。从上图可以归纳出Trie树的基本性质： 根节点不包含字符，除节结点外的每一个节点都包含一个字符。 从根节点到某一个节点，路径上经过的字符连接起来，为该节点对应的字符串。 每个节点的所有子节点包含的字符互不相同。 通常在实现的时候，会在节点结构中设置一个标志，用来标记该结点处是否构成一个单词（关键字）。 可以看出，Trie树的关键字一般都是字符串，而且Trie树把每个关键字保存在一条路径上，而不是一个结点中。另外，两个有公共前缀的关键字，在Trie树中前缀部分的路径相同，所以Trie树又叫做前缀树（Prefix Tree）。 二、Trie的优缺点Trie的核心思想是空间换时间。利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。 2.1 优点Trie的强大之处就在于它的时间复杂度，插入和查询的效率很高，都为$O(K)$，其中 $K$ 是待插入/查询的字符串的长度，而与Trie中保存了多少个元素无关。 关于查询，会有人说 hash 表时间复杂度是$O(1)$不是更快？但是，哈希搜索的效率通常取决于 hash 函数的好坏，若一个坏的 hash 函数导致很多的冲突，效率并不一定比Trie树高。 而Trie树中不同的关键字就不会产生冲突。它只有在允许一个关键字关联多个值的情况下才有类似hash碰撞发生。 此外，Trie树不用求 hash 值，对短字符串有更快的速度。因为通常，求hash值也是需要遍历字符串的。 Trie树可以对关键字按字典序排序。 2.2 缺点当然，当 hash 函数很好时，Trie树的查找效率会低于哈希搜索。 其次因为Trie的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。所以它的空间消耗比较大。 三、Trie树的应用典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：最大限度地减少无谓的字符串比较，查询效率比哈希表高。 3.1 字符串检索给出N个单词组成的熟词表，以及一篇全用小写英文书写的文章，请你按最早出现的顺序写出所有不在熟词表中的生词。 检索/查询功能是Trie树最原始的功能。给定一组字符串，查找某个字符串是否出现过，思路就是从根节点开始一个一个字符进行比较： 如果沿路比较，发现不同的字符，则表示该字符串在集合中不存在。 如果所有的字符全部比较完并且全部相同，还需判断最后一个节点的标志位（标记该节点是否代表一个关键字）。 12345struct trie_node&#123; bool isKey; // 标记该节点是否代表一个关键字 trie_node *children[26]; // 各个子节点 &#125;; 3.2 词频统计Trie树常被搜索引擎系统用于文本词频统计 。 12345struct trie_node&#123; int count; // 记录该节点代表的单词的个数 trie_node *children[26]; // 各个子节点 &#125;; 思路：为了实现词频统计，我们修改了节点结构，用一个整型变量count来计数。对每一个关键字执行插入操作，若已存在，计数加1，若不存在，插入后count置1。 3.3 排序Trie树可以对大量字符串按字典序进行排序，思路也很简单：给定N个互不相同的仅由一个单词构成的英文名，让你将他们按字典序从小到大输出。用字典树进行排序，采用数组的方式创建字典树，这棵树的每个结点的所有儿子很显然地按照其字母大小排序。对这棵树进行先序遍历即可。 3.4 前缀匹配例如：找出一个字符串集合中所有以ab开头的字符串。我们只需要用所有字符串构造一个trie树，然后输出以$a-&gt;b-&gt;$开头的路径上的关键字即可。 trie树前缀匹配常用于搜索提示。如当输入一个网址，可以自动搜索出可能的选择。当没有完全匹配的搜索结果，可以返回前缀最相似的可能 3.5 最长公共前缀查找一组字符串的最长公共前缀，只需要将这组字符串构建成Trie树，然后从跟节点开始遍历，直到出现多个节点为止（即出现分叉）。 举例说明：给出N 个小写英文字母串，以及Q 个询问，即询问某两个串的最长公共前缀的长度是多少？ 解决方案：首先对所有的串建立其对应的字母树。此时发现，对于两个串的最长公共前缀的长度即它们所在结点的公共祖先个数，于是，问题就转化为了离线（Offline）的最近公共祖先（Least Common Ancestor，简称LCA）问题。而最近公共祖先问题同样是一个经典问题，可以用下面几种方法： 利用并查集（Disjoint Set），可以采用采用经典的Tarjan 算法； 求出字母树的欧拉序列（Euler Sequence ）后，就可以转为经典的最小值查询（Range Minimum Query，简称RMQ）问题了； 关于并查集，Tarjan算法，RMQ问题，网上有很多资料。 3.6 作为辅助结构如后缀树，AC自动机等。 3.7 应用实例 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。 之前在此文：海量数据处理面试题集锦与Bit-map详解中给出的参考答案：用trie树统计每个词出现的次数，时间复杂度是$O(nle)$（le表示单词的平均长度），然后是找出出现最频繁的前10个词。也可以用堆来实现（具体的操作可参考第三章、寻找最小的k个数），时间复杂度是$O(nlg10)$。所以总的时间复杂度，是$O(nle)$与$O(nlg10)$中较大的哪一个。 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。 1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？ 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。 寻找热门查询：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录，这些查询串的重复读比较高，虽然总数是1千万，但是如果去除重复和，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就越热门。请你统计最热门的10个查询串，要求使用的内存不能超过1G。 (1) 请描述你解决这个问题的思路； (2) 请给出主要的处理流程，算法，以及算法的复杂度。 四、Trie树的实现Trie树的插入、删除、查找的操作都是一样的，只需要简单的对树进行一遍遍历即可，时间复杂度：O（n）（n是字符串的长度）。 trie树每一层的节点数是$26^i$级别的。所以为了节省空间，对于Tried树的实现可以使用数组和链表两种方式。空间的花费，不会超过单词数×单词长度。 数组：由于我们知道一个Tried树节点的子节点的数量是固定26个（针对不同情况会不同，比如兼容数字，则是36等），所以可以使用固定长度的数组来保存节点的子节点 优点：在对子节点进行查找时速度快 缺点：浪费空间，不管子节点有多少个，总是需要分配26个空间 链表：使用链表的话我们需要在每个子节点中保存其兄弟节点的链接，当我们在一个节点的子节点中查找是否存在一个字符时，需要先找到其子节点，然后顺着子节点的链表从左往右进行遍历 优点：节省空间，有多少个子节点就占用多少空间，不会造成空间浪费 缺点：对子节点进行查找相对较慢，需要进行链表遍历，同时实现也较数组麻烦 java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169import java.util.ArrayList;import java.util.List;/** * 单词查找树 */class Trie &#123; /** 单词查找树根节点，根节点为一个空的节点 */ private Vertex root = new Vertex(); /** 单词查找树的节点(内部类) */ private class Vertex &#123; /** 单词出现次数统计 */ int wordCount; /** 以某个前缀开头的单词，它的出现次数 */ int prefixCount; /** 子节点用数组表示 */ Vertex[] vertexs = new Vertex[26]; /** * 树节点的构造函数 */ public Vertex() &#123; wordCount = 0; prefixCount = 0; &#125; &#125; /** * 单词查找树构造函数 */ public Trie() &#123; &#125; /** * 向单词查找树添加一个新单词 * * @param word * 单词 */ public void addWord(String word) &#123; addWord(root, word.toLowerCase()); &#125; /** * 向单词查找树添加一个新单词 * * @param root * 单词查找树节点 * @param word * 单词 */ private void addWord(Vertex vertex, String word) &#123; if (word.length() == 0) &#123; vertex.wordCount++; &#125; else if (word.length() &gt; 0) &#123; vertex.prefixCount++; char c = word.charAt(0); int index = c - 'a'; if (null == vertex.vertexs[index]) &#123; vertex.vertexs[index] = new Vertex(); &#125; addWord(vertex.vertexs[index], word.substring(1)); &#125; &#125; /** * 统计某个单词出现次数 * * @param word * 单词 * @return 出现次数 */ public int countWord(String word) &#123; return countWord(root, word); &#125; /** * 统计某个单词出现次数 * * @param root * 单词查找树节点 * @param word * 单词 * @return 出现次数 */ private int countWord(Vertex vertex, String word) &#123; if (word.length() == 0) &#123; return vertex.wordCount; &#125; else &#123; char c = word.charAt(0); int index = c - 'a'; if (null == vertex.vertexs[index]) &#123; return 0; &#125; else &#123; return countWord(vertex.vertexs[index], word.substring(1)); &#125; &#125; &#125; /** * 统计以某个前缀开始的单词，它的出现次数 * * @param word * 前缀 * @return 出现次数 */ public int countPrefix(String word) &#123; return countPrefix(root, word); &#125; /** * 统计以某个前缀开始的单词，它的出现次数(前缀本身不算在内) * * @param root * 单词查找树节点 * @param word * 前缀 * @return 出现次数 */ private int countPrefix(Vertex vertex, String prefixSegment) &#123; if (prefixSegment.length() == 0) &#123; return vertex.prefixCount; &#125; else &#123; char c = prefixSegment.charAt(0); int index = c - 'a'; if (null == vertex.vertexs[index]) &#123; return 0; &#125; else &#123; return countPrefix(vertex.vertexs[index], prefixSegment.substring(1)); &#125; &#125; &#125; /** * 调用深度递归算法得到所有单词 * @return 单词集合 */ public List&lt;String&gt; listAllWords() &#123; List&lt;String&gt; allWords = new ArrayList&lt;String&gt;(); return depthSearchWords(allWords, root, ""); &#125; /** * 递归生成所有单词 * @param allWords 单词集合 * @param vertex 单词查找树的节点 * @param wordSegment 单词片段 * @return 单词集合 */ private List&lt;String&gt; depthSearchWords(List&lt;String&gt; allWords, Vertex vertex, String wordSegment) &#123; Vertex[] vertexs = vertex.vertexs; for (int i = 0; i &lt; vertexs.length; i++) &#123; if (null != vertexs[i]) &#123; if (vertexs[i].wordCount &gt; 0) &#123; allWords.add(wordSegment + (char)(i + 'a')); if(vertexs[i].prefixCount &gt; 0)&#123; depthSearchWords(allWords, vertexs[i], wordSegment + (char)(i + 'a')); &#125; &#125; else &#123; depthSearchWords(allWords, vertexs[i], wordSegment + (char)(i + 'a')); &#125; &#125; &#125; return allWords; &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; Trie trie = new Trie(); trie.addWord("abc"); trie.addWord("abcd"); trie.addWord("abcde"); trie.addWord("abcdef"); System.out.println(trie.countPrefix("abc")); System.out.println(trie.countWord("abc")); System.out.println(trie.listAllWords()); &#125;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>Trie树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（29）：Sparsity and Some Basics of L1 Regularization]]></title>
    <url>%2F2017%2F07%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8829%EF%BC%89%EF%BC%9ASparsity%20and%20Some%20Basics%20of%20L1%20Regularization%2F</url>
    <content type="text"><![CDATA[转载自pluskid的个人博客Sparsity 是当今机器学习领域中的一个重要话题。John Lafferty 和 Larry Wasserman 在 2006 年的一篇评论中提到： Some current challenges … are high dimensional data, sparsity, semi-supervised learning, the relation between computation and risk, and structured prediction. —John Lafferty and Larry Wasserman. Challenges in statistical machine learning. Statistica Sinica. Volume 16, Number 2, pp. 307-323, 2006. Sparsity 的最重要的“客户”大概要属 high dimensional data 了吧。现在的机器学习问题中，具有非常高维度的数据随处可见。例如，在文档或图片分类中常用的 bag of words 模型里，如果词典的大小是一百万，那么每个文档将由一百万维的向量来表示。高维度带来的的一个问题就是计算量：在一百万维的空间中，即使计算向量的内积这样的基本操作也会是非常费力的。不过，如果向量是稀疏的的话（事实上在 bag of words 模型中文档向量通常都是非常稀疏的），例如两个向量分别只有$L_1$和$L_2$个非零元素，那么计算内积可以只使用$min(L_1,L_2 )$次乘法完成。因此稀疏性对于解决高维度数据的计算量问题是非常有效的。 当然高维度带来的问题不止是在计算量上。例如在许多生物相关的问题中，数据的维度非常高，但是由于收集数据需要昂贵的实验，因此可用的训练数据却相当少，这样的问题通常称为“small , large problem”—我们一般用 表示数据点的个数，用 表示变量的个数，即数据维度。当$p&gt;&gt;n$的时候，不做任何其他假设或者限制的话，学习问题基本上是没法进行的。因为如果用上所有变量的话，$p$越大，通常会导致模型越复杂，但是反过来$n$又很小，于是就会出现很严重的 overfitting 问题。例如，最简单的线性回归模型： f(X)=\sum_{j=1}^p使用square loss来学习的话，就变成最小化如下的问题： J(w)=\frac{1}{n}\sum_{i=1}^n(y_i-f(x_i))^2=\frac{1}{n}||y-Xw||^2这里$X=(x_1,······,x_n)^T \in R^{n \times p}$是数据矩阵，而$y=(y_1,······,y_n)^T$是由标签组成的列向量。该问题具有解析解$\hat{w}=(X^TX)^{-1}X^Ty$然而，如果$p&gt;n$的话，矩阵$X^TX$将会不是满秩的，而这个解也没法算出来。捉着更确切地说，将会有无穷多个解。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解随机选一个的话，很可能并不是很好地解，总而言之，我们过拟合了。 解决 overfitting 最常用的办法就是 regularization ，例如著名的 ridge regression 就是添加一个 $\ell_2 $regularizer ： J_R(w)=\frac{1}{n}||y-Xw||^2+\lambda ||w||^2直观地看，添加这个regularizer会使得模型的解偏向于norm较小的w。从凸优化的角度来说，最小化上面这个$J(w)$等价于如下问题： \underset{w}{min}\frac{1}{n}||y-Xw||^2其中$C$和$\lambda$对应的是个常数。也就是说，也就是说，我们通过限制$w$的norm的大小实现了对模型空间的限制，从而在一定程度上（取决于$\lambda$的大小）避免了overfitting。不过ridge regression并不具有产生稀疏解的能力，得到的系数$w$仍然需要数据中的所有特征才能计算预测结果，从计算量上来说并没有得到改观。 不过，特别是在像生物或者医学等通常需要和人交互的领域，稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。比如说，一个病如果依赖于 5 个变量的话，将会更易于医生理解、描述和总结规律，但是如果依赖于 5000 个变量的话，基本上就超出人肉可处理的范围了。 在这里引入稀疏性的方法是用$L_1$regularization 代替 $L_2$regularization，得到如下的目标函数： J_L(w)=\frac{1}{n}||y-Xw||^2+\lambda ||w||_1该问题通常被称为LASSO（least absolute shrinkage and selection operator）。LASSO 仍然是一个 convex optimization 问题，不过不再具有解析解。它的优良性质是能产生稀疏性，导致$w$中许多项变成零。 可是，为什么它能产生稀疏性呢？这也是一直让我挺感兴趣的一个问题，事实上在之前申请学校的时候一次电话面试中我也被问到了这个问题。我当时的回答是背后的理论我并不是很清楚，但是我知道一个直观上的理解。下面我们就先来看一下这个直观上的理解。 首先，和 ridge regression 类似，上面形式的 LASSO 问题也等价于如下形式： \underset {w}{min }\frac{1}{n}||y-Xw||^2, \ \ \ s.t. ||w||_1≤C也就是说，我们将模型空间限制在$w$的一个 $\ell_1$-ball中。为了便于可视化，我们考虑两维的情况，在 $(w^1,w^2)$平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为$C$的一个 norm ball 。等高线与 norm ball 首次相交的地方就是最优解。如图 所示： 可以看到，$\ell_1-ball$与$\ell_2-ball$的不同就在于他和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置为产生稀疏性，例如图中的相交点就有$w^1=0$ ，而更高维的时候（想象一下三维的 $\ell_1 $-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。 相比之下，$\ell_2 $-ball 就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么$\ell_1$ regularization 能产生稀疏性，而$\ell_2$regularization 不行的原因了。 不过，如果只限于 intuitive 的解释的话，就不那么好玩了，但是背后完整的理论又不是那么容易能够搞清楚的，既然这次的标题是 Basics ，我们就先来看一个简单的特殊情况好了。 接下来我们考虑 orthonormal design 的情况：$\frac{1}{n}X^TX=I$，然后看看LASSO的解具体是什么样子。注意orthonormal design 实际上是要求特征之间相互正交。这可以通过对数据进行 PCA以及模长 normalize 来实现。 注意到LASSO 的目标函数是 convex 的，根据 KKT 条件，在最优解的地方要求 gradient 。不过这里有一点小问题：$\ell_1$ -norm 不是光滑的，不存在 gradient ，所以我们需要用一点 subgradient 的东西。 定义：（subgradient，subdifferential）.对于在$p$维欧式空间中的凸开子集$U$上定义的实值函数$f:U \rightarrow R$，一个向量$p$维向量$v$称为$f$在一点$x_0 \in U$处的subgradient，如果对于任意$x \in U$，满足 f(x)-f(x_0)≥v·(x-x_0)由在点$x_0$处的所有subgradient所组成的集合称为$x_0$处的subdifferential，记为$\partial f(x_0)$ 注意 subgradient 和 subdifferential 只是对凸函数定义的。例如一维的情况，$f(x)=|x|$，在$x=0$处的subdifferential 就是$[-1,+1]$这个区间（集合）。注意在$f$的 gradient 存在的点，subdifferential 将是由 gradient 构成的一个单点集合。这样就将 gradient 的概念加以推广了。这个推广有一个很好的性质。 性质（CONDITION GLOBAL MINIMIZER）.点$x_0$是凸函数$f$的一个全局最小值点，当且仅当$0\in \partial f(x_0)$ 证明很简单，将$0\in \partial f(x_0)$带入定义的那个式子就可以得到。有了这个工具之后，就可以对 LASSO 的最优解进行分析了。在此之前，我们先看一下原始的 least square 问题的最优解现在变成了什么样子，由于 orthonormal design ，我们有 \hat{w}=\frac{1}{n}X^Ty然后我们再来看LASSO，假设$\bar{w}=(\bar{w}^1,······，\bar{w}^p)^T$是$J_L(w)$的全局最优值点。考虑第$j$个变量$\bar{w}^j$，有两种情况。 第一种情况：gradient存在，此时$\bar{w}^j≠0$ 由于gradient在最小值点必须等于零，我们有 \frac{\partial J_L(w)}{\partial w_j}|_{\bar{w}_j}=0亦即 -\frac{2}{n}(X^Ty-X^TX\bar{w})_j+\lambda sign(\bar{w}^j)=0根据orthonormal design性质以及least square问题在orthonormal design时的解$\hat{w}^j$化简得到 \bar{w}^j=\hat{w}^j-\frac{\lambda}{2}sign(\bar{w}^j)从这个式子也可以明显地看出$\bar{w}^j$和$\hat{w}^j$是同号的，于是$sign(\bar{w}^j)=sign(\hat{w}^j)$所以上面的式子变为 \bar{w}^j=\hat{w}^j-\frac{\lambda}{2}sign(\bar{w}^j)=sign(\hat{w}^j)(|\hat{w}^j|-\frac{\lambda}{2})再用一次$sign(\bar{w}^j)=sign(\hat{w}^j)$，两边同时乘以$sign(\bar{w}^j)$，可以得到 |\hat{w}^j|-\frac{\lambda}{2}=|\bar{w}^j|≥0于是刚才的式子可以进一步写成 \bar{w}^j=sign(\hat{w}^j)(|\hat{w}^j|-\frac{\lambda}{2})_+这里$(x)_+=max \{x,0\}$表示$x$ 的正部。 第二种情况：gradient不存在，此时$\bar{w}^j=0$根据subgradient在最小值点出的性质，此时有： 0=\bar{w}^j\in \partial {J_L(\bar{w})}=\{-\frac{2}{n}(X^Ty-X^TX\bar{w})_j+\lambda e:e \in [-1,1]\}亦即存在$e_0 \in [-1,1]$使得 0=2\bar{w}^j-2\hat{w}^j+\lambda e_0于是 |\hat{w}^j|=\frac{\lambda}{2}|e_0|≤\frac{\lambda}{2}又因为$\bar{w}^j=0$，所以这个时候式子也可以统一为 \bar{w}^j=sign(\hat{w}^j)(|\hat{w}^j|-\frac{\lambda}{2})_+的形式。 如此一来，在 orthonormal design 的情况下，LASSO 的最优解就可以写为 \bar{w}^j=sign(\hat{w}^j)(|\hat{w}^j|-\frac{\lambda}{2})_+可以用图形象地表达出来。 图上画了原始的 least square 解，LASSO 的解以及 ridge regression 的解，用上面同样的方法（不过由于 ridge regularizer 是 smooth 的，所以过程却简单得多）可以得知 ridge regression 的解是如下形式 \frac{n}{1+n\lambda}\hat{w}^j可以认为ridge regression 只是做了一个全局缩放，而 LASSO 则是做了一个 soft thresholding ：将绝对值小于$\frac{\lambda}{2}$的那些系数直接变成零了，这也就更加令人信服地解释了 LASSO 为何能够产生稀疏解了。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>L1正则</tag>
        <tag>稀疏性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（28）：L1、L2正则化]]></title>
    <url>%2F2017%2F07%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8828%EF%BC%89%EF%BC%9AL1%E3%80%81L2%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[之前讨论了机器学习中的偏差-方差权衡。机器学习里的损失函数（代价函数）可以用来描述模型与真模型（ground truth）之间的差距，因此可以解决“偏差”的问题。但是仅有损失函数，我们无法解决方差的问题，因而会有过拟合风险。 这次我们讨论损失函数的反面——正则项，看看L1正则项和L2正则项是如何使机器学习模型避免过拟合的。 我们希望选择或学习一个合适的模型。若在空间中存在“真模型”，那我们所选择的模型要与真模型的参数个数相同，所选择的模型的参数向量与真模型的参数向量相近。 过拟合指的是我们以为追求提高模型对训练数据的预测能力，所选模型的复杂度往往会比真模型更高。即学习时选择的模型所包含的参数过多，以致于出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。 一、从经验风险最小化到结构经验最小化经验风险最小化（empirical risk minimization）认为经验风险最小的模型是最优的模型，即求解最优化问题： \underset{f\in\mathscr{F}}{\min}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}当样本容量足够大的时候，经验风险最小化学习效果良好。比如极大似然估计，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。 但是当样本容量很小时，经验风险最小化学习会产生过拟合（over-fitting）的现象。这就引出了结构风险最小化，它等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term），它的定义为： R_{srm}\left(f\right)=\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)其中$J(f)$为模型的复杂度，模型$f$越复杂，复杂度$J(f)$就越大；反之，模型越简单，复杂度$J(f)$就越小，即复杂度表示了对复杂模型的惩罚。$\lambda≥0$是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险和模型复杂度同时小。结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。比如贝叶斯估计中的最大后验概率估计就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。结构风险最小化的策略认为结构风险最小的模型是最优的模型，求解最优模型即求解最优化问题： \min_{f\in\mathscr{F}}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)这样，监督学习问题变成了经验风险或结构风险函数的最优化问题。 其中正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如，正则化项可以是模型参数向量的范数。它的一般形式如下： \min_{f\in\mathscr{F}}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)第一项是经验风险，第二项是正则化项，$\lambda≥0$为调整两者之间关系的系数。 二、范数与正则项在线性代数、函数分析等数学分支中，范数（Norm）是一个函数，其赋予某个向量空间（或矩阵）中的每个向量以长度或大小。对于零向量，另其长度为零。直观的说，向量或矩阵的范数越大，则我们可以说这个向量或矩阵也就越大。有时范数有很多更为常见的叫法，如绝对值其实便是一维向量空间中实数或复数的范数，而Euclidean距离也是一种范数。 范数满足通常意义上长度的三个基本性质： 非负性：$||\vec{x}||≥0$ 齐次性：$||c·\vec{x}||=|c|||\vec{x}||$ 三角不等式：$||\vec{x}+\vec{y}||≤||\vec{x}||+||\vec{y}||$ 在这里，我们需要关注的最主要是范数的「非负性」。我们刚才讲，损失函数通常是一个有下确界的函数。而这个性质保证了我们可以对损失函数做最优化求解。如果我们要保证目标函数依然可以做最优化求解，那么我们就必须让正则项也有一个下界。非负性无疑提供了这样的下界，而且它是一个下确界——由齐次性保证（当 c=0 时）。 因此，我们说，范数的性质使得它天然地适合作为机器学习的正则项。而范数需要的向量，则是机器学习的学习目标——参数向量。 范数的一般化定义：设$p≥1$的实数，$p-norm$定义为： ||x||_p:=(\sum_{i=1}^n|x_i|^p)^{\frac{1}{p}} \ \ \ \ （1）机器学习中有几个常用的范数，分别是： $L_1-$范数：$||x⃗ ||=∑^d_{i=1}| x_i|$; $L_2-$范数：$||x⃗ ||_2=(∑^d_{i=1}x^2_i)^{1/2}$； $L_p-$范数：$||x||_p=(\sum_{i=1}^n|x_i|^p)^{\frac{1}{p}}$ $L_∞-$范数：$||x⃗ ||_∞=lim_{p→+∞}(∑^d_{i=1}x^p_i)^{1/p}$。 当p=1时，我们称之为taxicab Norm，也叫Manhattan Norm。其来源是曼哈顿的出租车司机在四四方方的曼哈顿街道中从一点到另一点所需要走过的距离。也即我们所要讨论的l1范数。其表示某个向量中所有元素绝对值的和。 而当p=2时，则是我们最为常见的Euclidean norm。也称为Euclidean distance。也即我们要讨论的l2范数。 而当p=0时，因其不再满足三角不等性，严格的说此时p已不算是范数了，但很多人仍然称之为l0范数。 这三个范数有很多非常有意思的特征，尤其是在机器学习中的正则化（Regularization）以及稀疏编码（Sparse Coding）有非常有趣的应用。 下图给出了一个Lp球的形状随着P的减少的可视化图。 在机器学习中，如果使用了$||\vec{w}||_p作为正则项$，则我们说，该机器学习任务引入了$L_p-$正则项。 2.1 $L_0$与$L_1-$正则项（LASSO regularizer）在机器学习里，最简单的学习算法可能是所谓的线性回归模型 F(x⃗ ;w⃗ ,b)=∑_{i=1}^nw_i⋅x_i+b我们考虑这样一种普遍的情况，即：预测目标背后的真是规律，可能只和某几个维度的特征有关；而其它维度的特征，要不然作用非常小，要不然纯粹是噪声。在这种情况下，除了这几个维度的特征对应的参数之外，其它维度的参数应该为零。若不然，则当其它维度的特征存在噪音时，模型的行为会发生预期之外的变化，导致过拟合。 于是，我们得到了避免过拟合的第一个思路：使尽可能多的参数为零。为此，最直观地我们可以引入$L_0$-范数。令 Ω(F(x⃗ ;w⃗ ))\overset{def}=ℓ_0\frac{∥w⃗ ∥_0}{n},ℓ_0>0这意味着，我们希望绝大多数$\vec{w}$的分量为零。 通过引入 $L_0-$正则项，我们实际上引入了一种「惩罚」机制，即：若要增加模型复杂度以加强模型的表达能力降低损失函数，则每次使得一个参数非零，则引入 $ℓ_0$ 的惩罚系数。也就是说，如果使得一个参数非零得到的收益（损失函数上的收益）不足 $ℓ_0$；那么增加这样的复杂度是得不偿失的。 通过引入$L_0-$正则项，我们可以使模型稀疏化且易于解释，并且在某种意义上实现了「特征选择」。这看起来很美好，但是 $L_0-$正则项也有绕不过去坎： 非连续 非凸 不可求导 因此，$L_0$正则项虽好，但是求解这样的最优化问题，难以在多项式时间内找到有效解（NP-Hard 问题）。于是，我们转而考虑 L0L0-范数最紧的凸放松（tightest convex relaxation）：L1-范数。令 Ω(F(x⃗ ;w⃗ ))\overset{def}{=}ℓ_1\frac{∥w⃗ ∥_1}{n},ℓ_1>0我们来看一下参数更新的过程，有哪些变化。考虑目标函数 Obj(F)=L(F)+γ⋅ℓ_1\frac{∥w⃗ ∥_1}{n}对参数$w_i$求偏导数 \frac{∂Obj}{∂w_i}=\frac{∂L}{∂w_i}+\frac{γℓ_1}{n}sgn(w_i)因此参数更新的过程为 w_i→w′_i\overset{def}{=}w_i−η\frac{∂L}{∂w_i}−η\frac{γℓ_1}{n}sgn(w_i)因为$η\frac{γℓ_1}{n}&gt;0$所以多出的项$η\frac{γℓ_1}{n}sgn(w_i)$使得$w_i→0$，实现稀疏化。 2.2 $L_2$正则项（Ridge Regularizer）让我们回过头，考虑多项式模型，它的一般形式为： F=∑_{i=1}^nw_i⋅x^i+b我们注意到，当多项式模型过拟合时，函数曲线倾向于靠近噪声点。这意味着，函数曲线会在噪声点之间来回扭曲跳跃。这也就是说，在某些局部，函数曲线的切线斜率会非常高（函数导数的绝对值非常大）。对于多项式模型来说，函数导数的绝对值，实际上就是多项式系数的一个线性加和。这也就是说，过拟合的多项式模型，它的参数的绝对值会非常大（至少某几个参数分量的绝对值非常大）。因此，如果我们有办法使得这些参数的值，比较稠密均匀地集中在0附近，就能有效地避免过拟合。 于是我们引入$L_2-$正则项，令 Ω(F(x⃗ ;w⃗ ))\overset{def}=ℓ_2\frac{∥w⃗ ∥_2 }{2n},ℓ_2>0因此有目标函数 Obj(F)=L(F)+γ⋅ℓ_2\frac{∥w⃗ ∥_2 }{2n}对参数$w_i$求偏导数，有 \frac{∂Obj}{∂w_i}=\frac{∂L}{∂w_i}+\frac{γℓ_2}{n}w_i再有参数更新 w_i→w′_i\overset{def}{=}w_i−η\frac{∂L}{∂w_i}−η\frac{γℓ_2}{n}w_i=(1−η\frac{γℓ_2}n)w_i−η\frac{∂L}{∂w_i}考虑到$η\frac{γℓ_2}{n}&gt;0$，因此，引入$L_2-$正则项之后，相当于衰减了（decay）参数的权重，使参数趋近于0。 2.3 $L_1-$正则项与$L_2-$正则项的区别现在，我们考虑这样一个问题：为什么使用$L-1-$正则项，会倾向于使得参数稀疏化；而使用$L_2-$正则项，会倾向于使得参数稠密地接近于0？ 这里引用一张来自周志华老师的著作，《机器学习》（西瓜书）里的插图，尝试解释这个问题。 为了简便起见，我们只考虑模型有两个参数$w_1$和$w_2$的情形。 在图中，我们有三组等值线，位于同一条等值线上的$w_1$与$w_2$映射到相同的平方损失项、$L_1-$范数和$L_2-$范数。并且，对于三组等值线来说，当$(w_1,w_2)$沿着等值线法线方向，向外扩张，则对应的值增大；反之，若沿着法线向内收缩，则对应的值减小。 因此，对于目标函数$Obj(F)$来说，实际上是要在正则项的等值线与损失函数的等值线中寻找一个交点，使得二者的和最小。 对于$L_1-$正则项来说，因为$L_1-$正则项是一组菱形，这些交点容易落在坐标轴上。因此，另一个参数的值在这个交点上就是0，从而实现了稀疏化。 对于 $L_2-$正则项来说，因为 $L_2-$正则项的等值线是一组圆形。所以，这些交点可能落在整个平面的任意位置。所以它不能实现「稀疏化」。但是，另一方面，由于 $(w_1,w_2)$ 落在圆上，所以它们的值会比较接近。这就是为什么 $L_2-$正则项可以使得参数在零附近稠密而平滑。 三、贝叶斯先验从贝叶斯的角度来看，正则化等价于对模型参数引入先验分布。 3.1 Linear Regression我们先看下最原始的线性回归： \begin{align*} & p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(\epsilon^{(i)})^2}{2\delta^2} \right)\\ \Rightarrow & p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})^2}{2\delta^2} \right) \end{align*}由最大似然估计（MLE）： \begin{align*} L(w) & = p(\vec{y}|X;w)\\ & = \prod_{i=1}^{m} p(y^{(i)}|x^{(i)};\theta)\\ & = \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})^2}{2\delta^2} \right) \end{align*}取对数： \begin{align*} l(w) & = \log L(w)\\ & =\log \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})}{2\delta^2} \right)\\ & = \sum_{i=1}^{m} \log \frac{1}{\sqrt{2\pi}\delta}exp\left( -\frac{(y^{(i)} - w^Tx^{(i)})^2}{2\delta^2} \right)\\ & = m \log \frac{1}{\sqrt{2\pi}\delta} - \frac{1}{\delta^2}\cdot \frac{1}{2} \sum_{i=1}^{m} (y^{(i)} - w^Tx^{(i)})^2 \end{align*}即 w_{MLE} = \arg \underset{w}{\min} \sum_{i=1}^{m} (y^{(i)} - w^Tx^{(i)})^2这就导出了我们最原始的$least-squares$损失函数，但这是在我们对参数$w$没有加入任何先验分布的情况下。在数据维度很高的情况下，我们的模型参数很多，模型复杂度高，容易发生过拟合。 比如我们常说的 “small n, large p problem”。（我们一般用 n 表示数据点的个数，用 p 表示变量的个数 ，即数据维度。当 的时候，不做任何其他假设或者限制的话，学习问题基本上是没法进行的。因为如果用上所有变量的话， p 越大，通常会导致模型越复杂，但是反过来 n 又很小，于是就会出现很严重的 overfitting 问题。Linear regression一般只对low dimension适用，比如n=50, p=5，而且这五个变量还不存在multicolinearity. 这个时候，我们可以对参数$w$引入先验分布，降低模型复杂度。 3.2 Ridge RegressionRidge Regression的提出就是为了解决multicolinearity的，加一个L2 penalty term也是因为算起来方便。然而它并不能shrink parameters to 0.所以没法做variable selection。 我们对参数 w 引入协方差为 $\alpha$ 的零均值高斯先验。 取对数： 等价于： 这不就是Ridge Regression吗？看我们得到的参数，在零附近是不是很密集，老实说 ridge regression 并不具有产生稀疏解的能力，也就是说参数并不会真出现很多零。假设我们的预测结果与两个特征相关，L2正则倾向于综合两者的影响，给影响大的特征赋予高的权重；而L1正则倾向于选择影响较大的参数，而舍弃掉影响较小的那个。实际应用中 L2正则表现往往会优于 L1正则，但 L1正则会大大降低我们的计算量。 Typically ridge or ℓ2 penalties are much better for minimizing prediction error rather than ℓ1 penalties. The reason for this is that when two predictors are highly correlated, ℓ1 regularizer will simply pick one of the two predictors. In contrast, the ℓ2 regularizer will keep both of them and jointly shrink the corresponding coefficients a little bit. Thus, while the ℓ1 penalty can certainly reduce overfitting, you may also experience a loss in predictive power. 那现在我们知道了，对参数引入 高斯先验 等价于L2正则化。 3.3 LASSOLASSO是针对Ridge Regression的没法做variable selection的问题提出来的，L1 penalty虽然算起来麻烦，没有解析解，但是可以把某些系数shrink到0。 在Ridge Regression中，我们对 w 引入了高斯分布，那么拉普拉斯分布(Laplace distribution)呢？ 注：LASSO - least absolute shrinkage and selection operator. 我们看下拉普拉斯分布长啥样： 关于拉普拉斯和正态分布的渊源，大家可以参见 正态分布的前世今生。重复之前的推导过程我们很容易得到： 该问题通常被称为 LASSO (least absolute shrinkage and selection operator) 。LASSO 仍然是一个 convex optimization 问题，不具有解析解。它的优良性质是能产生稀疏性，导致 w 中许多项变成零。 再次总结下，对参数引入 拉普拉斯先验 等价于 L1正则化。 3.4 Elastic Net然而LASSO虽然可以做variable selection，但是不consistent啊，而且当n很小时至多只能选出n个变量；而且不能做group selection。 可能有同学会想，既然 L1和 L2正则各自都有自己的优势，那我们能不能将他们 combine 起来？ 于是有了在L1和L2 penalty之间做个权重就是elastic net 因为lasso在解决之前提到的“small n, large p problem”存在一定缺陷。 得到结果： 此外针对不consistent有了adaptive lasso，针对不能做group selection有了group lasso, 在graphical models里有了graphical lasso。然后有人说unbiasedness, sparsity and continuity这三条都满足多好，于是有了MCP和SCAD同时满足这三条性质。还有很多penalized regression的方法。 3.5 总结 正则化参数等价于对参数引入 先验分布，使得 模型复杂度 变小（缩小解空间），对于噪声以及 outliers 的鲁棒性增强（泛化能力）。整个最优化问题从贝叶斯观点来看是一种贝叶斯最大后验估计，其中 正则化项对应后验估计中的先验信息，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计的形式。 这篇文章从理论推导讲到算法实现。除了高斯先验、拉普拉斯先验，还讲了其他先验：Lazy Sparse Stochastic Gradient Descent for Regularized Mutlinomial Logistic Regression]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>L1正则</tag>
        <tag>L2正则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（8）：红黑树]]></title>
    <url>%2F2017%2F07%2F22%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%888%EF%BC%89%EF%BC%9A%E7%BA%A2%E9%BB%91%E6%A0%91%2F</url>
    <content type="text"><![CDATA[红黑树，即R-B Tree，本文的主要内容包括：红黑树的特性、红黑树的时间复杂度和它的证明，红黑树的时间复杂度和它的证明，红黑树的左旋、右旋、插入、删除等操作 一、红黑树的定义1.1 红黑树的定义R-B Tree，全称是Red-black Tree，又称为“红黑树”，它是一种特殊的二叉查找树。红黑树的每个节点上都有存储位表示节点的颜色，可以是红（Red）或黑（Black） 红黑树是一种自平衡二叉查找树，是在计算机科学在用到的一种数据结构，典型的用途是实现关联数组。它是在1972年由鲁道夫.贝尔发明的，称之为“对称二叉B树”，它的现代名字是在 Leo J. Guibas 和 Robert Sedgewick 于1978年写的一篇论文中获得的。它是复杂的，但它的操作有着良好的最坏情况运行时间，并且在实践中是高效的：他可以在$O(logn)$的时间内做查找、插入和删除，这里的n是树中元素的数目。 红黑树是每个节点都带有颜色属性的二叉查找树，颜色为红色或黑色。在二叉查找树强制的一般要求以外，对于任何有效的红黑树我们增加了如下的额外要求: 性质1：节点是红色或黑色 性质2：根节点是黑色 性质3：所有叶子节点都是黑色（叶子是NIL节点） 性质4：每个红色节点必须有两个黑色的子节点。(从每个叶子到根的所有路径上不能有两个连续的红色节点。) 性质5.：从任一节点到其每个叶子的所有简单路径都包含相同数目的黑色节点。 注意： 性质3中的叶子节点，是只为空(NIL或null)的节点。 性质5确保从根到叶子的最长的可能路径不多于最短的可能路径的两倍长，因而，红黑树是相对是接近平衡的二叉树。因为操作比如插入、删除和查找某个值的最坏情况时间都要求与树的高度成比例，这个在高度上的理论上限允许红黑树在最坏情况下都是高效的，而不同于普通的二叉查找树。 要知道为什么这些性质确保了这个结果，注意到性质4导致了路径不能有两个毗连的红色节点就足够了。最短的可能路径都是黑色节点，最长的可能路径有交替的红色和黑色节点。因为根据性质5所有最长的路径都有相同数目的黑色节点，这就表明了没有路径能多于任何其他路径的两倍长。 【提问】请解释红黑性，以及为什么每次操作都是稳定O(logn)的复杂度 下面是一个具体的红黑树的图例： 1.2 红黑树的应用红黑树的应用比较广泛，主要是用它来存储有序的数据，它的时间复杂度是$O(lgn)$，效率非常之高。 例如，Java集合中的$TreeSet$和$TreeMap$，C++的STL中的Set、Map，以及Linux虚拟内存的管理，都是通过红黑树去实现的。 二、红黑树的基本操作：左旋和右旋红黑树的基本操作是添加、删除。在对红黑树进行添加或删除之后，都会用到旋转方法。为什么呢？道理很简单，添加或删除红黑树中的节点之后，红黑树就发生了变化，可能不满足红黑树的5条性质，也就不再是一颗红黑树了，而是一颗普通的树。而通过旋转，可以使这颗树重新成为红黑树。简单点说，旋转的目的是让树保持红黑树的特性。 恢复红黑树的性质需要少量(O(logn))的颜色变更(实际是非常快速的)和不超过三次树旋转(对于插入操作是两次)。虽然插入和删除很复杂，但操作时间仍可以保持为O(logn) 次。 旋转包括两种：左旋 和 右旋。下面分别对它们进行介绍。 2.1 左旋对X进行左旋，意味着“将X变成一个左节点”。 左旋的伪代码《算法导论》：参考上面的示意图和下面的伪代码，理解“红黑树T的节点x进行左旋”是如何进行的。 123456789101112LEFT-ROTATE(T, x) 01 y ← right[x] // 前提：这里假设x的右孩子为y。下面开始正式操作02 right[x] ← left[y] // 将 “y的左孩子” 设为 “x的右孩子”，即 将β设为x的右孩子03 p[left[y]] ← x // 将 “x” 设为 “y的左孩子的父亲”，即 将β的父亲设为x04 p[y] ← p[x] // 将 “x的父亲” 设为 “y的父亲”05 if p[x] = nil[T] 06 then root[T] ← y // 情况1：如果 “x的父亲” 是空节点，则将y设为根节点07 else if x = left[p[x]] 08 then left[p[x]] ← y // 情况2：如果 x是它父节点的左孩子，则将y设为“x的父节点的左孩子”09 else right[p[x]] ← y // 情况3：(x是它父节点的右孩子) 将y设为“x的父节点的右孩子”10 left[y] ← x // 将 “x” 设为 “y的左孩子”11 p[x] ← y // 将 “x的父节点” 设为 “y” 理解左旋之后，看看下面更鲜明的例子。 2.2 右旋对Y进行右旋，意味着“将Y变成一个右节点” 右旋的伪代码《算法导论》：参考上面的示意图和下面的伪代码，理解“红黑树T的节点y进行右旋”是如何进行的。 123456789101112RIGHT-ROTATE(T, y) 01 x ← left[y] // 前提：这里假设y的左孩子为x。下面开始正式操作02 left[y] ← right[x] // 将 “x的右孩子” 设为 “y的左孩子”，即 将β设为y的左孩子03 p[right[x]] ← y // 将 “y” 设为 “x的右孩子的父亲”，即 将β的父亲设为y04 p[x] ← p[y] // 将 “y的父亲” 设为 “x的父亲”05 if p[y] = nil[T] 06 then root[T] ← x // 情况1：如果 “y的父亲” 是空节点，则将x设为根节点07 else if y = right[p[y]] 08 then right[p[y]] ← x // 情况2：如果 y是它父节点的右孩子，则将x设为“y的父节点的左孩子”09 else left[p[y]] ← x // 情况3：(y是它父节点的左孩子) 将x设为“y的父节点的左孩子”10 right[x] ← y // 将 “y” 设为 “x的右孩子”11 p[y] ← x // 将 “y的父节点” 设为 “x” 理解右旋之后，看看下面一个更鲜明的例子。 综上，左旋和右旋是相对的两个概念，原理类似。理解一个也就理解了另一个。 在实际应用中，若没有彻底理解左旋和右旋，可能会将它们混淆。下面谈谈我对如何区分左旋 和右旋 的理解。 2.3 区分左旋和右旋仔细观察上面”左旋”和”右旋”的示意图。我们能清晰的发现，它们是对称的。无论是左旋还是右旋，被旋转的树，在旋转前是二叉查找树，并且旋转之后仍然是一颗二叉查找树。 左旋示例图(以x为节点进行左旋)： 12345 z x / / \ --(左旋)--&gt; xy z / y 对x进行左旋，意味着，将“x的右孩子”设为“x的父亲节点”；即，将 x变成了一个左节点(x成了为z的左孩子)！。 因此，左旋中的“左”，意味着“被旋转的节点将变成一个左节点”。 右旋示例图(以x为节点进行右旋)： 12345 y x \ / \ --(右旋)--&gt; xy z \ z 对x进行右旋，意味着，将“x的左孩子”设为“x的父亲节点”；即，将 x变成了一个右节点(x成了为y的右孩子)！ 因此，右旋中的“右”，意味着“被旋转的节点将变成一个右节点”。 三、红黑树的基本操作：添加将一个节点插入到红黑树中，需要执行哪些步骤呢？首先，将红黑树当作一颗二叉查找树，将节点插入；然后，将节点着色为红色；最后，通过旋转和重新着色等方法来修正该树，使之重新成为一颗红黑树。详细描述如下： 第一步: 将红黑树当作一颗二叉查找树，将节点插入。 红黑树本身就是一颗二叉查找树，将节点插入后，该树仍然是一颗二叉查找树。也就意味着，树的键值仍然是有序的。此外，无论是左旋还是右旋，若旋转之前这棵树是二叉查找树，旋转之后它一定还是二叉查找树。这也就意味着，任何的旋转和重新着色操作，都不会改变它仍然是一颗二叉查找树的事实。 好吧？那接下来，我们就来想方设法的旋转以及重新着色，使这颗树重新成为红黑树！ 第二步：将插入的节点着色为”红色”。 为什么着色成红色，而不是黑色呢？为什么呢？在回答之前，我们需要重新温习一下红黑树的特性： 每个节点或者是黑色，或者是红色。 根节点是黑色。 每个叶子节点是黑色。 [注意：这里叶子节点，是指为空的叶子节点！] 如果一个节点是红色的，则它的子节点必须是黑色的。 从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。 将插入的节点着色为红色，不会违背”特性(5)”！少违背一条特性，就意味着我们需要处理的情况越少。接下来，就要努力的让这棵树满足其它性质即可；满足了的话，它就又是一颗红黑树了。o(∩∩)o…哈哈 第三步: 通过一系列的旋转或着色等操作，使之重新成为一颗红黑树。 第二步中，将插入节点着色为”红色”之后，不会违背”特性(5)”。那它到底会违背哪些特性呢？ 对于”特性(1)”，显然不会违背了。因为我们已经将它涂成红色了。 对于”特性(2)”，显然也不会违背。在第一步中，我们是将红黑树当作二叉查找树，然后执行的插入操作。而根据二叉查找数的特点，插入操作不会改变根节点。所以，根节点仍然是黑色。 对于”特性(3)”，显然不会违背了。这里的叶子节点是指的空叶子节点，插入非空节点并不会对它们造成影响。 对于”特性(4)”，是有可能违背的！ 那接下来，想办法使之”满足特性(4)”，就可以将树重新构造成红黑树了。 下面看看代码到底是怎样实现这三步的。 添加操作的伪代码《算法导论》 123456789101112131415161718RB-INSERT(T, z) y ← nil[T] // 新建节点“y”，将y设为空节点。 x ← root[T] // 设“红黑树T”的根节点为“x” while x ≠ nil[T] // 找出要插入的节点“z”在二叉树T中的位置“y” do y ← x if key[z] &lt; key[x] then x ← left[x] else x ← right[x] p[z] ← y // 设置 “z的父亲” 为 “y” if y = nil[T] then root[T] ← z // 情况1：若y是空节点，则将z设为根 else if key[z] &lt; key[y] then left[y] ← z // 情况2：若“z所包含的值” &lt; “y所包含的值”，则将z设为“y的左孩子” else right[y] ← z // 情况3：(“z所包含的值” &gt;= “y所包含的值”)将z设为“y的右孩子” left[z] ← nil[T] // z的左孩子设为空 right[z] ← nil[T] // z的右孩子设为空。至此，已经完成将“节点z插入到二叉树”中了。 color[z] ← RED // 将z着色为“红色” RB-INSERT-FIXUP(T, z) // 通过RB-INSERT-FIXUP对红黑树的节点进行颜色修改以及旋转，让树T仍然是一颗红黑树 结合伪代码以及为代码上面的说明，先理解RB-INSERT。理解了RB-INSERT之后，我们接着对 RB-INSERT-FIXUP的伪代码进行说明。 添加修正操作的伪代码《算法导论》 1234567891011121314151617RB-INSERT-FIXUP(T, z)while color[p[z]] = RED // 若“当前节点(z)的父节点是红色”，则进行以下处理。 do if p[z] = left[p[p[z]]] // 若“z的父节点”是“z的祖父节点的左孩子”，则进行以下处理。 then y ← right[p[p[z]]] // 将y设置为“z的叔叔节点(z的祖父节点的右孩子)” if color[y] = RED // Case 1条件：叔叔是红色 then color[p[z]] ← BLACK ▹ Case 1 // (01) 将“父节点”设为黑色。 color[y] ← BLACK ▹ Case 1 // (02) 将“叔叔节点”设为黑色。 color[p[p[z]]] ← RED ▹ Case 1 // (03) 将“祖父节点”设为“红色”。 z ← p[p[z]] ▹ Case 1 // (04) 将“祖父节点”设为“当前节点”(红色节点) else if z = right[p[z]] // Case 2条件：叔叔是黑色，且当前节点是右孩子 then z ← p[z] ▹ Case 2 // (01) 将“父节点”作为“新的当前节点”。 LEFT-ROTATE(T, z) ▹ Case 2 // (02) 以“新的当前节点”为支点进行左旋。 color[p[z]] ← BLACK ▹ Case 3 // Case 3条件：叔叔是黑色，且当前节点是左孩子。(01) 将“父节点”设为“黑色”。 color[p[p[z]]] ← RED ▹ Case 3 // (02) 将“祖父节点”设为“红色”。 RIGHT-ROTATE(T, p[p[z]]) ▹ Case 3 // (03) 以“祖父节点”为支点进行右旋。 else (same as then clause with &quot;right&quot; and &quot;left&quot; exchanged) // 若“z的父节点”是“z的祖父节点的右孩子”，将上面的操作中“right”和“left”交换位置，然后依次执行。color[root[T]] ← BLACK 根据被插入节点的父节点的情况，可以将”当节点z被着色为红色节点，并插入二叉树”划分为三种情况来处理。 第一种情况说明：被插入的节点是根节点。处理方法：直接把此节点涂为黑色。 第二种情况说明：被插入的节点的父节点是黑色。处理方法：什么也不需要做。节点被插入后，仍然是红黑树。 第三种情况说明：被插入的节点的父节点是红色。处理方法：那么，该情况与红黑树的“特性(5)”相冲突。这种情况下，被插入节点是一定存在非空祖父节点的；进一步的讲，被插入节点也一定存在叔叔节点(即使叔叔节点为空，我们也视之为存在，空节点本身就是黑色节点)。理解这点之后，我们依据”叔叔节点的情况”，将这种情况进一步划分为3种情况(Case)。 上面三种情况(Case)处理问题的核心思路都是：将红色的节点移到根节点；然后，将根节点设为黑色。下面对它们详细进行介绍。 Case 1：叔叔是红色 现象说明：当前节点(即，被插入节点)的父节点是红色，且当前节点的祖父节点的另一个子节点（叔叔节点）也是红色。 处理策略 将“父节点”设为黑色。 将“叔叔节点”设为黑色。 将“祖父节点”设为“红色”。 将“祖父节点”设为“当前节点”(红色节点)；即，之后继续对“当前节点”进行操作。 下面谈谈为什么要这样处理。(建议理解的时候，通过下面的图进行对比) “当前节点”和“父节点”都是红色，违背“特性(4)”。所以，将“父节点”设置“黑色”以解决这个问题。 但是，将“父节点”由“红色”变成“黑色”之后，违背了“特性(5)”：因为，包含“父节点”的分支的黑色节点的总数增加了1。 解决这个问题的办法是：将“祖父节点”由“黑色”变成红色，同时，将“叔叔节点”由“红色”变成“黑色”。关于这里，说明几点：第一，为什么“祖父节点”之前是黑色？这个应该很容易想明白，因为在变换操作之前，该树是红黑树，“父节点”是红色，那么“祖父节点”一定是黑色。 第二，为什么将“祖父节点”由“黑色”变成红色，同时，将“叔叔节点”由“红色”变成“黑色”；能解决“包含‘父节点’的分支的黑色节点的总数增加了1”的问题。这个道理也很简单。“包含‘父节点’的分支的黑色节点的总数增加了1” 同时也意味着 “包含‘祖父节点’的分支的黑色节点的总数增加了1”，既然这样，我们通过将“祖父节点”由“黑色”变成“红色”以解决“包含‘祖父节点’的分支的黑色节点的总数增加了1”的问题； 但是，这样处理之后又会引起另一个问题“包含‘叔叔’节点的分支的黑色节点的总数减少了1”，现在我们已知“叔叔节点”是“红色”，将“叔叔节点”设为“黑色”就能解决这个问题。 所以，将“祖父节点”由“黑色”变成红色，同时，将“叔叔节点”由“红色”变成“黑色”；就解决了该问题。 按照上面的步骤处理之后：当前节点、父节点、叔叔节点之间都不会违背红黑树特性，但祖父节点却不一定。若此时，祖父节点是根节点，直接将祖父节点设为“黑色”，那就完全解决这个问题了；若祖父节点不是根节点，那我们需要将“祖父节点”设为“新的当前节点”，接着对“新的当前节点”进行分析。 Case 2：叔叔是黑色，且当前节点是右孩子 现象说明：当前节点(即，被插入节点)的父节点是红色，叔叔节点是黑色，且当前节点是其父节点的右孩子 处理策略 将“父节点”作为“新的当前节点”。 以“新的当前节点”为支点进行左旋。 下面谈谈为什么要这样处理。(建议理解的时候，通过下面的图进行对比) 首先，将“父节点”作为“新的当前节点”；接着，以“新的当前节点”为支点进行左旋。 为了便于理解，我们先说明第(02)步，再说明第(01)步；为了便于说明，我们设置“父节点”的代号为F(Father)，“当前节点”的代号为S(Son)。 为什么要“以F为支点进行左旋”呢？根据已知条件可知：S是F的右孩子。而之前我们说过，我们处理红黑树的核心思想：将红色的节点移到根节点；然后，将根节点设为黑色。既然是“将红色的节点移到根节点”，那就是说要不断的将破坏红黑树特性的红色节点上移(即向根方向移动)。 而S又是一个右孩子，因此，我们可以通过“左旋”来将S上移！ 按照上面的步骤(以F为支点进行左旋)处理之后：若S变成了根节点，那么直接将其设为“黑色”，就完全解决问题了；若S不是根节点，那我们需要执行步骤(01)，即“将F设为‘新的当前节点’”。那为什么不继续以S为新的当前节点继续处理，而需要以F为新的当前节点来进行处理呢？这是因为“左旋”之后，F变成了S的“子节点”，即S变成了F的父节点；而我们处理问题的时候，需要从下至上(由叶到根)方向进行处理；也就是说，必须先解决“孩子”的问题，再解决“父亲”的问题；所以，我们执行步骤(01)：将“父节点”作为“新的当前节点”。 Case 3：叔叔是黑色，且当前节点是左孩子 现象说明：当前节点(即，被插入节点)的父节点是红色，叔叔节点是黑色，且当前节点是其父节点的左孩子 处理策略 将“父节点”设为“黑色”。 将“祖父节点”设为“红色”。 以“祖父节点”为支点进行右旋。 下面谈谈为什么要这样处理。(建议理解的时候，通过下面的图进行对比) 为了便于说明，我们设置“当前节点”为S(Original Son)，“兄弟节点”为B(Brother)，“叔叔节点”为U(Uncle)，“父节点”为F(Father)，祖父节点为G(Grand-Father)。 S和F都是红色，违背了红黑树的“特性(4)”，我们可以将F由“红色”变为“黑色”，就解决了“违背‘特性(4)’”的问题；但却引起了其它问题：违背特性(5)，因为将F由红色改为黑色之后，所有经过F的分支的黑色节点的个数增加了1。那我们如何解决“所有经过F的分支的黑色节点的个数增加了1”的问题呢？ 我们可以通过“将G由黑色变成红色”，同时“以G为支点进行右旋”来解决。 提示：上面的进行Case 3处理之后，再将节点”120”当作当前节点，就变成了Case 2的情况。 四、红黑树的基本操作：删除将红黑树内的某一个节点删除。需要执行的操作依次是：首先，将红黑树当作一颗二叉查找树，将该节点从二叉查找树中删除；然后，通过”旋转和重新着色”等一系列来修正该树，使之重新成为一棵红黑树。详细描述如下： 第一步：将红黑树当作一颗二叉查找树，将节点删除。 这和”删除常规二叉查找树中删除节点的方法是一样的”。分3种情况： 被删除节点没有儿子，即为叶节点。那么，直接将该节点删除就OK了。 被删除节点只有一个儿子。那么，直接删除该节点，并用该节点的唯一子节点顶替它的位置。 被删除节点有两个儿子。那么，先找出它的后继节点；然后把“它的后继节点的内容”复制给“该节点的内容”；之后，删除“它的后继节点”。在这里，后继节点相当于替身，在将后继节点的内容复制给”被删除节点”之后，再将后继节点删除。这样就巧妙的将问题转换为”删除后继节点”的情况了，下面就考虑后继节点。 在”被删除节点”有两个非空子节点的情况下，它的后继节点不可能是双子非空。既然”的后继节点”不可能双子都非空，就意味着”该节点的后继节点”要么没有儿子，要么只有一个儿子。若没有儿子，则按”情况① “进行处理；若只有一个儿子，则按”情况2”进行处理。 第二步：通过”旋转和重新着色”等一系列来修正该树，使之重新成为一棵红黑树。 因为”第一步”中删除节点之后，可能会违背红黑树的特性。所以需要通过”旋转和重新着色”来修正该树，使之重新成为一棵红黑树。 删除操作的伪代码《算法导论》 12345678910111213141516171819RB-DELETE(T, z)if left[z] = nil[T] or right[z] = nil[T] then y ← z // 若“z的左孩子” 或 “z的右孩子”为空，则将“z”赋值给 “y”； else y ← TREE-SUCCESSOR(z) // 否则，将“z的后继节点”赋值给 “y”。if left[y] ≠ nil[T] then x ← left[y] // 若“y的左孩子” 不为空，则将“y的左孩子” 赋值给 “x”； else x ← right[y] // 否则，“y的右孩子” 赋值给 “x”。p[x] ← p[y] // 将“y的父节点” 设置为 “x的父节点”if p[y] = nil[T] then root[T] ← x // 情况1：若“y的父节点” 为空，则设置“x” 为 “根节点”。 else if y = left[p[y]] then left[p[y]] ← x // 情况2：若“y是它父节点的左孩子”，则设置“x” 为 “y的父节点的左孩子” else right[p[y]] ← x // 情况3：若“y是它父节点的右孩子”，则设置“x” 为 “y的父节点的右孩子”if y ≠ z then key[z] ← key[y] // 若“y的值” 赋值给 “z”。注意：这里只拷贝z的值给y，而没有拷贝z的颜色！！！ copy y&apos;s satellite data into z if color[y] = BLACK then RB-DELETE-FIXUP(T, x) // 若“y为黑节点”，则调用return y 结合伪代码以及为代码上面的说明，先理解RB-DELETE。理解了RB-DELETE之后，接着对 RB-DELETE-FIXUP的伪代码进行说明 123456789101112131415161718192021222324RB-DELETE-FIXUP(T, x)while x ≠ root[T] and color[x] = BLACK do if x = left[p[x]] then w ← right[p[x]] // 若 “x”是“它父节点的左孩子”，则设置 “w”为“x的叔叔”(即x为它父节点的右孩子) if color[w] = RED // Case 1: x是“黑+黑”节点，x的兄弟节点是红色。(此时x的父节点和x的兄弟节点的子节点都是黑节点)。 then color[w] ← BLACK ▹ Case 1 // (01) 将x的兄弟节点设为“黑色”。 color[p[x]] ← RED ▹ Case 1 // (02) 将x的父节点设为“红色”。 LEFT-ROTATE(T, p[x]) ▹ Case 1 // (03) 对x的父节点进行左旋。 w ← right[p[x]] ▹ Case 1 // (04) 左旋后，重新设置x的兄弟节点。 if color[left[w]] = BLACK and color[right[w]] = BLACK // Case 2: x是“黑+黑”节点，x的兄弟节点是黑色，x的兄弟节点的两个孩子都是黑色。 then color[w] ← RED ▹ Case 2 // (01) 将x的兄弟节点设为“红色”。 x ← p[x] ▹ Case 2 // (02) 设置“x的父节点”为“新的x节点”。 else if color[right[w]] = BLACK // Case 3: x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的左孩子是红色，右孩子是黑色的。 then color[left[w]] ← BLACK ▹ Case 3 // (01) 将x兄弟节点的左孩子设为“黑色”。 color[w] ← RED ▹ Case 3 // (02) 将x兄弟节点设为“红色”。 RIGHT-ROTATE(T, w) ▹ Case 3 // (03) 对x的兄弟节点进行右旋。 w ← right[p[x]] ▹ Case 3 // (04) 右旋后，重新设置x的兄弟节点。 color[w] ← color[p[x]] ▹ Case 4 // Case 4: x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的右孩子是红色的。(01) 将x父节点颜色 赋值给 x的兄弟节点。 color[p[x]] ← BLACK ▹ Case 4 // (02) 将x父节点设为“黑色”。 color[right[w]] ← BLACK ▹ Case 4 // (03) 将x兄弟节点的右子节设为“黑色”。 LEFT-ROTATE(T, p[x]) ▹ Case 4 // (04) 对x的父节点进行左旋。 x ← root[T] ▹ Case 4 // (05) 设置“x”为“根节点”。 else (same as then clause with &quot;right&quot; and &quot;left&quot; exchanged) // 若 “x”是“它父节点的右孩子”，将上面的操作中“right”和“left”交换位置，然后依次执行。color[x] ← BLACK 下面对删除函数进行分析。在分析之前，我们再次温习一下红黑树的几个特性： 每个节点或者是黑色，或者是红色。 根节点是黑色。 每个叶子节点是黑色。 [注意：这里叶子节点，是指为空的叶子节点！] 如果一个节点是红色的，则它的子节点必须是黑色的。 从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。 前面我们将”删除红黑树中的节点”大致分为两步，在第一步中”将红黑树当作一颗二叉查找树，将节点删除”后，可能违反”特性(2)、(4)、(5)”三个特性。第二步需要解决上面的三个问题，进而保持红黑树的全部特性。 为了便于分析，我们假设”x包含一个额外的黑色”(x原本的颜色还存在)，这样就不会违反”特性(5)”。为什么呢？ 通过RB-DELETE算法，我们知道：删除节点y之后，x占据了原来节点y的位置。 既然删除y(y是黑色)，意味着减少一个黑色节点；那么，再在该位置上增加一个黑色即可。这样，当我们假设”x包含一个额外的黑色”，就正好弥补了”删除y所丢失的黑色节点”，也就不会违反”特性(5)”。 因此，假设”x包含一个额外的黑色”(x原本的颜色还存在)，这样就不会违反”特性(5)”。 现在，x不仅包含它原本的颜色属性，x还包含一个额外的黑色。即x的颜色属性是”红+黑”或”黑+黑”，它违反了”特性(1)”。 现在，我们面临的问题，由解决”违反了特性(2)、(4)、(5)三个特性”转换成了”解决违反特性(1)、(2)、(4)三个特性”。RB-DELETE-FIXUP需要做的就是通过算法恢复红黑树的特性(1)、(2)、(4)。RB-DELETE-FIXUP的思想是：将x所包含的额外的黑色不断沿树上移(向根方向移动)，直到出现下面的姿态： x指向一个”红+黑”节点。此时，将x设为一个”黑”节点即可。 x指向根。此时，将x设为一个”黑”节点即可。 非前面两种姿态。 将上面的姿态，可以概括为3种情况。 第一种情况说明：x是“红+黑”节点。处理方法：直接把x设为黑色，结束。此时红黑树性质全部恢复。 第二种情况说明：x是“黑+黑”节点，且x是根。处理方法：什么都不做，结束。此时红黑树性质全部恢复。 第三种情况说明：x是“黑+黑”节点，且x不是根。处理方法：这种情况又可以划分为4种子情况。这4种子情况如下表所示： Case 1：x是”黑+黑”节点，x的兄弟节点是红色 现象说明：x是”黑+黑”节点，x的兄弟节点是红色。(此时x的父节点和x的兄弟节点的子节点都是黑节点)。 处理策略 将x的兄弟节点设为“黑色”。 将x的父节点设为“红色”。 对x的父节点进行左旋。 左旋后，重新设置x的兄弟节点。 下面谈谈为什么要这样处理。(建议理解的时候，通过下面的图进行对比) 这样做的目的是将“Case 1”转换为“Case 2”、“Case 3”或“Case 4”，从而进行进一步的处理。对x的父节点进行左旋；左旋后，为了保持红黑树特性，就需要在左旋前“将x的兄弟节点设为黑色”，同时“将x的父节点设为红色”；左旋后，由于x的兄弟节点发生了变化，需要更新x的兄弟节点，从而进行后续处理。 Case 3：x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的左孩子是红色，右孩子是黑色的 现象说明：x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的左孩子是红色，右孩子是黑色的。 处理策略 将x兄弟节点的左孩子设为“黑色”。 将x兄弟节点设为“红色”。 对x的兄弟节点进行右旋。 右旋后，重新设置x的兄弟节点。 下面谈谈为什么要这样处理。(建议理解的时候，通过下面的图进行对比) 我们处理“Case 3”的目的是为了将“Case 3”进行转换，转换成“Case 4”,从而进行进一步的处理。转换的方式是对x的兄弟节点进行右旋；为了保证右旋后，它仍然是红黑树，就需要在右旋前“将x的兄弟节点的左孩子设为黑色”，同时“将x的兄弟节点设为红色”；右旋后，由于x的兄弟节点发生了变化，需要更新x的兄弟节点，从而进行后续处理。 Case 4：x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的右孩子是红色的，x的兄弟节点的左孩子任意颜色 现象说明：x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的右孩子是红色的，x的兄弟节点的左孩子任意颜色。 处理策略 将x父节点颜色 赋值给 x的兄弟节点。 将x父节点设为“黑色”。 将x兄弟节点的右子节设为“黑色”。 对x的父节点进行左旋。 设置“x”为“根节点”。 下面谈谈为什么要这样处理。(建议理解的时候，通过下面的图进行对比) 我们处理“Case 4”的目的是：去掉x中额外的黑色，将x变成单独的黑色。处理的方式是“：进行颜色修改，然后对x的父节点进行左旋。下面，我们来分析是如何实现的。 为了便于说明，我们设置“当前节点”为S(Original Son)，“兄弟节点”为B(Brother)，“兄弟节点的左孩子”为BLS(Brother’s Left Son)，“兄弟节点的右孩子”为BRS(Brother’s Right Son)，“父节点”为F(Father)。 我们要对F进行左旋。但在左旋前，我们需要调换F和B的颜色，并设置BRS为黑色。为什么需要这里处理呢？因为左旋后，F和BLS是父子关系，而我们已知BL是红色，如果F是红色，则违背了“特性(4)”；为了解决这一问题，我们将“F设置为黑色”。 但是，F设置为黑色之后，为了保证满足“特性(5)”，即为了保证左旋之后： 第一，“同时经过根节点和S的分支的黑色节点个数不变”。若满足“第一”，只需要S丢弃它多余的颜色即可。因为S的颜色是“黑+黑”，而左旋后“同时经过根节点和S的分支的黑色节点个数”增加了1；现在，只需将S由“黑+黑”变成单独的“黑”节点，即可满足“第一”。 第二，“同时经过根节点和BLS的分支的黑色节点数不变”。若满足“第二”，只需要将“F的原始颜色”赋值给B即可。之前，我们已经将“F设置为黑色”(即，将B的颜色”黑色”，赋值给了F)。至此，我们算是调换了F和B的颜色。 第三，“同时经过根节点和BRS的分支的黑色节点数不变”。在“第二”已经满足的情况下，若要满足“第三”，只需要将BRS设置为“黑色”即可。 经过，上面的处理之后。红黑树的特性全部得到的满足！接着，我们将x设为根节点，就可以跳出while循环(参考伪代码)；即完成了全部处理。 至此，我们就完成了Case 4的处理。理解Case 4的核心，是了解如何“去掉当前节点额外的黑色”。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>树</tag>
        <tag>红黑树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（7）：数据库索引原理及优化]]></title>
    <url>%2F2017%2F07%2F21%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%887%EF%BC%89%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[本文以MySQL数据库为研究对象，讨论与数据库索引相关的一些话题。特别需要说明的是，MySQL支持诸多存储引擎，而各种存储引擎对索引的支持也各不相同，因此MySQL数据库支持多种索引类型，如BTree索引，哈希索引，全文索引等等。为了避免混乱，本文将只关注于BTree索引，因为这是平常使用MySQL时主要打交道的索引，至于哈希索引和全文索引本文暂不讨论。 文章主要内容分为三个部分。 第一部分主要从数据结构及算法理论层面讨论MySQL数据库索引的数理基础。 第二部分结合MySQL数据库中MyISAM和InnoDB数据存储引擎中索引的架构实现讨论聚集索引、非聚集索引及覆盖索引等话题。 第三部分根据上面的理论基础，讨论MySQL中高性能使用索引的策略。 一、数据结构及算法基础为什么这里要讲查询算法和数据结构呢？因为之所以要建立索引，其实就是为了构建一种数据结构，可以在上面应用一种高效的查询算法，最终提高数据的查询速度。 1.1 索引的本质MySQL官方对索引的定义为：索引（Index）是帮助MySQL高效获取数据的数据结构。提取句子主干，就可以得到索引的本质：索引是数据结构。 我们知道，数据库查询是数据库的最主要功能之一。我们都希望查询数据的速度能尽可能的快，因此数据库系统的设计者会从查询算法的角度进行优化。最基本的查询算法当然是顺序查找（linear search），这种复杂度为O(n)的算法在数据量很大时显然是糟糕的，好在计算机科学的发展提供了很多更优秀的查找算法，例如二分查找（binary search）、二叉树查找（binary tree search）等。 如果稍微分析一下会发现，每种查找算法都只能应用于特定的数据结构之上，例如二分查找要求被检索数据有序，而二叉树查找只能应用于二叉查找树上，但是数据本身的组织结构不可能完全满足各种数据结构（例如，理论上不可能同时将两列都按顺序进行组织），所以，在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。 看一个例子： 图1展示了一种可能的索引方式。左边是数据表，一共有两列七条记录，最左边的是数据记录的物理地址（注意逻辑上相邻的记录在磁盘上也并不是一定物理相邻的）。为了加快Col2的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找在O(log2n)O(log2n)的复杂度内获取到相应数据。 虽然这是一个货真价实的索引，但是实际的数据库系统几乎没有使用二叉查找树或其进化品种红黑树（red-black tree）实现的，原因会在下文介绍。 1.2 B树和B+树目前大部分数据库系统及文件系统都采用B-Tree或其变种B+Tree作为索引结构，在本文的下一节会结合存储器原理及计算机存取原理讨论为什么B-Tree和B+Tree在被如此广泛用于索引，这一节先单纯从数据结构角度描述它们。 要理解B树，必须从二叉查找树（Binary search tree）讲起。 二叉查找树是一种查找效率非常高的数据结构，它有三个特点。 123（1）每个节点最多只有两个子树。（2）左子树都为小于父节点的值，右子树都为大于父节点的值。（3）在n个节点中找到目标值，一般只需要log(n)次比较。 二叉查找树的结构不适合数据库，因为它的查找效率与层数相关。越处在下层的数据，就需要越多次比较。它的搜索时间复杂度为$O(log_2N)$，所以它的搜索效率和树的深度有关极端情况下，n个数据需要n次比较才能找到目标值。对于数据库来说，每进入一层，就要从硬盘读取一次数据，这非常致命，因为硬盘的读取时间远远大于数据处理时间，数据库读取硬盘的次数越少越好，这一点也会在后面深入剖析。 如果要提高查询速度，那么就要降低树的深度。要降低树的深度，很自然的方法就是采用多叉树，再结合平衡二叉树的思想，我们可以构建一个平衡多叉树结构，然后就可以在上面构建平衡多路查找算法，提高大数据量下的搜索效率。 1.2.1 B树B树（B-tree）是一种树状数据结构，能够用来存储排序后的数据。这种数据结构能够让查找数据、循序存取、插入数据及删除的动作，都在对数时间内完成。B树，概括来说是一个一般化的二叉查找树，可以拥有多于2个子节点。与自平衡二叉查找树不同，B-树为系统最优化大块数据的读和写操作。B-tree算法减少定位记录时所经历的中间过程，从而加快存取速度。这种数据结构常被应用在数据库和文件系统的实作上。 在B树中查找给定关键字的方法是，首先把根结点取来，在根结点所包含的关键字K1,…,Kn查找给定的关键字（可用顺序查找或二分查找法），若找到等于给定值的关键字，则查找成功；否则，一定可以确定要查找的关键字在Ki与Ki+1之间，Pi为指向子树根节点的指针，此时取指针Pi所指的结点继续查找，直至找到，或指针Pi为空时查找失败。 B树作为一种多路搜索树（并不是二叉的）： 定义任意非叶子结点最多只有M个儿子；且M&gt;2； 根结点的儿子数为[2, M]； 除根结点以外的非叶子结点的儿子数为[M/2, M]； 每个结点存放至少M/2-1（取上整）和至多M-1个关键字；(至少2个关键字） 非叶子结点的关键字个数=指向儿子的指针个数-1； 非叶子结点的关键字：K[1], K[2], …, K[M-1]；且K[i] &lt; K[i+1]； 非叶子结点的指针：P[1], P[2], …, P[M]；其中P[1]指向关键字小于K[1]的子树，P[M]指向关键字大于K[M-1]的子树，其它P[i]指向关键字属于(K[i-1], K[i])的子树； 所有叶子结点位于同一层； 此外，在B树中，除非数据已经填满，否则不会增加新的层。也就是说，B树追求层越少越好。 这样的数据结构，非常有利于减少读取硬盘的次数。假定一个节点可以容纳100个值，那么三层的B树可以容纳100万个数据，但若换成二叉查找树，则需要20层！假定操作系统一次读取一个节点，并且根节点保留在内存中，那么B树在100万个数据中查找目标值，只需要读取两次硬盘。 如下图为一个M=3的B树示例：B树创建的示意图： 1.2.2 B+树B+树是B树的变体，MySQL普遍使用B+Tree实现其索引结构。也是一种多路搜索树，其定义基本与B-树相同，除了： 1）非叶子结点的子树指针与关键字个数相同； 2）非叶子结点的子树指针P[i]，指向关键字值属于[K[i], K[i+1])的子树（B-树是开区间）； 3）为所有叶子结点增加一个链指针； 4）所有关键字都在叶子结点出现； 下图为M=3的B+树的示意图： B+树的搜索与B树也基本相同，区别是B+树只有达到叶子结点才命中（B树可以在非叶子结点命中），其性能也等价于在关键字全集做一次二分查找； B+树的性质： 1.所有关键字都出现在叶子结点的链表中（稠密索引），且链表中的关键字恰好是有序的； 2.不可能在非叶子结点命中； 3.非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层； 4.更适合文件索引系统。 下面为一个B+树创建的示意图： 一般在数据库系统或文件系统中使用的B+ Tree结构都在经典B+ Tree的基础上进行了优化，增加了顺序访问指针。做这个优化的目的是为了提高区间访问的性能，例如下图中如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。 【提问】数据库索引用什么建的 ：b+树；解释一下B+树，并说出数据库索引的原理； 二、计算机组成原理上文说过，红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B-/+Tree作为索引结构，这一节将结合计算机组成原理相关知识讨论B-/+Tree作为索引的理论基础。 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。下面先介绍内存和磁盘存取原理，然后再结合这些原理分析B-/+Tree作为索引的效率。 2.1 主存存取原理目前计算机使用的主存基本都是随机读写存储器（RAM），现代RAM的结构和存取原理比较复杂，这里本文抛却具体差别，抽象出一个十分简单的存取模型来说明RAM的工作原理。 从抽象角度看，主存是一系列的存储单元组成的矩阵，每个存储单元存储固定大小的数据。每个存储单元有唯一的地址，现代主存的编址规则比较复杂，这里将其简化成一个二维地址：通过一个行地址和一个列地址可以唯一定位到一个存储单元。上图展示了一个4 x 4的主存模型。 主存的存取过程如下： 当系统需要读取主存时，则将地址信号放到地址总线上传给主存，主存读到地址信号后，解析信号并定位到指定存储单元，然后将此存储单元数据放到数据总线上，供其它部件读取。写主存的过程类似，系统将要写入单元地址和数据分别放在地址总线和数据总线上，主存读取两个总线的内容，做相应的写操作。 这里可以看出，主存存取的时间仅与存取次数呈线性关系，因为不存在机械操作，两次存取的数据的“距离”不会对时间有任何影响，例如，先取A0再取A1和先取A0再取D3的时间消耗是一样的。 2.2 磁盘存取原理上文说过，索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O操作。与主存不同，磁盘I/O存在机械运动耗费，因此磁盘I/O的时间消耗是巨大的。 磁盘读取数据靠的是机械运动，当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。 为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间，最后便是对读取数据的传输。 所以每次读取数据花费的时间可以分为寻道时间、旋转延迟、传输时间三个部分。其中： 寻道时间是磁臂移动到指定磁道所需要的时间，主流磁盘一般在5ms以下。 旋转延迟就是我们经常听说的磁盘转速，比如一个磁盘7200转，表示每分钟能转7200次，也就是说1秒钟能转120次，旋转延迟就是1/120/2 = 4.17ms。 传输时间指的是从磁盘读出或将数据写入磁盘的时间，一般在零点几毫秒，相对于前两个时间可以忽略不计。 那么访问一次磁盘的时间，即一次磁盘IO的时间约等于5+4.17 = 9ms左右，听起来还挺不错的，但要知道一台500 -MIPS的机器每秒可以执行5亿条指令，因为指令依靠的是电的性质，换句话说执行一次IO的时间可以执行40万条指令，数据库动辄十万百万乃至千万级数据，每次9毫秒的时间，显然是个灾难。 2.3 局部性原理与磁盘预读由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 三、B-/+Tree索引的性能分析到这里终于可以分析为何数据库索引采用B-/+Tree存储结构了。上文说过数据库索引是存储到磁盘的而我们又一般以使用磁盘I/O次数来评价索引结构的优劣。先从B-Tree分析，根据B-Tree的定义，可知检索一次最多需要访问h-1个节点（根节点常驻内存）。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧：每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为$O(h)=O(log_dN)$。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。 综上所述，如果我们采用B-Tree存储结构，搜索时I/O次数一般不会超过3次，所以用B-Tree作为索引结构效率是非常高的。 3.1 B+树性能分析从上面介绍我们知道，B树的搜索复杂度为O(h)=O(logdN)，所以树的出度d越大，深度h就越小，I/O的次数就越少。B+Tree恰恰可以增加出度d的宽度，因为每个节点大小为一个页大小，所以出度的上限取决于节点内key和data的大小： 1dmax=floor(pagesize/(keysize+datasize+pointsize))//floor表示向下取整 由于B+Tree内节点去掉了data域，因此可以拥有更大的出度，从而拥有更好的性能。 3.2 B+树查找过程 B-树和B+树查找过程基本一致。如上图所示，如果要查找数据项29，那么首先会把磁盘块1由磁盘加载到内存，此时发生一次IO，在内存中用二分查找确定29在17和35之间，锁定磁盘块1的P2指针，内存时间因为非常短（相比磁盘的IO）可以忽略不计，通过磁盘块1的P2指针的磁盘地址把磁盘块3由磁盘加载到内存，发生第二次IO，29在26和30之间，锁定磁盘块3的P2指针，通过指针加载磁盘块8到内存，发生第三次IO，同时内存中做二分查找找到29，结束查询，总计三次IO。真实的情况是，3层的b+树可以表示上百万的数据，如果上百万的数据查找只需要三次IO，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次IO，那么总共需要百万次的IO，显然成本非常非常高。 这一章从理论角度讨论了与索引相关的数据结构与算法问题，下一章将讨论B+Tree是如何具体实现为MySQL中索引，同时将结合MyISAM和InnDB存储引擎介绍非聚集索引和聚集索引两种不同的索引实现形式。 四、MySQL索引实现在MySQL中，索引属于存储引擎级别的概念，不同存储引擎对索引的实现方式是不同的，本文主要讨论MyISAM和InnoDB两个存储引擎的索引实现方式。 4.1 MyISAM索引实现MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。下图是MyISAM索引的原理图： 这里设表一共有三列，假设我们以Col1为主键，则上图是一个MyISAM表的主索引（Primary key）示意。可以看出MyISAM的索引文件仅仅保存数据记录的地址。在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。如果我们在Col2上建立一个辅助索引，则此索引的结构如下图所示： 同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。 4.2 InnoDB索引实现虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。 第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 上图是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。 第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。例如，下图为定义在Col3上的一个辅助索引： 这里以英文字符的ASCII码作为比较准则。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。 再例如，用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择。 五、索引使用策略及优化MySQL的优化主要分为结构优化（Scheme optimization）和查询优化（Query optimization）。本章讨论的高性能索引策略主要属于结构优化范畴。本章的内容完全基于上文的理论基础，实际上一旦理解了索引背后的机制，那么选择高性能的策略就变成了纯粹的推理，并且可以理解这些策略背后的逻辑。 5.1 联合索引及最左前缀原理 联合索引（复合索引） 首先介绍一下联合索引。联合索引其实很简单，相对于一般索引只有一个字段，联合索引可以为多个字段创建一个索引。它的原理也很简单，比如，我们在（a,b,c）字段上创建一个联合索引，则索引记录会首先按照A字段排序，然后再按照B字段排序然后再是C字段，因此，联合索引的特点就是： 第一个字段一定是有序的 当第一个字段值相等的时候，第二个字段又是有序的，比如下表中当A=2时所有B的值是有序排列的，依次类推，当同一个B值得所有C字段是有序排列的 12345678| A | B | C | | 1 | 2 | 3 | | 1 | 4 | 2 | | 1 | 1 | 4 | | 2 | 3 | 5 | | 2 | 4 | 4 | | 2 | 4 | 6 | | 2 | 5 | 5 | 其实联合索引的查找就跟查字典是一样的，先根据第一个字母查，然后再根据第二个字母查，或者只根据第一个字母查，但是不能跳过第一个字母从第二个字母开始查。这就是所谓的最左前缀原理。 最左前缀原理 我们再来详细介绍一下联合索引的查询。还是上面例子，我们在（a,b,c）字段上建了一个联合索引，所以这个索引是先按a 再按b 再按c进行排列的，所以以下的查询方式都可以用到索引 123select * from table where a=1；select * from table where a=1 and b=2；select * from table where a=1 and b=2 and c=3； 上面三个查询按照 （a ）, （a，b ）,（a，b，c ）的顺序都可以利用到索引，这就是最左前缀匹配。 1select * from table where a=1 and c=3； //那么只会用到索引a。 比如： 12select * from table where b=2 and a=1；select * from table where b=2 and a=1 and c=3； 如果用到了最左前缀而只是颠倒了顺序，也是可以用到索引的，因为mysql查询优化器会判断纠正这条sql语句该以什么样的顺序执行效率最高，最后才生成真正的执行计划。但我们还是最好按照索引顺序来查询，这样查询优化器就不用重新编译了。 前缀索引 除了联合索引之外，对mysql来说其实还有一种前缀索引。前缀索引就是用列的前缀代替整个列作为索引key，当前缀长度合适时，可以做到既使得前缀索引的选择性接近全列索引，同时因为索引key变短而减少了索引文件的大小和维护开销。 字符串列(varchar,char,text等)，需要进行全字段匹配或者前匹配。也就是=‘xxx’ 或者 like ‘xxx%’ 字符串本身可能比较长，而且前几个字符就开始不相同。比如我们对中国人的姓名使用前缀索引就没啥意义，因为中国人名字都很短，另外对收件地址使用前缀索引也不是很实用，因为一方面收件地址一般都是以XX省开头，也就是说前几个字符都是差不多的，而且收件地址进行检索一般都是like ’%xxx%’，不会用到前匹配。相反对外国人的姓名可以使用前缀索引，因为其字符较长，而且前几个字符的选择性比较高。同样电子邮件也是一个可以使用前缀索引的字段。 前一半字符的索引选择性就已经接近于全字段的索引选择性。如果整个字段的长度为20，索引选择性为0.9，而我们对前10个字符建立前缀索引其选择性也只有0.5，那么我们需要继续加大前缀字符的长度，但是这个时候前缀索引的优势已经不明显，没有太大的建前缀索引的必要了。 一些文章中也提到： MySQL 前缀索引能有效减小索引文件的大小，提高索引的速度。但是前缀索引也有它的坏处：MySQL 不能在 ORDER BY 或 GROUP BY 中使用前缀索引，也不能把它们用作覆盖索引(Covering Index)。 5.2 索引优化策略 最左前缀匹配原则，上面讲到了 主键外检一定要建索引 对 where,on,group by,order by 中出现的列使用索引 尽量选择区分度高的列作为索引,区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0 对较小的数据列使用索引,这样会使索引文件更小,同时内存中也可以装载更多的索引键 索引列不能参与计算，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’); 为较长的字符串使用前缀索引 尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可 不要过多创建索引, 权衡索引个数与DML之间关系，DML也就是插入、删除数据操作。这里需要权衡一个问题，建立索引的目的是为了提高查询效率的，但建立的索引过多，会影响插入、删除数据的速度，因为我们修改的表数据，索引也需要进行调整重建 对于like查询，”%”不要放在前面。 12SELECT * FROMhoudunwangWHEREunameLIKE&apos;后盾%&apos; -- 走索引 SELECT * FROMhoudunwangWHEREunameLIKE &quot;%后盾%&quot; -- 不走索引 查询where条件数据类型不匹配也无法使用索引 字符串与数字比较不使用索引; 123CREATE TABLEa(achar(10)); EXPLAIN SELECT * FROMaWHEREa=&quot;1&quot; – 走索引 EXPLAIN SELECT * FROM a WHERE a=1 – 不走索引 正则表达式不使用索引,这应该很好理解,所以为什么在SQL中很难看到regexp关键字的原因]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>数据库索引原理及优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（6）：B树、B+树]]></title>
    <url>%2F2017%2F07%2F20%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%886%EF%BC%89%EF%BC%9AB%E6%A0%91%E3%80%81B%2B%E6%A0%91%2F</url>
    <content type="text"><![CDATA[具体讲解之前，有一点，再次强调下：B-树，即为B树。因为B树的原英文名称为B-tree，而国内很多人喜欢把B-tree译作B-树，其实，这是个非常不好的直译，很容易让人产生误解。如人们可能会以为B-树是一种树，而B树又是一种一种树。而事实上是，B-tree就是指的B树。特此说明。 一、B树B树（B-tree）是一种树状数据结构，能够用来存储排序后的数据。这种数据结构能够让查找数据、循序存取、插入数据及删除的动作，都在对数时间内完成。B树，概括来说是一个一般化的二叉查找树，可以拥有多于2个子节点。与自平衡二叉查找树不同，B-树为系统最优化大块数据的读和写操作。B-tree算法减少定位记录时所经历的中间过程，从而加快存取速度。这种数据结构常被应用在数据库和文件系统的实作上。 在B树中查找给定关键字的方法是，首先把根结点取来，在根结点所包含的关键字K1,…,Kn查找给定的关键字（可用顺序查找或二分查找法），若找到等于给定值的关键字，则查找成功；否则，一定可以确定要查找的关键字在Ki与Ki+1之间，Pi为指向子树根节点的指针，此时取指针Pi所指的结点继续查找，直至找到，或指针Pi为空时查找失败。 B树作为一种多路搜索树（并不是二叉的）： 定义任意非叶子结点最多只有M个儿子；且M&gt;2； 根结点的儿子数为[2, M]； 除根结点以外的非叶子结点的儿子数为[M/2, M]； 每个结点存放至少M/2-1（取上整）和至多M-1个关键字；(至少2个关键字） 非叶子结点的关键字个数=指向儿子的指针个数-1； 非叶子结点的关键字：K[1], K[2], …, K[M-1]；且K[i] &lt; K[i+1]； 非叶子结点的指针：P[1], P[2], …, P[M]；其中P[1]指向关键字小于K[1]的子树，P[M]指向关键字大于K[M-1]的子树，其它P[i]指向关键字属于(K[i-1], K[i])的子树； 所有叶子结点位于同一层； 如下图为一个M=3的B树示例：B树创建的示意图： 二、B+树B+树是B树的变体，也是一种多路搜索树，其定义基本与B-树相同，除了： 1）非叶子结点的子树指针与关键字个数相同； 2）非叶子结点的子树指针P[i]，指向关键字值属于[K[i], K[i+1])的子树（B-树是开区间）； 3）为所有叶子结点增加一个链指针； 4）所有关键字都在叶子结点出现； 下图为M=3的B+树的示意图： B+树的搜索与B树也基本相同，区别是B+树只有达到叶子结点才命中（B树可以在非叶子结点命中），其性能也等价于在关键字全集做一次二分查找； B+树的性质： 1.所有关键字都出现在叶子结点的链表中（稠密索引），且链表中的关键字恰好是有序的； 2.不可能在非叶子结点命中； 3.非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层； 4.更适合文件索引系统。 下面为一个B+树创建的示意图： 三、B*树$B^*$树是B+树的变体，在B+树的非根和非叶子结点再增加指向兄弟的指针，将结点的最低利用率从1/2提高到2/3。 $B^*$树如下图所示： $B^*$树定义了非叶子结点关键字个数至少为$\frac{2}{3}M$，即块的最低使用率为2/3（代替B+树的1/2）； B+树的分裂：当一个结点满时，分配一个新的结点，并将原结点中1/2的数据复制到新结点，最后在父结点中增加新结点的指针；B+树的分裂只影响原结点和父结点，而不会影响兄弟结点，所以它不需要指向兄弟的指针； $B^*$树的分裂：当一个结点满时，如果它的下一个兄弟结点未满，那么将一部分数据移到兄弟结点中，再在原结点插入关键字，最后修改父结点中兄弟结点的关键字（因为兄弟结点的关键字范围改变了）；如果兄弟也满了，则在原结点与兄弟结点之间增加新结点，并各复制1/3的数据到新结点，最后在父结点增加新结点的指针； 所以，$B^*$树分配新结点的概率比B+树要低，空间使用率更高。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>B树</tag>
        <tag>B+树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（5）：AVL树]]></title>
    <url>%2F2017%2F07%2F19%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%885%EF%BC%89%EF%BC%9AAVL%E6%A0%91%2F</url>
    <content type="text"><![CDATA[我们知道，对于一般的二叉搜索树（Binary Search Tree），其期望高度（即为一棵平衡树时）为$log_2n$，其各操作的时间复杂度$O(log_2n)$同时也由此而决定。但是，在某些极端的情况下（如在插入的序列是有序的时），二叉搜索树将退化成近似链或链，此时，其操作的时间复杂度将退化成线性的，即O(n)。我们可以通过随机化建立二叉搜索树来尽量的避免这种情况，但是在进行了多次的操作之后，由于在删除时，我们总是选择将待删除节点的后继代替它本身，这样就会造成总是右边的节点数目减少，以至于树向左偏沉。这同时也会造成树的平衡性受到破坏，提高它的操作的时间复杂度。于是就有了我们下边介绍的平衡二叉树。 一、平衡二叉树平衡二叉树定义：平衡二叉树（Balanced Binary Tree）又被称为AVL树（有别于AVL算法），且具有以下性质：它是一 棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。平衡二叉树的常用算法有红黑树、AVL树等。在平衡二叉搜索树中，我们可以看到，其高度一般都良好地维持在O(log2n)，大大降低了操作的时间复杂度。 其严格定义为：一颗空树是平衡二叉树；若T是一棵非空二叉树，其左、右子树为TL和TR，令hl和hr分别为左、右子树的深度。当且仅当 TL、TR都是平衡二叉树； 并且满足公式$|hl-hr|≤1$时，则T是平衡二叉树 相应地，定义$hl-hr$为二叉平衡树的平衡因子（balance factor）。因此，平衡二叉树上所有结点的平衡因子可能是-1，0，1。换言之，若一颗二叉树上任一结点的平衡因子的绝对值都不大于1，则该树就是平衡二叉树。 最小二叉平衡树的节点的公式如下：$F(n)=F(n-1)+F(n-2)+1$ 这个类似于一个递归的数列，可以参考Fibonacci数列，1是根节点，$F(n-1)$是左子树的节点数量，$F(n-2)$是右子树的节点数量。 二、平衡查找树（AVL树）2.1 AVL树的定义平衡二叉树（AVL树，发明者的姓名缩写）：一种高度平衡的排序二叉树，其每一个节点的左子树和右子树的高度差最多等于1。平衡二叉树首先必须是一棵二叉排序树！ 平衡因子（Balance Factor）：将二叉树上节点的左子树深度减去右子树深度的值。对于平衡二叉树所有包括分支节点和叶节点的平衡因子只可能是-1,0和1，只要有一个节点的因子不在这三个值之内，该二叉树就是不平衡的。 最小不平衡子树：距离插入结点最近的，且平衡因子的绝对值大于1的节点为根的子树。 n个结点的AVL树最大深度约1.44log2n。查找、插入和删除在平均和最坏情况下都是O(logn)。增加和删除可能需要通过一次或多次树旋转来重新平衡这个树。这个方案很好的解决了二叉查找树退化成链表的问题，把插入，查找，删除的时间复杂度最好情况和最坏情况都维持在O(logN)。但是频繁旋转会使插入和删除牺牲掉O(logN)左右的时间，不过相对二叉查找树来说，时间上稳定了很多。 可以采用动态平衡技术保持一个平衡二叉树。构造平衡二叉树的时候，也可以采用相同的方法，默认初始时，是一个空树，插入节点时，通过动态平衡技术对二叉树进行调整。 Adeleon-Velskii和Landis提出了一个动态地保持二叉排序树平衡的方法，其基本思想是：在构造二叉排序树的过程中，每当插入一个结点时，首先检查是否因插入而破坏了树的平衡性，若是因插入结点而破坏了树的平衡性，则找出其中最小不平衡树，在保持排序树特性的前提下，调整最小不平衡子树各结点之间的连接关系，以达到新的平衡。通常这样得到的平衡二叉排序树简称为AVL树。 2.2 AVL树的自平衡操作——旋转AVL树最关键的也是最难的一步操作就是旋转。旋转主要是为了实现AVL树在实施了插入和删除操作以后，树重新回到平衡的方法。下面我们重点研究一下AVL树的旋转。 2.2.1 不平衡的四种情况对于一个平衡的节点，由于任意节点最多有两个儿子，因此高度不平衡时，此节点的两颗子树的高度差2。容易看出，这种不平衡出现在下面四种情况： 1) 6节点的左子树3节点高度比右子树7节点大2，左子树3节点的左子树1节点高度大于右子树4节点，这种情况成为左左。 2) 6节点的左子树2节点高度比右子树7节点大2，左子树2节点的左子树1节点高度小于右子树4节点，这种情况成为左右。 3) 2节点的左子树1节点高度比右子树5节点小2，右子树5节点的左子树3节点高度大于右子树6节点，这种情况成为右左。 4) 2节点的左子树1节点高度比右子树4节点小2，右子树4节点的左子树3节点高度小于右子树6节点，这种情况成为右右。 从图2中可以可以看出，1和4两种情况是对称的，这两种情况的旋转算法是一致的，只需要经过一次旋转就可以达到目标，我们称之为单旋转。2和3两种情况也是对称的，这两种情况的旋转算法也是一致的，需要进行两次旋转，我们称之为双旋转。 2.2.2 单旋转单旋转是针对于左左和右右这两种情况的解决方案，这两种情况是对称的，只要解决了左左这种情况，右右就很好办了。图3是左左情况的解决方案，节点k2不满足平衡特性，因为它的左子树k1比右子树Z深2层，而且k1子树中，更深的一层的是k1的左子树X子树，所以属于左左情况。 为使树恢复平衡，我们把k2变成这棵树的根节点，因为k2大于k1，把k2置于k1的右子树上，而原本在k1右子树的Y大于k1，小于k2，就把Y置于k2的左子树上，这样既满足了二叉查找树的性质，又满足了平衡二叉树的性质。 这样的操作只需要一部分指针改变，结果我们得到另外一颗二叉查找树，它是一棵AVL树，因为X向上一移动了一层，Y还停留在原来的层面上，Z向下移动了一层。整棵树的新高度和之前没有在左子树上插入的高度相同，插入操作使得X高度长高了。因此，由于这颗子树高度没有变化，所以通往根节点的路径就不需要继续旋转了。 2.2.3 双旋转双旋转：对于左右和右左这两种情况，单旋转不能使它达到一个平衡状态，要经过两次旋转。双旋转是针对于这两种情况的解决方案，同样的，这样两种情况也是对称的，只要解决了左右这种情况，右左就很好办了。图4是左右情况的解决方案，节点k3不满足平衡特性，因为它的左子树k1比右子树Z深2层，而且k1子树中，更深的一层的是k1的右子树k2子树，所以属于左右情况。 为使树恢复平衡，我们需要进行两步，第一步，把k1作为根，进行一次右右旋转，旋转之后就变成了左左情况，所以第二步再进行一次左左旋转，最后得到了一棵以k2为根的平衡二叉树树。 三、构建过程下面是由[1,2,3,4,5,6,7,10,9]构建平衡二叉树]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>树</tag>
        <tag>平衡二叉树</tag>
        <tag>AVL树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（4）：二叉查找树]]></title>
    <url>%2F2017%2F07%2F19%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%884%EF%BC%89%EF%BC%9A%E4%BA%8C%E5%8F%89%E6%9F%A5%E6%89%BE%E6%A0%91%2F</url>
    <content type="text"><![CDATA[一、定义二叉排序树（Binary Sort Tree）又称为二叉查找树（Binary Search Tree）、二叉搜索树。它是特殊的二叉树：对于二叉树，假设x为二叉树中的任意一个结点，x节点包含关键字key，节点x的$key$值记为$key[x]$。如果y是x的左子树中的一个结点，则$key[y]&lt;= key[x]$；如果y是x的右子树的一个结点，则$key[y] &gt;= key[x]$。那么，这棵树就是二叉查找树。 二叉查找树是先对待查找的数据进行生成树，确保树的左分支的值小于右分支的值，然后再就行和每个节点的父节点比较大小，查找最合适的范围。这个算法的效率查找效率很高，但是如果使用这种查找方法要首先创建树。 它或者是一颗空树，或者是具有下列性质的二叉树： 若任意节点的左子树不为空，则左子树上的所有节点的值均小于它的根结点的值； 若任意节点的右子树不为空，则左子树上所有节点的值均小于它的根结点的值； 任意节点的左、右子树也分别为二叉查找树。 没有键值相等的节点。 二叉查找树的性质：对二叉查找树进行中序遍历，即可得到有序的数列。 构造一颗二叉排序树的目的，其实并不是为了排序，而是为了提高查找和插入删除关键字的速度。不管怎么说，在一个有序数据集上的查找，速度总是要快于无序的数据集的，而二叉排序树这样的非线性结构，也有利于插入和排序的实现。 二叉查找树的高度决定了二叉查找树的查找效率。 二、查找、插入与删除2.1 查找在二叉查找树中查找x的过程如下： 若二叉树是空树，则查找失败。 若x等于根结点的数据，则查找成功，否则。 若x小于根结点的数据，则递归查找其左子树，否则。 递归查找其右子树。 复杂度分析，它和二分查找一样，插入和查找的时间复杂度均为O(logn)，但是在最坏的情况下仍然会有O(n)的时间复杂度。原因在于插入和删除元素的时候，树没有保持平衡（比如，我们查找上图（b）中的“93”，我们需要进行n次查找操作）。我们追求的是在最坏的情况下仍然有较好的时间复杂度，这就是平衡查找树设计的初衷。 根据上述的步骤，写出其查找操作的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/**查找指定的元素,默认从 * 根结点出开始查询*/ public boolean contains(T t) &#123; return contains(t, rootTree); &#125; /**从某个结点出开始查找元素*/ public boolean contains(T t, BinaryNode&lt;T&gt; node) &#123; if(node==null) return false;//结点为空，查找失败 int result = t.compareTo(node.data); if(result&gt;0) return contains(t,node.right);//递归查询右子树 else if(result&lt;0) return contains(t, node.left);//递归查询左子树 else return true; &#125; /** 这里我提供一个对二叉树最大值 最小值的搜索*/ /**找到二叉查找树中的最小值*/ public T findMin() &#123; if(isEmpty()) &#123; System.out.println("二叉树为空"); return null; &#125;else return findMin(rootTree).data; &#125; /**找到二叉查找树中的最大值*/ public T findMax() &#123; if(isEmpty()) &#123; System.out.println("二叉树为空"); return null; &#125;else return findMax(rootTree).data; &#125; /**查询出最小元素所在的结点*/ public BinaryNode&lt;T&gt; findMin(BinaryNode&lt;T&gt; node) &#123; if(node==null) return null; else if(node.left==null) return node; return findMin(node.left);//递归查找 &#125; /**查询出最大元素所在的结点*/ public BinaryNode&lt;T&gt; findMax(BinaryNode&lt;T&gt; node) &#123; if(node!=null) &#123; while(node.right!=null) node=node.right; &#125; return node; &#125; 2.2 插入插入：从根结点开始逐个与关键字进行对比，小了去左边，大了去右边，碰到子树为空的情况就将新的节点连接。二叉查找树的插入过程如下： 1) 若当前的二叉查找树为空，则插入的元素为根节点; 2) 若插入的元素值小于根节点值，则将元素插入到左子树中; 3) 若插入的元素值不小于根节点值，则将元素插入到右子树中。 12345678910111213141516171819202122/**插入元素*/ public void insert(T t) &#123; rootTree = insert(t, rootTree); &#125; /**在某个位置开始判断插入元素*/ public BinaryNode&lt;T&gt; insert(T t,BinaryNode&lt;T&gt; node) &#123; if(node==null) &#123; //新构造一个二叉查找树 return new BinaryNode&lt;T&gt;(t, null, null); &#125; int result = t.compareTo(node.data); if(result&lt;0) node.left= insert(t,node.left); else if(result&gt;0) node.right= insert(t,node.right); else ;//doNothing return node; &#125; 2.3 删除如果要删除的节点是叶子，直接删；如果只有左子树或只有右子树，则删除节点后，将子树连接到父节点即可；如果同时有左右子树，则可以将二叉排序树进行中序遍历，取将要被删除的节点的前驱或者后继节点替代这个被删除的节点的位置。 二叉查找树的删除，分三种情况进行处理： 1) p为叶子节点，直接删除该节点，再修改其父节点的指针（注意分是根节点和不是根节点），如图a; p为单支节点（即只有左子树或右子树）。让p的子树与p的父亲节点相连，删除p即可（注意分是根节点和不是根节点），如图b; p的左子树和右子树均不空。找到p的后继y，因为y一定没有左子树，所以可以删除y，并让y的父亲节点成为y的右子树的父亲节点，并用y的值代替p的值；或者方法二是找到p的前驱x，x一定没有右子树，所以可以删除x，并让x的父亲节点成为y的左子树的父亲节点。如图c。 123456789101112131415161718192021222324/**删除元素*/ public void remove(T t) &#123; rootTree = remove(t,rootTree); &#125; /**在某个位置开始判断删除某个结点*/ public BinaryNode&lt;T&gt; remove(T t,BinaryNode&lt;T&gt; node) &#123; if(node == null) return node;//没有找到,doNothing int result = t.compareTo(node.data); if(result&gt;0) node.right = remove(t,node.right); else if(result&lt;0) node.left = remove(t,node.left); else if(node.left!=null&amp;&amp;node.right!=null) &#123; node.data = findMin(node.right).data; node.right = remove(node.data,node.right); &#125; else node = (node.left!=null)?node.left:node.right; return node; &#125; 2.4 总结二叉排序树总结： 二叉排序树以链式进行存储，保持了链式结构在插入和删除操作上的优点。 在极端情况下，查询次数为1，但最大操作次数不会超过树的深度。也就是说，二叉排序树的查找性能取决于二叉排序书的形状，也就引申除了后面的平衡二叉树。 给定一个元素集合，可以构造不同的二叉排序树，当它同时是一个完全二叉树的时候，查找的时间复杂度为$O(log(n))$，近似于二分查找。 当出现最极端的斜树时，时间复杂度为$O(n)$，等同于顺序查找，效果最差。 下图为二叉树查找和顺序查找以及二分查找性能的对比图： 基于二叉查找树进行优化，进而可以得到其他的树表查找算法，比如平衡树、红黑树等高效算法。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>树</tag>
        <tag>二叉查找树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（3）：二叉树]]></title>
    <url>%2F2017%2F07%2F19%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%883%EF%BC%89%EF%BC%9A%E4%BA%8C%E5%8F%89%E6%A0%91%2F</url>
    <content type="text"><![CDATA[数据结构中有很多树的结构，这里整理了二叉树、二叉查找树、AVL树、红黑树、B树、B+树、trie树的基本概念与操作。 一、二叉树的概念1.1 树的基本概念树是一种数据结构，它是由n（n&gt;=1）个有限节点组成一个具有层次关系的集合。它具有以下特点： 每个节点有零个或多个子节点； 没有父节点的节点称为根节点； 每一个非根节点有且只有一个父节点； 除了根节点外，每个子节点可以分为多个不相交的子树。 若一个结点有子树，那么该结点称为子树根的”双亲”，子树的根是该结点的”孩子”。有相同双亲的结点互为”兄弟”。一个结点的所有子树上的任何结点都是该结点的后裔。从根结点到某个结点的路径上的所有结点都是该结点的祖先。 结点的度：结点拥有的子树的数目。 叶子：度为零的结点。 分支结点：度不为零的结点。 树的度：树中结点的最大的度。 层次：根结点的层次为1，其余结点的层次等于该结点的双亲结点的层次加1。 树的高度：树中结点的最大层次。 无序树：如果树中结点的各子树之间的次序是不重要的，可以交换位置。 有序树：如果树中结点的各子树之间的次序是重要的, 不可以交换位置。 森林：0个或多个不相交的树组成。对森林加上一个根，森林即成为树；删去根，树即成为森林。 1.2 二叉树的定义二叉树是每个节点最多有两个子树（不存在度大于2的结点）的树结构。二叉树的子树有左右之分，次序不能颠倒。它有5种基本形态：二叉树可以是空集；根可以有空的左子树或右子树；或者左右子树皆为空。 1.3 二叉树的性质1.3.1 性质一二叉树的第i层至多有$2^{i-1}$个结点； 证明：下面用”数学归纳法”进行证明。 当$i=1$时，第$i$层的节点数目为$2^{i-1}=2^{0}=1$。因为第1层上只有一个根结点，所以命题成立。 假设当$i&gt;1$，第$i$层的节点数目为$2^{i-1}$。这个是根据(01)推断出来的！ 下面根据这个假设，推断出”第$(i+1)$层的节点数目为$2^{i}$”即可。由于二叉树的每个结点至多有两个孩子，故”第$(i+1)$层上的结点数目” 最多是 “第i层的结点数目的2倍”。即，第$(i+1)$层上的结点数目最大值$=2×2^{i-1}=2^{i}$。 故假设成立，原命题得证！ 1.3.2 性质二深度为k的二叉树至多有$2^{k-1}$个结点； 证明：在具有相同深度的二叉树中，当每一层都含有最大结点数时，其树中结点数最多。利用”性质1”可知，深度为k的二叉树的结点数至多为: $2^0+2^1+…+2^{k-1}=2^k-1$故原命题得证！ 1.3.3 性质三包含n个结点的二叉树的高度至少为$log_2 (n+1)$； 证明：根据”性质2”可知，高度为h的二叉树最多有2{h}–1个结点。反之，对于包含n个节点的二叉树的高度至少为$log_2(n+1)$。 1.3.4 性质四对任何一颗二叉树T，如果其终端结点数为$n_0$，度为2的结点数为$n_2$，则$n_0=n_2+1$ 证明：因为二叉树中所有结点的度数均不大于2，所以结点总数(记为n)=”0度结点数$(n_0)$” + “1度结点数$(n_1)$” + “2度结点数$(n_2)$”。由此，得到等式一。(等式一) $n=n_0+n_1+n_2$ 另一方面，0度结点没有孩子，1度结点有一个孩子，2度结点有两个孩子，故二叉树中孩子结点总数是：$n_1+2n_2$。 此外，只有根不是任何结点的孩子。故二叉树中的结点总数又可表示为等式二。 (等式二) $n=n1+2n_2+1$ 由(等式一)和(等式二)计算得到：n0=n2+1。原命题得证！ 1.4 满二叉树和完全二叉树1.4.1 满二叉树满二叉树的定义：除最后一层无任何子节点外，每一层上的所有结点都有两个子结点。也可以这样理解，除叶子结点外的所有结点均有两个子结点。节点数达到最大值，所有叶子结点必须在同一层上。 满二叉树的性质： 一棵树的深度为h，最大层数为k，深度与最大层数相同，k=h； 叶子数为$2^h$ 第k层的结点数是：$2^{k-1}$； 总结点数是$2^k-1$，且总节点数一定是奇数。 1.4.2 完全二叉树定义：一颗二叉树中，只有最小面两层结点的度可以小于2，并且最下一层的叶结点集中在靠左的若干位置上。这样的二叉树称为完全二叉树。 特点：叶子结点只能出现在最下层和次下层，且最小层的叶子结点集中在树的左部。显然，一颗满二叉树必定是一颗完全二叉树，而完全二叉树未必是满二叉树。 注意：完全二叉树是效率很高的数据结构，堆是一种完全二叉树或者近似完全二叉树，所以效率极高，像十分常用的排序算法、Dijkstra算法、Prim算法等都要用堆才能优化，二叉排序树的效率也要借助平衡树来提高，而平衡性基于完全二叉树。 二、二叉树的遍历【提问】 请分别写出并解释二叉树的先序、中序、后续遍历的递归与非递归版本 给定二叉树的先序跟后序遍历，能不能将二叉树重建：不能，因为先序为父节点-左节点-右节点，后序为左节点-右节点-父节点，两者的拓扑序列是一样的，所以无法建立；如果换成一棵二叉搜索树的后续能不能建立：可以，因为只要将遍历结果排序就可以得到中序结果。 这块内容讨论二叉树的常见遍历方式的代码（java）实现，包括前序（preorder）、中序（inorder）、后序（postorder）、层序（levelorder），进一步考虑递归和非递归的实现方式。递归的实现方法相对简单，但由于递归的执行方式每次都会产生一个新的方法调用栈，如果递归层级较深，会造成较大的内存开销，相比之下，非递归的方式则可以避免这个问题。递归遍历容易实现，非递归则没那么简单，非递归调用本质上是通过维护一个栈，模拟递归调用的方法调用栈的行为。 在此之前，先简单定义节点的数据结构： 二叉树节点最多只有两个儿子，并保存一个节点的值，为了实验的方便，假定它为 int。同时，我们直接使用 Java 的 System.out.print 方法来输出节点值，以显示遍历结果。 1234567891011class Node&#123; public int value; public Node left; public Node right; public Node(int v)&#123; this.value=v; this.left=null; this.right=null; &#125; &#125; 2.1 前序遍历2.1.1 递归实现递归实现很简单，在每次访问到某个节点时，先输出节点值，然后再依次递归的对左儿子、右儿子调用遍历的方法。代码如下 java 1234567public void preOrder(Node root)&#123; if(root!=null)&#123; System.out.print(root.value); preOrder(root.left); preOrder(root.right); &#125;&#125; 2.1.2 非递归实现利用栈实现循环先序遍历二叉树，维护一个栈，将根节点入栈，只要栈不为空，出栈并访问，接着依次将访问节点的右节点、左节点入栈。这种方式是对先序遍历的一种特殊实现，简洁明了，但是不具备很好地扩展性，在中序和后序方式中不适用。 1234567891011public void preOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; stack = new Stack&lt;Node&gt;(); stack.push(root); while(!stack.isEmpty)&#123; Node temp = stack.pop(); System.out.print(temp.value); if(temp.right!=null)stack.push(temp.right); if(temp.left!=null)stack.push(temp.left); &#125;&#125; 还有一种方式就是利用栈模拟递归过程实现循环先序遍历二叉树。这种方式具备扩展性，它模拟了递归的过程，将左子树不断的压入栈，直到null，然后处理栈顶节点的右子树。 java 12345678910111213public void preOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; s = new Stack&lt;Node&gt;(); while(root!=null||!s.isEmtpy())&#123; while(root!=null)&#123; System.out.print(root.value);、//先访问 s.push(root);//再入栈 root = root.left; &#125; root = s.pop(); root = root.right;//如果是null，出栈并处理右子树 &#125;&#125; 2.2 中序遍历2.2.1 递归实现1234567public void inOrder(Node root)&#123; if(root!=null)&#123; preOrder(root.left); System.out.print(root.value); preOrder(root.right); &#125;&#125; 2.2.2 非递归实现利用栈模拟递归过程实现循环中序遍历二叉树。跟前序遍历的非递归实现方法二很类似。唯一的不同是访问当前节点的时机：前序遍历在入栈前访问，而中序遍历在出栈后访问。 java 12345678910111213public void inOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; s = Stack&lt;Node&gt;(); while(root!=null||s.isEmpty())&#123; while(root!=null)&#123; s.push(root); root=root.left; &#125; root = s.pop(root); System.out.print(root.value); root = root.right; &#125;&#125; 2.3 后序遍历2.3.1 递归实现1234567public void inOrder(Node root)&#123; if(root!=null)&#123; preOrder(root.left); preOrder(root.right); System.out.print(root.value); &#125;&#125; 2.3.2 非递归实现1234567891011121314151617public void postOrder(Node root)&#123; if(root==null)return; Stack&lt;Node&gt; s1 = new Stack&lt;Node&gt;(); Stack&lt;Node&gt; s2 = new Stack&lt;Node&gt;(); Node node = root; s1.push(node); while(s1!=null)&#123;//这个while循环的功能是找出后序遍历的逆序，存在s2里面 node = s1.pop(); if(node.left!=null) s1.push(node.left); if(node.right!=null)s1.push(node.right); s2.push(node); &#125; while(s2!=null)&#123;//将s2中的元素出栈，即为后序遍历次序 node = s2.pop(); System.out.print(node.value); &#125;&#125; 2.4 层序遍历1234567891011public static void levelTravel(Node root)&#123; if(root==null)return; Queue&lt;Node&gt; q=new LinkedList&lt;Node&gt;(); q.add(root); while(!q.isEmpty())&#123; Node temp = q.poll(); System.out.println(temp.value); if(temp.left!=null)q.add(temp.left); if(temp.right!=null)q.add(temp.right); &#125; &#125; 总结一下：树的遍历主要有两种，一种是深度优先遍历，像前序、中序、后序；另一种是广度优先遍历，像层次遍历。在树结构中两者的区别还不是非常明显，但从树扩展到有向图，到无向图的时候，深度优先搜索和广度优先搜索的效率和作用还是有很大不同的。 深度优先一般用递归，广度优先一般用队列。一般情况下能用递归实现的算法大部分也能用堆栈来实现。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>树</tag>
        <tag>先序遍历</tag>
        <tag>中序遍历</tag>
        <tag>后序遍历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（2）：栈与队列]]></title>
    <url>%2F2017%2F07%2F18%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%882%EF%BC%89%EF%BC%9A%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[一、栈1.1 栈的定义栈（stack）是限定仅在表尾进行插入和删除操作的线性表。允许插入和删除的一端称为栈顶（top），另一端称为栈底，不含任何数据元素的栈称为空栈。它有以下几个特点： 栈中的数据是按照“后进先出（LIFO，Last In First Out）”方式进出栈的。 向栈中添加、删除数据时，只能从栈顶进行操作 栈通常包括的三种操作：push、peek、pop push：向栈中添加元素 peek：返回栈顶元素 pop：返回并删除栈顶元素的操作。 1.2 进栈与出栈下图为栈的示例图，栈中的数据依次为：$30=&gt;20=&gt;10$ 下图为出栈的示意图： 出栈前：栈顶元素是30。此时，栈中的元素依次是$30 = &gt; 20 =&gt; 10 $ 出栈后：30出栈之后，栈顶元素变成20。此时，栈中的元素依次是 $20 =&gt; 10$ 下图为入栈的示意图： 入栈前：栈顶元素是20。此时，栈中的元素依次是$20 =&gt; 10 $ 入栈后：40入栈之后，栈顶元素变成40。此时，栈中的元素依次是 $40 =&gt; 20 =&gt; 10$ 二、队列2.1 队列的定义队列（queue）是只允许在一段进行插入操作，而在另一端进行删除操作的线性表。允许插入的一段称为队尾，允许删除的一端称为队头。它有以下几个特点： 队列中数据是按照“先进先出（FIFO，First-In-First-Out）”方式进出队列的。 队列只允许在“队首”进行删除操作，而在“队尾”进行插入操作。 2.2 出队列和入队列下图为队列的示意图：队列中有10，20，30共3个数据。 下图为出队列的示意图： 出队列前：队首是10，队尾是30。 出队列后：出队列(队首)之后。队首是20，队尾是30。 下图为入队列的示意图： 入队列前：队首是20，队尾是30。 入队列后：40入队列(队尾)之后。队首是20，队尾是40。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>栈</tag>
        <tag>队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法（1）：数组与链表]]></title>
    <url>%2F2017%2F07%2F18%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%881%EF%BC%89%EF%BC%9A%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[线性表是一种线性结构，它是具有相同类型的n个数据元素组成的优先序列。介绍线性表的几个基本组成部分：数组、单向链表、双向链表。 一、数组数组有上界和下界，数组的元素在上下界内是连续的。 存储10，20，30，40，50的数组的示意图如下：数组的特点是：数据是连续的；随机访问速度块。 数组中稍微复杂一点的是多维数组和动态数组。对于C语言而言，多维数组本质上也是通过一维数组实现的。至于动态数组，是指数组的容量能动态增长的数组；对于C语言而言，若要提供动态数组，需要手动实现；而对于C++而言，STL提供了Vector；对于Java而言，Collection集合中提供了ArrayList和Vector。 数组是将元素在内存中连续存放，由于每个元素占用内存相同，可以通过下标迅速访问数组中任何元素。但是如果要在数组中增加一个元素，需要移动大量元素，在内存中空出一个元素的空间，然后将要增加的元素放在其中。同样的道理，如果想删除一个元素，同样需要移动大量元素去填掉被移动的元素。如果应用需要快速访问数据，很少或不插入和删除元素，就应该用数组。 二、单向链表2.1 定义单向链表（单链表）是链表的一种，它由节点组成，每个节点都包含下一个节点的指针。 单链表的示意图如下： 表头为空，表头的后继节点是“节点10”（数据为10的节点），“节点10”的后继节点是“节点20”（数据为20的结点）…… 2.2 单链表删除节点删除“节点30”删除之前：“节点20”的后继节点为“节点30”，而“节点30”的后继节点为“节点40”删除之后：“节点20”的后继节点为“节点40” 2.3 单链表添加节点在“节点10 ”与“节点20”之间添加“节点15” 添加之前：“节点10”的后继节点为“节点20”添加之后：“节点10”的后继节点为“节点15”，而“节点15”的后继节点为“节点20”。 单链表的特点是：节点的链接方向是单向的；相对于数组来说，单链表的随机访问速度较慢，但是单链表删除、添加数据的效率很高。 三、双向链表3.1 定义双向链表（双链表）是链表的一种。和单链表一样，双链表也是由节点组成，它的每一个数据节点中都有两个指针，分别指向直接后继和直接前驱。所以，从双向链表的任意一个节点开始，都可以很方便地访问它的前驱结点和后继节点。一般我们都构造双向循环链表。 双链表的示意图如下：表头为空，表头的后继节点为“节点10”（数据为10的结点）；“节点10”的后继节点是“节点20”（数据为10的节点），“节点20”的前继节点是“节点10”；“节点20”的后继节点是“节点30”，“节点30”的前继节点是“节点20”;……；末尾节点的后继节点是表头。 3.2 双链表删除节点删除“节点30” 删除之前：“节点20”的后继节点为“节点30”，“节点30”的前继节点为“节点20”。“节点30”的后继节点为“节点40”，“节点40”的前继节点为“节点30”删除之后：“节点20”的后继节点为“节点40”，“节点40”的前继节点为“节点20” 3.3 双链表添加节点在”节点10”与”节点20”之间添加”节点15”添加之前：”节点10”的后继节点为”节点20”，”节点20” 的前继节点为”节点10”。添加之后：”节点10”的后继节点为”节点15”，”节点15” 的前继节点为”节点10”。”节点15”的后继节点为”节点20”，”节点20” 的前继节点为”节点15”。 四、数组与链表的区别数组是将元素在内存中连续存放，由于每个元素占用内存相同，可以通过下标迅速访问数组中任何元素。但是如果要在数组中增加一个元素，需要移动大量元素，在内存中空出一个元素的空间，然后将要增加的元素放在其中。同样的道理，如果想删除一个元素，同样需要移动大量元素去填掉被移动的元素。如果应用需要快速访问数据，很少或不插入和删除元素，就应该用数组。 链表恰好相反，链表中的元素在内存中不是顺序存储的，而是通过存在元素中的指针联系到一起。比如：上一个元素有个指针指到下一个元素，以此类推，直到最后一个元素。如果要访问链表中一个元素，需要从第一个元素开始，一直找到需要的元素位置。但是增加和删除一个元素对于链表数据结构就非常简单了，只要修改元素中的指针就可以了。如果应用需要经常插入和删除元素你就需要用链表数据结构了。 C++语言中可以用数组处理一组数据类型相同的数据，但不允许动态定义数组的大小，即在使用数组之前必须确定数组的大小。而在实际应用中，用户使用数组之前有时无法准确确定数组的大小，只能将数组定义成足够大小，这样数组中有些空间可能不被使用，从而造成内存空间的浪费。链表是一种常见的数据组织形式，它采用动态分配内存的形式实现。需要时可以用new分配内存空间，不需要时用delete将已分配的空间释放，不会造成内存空间的浪费。 (1) 从逻辑结构角度来看 a：数组必须事先定义固定的长度（元素个数），不能适应数据动态地增减的情况。当数据增加时，可能超出原先定义的元素个数；当数据减少时，造成内存浪费。 b：链表动态地进行存储分配，可以适应数据动态地增减的情况，且可以方便地插入、删除数据项。（数组中插入、删除数据项时，需要移动其它数据项） (2)从内存存储角度来看 a：(静态)数组从栈中分配空间, 对于程序员方便快速,但自由度小。 b：链表从堆中分配空间, 自由度大但申请管理比较麻烦. 总结 数组静态分配内存，链表动态分配内存；数组在内存中连续，链表不连续；数组元素在栈区，链表元素在堆区；数组利用下标定位，时间复杂度为O(1)，链表定位元素时间复杂度O(n)；数组插入或删除元素的时间复杂度O(n)，链表的时间复杂度O(1)。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
        <tag>链表</tag>
        <tag>数组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（27）：Isolation Forest]]></title>
    <url>%2F2017%2F07%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8827%EF%BC%89%EF%BC%9AIsolation%20Forest%2F</url>
    <content type="text"><![CDATA[“An outlier is an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism.” — D. M. Hawkins, Identification of Outliers, Chapman and Hall, 1980. 异常检测 (anomaly detection)，或者又被称为“离群点检测” (outlier detection)，是机器学习研究领域中跟现实紧密联系、有广泛应用需求的一类问题。但是，什么是异常，并没有标准答案，通常因具体应用场景而异。如果要给一个比较通用的定义，很多文献通常会引用 Hawkins 在文章开头那段话。很多后来者的说法，跟这个定义大同小异。这些定义虽然笼统，但其实暗含了认定“异常”的两个标准或者说假设： 1）异常数据跟样本中大多数数据不太一样。 2）异常数据在整体数据样本中占比比较小。 为了刻画异常数据的“不一样”，最直接的做法是利用各种统计的、距离的、密度的量化指标去描述数据样本跟其他样本的疏离程度。而 Isolation Forest (Liu et al. 2011) 的想法要巧妙一些，它尝试直接去刻画数据的“疏离”(isolation)程度，而不借助其他量化指标。Isolation Forest 因为简单、高效，在学术界和工业界都有着不错的名声。 一、简介我们先用一个简单的例子来说明 Isolation Forest 的基本想法。假设现在有一组一维数据（如下图所示），我们要对这组数据进行随机切分，希望可以把点 A 和点 B 单独切分出来。具体的，我们先在最大值和最小值之间随机选择一个值 x，然后按照 =x 可以把数据分成左右两组。然后，在这两组数据中分别重复这个步骤，直到数据不可再分。显然，点 B 跟其他数据比较疏离，可能用很少的次数就可以把它切分出来；点 A 跟其他数据点聚在一起，可能需要更多的次数才能把它切分出来。 我们把数据从一维扩展到两维。同样的，我们沿着两个坐标轴进行随机切分，尝试把下图中的点A’和点B’分别切分出来。我们先随机选择一个特征维度，在这个特征的最大值和最小值之间随机选择一个值，按照跟特征值的大小关系将数据进行左右切分。然后，在左右两组数据中，我们重复上述步骤，再随机的按某个特征维度的取值把数据进行细分，直到无法细分，即：只剩下一个数据点，或者剩下的数据全部相同。跟先前的例子类似，直观上，点B’跟其他数据点比较疏离，可能只需要很少的几次操作就可以将它细分出来；点A’需要的切分次数可能会更多一些。按照先前提到的关于“异常”的两个假设，一般情况下，在上面的例子中，点B和点B’ 由于跟其他数据隔的比较远，会被认为是异常数据，而点A和点A’ 会被认为是正常数据。直观上，异常数据由于跟其他数据点较为疏离，可能需要较少几次切分就可以将它们单独划分出来，而正常数据恰恰相反。这其实正是 Isolation Forest（IF）的核心概念。IF采用二叉树去对数据进行切分，数据点在二叉树中所处的深度反应了该条数据的“疏离”程度。整个算法大致可以分为两步： 训练：抽取多个样本，构建多棵二叉树（Isolation Tree，即 iTree）； 预测：综合多棵二叉树的结果，计算每个数据点的异常分值。 训练： 构建一棵 iTree 时，先从全量数据中抽取一批样本，然后随机选择一个特征作为起始节点，并在该特征的最大值和最小值之间随机选择一个值，将样本中小于该取值的数据划到左分支，大于等于该取值的划到右分支。然后，在左右两个分支数据中，重复上述步骤，直到满足如下条件： 1）数据不可再分，即：只包含一条数据，或者全部数据相同。 2）二叉树达到限定的最大深度。 预测： 计算数据 x 的异常分值时，先要估算它在每棵 iTree 中的路径长度（也可以叫深度）。具体的，先沿着一棵 iTree，从根节点开始按不同特征的取值从上往下，直到到达某叶子节点。假设 iTree 的训练样本中同样落在 x 所在叶子节点的样本数为 T.size，则数据 x 在这棵 iTree 上的路径长度 h(x)，可以用下面这个公式计算： h(x)=e+C(T.size)公式中，$e$表示数据$x$从$iTree$的根节点到叶节点过程中经过的边的数目，$C(T.size)$ 可以认为是一个修正值，它表示在一棵用 $T.size$ 条样本数据构建的二叉树的平均路径长度。一般的，$C(n)$的计算公式如下： C(n)=2H(n-1)-\frac{2(n-1)}{n}其中，$H(n-1)$可用$ln(n-1)+0.5772156649$估算，这里的常数是欧拉常数。数据 x 最终的异常分值 Score(x) 综合了多棵 iTree 的结果： Score(x)=2^{-\frac{E[h(x)]}{C(\psi)}}公式中，$E(h(x)) $表示数据 x 在多棵 iTree 的路径长度的均值，$\psi$表示单棵 iTree 的训练样本的样本数，$C(\psi)$ 表示用$\psi$条数据构建的二叉树的平均路径长度，它在这里主要用来做归一化。 从异常分值的公式看，如果数据 x 在多棵 iTree 中的平均路径长度越短，得分越接近 1，表明数据 x 越异常；如果数据 x 在多棵 iTree 中的平均路径长度越长，得分越接近 0，表示数据 x 越正常；如果数据 x 在多棵 iTree 中的平均路径长度接近整体均值，则打分会在 0.5 附近。 二、算法特点在论文中，也比较了其它的常用异常挖掘的算法。比如常用的统计方法，基于分类的方法，和基于聚类的方法，这些传统算法通常是对正常的数据构建一个模型，然后把不符合这个模型的数据，认为是异常数据。而且，这些模型通常为正常数据作优化，而不是为异常数据作优化。而iForest可以显示地找出异常数据，而不用对正常的数据构建模型。 由于异常数据的两个特征（少且不同： few and different）：异常数据只占很少量;异常数据特征值和正常数据差别很大。 异常数据的这两个特征，确定了算法的理论基础。因此，构建二叉树型结构的时候，异常数据离根更近，而正常数据离根更远，更深。算法为了效率考虑，也限定了树的深度：ceil(log2(n))，这个值近似于树的平均深度，因为只需要关心那些低于平均高度的数据点，而不需要树进行完全生成。 算法只需要两个参数：树的多少与采样的多少。实验发现，在100颗树的时候，路径的长度就已经覆盖得比较好了，因此选100颗也就够了。采样，是为了更好的将正常数据和异常数据分离开来。有别于其它模型，采样数据越多，反面会降低iForest识别异常数据的能力。因为，通常使用256个样本，这也是scikit-learn实现时默认使用的采样数。 由于算法只需要采样数据256条样本，并且对树的深度也有限制，因此，算法对内存要求很低，且处理速度很快，其时间复杂度也是线性的。 不像其它算法，需要计算距离或者密度来寻找异常数据，iForest算法可以很好的处理高维数据和大数据，并且也可以作为在线预测。假设采样为256条，结点最大为511个，假设一个节点占b字节，共使用t颗树，那么需要的内存只有511tb字节，基本上只需要几M到几十M的内存就够了。数据还显示，预测287,748条数据只花了7.6秒。 另外，iForest既能发现群异常数据，也能发现散点异常数据。同时也能处理训练数据中不包含异常数据的情况。 三、代码示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 print(__doc__)import numpy as npimport matplotlib.pyplot as pltfrom sklearn.ensemble import IsolationForestrng = np.random.RandomState(42)# Generate train dataX = 0.3 * rng.randn(100, 2)X_train = np.r_[X + 2, X - 2]# Generate some regular novel observationsX = 0.3 * rng.randn(20, 2)X_test = np.r_[X + 2, X - 2]# Generate some abnormal novel observationsX_outliers = rng.uniform(low=-4, high=4, size=(20, 2))# fit the modelclf = IsolationForest(max_samples=100, random_state=rng)clf.fit(X_train)y_pred_train = clf.predict(X_train)y_pred_test = clf.predict(X_test)y_pred_outliers = clf.predict(X_outliers)# plot the line, the samples, and the nearest vectors to the planexx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])Z = Z.reshape(xx.shape)plt.title("IsolationForest")plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=20, edgecolor='k')b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green', s=20, edgecolor='k')c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red', s=20, edgecolor='k')plt.axis('tight')plt.xlim((-5, 5))plt.ylim((-5, 5))plt.legend([b1, b2, c], ["training observations", "new regular observations", "new abnormal observations"], loc="upper left")plt.show() 算法基本上不需要配置参数就可以直接使用，通常就以下几个(参数明显比随机森林简单)： n_estimators: 默认为100，配置iTree树的多少 max_samples: 默认为265，配置采样大小 max_features: 默认为全部特征，对高维数据，可以只选取部分特征]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>iForest</tag>
        <tag>异常检测算法</tag>
        <tag>集成算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（26）：因子分解机（FM）与场感知分解机（FFM）]]></title>
    <url>%2F2017%2F07%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8826%EF%BC%89%EF%BC%9A%E5%9B%A0%E5%AD%90%E5%88%86%E8%A7%A3%E6%9C%BA%EF%BC%88FM%EF%BC%89%E4%B8%8E%E5%9C%BA%E6%84%9F%E7%9F%A5%E5%88%86%E8%A7%A3%E6%9C%BA%EF%BC%88FFM%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本文转载自美团点评技术团队 FM和FFM模型是最近几年提出的模型，凭借其在数据量比较打并且特征稀疏的情况下，忍让能够得到优秀的性能和效果，屡次在各大公司举办的CTR预估比赛中获得不错的战绩。 在计算广告领域，点击率CTR（click-through rate）和转化率CVR（conversion rate）是衡量广告流量的两个关键指标。准确的估计CTR、CVR对于提高流量的价值，增加广告收入有重要的指导作用。预估CTR、CVR，业界常用的方法由人工特征工程+LR（Logistic Regression）、GBDT（Gradient Boosting Decision Tree）+LR、FM（Factorization Machine）和FFM（Field-aware Factorization Machine）模型。在这些模型中，FM和FFM近年来表现突出，分别在Criteo和Avazu举办的CTR预测竞赛中夺得冠军。 本文基于对FFM模型的深度调研和使用经验，从原理、实现和应用几个方面对FFM进行探讨，希望能够从原理上解释FFM模型在点击率预估上取得优秀效果的原因。因为FFM是在FM的基础上改进得来的，所以，我们首先引入FM模型。 一、FM（因子分解机）1.1 FM的原理及推导因子分解机（Factorization Machine，简称FM），又称分解机。是由德国康斯坦茨大学的Steffen Rendle（现任职于Google）于2010年最早提出的，旨在解决大规模稀疏数据下的特征组合问题。在系统介绍FM之前，先了解一下在实际场景中，稀疏数据是怎样产生的。 假设一个广告分类的问题，根据用户和广告位相关的特征，预测用户是否点击了广告。元数据如下： Clicked? Country Day Ad_type 1 USA 26/11/15 Movie 0 China 1/7/14 Game 1 China 19/2/15 Game “Clicked？”是label，Country、Day、Ad_type是特征。由于三种特征都是categorical类型的，需要经过独热编码（One-Hot Encoding）转换成数值型特征。 Clicked? Country=USA Country=China Day=26/11/15 Day=1/7/14 Day=19/2/15 Ad_type=Movie Ad_type=Game 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 由上表可以看出，经过One-Hot编码之后，大部分样本数据特征是比较稀疏的。上面的样例中，每个样本有7维特征，但平均仅有3维特征具有非零值。实际上，这种情况并不是此例独有的，在真实应用场景中这种情况普遍存在。例如，CTR/CVR预测时，用户的性别、职业、教育水平、品类偏好、商品的品类等，经过One-Hot编码转换后都会导致样本数据的稀疏性。特别是商品品类这种类型的特征，如商品的末级品类约有550个，采用One-Hot编码生成550个数值特征，但每个样本的这550个特征，有且仅有一个是有效的（非零）。由此可见，数据稀疏性是实际问题中不可避免的挑战。 One-Hot编码的另一个特点就是导致特征空间大。例如，商品品类有550维特征，一个categorical特征转换为550维数值特征，特征空间剧增。 同时通过观察大量的样本数据可以发现，某些特征经过关联之后，与label之间的相关性就会提高。如：“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征，对用户的点击有着正向的影响。换句话说，来自“China”的用户很可能会在“Chinese New Year”有大量的浏览、购买行为，而在“Thanksgiving”却不会有特别的消费行为。这种关联特征与label的正向相关性在实际问题中是普遍存在的，如“化妆品”类商品与“女”性，“球类运动配件”的商品与“男”性，“电影票”的商品与“电影”品类偏好等。因此，引入两个特征的组合是非常有意义的。 表示特征之间的关联，最直接的方法的是构造组合特征。样本中特征之间的关联信息在one-hot编码和浅层学习模型（如LR、SVM）是做不到的。目前工业界主要有两种手段得到组合特征： 1）人工特征工程（数据分析＋人工构造）； 2）通过模型做组合特征的学习（深度学习方法、FM/FFM方法） 本章主要讨论FM和FFM用来学习特征之间的关联。多项式模型是包含特征组合的最直观的模型。在多项式模型中，特征 $x_i$ 和 $x_j$ 的组合采用 $x_i$ 表示，即 $x_i$ 和 $x_j$ 都非零时，组合特征 $x_ix_j$ 才有意义。从对比的角度，本文只讨论二阶多项式模型。模型的表达式如下： y(x)=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^nw_{ij}x_ix_j其中，$n$代表样本的特征数量，$x_i$是第$i$个特征的值，$w_0、w_i、w_{ij}$是模型的参数。 从这个公式可以看出，组合特征的参数一共有$\frac{n(n-1)}{2}$个，任意两个参数都是独立的。然而，在数据稀疏性普遍存在的实际应用场景中，二次项参数的训练是很困难的。其原因是，回归模型的参数$w$的学习结果就是从训练样本中计算充分统计量（凡是符合指数族分布的模型都具有此性质），而在这里交叉项的每一个参数$w_{ij}$的学习过程需要大量的$x_i$、$x_j$同时非零的训练样本数据。由于样本数据本来就很稀疏，能够满足“$x_i$和$x_j$都非零”的样本数就会更少。训练样本不充分，学到的参数$w_{ij}$就不是充分统计量结果，导致参数$w_{ij}$不准确，而这会严重影响模型预测的效果（performance）和稳定性。 那么，如何解决二次项参数的训练问题呢？矩阵分解提供了一种解决思路。在Model-based的协同过滤中，一个rating矩阵可以分解为user矩阵和item矩阵，每个user和item都可以采用一个隐向量表示。比如在下图中的例子，我们把每个user表示成一个二维向量，同时把每个item表示成一个二维向量，两个向量点积就是矩阵中user对item的打分。 类似地，所有二次项参数 $w_{ij}$可以组成一个对称阵 $W$（为了方便说明FM的由来，对角元素可以设置为正实数），那么这个矩阵就可以分解为 $W=V^TV$，$V$ 的第$ j$列便是第 $j$ 维特征的隐向量。换句话说，每个参数 $w_{ij}=⟨v_i,v_j⟩$，这就是FM模型的核心思想。因此，FM的模型方程为（本文不讨论FM的高阶形式） y(x)=w_0+\sum _{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n⟨vi,vj⟩x_ix_j \ \ \ \ \ \ ···（2）其中，$v_i$是第i维特征的隐向量，$⟨⋅,⋅⟩$代表向量点积，计算公式为 ⟨v_i,v_j⟩=\sum_{f=1}^kv_{i,f}·v_{j,f}隐向量的长度为$k(k&lt;&lt;n)$，包含k个描述特征的因子。具体解读一下这个公式 线性模型+交叉项：直观地看FM模型表达式，前两项是线性回归模型的表达式，最后一项是二阶特征交叉项（又称组合特征项），表示模型将两个互异的特征分量之间的关联信息考虑进来。用交叉项表示组合特征，从而建立特征与结果之间的非线性关系。 交叉项系数 → 隐向量内积：由于FM模型是在线性回归基础上加入了特征交叉项，模型求解时不直接求特征交叉项的系数$w_{ij}$（因为对应的组合特征数据稀疏，参数学习不充分），故而采用隐向量的内积$⟨v_i,v_j⟩$表示$w_{ij}$。具体的，FM求解过程中的做法是：对每一个特征分量$x_i$引入隐向量$v_i＝(v_{i,1},v_{i,2},⋯,v_{i,k})$，利用$v_iv^T_j$内积结果对交叉项的系数$w_{ij}$进行估计，公式表示：$ŵ_{ij}=v_iv^T_j$ 根据上式，二次项的参数数量减少为$kn$个，远少于多项式模型的参数数量。 此外，参数因子化表示后，使得$x_hx_i$的参数与$x_ix_j$的参数不再相互独立。这样我们就可以在样本系数的情况下相对合理地估计FM模型交叉项的参数。具体地： ⟨v_h,v_i⟩=\sum_{f=1}^k v_{h,f}·v_{i,f}⟨v_i,v_j⟩=\sum_{f=1}^k v_{i,f}·v_{j,f}$x_hx_i$与$x_ix_j$的系数分别为$⟨v_h,v_i⟩$和$⟨v_i,v_j⟩$，它们之间有共同项$v_i$，也就是说，所有包含$x_i$的非零组合特征（存在某个$j≠i$,使得$x_ix_j≠0$）的样本都可以用来学习隐向量$v_i$，这在很大程度上避免了数据系数行造成参数估计不准确的影响。而在多项式模型中，$w_{hi}$和$w_{ij}$是相互独立的。 显而易见，公式(2)是一个通用的拟合方程，可以采用不同的损失函数用于解决回归、二元分类等问题，比如可以采用MSE（Mean Square Error）损失函数来求解回归问题，也可以采用Hinge、Cross-Entropy损失来求解分类问题。当然，在进行二元分类时，FM的输出需要经过Sigmoid变换，这与Logistic回归是一样的。 FM应用场景 损失函数 说明 回归 均方误差（MSE）损失 Mean Square Error，与平方误差类似 二类分类 Hinge/Cross-Entopy损失 分类时，结果需要做sigmoid变换 排序 直观上看，FM的复杂度是$O(kn^2)$，但是，通过下面的等价转换，可以将FM的二次项化简，其复杂度可以优化到$O(kn)$，即： \sum_{i=1}^n\sum_{j=i+1}^n⟨v_i,v_j⟩x_i,x_j=\frac{1}{2}\sum_{f=1}^k[(\sum_{i=1}^nv_{i,f}x_i)^2-\sum_{i=1}^nv_{i,f}^2x_i^2]下面给出详细推导： \sum_{i=1}^n\sum_{j=i+1}^n⟨v_i,v_j⟩x_ix_j \\ =\frac{1}{2}\sum_{i=1}^n\sum_{f=1}^n⟨v_i,v_j⟩x_ix_j-\frac{1}{2}\sum_{i=1}^n⟨v_i,v_i⟩x_ix_i\\=\frac{1}{2}(\sum_{i=1}^n\sum_{j=1}^n\sum_{f=1}^kv_{i,f}v_{j,f}x_ix_j-\sum_{i=1}^n\sum_{f=1}^kv_{i,f}v_{i,f}x_ix_i)\\=\frac{1}{2}\sum_{f=1}^k[(\sum_{i=1}^nv_{i,f}x_i)·(\sum_{j=1}^nv_{j,f}x_j)-\sum_{i=1}^nv_{i,f}^2x_i^2]\\=\frac{1}{2}\sum_{f=1}^k[(\sum_{i=1}^nv_{i,f}x_i)^2- \sum_{i=1}^nv_{i,f}^2x_i^2]解读第一步到第二部，这里用A表示系数矩阵V的上三角元素，B表示对角线上的交叉项系数。由于系数矩阵V是一个对称阵，所以下三角和上三角相等，有下式成立： A=\frac{1}{2}(2A+B)-\frac{1}{2}B其中， A=\sum_{i=1}^n\sum_{j=i+1}^n⟨v_i,v_j⟩x_ix_j,B=\sum_{i=1}^n⟨v_i,v_j⟩x_ix_i如果用随机梯度下降（SGD）法学系模型参数。那么模型各个参数的梯度如下： \frac{\partial}{\partial\theta}y\left(x\right)=\left\{\begin{array}{l} 1,\ \ if\ \theta\ is\ w_0\left(\textrm{常数项}\right)\\ x_i,\ if\ \theta\ is\ w_i\left(\textrm{线性项}\right)\\ x_i\underset{j=1}{\overset{n}{\varSigma}}v_{j,f}x_j-v_{i,f}x_{i}^{2},\ if\ \theta\ is\ v_{i,f}\left(\textrm{交叉项}\right)\\ \end{array}\right.其中，$v_{j,f}$是隐向量$v_j$的第f个元素。 由于$\underset{j=1}{\overset{n}{\varSigma}}v_{j,f}x_j$只与f有关，在参数迭代过程中，只需要计算第一次所有f的$\underset{j=1}{\overset{n}{\varSigma}}v_{j,f}x_j$，就能够方便地得到所有$v_{i,f}$的梯度。显然，计算所有f的$\underset{j=1}{\overset{n}{\varSigma}}v_{j,f}x_j$的复杂度是$O(kn)$；已知$\underset{j=1}{\overset{n}{\varSigma}}v_{j,f}x_j$时，计算每个参数梯度的复杂度是$O(n)$；得到梯度后，更新每个参数的复杂度是$O(1)$；模型参数一共有$nk+n+1$个。因此，FM参数训练的时间复杂度为$O(kn )$ 1.2 FM的优势综上可知，FM算法可以再线性时间内完成模型训练，以及对新样本作出预测，所以说FM是一个非常高效的模型。FM模型的核心作用可以概括为以下三个： 1）FM降低了交叉项参数学习不充分的影响：one-hot编码后的样本数据非常稀疏，组合特征更是如此。为了解决交叉项参数学习不充分、导致模型有偏或不稳定的问题。作者借鉴矩阵分解的思路：每一维特征用k维的隐向量表示，交叉项的参数$w_ij$用对应特征隐向量的内积表示，即$⟨v_i,v_j⟩$。这样参数学习由之前学习交叉项参数$w_{ij}$的过程，转变为学习$n$个单特征对应k维隐向量的过程。很明显，单特征参数（k维隐向量$v_i$）的学习要比交叉项参数$w_{ij}$学习的更加充分。示例说明：假如有10w条训练样本，其中出现女性特征的样本数为3w，出现男性特征的样本数为7w，出现汽车特征的样本数为2000，出现化妆品的样本数为1000。特征共现的样本数如下： 共现交叉特征 样本数 注 &lt;女性，汽车&gt; 500 同时出现&lt;女性，汽车&gt;的样本数 &lt;女性，化妆品&gt; 1000 同时出现&lt;女性，化妆品&gt;的样本数 &lt;男性，汽车&gt; 1500 同时出现&lt;男性，汽车&gt;的样本数 &lt;男性，化妆品&gt; 0 样本中无此特征组合项 &lt;女性，汽车&gt;的含义是女性看汽车广告。可以看到，但特征对应的样本数远大于组合特征对应的样本数。训练时，但特征参数相比交叉项特征参数会学习地更充分。因此，可以说FM降低了因数据稀疏，导致交叉项参数学习不充分的影响。 2）FM提升了模型预估能力。依然看上面的示例，样本中没有没有&lt;男性，化妆品&gt;交叉特征，即没有男性看化妆品广告的数据。如果yoga多项式模型来建模，对应的交叉项参数$w_{男性，化妆品}$是学不出来的，因为数据中没有对应的共现交叉特征。那么多项式模型就不能对出现的男性看化妆品广告场景给出准确地预估。FM模型是否能得到交叉项参数$w_{男性，化妆品}$呢？答案是肯定的。由于FM模型是把交叉项参数用对应的特征隐向量内积表示，这里表示为$w_{男性，化妆品}=$，即用男性特征隐向量$v_{男性}$和化妆品特征隐向量$v_{化妆品}$的内积表示交叉项参数$w_{男性，化妆品}$由于FM学习的参数就是单特征的隐向量，那么男性看化妆品广告的预估结果可以用$$得到。这样，即便训练集中没有出现男性看化妆品广告的样本，FM模型仍然可以用来预估，提升了预估呢不给力。 3）FM提升了参数学习效率：这个显而易见，参数个数由$(n2+n+1)(n2+n+1)$变为$(nk+n+1)(nk+n+1)$个，模型训练复杂度也由$O(mn^2)$变为$O(mnk)$。mm为训练样本数。对于训练样本和特征数而言，都是线性复杂度。此外，就FM模型本身而言，它是在多项式模型基础上对参数的计算做了调整，因此也有人把FM模型称为多项式的广义线性模型，也是恰如其分的。从交互项的角度看，FM仅仅是一个可以表示特征之间交互关系的函数表法式，可以推广到更高阶形式，即将多个互异特征分量之间的关联信息考虑进来。例如在广告业务场景中，如果考虑User-Ad-Context三个维度特征之间的关系，在FM模型中对应的degree为3。 最后一句话总结，FM最大特点和优势：FM模型对稀疏数据有更好的学习能力，通过交互项可以学习特征之间的关联关系，并且保证了学习效率和预估能力。 与其他模型相比，它的优势如下： FM是一种比较灵活的模型，通过合适的特征变换方式，FM可以模拟二阶多项式核的SVM模型、MF模型、SVD++模型等； 相比SVM的二阶多项式核而言，FM在样本稀疏的情况下是有优势的；而且，FM的训练/预测复杂度是线性的，而二项多项式核SVM需要计算核矩阵，核矩阵复杂度就是N平方。 相比MF而言，我们把MF中每一项的rating分改写为 $r_{ui}∼β_u+γ_i+x^T_uy_i$，从公式(2)中可以看出，这相当于只有两类特征 $u$ 和$ i$ 的FM模型。对于FM而言，我们可以加任意多的特征，比如user的历史购买平均值，item的历史购买平均值等，但是MF只能局限在两类特征。SVD++与MF类似，在特征的扩展性上都不如FM，在此不再赘述。 二、FFM（场感知分解机器）2.1 FFM的原理及推导场感知分解机器（Field-aware Factorization Machine ，简称FFM）最初的概念来自Yu-Chin Juan(阮毓钦，毕业于中国台湾大学，现在美国Criteo工作)与其比赛队员，是他们借鉴了来自Michael Jahrer的论文中的field概念提出了FM的升级版模型。通过引入field的概念，FFM把相同性质的特征归于同一个field。以上面的广告分类为例，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中。同理，商品的末级品类编码生成了550个特征，这550个特征都是说明商品所属的品类，因此它们也可以放到同一个field中。简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户性别、职业、品类偏好等。在FFM中，每一维特征 $x_i$，针对其它特征的每一种field $f_j$，都会学习一个隐向量 $v_{i,f_j}$。因此，隐向量不仅与特征相关，也与field相关。也就是说，“Day=26/11/15”这个特征与“Country”特征和“Ad_type”特征进行关联的时候使用不同的隐向量，这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来。 假设样本的 nn 个特征属于 ff 个field，那么FFM的二次项有 nfnf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型。根据FFM的field敏感特性，可以导出其模型方程。 y(x)=w_0+∑_{i=1}^nw_ix_i+∑_{i=1}^n∑_{j=i+1}^n⟨v_{i,fj},v_{j,f_i}⟩x_ix_j其中，$f_j$是第j个特征所属的field。如果隐向量的长度为k，那么FFM的二次参数有nfk个，远多于FM模型的nk个。此外，由于隐向量与field相关，FFM二次项并不能够化简，其复杂度为$O(kn^2)$。 下面以一个例子简单说明FFM的特征组合方式。输入记录如下 User Movie Genre Price YuChin 3Idiots Comedy, Drama $9.99 这条记录可以编码成5个特征，其中“Genre=Comedy”和“Genre=Drama”属于同一个field，“Price”是数值型，不用One-Hot编码转换。为了方便说明FFM的样本格式，我们将所有的特征和对应的field映射成整数编号。 Field name Field index Feature name Feature index User 1 User=YuChin 1 Movie 2 Movie=3Idiots 2 Genre 3 Genre=Comedy 3 Genre=Drama 4 Price 4 Price 5 那么，FFM的组合特征有10项，如下图所示。 其中，红色表示Field编码，蓝色表示Feature编码，绿色表示样本的组合特征取值（离散化后的结果）。二阶交叉项的系数是通过与Field相关的隐向量的内积得到的。如果单特征有n个，全部做二阶特征组合的话，会有$C^2_n=\frac{n(n−1)}{2}$个。 2.2 FFM的应用在DSP的场景中，FFM主要用来预估站内的CTR和CVR，即一个用户对一个商品的潜在点击率和点击后的转化率。 CTR和CVR预估模型都是在线下训练，然后用于线上预测。两个模型采用的特征大同小异，主要有三类：用户相关的特征、商品相关的特征、以及用户-商品匹配特征。用户相关的特征包括年龄、性别、职业、兴趣、品类偏好、浏览/购买品类等基本信息，以及用户近期点击量、购买量、消费额等统计信息。商品相关的特征包括所属品类、销量、价格、评分、历史CTR/CVR等信息。用户-商品匹配特征主要有浏览/购买品类匹配、浏览/购买商家匹配、兴趣偏好匹配等几个维度。 为了使用FFM方法，所有的特征必须转换成“field_id:feat_id:value”格式，field_id代表特征所属field的编号，feat_id是特征编号，value是特征的值。数值型的特征比较容易处理，只需分配单独的field编号，如用户评论得分、商品的历史CTR/CVR等。categorical特征需要经过One-Hot编码成数值型，编码产生的所有特征同属于一个field，而特征的值只能是0或1，如用户的性别、年龄段，商品的品类id等。除此之外，还有第三类特征，如用户浏览/购买品类，有多个品类id且用一个数值衡量用户浏览或购买每个品类商品的数量。这类特征按照categorical特征处理，不同的只是特征的值不是0或1，而是代表用户浏览或购买数量的数值。按前述方法得到field_id之后，再对转换后特征顺序编号，得到feat_id，特征的值也可以按照之前的方法获得。 CTR、CVR预估样本的类别是按不同方式获取的。CTR预估的正样本是站内点击的用户-商品记录，负样本是展现但未点击的记录；CVR预估的正样本是站内支付（发生转化）的用户-商品记录，负样本是点击但未支付的记录。构建出样本数据后，采用FFM训练预估模型，并测试模型的性能。 #(field) #(feature) AUC Logloss 站内CTR 39 2456 0.77 0.38 站内CVR 67 2441 0.92 0.13 由于模型是按天训练的，每天的性能指标可能会有些波动，但变化幅度不是很大。这个表的结果说明，站内CTR/CVR预估模型是非常有效的。 在训练FFM的过程中，有许多小细节值得特别关注。 第一，样本归一化。FFM默认是进行样本数据的归一化，即 pa.normpa.norm 为真；若此参数设置为假，很容易造成数据inf溢出，进而引起梯度计算的nan错误。因此，样本层面的数据是推荐进行归一化的。 第二，特征归一化。CTR/CVR模型采用了多种类型的源特征，包括数值型和categorical类型等。但是，categorical类编码后的特征取值只有0或1，较大的数值型特征会造成样本归一化后categorical类生成特征的值非常小，没有区分性。例如，一条用户-商品记录，用户为“男”性，商品的销量是5000个（假设其它特征的值为零），那么归一化后特征“sex=male”（性别为男）的值略小于0.0002，而“volume”（销量）的值近似为1。特征“sex=male”在这个样本中的作用几乎可以忽略不计，这是相当不合理的。因此，将源数值型特征的值归一化到 [0,1][0,1] 是非常必要的。 第三，省略零值特征。从FFM模型的表达式可以看出，零值特征对模型完全没有贡献。包含零值特征的一次项和组合项均为零，对于训练模型参数或者目标值预估是没有作用的。因此，可以省去零值特征，提高FFM模型训练和预测的速度，这也是稀疏样本采用FFM的显著优势。 2.3 FFM实现Yu-Chin Juan实现了一个C++版的FFM模型，源码可从Github下载[10]。这个版本的FFM省略了常数项和一次项，模型方程如下。 ϕ(w,x)=∑_{j1,j2∈C_2}⟨w_{j_1,f_2},w_{j_2,f_1}⟩x_{j_1}x_{j_2}其中，$C_2$是非零特征的二元组合，$j_1$是特征，属于field $f_1$，$w_{j_1,f_2}$是特征 $j_1$对field $f_2$ 的隐向量。此FFM模型采用logistic loss作为损失函数，和L2惩罚项，因此只能用于二元分类问题。 \underset{w}{min}∑_{i=1}^Llog(1+exp{−y_iϕ(w,x_i)})+\frac{λ}{2}‖w‖2其中，$y_i∈{−1,1}$是第 i个样本的label，L是训练样本数量，λ 是惩罚项系数。模型采用SGD优化，优化流程如下。参考 Algorithm1, 下面简单解释一下FFM的SGD优化过程。算法的输入 tr、va、pa 分别是训练样本集、验证样本集和训练参数设置。 根据样本特征数量（tr.ntr.n）、field的个数（tr.mtr.m）和训练参数（papa），生成初始化模型，即随机生成模型的参数； 如果归一化参数 pa.normpa.norm 为真，计算训练和验证样本的归一化系数，样本i的归一化系数为R[i]=\frac{1}{||X[i]||} 对每一轮迭代，如果随机更新参数 pa.randpa.rand 为真，随机打乱训练样本的顺序； 对每一个训练样本，执行如下操作: 计算每一个样本的FFM项，即公式中的输出 $ϕ$； 计算每一个样本的训练误差，如算法所示，这里采用的是交叉熵损失函数 $log(1+eϕ)$； 利用单个样本的损失函数计算梯度 $gΦ$，再根据梯度更新模型参数； 对每一个验证样本，计算样本的FFM输出，计算验证误差； 重复步骤3~5，直到迭代结束或验证误差达到最小。 在SGD寻优时，代码采用了一些小技巧，对于提升计算效率是非常有效的。 第一，梯度分步计算。采用SGD训练FFM模型时，只采用单个样本的损失函数来计算模型参数的梯度。 L=L_{err}+L_{reg}=log(1+exp\{−y_iϕ(w,x_i)\})+\frac{λ}{2}‖w‖^2\frac{∂L}{∂w}=\frac{∂L_{err}}{∂ϕ}\frac{∂ϕ}{∂w}+\frac{∂L_{reg}}{∂w}上面的公式表明，$\frac{∂L_{err}}{∂ϕ}$与具体的模型参数无关。因此，每次更新模型时，只需计算一次，之后直接调用$\frac{∂L_{err}}{∂ϕ}$的值即可。对于更新 $nfk$个模型参数，这种方式能够极大提升运算效率。 第二，自适应学习率。此版本的FFM实现没有采用常用的指数递减的学习率更新策略，而是利用 $nfk$ 个浮点数的临时空间，自适应地更新学习率。学习率是参考AdaGrad算法计算的[11]，按如下方式更新 w_{j_1,j_2}^ {'}=w_{j_1,f_2}-\frac{η}{\sqrt{1+\sum_t(g^t_{w_{j_1,f_2}})^2}}·g_{w_{j_1,f_2}}其中，$w_{j_1,f_2}$是特征 $j_1$ 对field $f_2$ 隐向量的一个元素，元素下标未标出；$g_{w_{j_1,f_2}}$是损失函数对参数 $w_{j_1,f_2}$的梯度；$g^t_{w_{j_1,f_2}}$是第 t 次迭代的梯度；η是初始学习率。可以看出，随着迭代的进行，每个参数的历史梯度会慢慢累加，导致每个参数的学习率逐渐减小。另外，每个参数的学习率更新速度是不同的，与其历史梯度有关，根据AdaGrad的特点，对于样本比较稀疏的特征，学习率高于样本比较密集的特征，因此每个参数既可以比较快速达到最优，也不会导致验证误差出现很大的震荡。 第三，OpenMP多核并行计算。OpenMP是用于共享内存并行系统的多处理器程序设计的编译方案，便于移植和多核扩展[12]。FFM的源码采用了OpenMP的API，对参数训练过程SGD进行了多线程扩展，支持多线程编译。因此，OpenMP技术极大地提高了FFM的训练效率和多核CPU的利用率。在训练模型时，输入的训练参数ns_threads指定了线程数量，一般设定为CPU的核心数，便于完全利用CPU资源。 第四，SSE3指令并行编程。SSE3全称为数据流单指令多数据扩展指令集3，是CPU对数据层并行的关键指令，主要用于多媒体和游戏的应用程序中。SSE3指令采用128位的寄存器，同时操作4个单精度浮点数或整数。SSE3指令的功能非常类似于向量运算。例如，a 和 b 采用SSE3指令相加（a 和 b 分别包含4个数据），其功能是 a 中的4个元素与 b 中4个元素对应相加，得到4个相加后的值。采用SSE3指令后，向量运算的速度更加快捷，这对包含大量向量运算的FFM模型是非常有利的。 除了上面的技巧之外，FFM的实现中还有很多调优技巧需要探索。例如，代码是按field和特征的编号申请参数空间的，如果选取了非连续或过大的编号，就会造成大量的内存浪费；在每个样本中加入值为1的新特征，相当于引入了因子化的一次项，避免了缺少一次项带来的模型偏差等。 后记本文主要介绍了FFM的思路来源和理论原理，并结合源码说明FFM的实际应用和一些小细节。从理论上分析，FFM的参数因子化方式具有一些显著的优势，特别适合处理样本稀疏性问题，且确保了较好的性能；从应用结果来看，站内CTR/CVR预估采用FFM是非常合理的，各项指标都说明了FFM在点击率预估方面的卓越表现。当然，FFM不一定适用于所有场景且具有超越其他模型的性能，合适的应用场景才能成就FFM的“威名”。 参考文献 http://blog.csdn.net/lilyth_lilyth/article/details/48032119 http://www.cnblogs.com/Matrix_Yao/p/4773221.html http://www.herbrich.me/papers/adclicksfacebook.pdf https://www.kaggle.com/c/criteo-display-ad-challenge https://www.kaggle.com/c/avazu-ctr-prediction https://en.wikipedia.org/wiki/Demand-side_platform http://www.algo.uni-konstanz.de/members/rendle/pdf/Rendle2010FM.pdf http://www.cs.cmu.edu/~wcohen/10-605/2015-guest-lecture/FM.pdf http://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf https://github.com/guestwalk/libffm https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad http://openmp.org/wp/openmp-specifications/ http://blog.csdn.net/gengshenghong/article/details/7008704 https://kaggle2.blob.core.windows.net/competitions/kddcup2012/2748/media/Opera.pdf]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>FM</tag>
        <tag>FFM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（25）：最速下降法、牛顿法、拟牛顿法]]></title>
    <url>%2F2017%2F07%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8825%EF%BC%89%EF%BC%9A%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D%E6%B3%95%E3%80%81%E7%89%9B%E9%A1%BF%E6%B3%95%E3%80%81%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、最速下降法最速下降法，又称为梯度下降法，是无约束最优化领域中最简单的算法，单独就这种算法来看，属于早就“过时”了的一种算法。但是，它的理念是其他某些算法的组成部分，或者说是在其他算法中，也有最速下降法的影子。它是一种迭代算法，每一步需要求解目标函数的梯度向量。 假设$f(x)$是$R^n$上具有一阶连续偏导的函数。要求解的无约束最优化问题是： \underset {x\in R^n}{min} \ f(x)梯度下降法是一种迭代算法。选取适当的初值$x^{(0)}$，不断迭代，更新$x$的值，进行目标函数的极小化，直到收敛。由于负梯度方向是使函数值下降最快的方向，在迭代的每一步，以负梯度方向更新$x$的值，从而达到减少函数值的目的。 由于$f(x)$具有一阶连续偏导数，若第$k$次迭代值为$x^{(k)}$，则可将$f(x)$在$x^{(k)}$附近进行一阶泰勒展开： f(x)=f(x^{(k)})+g_k^T(x-x^{(k)})这里，$g_k=g(x^{(k)})=∇f(x^{(k)})$为$f(x)$在$x^{(k)}$的梯度。 求出第$k+1$次迭代值$x^{(k+1)}$: x^{(k+1)}\leftarrow x^{(k)}+\lambda_kp_k其中$p_k$是搜索方向，取负梯度方向$p_k=-∇f(x^{(k)})$,$\lambda _k$是步长，由一维搜索确定，即$\lambda_k$使得 f(x^{(k)}+\lambda_kp_k)=\underset {\lambda≥0}{min} \ f(x^{(k)}+\lambda p_k)算法步骤如下： 输入：目标函数$f(x)$，梯度函数$g(x)=∇f(x)$，计算精度$\xi$; 输出：$f(x)$的极小点$x^*$ 取初始值$x^{(0)}\in R^{n}$，置$k=0$ 计算 $f(x^{(k)})$ 计算梯度$g_k=g(x^{(k)})$，当$||g_k||＜\xi$时，停止迭代，令$x^*=x^{(k)}$；否则，令$p_k = -g(x^{(k)})$，求$\lambda_k$，使f(x^{(k)}+\lambda_kp_k)=\underset {\lambda≥0}{min} \ f(x^{(k)}+\lambda p_k) 置$x^{(k+1 )}=x^{(k)}+\lambda _kp_k$，计算$f(x^{(k+1)})$当$||f(x^{(k+1)})-f(x^{(k)})||＜\xi$或$||x^{(k=1)}-x^{(k)}||＜\xi$，停止迭代，令$x^*=x^{(k+1)}$ 否则，置$k=k+1$，转到步骤3。 当目标函数是凸函数时，梯度下降法的解释全局最优解。一般情况下，其解不保证是全局最优解。梯度下降法的收敛速度也未必是很快的。 二、牛顿法考虑如下无约束的极小化问题 \underset{X}{min}\ f(x)其中$X=(x_1,x_2,x_3，···，x_N)^T \in R^N$这里我们假定$f$为凸函数，且两阶连续可微。记$x^*$为目标函数的极小值。 为了简单起见，首先考虑$N=1$的简单情形（此时目标函数$f(X)$变为$f(x)$）。牛顿法的基本思想是：在现有极小值估计值的附近对$f(x)$作二阶泰勒展开，进而找极小点的下一个估计值。设$x_k$为当前的极小点估计值，则 f(x) = f(x_k)+ f^{'}(x_k)(x-x_k)+\frac{1}{2}f^{''}(x_k)(x-x_k)^2表示$f(x)$在$x_k$附近的二阶泰勒展开式（略去了关于$x-x_k$的高阶项）。由于求得是最值，由极值必要条件可知，$f(x)$应该满足f^{'}(x)=0，即 f^{'}(x_k)+f^{''}(x_k)(x-x_k)^2从而求得 x=x_k-\frac{f^{'}(x_k)}{f^{''}{(x_k)}}于是，若给定初始值$x_0$，则可以构造如下的迭代格式 x_{k+1}=x_k-\frac{f^{'}(x_k)}{f^{''}{(x_k)}}于是，若给定初始值$x_0$，则可以构造如下的迭代格式 x_{k+1}=x_k-\frac{f^{'}(x_k)}{f^{''}{(x_k)}} , \ k=0,1,···产生序列$\{x_k\}$来逼近$f(x)$的极小点。在一定条件下$\{x_k\}$可以收敛到$f(x)$的极小点。 对于$N&gt;1$的情形，二阶泰勒展开式可以做推广，此时 f(X)=f(X_k)+∇f(X_k)\ ·\ (X-X_k)+\frac{1}{2}· (X-X_k)^T·∇^2f(X_k)·(X-X_k)其中$∇f$为$f$的梯度向量，$∇^2f$为海森矩阵，其定义分别为 \nabla f=\left[\begin{array}{c} \frac{\partial f}{\partial x_1}\\ \frac{\partial f}{\partial x_2}\\ ···\\ \frac{\partial f}{\partial x_N}\\ \end{array}\right],\\nabla f^2=\left[\begin{matrix} \frac{\partial^2f}{\partial x_{1}^{2}}& \frac{\partial^2f}{\partial x_1\partial x_2}& ···& \frac{\partial^2f}{\partial x_1\partial x_N}\\ \frac{\partial^2f}{\partial x_2\partial x_1}& \frac{\partial^2f}{\partial x_{2}^{2}}& ···& \frac{\partial^2f}{\partial x_2\partial x_N}\\ ···& ···& ···& ···\\ \frac{\partial^2f}{\partial x_N\partial x_1}& \frac{\partial^2f}{\partial x_N\partial x_2}& ···& \frac{\partial^2f}{\partial x_{N}^{2}}\\ \end{matrix}\right]注意，$∇f$和$∇^2f$中的元素均为关于$X$的函数，以下分别将其简记为$g$和$H$。特别地，若$f$的混合偏导数可交换次序(即对$\forall\ i,j$，成立$\frac{\partial^2f}{\partial x_i\partial x_j}=\frac{\partial^2f}{\partial x_j\partial x_i}$)，则海森矩阵$H$为对称矩阵，而$∇f(X_k)$和$∇^2f(X_k )$则表示将$X$取为$X_k$后得到的实值向量和矩阵，以下分别将其简记为$g_k$和$H_k$（这里字母g表示gradient，H表示Hessian） 同样地，由于是求极小点，极值必要条件要求它为$f(X)$的驻点，即 ∇f(X)=0亦即对二阶泰勒展开作用一个梯度算子 g_k+H_k·(X-X_k)=0进一步，若矩阵$H_k$非奇异，则可解得 X=X_k-H_k^{-1}·g_k于是，若给定初始值$X_0$，则同样可以构造出迭代格式 X_{k+1}=X_k-H^{-1}_k·g_k这就是原始的牛顿迭代法，其迭代格式中的搜索方向$d_k=-H^{-1}_k·g_k$称为牛顿方向。下面给出牛顿法的完整算法描述： 给定初值$X_0$和精度阀值$\xi$，并令$k:=0$ 计算$g_k$和$H_k$ 若$||g_k||＜\xi$，则停止迭代；否则确定搜索方向$d_k=-H^{-1}_k·g_k$ 计算新的迭代点$X_{k+1}:=X_k+d_k$ 令k:=k+1，转至步2 当目标函数是二次函数时，由于二次泰勒展开函数与原目标函数不是近似而是完全相同的二次式，海森矩阵退化成一个常数矩阵，从任一初始点出发，秩序一步迭代即可达到$f(X)$的极小点$X^*$，因此牛顿法是一种具有二次收敛性的算法。对于非二次函数，若函数的二次形性态较强，或迭代点已进入极小点的领域，则其收敛速度也是很快的，这是牛顿法的主要优点。 但原始牛顿法由于迭代公式中没有步长因子，而是定步长迭代，对于非二次型目标函数，有时会使函数值上升，即出现$f(X_{k=1})&gt;f(X_k )$的情况，这表明原始牛顿法不能保证函数值稳定地下降，在严重的情况下甚至可能造成迭代点列$\{X_k\}$的发散而导致计算失败。 为了消除这个弊病，人们提出了“阻尼牛顿法”，阻尼牛顿法每次迭代的方向仍然采用$d_k$，但每次迭代需沿此方向作一维搜索（line search），寻求最优的步长因子$\lambda _k $，即 \lambda_k =arg \underset{\lambda \in R}{min}f(X_k+\lambda d_k)下面给出阻尼牛顿法的完整算法描述： 给定初值$X_0$和精度阀值$\xi$，并令$k:=0$ 计算$g_k$和$H_k$ 若$||g_k||＜\xi$，则停止迭代；否则确定搜索方向$d_k=-H^{-1}_k·g_k$ 利用$\lambda_k =arg \underset{\lambda \in R}{min}f(X_k+\lambda d_k)$得到步长$\lambda _k$，计算新的迭代点$X_{k+1}:=X_k+d_k$ 令k:=k+1，转至步2 至此完成了牛顿法的算法介绍，接下来对其做个小结： 牛顿法是梯度下降法的进一步发展，梯度下降法利用目标函数的一阶偏导数信息、以负梯度方向作为搜索方向，只考虑目标函数在迭代点的局部性质；而牛顿法不仅使用目标函数的一阶偏导数，还进一步利用了目标函数的二阶偏导数，这样就考虑了梯度变化的趋势，因而能更全面地确定合适的搜索方向加快收敛，它具二阶收敛速度。但牛顿法主要存在以下两个缺点： 对目标函数有较严格的要求。函数必须具有连续的一、二阶偏导数，海森矩阵必须正定。 极端相当复杂，除需要计算梯度以外，还需要计算二阶偏导数矩阵和它的逆矩阵。计算量、存储量均很大，且均以维数$N$的平方比增加，当$N$很大时这个问题更加突出。 三、拟牛顿法牛顿法虽然收敛速度快，但是计算过程中需要计算目标函数的二阶偏导数，计算复杂度较大。而且有时目标函数的海森矩阵无法保持正定，从而使牛顿法失效。为了克服这两个问题，人们提出了拟牛顿法。这个方法的基本思想是：不用二阶偏导数而构造出可以近似海森矩阵或者海森矩阵的逆的正定对称阵，在拟牛顿的条件下优化目标函数。不同的构造方法就产生了不同的拟牛顿法。 也有人把“拟牛顿法”翻译成“准牛顿法”，其实都是表示“类似于牛顿法”的意思，因此只是对算法中用来计算搜索方向的海森矩阵（或海森矩阵的逆）作了近似计算罢了。 在介绍具体的拟牛顿法之前，我们先推到一个拟牛顿条件，或者叫拟牛顿方程，还有的叫做割线条件。因为对海森矩阵（或海森矩阵的逆）做近似总不能随便近似，也需要理论指导，而拟牛顿条件则是用来提供理论指导的，它指出了用来近似的矩阵应该满足的条件。 为明确起见，下文中用$B$表示对海森矩阵$H$本身的近似，而用$D$表示对海森矩阵的逆$H^{-1}$的近似，即$B≈H,D≈H^{-1}$ 3.1 拟牛顿条件设经过$k+1$次迭代后得到$X_{k+1}$，此时将目标函数$f(X)$在$X_{k+1}$附近作泰勒展开，取二阶近似，得到 f(X)≈ f(X_{k+1})+∇f(X_{k+1})\ ·\ (X-X_{k+1})+\frac{1}{2}· (X-X_{k+1})^T·∇^2f(X_{k+1})·(X-X_{k+1})在两边同时作用一个梯度算子$∇$，可得 ∇f(X)≈∇f（X_{k+1}）+H_{k+1}·(X-X_{k+ 1})取$X=X_k$并整理，可得 g_{k+1}-g_k≈H_{k+1}·(X_{k+1}-X_k)若引入记号$s_k=X_{k+1}， y_k=g_{k+1}-g_k$则可以改写成 y_k≈H_{k+1}·s_k或者 s_k≈H^{-1}_{k+1}·y_k这就是所谓的拟牛顿条件，它对迭代过程中的海森矩阵$H_{k+1}$作约束，因此，对$H_{k+1}$做近似的$B_{k+1}$，以及对$H_{k+1}^{-1}$做近似的$D_{k+1 }$可以将 y_k≈H_{k+1}·s_k或者 s_k≈H^{-1}_{k+1}·y_k作为指导。 3.2 DFP算法DFP算法是以William C.Davidon、Roger Fletcher、Michael J.D.Powell三个人的名字的首字母命名的，它由Davidon于1959年首先提出，是最早的拟牛顿法。该算法的核心是：通过迭代的方法，对$H_{k+1}^{-1}$做近似，迭代格式为 D_{k+1}=D_k+\Delta D_k , k=0,1,2,···其中的$D_0$通常取为单位矩阵$I$。因此，关键是每一步的校正矩阵$\Delta D_k$如何构造。 注意，我们猜想$\Delta D_k$可能与$s_k,y_k$和$D_k$发生关联。这里，我们采用“待定法”，即首先将$\Delta D_k$待定城某种形式，然后结合拟牛顿条件来进行推导。 那将$\Delta D_k$待定成什么形式呢？说起来比较tricky，我们将其待定为 \Delta D_k=\alpha uu^T+\beta vv^T其中$\alpha$和$\beta$为待定向量。从形式上看，这种待定公式至少保证了矩阵$\Delta D_k$的对称性（因为$uu^T$和$vv^T$均为对称矩阵） 将其代入迭代式，并结合拟牛顿指导条件，可得 s_k=D_ky_k+\alpha uu^Ty_k+\beta vv^Ty_k将其改写一下 s_k=D_ky_k+u(\alpha u^Ty_k)+v(\beta v^Ty_k)\\=D_ky_k+(\alpha u^Ty_k)u+(\beta v^Ty_k)v括号中为两个数，既然是数，我们不妨作如下简单赋值 \alpha u^Ty_k=1 ，\ \beta v^Ty_k=-1$$即$$\alpha=\frac{1}{u^Ty_k},\beta=-\frac{1}{v^Ty_k}其中向量$u,v$仍有待确定。 我们把$s_k=D_ky_k+u-v$写作 u-v=s_k-D_ky_k要上式成立，不妨直接取 u=s_k,v=D_ky_k代入求$\alpha$和$\beta$的式子，便得到 \alpha=\frac{1}{s^T_ky_k},\beta=\frac{1}{(D_ky_k)^Ty_k}=-\frac{1}{y^T_kD_ky_k}其中第二个式子用到了$D_k$的对称性。至此，我们已经将校正矩阵$\Delta D_k$构造出来了，我们就可以得到 \Delta D_k=\frac{s_ks_k^T}{s_k^Ty_k}-\frac{D_ky_ky_k^TD_k}{y_k^TD_ky_k}综上，我们给出DFP算法的一个完整的算法描述。 给定初值$X_0$和精度阀值$\xi$，并令$k:=0$ 确定搜索方向$d_k=-D^{-1}_k·g_k$ 利用$\lambda_k =arg \underset{\lambda \in R}{min}f(X_k+\lambda d_k)$得到步长$\lambda _k$，令$s_k=\lambda_kd_k$，计算新的迭代点$X_{k+1}:=X_k+s_k$ 若$||g_{k=1}||&lt;\xi$，则算法结束 计算$y_k=g_{k+1}-g_k$ 计算D_{k+1}=D_k+\frac{s_ks_k^T}{s_k^Ty_k}-\frac{D_ky_ky_k^TD_k}{y_k^TD_ky_k} 令$k:=k+1$转至步骤2. 3.3 BFGS算法BFGS算法是以其发明者Broyden、Fletcher、Goldfarb和Shanno四个人的名字的首字母命名的。与DFP算法相比，BFGS算法性能更加。目前它已成为求解无约束非线性优化问题最常用的方法之一。BFGS算法已有较完善的局部收敛理论，对其全局收敛的研究也取得了重要成果。 BFGS算法中核心公式的推导过程和DFP完全类似，只是互换了其中$s_k$和$y_k$的位置。需要注意的是，BFGS算法是直接逼近海森矩阵，即$B_k≈H_k$,仍采用迭代方法，设迭代格式为 B_{k+1}=B_k+\Delta B_k , k=0,1,2,···其中的$B_0$也常取为单位矩阵$I$。因此，关键是每一步的校正矩阵$\Delta B_k$如何构造，同样，将其待定为 \Delta B_k=\alpha uu^T+\beta vv^T将其代入上式，并结合指导条件$y_k≈H_{k+1}·s_k$，可得 y_k=B_ks_k+(au^Ts_k)u+(\beta v^Ts_k)v通过令$au^Ts_k=1,\beta v^Ts_k=-1$,以及 u=y_k,v=B_ks_k$$可以算得 $$\alpha=\frac{1}{y^T_ks_k},\beta = -\frac{1}{s^T_kB_ks_k}综上，便得到了如下的校正矩阵$\Delta B_k$的公式 \Delta B_k=\frac{y_ky_k^T}{y_k^Ts_k}-\frac{B_ks_ks_k^TB_k}{s^T_kB_ks_k}好了，现在把矩阵$\Delta B_k$和$\Delta D_k$拿出来对比一下，除了你将$D$换成$B$外，就是把$s_k$和$y_k$互换了一下位置。 最后，给出BFGS算法的一个完整算法描述： 给定初值$X_0$和精度阀值$\xi$，并令$k:=0$ 确定搜索方向$d_k=-B^{-1}_k·g_k$ 利用$\lambda_k =arg \underset{\lambda \in R}{min}f(X_k+\lambda d_k)$得到步长$\lambda _k$，令$s_k=\lambda_kd_k$，计算新的迭代点$X_{k+1}:=X_k+s_k$ 若$||g_{k=1}||&lt;\xi$，则算法结束 计算$y_k=g_{k+1}-g_k$ 计算B_{k+1}=B_k+\frac{y_ky_k^T}{y_k^Ts_k}-\frac{B_ks_ks_k^TB_k}{s^T_kB_ks_k} 令$k:=k+1$转至步骤2. 3.4 L-BFGS算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>损失函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（24）：机器学习中的损失函数]]></title>
    <url>%2F2017%2F07%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8824%EF%BC%89%EF%BC%9A%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[损失函数（loss function）是用来估量模型的预测值f(x)与真实值$Y$不一致的程度，它是一个非负实数值函数，通常使用$L(Y,f(x))$来表示，损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数的重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下的式子： \theta^* = argmin_\theta \frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i;\theta))+\lambda Φ(θ) 前面的均值函数表示的是经验风险函数，$L$代表的是损失函数，后面的$Φ$是正则化项（regularizer）或者叫惩罚项（penalty term）,它可以是$L_1$，也可以是$L_2$等其他的正则函数。整个式子表示的意思是找到使目标函数最小时的$\theta$值。下面列出集中常见的损失函数。 一、对数损失函数（逻辑回归）有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到，而逻辑回归得到的并不是平方损失。在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数（即$max F(y, f(x)) —&gt; min -F(y, f(x))$)。从损失函数的视角来看，它就成了log损失函数了。 Log损失函数的标准形式： L(Y,P(Y|X))=-logP(Y|X)刚刚说到，取对数是为了方便计算极大似然估计，因为在MLE中，直接求导比较困难，所以通常都是先取对数再求导找极值点。损失函数$L(Y.P(Y|X))$表达的是样本在分类$Y$的情况下，使概率$P(Y|X)$达到最大值（换言之，就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者什么样的参数才能使我们观测到目前这组数据的概率最大）。因为log函数是单调递增的，所以$logP(Y|X)$也会达到最大值，因此在前面加上负号之后，最大化$P(Y|X)$就等价于最小化$L$了。 logistic回归的$P(y|x)$表达式如下（为了将类别标签y统一为1和0，下面将表达式分开表示）： P\left(Y=y^{\left(i\right)}|x^{\left(i\right)};\theta\right)=\left\{\begin{array}{l} h_{\theta}\left(x^{\left(i\right)}\right)=\frac{1}{1+e^{-\theta^Tx}},\,\,y^{\left(i\right)}=1\\ 1-h_{\theta}\left(x^{\left(i\right)}\right)=\frac{e^{-\theta^Tx}}{1+e^{-\theta^Tx}},\,\,y^{\left(i\right)}=0\\ \end{array}\right.将上面的公式合并在一起，可得到第$i$个样本正确预测的概率： P(y^{(i)}|x^{(i)};\theta)=(h_\theta(x^{(i)}))^{y(i)}·(1-h_\theta(x^{(i)}))^{1-y(i)}上式是对一个样本进行建模的数据表达。对于所有的样本，假设每条样本生成过程独立，在整个样本空间中（N个样本）的概率分布为： P\left(Y\ | \ X;\theta\right)=\prod_{i=1}^N{\left(\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)^{y^{\left(i\right)}}\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)^{1-y^{\left(i\right)}}\right)}将上式代入到对数损失函数中，得到最终的损失函数为： J(\theta) = -\frac{1}{N}\sum_{i=1}^N{y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)}之所以有人认为逻辑回归是平方损失，是因为在使用梯度下降来求最优解的时候，它的迭代式子与平方损失求导后的式子非常相似，从而给人一种直观上的错觉。 二、平方损失函数（最小二乘法，Ordinary Least Squares）最小二乘法是线性回归的一种，OLS将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理，可以参考【central limit theorem】），最后通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。换言之，OLS是基于距离的，而这个距离就是我们用的最多的欧几里得距离。为什么它会选择使用欧式距离作为误差度量呢（即Mean squared error， MSE），主要有以下几个原因： 简单，计算方便； 欧氏距离是一种很好的相似性度量标准； 在不同的表示域变换后特征性质不变。 平方损失（Square loss）的标准形式如下：L(Y,f(X))=(Y-f(x))^2当样本个数为n时，此时的损失函数变为：$$L(Y,f(X))=\sum_{i=1}^n(Y-f(X))^2$$$Y-f(X)$表示的是残差，整个式子表示的是残差的平方和，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是最小化残差的平方和（residual sum of squares，RSS）。 而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标，公式如下： MSE=\frac{1}{N}\sum_{i=1}^N(\tilde{Y_i}-Y_i)^2上面提到了线性回归，这里额外补充一句，我们通常说的线性有两种情况，一种是因变量y是自变量x的线性函数，一种是因变量y是参数α的线性函数。在机器学习中，通常指的都是后一种情况。 三、指数损失函数（Adaboost）学过Adaboost算法的人都知道，它是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。在Adaboost中，经过m此迭代之后，可以得到$f_m(x)$: f_m(x)=f_{m-1}(x)+a_mG_m(x)Adaboost每次迭代时的目的是为了找到最小化下列式子时的参数$a$和G： arg\underset{a,G}{min}=\sum_{i=1}^Nexp[-y_i(f_{m-1}(x_i)+aG(x_i))]而指数损失函数(exp-loss）的标准形式如下: L(y,f(x))=exp[-yf(x)]可以看出，Adaboost的目标式子就是指数损失，在给定N个样本的情况下，Adaboost的损失函数为： L(y,f(x))=\frac{1}{N}\sum_{i=1}^nexp[-y_if(x_i)]四、Hinge损失函数（SVM）4.1 Hinge损失函数（SVM）线性支持向量机学习除了原始最优化问题，还有另外一种解释，就是最优化以下目标函数： \sum_i^{N}[1-y_i(w·x_i+b)]_++\lambda||w||^2目标函数的第一项是经验损失或经验风险，函数 L(y·(w·x+b))=[1-y(w·x+b)]_+称为合页损失函数（hinge loss function）。下标”+”表示以下取正值的函数： \left[z\right]_+=\left\{\begin{array}{l} z\ ,\ z>0\\ 0\ ,\ z\le 0\\ \end{array}\right.这就是说，当样本点$(x_i,y_i)$被正确分类且函数间隔（确信度）$y_i(w·x_i+b)$大于1时，损失是0，否则损失是$1-y_i(w·x_i+b)$。目标函数的第二项是系数为$\lambda$的$w$的$L_2$范数，是正则化项。 接下来证明线性支持向量机原始最优化问题： \underset{w,b,\xi}{\min}\ \frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i} s.t.\ \ y_i\left( w·x_i+b \right) \geqslant 1-\xi _i\ ,\ i=1,2,···,N \xi _i\geqslant 0,\ i=1,2,···\mathrm{，}N等价于最优化问题 \underset{w,b}{min }\sum_i^{N}[1-y_i(w·x_i+b)]_++\lambda||w||^2先令$[1-y_i(w·x_i+b)]_+=\xi_i$，则$\xi_i≥0$，第二个约束条件成立；由$[1-y_i(w·x_i+b)]_+=\xi_i$，当$1-y_i(w·x_i+b)&gt;0$时，有$y_i(w·x_i+b)=1-\xi_i$;当$1-y_i(w·x_i+b)≤0$时，$\xi_i=0$，有$y_i(w·x_i+b)≥1-\xi_i$，所以第一个约束条件成立。所以两个约束条件都满足，最优化问题可以写作 \underset{w,b}{min}\sum_{i=1}^N\xi_i+\lambda||w||^2若取$\lambda =\frac{1}{2C}$则 \underset{w,b}{min} \frac{1}{C}(\frac{1}{2} ||w||^2+C\sum_{i=1}^N \xi_i)与原始最优化问题等价。 合页损失函数图像如图所示，横轴是函数间隔$y(w·x+b)$，纵轴是损失。由于函数形状像一个合页，故名合页损失函数。 图中还画出了0-1损失函数，可以认为它是一个二类分类问题的真正的损失函数，而合页损失函数是0-1损失函数的上界。由于0-1损失函数不是连续可导的，直接优化其构成的目标函数比较困难，可以认为线性支持向量机是优化由0-1损失函数的上界（合页损失函数）构成的目标函数。这时的上界损失函数又称为代理损失函数（surrogate function）。图中虚线显示的是感知机的损失函数$[-y_i(w·x_i+b)]_+$。这时当样本点$(x_i,y_i)$被正确分类时，损失是0，否则损失是$-y_i(w·x_i+b)$，相比之下，合页损失函数不仅要分类正确，而且确信度足够高时损失才是0，也就是说，合页损失函数对学习有更高的要求 4.2 逻辑斯谛回归和SVM的损失函数对比我们先来看一下带松弛变量的 SVM 和正则化的逻辑回归它们的损失函数：其中 $g(z)=(1+exp(−z))^{−1}$可以将两者统一起来: 这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重,两者的根本目的都是一样的。 svm考虑局部（支持向量），而logistic回归考虑全局，就像大学里的辅导员和教师间的区别。 辅导员关心的是挂科边缘的人，常常找他们谈话，告诫他们一定得好好学习，不要浪费大好青春，挂科了会拿不到毕业证、学位证等等，相反，对于那些相对优秀或者良好的学生，他们却很少去问，因为辅导员相信他们一定会按部就班的做好分内的事；而大学里的教师却不是这样的，他们关心的是班里的整体情况，大家是不是基本都理解了，平均分怎么样，至于某个人的分数是59还是61，他们倒不是很在意。 总结： LR采用log损失，SVM采用合页损失。 LR对异常值敏感，SVM对异常值不敏感。 在训练集较小时，SVM较适用，而LR需要较多的样本。 LR模型找到的那个超平面，是尽量让所有点都远离他，而SVM寻找的那个超平面，是只让最靠近中间分割线的那些点尽量远离，即只用到那些支持向量的样本。 对非线性问题的处理方式不同，LR主要靠特征构造，必须组合交叉特征，特征离散化。SVM也可以这样，还可以通过kernel。 svm 更多的属于非参数模型，而logistic regression 是参数模型，本质不同。其区别就可以参考参数模型和非参模型的区别 那怎么根据特征数量和样本量来选择SVM和LR模型呢？Andrew NG的课程中给出了以下建议： 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。(LR和不带核函数的SVM比较类似。)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>损失函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（23）：TF-IDF与余弦相似度]]></title>
    <url>%2F2017%2F07%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8823%EF%BC%89%EF%BC%9ATF-IDF%E4%B8%8E%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[TF-IDF(term frequency=inverse document frequency)是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常备搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。 一、原理设想现在我们正在阅读新闻，如何最快速的了解新闻的主旨？毫无疑问——关键词。TF-IDF就具有这样的能力：提取关键词。 1.1 TF假设一个词在一篇文章中出现的次数越多，那么它就越”紧扣主题”。以本文为例，我们可以统计词频(TF)，不难发现“TF-IDF”,“应用”、“原理”是出现频率很高的词，后文称keywords。这符合我们的假设，但是有些词却出现的次数更多，如：的、是、有等。这类词语没有明确意义，我们称为停顿词(Stopwords)。 如果单纯按照词频算关键词，你会发现几乎所有的文章都是stopwords的词频最高。换句话说，像这种”万金油”，是没有区分度的词语，不能很好的起到将文章分类的作用。 此外，抛开停用词，如果该文档中的几个词出现的频率一样，也不意味着，作为关键词，它们的重要性是一致的。比如这篇文档中，“TF-IDF”、“意义”、“文档”这三个词的词频出现的次数一样多，但因为“意义”是很常见的词，相对而言，“TF-IDF”、“文档”不那么常见。即使它们的词频一样，我们也有理由认为，“TF-IDF”和“文档”的重要性大于“意义”，也就是使，在关键词排序上，“TF-IDF”和“文档”也应该排在“意义”的前面。 所以，我们需要一个重要性调整系数，衡量一个词是不是常见词。如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。这时就需要祭出逆文档频率(IDF)来解决词语权重的问题。 1.2 IDF用统计学语言表达，就是在词频的基础上，要对每个词分配一个”重要性”权重。最常见的词（”的”、”是”、”在”）给予最小的权重，较常见的词（”中国”）给予较小的权重，较少见的词（”蜜蜂”、”养殖”）给予较大的权重。这个权重叫做”逆文档频率”（Inverse Document Frequency，缩写为IDF），它的大小与一个词的常见程度成反比。 知道了”词频”（TF）和”逆文档频率”（IDF）以后，将这两个值相乘，就得到了一个词的TF-IDF值。某个词对文章的重要性越高，它的TF-IDF值就越大。所以，排在最前面的几个词，就是这篇文章的关键词。 1.3 公式化表达对于在某一特定文件里的词语$t_i$来说，它的重要性可表示为： TF_{i,j}=\frac{n_{i,j}}{\sum_kn_{k,j}}以上式子中$n_{i,j}$是该词在文件$d_{j}$中的出现次数而分母则是在文件$d_j$中所有字词的出现次数之和。 逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到： IDF_i=log\frac{|D|}{|{j:t_i\in d_j}|}其中 $|D|$：语料库中的文件总数 $|{j:t_i\in d_j}|$：包含词语$t_i$的文件数目（即$n_{i,j}≠0的文件数目$）如果该词语不在语料库中，就会导致分母为零，因此一般情况下使用$1+|{j:t_i\in d_j}|$ 然后 TF-IDF = TF_{i,j}\times IDF _i某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的tf-idf。因此，tf-idf倾向于过滤掉常见的词语，保留重要的词语。 1.4 应用我们通过Google搜索结果数为例，将含有中文“的”结果数15.8亿作为整个语料库大小，计算一些关键词和停用词的TF-IDF值。为了计算简便，假设全文分词后一共500词，则结果如下： TF-IDF的优点是计算简单，利于理解，性价比极高。但是它也有缺陷，首先单纯依据文章中的TF来衡量重要性，忽略了位置信息。如段首，句首一般权重更高；其次，有的文章可能关键词只出现1-2次，但可能通篇都是围绕其进行阐述和解释，所以单纯靠TF仍然不能解决所有的情况。 二、余弦相似度余弦相似性通过测量两个向量的夹角的余弦值来度量它们之间的相似性。0度角的余弦值是1，而其他任何角度的余弦值都不大于1；并且其最小值是-1。从而两个向量之间的角度的余弦值确定两个向量是否大致指向相同的方向。两个向量有相同的指向时，余弦相似度的值为1；两个向量夹角为90°时，余弦相似度的值为0；两个向量指向完全相反的方向时，余弦相似度的值为-1。这结果是与向量的长度无关的，仅仅与向量的指向方向相关。余弦相似度通常用于正空间，因此给出的值为0到1之间。 注意这上下界对任何维度的向量空间中都适用，而且余弦相似性最常用于高维正空间。例如在信息检索中，每个词项被赋予不同的维度，而一个文档由一个向量表示，其各个维度上的值对应于该词项在文档中出现的频率。余弦相似度因此可以给出两篇文档在其主题方面的相似度。 2.1 定义两个向量间的余弦值可以通过使用欧几里得点积公式求出： a·b=|a|·|b|\ cos \theta给定两个属性向量$A$和$B$，其余相似性$\theta$由点积和向量长度给出，如下所示： similarity = cos(\theta)=\frac{A·B}{|A||B|}=\frac{\sum_{i=1}^nA_i\times B_i}{\sqrt{\sum_{i=1}^n(A_i)^2}\times \sqrt{\sum_{i=1}^n(B_i)^2}}这里的$A_i$和$B_i$分别代表向量$A$和$B$的各分量。 给出的相似性范围从-1到1：-1意味着两个向量指向的方向正好截然相反，1表示它们的指向是完全相同的，0通常表示它们之间是独立的，而在这之间的值则表示中间的相似性或相异性。 对于文本匹配，属性向量A 和B 通常是文档中的词频向量。余弦相似性，可以被看作是在比较过程中把文件长度正规化的方法。 在信息检索的情况下，由于一个词的频率（TF-IDF权）不能为负数，所以这两个文档的余弦相似性范围从0到1。并且，两个词的频率向量之间的角度不能大于90°。 由此，我们就得到了”找出相似文章”的一种算法： 1）使用TF-IDF算法，找出两篇文章的关键词； 2）每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）； 3）生成两篇文章各自的词频向量； 4）计算两个向量的余弦相似度，值越大就表示越相似。 “余弦相似度”是一种非常有用的算法，只要是计算两个向量的相似程度，都可以采用它。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>TF-IDF</tag>
        <tag>余弦相似度</tag>
        <tag>文档检索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（22）：主成分分析]]></title>
    <url>%2F2017%2F07%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8822%EF%BC%89%EF%BC%9A%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。 一、数据的向量表示及降维问题一般情况下，在数据挖掘和机器学习中，数据被表示为向量。例如某个淘宝店2012年全年的流量及交易情况可以看成一组记录的集合，其中每一天的数据是一条记录，格式如下： (日期, 浏览量, 访客数, 下单数, 成交数, 成交金额) 其中“日期”是一个记录标志而非度量值，而数据挖掘关心的大多是度量值，因此如果我们忽略日期这个字段后，我们得到一组记录，每条记录可以被表示为一个五维向量，其中一条看起来大约是这个样子： (500,240,25,13,2312.15)^𝖳注意这里用了转置，因为习惯上使用列向量表示一条记录（后面会看到原因），本文后面也会遵循这个准则。不过为了方便有时会省略转置符号，但我们说到向量默认都是指列向量。 我们当然可以对这一组五维向量进行分析和挖掘，不过我们知道，很多机器学习算法的复杂度和数据的维数有着密切关系，甚至与维数呈指数级关联。当然，这里区区五维的数据，也许还无所谓，但是实际机器学习中处理成千上万甚至几十万维的情况也并不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行降维。 降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。 举个例子，假如某学籍数据有两列M和F，其中M列的取值是如何此学生为男性取值1，为女性取值0；而F列是学生为女性取值1，男性取值0。此时如果我们统计全部学籍数据，会发现对于任何一条记录来说，当M为1时F必定为0，反之当M为0时F必定为1。在这种情况下，我们将M或F去掉实际上没有任何信息的损失，因为只要保留一列就可以完全还原另一列。 当然上面是一个极端的情况，在现实中也许不会出现，不过类似的情况还是很常见的。例如上面淘宝店铺的数据，从经验我们可以知道，“浏览量”和“访客数”往往具有较强的相关关系，而“下单数”和“成交数”也具有较强的相关关系。这里我们非正式的使用“相关关系”这个词，可以直观理解为“当某一天这个店铺的浏览量较高（或较低）时，我们应该很大程度上认为这天的访客数也较高（或较低）”。后面的章节中我们会给出相关性的严格数学定义。 这种情况表明，如果我们删除浏览量或访客数其中一个指标，我们应该期待并不会丢失太多信息。因此我们可以删除一个，以降低机器学习算法的复杂度。 上面给出的是降维的朴素思想描述，可以有助于直观理解降维的动机和可行性，但并不具有操作指导意义。例如，我们到底删除哪一列损失的信息才最小？亦或根本不是单纯删除几列，而是通过某些变换将原始数据变为更少的列但又使得丢失的信息最小？到底如何度量丢失信息的多少？如何根据原始数据决定具体的降维操作步骤？ 要回答上面的问题，就要对降维问题进行数学化和形式化的讨论。而PCA是一种具有严格数学基础并且已被广泛采用的降维方法。下面我不会直接描述PCA，而是通过逐步分析问题，让我们一起重新“发明”一遍PCA。 二、向量的表示及基变换既然我们面对的数据被抽象为一组向量，那么下面有必要研究一些向量的数学性质。而这些数学性质将成为后续导出PCA的理论基础。 2.1 内积与投影下面先来看一个高中就学过的向量运算：内积。两个维数相同的向量的内积被定义为： (a_1,a_2,⋯,a_n)^𝖳⋅(b_1,b_2,⋯,b_n)𝖳=a_1b_1+a_2b_2+⋯+a_nb_n内积运算将两个向量映射为一个实数。其计算方式非常容易理解，但是其意义并不明显。下面我们分析内积的几何意义。 假设A和B是两个n维向量，我们知道n维向量可以等价表示为n维空间中的一条从原点发射的有向线段，为了简单起见我们假设A和B均为二维向量，则$A=(x_1,y_1)$，$B=(x_2,y_2)$。则在二维平面上A和B可以用两条发自原点的有向线段表示，见下图：现在我们从A点向B所在直线引一条垂线。我们知道垂线与B的交点叫做A在B上的投影，再设A与B的夹角是a，则投影的矢量长度为$|A|cos(a)|A|cos(a)$，其中$|A|=\sqrt{x^2_1+y^2_1}$是向量A的模，也就是A线段的标量长度。 注意这里我们专门区分了矢量长度和标量长度，标量长度总是大于等于0，值就是线段的长度；而矢量长度可能为负，其绝对值是线段长度，而符号取决于其方向与标准方向相同或相反。 到这里还是看不出内积和这东西有什么关系，不过如果我们将内积表示为另一种我们熟悉的形式： A⋅B=|A||B|cos(a)A与B的内积等于A到B的投影长度乘以B的模。再进一步，如果我们假设B的模为1，即让$|B|=1$，那么就变成了： A⋅B=|A|cos(a)也就是说，设向量B的模为1，则A与B的内积值等于A向B所在直线投影的矢量长度！这就是内积的一种几何解释，也是我们得到的第一个重要结论。 2.2 基下面我们继续在二维空间内讨论向量。上文说过，一个二维向量可以对应二维笛卡尔直角坐标系中从原点出发的一个有向线段。例如下面这个向量： 在代数表示方面，我们经常用线段终点的点坐标表示向量，例如上面的向量可以表示为(3,2)，这是我们再熟悉不过的向量表示。 不过我们常常忽略，只有一个(3,2)本身是不能够精确表示一个向量的。我们仔细看一下，这里的3实际表示的是向量在x轴上的投影值是3，在y轴上的投影值是2。也就是说我们其实隐式引入了一个定义：以x轴和y轴上正方向长度为1的向量为标准。那么一个向量(3,2)实际是说在x轴投影为3而y轴的投影为2。注意投影是一个矢量，所以可以为负。 更正式的说，向量(x,y)实际上表示线性组合： x(1,0)^T+y(0,1)^T不难证明所有二维向量都可以表示为这样的线性组合。此处（1，0）和（0，1）叫做二维空间中的一组基。 所以，要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了。只不过我们经常省略第一步，而默认以(1,0)和(0,1)为基。 我们之所以默认选择$(1,0)$和$(0,1)$为基，当然是比较方便，因为它们分别是x和y轴正方向上的单位向量，因此就使得二维平面上点坐标和向量一一对应，非常方便。但实际上任何两个线性无关的二维向量都可以成为一组基，所谓线性无关在二维平面内可以直观认为是两个不在一条直线上的向量。 例如，$(1,1)$和$(-1,1)$也可以成为一组基。一般来说，我们希望基的模是1，因为从内积的意义可以看到，如果基的模是1，那么就可以方便的用向量点乘基而直接获得其在新基上的坐标了！实际上，对应任何一个向量我们总可以找到其同方向上模为1的向量，只要让两个分量分别除以模就好了。例如，上面的基可以变为$(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$和$-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}$。 现在，我们想获得$(3,2)$在新基上的坐标，即在两个方向上的投影矢量值，那么根据内积的几何意义，我们只要分别计算(3,2)和两个基的内积，不难得到新的坐标为$(\frac{5}{\sqrt{2}},−\frac{1}{\sqrt{2}}$。下图给出了新的基以及(3,2)在新基上坐标值的示意图： 另外这里要注意的是，我们列举的例子中基是正交的（即内积为0，或直观说相互垂直），但可以成为一组基的唯一要求就是线性无关，非正交的基也是可以的。不过因为正交基有较好的性质，所以一般使用的基都是正交的。 2.3 基变换的矩阵表示下面我们找一种简便的方式来表示基变换。还是拿上面的例子，想一下，将$(3,2)$变换为新基上的坐标，就是用$(3,2)$与第一个基做内积运算，作为第一个新的坐标分量，然后用(3,2)与第二个基做内积运算，作为第二个新坐标的分量。实际上，我们可以用矩阵相乘的形式简洁的表示这个变换： \left[\begin{matrix} \frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\\ -\frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\\ \end{matrix}\right]\left[\begin{array}{c} 3\\ 2\\ \end{array}\right]=\left[\begin{array}{c} \frac{5}{\sqrt{2}}\\ -\frac{1}{\sqrt{2}}\\ \end{array}\right]其中矩阵的两行分别为两个基，乘以原向量，其结果刚好为新基的坐标。可以稍微推广一下，如果我们有m个二维向量，只要将二维向量按列排成一个两行m列矩阵，然后用“基矩阵”乘以这个矩阵，就得到了所有这些向量在新基下的值。例如(1,1)，(2,2)，(3,3)，想变换到刚才那组基上，则可以这样表示： \left[\begin{matrix} \frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\\ -\frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\\ \end{matrix}\right]\left[\begin{matrix} 1& 2& 3\\ 1& 2& 3\\ \end{matrix}\right]=\left[\begin{matrix} \frac{2}{\sqrt{2}}& \frac{4}{\sqrt{2}}& \frac{6}{\sqrt{2}}\\ 0& 0& 0\\ \end{matrix}\right]于是一组向量的基变换被干净的表示为矩阵的相乘。 一般的，如果我们有M个N维向量，想将其变换为由R个N维向量表示的新空间中，那么首先将R个基按行组成矩阵A，然后将向量按列组成矩阵B，那么两矩阵的乘积AB就是变换结果，其中AB的第m列为A中第m列变换后的结果。 数学表示为： \left[\begin{array}{c} p_1\\ p_2\\ ···\\ p_r\\ \end{array}\right]\left[\begin{matrix} a_1& a_2& ···& a_M\\ \end{matrix}\right]=\left[\begin{matrix} p_1a_1& p_1a_2& ···& p_1a_M\\ p_2a_1& p_2a_2& ···& p_2a_M\\ ···& ···& ···& ···\\ p_ra_1& p_ra_2& ···& p_ra_M\\ \end{matrix}\right]其中$p_i$是一个行向量，表示第$i$个基，$a_j$是一个列向量，表示第$j$个原始数据记录。 特别要注意的是，这里R可以小于N，而R决定了变换后数据的维数。也就是说，我们可以将一N维数据变换到更低维度的空间中去，变换后的维度取决于基的数量。因此这种矩阵相乘的表示也可以表示降维变换。 最后，上述分析同时给矩阵相乘找到了一种物理解释：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，一个矩阵可以表示一种线性变换。很多同学在学线性代数时对矩阵相乘的方法感到奇怪，但是如果明白了矩阵相乘的物理意义，其合理性就一目了然了。 三、协方差矩阵及优化目标上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示，而且如果基的数量少于向量本身的维数，则可以达到降维的效果。但是我们还没有回答一个最最关键的问题：如何选择基才是最优的。或者说，如果我们有一组N维向量，现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息？ 要完全数学化这个问题非常繁杂，这里我们用一种非形式化的直观方法来看这个问题。 为了避免过于抽象的讨论，我们仍以一个具体的例子展开。假设我们的数据由五条记录组成，将它们表示成矩阵形式： \left[\begin{matrix} 1& 1& 2& 4& 2\\ 1& 3& 3& 4& 4\\ \end{matrix}\right]其中每一列为一条数据记录，而一行为一个字段。为了后续处理方便，我们首先将每个字段内所有值都减去字段均值，其结果是将每个字段都变为均值为0（这样做的道理和好处后面会看到）。 我们看上面的数据，第一个字段均值为2，第二个字段均值为3，所以变换后： \left[\begin{matrix} -1& -1& 0& 2& 0\\ -2& 0& 0& 1& 1\\ \end{matrix}\right]我们可以看下五条数据在平面直角坐标系内的样子： 现在问题来了：如果我们必须使用一维来表示这些数据，又希望尽量保留原始的信息，你要如何选择？ 通过上一节对基变换的讨论我们知道，这个问题实际上是要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。 那么如何选择这个方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散。 以上图为例，可以看出如果向x轴投影，那么最左边的两个点会重叠在一起，中间的两个点也会重叠在一起，于是本身四个各不相同的二维点投影后只剩下两个不同的值了，这是一种严重的信息丢失，同理，如果向y轴投影最上面的两个点和分布在x轴上的两个点也会重叠。所以看来x和y轴都不是最好的投影选择。我们直观目测，如果向通过第一象限和第三象限的斜线投影，则五个点在投影后还是可以区分的。 下面，我们用数学方法表述这个问题。 3.1 方差上文说到，我们希望投影后投影值尽可能分散，而这种分散程度，可以用数学上的方差来表述。此处，一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值，即： Var(a)=\frac{1}{m}\sum^m_{i=1}(a_i-\mu)^2由于上面我们已经将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示： Var(a)=\frac{1}{m}\sum^m_{i=1}a_i^2于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。 3.2 协方差对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。 如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。 数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则： Cov(a,b) = \frac{1}{m}\sum_{i=1}^ma_ib_i可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m。 当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。 至此，我们得到了降维问题的优化目标：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。 3.3 协方差矩阵上面我们导出了优化目标，但是这个目标似乎不能直接作为操作指南（或者说算法），因为它只说要什么，但根本没有说怎么做。所以我们要继续在数学上研究计算方案。 我们看到，最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们来了灵感： 假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X： X=\left[\begin{matrix} a_1& a_1& ···& a_m\\ b_1& b_2& ···& b_m\\ \end{matrix}\right]然后我们用X乘以X的转置，并乘上系数$1/m$： \frac{1}{m}XX^T = \left[\begin{matrix} \frac{1}{m}\sum_{i=1}^ma_i^2& \frac{1}{m}\sum_{i=1}^ma_ib_i\\ \frac{1}{m}\sum_{i=1}^ma_ib_i& \frac{1}{m}\sum_{i=1}^mb_i^2\\ \end{matrix}\right]奇迹出现了！这个对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。 根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况：设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设￥$C=\frac{1}{m}XX^𝖳$，则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差。 3.4 协方差矩阵对角化根据上述推导，我们发现要达到优化条件，等价于将协方差对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系： 设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设$Y=PX$，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系： D = \frac{1}{m}YY^T\\ =\frac{1}{m}(PX)(PX)^T\\ =\frac{1}{m}PXX^TP\\=PCP^T现在事情很明白了，我们要找的$P$不是别的，而是能让原始协方差矩阵对角化的$P$。换句话说，优化目标变成了寻找一个矩阵$P$，满足$PCP^T$是一个对角矩阵，并且对角元素按从小到大依次排列，那么$P$的前$K$行就是要寻找的基，用$P$的前$K$行组成的矩阵乘以$X$就使得$X$从$N$维降到了$K$维并满足上述优化条件。 至此，我们离“发明”PCA还有仅一步之遥！ 现在所有焦点都聚焦在了协方差矩阵对角化问题上，有时，我们真应该感谢数学家的先行，因为矩阵对角化在线性代数领域已经属于被玩烂了的东西，所以这在数学上根本不是问题。 由上文知道，协方差矩阵C是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质： 1）实对称矩阵不同特征值对应的特征向量必然正交。 2）设特征向量λλ重数为r，则必然存在r个线性无关的特征向量对应于λλ，因此可以将这r个特征向量单位正交化。 由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为$e_1,e_2,⋯,e_n$，我们将其按列组成矩阵： E = (e_1\ e_2 \ ··· \ e_n)则对协方差矩阵$C$有如下结论： E^TCE = \varLambda\ =\left[\begin{matrix} \lambda_1& & & \\ & \lambda_2& & \\ & & ···& \\ & & & \lambda_n\\ \end{matrix}\right]其中Λ为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。以上结论不再给出严格的数学证明，对证明感兴趣的朋友可以参考线性代数书籍关于“实对称矩阵对角化”的内容。 到这里，我们发现我们已经找到了需要的矩阵P：P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照ΛΛ中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。 至此我们完成了整个PCA的数学原理讨论。在下面的一节，我们将给出PCA的一个实例。 四、算法及实例为了巩固上面的理论，我们在这一节给出一个具体的PCA实例。 4.1 PCA算法总结一下PCA的算法步骤：设有m条n维数据。 1）将原始数据按列组成n行m列矩阵X 2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值 3）求出协方差矩阵$C=\frac{1}{m}XX^T$ 4）求出协方差矩阵的特征值及对应的特征向量 5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前K行组成矩阵P 6）$Y=PX$即为降维到K维后的数据 4.2 实例这里以上文提到的 \left[\begin{matrix} -1& -1& 0& 2& 0\\ -2& 0& 0& 1& 1\\ \end{matrix}\right]为例，我们用PCA方法将这组二维数据降到一维。 因为这个矩阵的每行已经是零均值，这里我们直接求协方差矩阵： C=\frac{1}{5} \left[\begin{matrix} -1& -1& 0& 2& 0\\ -2& 0& 0& 1& 1\\ \end{matrix}\right] \left[\begin{matrix} -1& -2\\ -1& 0\\ 0& 0\\ 2& 1\\ 0& 1\\ \end{matrix}\right]=\left[\begin{matrix} \frac{6}{5}& \frac{4}{5} \\ \frac{4}{5}& \frac{6}{5} \\ \end{matrix}\right]然后求其特征值和特征向量，具体求解方法不再详述。求解后特征值为： \lambda_1=2 , \lambda_2 =\frac{2}{5}其对应的特征向量分别是： c_1\left[\begin{matrix} -2\\ 0\\ \end{matrix}\right],c_2\left[\begin{matrix} -1\\ 1\\ \end{matrix}\right]其中对应的特征向量分别是一个通解，$c_1$和$c_2$可取任意实数。那么标准化后的特征向量为： \left[\begin{matrix} \frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{2}}\\ \end{matrix}\right],\left[\begin{matrix} -\frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{2}}\\ \end{matrix}\right]因此我们的矩阵P是： P=\left[\begin{matrix} \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\ \end{matrix}\right]可以验证协方差矩阵C的对角化： PCP^T=\left[\begin{matrix} \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\ \end{matrix}\right]\left[\begin{matrix} \frac{6}{5}& \frac{4}{5} \\ \frac{4}{5}& \frac{6}{5} \\ \end{matrix}\right]\left[\begin{matrix} \frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\ \end{matrix}\right]=\left[\begin{matrix} 2&0\\ 0&\frac{2}{5}\\ \end{matrix}\right]最后我们用P的第一行乘以数据矩阵，就得到了降维后的表示： Y =\left[\begin{matrix} \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\ \end{matrix}\right] \left[\begin{matrix} -1&-1&0&2&0\\ -2&0&0&1&1\\ \end{matrix}\right]=\left[\begin{matrix} -\frac{3}{\sqrt{2}}&\frac{1}{\sqrt{2}}&0&\frac{3}{\sqrt{2}}&-\frac{1}{2}\\ \end{matrix}\right]降维投影结果如下图： 五、理论意义PCA将n个特征降维到k个，可以用来进行数据压缩，如果100维的向量最后可以用10维来表示，那么压缩率为90%。同样图像处理领域的KL变换使用PCA做图像压缩。但PCA要保证降维后，还要保证数据的特性损失最小。再看回顾一下PCA的效果。经过PCA处理后，二维数据投影到一维上可以有以下几种情况： 我们认为左图好，一方面是投影后方差最大，一方面是点到直线的距离平方和最小，而且直线过样本点的中心点。为什么右边的投影效果比较差？直觉是因为坐标轴之间相关，以至于去掉一个坐标轴，就会使得坐标点无法被单独一个坐标轴确定。 PCA得到的k个坐标轴实际上是k个特征向量，由于协方差矩阵对称，因此k个特征向量正交。 得到的新的样例矩阵$Y=PX$就是m个样例到k个特征向量的投影，也是这k个特征向量的线性组合。P中e之间是正交的。从矩阵乘法中可以看出，PCA所做的变换是将原始样本点（n维），投影到k个正交的坐标系中去，丢弃其他维度的信息。举个例子，假设宇宙是n维的（霍金说是11维的），我们得到银河系中每个星星的坐标（相对于银河系中心的n维向量），然而我们想用二维坐标去逼近这些样本点，假设算出来的协方差矩阵的特征向量分别是图中的水平和竖直方向，那么我们建议以银河系中心为原点的x和y坐标轴，所有的星星都投影到x和y上，得到下面的图片。然而我们丢弃了每个星星离我们的远近距离等信息。 六、进一步讨论根据上面对PCA的数学原理的解释，我们可以了解到一些PCA的能力和限制。PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。 因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。 PCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。 但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。 上图中黑色点表示采样数据，排列成转盘的形状。容易想象，该数据的主元是$(P_1,P_2)$或是旋转角$\theta$。在这里PCA找出的主元将是$(P_1,P_2 )$。但是这显然不是最优和最简化的主元。$(P_1,P_2 )$之间存在着非线性的关系。根据先验的知识可知旋转角$\theta$是最优的主元（类比极坐标）。则在这种情况下，PCA就会失效。但是，如果加入先验的知识，对数据进行某种划归，就可以将数据转化为以$\theta$为线性的空间中。这类根据先验知识对数据预先进行非线性转换的方法就成为kernel-PCA，它扩展了PCA能够处理的问题的范围，又可以结合一些先验约束，是比较流行的方法。 有时数据的分布并不是满足高斯分布。如图表 5所示，在非高斯分布的情况下，PCA方法得出的主元可能并不是最优的。在寻找主元时不能将方差作为衡量重要性的标准。要根据数据的分布情况选择合适的描述完全分布的变量，然后根据概率分布式 P(y_1,y_2)=P(y_1)P(y_2 )来计算两个向量上数据分布的相关性。等价的，保持主元间的正交假设，寻找的主元同样要使$P(y_1,y_2)=0$。这一类方法被称为独立主元分解(ICA)。数据的分布并不满足高斯分布，呈明显的十字星状。这种情况下，方差最大的方向并不是最优主元方向。另外PCA还可以用于预测矩阵中缺失的元素。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>降维</tag>
        <tag>非监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（21）：SVD]]></title>
    <url>%2F2017%2F07%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8821%EF%BC%89%EF%BC%9ASVD%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%E5%8F%8A%E5%85%B6%E6%84%8F%E4%B9%89%2F</url>
    <content type="text"><![CDATA[一、简介SVD实际上是数学专业内容，但它现在已经深入到不同的领域中。SVD的过程不是很好理解，因为它不够直观，但它对矩阵分解的效果却非常好。比如，Netflix（一个提供在线电影租赁的公司）曾经就悬赏100万美金，如果谁能提高他的电影推荐系统评分预测率10%的话。令人惊讶的是，这个目标充满了挑战，来自世界各地的团队运用了各种不同的技术。最终的获胜队伍“BellKor’s Pragmatic Chaos”采用的核心算法就是基于SVD。 SVD提供了一种非常便捷的矩阵分解方式，能够发现数据中十分有意思的潜在模式，在这篇文章中，我们将会提供对SVD集合上的理解和一些简单的应用实例。 1.1 几何意义奇异值分解就是把一个线性变换分解成两个线性变换，一个线性变化代表旋转，另一个代表拉伸。 线性代数中最让人印象深刻的一点是，要将矩阵和空间中的线性变化视为同样的事物。比如对角矩阵$M$作用在任何一个向量上$$\left[\begin{matrix} 3&amp; 0\\ 0&amp; 1\\\end{matrix}\right]\left[\begin{array}{c} x\\ y\\\end{array}\right]=\left[\begin{array}{c} 3x\\ y\\\end{array}\right] 其几何意义为在水平$x$方向上拉伸3倍，$y$方向保持不变的线性变换。换言之对角矩阵起到作用是将水平垂直网格作水平拉伸（或者反射后水平拉伸）的线性变换。 ![](http://omu7tit09.bkt.clouddn.com/14990677634208.jpg) 如果$M$不是对角矩阵。而是一个对称矩阵：M=\left[\begin{matrix} 2&amp; 1\\ 1&amp; 2\\\end{matrix}\right] 那么我们也总能找到一组网格线，使得矩阵作用在该网格上仅仅表现为（反射）拉伸变换，而没有发生旋转变换。这个矩阵产生的变换效果如下图所示 ![](http://omu7tit09.bkt.clouddn.com/14992394690344.jpg) 考虑一下更一般的非对称矩阵M=\left[\begin{matrix} 1&amp; 1\\ 0&amp; 1\\\end{matrix}\right] 很遗憾，此时我们再也找不到一组网格，使得矩阵作用在该网格之后只有拉伸变换（找不到背后的数学原因就是对一般非对称矩阵无法保证在实数域上可对角化）。我们退而求其次，找到一组网格，使得矩阵作用在该网格之后允许有拉伸变换和旋转变换，但要保证变换后的网格依旧互相垂直。这是可以做到的![](http://omu7tit09.bkt.clouddn.com/14992307483348.jpg) 下面我们就可以自然过渡到奇异值分解的引入。奇异值分解的几何含义为：对于任何的一个矩阵，我们要找到一组两两正交单位向量序列，使得矩阵作用在此向量序列上后得到新的向量序列保持两两正交。下面我们要说明的是，奇异值的几何含义为：这组变换后的新的向量序列的长度。![屏幕快照 2017-07-05 下午1.04.29](http://omu7tit09.bkt.clouddn.com/屏幕快照 2017-07-05 下午1.04.29.png) 当矩阵$M$作用在正交单位向量$v_1$和$v_2$上之后，得到$Mv_1$和$Mv_2$也是正交的。令$u_1$和$u_2$分别是$Mv_1$和$Mv_2$方向上的单位向量，即$Mv_1 = \sigma_1u_1$，$Mv_2 = \sigma _2 u_2$，写在一起就是$M[v_1,v_2] = [\sigma_1 u_1 \ \sigma_2 u_2]$，整理得到M=M\left[v_1\ v_2\right]\left[\begin{array}{c} v_{1}^{T}\\ v_{2}^{T}\\\end{array}\right]=\left[\sigma_1u_1 \ \sigma_2u_2\right]\left[\begin{array}{c} v_{1}^{T}\\ v_{2}^{T}\\\end{array}\right]=\left[u_1\ u_2\right]\left[\begin{matrix} \sigma_1&amp; 0\\ 0&amp; \sigma_2\\\end{matrix}\right]\left[\begin{array}{c} v_{1}^{T}\\ v_{2}^{T}\\\end{array}\right] 这样就得到矩阵$M$的奇异值分解。奇异值$\sigma_1$和$\sigma_2$分别是$Mv_1$和$Mv_2$的长度。很容易可以把结论推广到一般$n$维的情况 ## 二、、奇异值分解 ### 2.1 特征值分解 如果方阵对某个向量只产生伸缩，而不产生旋转效果，那么这个向量就称为矩阵的特征向量，伸缩的比例就是对应的特征值。 $$Ax=\lambda x 所以这其实是在平面上对一个轴进行的拉伸变换（如蓝色的箭头所示），在图中，蓝色的箭头是一个最主要的变化（变化方向可能不止一个），如果我们想要描述好一个变换，那我们描述好这个变换主要的变化方向就好了。反过来看看之前特征值分解的式子，分解得到的$\Sigma$矩阵是一个对角阵，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化的方向（从主要的变化到次要的变化排列）。 当矩阵是高维的情况下，那么这个矩阵就是高维空间下的一个线性变换，这个线性变化可能没法通过图片来表示，但是可以想象，这个变换也同样有很多的变换方向，我们通过特征值分解得到的前N个特征向量，那么就对应了这个矩阵最主要的N个变化方向。我们利用这前N个变化方向，就可以近似这个矩阵变换。也就是之前受的：提取这个矩阵最重要的特征。总结一下，特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间敢很多的事情。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。 学线性代数的时候，我们应该都学过这样一个定理： 若A为n阶实对称阵（方阵），则存在由特征值组成的对角阵$\varLambda$和特征向量组成的正交阵$Q$，使得： A=Q\varLambda Q^T 这就是我们所说的特征值分解（Eigenvalue decomposition: EVD）（$R^n → R^n$），而奇异值分解其实可以看做是特征值分解在任意矩阵$m\times n$上的推广形式($R^n →R^m$)。只有对方阵才有特征值的概念，所以对于任意的矩阵，我们引入了奇异值。 2.2 奇异值分解上面的特征值分解是一个提取矩阵特征很不错的方法，当它只是对方阵而言的，在现实的世界中，我们看到的大部分都不是方阵，比如说有N个学生，每个学生有M科成绩，这样形成的一个$N\times M$的矩阵就不可能是方阵！那么现在就来分析：对于任意的$m\times n$的矩阵，能否找到一组正交基使得经过它变换后还是正交基？答案是肯定的，它就是SVD分解的精髓所在。 下面我们从特征值分解出发，导出奇异值分解。 首先我们注意到$A^T A$为$n$阶对称矩阵，我们可以对它做特征值分解。 A^T A = VDV^T这个时候我们可以得到一组正交基，$\{v_1,v_2,···v_n\}$： (A^T A)v_i=\lambda _i v_i (Av_i,Av_j) = (Av_i)^T(Av_j) = v_i^TA^TAv_j = v_i^T(\lambda_jv_j) = \lambda_jv_i^Tv_j = 0由$r(A^T A)=r(A)=r$，这个时候我们得到了一组正交基，$\{Av_1,Av_2,···,Av_r\}$，先将其标准化，令： u_i = \frac{Av_i}{|Av_i|}=\frac{1}{\sqrt{\lambda _i}}Av_i\Rightarrow Av_i = \sqrt {\lambda _i} u_i = \delta_iu_i其中 |Av_i|^2=(Av_i,Av_i)=\lambda_iv_i^Tv_i=\lambda_i \Rightarrow |Av_i| = \sqrt {\lambda _i}=\delta _i(奇异值)将向量组$\{u_1,u_2,···,u_r\}$扩充为$R^m$中的标准正交基$\{u_1,u_2,···,u_r,···,u_m$，则： AV =A(v_1v_2···v_n) =(Av_1 \ Av_2 \ ···\ Av_r\ 0 ··· \ 0)\\=(\delta _1u_1 \ \delta_2u_2 ··· \delta _r u_r \ 0 ··· \ 0=U\Sigma\\\Rightarrow A =U\Sigma V^T我们可以从下图中直观的感受奇异值分解的矩阵相乘。任意的矩阵$A$是可以分解成三个矩阵。其中$V$表示了原始域的标准正交基，$U$表示经过A变化后的$co-domain$的标准正交基，$\Sigma$表示了$V$中的向量与$U$中相对应向量之间的关系。 在很多情况下，前10%甚至1%的奇异值的和就占了全部分奇异值之和的99%以上了。也就是说，我们也可以用前$r$大的奇异值来近似描述矩阵，这里定义一下部分奇异值分解： A_{m\times n}≈ U_{m\times r} \Sigma_{r\times r}V^T_{r\times n}$r$是一个远小于$m、n$的数，这样矩阵的乘法看起来像是下面的样子：右边的三个矩阵相乘的结果将会是一个接近于A的矩阵，在这儿，$r$越接近于n，则相乘的结果越接近于A。而这三个矩阵的面积之和（早存储观点来说，矩阵面积越小，存储量就越小）要远远小于原始的矩阵A，我们如果想要压缩空间来表示原矩阵A，我们存下这里的三个矩阵:$U、\Sigma、V$就好了。 三、应用实例3.1 推荐系统我们现在有一批高尔夫球手对九个不同hole的所需挥杆次数数据，我们希望基于这些数据建立模型，来预测选手对于某个给定hole的挥杆次数。（这个例子来自于： Singular Value Decomposition (SVD) Tutorial，强烈建议大家都去看看） 最简单的一个思路，我们队每个hole设立一个难度指标HoleDifficulty，对每位选手的能力也设立一个评价指标PlayAbility，实际的得分取决于这俩者的乘积： PredictedScore = HoleDifficulty · PlayerAbility我们可以简单地把每位选手的Ability都设为1，那么： 接着我们将HoleDifficulty 和 PlayAbility这两个向量标准化，可以得到如下的关系： 好熟悉，这不就是传说中的SVD吗，这样就出来了。 这里面蕴含了一个非常有趣的思想，也是SVD这么有用的核心： 最开始高尔夫球员和Holes之间是没有直接联系的，我们通过feature把它们联系在一起：不同的Hole进洞难度是不一样的，每个球手对进度难度的把控也是不一样的，那么我们就可以通过进洞难度这个feature将它们联系在一起，将它们乘起来就得到了我们想要的挥杆次数。 这个思想很重要，对于我们理解LSI和SVD再推荐系统中的应用相当重要。 SVD分解其实就是利用隐藏的Feature建立起矩阵行与列之间的联系。 大家可能注意到，上面那个矩阵秩为1，所以我们很容易就能将其分解，但是在实际问题中我们就得依靠SVD分解，这个时候的隐藏特征往往也不止一个了。 我们将上面的数据稍作修改：进行奇异值分解，可以得到：隐藏特征的重要性是与其对应的奇异值大小成正比的，也就是奇异值越大，其所对应的隐藏特征也越重要。 我们将这个思想推广一下 在推荐系统中，用户和物品之间没有直接联系。但是我们可以通过feature把它们联系在一起。对于电影来说，这样的特征可以是：喜剧还是悲剧，是动作片还是爱情片。用户和这样的feature之间是有关系的，比如某个用户喜欢看爱情片，另外一个用户喜欢看动作片；物品和feature之间也是有关系的，比如某个电影是喜剧，某个电影是悲剧。那么通过和feature之间的联系，我们就找到了用户和物品之间的关系。 3.2 数据压缩矩阵的奇异值是一个数学意义上的概念，一般是由奇异值分解（Singular Value Decomposition，简称SVD分解）得到。如果要问奇异值表示什么物理意义，那么就必须考虑在不同的实际工程应用中奇异值所对应的含义。下面先尽量避开严格的数学符号推导，直观的从一张图片出发，让我们来看看奇异值代表什么意义。 这是女神上野树里（Ueno Juri）的一张照片，像素为$450\times 333$我们都知道，图片实际上对应着一个矩阵，矩阵的大小就是像素大小，比如这张图对应的矩阵阶数就是450*333，矩阵上每个元素的数值对应着像素值。我们记这个像素矩阵为 AA 。现在我们对矩阵 AA 进行奇异值分解。直观上，奇异值分解将矩阵分解成若干个秩一矩阵之和，用公式表示就是： A = \sigma_1u_1v_1^T+\sigma_2u_2v_2^T+···+\sigma_ru_rv_r^T其中等式右边每一项前的系数$\sigma$就是奇异值，$u$和$v$分别表示列向量，秩一矩阵的意思是秩为1的矩阵。注意到每一项$uv^T$都是秩为1的矩阵。我们假定奇异值满足 \sigma_1≥\sigma_2≥···≥\sigma_r＞0（奇异值大于0是个重要的性质，但这里先别在意），如果不满足的话重新排列顺序即可，这无非是编号顺序的问题。 既然奇异值有从大到小排列的顺序，我们自然要问，如果只保留大的奇异值，舍去较小的奇异值，这样(1)式里的等式自然不再成立，那会得到怎样的矩阵——也就是图像？ 令 $A_1=σ_1u_1v^T_1$ ，这只保留(1)中等式右边第一项，然后作图结果就是完全看不清是啥···我们试着多增加几项进来： A_5=σ_1μ_1v^T_1+σ_2μ_2v^T_2+...+σ_5μ_5v^T_5再作图隐约可以辨别这是短发伽椰子的脸……但还是很模糊，毕竟我们只取了5个奇异值而已。下面我们取20个奇异值试试，也就是(1)式等式右边取前20项构成 A20。虽然还有些马赛克般的模糊，但我们总算能辨别出这是Juri酱的脸。当我们取到(1)式等式右边前50项时：我们得到和原图差别不大的图像。也就是说当k从1不断增大时，A_k不断的逼近A。让我们回到公式 A=σ_1μ_1v^T_1+σ_2μ_2v^T_2+...+σ_rμ_rv^T_r矩阵表示一个$450\times333$的矩阵，需要保存$450\times 333=149850$个元素的值。等式右边和分别是$450\times 1$和$333\times1$的向量，每一项有个元素。如果我们要存储很多高清的图片，而又受限于存储空间的限制，在尽可能保证图像可被识别的精度的前提下，我们可以保留奇异值较大的若干项，舍去奇异值较小的项即可。例如在上面的例子中，如果我们只保留奇异值分解的前50项，则需要存储的元素为，和存储原始矩阵相比，存储量仅为后者的26%。 奇异值往往对应着矩阵中隐含的重要信息，且重要性和奇异值大小正相关。每个矩阵A都可以表示为一系列秩为1的“小矩阵”之和，而奇异值则衡量了这些“小矩阵”对于A的权重。 奇异值分解也可以高效地表示数据。例如，假设我们想传送下列图片，包含$15 \times 25 $个黑色或者白色的像素阵列因为在图像中只有三种类型的列（如下）,它可以以更紧凑的形式被表示。就保留主要样本数据来看，该过程跟PCA( principal component analysis)技术有一些联系，PCA也使用了SVD去检测数据间依赖和冗余信息.如果对M进行奇异值分解的话，我们只会得到三个非零的奇异值。 \sigma _1 =14.72 \ \sigma_2 =5.22 \sigma _3 =3.31因此，矩阵可以如下表示 M=u_1σ_1v_1^T+ u_2σ_2v_2^T + u_3σ_3 v_3^T我们有三个包含15个元素的向量$v_i$，三个包含25个元素的向量$u_i$，以及三个奇异值$\sigma _i$，这意味着我们可以只用123个数字就能表示这个矩阵而不是出现在矩阵中的375个元素。在这种方式下，我们看到在矩阵中有三个线性独立的列，也就是说矩阵的秩是3。 3.3 图像去噪在图像处理领域，奇异值不仅可以应用在数据压缩上，还可以对图像去噪。如果一副图像包含噪声，我们有理由相信那些较小的奇异值就是由于噪声引起的。当我们强行令这些较小的奇异值为0时，就可以去除图片中的噪声。如下是一张$25*15$的图像（本例来源于[1]） 但往往我们只能得到如下带有噪声的图像（和无噪声图像相比，下图的部分白格子中带有灰色）： 通过奇异值分解，我们发现矩阵的奇异值从大到小分别为：14.15，4.67，3.00，0.21，……，0.05。除了前3个奇异值较大以外，其余奇异值相比之下都很小。强行令这些小奇异值为0，然后只用前3个奇异值构造新的矩阵，得到可以明显看出噪声减少了（白格子上灰白相间的图案减少了）。 3.4 数据分析我们搜集的数据中总是存在噪声：无论采用的设备多精密，方法有多好，总是会存在一些误差的。如果你们还记得上文提到的，大的奇异值对应了矩阵中的主要信息的话，运用SVD进行数据分析，提取其中的主要部分的话，还是相当合理的。 作为例子，假如我们搜集的数据如下所示： 我们将数据用矩阵的形式表示：经过奇异值分解后，得到 \sigma_1 = 6.04 \ \sigma _2 =0.22由于第一个奇异值远比第二个要大，数据中有包含一些噪声，第二个奇异值在原始矩阵分解相对应的部分可以忽略。经过SVD分解后，保留了主要样本点如图所示就保留主要样本数据来看，该过程跟PCA( principal component analysis)技术有一些联系，PCA也使用了SVD去检测数据间依赖和冗余信息. 3.5 潜在语义索引LSI 潜在语义索引（Latent Semantic Indexing）与PCA不太一样，至少不是实现了SVD就可以直接用的，不过LSI也是一个严重依赖于SVD的算法，之前吴军老师在矩阵计算与文本处理中的分类问题中谈到： “三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。” 上面这段话可能不太容易理解，不过这就是LSI的精髓内容，我下面举一个例子来说明一下，下面的例子来自LSA tutorial： 这就是一个矩阵，不过不太一样的是，这里的一行表示一个词在哪些title中出现了（一行就是之前说的一维feature），一列表示一个title中有哪些词，（这个矩阵其实是我们之前说的那种一行是一个sample的形式的一种转置，这个会使得我们的左右奇异向量的意义产生变化，但是不会影响我们计算的过程）。比如说T1这个title中就有guide、investing、market、stock四个词，各出现了一次，我们将这个矩阵进行SVD，得到下面的矩阵：左奇异向量表示词的一些特性，右奇异向量表示文档的一些特性，中间的奇异值矩阵表示左奇异向量的一行与右奇异向量的一列的重要程序，数字越大越重要。 继续看这个矩阵还可以发现一些有意思的东西，首先，左奇异向量的第一列表示每一个词的出现频繁程度，虽然不是线性的，但是可以认为是一个大概的描述，比如book是0.15对应文档中出现的2次，investing是0.74对应了文档中出现了9次，rich是0.36对应文档中出现了3次； 其次，右奇异向量中一的第一行表示每一篇文档中的出现词的个数的近似，比如说，T6是0.49，出现了5个词，T2是0.22，出现了2个词。 然后我们反过头来看，我们可以将左奇异向量和右奇异向量都取后2维（之前是3维的矩阵），投影到一个平面上，可以得到： 在图上，每一个红色的点，都表示一个词，每一个蓝色的点，都表示一篇文档，这样我们可以对这些词和文档进行聚类，比如说stock 和 market可以放在一类，因为他们老是出现在一起，real和estate可以放在一类，dads，guide这种词就看起来有点孤立了，我们就不对他们进行合并了。按这样聚类出现的效果，可以提取文档集合中的近义词，这样当用户检索文档的时候，是用语义级别（近义词集合）去检索了，而不是之前的词的级别。这样一减少我们的检索、存储量，因为这样压缩的文档集合和PCA是异曲同工的，二可以提高我们的用户体验，用户输入一个词，我们可以在这个词的近义词的集合中去找，这是传统的索引无法做到的。 3.6 主成分分析PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。方差的大小描述的是一个变量的信息量，我们在讲一个东西的稳定性的时候，往往说要减小方差，如果一个模型的方差很大，那就说明模型不稳定了。但是对于我们用于机器学习的数据（主要是训练数据），方差大才有意义，不然输入的数据都是同一个点，那方差就为0了，这样输入的多个数据就等同于一个数据了。以下面这张图为例子： 这个假设是一个摄像机采集一个物体运动得到的图片，上面的点表示物体运动的位置，假如我们想要用一条直线去拟合这些点，那我们会选择什么方向的线呢？当然是图上标有signal的那条线。如果我们把这些点单纯的投影到x轴或者y轴上，最后在x轴与y轴上得到的方差是相似的（因为这些点的趋势是在45度左右的方向，所以投影到x轴或者y轴上都是类似的），如果我们使用原来的xy坐标系去看这些点，容易看不出来这些点真正的方向是什么。但是如果我们进行坐标系的变化，横轴变成了signal的方向，纵轴变成了noise的方向，则就很容易发现什么方向的方差大，什么方向的方差小了。 一般来说，方差大的方向是信号的方向，方差小的方向是噪声的方向，我们在数据挖掘中或者数字信号处理中，往往要提高信号与噪声的比例，也就是信噪比。对上图来说，如果我们只保留signal方向的数据，也可以对原数据进行不错的近似了。 PCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。 还是假设我们矩阵每一行表示一个样本，每一列表示一个feature，用矩阵的语言来表示，将一个m * n的矩阵A的进行坐标轴的变化，P就是一个变换的矩阵从一个N维的空间变换到另一个N维的空间，在空间中就会进行一些类似于旋转、拉伸的变化。 A_{m\times n}P_{n\times n}=\tilde{A}_{m\times n}而将一个$m\times n$的矩阵$A$变换成一个$m\times r$的矩阵，这样就会使本来有$n$个feature的，变成了有$r$个feature了（r&lt;n），这r个其实就是对n个feature的一种提炼，我们就把这个称为feature的压缩。用数学语言表示就是： A_{m\times n}P_{n\times r}=\tilde{A}_{m\times r}但是这个和SCD扯上关系的呢？之前谈到，SVD得出的奇异向量也是从奇异值由大到小排列的，按照PCA的观点来看，就是方差最大的坐标轴就是第一个奇异向量，方差次大的坐标轴就是第二个奇异向量。之前得到的SVD式子如下： A_{m\times n}≈ U_{m\times r} \Sigma_{r\times r}V^T_{r\times n}在矩阵的两边同时乘上一个矩阵$V$，由于$V$是一个正交的矩阵，所以转置乘以$V$得到单位阵$I$，所以可以化成后面的式子： A_{m\times n}V_{r\times n}≈ U_{m\times r} \Sigma_{r\times r}V^T_{r\times n}V_{r\times n}=U_{m\times r} \Sigma_{r\times r}将后面的式子与$A\times P $那个$m\times n$矩阵变换为$m\times r$的矩阵的式子对照看看，在这里，其实$V$就是$P$，也就是一个变化的向量。这里将一个$m\times n$的矩阵压缩到一个$m \ times r$的矩阵，也就是对列进行压缩。 如果我们想对行进行压缩（在PCA的观点下，对行进行业所可以理解为，讲一些相似的sample合并在一起，或者将一些没有太大价值的sample去掉），同样我们写出一个通用的行压缩例子： P_{r\times m}A_{m\times n}=\tilde{A}_{r\times n}这样从一个$m$行的矩阵压缩到一个$r$行的矩阵了，对SVD来说也是一样的，我们对SVD分解的式子两边乘以一个转置$U$得到： U^T_{r\times m}A_{m\times n}≈ U^T_{r\times m}U_{m\times r} \Sigma_{r\times r}V^T_{r\times n}= \Sigma_{r\times r}V^T_{r\times n} 这样我们就得到了对行进行压缩的式子。可以看出，其实PCA几乎可以说是对SVD的一个包装，如果我们实现了SVD，那也就实现了PCA了，而且更好的地方是，有了SVD，我们就可以得到两个方向的PCA，如果我们对A’A进行特征值的分解，只能得到一个方向的PCA。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笑忘录（8）：茶卡盐湖-青海湖之行]]></title>
    <url>%2F2017%2F06%2F26%2F%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%888%EF%BC%89%EF%BC%9A%E8%8C%B6%E5%8D%A1%E7%9B%90%E6%B9%96-%E9%9D%92%E6%B5%B7%E6%B9%96%E4%B9%8B%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[这是第一次去西北。 四天的行程，印象最深还是它的天，干净到不忍心去增添一点点其余的色彩，生怕破坏了它原本的样子。 这个废旧的火车头安详地躺在轨道上，依傍着茶卡盐湖，携带着机械复制时代内燃机车独有的刚毅在湖畔守候着，迎接着这个世纪的人，等待他们挥扬双手、露出笑容和它留下一张张合影。 置身这片广袤的土地，被它的空气包裹，只想安静地闭上眼睛，感受这个神奇世界的馈赠。 火车头底下铺着五十多米长的铁轨、垫着被岁月勾兑地五黑而厚重的木枕木，枕木中间均匀散落茶卡盐湖的结晶盐，像一道白色的光，顺着火车头，向后延展。 依旧是火车头，衣服的搭配和它看起来很契合，便在此处快门了好几十次。多么想把这轨道揉进苍茫的大地，它横在那里，丝毫没有感受到来自东方神域的呼喊，反而与四处来的人打成了一片。 那天风很疾，打在脸上，来不及躲闪。坐在长凳子上，望着远处的山，它们似乎离自己很近，仅有的修饰就是顶上的那层雪，它不懂得如何变得招人喜欢，大自然给予它什么，它就接受什么样的。我需要你冷静的表情。 云层慢慢变厚了 春天是什么样子的？ 丹霞地貌，它的土是暗红色的。 阳光洒下来，透过你，穿过镜头，直抵我眼眶 一大波风景照向你袭来]]></content>
      <categories>
        <category>笑忘录</category>
      </categories>
      <tags>
        <tag>茶卡盐湖</tag>
        <tag>青海湖</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（20）：机器学习模型优化四要素]]></title>
    <url>%2F2017%2F06%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8820%EF%BC%89%EF%BC%9A%E9%A1%B9%E7%9B%AE%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E5%9B%9B%E8%A6%81%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[本文转载自美团点评技术团队博客，该文以业界视角介绍了机器学习如何发挥其实际价值。作者胡淏，目前是美团算法工程师，毕业于哥伦比亚大学。先后在携程、支付宝、美团从事算法开发工作。了解风控、基因、旅游、即时物流相关问题的行业领先算法方案与流程。 一、机器学习工程师的知识图谱图1 机器学习工程师的知识图谱 上图列出了我认为一个成功的机器学习工程师需要关注和积累的点。机器学习实践中，我们平时都在积累自己的“弹药库”：分类、回归、无监督模型、Kaggle上特征变换的黑魔法、样本失衡的处理办法、缺失值填充……这些大概可以归类成模型和特征两个点。我们需要参考成熟的做法、论文，并自己实现，此外还需要多反思自己方法上是否还可以改进。如果模型和特征这俩个点都已经做的很好了，你就拥有了一张绿卡，能跨过在数据相关行业发挥模型技术价值的准入门槛。 在这个时候，比较关键的一步，就是搞笑的技术变现能力。 所谓高效，就是解决业务核心问题的专业能力。本文将描述这些专业能力，也就是模型优化的四个要素：模型、数据、特征、业务，还有更重要的，就是他们在模型项目中的优先级。 二、模型项目推进的四要素项目推进过程中，四个要素相互之间的优先级大致是：业务&gt;特征&gt;数据&gt;模型。图2 四要素解决问题细分+优先级 2.1 业务一个模型项目有好的技术选型、完备的特征体系、高质量的数据一定是很加分的，不过真正决定项目好与坏还有一个大前提，就是在这个项目的技术目标是否在解决当下核心业务问题。 业务问题包含两个方面：业务KPI和Deadline。举个例子，业务问题在两周之内降低目前手机丢失带来的支付宝销赃风险。这时如果你的方案是研发手机丢失的核心特征，比如改密是否合理，基本上就死的很惨，因为两周根本完不成，改密合理性也未必是模型优化好的切入点；反之，如果你的方案是和运营同学看bad case，梳理现阶段的作案通用手段，并通过分析上线一个简单模型或者业务规则的补丁，就明智很多。如果上线之后，案件量真掉下来了，就算你的方案准确率很糟糕、方法很low，但你解决了业务问题，这才是最重要的。 虽然业务目标很关键，不过一般讲，业务运营同学真的不太懂得如何和技术有效的沟通业务目标，比如： 我们想做一个线下门店风险评级的项目，希望运营通过反作弊模型角度帮我们给门店打个分，这个分数包含的问题有：风险是怎么定义的、为什么要做风险评级、更大的业务目标是什么、怎么排期的、这个风险和我们反作弊模型之间的腋窝你是怎么看的？ 做一个区域未来10min的配送时间预估模型。我们想通过运营的模型衡量在恶劣天气的时候每个区域的运力是否被击穿（业务现状和排期？运力被击穿可以扫下盲吗？运力击穿和配送时间之间是个什么业务逻辑、时间预估是刻画运力紧张度的最有效手段么？业务的关键场景是恶劣天气的话，我们仅仅训练恶劣天气场景的时间预估模型是否就好了？） 为了保证整个技术项目没有做偏，项目一开始一定要和业务聊清楚三件事情： 业务核心问题、关键场景是什么。 如何评估该项目的成功，指标是什么。 通过项目输出什么关键信息给到业务，业务如何运营这个信息从而达到业务目标。 项目过程中，也要时刻回到业务，检查项目的健康度。 2.2 数据与特征要说正确的业务理解和切入，在为技术项目保驾护航，数据、特征便是一个模型项目性能方面的天花板。garbage in， garbage out 就在说这个问题。 这两天有位听众微信问我一个很难回答的问题，大概意思是，数据是特征拼起来构成的集合嘛，所以这不是两个要素。从逻辑上面讲，数据的确是一列一列的特征，不过数据与特征在概念层面是不同的：数据是已经采集的信息，特征是以兼容模型、最优化为目标对数据进行加工。就比如通过word2vec将非结构化数据结构化，就是将数据转化为特征的过程。 所以，我更认为特征工程是基于数据的一个非常精细、刻意的加工过程。从传统的特征转换、交互，到embedding、word2vec、高维分类变量数值化，最终目的都是更好的去利用现有的数据。之前有聊到的将推荐算法引入有监督学习模型优化中的做法，就是在把两个本不可用的高维ID类变量变成可用的数值变量。 观察到自己和童鞋们在特征工程中会遇到一些普遍问题，比如，特征设计不全面，没有耐心把现有特征做得细致……也整理出来一套方法论，仅供参考： 图3 变量体系、研发流程 在特征设计的时候，有两个点可以帮助我们把特征想的更全面： 现有的基础数据 业务“二维图” 这两个方面的整合，就是一个变量的体系。变量（特征），从技术层面是加工数据，而从业务层面实际在反应RD的业务理解和数据刻画业务能力。“二维图”，实际上未必是二维的，更重要的是我们需要把业务整个流程抽象成几个核心的维度，举几个例子： 这两个方面的整合，就是一个变量的体系。变量（特征），从技术层面是加工数据，而从业务层面实际在反应RD的业务理解和数据刻画业务能力。“二维图”，实际上未必是二维的，更重要的是我们需要把业务整个流程抽象成几个核心的维度，举几个例子： 外卖配送时间业务（维度甲：配送的环节，骑手到点、商家出餐、骑手配送、交付用户；维度乙：颗粒度，订单粒度、商家粒度、区域城市粒度；维度丙：配送类型，众包、自营……）。 反作弊变量体系（维度甲：作弊环节，登录、注册、实名、转账、交易、参与营销活动、改密……；维度乙：作弊介质，账户、设备、IP、WiFi、银行卡……）。 通过这些维度，你就可以展开一个“二维图”，把现有你可以想到的特征填上去，你一定会发现很多空白，比如下图，那么哪里还是特征设计的盲点就一目了然： 图4 账户维度在转账、红包方面的特征很少；没有考虑WiFi这个媒介；客满与事件数据没考虑 数据和特征决定了模型性能的天花板。deep learning当下在图像、语音、机器翻译、自动驾驶等领域非常火，但是 deep learning在生物信息、基因学这个领域就不是热词：这背后是因为在前者，我们已经知道数据从哪里来，怎么采集，这些数据带来的信息基本满足了模型做非常准确的识别；而后者，即便有了上亿个人体碱基构成的基因编码，技术选型还是不能长驱直入——超高的数据采集成本，人后天的行为数据的获取壁垒等一系列的问题，注定当下这个阶段在生物信息领域，人工智能能发出的声音很微弱，更大的舞台留给了生物学、临床医学、统计学。 2.3 模型图5 满房开房的技术选型、特征工程roadmap 模型这件事儿，许多时候追求的不仅仅是准确率，通常还有业务这一层更大的约束。如果你在做一些需要强业务可解释的模型，比如定价和反作弊，那实在没必要上一个黑箱模型来为难业务。这时候，统计学习模型就很有用。 这种情况下，比拼性能的话，我觉得下面这个不等式通常成立：Glmnet&gt;LASSO&gt;=Ridge&gt;LR/Logistic。相比最基本的LR/Logistic，ridge通过正则化约束缓解了LR在过拟合方面的问题，lasso更是通过L1约束做类似变量选择的工作。 不过两个算法的痛点是很难决定最优的约束强度，Glmnet是Stanford给出的一套非常高效的解决方案。所以目前，我认为线性结构的模型，Glmnet的痛点是最少的，而且在R、Python、Spark上面都开源了。 如果我们开发复杂模型，通常成立第二个不等式 RF（Random Forest，随机森林）&lt;= GBDT &lt;= XGBoost 。拿数据说话，29个Kaggle公开的winner solution里面，17个使用了类似GBDT这样的Boosting框架，其次是 DNN（Deep Neural Network，深度神经网络），RF的做法在Kaggle里面非常少见。 RF和GBDT两个算法的雏形是CART（Classification And Regression Trees），由L Breiman和J Friedman两位作者在1984年合作推出。但是在90年代在发展模型集成思想the ensemble的时候，两位作者代表着两个至今也很主流的派系：stacking/ Bagging &amp; Boosting。 一种是把相互独立的CART（randomized variables，bootstrap samples）水平铺开，一种是深耕的Boosting，在拟合完整体后更有在局部长尾精细刻画的能力。同时，GBDT模型相比RF更加简单，内存占用小，这都是业界喜欢的性质。XGBoost在模型的轻量化和快速训练上又做了进一步的工作，也是目前我们比较喜欢尝试的模型。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>业务</tag>
        <tag>特征</tag>
        <tag>数据</tag>
        <tag>模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle系列（4）：Rental Listing Inquiries（三）：XGBoost调参指南]]></title>
    <url>%2F2017%2F06%2F14%2Fkaggle%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9ARental%20Listing%20Inquiries%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9AXGBoost%E8%B0%83%E5%8F%82%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[这篇文章翻译自Complete Guide to Parameter Tuning in XGBoost (with codes in Python)，它详细介绍了XGBoost中参数的含义，然后在一个实例中对参数调整进行了实验。 一、Introduction如果你的预测模型效果不怎么好，使用XGBoost吧。XGBoost已经成为许多数据科学家的终极武器了！这是一个内部实现高度复杂的算法，在处理各种不规范的数据时有足够强大的表现。 利用XGBoost建立模型很简单，但是因为它使用了很多参数，以致使用XGBoost来提升预测效果比较有难度。为提高模型的预测能力，调参是必须要做的一步。但仍然有很多现实的挑战——哪些参数是我们需要调整的？每个参数的最佳值又应该是多少呢？ 这篇文章最适合刚刚接触XGBoost的人，在这篇文章中，我们将会介绍一些XGBoost相关的知识，同时了解一些XGBoost参数调整技艺。最后使用Python对一个数据集实践XGBoost。 二、What should you know ?XGBoost(eXtreme Gradient Boosting)是Gradient Boosting算法的一个优化的版本。因为我在前一篇文章，基于Python的Gradient Boosting算法参数调整完全指南，里面已经涵盖了Gradient Boosting算法的很多细节了。我强烈建议大家在读本篇文章之前，把那篇文章好好读一遍。它会帮助你对Boosting算法有一个宏观的理解，同时也会对GBM的参数调整有更好的体会。 特别鸣谢：我个人十分感谢Mr Sudalai Rajkumar (aka SRK)大神的支持，目前他在AV Rank中位列第二。如果没有他的帮助，就没有这篇文章。在他的帮助下，我们才能给无数的数据科学家指点迷津。给他一个大大的赞！ 三、Table of Contents3.1 The XGBoost AdvantageXGBoost算法可以给预测模型带来能力的提升。当我对它的表现有更多了解的时候，当我对它的高准确率背后的原理有更多了解的时候，我发现它具有很多优势： 正则化标准GBM的实现没有像XGBoost这样的正则化步骤。正则化对减少过拟合也是有帮助的。实际上，XGBoost以“正则化提升(regularized boosting)”技术而闻名。 并行处理不过，众所周知，Boosting算法是顺序处理的，它怎么可能并行呢？每一课树的构造都依赖于前一棵树，那具体是什么让我们能用多核处理器去构造一个树呢？我希望你理解了这句话的意思。如果你希望了解更多，点击这个链接。XGBoost 也支持Hadoop实现。 高度的灵活性XGBoost 允许用户定义自定义优化目标和评价标准它对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。 缺失值处理XGBoost内置处理缺失值的规则。用户需要提供一个和其它样本不同的值，然后把它作为一个参数传进去，以此来作为缺失值的取值。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。 剪枝当分裂时遇到一个负损失时，GBM会停止分裂。因此GBM实际上是一个贪心算法。XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂。这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。GBM会在-2处停下来，因为它遇到了一个负值。但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂。 内置交叉验证XGBoost允许在每一轮boosting迭代中使用交叉验证。因此，可以方便地获得最优boosting迭代次数。而GBM使用网格搜索，只能检测有限个值。 在已有的模型基础上继续XGBoost可以在上一轮的结果上继续训练。这个特性在某些特定的应用上是一个巨大的优势。sklearn中的GBM的实现也有这个功能，两种算法在这一点上是一致的。 相信你已经对XGBoost强大的功能有了点概念。注意这是我自己总结出来的几点，你如果有更多的想法，尽管在下面评论指出，我会更新这个列表的！你的胃口被我吊起来了吗？棒棒哒！如果你想更深入了解相关信息，可以参考下面这些文章： XGBoost Guide - Introduce to Boosted Trees Words from the Auther of XGBoost 3.2 Understanding XGBoost ParametersXGBoost的作者把所有参数分成了三类： 通用参数：宏观函数控制。 Booster参数：控制每一步的Booster（tree/regression） 学习目标参数：控制训练目标的表现 在这里我会类比GBM来讲解，所以作为一种基础知识，强烈推荐先阅读这篇文章。 3.2.1 通用参数（General Parameters）这些参数用来控制XGBoost的宏观功能。 booster[默认gbtree]：选择每次迭代的模型，有两种选择： gbtree：基于树的模型 gbliner：线性模型 silent[默认0]： 当这个参数值为1时，静默模式开启，不会输出任何信息。一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。 nthread[默认值为最大可能的线程数]： 这个参数用来进行多线程控制，应当输入系统的核数。如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。 还有两个参数，XGBoost会自动设置，目前你不用管它。接下来咱们一起看booster参数。 3.2.2 Booster参数（Booster Parameters）尽管有两种booster可供选择，我这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到。 eta[默认0.3]： 和GBM中的 learning rate 参数类似。通过减少每一步的权重，可以提高模型的鲁棒性。典型值为0.01-0.2。 min_child_weight[默认1]： 决定最小叶子节点样本权重和。和GBM的min_child_leaf 参数类似，但不完全一样。XGBoost的这个参数是最小样本权重的和，而GBM参数是最小样本总数。这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。 max_depth[默认6]： 和GBM中的参数相同，这个值为树的最大深度。这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。需要使用CV函数来进行调优。典型值：3-10 max_leaf_nodes： 树上最大的节点或叶子的数量。可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成 $n^2$ 个叶子。如果定义了这个参数，GBM会忽略max_depth参数。 gamma[默认0]： 在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。 max_delta_step[默认0]： 这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。 通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。 这个参数一般用不到，但是你可以挖掘出来它更多的用处。 subsample[默认1]： 和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。典型值：0.5-1。 colsample_bytree[默认1]： 和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。典型值：0.5-1 colsample_bylevel[默认1]： 用来控制树的每一级的每一次分裂，对列数的采样的占比。我个人一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。 lambda[默认1] 权重的L2正则化项。(和Ridge regression类似)。这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。 alpha[默认1]： 权重的L1正则化项。(和Lasso regression类似)。可以应用在很高维度的情况下，使得算法的速度更快。 scale_pos_weight[默认1]： 在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。 3.2.3 学习目标参数这个参数用来控制理想的优化目标和每一步结果的度量方法。 objective[默认reg:linear]：这个参数定义需要被最小化的损失函数。最常用的值有： binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。 multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。在这种情况下，你还需要多设一个参数：num_class(类别数目)。 multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。 eval_metric[默认值取决于objective参数的取值]：对于有效数据的度量方法。对于回归问题，默认值是rmse，对于分类问题，默认值是error。典型值有： rmse 均方根误差( ∑Ni=1ϵ2N−−−−−−√ ) mae 平均绝对误差( ∑Ni=1|ϵ|N ) logloss 负对数似然函数值 error 二分类错误率(阈值为0.5) merror 多分类错误率 mlogloss 多分类logloss损失函数 auc 曲线下面积 seed(默认0)：随机数的种子。设置它可以复现随机数据的结果，也可以用于调整参数 如果你之前用的是Scikit-learn,你可能不太熟悉这些参数。但是有个好消息，python的XGBoost模块有一个sklearn包，XGBClassifier。这个包中的参数是按sklearn风格命名的。会改变的函数名是： eta -&gt;learning_rate lambda-&gt;reg_lambda alpha-&gt;reg_alpha 你肯定在疑惑为啥咱们没有介绍和GBM中的n_estimators类似的参数。XGBClassifier中确实有一个类似的参数，但是，是在标准XGBoost实现中调用拟合函数时，把它作为num_boosting_rounds参数传入。 XGBoost Guide 的一些部分是我强烈推荐大家阅读的，通过它可以对代码和参数有一个更好的了解： XGBoost Parameters (official guide) XGBoost Demo Codes (xgboost GitHub repository) Python API Reference (official guide) 3.3 Tuning Parameters (with Example) 未完待续······]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
        <tag>XGBoost</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle系列（3）：Rental Listing Inquiries（二）：XGBoost]]></title>
    <url>%2F2017%2F06%2F13%2Fkaggle%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9ARental%20Listing%20Inquiries%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9AXGBoost%2F</url>
    <content type="text"><![CDATA[上一节我们对数据集进行了初步的探索，并将其可视化，对数据有了初步的了解。这样我们有了之前数据探索的基础之后，就有了对其建模的基础feature，结合目标变量，即可进行模型训练了。我们使用交叉验证的方法来判断线下的实验结果，也就是把训练集分成两部分，一部分是训练集，用来训练分类器，另一部分是验证集，用来计算损失评估模型的好坏。 在Kaggle的希格斯子信号识别竞赛中，XGBoost因为出众的效率与较高的预测准确度在比赛论坛中引起了参赛选手的广泛关注，在1700多支队伍的激烈竞争中占有一席之地。随着它在Kaggle社区知名度的提高，最近也有队伍借助XGBoost在比赛中夺得第一。其次，因为它的效果好，计算复杂度不高，也在工业界中有大量的应用。 今天，我们就先来跑一个XGBoost版的Base Model。先回顾一下XGBoost的原理吧：机器学习算法系列（8）：XgBoost 一、 准备工作首先我们导入需要的包： 12345678910import osimport sys import operatorimport numpy as npimport pandas as pdfrom scipy import sparseimport xgboost as xgbfrom sklearn import model_selection,preprocessing,ensemblefrom sklearn.metrics import log_lossfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer 其中一些包的用途会在之后具体用到的时候进行讲解。 导入我们的数据： 12345678910data_path = '../data/'train_file = data_path + "train.json"test_file = data_path +"test.json"train_df = pd.read_json(train_file)test_df = pd.read_json(test_file)print train_df.shapeprint test_df.shape(49352, 15)(74659, 14) 查看一下前两行： 1train_df.head(2) 二、特征构建我们不需要对数值型数据进行任何的预处理，所以首先建立一个数值型特征的列表，纳入features_to_use 1features_to_use = ["bathrooms","bedrooms","latitude","longitude","price"] 现在让我们根据已有的一些特征来构建一些新的特征： 1234567891011121314151617181920212223242526272829303132# 照片数量(num_photos)train_df['num_photos']=train_df['photos'].apply(len)test_df['num_photos']=train_df['photos'].apply(len)# 特征数量train_df['num_features']=train_df['features'].apply(len)test_df['num_features']=test_df['features'].apply(len)# 描述词汇数量train_df['num_description_words'] = train_df['description'].apply(lambda x: len(x.split(" ")))test_df['num_description_words'] = test_df['description'].apply(lambda x: len(x.split(" ")))#把创建的时间分解为多个特征 train_df['created']=pd.to_datetime(train_df['created'])test_df['created']=pd.to_datetime(test_df['created']) #让我们从时间中分解出一些特征，比如年，月，日，时#年train_df['created_year'] = train_df['created'].dt.yeartest_df['created_year'] = test_df['created'].dt.year#月train_df['created_month'] = train_df['created'].dt.monthtest_df['created_month'] = test_df['created'].dt.month#日train_df['created_day'] = train_df['created'].dt.daytest_df['created_day'] = test_df['created'].dt.day#时train_df['created_hour'] = train_df['created'].dt.hourtest_df['created_hour'] = test_df['created'].dt.hour#把这些特征都放到所需特征列表中（上面已经创建，并加入了数值型特征） features_to_use.extend(["num_photos","num_features","num_description_words","created_year","created_month","created_day","created_hour","listing_id"]) 我们有四个分类型的特征： display_address manager_id building_id street_address 可以对它们分别进行特征编码： 12345678categorical = ["display_address","manager_id",'building_id',"street_address"]for f in categorical: if train_df[f].dtype == 'object': lbl = preprocessing.LabelEncoder() lbl.fit(list(train_df[f].values)+list(test_df[f].values)) train_df[f] = lbl.transform(list(train_df[f].values)) test_df[f] = lbl.transform(list(test_df[f].values)) features_to_use.append(f) 还有一些字符串类型的特征，可以先把它们合并起来 1234train_df[&quot;features&quot;] = train_df[&quot;features&quot;].apply(lambda x:&quot; &quot;.join([&quot;_&quot;.join(i.split(&quot; &quot;))for i in x]))print train_df[&apos;features&apos;].head(2)test_df[&apos;features&apos;] = test_df[&quot;features&quot;].apply(lambda x: &quot; &quot;.join([&quot;_&quot;.join(i.split(&quot; &quot;))for i in x]))print test_df[&apos;features&apos;].head(2) 得到的字符串结果如下： 10000 Doorman Elevator Fitness_Center Cats_Allowed D…100004 Laundry_In_Building Dishwasher Hardwood_Floors… 然后CountVectorizer类来计算TF-IDF权重 123tfidf = CountVectorizer(stop_words =&quot;english&quot;,max_features=200)tr_sparse = tfidf.fit_transform(train_df[&quot;features&quot;])te_sparse = tfidf.transform(test_df[&quot;features&quot;]) 这里我们需要提一点，对数据集进行特征变换时，必须同时对训练集和测试集进行操作。现在把这些处理过的特征放到一个集合中（横向合并） 12train_X = sparse.hstack([train_df[features_to_use],tr_sparse]).tocsr()test_X = sparse.hstack([test_df[features_to_use],te_sparse]).tocsr() 然后把目标变量转换为0、1、2，如下 12345target_num_map = &#123;'high':0 , 'medium':1 , 'low':2&#125;train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))print train_X.shape,test_X.shape(49352, 217) (74659, 217) 可以看到，经过上面一系列的变量构造之后，其数量已经达到了217个。 接下来就可以进行建模啦。 三、XGB建模先写一个通用的XGB模型的函数： 12345678910111213141516171819202122232425262728def runXGB(train_X,train_y,test_X,test_y=None,feature_names=None,seed_val=0,num_rounds=1000): #参数设定 param = &#123;&#125; param['objective'] = 'multi:softprob'#多分类、输出概率值 param['eta'] = 0.1#学习率 param['max_depth'] = 6#最大深度，越大越容易过拟合 param['silent'] = 1#打印提示信息 param['num_class'] = 3#三个类别 param['eval_metric']= "mlogloss"#对数损失 param['min_child_weight']=1#停止条件，这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 param['subsample'] =0.7#随机采样训练样本 param['colsample_bytree'] = 0.7# 生成树时进行的列采样 param['seed'] = seed_val#随机数种子 num_rounds = num_rounds#迭代次数 plst = list(param.items()) xgtrain = xgb.DMatrix(train_X,label=train_y) if test_y is not None: xgtest = xgb.DMatrix(test_X,label=test_y) watchlist = [(xgtrain,'train'),(xgtest,'test')] model = xgb.train(plst,xgtrain,num_rounds,watchlist,early_stopping_rounds=20) # early_stopping_rounds 当设置的迭代次数较大时，early_stopping_rounds 可在一定的迭代次数内准确率没有提升就停止训练 else: xgtest = xgb.DMatrix(test_X) model = xgb.train(plst,xgtrain,num_rounds) pred_test_y = model.predict(xgtest) return pred_test_y,model 函数返回的是预测值和模型。 5折交叉验证将训练集划分为五份，其中的一份作为验证集。 12345678910cv_scores = []kf = model_selection.KFold(n_splits=5,shuffle=True,random_state=2016)for dev_index,val_index in kf.split(range(train_X.shape[0])): dev_X,val_X = train_X[dev_index,:],train_X[val_index,:] dev_y,val_y = train_y[dev_index],train_y[val_index] pred,model = runXGB(dev_X,dev_y,val_X,val_y) cv_scores.append(log_loss(val_y,preds)) print cv_scores break 结果如下： 123456789101112131415161718192021222324252627282930[0] train-mlogloss:1.04135 test-mlogloss:1.04229Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.Will train until test-mlogloss hasn't improved in 20 rounds.[1] train-mlogloss:0.989004 test-mlogloss:0.99087[2] train-mlogloss:0.944233 test-mlogloss:0.947047[3] train-mlogloss:0.90536 test-mlogloss:0.908933[4] train-mlogloss:0.872054 test-mlogloss:0.876526[5] train-mlogloss:0.841783 test-mlogloss:0.847383[6] train-mlogloss:0.815921 test-mlogloss:0.822307[7] train-mlogloss:0.793337 test-mlogloss:0.800476[8] train-mlogloss:0.773562 test-mlogloss:0.781413[9] train-mlogloss:0.754927 test-mlogloss:0.76381[10] train-mlogloss:0.738299 test-mlogloss:0.747959············[367] train-mlogloss:0.348196 test-mlogloss:0.548011[368] train-mlogloss:0.347768 test-mlogloss:0.547992[369] train-mlogloss:0.347303 test-mlogloss:0.548021[370] train-mlogloss:0.346807 test-mlogloss:0.548065[371] train-mlogloss:0.346514 test-mlogloss:0.548079[372] train-mlogloss:0.34615 test-mlogloss:0.548097[373] train-mlogloss:0.345859 test-mlogloss:0.548111[374] train-mlogloss:0.345377 test-mlogloss:0.548081[375] train-mlogloss:0.344961 test-mlogloss:0.548068[376] train-mlogloss:0.344493 test-mlogloss:0.548024[377] train-mlogloss:0.344086 test-mlogloss:0.547975Stopping. Best iteration:[357] train-mlogloss:0.352182 test-mlogloss:0.547867 迭代357次之后，在训练集上的对数损失为0.352182，在验证集上的损失为0.5478。 然后在对测试集进行预测： 1preds,model=runXGB(train_X,train_y,test_X,num_rounds=400) 把结果按照比赛规定的格式写入csv文件： 1234out_df = pd.DataFrame(preds)out_df.columns = ["high", "medium", "low"]out_df["listing_id"] = test_df.listing_id.valuesout_df.to_csv("xgb_starter2.csv", index=False) 看一下最后的结果：提交到kaggle上，这样我们整个建模的过程就完成了。 接下来两节中，我们重点讲一讲关于XGBoost的调参经验以及使用SK-learn计算TF-IDF。]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
        <tag>XGBoost</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle系列（2）：Rental Listing Inquiries（一）：EDA]]></title>
    <url>%2F2017%2F06%2F13%2Fkaggle%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9ARental%20Listing%20Inquiries%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AEDA%2F</url>
    <content type="text"><![CDATA[一、比赛简介1.1 比赛目的这个kaggle比赛是由Sigma和RentHop两家公司共同推出的比赛。比赛的数据来自于RentHop的租房信息，大概的思路就是根据出租房的一系列特征，比如地理位置（经纬度、街道地址）、发布时间、房间设施（浴室、卧室数量）、描述信息、发布的图片信息、价格等来预测消费者对出租房的喜好程度。 这样可以帮助RentHop公司更好地处理欺诈事件，让房主和中介更加理解租客的需求与偏好，做出更加合理的决策。 1.2 数据集在这个比赛中，房源的数据来自于renthop网站，这些公寓都位于纽约市。其目的之前已经提到过了，就是基于一系列特征预测公寓房源的受欢迎程度，其目标变量是：interest_level，它是指从在网站上发布房源起始的时间内，房源的询问次数。 其中，比赛一共给了五个数据文件，分别是： train.json：训练集 test.json：测试集 sample_submission.csv：格式正确的提交示例 images_sample.zip：租房图片集（只抽取了100个图片集） kaggle-renthop.7z：所有的租房图片集，一共有78.5GB的压缩文件。 给出的特征的含义： bathrooms: 浴室的数量 bedrooms: 卧室的数量 building_id： created：发布时间 description：一些描述 display_address：展出地址 features: 公寓的一些特征 latitude：纬度 listing_id longitude：经度 manager_id：管理ID photos: 租房图片集 price: 美元 street_address：街道地址 interest_level: 目标变量，受欢迎程度. 有三个类: ‘high’, ‘medium’, ‘low’ 1.3 提交要求这个比赛使用的是多分类对数似然损失函数来评价模型。因为每一个房源都有一个对应的最准确的类别，对每一个房源，需要提交它属于每一类的概率值，它的计算公式如下： \log loss=-\frac{1}{N}\sum_{i=1}^N{\sum_{j=1}^M{y_{ij}\log\left(p_{ij}\right)}}其中$N$是测试集中的样本数量，$M$是类别的数量（3类：high、medium、low）,$log$是自然对数，$y_{ij}$表示样本$i$属于$j$类则为1，否则为0.$p_{ij}$表示样本$i$属于类别$j$的预测概率值。 一个样本的属于三个类别的预测可能性不需要加和为1，因为已经预先归一化了。为避免对数函数的极端情况，预测概率被替代为$max(min(p,1-10^{-15}),10^{-15})$ 最后提交的文件为csv格式，它包含对每一类的预测概率值，行的顺序没有要求，文件必须要有一个表头，看起来像下面的示例： listing_id high medium low 7065104 0.07743 0.23002 0.69254 7089035 0.0 1.0 0.0 二、Exploratory Data Analysis在进行建模之前，我们都会对原始数据进行一些可视化探索，以便更快地熟悉数据，更有效进行之后的特征工程和建模。 我们先导入一些EDA过程中所需要的包： 12345678910import numpy as np import pandas as pd import matplotlib.pyplot as pltimport seaborn as snsimport jsoncolor = sns.color_palette() # 调色板%matplotlib inlinepd.options.mode.chained_assignment = None # default = 'warn' 其中numpy和pandas是数据分析处理中最流行的包，matplotlib和seaborn两个包用来绘制可视化图像，使用%matplotlib命令可以将matplotlib的图表直接嵌入到Notebook之中（%是魔术命令）。 2.1 数据初探使用pandas打开训练集文件train.json，取前两行观测： 12train_df = pd.read_json('data/train.json')train_df.head(8) 我们可以看到给定的数据中包含各种类型的特征，按照其特征可以分为以下几个类别： 特征类型 特征 数值型 bathrooms、bedrooms、price 高势集类别 building_id、display_address、manager_id、street_address 时间型 created 地理位置型特征 longitude、latitude 文本 description 稀疏特征 features id型特征 listing_id、index 看一下训练集和测试集分别有多少 12print "Train Rows:",train_df.shape[0]print "Test Rows:",test_df.shape[0] Train Rows: 49352Test Rows: 74659 训练集有49352个样例，测试集有74659个样例。 接下来我们一一对这些特征进行探索。 2.2 目标变量在深入探索之前，我们先看看目标变量Interest level 123456int_level = train_df['interest_level'].value_counts()plt.figure(figsize=(10,5))sns.barplot(int_level.index,int_level.values,alpha=0.8,color=color[2])plt.xlabel("number of occurrences",fontsize = 12)plt.ylabel("Interest Level",fontsize=12)plt.show() 兴趣度在大多数情况下都是低的，其次是中等，只有少部分的样例为高分。 2.3 数值型特征2.3.1 浴室（bathrooms）先看看浴室的数量分布123456cnt_srs = train_df['bathrooms'].value_counts()plt.figure(figsize=(10,5))sns.barplot(cnt_srs.index,cnt_srs.values,alpha=0.8,color=color[2])plt.xlabel("number of occurrences",fontsize = 12)plt.ylabel("bathrooms",fontsize=12)plt.show() 可以看到绝大多数的样例的浴室数量为1，其次为2个浴室。 再看看不同兴趣程度的浴室数量分布，运用小提琴图来呈现： 12345678#浴室数量大于3的记为3train_df['bathrooms'].loc[train_df['bathrooms']&gt;3]=3plt.figure(figsize=(12,6))sns.violinplot(x = 'interest_level',y = 'bathrooms',data= train_df,alpha=0.8)plt.xlabel("interest level",fontsize = 12)plt.ylabel("bathrooms",fontsize=12)plt.show() 可以看到在不同的兴趣程度上，浴室数量的分布差不多。 2.3.2 卧室（bedrooms）123456cnt_bedrooms = train_df['bedrooms'].value_counts()plt.figure(figsize=(10,5))sns.barplot(cnt_bedrooms.index,cnt_bedrooms.values,alpha=0.8,color=color[3])plt.ylabel("number of occurrences",fontsize = 12)plt.xlabel("bedrooms",fontsize=12)plt.show() 卧室数量基本集中在1和2，也有不少没有卧室，3个卧室的房子也不少。 看看不同兴趣程度的卧室数量分布，同样也用小提琴图来呈现： 12345plt.figure(figsize=(12,6))sns.countplot(x='bedrooms',hue='interest_level',data=train_df)plt.ylabel("number of occurrences",fontsize = 12)plt.xlabel("bedrooms",fontsize=12)plt.show() 2.3.3 价格（price）对价格排序，看一下价格的分布： 12345plt.figure(figsize=(10,5))plt.scatter(range(train_df.shape[0]),np.sort(train_df.price.values))plt.xlabel('index',fontsize=12)plt.ylabel('price',fontsize=12)plt.show() 可以观察到有几个价格格外的高，视为异常值，我们把它们移除掉，然后再绘制分布直方图。 1234567#99%分位数ulimit = np.percentile(train_df.price.values,99)train_df['price'].loc[train_df['price']&gt;ulimit]=ulimitplt.figure(figsize=(10,5))sns.distplot(train_df.price.values,bins=50,kde=True,color=color[3])plt.xlabel('price',fontsize=12)plt.show() 可以观察到分布略微有点右偏。 2.4 地理位置型2.4.1 纬度（latitude）先看看纬度的分布情况 1234567891011#避免极端情况llimit = np.percentile(train_df.latitude.values,1)ulimit = np.percentile(train_df.latitude.values,99)train_df['latitude'].loc[train_df['latitude']&lt;llimit]=llimittrain_df['latitude'].loc[train_df['latitude']&gt;ulimit]=ulimitplt.figure(figsize=(10,5))sns.distplot(train_df.latitude.values,bins=50,kde=True,color=color[3])plt.xlabel('latitude',fontsize=12)plt.show() 由图可知，纬度基本上介于40.6到40.9之间 2.4.2 经度（longitude）1234567891011#避免极端情况llimit = np.percentile(train_df.longitude.values,1)ulimit = np.percentile(train_df.longitude.values,99)train_df['longitude'].loc[train_df['longitude']&lt;llimit]=llimittrain_df['longitude'].loc[train_df['longitude']&gt;ulimit]=ulimitplt.figure(figsize=(12,6))sns.distplot(train_df.longitude.values,bins=50)plt.xlabel('longitude',fontsize=14)plt.show() 经度介于-73.8和-74.02之间。接下来，我们尝试把经纬度对应到地图上，绘制成热图，也就是房源在地理位置上的分布密度图。 1234567891011from mpl_toolkits.basemap import Basemapfrom matplotlib import cmwest,south,east,north =-74.02,40.64,-73.85,40.86fig =plt.figure(figsize=(16,12))ax = fig.add_subplot(111)m=Basemap(projection='merc', llcrnrlat=south,urcrnrlat=north, llcrnrlon=west,urcrnrlon=east, lat_ts=south,resolution='i')x,y=m(train_df['longitude'].values,train_df['latitude'].values)m.hexbin(x,y,gridsize=200,bins='log',cmap=cm.YlOrRd_r) 基本和纽约市的城市热图相匹配。 2.5 时间型2.5.1 发布时间（Created）先看一下不同时间的分布状况。 12345678910train_df['created']=pd.to_datetime(train_df['created'])train_df['date_created']=train_df['created'].dt.datecnt_srs = train_df['date_created'].value_counts()plt.figure(figsize=(14,7))ax = plt.subplot(111)ax.bar(cnt_srs.index,cnt_srs.values,alpha=0.8)ax.xaxis_date()plt.xticks(rotation='vertical')plt.show() 从图中观察到发布时间是从2016年的4月至6月，当然这是训练集的情况，对应的，再看看测试集的发布时间状况。 12345678910test_df['created']=pd.to_datetime(test_df['created'])test_df['date_created']=test_df['created'].dt.datecnt_srs = test_df['date_created'].value_counts()plt.figure(figsize=(12,6))ax = plt.subplot(111)ax.bar(cnt_srs.index,cnt_srs.values,alpha=0.8)ax.xaxis_date()plt.xticks(rotation='vertical')plt.show() 可知，测试集的时间分布和训练集类似。 再看看不同时刻的样本分布情况： 1234567train_df['hour_created'] = train_df['created'].dt.hourcnt_srs = train_df['hour_created'].value_counts()plt.figure(figsize=(14,7))sns.barplot(cnt_srs.index,cnt_srs.values,alpha=0.8,color=color[4])plt.xticks(rotation='vertical')plt.show() 看起来像是一天中比较早的几个小时创建的比较多。可能是那个时候流量不拥挤，数据就更新了。 2.6 其他类型特征2.6.1 展示地址（Display Address）1234cnt_srs = train_df.groupby('display_address')['display_address'].count()for i in [2,10,50,100,500]: print "Display_address that appear less than &#123;&#125; \ times:&#123;&#125;%".format(i,round((cnt_srs&lt;i).mean()*100,2)) 上述代码中（cnt_srs&lt;i）返回的是布尔值True | False。再求一个得到的结果为：Display_address that appear less than 2 times:63.22%Display_address that appear less than 10 times:89.6%Display_address that appear less than 50 times:97.73%Display_address that appear less than 100 times:99.26%Display_address that appear less than 500 times:100.0% 绘制展示地址频次分布直方图： 12345plt.figure(figsize=(12,6))plt.hist(cnt_srs.values,bins=150,log=True,alpha=0.9)plt.xlabel('Number of times display_adress appeared',fontsize=12)plt.ylabel('log(Count)',fontsize=12)plt.show() 大部分的展览地址出现次数在给定的数据集中少于100次。没有超过500次的。 再看看展示地址的词云图： 12345678# wordcloud for display addressplt.figure(figsize=(12,6))wordcloud = WordCloud(background_color='white', width=600, height=300, max_font_size=50, max_words=40).generate(text_da)wordcloud.recolor(random_state=0)plt.imshow(wordcloud)plt.title("Wordcloud for Display Address", fontsize=30)plt.axis("off")plt.show() 2.6.2 照片数量（Photos）这个比赛也有巨大的照片数据。让我们先看看照片的数量: 12345678train_df["num_photos"] = train_df["photos"].apply(len)cnt_srs = train_df['num_photos'].value_counts()plt.figure(figsize=(14,7))sns.barplot(x=cnt_srs.index,y=cnt_srs.values,alpha=0.8)plt.xlabel("number of photos",fontsize=14)plt.ylabel('number of occurrences',fontsize=14)plt.show() 大多数样例的照片数量集中在3~8张。 再来看看不同兴趣程度下的照片数量分布： 123456train_df['num_photos'].loc[train_df['num_photos']&gt;12]=12plt.figure(figsize=(14,7))sns.violinplot(x='num_photos',y='interest_level',data=train_df,order=['low','medium','high'])plt.xlabel('Number of photos',fontsize=12)plt.ylabel("Interest Level",fontsize=12)plt.show() 2.6.3 描述特征的数量（features）每一个房源都对应一个features列，它描述了该样例的特征，比如位于市中心呀、能养猫呀、可以肆意遛狗，类似于这种亲民的特点。有的时候，这种利民条件越多，或许会提高消费者的兴趣，当然也不一定，可以先来看看特征数量的分布：12345678train_df['num_features'] = train_df['features'].apply(len)cnt_srs = train_df['num_features'].value_counts()plt.figure(figsize=(14,7))sns.barplot(x=cnt_srs.index,y=cnt_srs.values,alpha=0.8)plt.ylabel('Number of Occurrences',fontsize=12)plt.xlabel('Number of features',fontsize=12)plt.show() 再看看不同兴趣程度下的描述特征数量分布： 123456789#避免极端情况train_df['num_features'].loc[train_df['num_features']&gt;17]=17plt.figure(figsize=(14,7))sns.violinplot(y='num_features',x='interest_level',\ data=train_df,order=['low','medium','high'])plt.xlabel('Interest Level',fontsize=12)plt.ylabel('Number of features',fontsize=12)plt.show() 也可以看看描述特征的词云： 12345678910111213141516from wordcloud import WordCloudtext = ''text_da = ''for index,row in train_df.iterrows(): for feature in row['features']: text = ' '.join([text,"_".join(feature.strip().split(" "))]) text_da = " ".join([text_da,"_".join(row['display_address'].strip().split(" "))])text = text.strip()text_da = text_da.strip()plt.figure(figsize=(14,7))wordcloud = WordCloud(background_color='white',width=600, height=300,max_font_size=50,max_words=40).generate(text)wordcloud.recolor(random_state=0)plt.imshow(wordcloud)plt.title("Wordcloud for features",fontsize=30)plt.axis("off")plt.show() 以上这些探索性分析只是对原始数据初步的认识与了解，完了就可以建立一个base model。随着之后的特征工程对其进行更深层次的探索挖掘，不断迭代，使得我们的模型的预测效果越来越好。下一篇就开始着手建立一些base model。]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
        <tag>EDA</tag>
        <tag>可视化</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析系列（3）：数据倾斜]]></title>
    <url>%2F2017%2F06%2F11%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%2F</url>
    <content type="text"><![CDATA[数据倾斜是大数据领域绕不开的拦路虎，当你所需处理的数据量到达了上亿甚至是千亿条的时候，数据倾斜将是横在你面前一道巨大的坎。很可能有几周甚至几月都要头疼于数据倾斜导致的各类诡异的问题。 数据倾斜是指：mapreduce程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完。Hive的执行是分阶段的，map处理数据量的差异取决于上一个stage的reduce输出，所以如何将数据均匀的分配到各个reduce中，就是解决数据倾斜的根本所在。 以下是一些常见的数据倾斜情形： 一、Group by 倾斜group by造成的倾斜相对来说比较容易解决。hive提供两个参数可以解决： 1.1 hive.map.aggr一个是hive.map.aggr，默认值已经为true，他的意思是做map aggregation，也就是在mapper里面做聚合。这个方法不同于直接写mapreduce的时候可以实现的combiner，但是却实现了类似combiner的效果。事实上各种基于mr的框架如pig，cascading等等用的都是map aggregation（或者叫partial aggregation）而非combiner的策略，也就是在mapper里面直接做聚合操作而不是输出到buffer给combiner做聚合。对于map aggregation，hive还会做检查，如果aggregation的效果不好，那么hive会自动放弃map aggregation。判断效果的依据就是经过一小批数据的处理之后，检查聚合后的数据量是否减小到一定的比例，默认是0.5，由hive.map.aggr.hash.min.reduction这个参数控制。所以如果确认数据里面确实有个别取值倾斜，但是大部分值是比较稀疏的，这个时候可以把比例强制设为1，避免极端情况下map aggr失效。hive.map.aggr还有一些相关参数，比如map aggr的内存占用等，具体可以参考这篇文章。 1.2 hive.groupby.skewindata另一个参数是hive.groupby.skewindata。这个参数的意思是做reduce操作的时候，拿到的key并不是所有相同值给同一个reduce，而是随机分发，然后reduce做聚合，做完之后再做一轮MR，拿前面聚合过的数据再算结果。所以这个参数其实跟hive.map.aggr做的是类似的事情，只是拿到reduce端来做，而且要额外启动一轮job，所以其实不怎么推荐用，效果不明显。 1.3 count distinct 改写另外需要注意的是count distinct操作往往需要改写SQL，可以按照下面这么做： 12345/*改写前*/select a, count(distinct b) as c from tbl group by a;/*改写后*/select a, count(*) as c from (select a, b from tbl group by a, b) group by a; 二、Join倾斜2.1 skew joinjoin造成的倾斜，常见情况是不能做map join的两个表(能做map join的话基本上可以避免倾斜)，其中一个是行为表，另一个应该是属性表。比如我们有三个表，一个用户属性表users，一个商品属性表items，还有一个用户对商品的操作行为表日志表logs。假设现在需要将行为表关联用户表： 1select * from logs a join users b on a.user_id = b.user_id; 其中logs表里面会有一个特殊用户user_id = 0，代表未登录用户，假如这种用户占了相当的比例，那么个别reduce会收到比其他reduce多得多的数据，因为它要接收所有user_id = 0的记录进行处理，使得其处理效果会非常差，其他reduce都跑完很久了它还在运行。 hive给出的解决方案叫skew join，其原理把这种user_id = 0的特殊值先不在reduce端计算掉，而是先写入hdfs，然后启动一轮map join专门做这个特殊值的计算，期望能提高计算这部分值的处理速度。当然你要告诉hive这个join是个skew join，即：set 1hive.optimize.skewjoin = true; 还有要告诉hive如何判断特殊值，根据hive.skewjoin.key设置的数量hive可以知道，比如默认值是100000，那么超过100000条记录的值就是特殊值。总结起来，skew join的流程可以用下图描述： 2.2 特殊值分开处理法不过，上述方法还要去考虑阈值之类的情况，其实也不够通用。所以针对join倾斜的问题，一般都是通过改写sql解决。对于上面这个问题，我们已经知道user_id = 0是一个特殊key，那么可以把特殊值隔离开来单独做join，这样特殊值肯定会转化成map join，非特殊值就是没有倾斜的普通join了： 12345678select *from (select * from logs where user_id = 0) a join (select * from users where user_id = 0) b on a.user_id = b.user_idunion allselect * from logs a join users bon a.user_id &lt;&gt; 0 and a.user_id = b.user_id; 2.3 随机数分配法上面这种个别key倾斜的情况只是一种倾斜情况。最常见的倾斜是因为数据分布本身就具有长尾性质，比如我们将日志表和商品表关联： 1select * from logs a join items b on a.item_id = b.item_id; 这个时候，分配到热门商品的reducer就会很慢，因为热门商品的行为日志肯定是最多的，而且我们也很难像上面处理特殊user那样去处理item。这个时候就会用到加随机数的方法，也就是在join的时候增加一个随机数，随机数的取值范围n相当于将item给分散到n个reducer： 12345select a.*, b.*from (select *, cast(rand() * 10 as int) as r_id from logs)ajoin (select *, r_id from items lateral view explode(range_list(1,10)) rl as r_id)bon a.item_id = b.item_id and a.r_id = b.r_id 上面的写法里，对行为表的每条记录生成一个1-10的随机整数，对于item属性表，每个item生成10条记录，随机key分别也是1-10，这样就能保证行为表关联上属性表。其中range_list(1,10)代表用udf实现的一个返回1-10整数序列的方法。这个做法是一个解决join倾斜比较根本性的通用思路，就是如何用随机数将key进行分散。当然，可以根据具体的业务场景做实现上的简化或变化。 2.4 业务设计除了上面两类情况，还有一类情况是因为业务设计导致的问题，也就是说即使行为日志里面join key的数据分布本身并不明显倾斜，但是业务设计导致其倾斜。比如对于商品item_id的编码，除了本身的id序列，还人为的把item的类型也作为编码放在最后两位，这样如果类型1（电子产品）的编码是00，类型2（家居产品）的编码是01，并且类型1是主要商品类，将会造成以00为结尾的商品整体倾斜。这时，如果reduce的数量恰好是100的整数倍，会造成partitioner把00结尾的item_id都hash到同一个reducer，引爆问题。这种特殊情况可以简单的设置合适的reduce值来解决，但是这种坑对于不了解业务的情况下就会比较隐蔽。 三、典型的业务场景3.1 空值产生的数据倾斜场景：如日志中，常会有信息丢失的问题，比如日志中的 user_id，如果取其中的 user_id 和 用户表中的user_id 关联，会碰到数据倾斜的问题。 解决方法1： user_id为空的不参与关联（红色字体为修改后） 1234567select * from log a join users b on a.user_id is not null and a.user_id = b.user_idunion allselect * from log a where a.user_id is null; 解决方法2 ：赋与空值分新的key值 1234select * from log a left outer join users b on case when a.user_id is null then concat(‘hive’,rand() ) else a.user_id end = b.user_id; 结论：方法2比方法1效率更好，不但io少了，而且作业数也少了。解决方法1中 log读取两次，jobs是2。解决方法2 job数是1 。这个优化适合无效 id (比如 -99 , ’’, null 等) 产生的倾斜问题。把空值的 key 变成一个字符串加上随机数，就能把倾斜的数据分到不同的reduce上 ,解决数据倾斜问题。 3.2 不同数据类型关联产生数据倾斜场景：用户表中user_id字段为int，log表中user_id字段既有string类型也有int类型。当按照user_id进行两个表的Join操作时，默认的Hash操作会按int型的id来进行分配，这样会导致所有string类型id的记录都分配到一个Reducer中。 解决方法：把数字类型转换成字符串类型 123select * from users a left outer join logs b on a.usr_id = cast(b.user_id as string) 3.3 小表不小不大，怎么用 map join 解决倾斜问题使用 map join 解决小表(记录数少)关联大表的数据倾斜问题，这个方法使用的频率非常高，但如果小表很大，大到map join会出现bug或异常，这时就需要特别的处理。 以下例子: 123select * from log a left outer join users b on a.user_id = b.user_id; users 表有 600w+ 的记录，把 users 分发到所有的 map 上也是个不小的开销，而且 map join 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。 12345678select /*+mapjoin(x)*/* from log a left outer join ( select /*+mapjoin(c)*/d.* from ( select distinct user_id from log ) c join users d on c.user_id = d.user_id ) x on a.user_id = b.user_id; 假如，log里user_id有上百万个，这就又回到原来map join问题。所幸，每日的会员uv不会太多，有交易的会员不会太多，有点击的会员不会太多，有佣金的会员不会太多等等。所以这个方法能解决很多场景下的数据倾斜问题。 四、总结使map的输出数据更均匀的分布到reduce中去，是我们的最终目标。由于Hash算法的局限性，按key Hash会或多或少的造成数据倾斜。大量经验表明数据倾斜的原因是人为的建表疏忽或业务逻辑可以规避的。在此给出较为通用的步骤： 1）采样log表，哪些user_id比较倾斜，得到一个结果表tmp1。由于对计算框架来说，所有的数据过来，他都是不知道数据分布情况的，所以采样是并不可少的。 2）数据的分布符合社会学统计规则，贫富不均。倾斜的key不会太多，就像一个社会的富人不多，奇特的人不多一样。所以tmp1记录数会很少。把tmp1和users做map join生成tmp2,把tmp2读到distribute file cache。这是一个map过程。 3）map读入users和log，假如记录来自log,则检查user_id是否在tmp2里，如果是，输出到本地文件a,否则生成的key,value对，假如记录来自member,生成的key,value对，进入reduce阶段。 4）最终把a文件，把Stage3 reduce阶段输出的文件合并起写到hdfs。 如果确认业务需要这样倾斜的逻辑，考虑以下的优化方案： 1）对于join，在判断小表不大于1G的情况下，使用map join 2）对于group by或distinct，设定 hive.groupby.skewindata=true 3）尽量使用上述的SQL语句调节进行优化]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据倾斜</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析系列（2）：卡方检验]]></title>
    <url>%2F2017%2F06%2F10%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9A%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[$\chi^2$检验（chi-square test）或称卡方检验，是一种用途较广的假设检验方法，，主要是比较两个及两个以上样本率( 构成比）以及两个分类变量的关联性分析。其根本思想就是在于比较理论频数和实际频数的吻合程度或拟合优度问题。 它的发明者卡尔·皮尔逊是一位历史上罕见的百科全书式的学者，研究领域涵盖了生物、历史、宗教、哲学、法律。在文本分类中可以用卡方值做特征选择（降维），也可以用卡方检验做异常用户的检测。 一、四格表资料的卡方检验两组大白鼠在不同致癌剂作用下的发癌率如下表，问两组发癌率有无差别？ 这四格资料表就专称四格表（fourfold table），或称2行2列表（2×2 contingency table）。从该资料算出的两组发癌率分别为73.24%和92.86%，两者的差别可能是抽样误差所致，亦可能是两组发癌率（总体率）确有所不同。这里可通过卡方检验来区别其差异有无统计学意义，检验的基本公式为： \chi^2=\sum \frac{(A-T)^2}{T}式中A为实际数，以上四格表的四个数据就是实际数。T为理论数，是根据检验假设推断出来的；即假设这两组的发癌率本无不同，差别仅是由抽样误差所致。这里可将两组合计发癌率作为理论上的发癌率，即91/113=80.3%，以此为依据便可推算出四格表中相应的四格的理论数。以表1资料为例检验如下。 检验步骤： 1）建立检验假设：$H_0:\ n_1=n_2 \ H_1 : n_1 ≠n_2;$ 2）计算理论数（TRC）,计算公式为：TRC = \frac{n_r\times n_c}{n}式中$TRC$是表示第R行C列格子的理论数，$n_r$是与理论数同行的合计数，$n_c$是与理论数同列的合计数，$n$为总例数。 第1行1列： 71×91/113=57.18第1行2列： 71×22/113=13.82第2行1列： 42×91/113=33.82第2行2列： 42×22/113=8.18 以推算结果，可与原四项实际数并列成下表：因为上表每行和每列合计数都是固定的，所以只要用TRC式求得其中一项理论数（例如T1.1=57.18），则其余三项理论数都可用同行或同列合计数相减，直接求出。 3）计算卡方值按公式代入\chi^2 = \frac{(52-57.18)^2}{57.18}···+ \frac{(3-8.18)^2}{8.18}=6.48 4）查卡方值表求$P$值 在查表之前应知本题自由度。按卡方检验的自由度v=（行数-1）（列数-1），则该题的自由度v=（2-1）（2-1）=1，查卡方界值表，找到$\chi^2_{0.05}(1)=3.85$，而本题卡方=6.48即卡方＞$\chi^2_{0.05}(1)$，P＜0.05，差异有显著统计学意义，按α=0.05水准，拒绝H0，可以认为两组发癌率有差别。 通过实例计算，读者对卡方的基本公式有如下理解：若各理论数与相应实际数相差越小，卡方值越小；如两者相同，则卡方值必为零，而卡方永远为正值。又因为每一对理论数和实际数都加入卡方值中，分组越多，即格子数越多，卡方值也会越大，因而每考虑卡方值大小的意义时同时要考虑到格子数。因此自由度大时，卡方的界值也相应增大。 二、四格表卡方值的校正卡方值表是数理统计根据正态分布中$\chi^2 = \sum (\frac{x_i-\mu }{\sigma})^2$的定义计算出来的。是一种近似。在自由度大于1、理论数皆大于5时，这种近似很好；当自由度为1时，尤其当1＜T＜5，而n＞40时，应用以下校正公式： \chi^2 = \frac{\sum{(|A-T|-0.5)^2}}{T}例2.某医师用甲、乙两疗法治疗小儿单纯性消化不良，结果小表试比较两种疗法效果有无差异？ 从表可见，T1.2和T2.2数值都＜5，且总例数大于40，故宜用校正公式检验。步骤如下： 1）检验假设：$H_0：π1=π2；H_1：π1≠π2；α=0.05$ 2）计算理论数：（已完成列入四格表括弧中） 3）计算卡方值：应用校正公式运算如下： \chi^2 = \frac{\sum{(|A-T|-0.5)^2}}{T}=2.746查卡方界值表$\chi^2_{0.05}(1)=3.84$，，故卡方＜$\chi^2_{0.05}(1)$，P＞0.05。按α=0.05水准，接受H0，两种疗效差异无统计学意义。 如果不采用校正公式，而用原基本公式，算得的结果卡方=4.068，则结论就不同了。 如果观察资料的T＜1或n＜40时，四格表资料用上述校正法也不行，可参考预防医学专业用的医学统计学教材中的Fisher精确检验法直接计算概率以作判断。 三、行列表的卡方检验适用于两个组以上的率或百分比差别的显著性检验。其检验步骤与上述相同，简单计算公式如下： \chi^2 = n(\sum \frac{A^2}{n_rn_c}-1 )式中n为总例数；A为各观察值；$n_r$和$n_c$为与各A值相应的行和列合计的总数。 例3.北方冬季日照短而南移，居宅设计如何适应以获得最大日照量，增强居民体质，减少小儿佝偻病，实属重要。胡氏等1986年在北京进行住宅建筑日照卫生标准的研究，对214幢楼房居民的婴幼儿712人体检，检出轻度佝偻病333例，比较了居室朝向与患病的关系。现将该资料归纳如表4作行列检验。 该表资料由2行4列组成，称2×4表，可用行×列卡方公式检验。 1）检验假设：H0：四类朝向居民婴幼儿佝偻病患病率相同；H1：四类朝向居民婴幼儿佝偻病患率不同；α=0.05 2）计算卡方值：\chi^2 = 712(\frac{180^2}{379\times 380 }+···+\frac{33^2}{333\times 98}-1)=15.079 3）确定P值和分析：本题v=（2-1)（4-3）=3，据此查卡方界值表：$\chi^2_{0.05}(3)=7.81$，本题卡方=15.08，卡方＞ $\chi^2_{0.05}(3)$，P＜0.05，拒绝$H_0$，可以认为居室朝向不同的居民，婴幼儿佝偻病患病率有差异。 一般认为行列表中不宜有1/5以上格子的理论数小于5，或有小于1的理论数。当理论数太小可采取下列方法处理：①增加样本含量以增大理论数；②删去上述理论数太小的行和列；③将太小理论数所在行或列与性质相近的邻行邻列中的实际数合并，使重新计算的理论数增大。由于后两法可能会损失信息，损害样本的随机性，不同的合并方式有可能影响推断结论，故不宜作常规方法。另外，不能把不同性质的实际数合并，如研究血型时，不能把不同的血型资料合并。 如检验结果拒绝检验假设，只能认为各总体率或总体构成比之间总的来说有差别，但不能说明它们彼此之间都有差别，或某两者间有差别。 四、应用场景卡方检验的一个典型应用场景是衡量特定条件下的分布是否与理论分布一致，比如：特定用户某项指标的分布与大盘的分布是否差异很大，这时通过临界概率可以合理又科学的筛选异常用户。 另外，x2值描述了自变量与因变量之间的相关程度：x2值越大，相关程度也越大，所以很自然的可以利用x2值来做降维，保留相关程度大的变量。再回到刚才新闻分类的场景，如果我们希望获取和娱乐类别相关性最强的100个词，以后就按照标题是否包含这100个词来确定新闻是否归属于娱乐类，怎么做？很简单，对娱乐类新闻标题所包含的每个词按上述步骤计算x2值，然后按x2值排序，取x2值最大的100个词。]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>卡方检验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析系列（1）：SQL查询执行顺序]]></title>
    <url>%2F2017%2F06%2F09%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9ASQL%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[SQL语句有一个让大部分人都感到困惑的特性，就是：SQL语句的执行顺序跟其语句的语法顺序并不一致。SQL语句的执行顺序是： FROM ON JOIN WHERE GROUP BY HAVING SELECT DISTINCT UNION ORDER BY LIMIT 一、准备工作这里的测试操作都是在MySQL数据库上完成的。 1.1 新建数据库首先我们新建一个测试数据库TestDB 1create database TestDB; 1.2 创建测试集table1和table212345678910111213CREATE TABLE table1 ( customer_id VARCHAR(10) NOT NULL, city VARCHAR(10) NOT NULL, PRIMARY KEY(customer_id) )ENGINE=INNODB DEFAULT CHARSET=UTF8; CREATE TABLE table2 ( order_id INT NOT NULL auto_increment, customer_id VARCHAR(10), PRIMARY KEY(order_id) )ENGINE=INNODB DEFAULT CHARSET=UTF8; 1.3 插入测试数据123456789101112INSERT INTO table1(customer_id,city) VALUES(&apos;163&apos;,&apos;hangzhou&apos;); INSERT INTO table1(customer_id,city) VALUES(&apos;9you&apos;,&apos;shanghai&apos;); INSERT INTO table1(customer_id,city) VALUES(&apos;tx&apos;,&apos;hangzhou&apos;); INSERT INTO table1(customer_id,city) VALUES(&apos;baidu&apos;,&apos;hangzhou&apos;); INSERT INTO table2(customer_id) VALUES(&apos;163&apos;); INSERT INTO table2(customer_id) VALUES(&apos;163&apos;); INSERT INTO table2(customer_id) VALUES(&apos;9you&apos;); INSERT INTO table2(customer_id) VALUES(&apos;9you&apos;); INSERT INTO table2(customer_id) VALUES(&apos;9you&apos;); INSERT INTO table2(customer_id) VALUES(&apos;tx&apos;); INSERT INTO table2(customer_id) VALUES(NULL); 准备工作做完以后，table1和table2看起来应该像下面这样： 123456789101112131415161718192021222324mysql&gt; select * from table1;+-------------+----------+| customer_id | city |+-------------+----------+| 163 | hangzhou || 9you | shanghai || baidu | hangzhou || tx | hangzhou |+-------------+----------+4 rows in set (0.00 sec)mysql&gt; select * from table2;+----------+-------------+| order_id | customer_id |+----------+-------------+| 1 | 163 || 2 | 163 || 3 | 9you || 4 | 9you || 5 | 9you || 6 | tx || 7 | NULL |+----------+-------------+7 rows in set (0.00 sec) 1.4 准备SQL查询语句12345678SELECT a.customer_id, COUNT(b.order_id) as total_orders FROM table1 AS a LEFT JOIN table2 AS b ON a.customer_id = b.customer_id WHERE a.city = &apos;hangzhou&apos; GROUP BY a.customer_id HAVING count(b.order_id) &lt; 2 ORDER BY total_orders DESC; 这些测试表和测试数据均来自《MySQL技术内幕：SQL编程》，接下来根据这个语句来详细地讲述SQL逻辑查询语句的执行顺序。 二、SQL查询语句执行顺序现在，我们先给出一个查询语句的执行顺序： 12345678910(7) SELECT (8) DISTINCT &lt;select_list&gt;(1) FROM &lt;left_table&gt;(3) &lt;join_type&gt; JOIN &lt;right_table&gt;(2) ON &lt;join_condition&gt;(4) WHERE &lt;where_condition&gt;(5) GROUP BY &lt;group_by_list&gt;(6) HAVING &lt;having_condition&gt;(9) ORDER BY &lt;order_by_condition&gt;(10) LIMIT &lt;limit_number&gt; 在这些SQL语句的执行过程中，都会产生一个虚拟表，用来保存SQL语句的执行结果（这是重点），现在就追踪这个虚拟表的变化，得到最终的查询结果的过程，来分析整个SQL逻辑查询的执行顺序和过程。 2.1 执行FROM语句第一步，执行FROM语句。我们首先需要知道最开始从哪个表开始的，这就是FROM告诉我们的。现在有了left table和right table两个表，我们到底从哪个表开始，还是会从两个表进行某种联系以后再开始呢？它们之间如何产生联系呢？——笛卡尔积，笛卡尔积是所有可能的有序对组成的集合，其中有序对的第一个对象是X的成员，第二个对象是Y的成员。 经过FROM语句对两个表执行笛卡尔积，会得到一个虚拟表，暂且叫VT1(vitual table 1)，内容如下： 1234567891011121314151617181920212223242526272829303132+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 9you | shanghai | 1 | 163 || baidu | hangzhou | 1 | 163 || tx | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 || 9you | shanghai | 2 | 163 || baidu | hangzhou | 2 | 163 || tx | hangzhou | 2 | 163 || 163 | hangzhou | 3 | 9you || 9you | shanghai | 3 | 9you || baidu | hangzhou | 3 | 9you || tx | hangzhou | 3 | 9you || 163 | hangzhou | 4 | 9you || 9you | shanghai | 4 | 9you || baidu | hangzhou | 4 | 9you || tx | hangzhou | 4 | 9you || 163 | hangzhou | 5 | 9you || 9you | shanghai | 5 | 9you || baidu | hangzhou | 5 | 9you || tx | hangzhou | 5 | 9you || 163 | hangzhou | 6 | tx || 9you | shanghai | 6 | tx || baidu | hangzhou | 6 | tx || tx | hangzhou | 6 | tx || 163 | hangzhou | 7 | NULL || 9you | shanghai | 7 | NULL || baidu | hangzhou | 7 | NULL || tx | hangzhou | 7 | NULL |+-------------+----------+----------+-------------+ 总共有28（table1的记录数*table2的记录总数）条记录。这就是VT1的结果，接下来的操作就在VT!的基础上进行。 2.2 执行ON过滤执行完笛卡尔积以后，接着就进行ON a.customer_id = b.customer_id条件过滤，根据ON中指定的条件，去掉那些不符合条件的数据，得到VT2表，内容如下： 12345678910+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 || 9you | shanghai | 3 | 9you || 9you | shanghai | 4 | 9you || 9you | shanghai | 5 | 9you || tx | hangzhou | 6 | tx |+-------------+----------+----------+-------------+ VT2就是经过ON条件筛选以后得到的有用数据，而接下来的操作将在VT2的基础上继续进行。 2.3 添加外部行这一步只有在连接类型为OUTER JOIN时才发生，如LEFT OUTER JOIN、RIGHT OUTER JOIN和FULL OUTER JOIN。在大多数的时候，我们都是会省略掉OUTER关键字的，但OUTER表示的就是外部行的概念。 LEFT OUTER JOIN把左表记为保留表，得到的结果为： 1234567891011+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 || 9you | shanghai | 3 | 9you || 9you | shanghai | 4 | 9you || 9you | shanghai | 5 | 9you || tx | hangzhou | 6 | tx || baidu | hangzhou | NULL | NULL |+-------------+----------+----------+-------------+ RIGHT OUTER JOIN把右表记为保留表，得到的结果为： 1234567891011+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 || 9you | shanghai | 3 | 9you || 9you | shanghai | 4 | 9you || 9you | shanghai | 5 | 9you || tx | hangzhou | 6 | tx || NULL | NULL | 7 | NULL |+-------------+----------+----------+-------------+ FULL OUTER JOIN把左右表都作为保留表，得到的结果为： 123456789101112+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 || 9you | shanghai | 3 | 9you || 9you | shanghai | 4 | 9you || 9you | shanghai | 5 | 9you || tx | hangzhou | 6 | tx || baidu | hangzhou | NULL | NULL || NULL | NULL | 7 | NULL |+-------------+----------+----------+-------------+ 添加外部行的工作就是在VT2表的基础上添加保留表中被过滤条件过滤掉的数据，非保留表中的数据被赋予了NULL值，最后生成虚拟表VT3。 由于在准备的测试SQL查询逻辑语句中使用的是LEFT JOIN，过滤掉了以下这条数据： 1| baidu | hangzhou | 现在就把这条数据添加到VT2表中，得到的VT3表如下： 1234567891011+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 || 9you | shanghai | 3 | 9you || 9you | shanghai | 4 | 9you || 9you | shanghai | 5 | 9you || tx | hangzhou | 6 | tx || baidu | hangzhou | NULL | NULL |+-------------+----------+----------+-------------+ 接下里的操作都会在该VT3表上进行。 2.4 执行WHERE过滤对添加外部行得到的VT3进行WHERE过滤，只有符合的记录才会输出到虚拟表VT4中。当我们执行WHERE a.city = &#39;hangzhou&#39;的时候，就会得到以下内容，并存在虚拟表VT4中： 12345678+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 || tx | hangzhou | 6 | tx || baidu | hangzhou | NULL | NULL |+-------------+----------+----------+-------------+ 但是在使用WHERE字句时，需要注意以下两点： 由于数据还没有分组，因此还不能在WHERE过滤器中使用where_condition =MIN(col)这类分组统计的过滤； 由于还没有进行列的选取操作，因此在WHERE中使用列的别名也是不被允许的，如：SELECT city AS c FROM t WHERE c=&#39;shanghai&#39;;是不允许出现的。 2.5 执行GROUP BY分组GROUP BY子句主要是对使用WHERE子句得到的虚拟表进行分组操作。即“根据(by)一定的规则进行分组(Group)”。它的作用是通过一定的规则将一个数据集划分成若干个小的区域，然后针对若干个小区域进行数据处理。我们执行测试语句中的GROUP BY a.customer_id，就是对VT4按照a.customer_id进行了分组，这里就得到了以下三个组别 1234567891011第一组+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| 163 | hangzhou | 1 | 163 || 163 | hangzhou | 2 | 163 |第二组| tx | hangzhou | 6 | tx |第三组| baidu | hangzhou | NULL | NULL |+-------------+----------+----------+-------------+ 得到的内容会存入虚拟表VT5中，此时，我们就得到了一个VT5虚拟表，接下来的操作都会在该表上完成。 2.6 执行HAVING过滤HAVING字句主要和GROUP BY字句配合使用，对分组得到的VT5虚拟表进行条件过滤。当我执行测试语句中的HAVING COUNT(b.order_id)&lt;2时，将得到以下内容： 123456+-------------+----------+----------+-------------+| customer_id | city | order_id | customer_id |+-------------+----------+----------+-------------+| baidu | hangzhou | NULL | NULL || tx | hangzhou | 6 | tx |+-------------+----------+----------+-------------+ 这就是虚拟表6 2.7 SELECT列表现在才会执行到SELECT子句，不要以为SELECT子句被写在第一行，就是第一个被执行的。 执行测试语句中的SELECT a.customer_id ,COUNT(b.oredr_id) as total_orders，我们从虚拟表VT6中选择我们需要的内容。我们将得到以下内容： 123456+-------------+--------------+| customer_id | total_orders |+-------------+--------------+| baidu | 0 || tx | 1 |+-------------+--------------+ 2.8 执行DISTINCT子句如果在查询中指定了DISTINCT子句，则会创建一张内存临时表（如果内存放不下，就需要存放在硬盘了）。这张临时表的表结构和上一步产生的虚拟表VT7是一样的，不同的是对进行DISTINCT操作的列增加了一个唯一索引，以此来去除重复数据。 由于测试SQL语句中并没有使用DISTINCT，所以，在该查询中，这一步不会生成一个虚拟表。 2.9 执行ORDER BY子句对虚拟表中的内容按照指定的列进行排序，然后返回一个新的虚拟表，我们执行测试SQL语句中的ORDER BY total_orders DESC，就会得到以下内容：123456+-------------+--------------+| customer_id | total_orders |+-------------+--------------+| tx | 1 || baidu | 0 |+-------------+--------------+ 可以看到这是对total_orders列进行降序排列。上述结果会存储在VT8中。 2.10执行LIMIT子句LIMIT子句从上一步得到的VT8虚拟表中选出从指定位置开始的指定行数据。对于没有营养ORDER BY的LIMIT子句，得到的结果同样是无序的，所以，很多时候，我们都会看到LIMIT子句会和ORDER BY子句一起使用。 MYSQL数据库的LIMIT支持如下形式的选择：1LIMIT n, m 表示从第n条记录开始选择m条记录。而很多开发人员喜欢使用该语句来解决分页问题。对于小数据，使用LIMIT子句没有任何问题，当数据量非常大的时候，使用LIMIT n, m是非常低效的。因为LIMIT的机制是每次都是从头开始扫描，如果需要从第60万行开始，读取3条数据，就需要先扫描定位到60万行，然后再进行读取，而扫描的过程是一个非常低效的过程。所以，对于大数据处理时，是非常有必要在应用层建立一定的缓存机制]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kaggle系列（1）：Kaggle 数据挖掘比赛经验分享]]></title>
    <url>%2F2017%2F06%2F05%2FKaggle%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9AKaggle%20%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[转载自知乎：Kaggle 数据挖掘比赛经验分享 作者是陈成龙，目前在腾讯社交与效果广告部任职数据挖掘工程师，负责 Lookalike 相似人群扩展相关工作。曾在 Kaggle 数据科学家排行榜排名全球第十，国内第一。 简介Kaggle 于 2010 年创立，专注于开展数据科学、机器学习相关的竞赛，是全球最大的数据科学社区和数据竞赛平台。笔者从 2013 年开始，陆续参加了多场 Kaggle上面举办的比赛，相继获得了 CrowdFlower 搜索相关性比赛第一名（1326支队伍）和 HomeDepot 商品搜索相关性比赛第三名（2125支队伍），曾在 Kaggle 数据科学家排行榜排名全球第十，国内第一。笔者目前在腾讯社交与效果广告部任职数据挖掘工程师，负责 Lookalike 相似人群扩展相关工作。此文分享笔者在参加数据挖掘比赛过程中的一点心得体会。 一、Kaggle基本介绍Kaggle 于 2010 年创立，专注于开展数据科学、机器学习相关的竞赛，是全球最大的数据科学社区和数据竞赛平台。在 Kaggle 上，企业或者研究机构发布商业和科研难题，悬赏吸引全球的数据科学家，通过众包的方式解决建模问题。而参赛者可以接触到丰富的真实数据，解决实际问题，角逐名次，赢取奖金。诸如 Google，Facebook，Microsoft 等知名科技公司均在 Kaggle 上面举办过数据挖掘比赛。2017年3月，Kaggle 被 Google CloudNext 收购。 1.1 参赛方式可以以个人或者组队的形式参加比赛。组队人数一般没有限制，但需要在 Merger Deadline 前完成组队。为了能参与到比赛中，需要在 Entry Deadline 前进行至少一次有效提交。最简单地，可以直接提交官方提供的 Sample Submission。关于组队，建议先单独个人进行数据探索和模型构建，以个人身份进行比赛，在比赛后期（譬如离比赛结束还有 2~3 周）再进行组队，以充分发挥组队的效果（类似于模型集成，模型差异性越大，越有可能有助于效果的提升，超越单模型的效果）。当然也可以一开始就组好队，方便分工协作，讨论问题和碰撞火花。 Kaggle 对比赛的公正性相当重视。在比赛中，每个人只允许使用一个账号进行提交。在比赛结束后 1~2 周内，Kaggle 会对使用多账号提交的 Cheater 进行剔除（一般会对 Top 100 的队伍进行 Cheater Detection）。在被剔除者的 Kaggle 个人页面上，该比赛的成绩也会被删除，相当于该选手从没参加过这个比赛。此外，队伍之间也不能私自分享代码或者数据，除非在论坛上面公开发布。 比赛一般只提交测试集的预测结果，无需提交代码。每人（或每个队伍）每天有提交次数的限制，一般为2次或者5次，在 Submission 页面会有提示。 1.2 比赛获奖Kaggle 比赛奖金丰厚，一般前三名均可以获得奖金。在最近落幕的第二届 National Data Science Bowl 中，总奖金池高达 100W 美刀，其中第一名可以获得 50W 美刀的奖励，即使是第十名也能收获 2.5W 美刀的奖金。获奖的队伍需要在比赛结束后 1~2 周内，准备好可执行的代码以及 README，算法说明文档等提交给 Kaggle 来进行获奖资格的审核。Kaggle 会邀请获奖队伍在 Kaggle Blog 中发表 Interview，来分享比赛故事和经验心得。对于某些比赛，Kaggle 或者主办方会邀请获奖队伍进行电话/视频会议，获奖队伍进行 Presentation，并与主办方团队进行交流。 1.3 比赛类型从 Kaggle 提供的官方分类来看，可以划分为以下类型（如下图1所示）： Featured：商业或科研难题，奖金一般较为丰厚； Recruitment：比赛的奖励为面试机会； Research：科研和学术性较强的比赛，也会有一定的奖金，一般需要较强的领域和专业知识； Playground：提供一些公开的数据集用于尝试模型和算法； Getting Started：提供一些简单的任务用于熟悉平台和比赛； In Class：用于课堂项目作业或者考试。 从领域归属划分：包含搜索相关性，广告点击率预估，销量预估，贷款违约判定，癌症检测等。 从任务目标划分：包含回归，分类（二分类，多分类，多标签），排序，混合体（分类+回归）等。 从数据载体划分：包含文本，语音，图像和时序序列等。 从特征形式划分：包含原始数据，明文特征，脱敏特征（特征的含义不清楚）等。 1.4 比赛流程一个数据挖掘比赛的基本流程如下图2所示，具体的模块我将在下一章进行展开陈述。 这里想特别强调的一点是，Kaggle 在计算得分的时候，有Public Leaderboard (LB)和 Private LB 之分。具体而言，参赛选手提交整个测试集的预测结果，Kaggle 使用测试集的一部分计算得分和排名，实时显示在 Public LB上，用于给选手提供及时的反馈和动态展示比赛的进行情况；测试集的剩余部分用于计算参赛选手的最终得分和排名，此即为 Private LB，在比赛结束后会揭晓。用于计算 Public LB 和 Private LB 的数据有不同的划分方式，具体视比赛和数据的类型而定，一般有随机划分，按时间划分或者按一定规则划分。 这个过程可以概括如下图3所示，其目的是避免模型过拟合，以得到泛化能力好的模型。如果不设置 Private LB（即所有的测试数据都用于计算 Public LB），选手不断地从 Public LB（即测试集）中获得反馈，进而调整或筛选模型。这种情况下，测试集实际上是作为验证集参与到模型的构建和调优中来。Public LB上面的效果并非是在真实未知数据上面的效果，不能可靠地反映模型的效果。划分 Public LB 和 Private LB 这样的设置，也在提醒参赛者，我们建模的目标是要获得一个在未知数据上表现良好的模型，而并非仅仅是在已知数据上效果好。 二、数据挖掘比赛流程从上面图2可以看到，做一个数据挖掘比赛，主要包含了数据分析，数据清洗，特征工程，模型训练和验证等四个大的模块，以下来一一对其进行介绍。 2.1 数据分析数据分析可能涉及以下方面： 分析特征变量的分布 特征变量为连续值：如果为长尾分布并且考虑使用线性模型，可以对变量进行幂变换或者对数变换。 特征变量为离散值：观察每个离散值的频率分布，对于频次较低的特征，可以考虑统一编码为“其他”类别。 分析目标变量的分布 目标变量为连续值：查看其值域范围是否较大，如果较大，可以考虑对其进行对数变换，并以变换后的值作为新的目标变量进行建模（在这种情况下，需要对预测结果进行逆变换）。一般情况下，可以对连续变量进行Box-Cox变换。通过变换可以使得模型更好的优化，通常也会带来效果上的提升。 目标变量为离散值：如果数据分布不平衡，考虑是否需要上采样/下采样；如果目标变量在某个ID上面分布不平衡，在划分本地训练集和验证集的时候，需要考虑分层采样（Stratified Sampling）。 分析变量之间两两的分布和相关度 可以用于发现高相关和共线性的特征。 通过对数据进行探索性分析（甚至有些情况下需要肉眼观察样本），还可以有助于启发数据清洗和特征抽取，譬如缺失值和异常值的处理，文本数据是否需要进行拼写纠正等。 2.2 数据清洗数据清洗是指对提供的原始数据进行一定的加工，使得其方便后续的特征抽取。其与特征抽取的界限有时也没有那么明确。常用的数据清洗一般包括： 数据的拼接 提供的数据散落在多个文件，需要根据相应的键值进行数据的拼接。 特征缺失值的处理 特征值为连续值：按不同的分布类型对缺失值进行补全：偏正态分布，使用均值代替，可以保持数据的均值；偏长尾分布，使用中值代替，避免受 outlier 的影响； 特征值为离散值：使用众数代替 文本数据的清洗 在比赛当中，如果数据包含文本，往往需要进行大量的数据清洗工作。如去除HTML 标签，分词，拼写纠正, 同义词替换，去除停词，抽词干，数字和单位格式统一等。 2.3 特征工程有一种说法是，特征决定了效果的上限，而不同模型只是以不同的方式或不同的程度来逼近这个上限。这样来看，好的特征输入对于模型的效果至关重要，正所谓”Garbage in, garbage out”。要做好特征工程，往往跟领域知识和对问题的理解程度有很大的关系，也跟一个人的经验相关。特征工程的做法也是Case by Case，以下就一些点，谈谈自己的一些看法。 2.3.1 特征变换主要针对一些长尾分布的特征，需要进行幂变换或者对数变换，使得模型（LR或者DNN）能更好的优化。需要注意的是，Random Forest 和 GBDT 等模型对单调的函数变换不敏感。其原因在于树模型在求解分裂点的时候，只考虑排序分位点。 2.3.2 特征编码对于离散的类别特征，往往需要进行必要的特征转换/编码才能将其作为特征输入到模型中。常用的编码方式有 LabelEncoder，OneHotEncoder（sklearn里面的接口）。譬如对于”性别”这个特征（取值为男性和女性），使用这两种方式可以分别编码为$\{0,1\}$和$\{[1,0], [0,1]\}$。 对于取值较多（如几十万）的类别特征（ID特征），直接进行OneHotEncoder编码会导致特征矩阵非常巨大，影响模型效果。可以使用如下的方式进行处理： 统计每个取值在样本中出现的频率，取 Top N 的取值进行 One-hot 编码，剩下的类别分到“其他“类目下，其中 N 需要根据模型效果进行调优； 统计每个 ID 特征的一些统计量（譬如历史平均点击率，历史平均浏览率）等代替该 ID 取值作为特征，具体可以参考 Avazu 点击率预估比赛第二名的获奖方案； 参考 word2vec 的方式，将每个类别特征的取值映射到一个连续的向量，对这个向量进行初始化，跟模型一起训练。训练结束后，可以同时得到每个ID的Embedding。具体的使用方式，可以参考 Rossmann 销量预估竞赛第三名的获奖方案(entron/entity-embedding-rossmann) 对于 Random Forest 和 GBDT 等模型，如果类别特征存在较多的取值，可以直接使用 LabelEncoder 后的结果作为特征。 2.4 模型训练与验证2.4.1 模型选择在处理好特征后，我们可以进行模型的训练和验证。 对于稀疏型特征（如文本特征，One-hot的ID类特征），我们一般使用线性模型，譬如 Linear Regression 或者 Logistic Regression。Random Forest 和 GBDT 等树模型不太适用于稀疏的特征，但可以先对特征进行降维（如PCA，SVD/LSA等），再使用这些特征。稀疏特征直接输入 DNN 会导致网络 weight 较多，不利于优化，也可以考虑先降维，或者对 ID 类特征使用 Embedding 的方式； 对于稠密型特征，推荐使用 XGBoost 进行建模，简单易用效果好； 数据中既有稀疏特征，又有稠密特征，可以考虑使用线性模型对稀疏特征进行建模，将其输出与稠密特征一起再输入 XGBoost/DNN 建模，具体可以参考2.5.2节 Stacking 部分。 2.4.2 调参和模型验证对于选定的特征和模型，我们往往还需要对模型进行超参数的调优，才能获得比较理想的效果。调参一般可以概括为以下三个步骤： 训练集和验证集的划分。根据比赛提供的训练集和测试集，模拟其划分方式对训练集进行划分为本地训练集和本地验证集。划分的方式视具体比赛和数据而定，常用的方式有： 随机划分：譬如随机采样 70% 作为训练集，剩余的 30% 作为测试集。在这种情况下，本地可以采用 KFold 或者 Stratified KFold 的方法来构造训练集和验证集。 按时间划分：一般对应于时序序列数据，譬如取前 7 天数据作为训练集，后 1 天数据作为测试集。这种情况下，划分本地训练集和验证集也需要按时间先后划分。常见的错误方式是随机划分，这种划分方式可能会导致模型效果被高估。 按某些规则划分：在 HomeDepot 搜索相关性比赛中，训练集和测试集中的 Query 集合并非完全重合，两者只有部分交集。而在另外一个相似的比赛中（CrowdFlower 搜索相关性比赛），训练集和测试集具有完全一致的 Query 集合。对于 HomeDepot 这个比赛中，训练集和验证集数据的划分，需要考虑 Query 集合并非完全重合这个情况，其中的一种方法可以参考第三名的获奖方案。 指定参数空间。在指定参数空间的时候，需要对模型参数以及其如何影响模型的效果有一定的了解，才能指定出合理的参数空间。譬如DNN或者XGBoost中学习率这个参数，一般就选 0.01 左右就 OK 了（太大可能会导致优化算法错过最优化点，太小导致优化收敛过慢）。再如 Random Forest，一般设定树的棵数范围为 100~200 就能有不错的效果，当然也有人固定数棵数为 500，然后只调整其他的超参数。 按照一定的方法进行参数搜索。常用的参数搜索方法有，Grid Search，Random Search以及一些自动化的方法（如 Hyperopt）。其中，Hyperopt 的方法，根据历史已经评估过的参数组合的效果，来推测本次评估使用哪个参数组合更有可能获得更好的效果。有关这些方法的介绍和对比，可以参考文献 [2]。 2.4.3 适当利用Public LB的反馈在2.4.2节中我们提到本地验证（Local Validation）结果，当将预测结果提交到 Kaggle 上时，我们还会接收到 Public LB 的反馈结果。如果这两个结果的变化趋势是一致的，如 Local Validation 有提升，Public LB 也有提升，我们可以借助 Local Validation 的变化来感知模型的演进情况，而无需靠大量的 Submission。如果两者的变化趋势不一致，需要考虑2.4.2节中提及的本地训练集和验证集的划分方式，是否跟训练集和测试集的划分方式一致。 另外，在以下一些情况下，往往 Public LB 反馈亦会提供有用信息，适当地使用这些反馈也许会给你带来优势。如图4所示，(a)和(b)表示数据与时间没有明显的关系（如图像分类），(c)和(d)表示数据随时间变化（如销量预估中的时序序列）。(a)和(b)的区别在于，训练集样本数相对于 Public LB 的量级大小，其中(a)中训练集样本数远超于 Public LB 的样本数，这种情况下基于训练集的 Local Validation 更可靠；而(b)中，训练集数目与 Public LB 相当，这种情况下，可以结合 Public LB 的反馈来指导模型的选择。一种融合的方式是根据 Local Validation 和 Public LB 的样本数目，按比例进行加权。譬如评估标准为正确率，Local Validation 的样本数为$N_l$，正确率为$A_l$；Public LB 的样本数为 $N_p$，正确率为 $A_p$。则可以使用融合后的指标：$（N_l A_l + N_p A_p）/(N_l + N_p)$，来进行模型的筛选。对于(c)和(d)，由于数据分布跟时间相关，很有必要使用 Public LB 的反馈来进行模型的选择，尤其对于(c)图所示的情况。 2.5 模型集成如果想在比赛中获得名次，几乎都要进行模型集成（组队也是一种模型集成）。关于模型集成的介绍，已经有比较好的博文了，可以参考 [3]。在这里，我简单介绍下常用的方法，以及个人的一些经验。 2.5.1 Averaging 和 Voting直接对多个模型的预测结果求平均或者投票。对于目标变量为连续值的任务，使用平均；对于目标变量为离散值的任务，使用投票的方式。 2.5.2 Stacking图5展示了使用 5-Fold 进行一次 Stacking 的过程（当然在其上可以再叠加 Stage 2, Stage 3 等）。其主要的步骤如下： 数据集划分。将训练数据按照5-Fold进行划分（如果数据跟时间有关，需要按时间划分，更一般的划分方式请参考3.4.2节，这里不再赘述）； 基础模型训练 I（如图5第一行左半部分所示）。按照交叉验证（Cross Validation）的方法，在训练集（Training Fold）上面训练模型（如图灰色部分所示），并在验证集（Validation Fold）上面做预测，得到预测结果（如图黄色部分所示）。最后综合得到整个训练集上面的预测结果（如图第一个黄色部分的CV Prediction所示）。 基础模型训练 II（如图5第二和三行左半部分所示）。在全量的训练集上训练模型（如图第二行灰色部分所示），并在测试集上面做预测，得到预测结果（如图第三行虚线后绿色部分所示）。 Stage 1 模型集成训练 I（如图5第一行右半部分所示）。将步骤 2 中得到的 CV Prediction 当作新的训练集，按照步骤 2 可以得到 Stage 1模型集成的 CV Prediction。 Stage 1 模型集成训练 II（如图5第二和三行右半部分所示）。将步骤 2 中得到的 CV Prediction 当作新的训练集和步骤 3 中得到的 Prediction 当作新的测试集，按照步骤 3 可以得到 Stage 1 模型集成的测试集 Prediction。此为 Stage 1 的输出，可以提交至 Kaggle 验证其效果。 在图5中，基础模型只展示了一个，而实际应用中，基础模型可以多种多样，如SVM，DNN，XGBoost 等。也可以相同的模型，不同的参数，或者不同的样本权重。重复4和5两个步骤，可以相继叠加 Stage 2, Stage 3 等模型。 2.5.3 BlendingBlending 与 Stacking 类似，但单独留出一部分数据（如 20%）用于训练 Stage X 模型。 2.5.4 Bagging Ensemble SelectionBagging Ensemble Selection [5] 是我在 CrowdFlower 搜索相关性比赛中使用的方法，其主要的优点在于可以以优化任意的指标来进行模型集成。这些指标可以是可导的（如 LogLoss 等）和不可导的（如正确率，AUC，Quadratic Weighted Kappa等）。它是一个前向贪婪算法，存在过拟合的可能性，作者在文献 [5] 中提出了一系列的方法（如 Bagging）来降低这种风险，稳定集成模型的性能。使用这个方法，需要有成百上千的基础模型。为此，在 CrowdFlower 的比赛中，我把在调参过程中所有的中间模型以及相应的预测结果保留下来，作为基础模型。这样做的好处是，不仅仅能够找到最优的单模型（Best Single Model），而且所有的中间模型还可以参与模型集成，进一步提升效果。 2.6 自动化框架从上面的介绍可以看到，做一个数据挖掘比赛涉及到的模块非常多，若有一个较自动化的框架会使得整个过程更加的高效。在 CrowdFlower 比赛较前期，我对整一个项目的代码架构进行了重构，抽象出来特征工程，模型调参和验证，以及模型集成等三大模块，极大的提高了尝试新特征，新模型的效率，也是我最终能斩获名次的一个有利因素。这份代码开源在 Github 上面，目前是 Github 有关 Kaggle 竞赛解决方案的 Most Stars，地址链接。 其主要包含以下部分： 模块化特征工程 接口统一，只需写少量的代码就能够生成新的特征； 自动将单独的特征拼接成特征矩阵。 自动化模型调参和验证 自定义训练集和验证集的划分方法； 使用 Grid Search / Hyperopt 等方法，对特定的模型在指定的参数空间进行调优，并记录最佳的模型参数以及相应的性能。 自动化模型集成 对于指定的基础模型，按照一定的方法（如Averaging/Stacking/Blending 等）生成集成模型。 三、Kaggle竞赛方案盘点3.1 图像分类到目前为止，Kaggle 平台上面已经举办了大大小小不同的赛事，覆盖图像分类，销量预估，搜索相关性，点击率预估等应用场景。在不少的比赛中，获胜者都会把自己的方案开源出来，并且非常乐于分享比赛经验和技巧心得。这些开源方案和经验分享对于广大的新手和老手来说，是入门和进阶非常好的参考资料。以下笔者结合自身的背景和兴趣，对不同场景的竞赛开源方案作一个简单的盘点，总结其常用的方法和工具，以期启发思路。 3.1.1 图像分类National Data Science Bowl 3.1.2 任务详情随着深度学习在视觉图像领域获得巨大成功，Kaggle 上面出现了越来越多跟视觉图像相关的比赛。这些比赛的发布吸引了众多参赛选手，探索基于深度学习的方法来解决垂直领域的图像问题。NDSB就是其中一个比较早期的图像分类相关的比赛。这个比赛的目标是利用提供的大量的海洋浮游生物的二值图像，通过构建模型，从而实现自动分类。 3.1.3 获奖方案1st place:Cyclic Pooling + Rolling Feature Maps + Unsupervised and Semi-Supervised Approaches。值得一提的是，这个队伍的主力队员也是Galaxy Zoo行星图像分类比赛的第一名，其也是Theano中基于FFT的Fast Conv的开发者。在两次比赛中，使用的都是 Theano，而且用的非常溜。方案链接：Classifying plankton with deep neural networks 2nd place：Deep CNN designing theory + VGG-like model + RReLU。这个队伍阵容也相当强大，有前MSRA 的研究员Xudong Cao，还有大神Tianqi Chen，Naiyan Wang，Bing XU等。Tianqi 等大神当时使用的是 CXXNet（MXNet 的前身），也在这个比赛中进行了推广。Tianqi 大神另外一个大名鼎鼎的作品就是 XGBoost，现在 Kaggle 上面几乎每场比赛的 Top 10 队伍都会使用。方案链接：National Data Science Bowl 17th place：Realtime data augmentation + BN + PReLU。方案链接：ChenglongChen/caffe-windows 3.1.4 常用工具 Theano: Welcome – Theano 0.9.0 documentation Keras: Keras Documentation Cuda-convnet2: akrizhevsky/cuda-convnet2 Caffe: Caffe | Deep Learning Framework CXXNET: dmlc/cxxnet MXNet: dmlc/mxnet 3.2 销量估计3.2.1 任务名称Walmart Recruiting – Store Sales Forecasting 3.2.2 任务详情Walmart 提供 2010-02-05 到 2012-11-01 期间的周销售记录作为训练数据，需要参赛选手建立模型预测 2012-11-02 到 2013-07-26 周销售量。比赛提供的特征数据包含：Store ID, Department ID, CPI，气温，汽油价格，失业率，是否节假日等。 3.2.3 获奖方案1st place：Time series forecasting method: stlf + arima + ets。主要是基于时序序列的统计方法，大量使用了 Rob J Hyndman 的 forecast R 包。方案链接：Walmart Recruiting – Store Sales Forecasting2nd place：Time series forecasting + ML: arima + RF + LR + PCR。时序序列的统计方法+传统机器学习方法的混合，方案链接：Walmart Recruiting – Store Sales Forecasting16th placeFeature engineering + GBM。方案链接：ChenglongChen/Kaggle_Walmart-Recruiting-Store-Sales-Forecasting 3.2.4 常用工具 R forecast package: https://cran.r-project.org/web/packages/forecast/index.html R GBM package: https://cran.r-project.org/web/packages/gbm/index.html 3.3 搜索相关性3.3.1 任务名称CrowdFlower Search Results Relevance 3.3.2 任务详情比赛要求选手利用约几万个 (query, title, description) 元组的数据作为训练样本，构建模型预测其相关性打分 {1, 2, 3, 4}。比赛提供了 query, title和description的原始文本数据。比赛使用 Quadratic Weighted Kappa 作为评估标准，使得该任务有别于常见的回归和分类任务。 3.3.3 获奖方案1st place：Data Cleaning + Feature Engineering + Base Model + Ensemble。对原始文本数据进行清洗后，提取了属性特征，距离特征和基于分组的统计特征等大量的特征，使用了不同的目标函数训练不同的模型（回归，分类，排序等），最后使用模型集成的方法对不同模型的预测结果进行融合。方案链接：ChenglongChen/Kaggle_CrowdFlower 3.3.4 常用工具 NLTK: Natural Language Toolkit Gensim: gensim: topic modelling for humans XGBoost: dmlc/xgboost RGF: baidu/fast_rgf 3.4 点击率预估I3.4.1 任务名称Criteo Display Advertising Challenge 3.4.2 任务详情经典的点击率预估比赛。该比赛中提供了7天的训练数据，1 天的测试数据。其中有13 个整数特征，26 个类别特征，均脱敏，因此无法知道具体特征含义。 3.4.3 获奖方案1st place：GBDT 特征编码 + FFM。台大的队伍，借鉴了Facebook的方案 [6]，使用 GBDT 对特征进行编码，然后将编码后的特征以及其他特征输入到 Field-aware Factorization Machine（FFM） 中进行建模。方案链接：Display Advertising Challenge | Kaggle 3rd place：Quadratic Feature Generation + FTRL。传统特征工程和 FTRL 线性模型的结合。方案链接：Display Advertising Challenge | Kaggle 4th place：Feature Engineering + Sparse DNN 3.4.4 常用工具 Vowpal Wabbit: JohnLangford/vowpal_wabbit XGBoost: dmlc/xgboost LIBFFM: LIBFFM: A Library for Field-aware Factorization Machines 3.5 点击率预估II3.5.1 任务名称Avazu Click-Through Rate Prediction 3.5.2 任务详情点击率预估比赛。提供了 10 天的训练数据，1 天的测试数据，并且提供时间，banner 位置，site, app, device 特征等，8个脱敏类别特征。 3.5.3 获奖方案1st place：Feature Engineering + FFM + Ensemble。还是台大的队伍，这次比赛，他们大量使用了 FFM，并只基于 FFM 进行集成。方案链接：Click-Through Rate Prediction | Kaggle 2nd place：Feature Engineering + GBDT 特征编码 + FFM + Blending。Owenzhang（曾经长时间雄霸 Kaggle 排行榜第一）的竞赛方案。Owenzhang 的特征工程做得非常有参考价值。方案链接：owenzhang/kaggle-avazu 3.5.4 常用工具 LIBFFM: LIBFFM: A Library for Field-aware Factorization Machines XGBoost: dmlc/xgboost 四、参考资料[1] Owenzhang 的分享： Tips for Data Science Competitions [2] Algorithms for Hyper-Parameter Optimization [3] MLWave博客：Kaggle Ensembling Guide [4] Jeong-Yoon Lee 的分享：Winning Data Science Competitions [5] Ensemble Selection from Libraries of Models [6] Practical Lessons from Predicting Clicks on Ads at Facebook 五、结语作为曾经的学生党，十分感激和庆幸有 Kaggle 这样的平台，提供了不同领域极具挑战的任务以及丰富多样的数据。让我这种空有满（yi）腔（xie）理（wai）论（li）的数据挖掘小白，可以在真实的问题场景和业务数据中进行实操练手，提升自己的数据挖掘技能，一不小心，还能拿名次，赢奖金。如果你也跃跃欲试，不妨选一个合适的任务，开启数据挖掘之旅吧。 转载自知乎：Kaggle 数据挖掘比赛经验分享]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>EDA</tag>
        <tag>特征工程</tag>
        <tag>Kaggle</tag>
        <tag>Voting</tag>
        <tag>Stacking</tag>
        <tag>Blending</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（19）：机器学习性能评价指标]]></title>
    <url>%2F2017%2F05%2F24%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8819%EF%BC%89%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%A7%E8%83%BD%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[一、分类问题的评价指标1.1 混淆矩阵对一个二分类问题，将实例分成正类（postive）或者负类（negative），但在实际分类中，会出现以下四种情况： True Positive（真正，TP）：将正类预测为正类数 True Negative（真负，TN）：将负类预测为负类数 False Positive（假正，FP）：将负类预测为正类数 False Negative（假负，FN）：将正类预测为负类数 从下图可以直观的看出四者的关系： 混淆矩阵（Confusion matrix）又被称为错误矩阵，它是一种特定的矩阵来呈现算法性能的可视化呈现。其每一列代表预测值，每一行代表的是实际的类别，这个名字来源于他是否可以非常容易的表明多个类别是否有混淆（也就是一个class被预测为另一个class）混淆矩阵的$i$行$j$列是列别$i$被分为类别$j$的样本个数。 1.2 精确率、召回率与F1值 精确率（precision rate）定义为： P=\frac{TP}{TP+FP} 这里需要注意的是精确率（precision）和准确率（accuracy）是不一样的 ACC=\frac{TP+TN}{TP+TN+FP+FN} 在非平衡数据的情况下，准确率这个评价指标有很大的缺陷。比如在互联网广告里面，点击的数量是很少的，一般只有千分之几，如果用Accuracy，即使全部预测成负类（不点击），ACC也达到了99%以上，这就没有意义了。 召回率（Recall rate）定义为： R=\frac{TP}{TP+FN}此外，还有F1值，它是精确率和召回率的调和均值，即 \frac{2}{F_1}=\frac{1}{P}+\frac{1}{R} F_1=\frac{2TP}{2TP+FP+FN}精确率与召回率都很高时，$F_1$值也会很高。 1.4 通俗理解通俗来讲，精确率是针对我们的预测结果而言的，他表示的是预测为正的样本中有多少是对的，那么预测为正就有两种可能了，一种就是把正类预测为正类（TP），另一种就是把负类预测为正类（FP）。 而召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类（TP），另一种就是把原来的正类预测为负类（FN）。 在信息搜索领域，精确率和召回率又被称为查准率和查全率 查准率=\frac{检索出的相关信息量}{检索出的信息总量}查全率=\frac{检索出的相关信息量}{系统中的相关信息总量}1.5 ROC曲线ROC曲线首先是由二战中的电子工程师和雷达工程师发明的，用来侦测战场上的敌军载具（飞机、船舰），也就是信号检测理论。之后很快就被引入了心理学来进行信号的知觉检测。数十年来，ROC分析被用于医学、无线电、生物学、犯罪心理学领域中，而且最近在机器学习（machine learning）和数据挖掘（data mining）领域也得到了很好的发展。 下图是一个ROC曲线的示例图。 在这个ROC曲线的示例图中，横坐标为false positive rate(FPR)，纵坐标为true positive rate（TPR）。由混淆矩阵可得到横纵轴的计算公式。 1）$TPR=\frac{TP}{TP+FN}$ 代表分类器预测的正类中实际正实例占所有正实例的比例。直观上代表能将正例分对的概率。 2）$FPR=\frac{FP}{FP+TN}$ 代表分类器预测的正类中实际负实例占所有负实例的比例。直观上代表将负类错分为正例的概率。 假设采用逻辑回归分类器，其给出针对每个实例为正类的概率，那么通过设定一个阈值如0.6，概率大于等于0.6的为正类，小于0.6的为负类。对应的就可以算出一组(FPR,TPR)，随着阈值的逐渐减小，越来越多的实例被划分为正类，但是这些正类中同样也掺杂着更多的负实例，即TPR和FPR会同时增大。阈值最大时，对应坐标点（0，0），阈值最小时，对应坐标点（1，1）。 接下来我们考虑ROC曲线图中的四个点和一条线。第一个点，(0,1)，即FPR=0, TPR=1，这意味着FN（false negative）=0，并且FP（false positive）=0。这是一个完美的分类器，它将所有的样本都正确分类。第二个点，(1,0)，即FPR=1，TPR=0，类似地分析可以发现这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。第三个点，(0,0)，即FPR=TPR=0，即FP（false positive）=TP（true positive）=0，可以发现该分类器预测所有的样本都为负样本（negative）。类似的，第四个点（1,1），分类器实际上预测所有的样本都为正样本。经过以上的分析，我们可以断言，ROC曲线越接近左上角，该分类器的性能越好。 下面考虑ROC曲线图中的虚线y=x上的点。这条对角线上的点其实表示的是一个采用随机猜测策略的分类器的结果，例如(0.5,0.5)，表示该分类器随机对于一半的样本猜测其为正样本，另外一半的样本为负样本。 如何绘制ROC曲线呢？ 假设已经得出一系列样本被划分为正类的概率，然后按照大小排序，下图是一个示例，图中共有20个测试样本，“class”一栏表示每个测试样本真正的标签（P表示正样本，n表示负样本），“Score”表示每个测试样本属于正样本的概率。 接下来，我们从高到低，依次将“Score”值作为阈值的threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第四个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。每次选取一个不同的threshold，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。这样一来，我们一共得到了20组FPR和TPR的值，将它们画在ROC曲线的结果如下图： 1.6 AUCAUC（Area under Curve）指的是ROC曲线下的面积，介于0和1之间。AUC作为数值可以直观地评价分类器的好坏，值越大越好。 The AUC value is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example. 首先AUC是一个概率值，当你随机挑选一个正样本以及负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值，AUC值越大，当前分类算法越有可能将正样本排在负样本前面，从而能够更好地分类。 以下是根据AUC判断分类器优劣的标准： 1）AUC=1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数场合，不存在完美的分类器。 2）0.5&lt;AUC&lt;1，优于随机猜测。这个分类器妥善设定阈值的话，能有预测价值。 3）AUC=0.5，跟随机猜测一样（如丢硬币），模型没有预测价值。 4）AUC&lt;0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。 那我们为什么使用ROC曲线呢？ 既然已经有那么多的评价标准，为何还要使用ROC和AUC曲线呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现非平衡数据的现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。下图是ROC曲线和Precision-Recall曲线的对比： 在上图中，a和c为ROC曲线，b和d为Precision-Recall曲线。a和b展示的是分类器在原始测试集（正负样本分布平衡）的结果，c和d是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall则变化较大。 二、回归问题的评价指标2.1 平均绝对误差平均绝对误差MAE（Mean Absolute Reeor）又被称为L1范数损失（L1-norm loss）： {\rm MAE}(y, \hat{y})=\frac{1}{n_{\rm samples}}\sum\limits_{i=1}^{n_{\rm samples}}|y_i-\hat{y}_i|2.2 平均平方误差平均平方误差MSE（Mean Squared Error）又被称为L2范数损失（L2-norm loss）: {\rm MSE}(y, \hat{y})=\frac{1}{n_{\rm samples}}\sum\limits_{i=1}^{n_{\rm samples}}(y_i-\hat{y}_i)^2]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>评价指标</tag>
        <tag>精确率</tag>
        <tag>召回率</tag>
        <tag>ROC</tag>
        <tag>AUC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（12）：pytorch实现卷积神经网络]]></title>
    <url>%2F2017%2F05%2F21%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9Apytorch%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[1.载入模块12345import torch import torch.nn as nnimport torchvision.datasets as dsetsimport torchvision.transforms as transformsfrom torch.autograd import Variable 其中torchvision.transforms 用于数据预处理，torchvision.datasets加载内置数据集 2.设置参数123num_epochs = 5batch_size = 100learning_rate = 0.001 迭代次数num_epochs设置为5；批处理样本数batch_size设置为100；学习率learning_rate设置为0.001。 3.加载数据集加载训练集，将MNIST数据集自动从网上下载并解压，train=true表示取出训练集部分，并变换为张量。 1234train_dataset = dsets.MNIST(root='../data/', train=True, transform=transforms.ToTensor(), download=True) 加载测试集，train=False即表示取出测试集部分，并变换为张量。 123test_dataset = dsets.MNIST(root='../data/', train=False, transform=transforms.ToTensor()) 将训练集的60000张图片划分成600份，每份100张图，用于mini-batch输入。同时将测试集的10000张图片分成100份，每份100张图。shffule=True在表示不同批次的数据遍历时，打乱顺序，反之则不打乱顺序。 123456train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) 4.CNN模型（两个卷积层）1234567891011121314151617181920212223class CNN(nn.Module): def __init__(self): super(CNN, self).__init__() self.layer1 = nn.Sequential( nn.Conv2d(1, 16, kernel_size=5, padding=2),#卷积：1 input image channel, 16 output channels, 5x5 square convolution kernel，2 zero padding） nn.BatchNorm2d(16),#归一化 nn.ReLU(),#非线性激活函数ReLU nn.MaxPool2d(2))#池化层 self.layer2 = nn.Sequential( nn.Conv2d(16, 32, kernel_size=5, padding=2), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2)) self.fc = nn.Linear(7*7*32, 10)#全连接层，in_features, out_features, bias=True def forward(self, x): out = self.layer1(x) out = self.layer2(out) out = out.view(out.size(0), -1) out = self.fc(out) return out# 正常情况下, 我们都会用类进行封装一个网络 cnn = CNN() 5.损失函数与优化方法12criterion = nn.CrossEntropyLoss()#损失函数，这里为交叉熵optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)#优化方法，这里使用Adam 6.训练模型12345678910111213141516for epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): # wrap them in Variable images = Variable(images) labels = Variable(labels) # Forward + Backward + Optimize optimizer.zero_grad() outputs = cnn(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics if (i+1) % 100 == 0: print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0])) Epoch [1/5], Iter [100/600] Loss: 0.1363 Epoch [1/5], Iter [200/600] Loss: 0.0487 Epoch [1/5], Iter [300/600] Loss: 0.0688 Epoch [1/5], Iter [400/600] Loss: 0.1273 Epoch [1/5], Iter [500/600] Loss: 0.0283 Epoch [1/5], Iter [600/600] Loss: 0.0375 Epoch [2/5], Iter [100/600] Loss: 0.0398 Epoch [2/5], Iter [200/600] Loss: 0.0595 Epoch [2/5], Iter [300/600] Loss: 0.0793 Epoch [2/5], Iter [400/600] Loss: 0.0166 Epoch [2/5], Iter [500/600] Loss: 0.0235 Epoch [2/5], Iter [600/600] Loss: 0.0128 Epoch [3/5], Iter [100/600] Loss: 0.0273 Epoch [3/5], Iter [200/600] Loss: 0.0507 Epoch [3/5], Iter [300/600] Loss: 0.0384 Epoch [3/5], Iter [400/600] Loss: 0.0150 Epoch [3/5], Iter [500/600] Loss: 0.0086 Epoch [3/5], Iter [600/600] Loss: 0.0616 Epoch [4/5], Iter [100/600] Loss: 0.0243 Epoch [4/5], Iter [200/600] Loss: 0.0112 Epoch [4/5], Iter [300/600] Loss: 0.0391 Epoch [4/5], Iter [400/600] Loss: 0.0140 Epoch [4/5], Iter [500/600] Loss: 0.0324 Epoch [4/5], Iter [600/600] Loss: 0.0053 Epoch [5/5], Iter [100/600] Loss: 0.0358 Epoch [5/5], Iter [200/600] Loss: 0.0109 Epoch [5/5], Iter [300/600] Loss: 0.0066 Epoch [5/5], Iter [400/600] Loss: 0.0028 Epoch [5/5], Iter [500/600] Loss: 0.0380 Epoch [5/5], Iter [600/600] Loss: 0.0518 7.模型测试1234567891011cnn.eval()correct = 0total = 0for images,labels in test_loader: images = Variable(images) outputs = cnn(images) _,predicted = torch.max(outputs.data,1) total += labels.size(0) correct += (predicted == labels).sum()print ('Test Accuracy of model on the 10000 test images:%d %%'%(100*correct/total)) Test Accuracy of model on the 10000 test images:99 % 8.保存模型1torch.save(cnn.state_dict(),'cnn.pkl')]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（11）：神经网络防止过拟合的方法]]></title>
    <url>%2F2017%2F05%2F20%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[过拟合（overfitting）是指在模型参数拟合过程中的问题，由于训练数据包含抽样误差，训练时，复杂的模型将抽样误差也考虑在内，将抽样误差也进行了很好的拟合。具体表现就是最终模型在训练集上效果好，而在测试集上的效果很差，模型的泛化能力比较弱。 那为什么要解决过拟合现象呢？这是因为我们拟合的模型一般是用来预测未知的结果（不在训练集内），过你个虽然在训练集上效果很好，但在实际使用时（测试集）效果很差。同时，在很多问题上，我们无法穷尽所以状态，不可能将所有情况都包含在训练集上。所以，必须要解决过拟合问题。 之所以过拟合在机器学习中比较常见，就是因为机器学习算法为了满足尽可能复杂的任务，其模型的拟合能力一般远远高于问题复杂度，也就是说，机器学习算法有“拟合出正确规则的前提下，进一步拟合噪声”的能力。 过拟合主要是有两个原因造成的：数据太少+模型太复杂。所以，我们可以通过使用合适复杂度的模型来防止过拟合问题，让其足够拟合真正的规则，同时又不至于拟合太多抽样误差。 通过上图可以看出，随着模型训练的进行，模型的复杂度会增加，此时模型在训练数据集上的训练误差会逐渐减小，但是在模型的复杂度达到一定程度时，模型在验证集上的误差反而随着模型的复杂度增加而增大。此时便发生了过拟合，即模型的复杂度升高，但是该模型在除训练集之外的数据集上却不work。 为了防止过拟合，我们需要用到一些方法，如下所示： 一、获取更多的数据所有的过拟合无非就是训练样本的缺乏和训练参数的增加。一般要想获得更好的模型，需要大量的训练参数，这也是为什么CNN网络越来越深的原因之一，而如果训练样本缺乏多样性，那再多的训练参数也毫无意义，因为这造成了过拟合，训练的模型泛化能力相应也会很差。大量数据带来的特征多样性有助于充分利用所有的训练参数。 在数据挖掘领域流行着这样的一句话，“有时候往往拥有更多的数据胜过一个好的模型”。因为我们在使用训练数据训练模型，通过这个模型对将来的数据进行拟合，而在这之间又一个假设便是，训练数据与将来的数据是独立同分布的。即使用当前的训练数据来对将来的数据进行估计与模拟，而更多的数据往往估计与模拟地更准确。因此，更多的数据有时候更优秀。但是往往条件有限，如人力物力财力的不足，而不能收集到更多的数据，如在进行分类的任务中，需要对数据进行打标，并且很多情况下都是人工得进行打标，因此一旦需要打标的数据量过多，就会导致效率低下以及可能出错的情况。所以，往往在这时候，需要采取一些计算的方式与策略在已有的数据集上进行手脚，以得到更多的数据。通俗得讲，数据扩增即需要得到更多的符合要求的数据，即和已有的数据是独立同分布的，或者近似独立同分布的。 如何获取更多的数据，一般有以下几个方法： 1）从数据源头获取更多数据：这个是容易想到的，例如物体分类，我就再多拍几张照片好了；但是，在很多情况下，大幅增加数据本身就不容易；另外，我们不清楚获取多少数据才算够； 2）根据当前数据集估计数据分布参数，使用该分布产生更多数据：这个一般不用，因为估计分布参数的过程也会代入抽样误差。 3）通过一定规则扩充数据，即数据增强（Data Augmentation）。如在物体分类问题里，物体在图像中的位置、姿态、尺度，整体图片明暗度等都不会影响分类结果。我们就可以通过图像平移、翻转、缩放、切割等手段将数据库成倍扩充，以下为具体的方案： 二、使用合适的模型2.1 限制权值 Weight Decay常用的weight decay有L1和L2正则化，L1较L2能够获得更稀疏的参数，但L1零点不可导。在损失函数中，weight decay是放在正则项（regularization）前面的一个系数，正则项一般指示模型的复杂度，所以weight decay的作用是调节模型复杂度对损失函数的影响，若weight decay很大，则复杂的模型损失函数的值也就大。 L1和L2正则化是很重要的过拟合方法，后边专门用一篇文章来讲。 2.2 训练时间 Early stopping提前停止其实是另一种正则化方法，就是在训练集和验证集上，一次迭代之后计算各自的错误率，当在验证集上的错误率最小，在没开始增大之前停止训练，因为如果接着训练，训练集上的错误率一般是会继续减小的，但验证集上的错误率会上升，这就说明模型的泛化能力开始变差了，出现过拟合问题，及时停止能获得泛化更好的模型。如下图（左边是训练集错误率，右图是验证集错误率，在虚线处提前结束训练）： Early stopping方法的具体做法是，在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练。这种做法很符合直观感受，因为accurary都不再提高了，在继续训练也是无益的，只会提高训练的时间。那么该做法的一个重点便是怎样才认为validation accurary不再提高了呢？并不是说validation accuracy一降下来便认为不再提高了，因为可能经过这个Epoch后，accuracy降低了，但是随后的Epoch又让accuracy又上去了，所以不能根据一两次的连续降低就判断不再提高。一般的做法是，在训练的过程中，记录到目前为止最好的validation accuracy，当连续10次Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了（Early Stopping）。这种策略也称为“No-improvement-in-n”，n即Epoch的次数，可以根据实际情况取，如10、20、30。 在神经网络中，对于每个神经元而言，其激活函数在不同的区间的性能是不同的： 当网络权值较小时，神经元的激活函数工作在线性区，此时神经元的拟合能力较弱（类似线性神经元）。有了以上共识之后，就可以解释为什么训练时间（early stopping）有用：因为我们在初始化网络的时候一般都是初始为较小的权值。训练时间越长，部分网络权值可能越大。如果我们在合适时间停止训练，就可以将网络的能力限制在一定范围内。 2.3 网络结构这个很好理解，减少网络的层数、神经元个数等均可以限制网络的拟合能力。 2.4 增加噪声给网络加噪声也有很多方法： 2.4.1 在输入中加噪声噪声会随着网络传播，按照权值的平方放大，并传播到输出层，对误差 Cost 产生影响。推导直接看 Hinton 的 PPT 吧： 在输入中加高斯噪声，会在输出中生成$\sum_i\sigma _i^2w_i^2$的干扰项。训练时，减小误差，同时也会对噪声产生的干扰项进行惩罚，达到减小权值的平方的目的，达到与L2 regularization类似的效果（对比公式）。 2.4.2 在权值上加噪声在初始化网络的时候，用0均值的高斯分布作为初始化。Alex Graves 的手写识别 RNN 就是用了这个方法： Graves, Alex, et al. “A novel connectionist system for unconstrained handwriting recognition.” IEEE transactions on pattern analysis and machine intelligence 31.5 (2009): 855-868. It may work better, especially in recurrent networks (Hinton) 2.4.3 对网络的响应加噪声如在前向传播过程中，让某些神经元的输出变为 binary 或 random。显然，这种有点乱来的做法会打乱网络的训练过程，让训练更慢，但据 Hinton 说，在测试集上效果会有显著提升 （But it does significantly better on the test set!）。 三、结合多种模型简而言之，训练多个模型，以每个模型的平均输出作为结果。 从 N 个模型里随机选择一个作为输出的期望误差$&lt;[(t-y_i)]^2&gt;$ ，会比所有模型的平均输出的误差$&lt;[(t-\bar{y})]^2&gt;$大: 大概基于这个原理，就可以有很多方法了。 3.1 Bagging和Boost简单理解，就是分段函数的概念：用不同的模型拟合不同部分的训练集。以随机森林（Rand Forests）为例，就是训练了一堆互不关联的决策树。但由于训练神经网络本身就需要耗费较多自由，所以一般不单独使用神经网络做Bagging。 bagging和boosting详细可见机器学习算法系列（6）：AdaBoost 3.2 Dropout正则是通过在代价函数后面加上正则项来防止模型过拟合的。而在神经网络中，有一种方法是通过修改神经网络本身结构来实现的，其名为Dropout。该方法是在对网络进行训练时用一种技巧（trick）， Dropout是hintion最近2年提出的，源于其文章Improving neural networks by preventing co-adaptation of feature detectors.中文大意为：通过阻止特征检测器的共同作用来提高神经网络的性能。 在训练时，每次随机（如50%概率）忽略隐层的某些节点；这样，我们相当于随机从$2^H$个模型中采样选择模型；同时，由于每个网络只见过一个训练数据（每次都是随机的新网络），所以类似 bagging 的做法，这就是我为什么将它分类到「结合多种模型」中； 此外，而不同模型之间权值共享（共同使用这 H 个神经元的连接权值），相当于一种权值正则方法，实际效果比 L2 regularization 更好。 正则化方法：L1和L2 regularization、数据集扩增、dropout]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
        <tag>dropout</tag>
        <tag>过拟合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（10）：DMC—卷积神经网络分享]]></title>
    <url>%2F2017%2F05%2F19%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%8810%EF%BC%89%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[PDF链接：卷积神经网络PPT链接（动图）：百度云]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
        <tag>Batch Normalization</tag>
        <tag>dropout</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（9）：Batch Normalization]]></title>
    <url>%2F2017%2F05%2F14%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%889%EF%BC%89%EF%BC%9ABatch%20Normalization%2F</url>
    <content type="text"><![CDATA[batch normalization(Ioffe and Szegedy, 2015) 是优化深度神经网络中最激动人心的创新之一。实际上它并不是一个优化算法，而是一个自适应的重新参数化 的方法，试图解决训练非常深层模型的困难。Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift机器学习领域有一个很重要的假设：iid独立同分布假设，就是假设训练数据和测试数据满足相同分布，这是通过训练数据训练出来的模型能够在测试集上获得好的效果的一个基本保证。Batch Normalization就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布。 一、Internal covariate shift首先给出covariate shift的定义：模型实例集合中的输入值X的分布总是变化，违背了idd独立通同布假设。 深度学习网络包含很多隐层的网络结构，在训练过程中参数会不断发生改变，导致后续每一层输入的分布也面临着covariate shift，也就是在训练过程中，隐层的输入分布总是发生改变，这就是所谓的Internal covariate shift，Internal指的是深层网络的隐层，covariate shift发生在深度神经网络内部，就被称作Internal covariate shift。 在DNN的实验中，对数据进行预处理时，例如白化或者zscore，甚至是简单的减均值操作都是可以加速收敛的。为什么减均值、白化可以加快训练，作如下分析： 首先，图像数据的每一维一般都是0~255之间的数字，因此数据点智慧落在第一象限，而且图像数据具有很强的相关性，比如第一个灰度值为30，比较黑，那它旁边的一个像素值一般不会超过100，否则给人的感觉就像噪声一样。由于强相关性，数据点仅会落在第一象限的小区域内，形成类似第一个图的狭长分布。 其次，神经网络模型在初始化的时候，权重W都是随机采样生成的，一般都是零均值，因此起初的拟合y=Wx+b，基本过原点附近，如图b红色虚线。因此，网络需要经过多次迭代学习才能逐步达到如紫色实线的拟合，即收敛的比较慢。更何况，这里只是个二维的演示，数据占据四个象限中的一个，但如果是几百、几千、上万维呢？而且数据在第一象限也只是占了很小的一部分区域而已，可想而知若不对数据进行预处理带来了多少运算资源的浪费，而且大量的数据外分割面在迭代时很可能会在刚进入数据中是就遇到了一个局部最优，导致overfit的问题。如果我们对输入数据先作减均值操作，如图c，数据点就不再只分布在第一象限，这是一个随机分界面落入数据分布的概率增加了$2^n$倍，大大加快学习。更进一步的，我们对数据再进行去相关操作，例如PCA和ZCA白化，数据不再是一个狭长的分布，随机分界面有效的概率就又大大增加了，使得数据更加容易区分，这样又会加快训练，如图d。 不过计算协方差的特征值太耗时也太耗空间，一般最多只用到z-score处理，即每一维减去自身均值，再除以自身标准差，这样能使数据点在每维上具有相似的宽度，可以起到增大数据分布范围，进而使更多随机分界面有意义的作用。 二、Batch Normalization2.1 直观解释Batch Normalization的基本思想其实很直观：因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B,U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或者正值），所以这导致反向传播的时候低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因，而BN就是通过一定的规范化手段，对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛地快。 但是这里有个问题，如果都通过Batch Normalization，那么不就跟把非线性函数替换成线性函数效果相同了？我们知道，如果是多层的线性函数变换，其实这个深层是没有意义的，因为多层线性网络跟一层线性网络是等价的。这意味着网络的表达能力下降了，这也意味着深度的意义就没有了。比如下图，在使用sigmoid激活函数的时候，如果把数据限制到零均值单位方差，那么相当于只使用了激活函数中近似线性的部分，这显然会降低模型的表达能力。BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale*x+shift)，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把这个值从标准正态分布左移或者由移一点并长胖一点或者变瘦一点，每个实例挪动的程度不一样，这样等价于非线性函数的值从正中心周围的线性区往非线性区动了动，让因训练所需而“刻意”加入的BN能够有可能还原最初的输入。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。从而保证整个网络的capacity。 2.2 算法过程假设对于一个深层神经网络来说，其中两层结构如下：要对每个隐藏神经元的激活值做BN，可以想象成每个隐层又加上了一层BN操作层，它位于X=WY+B激活值获得之后，非线性函数变换之前，其图示如下：对Mini-Batch SGD来说，一次训练过程里面包含m个训练实例，其具体BN操作就是对于隐层内每个神经元的激活值来说，进行如下变换： \hat{x}^{\left(k\right)}=\frac{x^{\left(k\right)}-E\left[x^{\left(k\right)}\right]}{\sqrt{var\left[x^{\left(k\right)}\right]}}要注意，这里t层某个神经元的$x(k)$不是指原始输入，就是说不是$t-1$层每个神经元的输出，而是$t$曾这个神经元的激活$x=WU+B$，这里的$U$才是$t-1$层神经元的输出。还有一点，上述公式中用到了均值和方差，在理想情况下均值和方差是针对整个数据集的，但显然这是不现实的，因此，作者做了简化，用一个Batch的均值和方差作为对整个数据集均值和方差的估计。这个变换就是：某个神经元对应的原始的激活$x$减去Mini-Batch内$m$个激活$x$求得的均值$E(x)$并除以求得的方差$Var(x)$来进行转换。 上文说过经过这个变换后某个神经元的激活$x$形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大梯度，增强反向传播信息流行性，加快训练收敛速度。但是这样会导致网络表达能力下降，为了防止这一点，每个神经元增加两个调节参数（scale和shift），这俩个参数是通过训练来学习的，用来对变换后的激活反变换，使得网络表达能力增强，即对变换后的激活进行如下的scle和shift操作，这其实是变换的反操作： y^{\left(k\right)}=\gamma^{\left(k\right)}\hat{x}^{\left(k\right)}+\beta^{\left(k\right)}其整个算法流程如下： 2.3 推理过程BN在训练的时候可以根据Mini-Batch数据里可以得到的统计量，那就想其他办法来获得这个统计量，就是均值和方差。可以用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量，因为本来就打算用全局的统计量，知识因为计算量等太大所以才会用Mini-Batch这种简化方式的，那么在推理的时候直接用全局统计量即可。 决定了获得统计量的数据范围，那么接下来的问题就是如何获得均值和方差的问题。很简单，因为每次做Mini-Batch训练时，都会有那个Mini-Batch里m个训练实例获得的均值和方差，现在要全局统计量，只要把每个Mini-batch的均值和方差统计量记住，然后对这些均值和方差求其对应的数学期望即可得出全局统计量，即： E\left[x\right]\gets E_{\beta}\left[\mu_{\beta}\right] Var\left[x\right]\gets\frac{m}{m-1}E_{\beta}\left[\sigma_{\beta}^{2}\right]有了均值和方差，每个隐藏神经元也已经有对应训练好的Scaling参数和Shift参数，就可以在推导的时候对每个神经元的激活数据计算BN进行变换了，在推理过程中进行BN采取如下方式： y=\frac{\gamma}{\sqrt{Var\left[x\right]+\epsilon}}·x+\left(\beta -\frac{\gamma E\left[x\right]}{\sqrt{Var\left[x\right]+\epsilon}}\right)这个公式其实和训练时 y^{\left(k\right)}=\gamma^{\left(k\right)}\hat{x}^{\left(k\right)}+\beta^{\left(k\right)}是等价的，通过简单的合并计算推导就可以得出这个结论。在实际运行时，按照这种变体形式可以减少计算量，因为对每一个隐节点来说：$\frac{\gamma}{\sqrt{Var\left[x\right]+\epsilon}}$和$\frac{\gamma E\left[x\right]}{\sqrt{Var\left[x\right]+\epsilon}}$都是固定值，这样两个值可以实现算好存起来，在推理的时候直接用就行了，比原始的公式每一步骤都少了出发的运算过程，乍一看也没少多少计算量，但是如果隐层节点个数多的话节省的计算量就比较多了。 2.4 参数训练以上是对算法原理的讲述，在反向传导的时候，我们需要求最终的损失函数对$\gamma$和$\beta$两个参数的导数，还要求损失函数对Wx+b中x的导数，一遍使误差继续向后传播。几个主要的公式如下，主要用到了链式法则。 三、Experiments作者在文章中也做了很多实验对比，这里简要说明两个： 下图a说明，BN可以加速训练。图b和c分别展示了训练过程中输入数据分布的变化情况。 下表是一个实验结果的对比，需要注意的是在使用BN的过程中，算法对sigmoid激活函数的提升非常明显，解决了困扰学术界十几年的sigmoid过饱和的问题，但sigmoid在分类问题上确实没有ReLU好用，大概是因为sigmoid的中间部分太“线性”了，不像ReLU一个很大的转折，在拟合复杂非线性函数的时候可能没那么高效。 四、算法优势论文中罗列了Batch Normalization的很多作用，一一列举如下： 1）可以使用很高的学习率。如果每层的scale不一致，实际上每层需要的学习率是不一样的，同一层不同维度的scale往往也需要不同大小的学习率，通常需要使用最小的那个学习率才能保证损失函数有效下降，Batch Normalization 2）移除或使用较低的dropout。dropout是常用的防止overfitting的方法，而导致overfitting的位置往往在数据边界处，如果初始化权重就已经落在数据内部，overfitting现象就可以得到一定的缓解。论文中最后的模型分别使用10%、5%和0%的dropout训练模型，与之前的40%~50%相比，可以大大提高训练速度。 3） 降低L2权重衰减系数。 还是一样的问题，边界处的局部最优往往有几维的权重（斜率）较大，使用L2衰减可以缓解这一问题，现在用了Batch Normalization，就可以把这个值降低了，论文中降低为原来的5倍。 4）取消Local Response Normalization层。 由于使用了一种Normalization，再使用LRN就显得没那么必要了。而且LRN实际上也没那么work。 5）减少图像扭曲的使用。 由于现在训练epoch数降低，所以要对输入数据少做一些扭曲，让神经网络多看看真实的数据。 说完BN的优势，自然可以知道什么时候用BN比较好。例如，在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Batch Normalization</tag>
        <tag>过拟合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（8）：激活函数]]></title>
    <url>%2F2017%2F05%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%888%EF%BC%89%EF%BC%9A%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[深度学习的基本原理是基于人工神经网络，信号从一个神经元进入，经过非线性的activation function，传入到下一层神经元；再经过该层神经元的activate function，继续往下传递，如此循环往复，直到输出层。其中的激活函数的主要作用是提供网络的非线性建模能力，使得神经网络有足够的capacity来抓取复杂的pattern，在各个领域取得state-of-the-art的结果。 现在假设一个神经网络中仅包含线性激励和全连接运算，那么该网络仅仅能够表达线性映射，即使增加网络的深度也依旧还是线性映射，即输出都是输入的线性组合，失去了隐藏层存在的意义，难以有效建模实际环境中非线性分布的数据。加入非线性激活函数之后，深度学习网络可以逼近任意函数，具备了分层的非线性映射学习能力。加拿大蒙特利尔大学的Bengio教授在 ICML 2016 的文章中给出了激活函数的定义：激活函数是映射 h:R→R，且几乎处处可导。从定义来看，几乎所有连续可导函数都可以用作激活函数。但目前常见的多是分段线性和具有指数形状的非线性函数。 显而易见，activation function在深度学习中举足轻重，也是很活跃的研究领域之一。目前来讲，选择怎样的activation function不在于它能否模拟真正的神经元，而在于能否便于优化整个深度神经网络。 一、软饱和与硬饱和激活函数Bengio 教授等将具有 1）在定义域内处处可导 2）两侧导数逐渐趋近于0，即$\lim_{x\rightarrow\infty}f’\left(x\right)=0$。的激活函数定义为软饱和激活函数。 与极限的定义类似，饱和也分为左饱和与右饱和，左侧软饱和为： \lim_{x\rightarrow -\infty}f'\left(x\right)=0右侧软饱和为： \lim_{x\rightarrow +\infty}f'\left(x\right)=0与软饱和激活函数相对的是硬饱和激活函数，即： f'(x)=0, 当|x|>c,c为常数同理，应饱和也分为左饱和与右饱和，左侧硬饱和为： f'(x)=0, 当-x>c, c为正数右侧硬饱和为： f'(x)=0, 当x>c, c为正数二、sigmoidsigmoid非线性函数的数学公式为： \sigma\left(x\right)=\frac{1}{1+e^{-x}}函数图像及梯度函数图像如下所示：它将输入实数值“挤压”到0-1范围内。更具体地说，很大的负数变成0，很大的正数变成1。它是便于求导的平滑函数，其导数为$\sigma(x)(1-\sigma(x))$，这是它的优点。sigmoid 在定义域内处处可导，且两侧导数逐渐趋近于0，即：$\lim_{x\rightarrow\infty}f’\left(x\right)=0$ 然而现在sigmoid函数已经不太受欢迎，实际很少使用了，这是因为它有三个主要缺点： 1）梯度消失。Sigmoid 的软饱和性，使得深度神经网络在二三十年里一直难以有效的训练，是阻碍神经网络发展的重要原因。具体地，我们知道优化神经网络的方法是Back Propagation，即导数的反向传递：先计算输出层对应的loss，然后将loss以导数的形式不断向上一层网络传递，修正相应的参数，达到降低loss的目的。sigmoid反向传导的梯度包含了一个f’(x) 因子（sigmoid关于输入的导数），因此一旦输入落入饱和区，f’(x) 就会变得接近于0，导致了向底层传递的梯度也变得非常小。此时，网络参数很难得到有效训练。这种现象被称为梯度消失。一般来说， sigmoid 网络在 5 层之内就会产生梯度消失现象。我们也可以在图中看出原因，主要在于两点：(1) 在上图中容易看出，当$\sigma(x)$中x较大或较小时，导数接近0，而后向传递的数学依据是微积分求导的链式法则，当前层的导数需要之前各层导数的乘积，几个小数的相乘，结果会很接近0 (2) Sigmoid导数的最大值是0.25，这意味着导数在每一层至少会被压缩为原来的1/4，通过两层后被变为1/16，…，通过10层后为1/1048576。请注意这里是“至少”，导数达到最大值这种情况还是很少见的。梯度消失问题至今仍然存在，但被新的优化方法有效缓解了，例如DBN中的分层预训练，Batch Normalization的逐层归一化，Xavier和MSRA权重初始化等代表性技术。 2）Sigmoid函数的输出不是Zero-centered的。这个性质并不是我们想要的，因为在神经网络后面层中的神经元得到的数据将不是零中心的。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数(比如在$f=w^Tx+b$中每个元素都x&gt;0),那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式f而定）。这将会导致梯度下降权重更新时出现z字型的下降（如下图所示）。然而，可以看到整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。因此，该问题相对于上面的神经元饱和问题来说只是个小麻烦，没有那么严重。 3）幂运算相对耗时：相对于前两项，这其实并不是一个大问题，我们目前是具备相应计算能力的，但面对深度学习中庞大的计算量，最好是能省则省。之后我们会看到，在ReLU函数中，需要做的仅仅是一个thresholding，相对于幂运算来讲会快很多。 三、tanhtanh非线性函数的数学公式为： tanh x = \frac{e^x-e^{-x}}{e^x+e^{-x}}函数图像及梯度函数图像如下所示：如上图所示，计算可以知道：$tanh(x)=2sigmoid(2x)-1$，它其实是一个简单放大的sigmoid神经元，和sigmoid神经元一样，也具有软饱和性。但是和sigmoid神经元不同的是，它解决了zero-centered的输出问题，因此，在实际操作中，tanh非线性函数比sigmoid非线性函数更受欢迎。然而，gradient vanishing的问题和幂运算的问题仍然存在。Xavier在文献[]中分析了sigmoid与tanh的饱和现象及特点，具体见原论文。此外，文献[]中提到了tanh网络的收敛速度要比sigmoid块。因为tanh的输出均值比sigmoid更接近0，SGD会更接近natural gradient（一种二次优化技术），从而降低所需的迭代次数。 四、ReLUReLU非线性函数的数学公式为： ReLU(x)=max(0,x)函数图像及梯度函数图像如下所示：虽然2006年Hinton教授提出通过分层无监督预训练解决深层网络训练困难的问题，但是深度网络的直接监督式训练的最终突破，最主要的原因是新型激活函数ReLU。它有以下几大优点： 1）解决了gradient vanishing问题：ReLU在$x0$时导数为1，所以，ReLU能够在$x&gt;0$时保持梯度不衰减，从而缓解梯度消失问题。 2）计算速度非常快。对比sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，ReLU可以简单地通过对一个矩阵进行阈值计算得到。 3）收敛速度非常快。相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用。下图是从 Krizhevsky 等的论文中截取的图表，指明使用ReLU比使用tanh的收敛快6倍。 4）ReLU另外一个性质是提供神经网络的稀疏表达能力，在Bengio教授的Deep Sparse Rectifier Neural Network[6]一文中被认为是ReLU带来网络性能提升的原因之一。但后来的研究发现稀疏性并非性能提升的必要条件，文献 RReLU [9]也指明了这一点。 PReLU[10]、ELU[7]等激活函数不具备这种稀疏性，但都能够提升网络性能。本文作者在文章[8]中给出了一些实验比较结果。首先，在cifar10上采用NIN网络，实验结果为 PReLU &gt; ELU &gt; ReLU，稀疏性并没有带来性能提升。其次，在 ImageNet上采用类似于[11] 中model E的15 层网络，实验结果则是ReLU最好。为了验证是否是稀疏性的影响，以 LReLU [12]为例进一步做了四次实验，负半轴的斜率分别为1，0.5，0.25, 0.1，需要特别说明的是，当负半轴斜率为1时，LReLU退化为线性函数，因此性能损失最大。实验结果展现了斜率大小与网络性能的一致性。综合上述实验可知，ReLU的稀疏性与网络性能之间并不存在绝对正负比关系。 ReLU也有几个缺点： 1）Dead ReLU Problem。随着训练的推进，部分输入会落入硬饱和区，某些神经元可能永远不会被激活，这个ReLU单元在训练中将不可逆转的死亡，导致相应的参数永远不能被更新，使得数据多样化丢失。这种现象被称为“神经元死亡”。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。 2）偏移现象。即输出均值恒大于零。偏移现象和Dead ReLU Problem会共同影响网络的收敛性。 尽管存在上述几个问题，ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试！ 五、Leaky ReLULeaky ReLU非线性函数的数学公式为： f(x)=max(0.01x,x)函数图像及梯度函数图像如下所示：人们为了解决Dead ReLU Problem，提出了将ReLU的前半段设为0.01x而非0。理论上来说，Leaky ReLU拥有ReLU的所有优点，外加不会有Dead ReLU problem，但是在实际操作中，并没有完全证明Leaky ReLU总是好于ReLU。有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定。 六、PReLUParametric ReLU非线性函数的数学公式为： f(x)=max(\alpha x,x)PReLU是ReLU和LReLU的改进版本，具有非饱和性。与LReLU相比，PReLU中的负半轴斜率$\alpha$由back propagation学习而非固定。原文献建议初始化$\alpha$为0.25，不采用正则。 虽然PReLU 引入了额外的参数，但基本不需要担心过拟合。例如，在cifar10+NIN实验中， PReLU比ReLU和ELU多引入了参数，但也展现了更优秀的性能。所以实验中若发现网络性能不好，建议从其他角度寻找原因。 与ReLU相比，PReLU收敛速度更快。因为PReLU的输出更接近0均值，使得SGD更接近natural gradient。证明过程参见原文[10]。 七、RReLU数学形式与PReLU类似，但RReLU[9]是一种非确定性激活函数，其参数是随机的。这种随机性类似于一种噪声，能够在一定程度上起到正则效果。作者在cifar10/100上观察到了性能提升。 综上，ReLU家族讲完了，总结如下图：其中表格为在cifar10上采用NIN网络的实验结果。 八、MaxoutMaxout[13]是ReLU的推广，其发生饱和是一个零测集事件（measure zero event）。正式定义为： max(w_1^Tx+b_1,w^T_2+b_2,···,w_n^Tx+b_n)Maxout网络能够近似任意连续函数，且Maxout是对ReLU和leaky ReLU的一般化归纳，当$w_2,b_2,···,w_n,b_n$为0时，退化为ReLU。其实，Maxout的思想在视觉领域存在已久。例如，在HOG特征里有这么一个过程：计算三个通道的梯度强度，然后在每一个像素位置上，仅取三个通道中梯度强度最大的数值，最终形成一个通道。这其实就是Maxout的一种特例。 所以Maxout神经元就拥有ReLU单元的所有优点（线性操作和不饱和，能够缓解梯度消失），而没有它的缺点（死亡的ReLU单元）。然而和ReLU对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。 九、ELUELU（Exponential Linear Units）非线性函数的数学公式为： f(x)=max(0,x)+\alpha·min(0,exp(x)-1)函数图像及梯度函数图像如下所示：ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点，并有自身的特点，罗列如下： 1）右侧线性部分使得ELU能够缓解梯度消失，而左侧软饱和能够燃ELU对输入变换或噪声更加鲁棒。 2）ELU的输出均值接近于零，即zero-centered，所以收敛速度更快。经ELU的作者实验，ELU的收敛性质的确优于ReLU和PReLU。在cifar10上，ELU 网络的loss 降低速度更快；在 ImageNet上，不加 Batch Normalization 30 层以上的 ReLU 网络会无法收敛，PReLU网络在MSRA的Fan-in （caffe ）初始化下会发散，而 ELU 网络在Fan-in/Fan-out下都能收敛 。 它的一个小问题在于计算量稍大，类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据证明ELU总是优于ReLU。 十、Noisy Activation FunctionsBengio教授在ICML2016提出了一种激活策略[1]，可用于多种软饱和激活函数，例如sigmoid和tanh。当激活函数发生饱和时，网络参数还能够在两种动力下继续更新：正则项梯度和噪声梯度。引入适当的噪声能够扩大SGD的参数搜索范围，从而有机会跳出包河区。在激活函数中引入噪声的更早工作可追溯到[5]，但文献[5]的工作并不考虑噪声引入的时间和大小。本篇的特点在于，只在饱和区引入噪声，且噪声量与饱和程度相关（原式与泰勒展开式一次项之差$\delta$）。算法1中g表示sigmoid，用于归一化$\delta$。注意，ReLU的$\delta$恒为0，无法直接加噪声，所以作者把噪声加在了输入上。 CReLUMPELU十一、小结建议用ReLU非线性函数。但是要注意初始化和learning rate的设置，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，不要再用sigmoid了。也可以试试tanh，但是其效果应该不如ReLU或者Maxout。 参考资料[1] Gulcehre, C., et al., Noisy Activation Functions, in ICML 2016. 2016.[2] Glorot, X. and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. AISTATS 2010.[3] LeCun, Y., et al., Backpropagation applied to handwritten zip code recognition. Neural computation, 1989. 1(4): p. 541-551.[4] Amari, S.-I., Natural gradient works efficiently in learning. Neural computation, 1998. 10(2): p. 251-276.[5] Nair, V. and G.E. Hinton. Rectified linear units improve Restricted Boltzmann machines. ICML 2010.[6] Glorot, X., A. Bordes, and Y. Bengio. Deep Sparse Rectifier Neural Networks.AISTATS 2011.[7] Djork-Arné Clevert, T.U., Sepp Hochreiter. Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs). ICLR 2016.[][9] Xu, B., et al. Empirical Evaluation of Rectified Activations in Convolutional Network. ICML Deep Learning Workshop 2015.[10] He, K., et al. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. ICCV 2015.[11] He, K. and J. Sun Convolutional Neural Networks at Constrained Time Cost. CVPR 2015.[12] Maas, A.L., Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural network acoustic models. in ICML 2013.[13] Goodfellow, I.J., et al. Maxout Networks. ICML 2013..]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>激活函数</tag>
        <tag>ReLU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（7）：神经网络的优化方法]]></title>
    <url>%2F2017%2F05%2F11%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%887%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、Gradient Descent [Robbins and Monro, 1951, Kiefer et al., 1952]机器学习中，梯度下降法常用来对相应的算法进行训练。常用的梯度下降法包含三种不同的形式，分别是BGD、SGD和MBGD，它们的不同之处在于我们在对目标函数进行梯度更新时所使用的样本量的多少。 以线性回归算法来对三种梯度下降法进行比较。一般线性回归函数的假设函数为： h_{\theta}=\sum_{j=0}^n{\theta_jx_j}（即有n个特征）对应的损失函数为 L\left(\theta\right)=\frac{1}{2m}\sum_{i=1}^m{\left(h\left(x_i\right)-y_i\right)^2}下图即为一个二维参数$\theta _0$和$\theta _1$组对应的损失函数可视化图像： 1.1 BGD（Batch Gradient Descent）批量梯度下降法（Batch Gradient Descent，简称BGD）是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新，其数学形式如下： 1）对上述的损失函数求偏导： \frac{\partial L\left(\theta\right)}{\partial\theta_j}=-\frac{1}{m}\sum_{i=1}^m{\left(y^{\left(i\right)}-h_{\theta}\left(x^{\left(i\right)}\right)\right)}x_{j}^{\left(i\right)} 2）由于是最小化损失函数，所以按照每个参数$\theta$的梯度负方向来更新每个$\theta$：\theta_{j}^{'}=\theta_j+\frac{1}{m}\sum_{i=1}^m{\left(y^{\left(i\right)}-h_{\theta}\left(x^{\left(i\right)}\right)\right)x_{j}^{\left(i\right)}}其伪代码如下： 123for i in range(nb_epochs): params_grad = evaluate_gradient(loss_function, data, params) params = params - learning_rate * params_grad 从上面的公式可以看到，它得到的是全局最优解，但是每迭代一步，都要用到训练集所有的数据，若样本数目$m$很大，那么迭代速度会大大降低。其优缺点如下： 优点：全局最优解；易于并行实现； 缺点：当样本量很大时，训练过程会很慢 1.2 SGD（Stochastic Gradient Descent）由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常缓慢。随机梯度下降法（Stochastic Gradient Descent，简称SGD）正是为了解决批量梯度下降法这一弊端而提出的。对每个样本的损失函数对$\theta$求偏导得到对应的梯度，来更新$\theta$： \theta_{j}^{'}=\theta_j+\left(y^{\left(i\right)}-h_{\theta}\left(x^{\left(i\right)}\right)\right)x_{j}^{\left(i\right)}具体的伪代码形式为 12345for i in range(nb_epochs): np.random.shuffle(data) for example in data: params_grad = evaluate_gradient(loss_function, example, params) params = params - learning_rate * params_grad 随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已将将$\theta$迭代到最优解了，对比上面的批量梯度下降，迭代一次不可能最优，如果迭代十次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。其优缺点如下： 优点：训练速度快； 缺点：准确度下降，并不是全局最优；不易于并行实现。 从迭代次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图表示如下： 1.3 MBGD（Mini-batch Gradient Descent）从上述的两种梯度下降法可以看出，其各自均有优缺点，那么能否在两种方法的性能之间取得一个折中呢？即，算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这正是小批量梯度下降法（Mini-batch Gradient Descent，简称MBGD）的初衷。 下面的伪代码中，我们每轮迭代的mini-batches设置为50： 12345for i in range(nb_epochs): np.random.shuffle(data) for batch in get_batches(data, batch_size=50): params_grad = evaluate_gradient(loss_function, batch, params) params = params - learning_rate * params_grad 1.4 梯度下降算法的局限虽然梯度下降算法效果很好，并且被广泛的使用，但它存在着一些需要解决的问题： 1）首先选择一个合适的学习速率很难。若学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么会阻碍收敛，即在极值点附近振荡 2）学习速率调整（又称学习速率调度，Learning rate schedules）试图在每次更新过程中，改变学习速率，如模拟退火按照预先设定的调度算法或者当相邻的迭代中目标变化小于一个阈值时候减小学习速率。但是梯度下降算法的调度和阈值需要预先设置，无法对数据集特征进行自适应。 3）模型所有的参数每次更新都是使用相同的学习速率。如果我们的数据很稀疏并且我们的特征出现的次数不同，我们可能不会希望所有的参数以某种相同的幅度进行更新，而是针对很少出现的特征进行一次大幅度更新。 4）在神经网络中常见的极小化highly non-convex error functions的一个关键挑战是避免步入大量的suboptimal local minima。Dauphin等人认为实践中的困难来自saddle points而非local minima。这些saddle points（鞍点）经常被一个相等误差的平原包围，导致SGD很难摆脱，因为梯度在所有方向都近似于0。 二、Momentum这是一种启发式算法。形式如下： v_t=\gamma v_{t-1}+\eta\nabla_{\theta}J\left(\theta\right) \theta =\theta -v_t我们用物理上的动能势能转换来理解它。即物体在这一时刻的动能=物体在上一时刻的动能+上一时刻的势能差。由于有阻力和转换时的损失，所以两者都乘以一个系数。 就像一个小球从坡上向下滚，当前的速度取决于上一时刻的速度和势能的改变量。 这样在更新参数时，除了考虑到梯度以外，还考虑了上一时刻参数的历史变更幅度。例如，参数上一次更新幅度较大，并且梯度也较大，那么在更新时是不是得更加猛烈一些了。这样的启发式算法，从直观感知上确实有道理。 下面两张图直观的展示了Momentum算法，其中绿色箭头表示上一时刻参数的变更幅度，红色箭头表示梯度，两者向量叠加即得到蓝色箭头即真实的更新幅度。 三、NAG（Nesterov accelerated gradient） [Nesterov, 1983]还是以上面小球的例子来看，momentum方式下小球完全是盲目被动的方式滚下的。这样有个缺点就是在邻近最优点附近是控制不住速度的。我们希望小球可以预判后面的“地形”，要是后面地形还是很陡峭，那就继续坚定不移地大胆走下去，不然的话就减缓速度。 当然，小球自己也不知道真正要走到哪里，这里以 \theta - \gamma v_{t-1}作为下一个位置的近似，将动量的公式更改为： v_t=\gamma v_{t-1}+\eta\nabla_{\theta}J\left(\theta - \gamma v_{t-1}\right) \theta =\theta -v_t相比于动量方式考虑的是上一时刻的动能和当前点的梯度，而NAG考虑的是上一时刻的梯度和近似下一点的梯度，这使得它可以先往前探探路，然后慎重前进。 Hinton的slides是这样给出的：其中两个blue vectors分别理解为梯度和动能，两个向量和即为momentum方式的作用结果。 而靠左边的brown vector是动能，可以看出它那条blue vector是平行的，但它预测了下一阶段的梯度是red vector，因此向量和就是green vector，即NAG方式的作用结果。 momentum项和nesterov项都是为了使梯度更新更加灵活，对不同情况有针对性。但是，人工设置一些学习率总还是有些生硬，接下来介绍几种自适应学习率的方法 四、学习率退火训练深度网络的时候，可以让学习率随着时间退火。因为如果学习率很高，系统的动能就过大，参数向量就会无规律地变动，无法稳定到损失函数更深更窄的部分去。对学习率衰减的时机把握很有技巧：如果慢慢减小，可能在很长时间内只能浪费计算资源然后看着它混沌地跳动，实际进展很少；但如果快速地减少，系统可能过快地失去能量，不能到达原本可以到达的最好位置。通常，实现学习率退火有三种方式： 1）随步数衰减：每进行几个周期就根据一些因素降低学习率。通常是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的十分之一。这些数值的设定是严重依赖具体问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。 2）指数衰减。数学公式是$\alpha=\alpha_0e^{-kt}$，其中$\alpha_0,k$是超参数，$t$是迭代次数（也可以使用周期作为单位）。 3）$1/t$衰减的数学公式是$\alpha=\alpha_0/(1+kt)$，其中$\alpha_0,k$是超参数，t是迭代次数。 在实践中，我们发现随步数衰减的随机失活（dropout）更受欢迎，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比k更有解释性。但如果你有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。 五、自适应学习率方法5.1 Adagrad [Duchi et al., 2011]之前的方法中所有参数在更新时均使用同一个Learning rate。而Learning rate调整是一个非常耗费计算资源的过程，所以如果能够自适应地对参数进行调整的话，就大大降低了成本。在Adagrad的每一个参数的每一次更新中都使用不同的learning rate。这样的话，令第$t$步更新时对第$i$个参数的梯度为 g_{t,i}=\nabla_{\theta}J\left(\theta_j\right)参数的更新的一般形式为： \theta_{t+1,i}=\theta_{t,i}-\eta g_{t,i}如上所述，Adagrad的差异之处正是在于learning rate不同于其他，将learning rate改为如下： \theta_{t+1,i}=\theta_{t,i}-\frac{\eta}{\sqrt{\sum_{i=0}^t{\left(g^i\right)^2}+\epsilon}}·g_{t,i}实质上是对学习率形成了一个约束项regularizer：$\frac{1}{\sqrt{\sum_{i=0}^t{\left(g^i\right)^2}+\epsilon}}$，${\sum_{i=0}^t{\left(g^i\right)^2}}$是对直至t次迭代的梯度平方和的累加和，$\epsilon $是一个防止分母为0的很小的平滑项。不用平方根操作，算法性能会变差很多 我们可以将到累加的梯度平方和放在一个对角矩阵中$G_t\in\mathbb{R}^{d×d}$中，其中每个对角元素$(i,i)$是参数$\theta_i$到时刻$t$为止所有时刻梯度的平方之和。由于$G_t$的对角包含着所有参数过去时刻的平方之和，我们可以通过在$G_t$和$g_t$执行element-wise matrix vector mulitiplication来向量化我们的操作： \theta_{t+1}=\theta_t-\frac{\eta}{\sqrt{G_t+\epsilon}}\odot g_t 优点：Adagrad让学习速率自适应于参数，在前期$g_t$较小的时候，regularizer较大，能够放大梯度；后期$g_t$较大的时候，regularizer较小，能够约束梯度；因为这一点，它非常适合处理稀疏数据。Dean等人发现Adagrad大大地提高了SGD的鲁棒性并在谷歌的大规模神经网络训练中采用了它进行参数更新，其中包含了在Youtube视频中进行猫脸识别。此外，由于低频词（参数）需要更大幅度的更新，Pennington等人在GloVe word embeddings的训练中也采用了Adagrad。 缺点：由公式可以看出，仍依赖于人工设置一个全局学习率；$\eta$设置过大的话，会使得regularizer过于敏感，对梯度的调节太大；中后期，分母上梯度平方的累加将会越来越大，使得梯度为0，训练提前结束。 5.2 RMSprop [Hinton]RMSprop是一个没有公开发表的适应性学习率方法，它是Hinton在他的课上提出的一种自适应学习速率方法。有趣的是，每个使用这个方法的人在他们的论文中都引用自Geoff Hinton的Coursera课程的第六课的第29页PPT。它用了一种很简单的方式修改了Adagrad方法，让它不过于激进而过早停止学习。具体说来就是，它使用了一个梯度平方的滑动平均，仍然是基于梯度的大小来对每个权重的学习率进行修改，效果不错。但是和Adagrad不同的是，其更新不会让学习率单调变小。 下图展示了RMSprop的计算过程，其中$\alpha$是一个超参数，常用的值是[0.9,0.99,0.999]： 5.3 Adadelta [Zeiler, 2012]Adadelta是Adagrad的一种扩展，以缓解Adagrad学习速率单调递减问题的算法。Adadelta不是对过去所有时刻的梯度平方进行累加，而是将累加时刻限制在窗口大小为的$w$区间。 但梯度累加没有采用简单的存储前$w$个时刻的梯度平方，而是递归地定义为过去所有时刻梯度平方的decaying average$E[g^2]_t$。$t$时刻的running average仅仅依赖于之前average和当前的梯度： E\left[g^2\right]_t=\gamma E\left[g^2\right]_{t-1}+\left(1-\gamma\right)g_{t}^{2}类似momentum term，我们将$\gamma$取值在0.9附近。简介起见，我们从参数更新向量$\Delta\theta_t$角度重写普通SGD的参数更新： \Delta\theta_t=-\eta ·g_{t,i} \theta_{t+1}=\theta_t+\Delta\theta_tAdagrad中我们推导的参数更新向量现在就以下述形式出现： \Delta \theta_t=-\frac{\eta}{\sqrt{G_t+\epsilon}}\odot g_t现在我们简单地将对角矩阵替换为过去时刻梯度平方的decaying average $E[g^2]_t$： \Delta \theta_t=-\frac{\eta}{\sqrt{E[g^2]_t+\epsilon}}\odot g_t由于分母是root mean squared (RMS) error criterion of the gradient，则上面公式可以替换为： \Delta \theta_t=-\frac{\eta}{RMS[g]_t}作者发现（和SGD，Momentum或者Adagrad一样）上述更新中的单元不匹配，即只有部分参数进行更新，也就是参数和更新应该有着相同的hypothetical units。为了实现这个目的，他们首先定义了另外一个exponentially decaying average，这一次对更新参数的平方进行操作，而不只是对梯度的平方进行操作： E[\Delta\theta^2]_t=\gamma·E[\Delta\theta^2]_t+(1-\gamma)\Delta\theta^2参数更新中的root mean squared error则为： RMS[\Delta\theta]_t=\sqrt{E[\Delta\theta^2]_t+\epsilon}将以前的更新规则中的学习速率替换为参数更新的RMS，则得到Adadelta更新规则: \Delta\theta_t=-\frac{RMS[\Delta\theta]_t}{RMS[g]_t}·g_t$$$$\theta_{t+1}=\theta_t+\Delta\theta由于Adadelta更新规则中没有了学习速率这一项，我们甚至都不用对学习速率进行设置。 5.4 Adam [Kingma and Ba, 2014]Adaptive Moment Estimation (Adam)是另外一种对每个参数进行自适应学习速率计算的方法，除了像Adadelta和RMSprop一样保存去过梯度平方和的exponentially decaying average外，Adam还保存类似momentum一样过去梯度的exponentially decaying average。它看起来像是RMSProp的动量版。 m_t = \beta_1·m_{t-1}+(1-\beta)·g_tv_t = \beta_2·v_{t-1}+(1-\beta_2)·g^2_t$m_t$和$v_t$分别是分别是梯度的一阶矩（均值）和二阶距（偏方差）的估计，由于$m_t$和$v_t$由全零的向量来初始化，Adam的作者观察到他们会被偏向0，特别是在initial time steps或decay rates很小的时候（即$\beta_1$和$\beta_2$都接近于1）,于是他们通过计算bias-corrected一阶矩和二阶矩的估计低消掉偏差。 \hat{m}=\frac{m}{1-\beta_{1}^{t}} \hat{v}=\frac{v}{1-\beta_{2}^{t}}然后使用上述项和Adadelta和RMSprop一样进行参数更新，可以得到Adam的更新规则： \theta_{t+1}=\theta_t-\frac{\eta}{\sqrt{\hat{v}+\epsilon}}\hat{m}Adam的完整更新过程如下图所示，其中它推荐默认设置$\alpha=0.001,\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-8}$，在实际操作中，推荐将Adam作为默认的算法，一般而言跑起来比RMSProp要好一些。但也可以试试SGD+Nesterov动量。 六、算法可视化下面两幅动画让我们直观感受一些优化算法的优化过程。 在第一幅动图中，我们看到他们随着时间推移在损失表面的轮廓（contours of a loss surface）的移动。注意到Adagrad、Adadelta和RMSprop几乎立刻转向正确的方向并快速收敛，但是Momentum和NAG被引导偏离了轨道。这让我们感觉就像看滚下山的小球。然而，由于NAG拥有通过远眺所提高的警惕，它能够修正他的轨迹并转向极小值。 第二幅动图中为各种算法在saddle point（鞍点）上的表现。所谓saddle point也就是某个维度是positive slope，其他维度为negative lope。前文中我们已经提及了它给SGD所带来的困难。注意到SGD、Momentum和NAG很难打破对称，虽然后两者最后还是逃离了saddle point。然而Adagrad, RMSprop, and Adadelta迅速地沿着negative slope下滑。 七、二阶方法在深度网络背景下，第二类常用的最优化方法是基于牛顿法的，其迭代如下： \displaystyle x\leftarrow x-[Hf(x)]^{-1}\nabla f(x)这里$Hf(x)$是Hessian矩阵，它是函数的二阶偏导数的平方矩阵。$\nabla f(x)$是梯度向量，这和梯度下降中一样。直观理解上，Hessian矩阵描述了损失函数的局部曲率，从而使得可以进行更高效的参数更新。具体来说，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进。需要重点注意的是，在这个公式中是没有学习率这个超参数的，这相较于一阶方法是一个巨大的优势。 然而上述更新方法很难运用到实际的深度学习应用中去，这是因为计算（以及求逆）Hessian矩阵操作非常耗费时间和空间。举例来说，假设一个有一百万个参数的神经网络，其Hessian矩阵大小就是[1,000,000 x 1,000,000]，将占用将近3,725GB的内存。这样，各种各样的拟-牛顿法就被发明出来用于近似转置Hessian矩阵。在这些方法中最流行的是L-BFGS，L-BFGS使用随时间的梯度中的信息来隐式地近似（也就是说整个矩阵是从来没有被计算的）。 然而，即使解决了存储空间的问题，L-BFGS应用的一个巨大劣势是需要对整个训练集进行计算，而整个训练集一般包含几百万的样本。和小批量随机梯度下降（mini-batch SGD）不同，让L-BFGS在小批量上运行起来是很需要技巧，同时也是研究热点。 实践时在深度学习和卷积神经网络中，使用L-BFGS之类的二阶方法并不常见。相反，基于（Nesterov的）动量更新的各种随机梯度下降方法更加常用，因为它们更加简单且容易扩展。 参考资料[1] Kiefer, J., Wolfowitz, J., et al. (1952). Stochastic estimation of the maximum of a regression function. The Annals of Mathematical Statistics, 23(3):462–466.[2] Nesterov, Y. (1983). A method of solving a convex programming problem with convergence rate o (1/k2). In Soviet Mathematics Doklady, volume 27, pages 372–376.[3] Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.[4] Hinton. Neural Networks for Machine Learning[5] Zeiler, M. D. (2012). Adadelta: An adaptive learning rate method.[6] Kingma, D. and Ba, J. (2014). Adam: A method for stochastic optimization.[7] CS231n Convolutional Neural Networks for Visual Recognition.[8] Sebastian Ruder. An overview of gradient descent optimization algorithms]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>优化方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（6）：递归神经网络]]></title>
    <url>%2F2017%2F04%2F26%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%886%EF%BC%89%EF%BC%9A%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[上一篇我们学习了循环神经网络，它可以用来处理包含序列的信息。然而，除此之外，信息往往还存在着诸如树结构、图结构等更复杂的结构。对于这种复杂的结构。循环神经网络就无能为力了。本文学习一种更为强大、复杂的神经网络：递归神经网络（Recursive Neural NetWork，RNN），以及它的训练算法BPTS（Back Propagation Through Structure）。顾名思义，递归神经网络可以处理诸如树、图这样的递归网络。 一、递归神经网络的定义因为神经网络的输入层单元个数是固定的，因此必须用循环或者递归的方式来处理长度可变的输入。循环神经网络实现了前者，通过将长度不定的输入分割为等长度的小块，然后再依次的输入到网络中，从而实现了神经网络对变长输入的处理。一个典型的例子是，当我们处理一句话的时候，我们可以把一句话看作是词组成的序列，然后，每次向循环神经网络输入一个词，如此循环直至整句话输入完毕，循环神经网络将产生对应的输出。如此，我们就能处理任意长度的句子了。如下图所示：然而，有时候把句子看作是词的序列是不够的，比如下面这句话“两个外语学院的学生”：上图显示了这句话的两个不同的语法解析树。可以看出这句话有歧义，不同的语法解析树则对应了不同的意思。一个是『两个外语学院的/学生』，也就是学生可能有许多，但他们来自于两所外语学校；另一个是『两个/外语学院的学生』，也就是只有两个学生，他们是外语学院的。为了能够让模型区分出两个不同的意思，我们的模型必须能够按照树结构去处理信息，而不是序列，这就是递归神经网络的作用。当面对按照树/图结构处理信息更有效的任务时，递归神经网络通常都会获得不错的结果。 递归神经网络可以把一个树、图结构信息编码为一个向量，也就是把信息映射到一个语义向量空间中。这个语义向量空间满足某类性质，比如语义相似的向量距离更近。也就是说，如果两句话（尽管内容不容）它的意思是相似的，那么把它们分别编码后的两个向量的距离也更近；反之，如果两句话的意思截然不同，那么编码后的距离则更远。如下图所示： 从上图我们可以看到，递归神经网络将所有的词、句都映射到一个2维向量空间中。句子“the country of my birth”和句子“the place where I was born”的意思是非常接近的，所以表示它们的两个向量在向量空间中的距离很近。另外两个词“Germany”和“France”因为表示的都是地点，它们的向量与上面两句话的向量的距离，就比另外两个表示时间的词“Monday”和“Tuesday”的向量的距离近得多。这样，通过向量的距离，就得到了一种语义的表示。 上图还显示了自然语言可组合的性质：词可以组成句、句可以组成段落、段落可以组成篇章，而更高层的语义取决于底层的语义以及它们的组合方式。递归神经网络是一种表示学习，它可以将词、句、段、篇按照他们的语义映射到同一个向量空间中，也就是把可组合（树/图结构）的信息表示为一个个有意义的向量。比如上面这个例子，递归神经网络把句子”the country of my birth”表示为二维向量[1,5]。有了这个『编码器』之后，我们就可以以这些有意义的向量为基础去完成更高级的任务（比如情感分析等）。如下图所示，递归神经网络在做情感分析时，可以比较好的处理否定句，这是胜过其他一些模型的：在上图中，蓝色表示正面评价，红色表示负面评价。每个节点是一个向量，这个向量表达了以它为根的子树的情感评价。比如”intelligent humor”是正面评价，而”care about cleverness wit or any other kind of intelligent humor”是中性评价。我们可以看到，模型能够正确的处理doesn’t的含义，将正面评价转变为负面评价。 尽管递归神经网络具有更为强大的表示能力，但是在实际应用中并不太流行。其中一个主要原因是，递归神经网络的输入是树/图结构，而这种结构需要花费很多人工去标注。想象一下，如果我们用循环神经网络处理句子，那么我们可以直接把句子作为输入。然而，如果我们用递归神经网络处理句子，我们就必须把每个句子标注为语法解析树的形式，这无疑要花费非常大的精力。很多时候，相对于递归神经网络能够带来的性能提升，这个投入是不太划算的。 二、递归神经网络的前向计算接下来，我们详细介绍一下递归神经网络是如何处理树/图结构的信息的。在这里，我们以处理树型信息为例进行介绍。 递归神经网络的输入是两个子节点（也可以是多个），输出就是将这两个子节点编码后产生的父节点，父节点的维度和每个子节点是相同的。如下图所示： $c_1$和$c_2$分别是表示两个子节点的向量，$p$是表示父节点的向量。子节点和父节点组成一个全连接神经网络，也就是子节点的每个神经元都和父节点的每个神经元两两相连。我们用矩阵$W$表示这些连接上的权重，它的维度将是$d×2d$，其中，$d$表示每个节点的维度。父节点的计算公式可以写成： p=\tan\textrm{h}\left(W\left[\begin{array}{c} c_1\\ c_2\\ \end{array}\right]+b\right)在上式中，tanh是激活函数（当然也可以用其它的激活函数），是偏置项，它也是一个维度为的向量。 然后，我们把产生的父节点的向量和其他子节点的向量再次作为网络的输入，再次产生它们的父节点。如此递归下去，直至整棵树处理完毕。最终，我们将得到根节点的向量，我们可以认为它是对整棵树的表示，这样我们就实现了把树映射为一个向量。在下图中，我们使用递归神经网络处理一棵树，最终得到的向量$p_3$，就是对整棵树的表示：举个例子，我们使用递归神将网络将”两个外语学校的学生”映射为一个向量，如下图所示：最后得到的向量$p_3$就是对整个句子”两个外语学校的学生”的表示。由于整个结构是递归的，不仅仅是根节点，事实上每个节点都是以其为根的子树的表示。比如，在左边的这棵树中，向量$p_2$是短语”外语学院的学生”的表示，而向量$p_1$是短语”外语学院的”的表示。 p=\tan\textrm{h}\left(W\left[\begin{array}{c} c_1\\ c_2\\ \end{array}\right]+b\right)该式就是递归神经网络的前向计算算法，它和全连接神经网络没有什么区别，只是在输入的过程中需要根据输入的树结构依次输入每个子节点。 需要特别注意的是，递归神经网络的权重$W$和偏置项$b$在所有节点都是共享的。 三、递归神经网络的训练递归神经网络的训练算法和循环神经网络类似，两者不同之处在于，前者需要将残差$\selta$从根节点反向传播到各个子节点，而后者是将残差$\delta$从当前时刻$t_k$反向传播到初始时刻$t_1$。 下面，我们介绍适用于递归神经网络的训练算法，也就是BPTS算法。 3.1 误差项的传递首先，我们先推导将误差从父节点传递到子节点的公式，如下图：定义$\delta_p$为误差函数E相对于父节点$p$的加权输入$net_p$的导数，即： \delta_p=\frac{\partial E}{\partial net_p}设$net_p$是父节点的加权输入，则 net_p=W\left[\begin{array}{c} c_1\\ c_2\\ \end{array}\right]+b在上述式子里，$net_p、c_1、c_2$都是向量，而$W$是矩阵。为了看清楚它们的关系，我们将其展开： \left[\begin{array}{c} net_{p1}\\ net_{p2}\\ ···\\ net_{pn}\\ \end{array}\right]=\left[\begin{matrix} w_{p1c11}& w_{p1c12}& ···& w_{p1c21}···\\ w_{p2c11}& w_{p2c12}& ···& w_{p2c21}···\\ ···& ···& ···& ···\\ w_{pnc11}& w_{pnc12}& ···& w_{pnc21}···\\ \end{matrix}\right]\left[\begin{array}{c} net_{c11}\\ net_{c12}\\ ···\\ net_{c21}\\ net_{c22}\\ ···\\ \end{array}\right]在上面的公式中，$p_i$表示父节点$p$的第i个分量；$c_{1i}$表示子节点的第i个分量；$c_{2i}$表示$c_2$子节点的第$i$个分量；$w_{p_ic_{jk} }$表子节点$c_j$的第k个分量到父节点p的第i个分量的权重。根据上面展开后的矩阵乘法形式，我们不难看出，对于子节点$c_{jk}$来说，它会影响父节点所有的分量。因此，我们求误差函数E对$c_{jk}$的导数时，必须用到全导数公式，也就是： \frac{\partial E}{\partial c_{jk}}=\sum_i{\frac{\partial E}{\partial net_{p_i}}\frac{\partial net_{p_i}}{\partial c_{jk}}}=\sum_i{\delta_{p_i}w_{p_ic_{jk}}}有了上式，我们就可以把它表示为矩阵形式，从而得到一个向量化表达： \frac{\partial E}{\partial c_j}=U_j\delta_p其中，矩阵$U_j$是从矩阵$W$中提取部分元素组成的矩阵。其单元为$u_{j_{ik}}=w_{p_k}c_{ji}$上式看上出可能有点抽象，从下图，我们可以直观的看到$U_j$到底是啥。首先我们把$W$矩阵拆分为两个矩阵$W_1$和$W_2$，如下图所示： 显然，子矩阵$W_1$和$W_2$分别对应子节点$c_1$和$c_2$的到父节点$p$权重。则矩阵$U_j$为： U_j=W_j^T也就是说，将误差项反向传递到相应子节点$c_j$的矩阵$U_j$就是其对应权重矩阵$W_j$的转置。 现在，我们设$net_{c_j}$是子节点$c_j$的加权输入，$f$是子节点$c$的激活函数，则： c_j=f(net_{c_j})这样，我们得到： \delta_{c_j}=\frac{\partial E}{\partial net_{c_j}}=\frac{\partial E}{\partial c_j}\frac{\partial c_j}{\partial net_{c_j}}=W_{j}^{T}\delta_p°f'\left(net_{c_j}\right)如果我们将不同子节点$c_j$对应的误差项$\delta_{c_j}$连接成一个向量 \delta_c=\left[\begin{array}{c} \delta_{c_1}\\ \delta_{c_2}\\ \end{array}\right]那么，上式可以写成 \delta_c=W^T\delta_p°f'\left(net_c\right)它就是将误差项从父节点传递到其子节点的公式。注意上式中的$net_c$也是将两个子结点的加权输入$net_{c_1}$和$net_{c_2}$连在一起的向量。有了传递一层的公式，我们就不难写出逐层传递的公式。上图是在树型结构中反向传递项的全景图，反复应用上式，在已知$\delta_p^{(3)}$的情况下，我们不难算出$\delta_p^{(1)}$为： \delta^{\left(2\right)}=W^T\delta_{p}^{\left(3\right)}°f'\left(net^{\left(2\right)}\right) \delta_{p}^{\left(2\right)}=\left[\delta^{\left(2\right)}\right]_p \delta^{\left(1\right)}=W^T\delta_{p}^{\left(2\right)}°f'\left(net^{\left(1\right)}\right) \delta_{p}^{\left(1\right)}=\left[\delta^{\left(1\right)}\right]_p在上面的公式中 \delta^{\left(2\right)}=\left[\begin{array}{c} \delta_{c}^{\left(2\right)}\\ \delta_{p}^{\left(2\right)}\\ \end{array}\right]$\left[\delta^{\left(2\right)}\right]_p$表示取向量$\delta^{(2)}$属于节点p的部分。 3.2 权重梯度的计算根据加权输入的计算公式： net_p^{(l)}=Wc^{(l)}+b其中，$net_p^{(l)}$表示第$l$层的父节点的加权输入，$c^{(l)}$表示第$l$层的子节点。W是权重矩阵，$b$是偏置项，将其展开可得： net_{p_j}^{l}=\sum_i{w_{ji}c_{i}^{l}}+b_j那么，我们可以求得误差函数在第$l$层对权重的梯度为： \frac{\partial E}{\partial w_{ji}^{\left(l\right)}}=\frac{\partial E}{\partial net_{p_j}^{\left(l\right)}}\frac{\partial net_{p_j}^{\left(l\right)}}{\partial w_{ji}^{\left(l\right)}}=\delta_{p_j}^{\left(l\right)}·c_{i}^{\left(l\right)}上式是针对一个权重项$w_{ji}$的公式，现在需要把它扩展为对所有的权重项的公式。我们可以把上式写成写成矩阵的形式（在下面的公式中，m=2n）: \frac{\partial E}{\partial W^{\left(l\right)}}=\delta^{\left(l\right)}·\left(c^{\left(l\right)}\right)^T这就是第$l$层权重项的梯度计算公式。我们知道，由于权重$W$是在所有层共享的，所以和循环神经网络一样，递归神经网络的最终权重梯度是各个层权重梯度之和。即： \frac{\partial E}{\partial W}=\sum_l{\frac{\partial E}{\partial W^{\left(l\right)}}}和循环神经网络一样，递归神经网络最终梯度之和是各层梯度之和。 接下来，我们求偏置项$b$的梯度计算公式。先计算误差函数对第$l$层偏置项$b^{(l)}$的梯度： \frac{\partial E}{\partial b_{j}^{\left(l\right)}}=\frac{\partial E}{\partial net_{p_j}^{\left(l\right)}}\frac{\partial net_{p_j}^{\left(l\right)}}{\partial b_{j}^{\left(l\right)}}=\delta_{p_j}^{\left(l\right)}把上式扩展为矩阵的形式： \frac{\partial E}{\partial b^{\left(l\right)}}=\delta_{p}^{\left(l\right)}最终的偏置项梯度是各个层偏置项梯度之和，即： \frac{\partial E}{\partial b}=\sum_l{\frac{\partial E}{\partial b^{\left(l\right)}}}3.3 权重更新如果使用梯度下降优化算法，那么权重更新公式为： W\gets W+\eta\frac{\partial E}{\partial W}其中，$\eta$是学习速率常数。把之前的式子代入上式，即可完成权重的更新。同理，偏置项的更新公式为： b\gets b+\eta\frac{\partial E}{\partial b}同样把之前求得式子代入上式，即可完成偏置项的更新。 这就是递归神经网络的训练算法BPTS。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>递归神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（5）：长短时记忆网络（LSTM）]]></title>
    <url>%2F2017%2F04%2F25%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%885%EF%BC%89%EF%BC%9A%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%EF%BC%88LSTM%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、长期依赖问题（Long-Term Dependencies）循环神经网络（RNN）在实际应用中很难处理长距离依赖的问题。 有的时候，我们仅仅需要知道先前的信息来完成预测任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词，比如我们预测“the clouds are in the sky”最后的词的时候，我们不需要任何其他的上下文，很显然下一个词就是sky。在这种情况下，相关的信息与需要预测的词位置之间的间隔很小，而RNN可以学会使用较近距离的信息。 但是到了一个更加复杂的场景，假设我们试着预测“I grew up in France……I speak fluent French”中最后的词，从这句话的信息来看，下一个词很有可能是一种语言的名字，但具体到是哪种语言，我们就需要在与之距离较远的“I grew up in France”中得到。这说明相关信息与当前预测位置之间的间隔就肯定变得相当的大。 不幸的是，在这个间隔不断增大时，RNN会丧失学习到连接如此远的信息的能力。 当然，在理论上，RNN绝对可以处理这样的长期依赖问题。人们可以通过调参来解决，但是在实践中，RNN肯定不能够成功学习到这些知识。Bengio, et al. (1994)等人对该问题进行了深入的研究，它们发现一些使训练RNN变得非常困难的相当根本的原因。 既然找到了问题的原因，那我们就能解决它。从问题的定位到解决，科学家们大概花了7、8年的时间。终于有一天，Hochreiter和Schmidhuber两位科学家发明出长短时记忆网络，一举解决了这个问题。 二、LSTM的核心思想Long Short Term网络，一般就叫做LSTM，是一种特殊的RNN变体，它可以学习长期依赖信息。LSTM由Hochreiter和Schmidhuber在1997年提出，并在近期被Alex Graves进行了改良和推广。在很多问题上，LSTM都取得了相当巨大的成功，并得到了广泛的使用。LSTM通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是LSTM的默认属性，而非需要付出很大的代价才能获得的能力！所有的RNN都具有一种重复神经网络模块的链式的形式。在标准的RNN中，这个重复的模块只有一个非常简单的结构，例如一个tanh层。LSTM同样是这样的结构，但是其中重复的模块拥有一个不同的结构。不同于单一神经网络层，这里有四个以非常特殊的方式进行交互的小器件。图中每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表pointwise的操作，比如向量的和，而黄色的矩阵就是学习到的神经网络层。 LSTM的关键在于细胞（Cell），水平线在细胞内贯穿运行。细胞类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在水平线上很容易保持不变。 LSTM通过精心设计“门”结构来去除或者增加信息到Cell上。门是一种让信息选择式通过的方法（过滤器）。它们包含一个sigmoid神经网络层和一个pointwise乘法操作。 Sigmoid层输出0到1之间的数值，描述每个部分有多少量可以通过。0代表“不许任何量通过”，1就指“允许任意量通过” 三、LSTM的前向计算LSTM用两个门来控制单元状态Cell的内容，一个是遗忘门（forget gate），它决定了上一时刻的单元状态$c_t-1$有多少保留到当前时刻$c_t$；另一个是输入门（input gate），他决定了当前时刻网络的输入$x_t$有多少保存到单元状态$c_t$。LSTM用输出门（output gate）来控制单元状态$c_t$有多少输出到LSTM的当前输出值$h_t$。 3.1 遗忘门我们先看一下遗忘门： f_t=\sigma(W_f·[h_{t-1,x_t}]+b_f)上式中，$W_f$是遗忘门的权重矩阵，$[h_{t-1},x_t]$表示把两个向量连接成一个更长的向量，$b_f$是遗忘门的偏置项，$\sigma$是sigmoid函数。若输入的维度是$d_x$，隐藏层的维度是$d_h$，单元状态的维度是$d_c$（通常$d_c=d_h$），则遗忘门的权重矩阵$W_f$维度是$d_c×(d_h+d_x)$。事实上，权重矩阵$W_f$都是两个矩阵拼接而成的：一个是$W_{fh}$，它对应着输入项$h_{t-1}$，其维度为$d_c×d_h$；一个是$W_{fx}$，它对应着输入项$x_t$，其维度为$d_c×d_x$。$W_f$可以写为： \left[W_f\right]\left[\begin{array}{c} h_{t-1}\\ x_t\\ \end{array}\right]=\left[\begin{matrix} W_{fh}& W_{fx}\\ \end{matrix}\right]\left[\begin{array}{c} h_{t-1}\\ x_t\\ \end{array}\right]=W_{fh}·h_{t-1}+W_{fx}x_t所以总结一下，遗忘门的作用为控制有多少上一时刻的memory cell中的信息可以累积到当前时刻的memory cell中。其数学公式可以写作： f_t = sigmoid(W_{fx}·x_t+W_{fh}·h_{t-1}+b_i)其计算图示如下： 3.2 输入门接下来看输入门： i_t=\sigma(W_i·[h_{t-1},x_t]+b_i)上式中，$W_i$是输入们的权重矩阵，$b_i$是输入门的偏置项。下图表示了输入门的计算： 接下来，我们计算用于描述当前输入的单元状态$\tilde{c}_t$，它是根据上一次的输出和本次输入来计算的： \tilde{c}_t=\tan\textrm{h}\left(W_c·\left[h_{t-1},x_t\right]+b_c\right)下图是$\tilde{c}_t$的计算：现在，我们计算当前时刻的单元状态$c_t$。它是由上一次的单元状态$c_{t-1}$按元素乘以遗忘门$f_t$，再用当前输入的单元状态$\tilde{c}_t$按元素乘以输入门$i_t$，再将两个积加和产生的： c_t=f_t°c_{t-1}+i_t°\tilde{c}_t下图是$c_t$的计算图示：这样，我们就把LSTM关于当前的记忆$\tilde{c}_t$和长期的记忆$c_{t-1}$组合在一起，形成了新的单元状态$c_t$。由于遗忘门的控制，它可以保存很久很久之前的信息，由于输入门的控制，它又可以避免当前无关紧要的内容进入记忆。 3.3 输出门下面，我们要看看输入们，它控制了长期记忆对当前输出的影响： o_t=\sigma(W_o·[h_{t-1},x_t]+b_o)下图表示输出门的计算：LSTM最终的输出，是由输出门和单元状态共同确定的： h_t=o_t°\tan\textrm{h}\left(c_t\right)下图表示LSTM最终输出的计算： 四、LSTM的训练LSTM的训练算法仍然是反向传播算法，它主要有下面三个步骤： 1）前向计算每个神经元的输出值，对于LSTM来说，即$f_t、i_t、c_t、o_t、h_t$五个向量的值。 2）反向计算每个神经元的误差项$\delta$值。与循环神经网络一样，LSTM误差项的反向传播也是包括两个方向：一个是沿着时间的反向传播，即从当前t时刻开始，计算每个时刻的误差项；一个是将误差项向上一层传播。 3）根据相应的误差项，计算每个权重的梯度。 首先，我们队推导中用到的一些公式、符号做一下必要的说明。 接下来的推导中，我们设定gate的激活函数为sigmoid函数，输出的激活函数为tanh函数。它们的导数分别为： \sigma\left(z\right)=y=\frac{1}{1+e^{-z}} \sigma '\left(z\right)=y\left(1-y\right) \tan\textrm{h}\left(z\right)=y=\frac{e^z-e^{-z}}{e^z+e^{-z}} \tan\textrm{h'}\left(z\right)=1-y^2从上面可以看出，sigmoid和tanh函数的导数都是原函数的函数。这样，我们一旦计算原函数的值，就可以用它来计算出导数的值。 LSTM需要学习的参数共有8组，分别是：遗忘门的权重矩阵$W_f$和偏置项$b_f$、输入门的权重矩阵$W_i$和偏置项$b_i$、输出门的权重矩阵$W_o$和偏置项$b_o$，以及计算单元状态的权重矩阵$W_c$和偏置项$b_c$，因为权重矩阵的两部分在反向传播中使用不同的公式，因此在后续的推导中，权重矩阵$W_f、W_i、W_c、W_o$都会被写成分开的两个矩阵：$W_{fh}、W_{fx}、W_{ih}、W_{ix}、W_{oh}、W_{ox}、W_{ch}、W_{cx}$。 我们解释一下按元素乘$o$符号。当$o$作用于两个向量时，运算如下： a°b=\left[\begin{array}{c} a_1\\ a_2\\ ···\\ a_n\\ \end{array}\right]°\left[\begin{array}{c} b_1\\ b_2\\ ···\\ b_n\\ \end{array}\right]=\left[\begin{array}{c} a_1b_1\\ a_2b_2\\ ···\\ a_nb_n\\ \end{array}\right]当$o$作用于一个向量和一个矩阵时，运算如下： a°X=\left[\begin{array}{c} a_1\\ a_2\\ ···\\ a_n\\ \end{array}\right]°\left[\begin{matrix} x_{11}& x_{12}& ···& x_{1n}\\ x_{21}& x_{22}& ···& x_{2n}\\ ···& ···& ···& ···\\ x_{n1}& x_{n2}& ···& x_{nn}\\ \end{matrix}\right]=\left[\begin{matrix} a_1x_{11}& a_1x_{12}& ···& a_{1n}x_{1n}\\ a_2x_{21}& a_2x_{22}& ···& a_2x_{2n}\\ ···& ···& ···& ···\\ a_nx_{n1}& a_nx_{n2}& ···& a_nx_{nn}\\ \end{matrix}\right]当$o$作用于两个矩阵时，两个矩阵对应位置的元素相乘。按元素乘可以再某些情况下简化矩阵和向量的运算。例如，当一个对角矩阵右乘一个矩阵时，相当于用对角矩阵的对角线组成的向量按元素乘那个矩阵：$diag[a]·X=a °X$当一个行向量右乘一个对角矩阵时，相当于这个行向量按元素乘那个矩阵对角线组成的向量： a^T·diag[b]=a°b上面这俩点，在后续推导中会多次用到。 在t时刻，LSTM的输出值为$h_t$。我们定义t时刻的误差项$\delta_t$为： \delta_t=\frac{\partial E}{\partial h_t}注意，这里假设误差项是损失函数对输出值的导数，而不是对加权输入$net^l$的导数。因为LSTM有四个加权输入，分别对应$f_t、i_t、c_t、o_t$，我们希望往上一层传递一个误差项而不是四个。但我们仍然要定义出这四个加权输入，以及他们对应的误差项。 net_{f,t}=W_f[h_{t-1},x_t]+b_f=W_{fh}h_{t-1}+W_{fx}x_t+b_fnet_{i,t}=W_i[h_{t-1},x_t]+b_i=W_{ih}h_{t-1}+W_{ix}x_t+b_inet_{\tilde{c},t}=W_c\left[h_{t-1},x_t\right]+b_c=W_{ch}h_{t-1}+W_{cx}x_t+b_cnet_{o,t}=W_o\left[h_{t-1},x_t\right]+b_o=W_{oh}h_{t-1}+W_{ox}x_t+b_o \delta_{f,t}=\frac{\partial E}{\partial net_{f,t}} \delta_{i,t}=\frac{\partial E}{\partial net_{i,t}} \delta_{\tilde{c},t}=\frac{\partial E}{\partial net_{c,t}} \delta_{o,t}=\frac{\partial E}{\partial net_{o,t}}4.1 误差项沿时间的反向传播沿时间反向传导误差项，就是要计算出$t-1$时刻的误差项$\delta_{t-1}$。 \delta_{t-1}^{T}=\frac{\partial E}{\partial h_{t-1}} =\frac{\partial E}{\partial h_t}\frac{\partial h_t}{\partial h_{t-1}} =\delta_{t}^{T}\frac{\partial h_t}{\partial h_{t-1}}我们知道，$\frac{\partial h_t}{\partial h_{t-1}}$是一个jacobian矩阵。如果隐藏层$h$的维度是N的话，那么它就是一个$N×N$矩阵。为了求出它，我们列出$h_t$的计算公式： c_t=f_t°c_{t-1}+i_t°\tilde{c}_th_t=o_t°\tan\textrm{h}\left(c_t\right)显然，$o_t、f_t、i_t、\tilde{c}_t$都是$h_{t-1}$的函数，那么，利用全导数公式可得： 4.2 将误差项传递到上一层4.3 权重梯度的计算五、LSTM的变体—GRU（Gated Recurrent Unit）前面我们讲了一种最为普通的LSTM，事实上LSTM存在很多变体，许多论文中的LSTM都或多或少的不太一样。只要遵守几个关键点，就可以根据需求设计需要的Gated RNNS。在众多的LSTM变体中，GRU也许是最成功的一种。它对LSTM做了很多简化，同时却保持着和LSTM相同的效果。因此，GRU最近变得越来越流行。 GRU对LSTM做了两个大改动： 1）将输入门、遗忘门、输出门变为两个门：更新门（Update Gate）$z_t$和重置门（Reset Gate）$r_t$。 2）将单元状态与输出合并为一个状态：$h$ GRU的前向计算公式为： z_t=\sigma(W_z·[h_{t-1},x_t])r_t=\sigma(W_r·[h_{t-1},x_t]) \tilde{h}_t=\tan\textrm{h}\left(W·\left[r_t°h_{t-1},x_t\right]\right) h=\left(1-z_t\right)°h_{t-1}+z_t°\tilde{h}_t下图是GRU的示意图：GRU的训练算法比LSTM相对也要简单一些 当然还有很多其他的变体，如 Gers &amp; Schmidhuber (2000) 提出的LSTM变体增加了“peephole connection”；另一种变体使用coupled 遗忘和输入门对遗忘和需要的信息一同做出决定。Yao, et al. (2015) 提出的Depth Gated RNN。还有用一些完全不同的观点来解决长期依赖的问题，如Koutnik, et al. (2014) 提出的Clockwork RNN。 但Greff, et al. (2015)给出了流行变体的比较，结论是它们基本上是一样的。Jozefowicz, et al. (2015) 则在超过一万种RNN架构上进行了测试，发现一些架构在某些任务上也取得了比LSTM更好的结果。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>长短时记忆网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（4）：循环神经网络（RNN）]]></title>
    <url>%2F2017%2F04%2F23%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88RNN%EF%BC%89%2F</url>
    <content type="text"><![CDATA[之前学习了全连接神经网络和卷积神经网络，以及它们的训练与应用。它们都只能单独的去处理单个的输入，且前后的输入之间毫无关系。但是，在一些任务中，我们需要更好的去处理序列的信息，即前后的输入之间存在关系。比如，在理解一整句话的过程中，孤立理解组成这句话的词是不够的，我们需要整体的处理由这些词连接起来的整个序列；当我们处理视频时，我们也不能单独地仅仅分析每一帧，而要分析这些帧连接起来的整个序列。这就引出了深度学习领域中另一类非常重要的神经网络：循环神经网络（Recurrent Neural Network）。 一、语言模型RNN是在自然语言处理领域中最先使用的，如RNN可以为语言模型来建模。那么，何为语言模型？ 自然语言从它产生开始，逐渐演变成一种上下文相关的信息表达和传递的方式，因此让计算机处理自然语言，一个基本的问题就是为自然语言这种上下文相关的特性建立数学模型。这个数学模型就是在自然语言处理中常说的统计语言模型（Statistical Language Model）。它最先由贾里尼克提出。 我们可以和电脑玩一个游戏，我们写出一个句子前面的一些词，然后，让电脑帮我们写下接下来的一个词，比如下面这句： 我昨天上学迟到了，老师批评了____。 我们给电脑展示了这句话前面这些词，然后让电脑写下接下来的一个词。在这个例子中，接下来的这个词最有可能是“我”，而不可能是“小明”，甚至是“吃饭”。语言模型的出发点很简单：一个句子是否合理，就看看它的可能性大小如何。 语言模型有很多用处，比如在语音转文本（STT）的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要语言模型来从这些候选词中选择一个最有可能的。当然，它同样也可以用在图像到文本的识别中（OCR技术）。 在使用RNN之前，语言模型主要是采用N-Gram。N可以是一个自然数，比如2或者3.它的含义是，假设一个词出现的概率只和前面N个词相关。我们以2-Gram为例。首先，对前面的一句话进行切词： 我 昨天 上学 迟到 了 ， 老师 批评 了 ____。 如果用2-Gram进行建模，那么电脑在预测时，只会看到前面的“了”，然后，电脑会在语料库中，搜索“了”后面最有可能的一个词。不管最后电脑选的是不是“我”，这个模型看起来并不是那么靠谱，因为“了”前面的一大堆实际上丝毫没起作用。如果是3-Gram模型呢，会搜索“批评了”后面最有可能的词，看齐俩感觉比2-Gram靠谱了不少，但还是远远不够的。因为这句话最关键的信息“我”，远在9个词之前！ 似乎我们可以不断提升N的值，比如4-Gram、9-Gram·······。实际上，这个想法是没有实用性的。在实际应用中最多的是N=3的三元模型，更高阶的模型就很少使用了。主要有两个原因。首先，N元模型的大小（或者说空间复杂度）几乎是N的指数函数，即$O(|V|^N)$，这里$|V|$是一种语言词典的词汇量，一般在几万到几十万个。然后，使用N元模型的速度（或者说时间复杂度）也几乎是一个指数函数，即$O(|V|^{N-1})$。因此N不能太大。当N从1到2，再从2到3时，模型的效果上升显著。而当模型从3到4时，效果的提升就不是很显著了，而资源的耗费增加却非常快，所以，除非是不惜资源为了做到极致，很少有人使用四元以上的模型。Google的罗塞塔翻译系统和语言搜索系统，使用的是四元模型，该模型存储于500台以上的Google服务器中。 RNN就解决了N-Gram的缺陷，它在理论上可以往前看（往后看）任意多个词。 二、基本循环神经网络开始前，我们先回顾一下，简单的MLP三层神经网络模型：其中x是一个向量，它表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的权重矩阵；o也是一个向量，它表示输出层的值；V是隐藏层到输出层的权重矩阵。 再看下图中一个简单的循环神经网络图，它由输入层、一个隐藏层和一个输出层组成。我们可以看到，循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵W就是隐藏层上一次的值作为这一次的输入的权重。如果我们把上面的图展开，循环神经网络也可以画成下面这个样子： 现在看起来就清楚不少了，这个网络在t时刻接收到输入$X_t$之后，隐藏层的值是$S_t$，输出值是$o_t$。关键一点是，$s_t$的值不仅仅取决于$X_t$，还取决于$S_{t-1}$。我们可以使用下面的公式来表示循环神经网络的计算方法： o_t=g(Vs_t)$$$$s_t=f(Ux_t+Ws_{t-1})式1是输出层的计算公式，输出层是一个全连接层，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的权重矩阵，g是激活函数。式2是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次的值$s_{t-1}$作为这一次的输入的权重矩阵，f是激活函数。 从上面的公式可以看出，循环层和全连接层的区别就是多了一个权重矩阵W。 若反复把式2代入带式1，我们将得到： o_t=g(Vs_t)=g(Vf(Ux_t+Ws_{t-1}))=g(Vf(Ux_t+Wf(Ux_{t-1}+Ws_{t-2 })))=g(Vf(Ux_t+Wf(Ux_{t-1}+Wf(Ux_{t-2}+Ws_{t-3 }))))从上面可以看出，循环神经网络的输出值$o_t$，是受前面历次输入值$x_{t}$、$x_{t-1}$、$x_{t-2} …$的影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。 三、双向循环神经网络对于语言模型来说，很多时候光看前面的词是不够的，比如下面这句话： 我的手机坏了，我打算____一部新手机。 可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的，但是如果我们也看到了后面的词是“一部新手机”，那么横线上的词填“买”的概率就大很多了。 而这个在单向循环神经网络是无法建模的，因此我们需要双向循环神经网络，如下图所示： 我们先考虑$y-2$的计算，从上图可以看出，双向卷积神经网络的隐藏层要保存两个值，一个A参与正向计算，另一个$A’$参与反向计算。最终的输出值$y_2$取决于$A_2$和$A_2’$，其计算方法为： y_2=g(VA_2+V'A_2')$A_2$和$A_2’$则分别计算： A_2=f(WA_1+Ux_2) A_2'=f(W'A_3'+U'x_2 )现在，我们已经可以看出一般的规律：正向计算时，隐藏层的值$s_t$与$s_{t-1}$有关；反向计算时，隐藏层的值$s_t’$与$s_{t+1}’$有关；最终的输出取决于正向和反向计算的加和。现在，我们仿照式1和式2，写出双向循环神经网络的计算方法： o_t=g(Vs_t+V's_t') s_t=f(Ux_t+Ws_{t-1 }) s_t'=f(U'x_t+W's_{t+1}')从上面三个公式我们可以看到，正向计算和反向计算不共享权重，也就是说$U$和$U’$、$W$和$W’$、$V$和$V’$都是不同的权重矩阵。 四、深度循环神经网络前面我们介绍的循环神经网络只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了深度循环神经网络。如下图所示： 我们把第$i$个隐藏层的值表示为$s_t^{(i)}、s_t^{‘(i)}$，则深度循环神经网络的计算方式可以表示为： o_t=g(V^{(i)}s_t^{(i)}+V^{'(i)}s_t^{'(i)}) s_t^{(i)}=f(U^{(i)}s_t^{i-1}+W^{(i)}) s_t^{'(i)}=f(U^{'(i)}s_t^{'(i-1)}+W^{'(i)}s_{t+1}')··· s_t^{(1)}=f(U^{(1)}x_t+W^{(1)}s_{t-1}) s_t^{'(1)}=f(U^{'(1)}x_t+W^{'(1)}s_{t+1}')五、循环神经网络的训练算法：BPTTBPTT算法是针对循环层的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤： 1）前向计算每个神经元的输出值； 2）反向计算每个神经元的误差项$\delta_j$，它是误差函数E对神经元$j$的加权输入$net_j$的偏导数； 3）计算每个权重的梯度。 4）最后再用随机梯度下降算法更新权重。 循环层如下图所示： 5.1 前向计算使用前面的式2对循环层进行前向计算： s_t=f(Ux_t+Ws_{t-1})注意，上面的$s_t、x_t、s_{t-1}$都是向量，用黑体字表示；而$U、V$是矩阵，用大写字母表示。向量的下标表示时刻，例如，$s_t$表示在$t$时刻向量$s$的值。 我们假设输入向量x的维度是$m$，输出向量的维度是$n$，则矩阵$U$的维度是$n×m$，矩阵$W$的维度是$n×n$。下面是上式展开成矩阵的样子，看起来更直观一点： \left[\begin{array}{c} s_{1}^{t}\\ s_{2}^{t}\\ ·\\ s_{n}^{t}\\ \end{array}\right]=f\left(\left[\begin{matrix} u_{11}& u_{12}& ··& u_{1m}\\ u_{21}& u_{22}& ··& u_{2m}\\ ·& ·& ·& ·\\ u_{n1}& u_{n2}& ···& u_{nm}\\ \end{matrix}\right]\left[\begin{array}{c} x_1\\ x_2\\ ···\\ x_m\\ \end{array}\right]+\left[\begin{matrix} w_{11}& w_{12}& ···& w_{1n}\\ w_{21}& w_{22}& ···& w_{2n}\\ ··& ··& ···& ··\\ w_{n1}& w_{n2}& ···& w_{nn}\\ \end{matrix}\right]\left[\begin{array}{c} s_{1}^{t-1}\\ s_{2}^{t-1}\\ ···\\ s_{n}^{t-1}\\ \end{array}\right]\right)在这里我们用手写体字母表示向量的一个元素，它的下标表示它是这个向量的第几个元素，它的上标表示第几个时刻。例如，$s_j^t$表示向量$s$的第$j$个元素在$t$时刻的值。$u_{ji}$表示输入层第$i$个神经元到循环层第$j$个神经元的权重。$w_{ji}$表示循环层第$t-1$时刻的第$i$个时刻的第$j$个神经元的权重。 5.2 误差项的计算BTPP算法将第$l$层$t$时刻的误差项$\delta _t^l$值沿两个方向传播，一个方向是其传递到上一层网络，得到$\delta _t^{l-1}$，这部分只和权重矩阵$U$有关；另一个方向是将其沿着时间线传递到初始$t_1$时刻，得到$\delta_1^l$，这部分只和权重矩阵$W$有关。 我们用向量$net_j$表示神经元在$t$时刻的加权输入，因为： net_j=Ux_t+Ws_{t-1}$$$$s_{t-1}=f(net_{t-1})因此： \frac{\partial net_t}{\partial net_{t-1}}=\frac{\partial net_t}{\partial s_{t-1}}\frac{\partial s_{t-1}}{\partial net_{t-1}}我们用$a$表示列向量，用$a^T$表示行向量。上式的第一项是向量函数对向量求导，其结果为Jacobian矩阵： \frac{\partial net_t}{\partial s_{t-1}}=\left[\begin{matrix} w_{11}& w_{12}& ···& w_{1n}\\ w_{21}& w_{22}& ···& w_{2n}\\ ···& ···& ···& ···\\ w_{n1}& w_{n2}& ···& w_{nn}\\ \end{matrix}\right]=W上式第二项也是一个jacobian矩阵： \frac{\partial s_{t-1}}{\partial net_{t-1}}=\left[\begin{matrix} \frac{\partial s_{1}^{t-1}}{\partial net_{1}^{t-1}}& \frac{\partial s_{1}^{t-1}}{\partial net_{2}^{t-1}}& ···& \frac{\partial s_{1}^{t-1}}{\partial net_{n}^{t-1}}\\ \frac{\partial s_{2}^{t-1}}{\partial net_{1}^{t-1}}& \frac{\partial s_{2}^{t-1}}{\partial net_{2}^{t-1}}& ···& \frac{\partial s_{2}^{t-1}}{\partial net_{n}^{t-1}}\\ ···& ···& ···& ···\\ \frac{\partial s_{n}^{t-1}}{\partial net_{1}^{t-1}}& \frac{\partial s_{n}^{t-1}}{\partial net_{2}^{t-1}}& ···& \frac{\partial s_{n}^{t-1}}{\partial net_{n}^{t-1}}\\ \end{matrix}\right] =\left[\begin{matrix} f'\left(net_{1}^{t-1}\right)& 0& ···& 0\\ 0& f'\left(net_{2}^{t-1}\right)& 0& 0\\ 0& 0& ···& 0\\ 0& 0& 0& f'\left(net_{n}^{t-1}\right)\\ \end{matrix}\right] =diag\left[f'\left(net_{t-1}\right)\right]最后，将俩项合在一起，可得： \frac{\partial net_t}{\partial net_{t-1}}=\frac{\partial net_t}{\partial s_{t-1}}\frac{\partial s_{t-1}}{\partial net_{t-1}}=W·diag\left[f'\left(net_{t-1}\right)\right]上式描述了将$\delta$沿时间往前传递一个时刻的规律，有了这个规律，我们就可以求得任意时刻$k$的误差项$\delta_k$： \delta_{k}^{T}=\frac{\partial E}{\partial net_k}=\frac{\partial E}{\partial net_t}·\frac{\partial net_t}{\partial net_{t-1}}·\frac{\partial net_{t-1}}{\partial net_{t-2}}···\frac{\partial net_{k+1}}{\partial net_k} =\delta_{t}^{T}Wdiag\left[f'\left(net_{t-1}\right)\right]Wdiag\left[f'\left(net_{t-2}\right)\right]···Wdiag\left[f'\left(net_k\right)\right] =\delta_{t}^{T}\prod_{i=k}^{t-1}{Wdiag\left[f'\left(net_i\right)\right]}这个就是将误差项沿着时间反向传播的算法。 循环层将误差项反向传递到上一层网络，与普通的全连接层是完全一样的，在此简要描述一下：循环曾的加权输入$net^l$与上一层的加权输入$net^{l-1}$关系如下： net^l_t=Ua_t^{l-1}+Ws_{t-1} a_t^{l-1}=f^{l-1}(net_t^{l-1})上式中$net_t^l$是第$l$层神经元的加权输入（假如第$l$是循环层）；$net_t^{l-1}$是$l-1$层神经元的加权输入；$a_t^{l-1}$是第$l-1$层神经元的输出；$f^{l-1}$是第$l-1$层的激活函数。 \frac{\partial net_{t}^{l}}{\partial net_{t}^{l-1}}=\frac{\partial net_{t}^{l}}{\partial a_{t}^{l-1}}\frac{\partial a_{t}^{l-1}}{\partial net_{t}^{l-1}}=U ·diag\left[f'^{l-1}\left(net_{t}^{l-1}\right)\right]所以 \delta_{t}^{l-1}=\frac{\partial E}{\partial net_{t}^{l-1}}=\frac{\partial E}{\partial net_{t}^{l}}\frac{\partial net_{t}^{l}}{\partial net_{t}^{l-1}} =\delta_{t}^{l}·U·diag\left[f'^{l-1}\left(net_{t}^{l-1}\right)\right]上式就是将误差项传递到上一层算法。 5.3 权重梯度的计算接下来是BPTT算法的最后一步：计算每个权重的梯度。首先我们计算误差函数$E$对权重矩阵$W$的梯度 \frac{\partial E}{\partial W} 上图展示了我们到目前为止，在前两步中已经计算得到的量，包括每个时刻$t$循环层的输出值$s_t$，以及误差项$\delta_t$。 我们只要知道了任意一个时刻的误差项$\delta_t$，以及上一个时刻循环层的输出值$s_{t-1}$，就可以按照下面的公式求出权重矩阵在$t$时刻的梯度： \nabla_{w_t}E=\left[\begin{matrix} \delta_{1}^{t}s_{1}^{t-1}& \delta_{1}^{t}s_{2}^{t-1}& ···& \delta_{1}^{t}s_{n}^{t-1}\\ \delta_{2}^{t}s_{1}^{t-1}& \delta_{2}^{t}s_{2}^{t-1}& ···& \delta_{2}^{t}s_{n}^{t-1}\\ ···& ···& ···& ···\\ \delta_{n}^{t}s_{1}^{t-1}& \delta_{n}^{t}s_{2}^{t-1}& ···& \delta_{n}^{t}s_{n}^{t-1}\\ \end{matrix}\right]上式中，$\delta_i^t$表示$t$时刻误差项向量的第$i$个分量；$s_i^{t-1}$表示$t-1$时刻循环层第$i$个神经元的输出值。 下面我们简单推导一下上式。 我们知道 net_t=Ux_t+Ws_{t-1} \left[\begin{array}{c} net_{1}^{t}\\ net_{2}^{t}\\ ·\\ net_{n}^{t}\\ \end{array}\right]=Ux_t+\left[\begin{matrix} w_{11}& w_{12}& ···& w_{1n}\\ w_{21}& w_{22}& ···& w_{2n}\\ ···& ···& ···& ···\\ w_{n1}& w_{n2}& ···& w_{nn}\\ \end{matrix}\right]\left[\begin{array}{c} s_{1}^{t-1}\\ s_{2}^{t-1}\\ ···\\ s_{n}^{t-1}\\ \end{array}\right] =Ux_t+\left[\begin{array}{c} w_{11}s_{1}^{t-1}+w_{12}s_{2}^{t-1}+···+w_{1n}s_{n}^{t-1}\\ w_{21}s_{1}^{t-1}+w_{22}s_{2}^{t-1}+···+w_{2n}s_{n}^{t-1}\\ ···\\ w_{n1}s_{1}^{t-1}+w_{n2}s_{2}^{t-1}+···+w_{nn}s_{n}^{t-1}\\ \end{array}\right]因为对$W$求导与$Ux_t$无关，我们不加考虑。现在，我们考虑对权重项$w_{ji}$求导。通过观察上式我们可以看到$w_{ji}$只与$net^t_j$有关，所以： \frac{\partial E}{\partial w_{ji}}=\frac{\partial E}{\partial net_{j}^{t}}\frac{\partial net_{j}^{t}}{\partial w_{ji}}=\delta_{j}^{t}s_{i}^{t-1}按照这个规律就可以生成梯度矩阵$\nabla_{w_t}E$了。 我们已经求得权重矩阵$W$在$t$时刻的梯度$\nabla_{w_t}E$，最终的梯度$\nabla_{w_t}E$是各个时刻的梯度之和： =\left[\begin{matrix} \delta_{1}^{t}s_{1}^{t-1}& \delta_{1}^{t}s_{2}^{t-1}& ···& \delta_{1}^{t}s_{n}^{t-1}\\ \delta_{2}^{t}s_{1}^{t-1}& \delta_{2}^{t}s_{2}^{t-1}& ···& \delta_{2}^{t}s_{n}^{t-1}\\ ···& ···& ···& ···\\ \delta_{n}^{t}s_{1}^{t-1}& \delta_{n}^{t}s_{2}^{t-1}& ···& \delta_{n}^{t}s_{n}^{t-1}\\ \end{matrix}\right]+···+\left[\begin{matrix} \delta_{1}^{1}s_{1}^{0}& \delta_{1}^{1}s_{2}^{0}& ···& \delta_{1}^{1}s_{n}^{0}\\ \delta_{2}^{1}s_{1}^{0}& \delta_{2}^{1}s_{2}^{0}& ···& \delta_{2}^{1}s_{n}^{0}\\ ···& ···& ···& ···\\ \delta_{n}^{1}s_{1}^{0}& \delta_{n}^{1}s_{2}^{0}& ···& \delta_{n}^{1}s_{n}^{0}\\ \end{matrix}\right]这就是计算循环曾权重矩阵$W$的梯度的公式。 前面介绍了权重梯度的计算方法，看上去比较直观。但为什么最终的梯度的是各个时刻的梯度之和呢？我们前面只是直接用了这个结论，实际上这里面是有道理的。 我们从这个式子开始：net_t=Ux_t+Wf(net_{t-1})因为$Ux_t$与$W$完全无关，我们把它看做常量。现在，考虑第一个式子加号右边的部分，因为$W$和$f(net_{t-1})$都是$W$的函数，所以，对其求偏导得到： \frac{\partial net_t}{\partial W}=\frac{\partial W}{\partial W}f\left(net_{t-1}\right)+W\frac{\partial f\left(net_{t-1}\right)}{\partial W}我们最终需要计算的是 \nabla_WE=\frac{\partial E}{\partial W}=\frac{\partial E}{\partial net_t}\frac{\partial net_t}{\partial W}=\delta_{t}^{T}\frac{\partial W}{\partial W}f\left(net_{t-1}\right)+\delta_{t}^{T}W\frac{\partial f\left(net_{t-1}\right)}{\partial W}我们先计算加号左边的部分。$\frac{\partial W}{\partial W}$是矩阵对矩阵求导，其结果是一个四维张量（tensor），如下所示： \frac{\partial W}{\partial W}=\left[\begin{matrix} \frac{\partial w_{11}}{\partial W}& \frac{\partial w_{12}}{\partial W}& ···& \frac{\partial w_{1n}}{\partial W}\\ \frac{\partial w_{21}}{\partial W}& \frac{\partial w_{22}}{\partial W}& ···& \frac{\partial w_{2n}}{\partial W}\\ ···& ···& ···& ···\\ \frac{\partial w_{n1}}{\partial W}& \frac{\partial w_{n2}}{\partial W}& ···& \frac{\partial w_{nn}}{\partial W}\\ \end{matrix}\right] =\left[\begin{matrix} \left[\begin{matrix} \frac{\partial w_{11}}{\partial w_{11}}& ···& \frac{\partial w_{11}}{\partial w_{1n}}\\ ···& ···& ···\\ \frac{\partial w_{11}}{\partial w_{n1}}& ···& \frac{\partial w_{11}}{\partial w_{nn}}\\ \end{matrix}\right]& ···& ···& \left[\begin{matrix} \frac{\partial w_{1n}}{\partial w_{11}}& ···& \frac{\partial w_{1n}}{\partial w_{1n}}\\ ···& ···& ···\\ \frac{\partial w_{1n}}{\partial w_{n1}}& ···& \frac{\partial w_{1n}}{\partial w_{nn}}\\ \end{matrix}\right]\\ ···& ···& ···& ···\\ ···& ···& ···& ···\\ \left[\begin{matrix} \frac{\partial w_{n1}}{\partial w_{11}}& ···& \frac{\partial w_{n1}}{\partial w_{1n}}\\ ···& ···& ···\\ \frac{\partial w_{n1}}{\partial w_{n1}}& ···& \frac{\partial w_{n1}}{\partial w_{nn}}\\ \end{matrix}\right]& ···& ···& \left[\begin{matrix} \frac{\partial w_{nn}}{\partial w_{11}}& ···& \frac{\partial w_{nn}}{\partial w_{1n}}\\ ···& ···& ···\\ \frac{\partial w_{nn}}{\partial w_{n1}}& ···& \frac{\partial w_{nn}}{\partial w_{nn}}\\ \end{matrix}\right]\\ \end{matrix}\right] =\left[\begin{matrix} \left[\begin{matrix} 1& 0& ···& 0\\ 0& 0& ···& 0\\ ···& ···& ·& ···\\ 0& 0& 0& 0\\ \end{matrix}\right]& \left[\begin{matrix} 0& 1& 0& 0\\ 0& 0& ···& 0\\ ···& ···& ···& 0\\ 0& 0& ···& 0\\ \end{matrix}\right]& ···& ···\\ ···& ···& ···& ···\\ ···& ···& ···& ···\\ ···& ···& ···& ···\\ \end{matrix}\right]接下来，我们知道$s_{t-1=f(net_{t-1})}$，它是一个列向量。我们让上面的四维张量与这个向量相乘，得到了一个三维张量，再左乘行向量$\delta_t^T$，最终得到一个矩阵： \delta_{t}^{T}\frac{\partial W}{\partial W}f\left(net_{t-1}\right)=\delta_{t}^{T}\left[\begin{matrix} \left[\begin{matrix} 1& 0& ···& 0\\ 0& 0& ···& 0\\ ···& ···& ·& ···\\ 0& 0& 0& 0\\ \end{matrix}\right]& \left[\begin{matrix} 0& 1& 0& 0\\ 0& 0& ···& 0\\ ···& ···& ···& 0\\ 0& 0& ···& 0\\ \end{matrix}\right]& ···& ···\\ ···& ···& ···& ···\\ ···& ···& ···& ···\\ ···& ···& ···& ···\\ \end{matrix}\right]\left[\begin{array}{c} s_{1}^{t-1}\\ s_{2}^{t-1}\\ ···\\ s_{n}^{t-1}\\ \end{array}\right] =\left[\begin{matrix} \delta_{1}^{t}& \delta_{2}^{t}& ···& \delta_{n}^{t}\\ \end{matrix}\right]\left[\begin{matrix} \left[\begin{array}{c} s_{1}^{t-1}\\ 0\\ ···\\ 0\\ \end{array}\right]& \left[\begin{array}{c} s_{2}^{t-1}\\ 0\\ ···\\ 0\\ \end{array}\right]& ···& ···\\ ···& ···& ···& ···\\ ···& ···& ···& ···\\ ···& ···& ···& ···\\ \end{matrix}\right] =\left[\begin{matrix} \delta_{1}^{t}s_{1}^{t-1}& \delta_{1}^{t}s_{2}^{t-1}& ···& \delta_{1}^{t}s_{n}^{t-1}\\ \delta_{2}^{t}s_{1}^{t-1}& \delta_{2}^{t}s_{2}^{t-1}& ···& \delta_{2}^{t}s_{n}^{t-1}\\ ···& ···& ···& ···\\ \delta_{n}^{t}s_{1}^{t-1}& \delta_{n}^{t}s_{2}^{t-1}& ···& \delta_{n}^{t}s_{n}^{t-1}\\ \end{matrix}\right] =\nabla_{W_t}E接下来，我们计算加号右边的部分： \delta_{t}^{T}W\frac{\partial f\left(net_{t-1}\right)}{\partial W}=\delta_{t}^{T}W\frac{\partial f\left(net_{t-1}\right)}{\partial net_{t-1}}\frac{\partial net_{t-1}}{\partial W} =\delta_{t}^{T}Wf'\left(net_{t-1}\right)\frac{\partial net_{t-1}}{\partial W} =\delta_{t}^{T}\frac{\partial net_t}{\partial net_{t-1}}\frac{\partial net_{t-1}}{\partial W} =\delta_{t-1}^{T}\frac{\partial net_{t-1}}{\partial W}我们得到了如下递推公式： \nabla_WE=\frac{\partial E}{\partial W}=\nabla_{W_t}E+\delta_{t-1}^{T}\frac{\partial net_{t-1}}{\partial W} =\nabla_{W_t}E+\nabla_{W_{t-1}}E+\delta_{t-2}^{T}\frac{\partial net_{t-2}}{\partial W} =\nabla_{W_t}E+\nabla_{W_{t-2}}E+···+\nabla_{W_1}E =\sum_{k=1}^t{\nabla_{W_k}E}与权重矩阵$W$类似，我们可以得到权重矩阵$U$的计算方法。 \nabla_{U_t}E=\left[\begin{matrix} \delta_{1}^{t}x_{1}^{t}& \delta_{1}^{t}x_{2}^{t}& ···& \delta_{1}^{t}x_{m}^{t}\\ \delta_{2}^{t}x_{1}^{t}& \delta_{2}^{t}x_{2}^{t}& ···& \delta_{2}^{t}x_{m}^{t}\\ ···& ···& ···& ···\\ \delta_{n}^{t}x_{1}^{t}& \delta_{n}^{t}x_{2}^{t}& ···& \delta_{n}^{t}x_{m}^{t}\\ \end{matrix}\right]它是误差函数在$t$时刻对权重矩阵$U$的梯度。和权重矩阵$W$一样，最终的梯度也是各个时刻的梯度之和： \nabla_UE=\sum_{i=1}^t{\nabla_{U_i}E}具体地证明与上述类似。 六、梯度爆炸与梯度消失不幸的是，实践中前面介绍的集中RNNs并不能很好地处理较长的序列。一个主要的原因是，RNN在训练中很容易发生梯度爆炸和梯度消失，这导致训练时梯度不能再较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。 为什么RNN会产生梯度爆炸和梯度消失问题呢？我们根据下式来分析。之前推导过程中得到： \delta_{k}^{T}=\delta_{t}^{T}\prod_{i=k}^{t-1}{Wdiag\left[f'\left(net_i\right)\right]} ||\delta_{k}^{T}||\le ||\delta_{t}^{T}||\prod_{i=k}^{t-1}{||W||||diag\left[f'\left(net_i\right)\right]||} \le ||\delta_{t}^{T}||\left(\beta_W\beta_f\right)^{t-k}上式的$\beta$定义为矩阵的模的上界。因为上式是一个指数函数，如果$t-k$很大的话（也就是向前看得很远的时候），会导致对应的误差项的值增长或缩小的非常快，这样就会导致相应的梯度爆炸和梯度消失问题（取决于$\beta$大于1还是小于1）。 通常来说，梯度爆炸更容易处理一些。因为梯度爆炸时，我们的程序会收到NaN的错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。 梯度消失更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题： 1）合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。 2）使用ReLu代替sigmoid和tanh作为激活函数。 3）使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrenr Unit（GRU），这是最流行的做法。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>循环神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（3）：卷积神经网络（CNN）]]></title>
    <url>%2F2017%2F04%2F21%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[本文将要介绍一种更适合图像、语音识别任务的神经网络结构——卷积神经网络(Convolutional Neural Network, CNN)。说卷积神经网络是最重要的一种神经网络也不为过，它在最近几年大放异彩，几乎所有图像、语音识别领域的重要突破都是卷积神经网络取得的。它在 2012 年崭露头角，Alex Krizhevsky 凭借它们赢得了那一年的 ImageNet 挑战赛（大体上相当于计算机视觉的年度奥林匹克），他把分类误差记录从 26% 降到了 15%，在当时震惊了世界。自那之后，大量公司开始将深度学习用作服务的核心。Facebook 将神经网络用于自动标注算法、谷歌将它用于图片搜索、亚马逊将它用于商品推荐、Pinterest 将它用于个性化主页推送、Instagram 将它用于搜索架构。打败李世石的AlphaGo也用到了这种网络。本文将详细介绍卷积神经网络的结构以及它的训练算法 一、初识卷积神经网络1.1 全连接神经网络与卷积神经网络全连接神经网络之所以不太适合图像识别任务，主要有三个方面的问题： 1）参数数量太多：考虑一个输入为1000×1000像素的图片（100万像素，现在已经不能算大图了），输入层有100万个节点。假设第一个隐藏层有100个节点（这个数量并不多），那么仅这一层就有（1000×1000+1）×100=1亿 的参数，这实在是太多了！我们看到图像只扩大一点，参数数量就会多很多，因此其扩展性很差。显而易见，这种全连接方式效率低下，大量的参数也很快会导致过拟合。 2）没有利用像素之间的位置信息。对于图像识别任务来说，每个像素和其周围像素的联系是比较紧密的，和离得很远的像素的联系就比较小了。若一个神经元和上一层所有神经元相连，那么就相当于对于一个像素来说，把图像的所有像素都等同对待，这不符合实际情况。当我们完成每个连接权值的学习之后，最终可能会发现，有大量的权值，它们的值都是很小的也就是这些连接其实都是无关紧要的。努力学习大量并不重要的权值，这样的学习必将是非常低效的。比如下面这个喵星人：我们的任务就是想识别这只猫，按照之前的做法就是把这幅图片转换成一个一维向量，然后作为神经网络的输入。对于人眼的物体识别来说，虽然对人眼识别物体的原理并没有研究明白，但绝不是通过把物体转换为一维向量再做识别的。一张图片必然存在着一定的位置关系，比如猫的鼻子下面有嘴巴、鼻子上面有眼睛，这些都是很明确的位置关系，但要是转换成了一维向量，这些位置关系就被掩盖了。 3）网络层数限制：我们知道网络层数越多，其表达能力越强，但是通过梯度下降方法训练深度全连接神经网络很困难，因为全连接神经网络的梯度很难传递超过三层。因此，我们不可能得到一个很深的全连接神经网络，也就限制了它的能力。 那卷积神经网络是怎么解决全连接神经网络的这些问题的呢？主要有以下几个方面。 1.2 激活函数——Relu最近几年卷积神经网络中，激活函数往往不选择sigmoid或者tanh函数，而是选择relu函数。relu函数的定义是： f\left(x\right)=\max\left(x,0\right)Relu函数图像如下所示： Relu函数作为激活函数，有以下几大优势： 1）速度快：采用sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大。而Relu函数其实就是一个max（x,0），整个过程的计算量节省很多。 2）减轻梯度消失问题：回忆一下计算梯度的公式$\nabla =\sigma^,\delta x$。其中$\sigma ^,$是sigmoid函数的导数。在使用反向传播算法进行梯度计算时，没经过一层sigmoid神经元，梯度就要乘上一个$\sigma ^, $。从下图可以看出，$\sigma^,$函数最大值是$\frac{1}{4}$。因此，乘一个$\sigma ^,$会导致梯度越来越小，这对于深层网络的训练是个很大的问题。而Relu函数的导数是1，不会导致梯度变小。当然，激活函数仅仅是导致梯度减小的一个因素，但无论如何在这方面Relu的表现强于sigmoid。使用Relu激活函数可以让你训练更深的网络。 3）通过对大脑的研究发现，大脑在工作的时候只有大约5%的神经元是激活的，而采用sigmoid激活函数的人工神经网络，其激活率大约是50%。有论文声称人工神经网络在15%~30%的激活率时是比较理想的。因为Relu函数在输入小于0的时候是完全不激活的，因此可以获得一个更低的激活率。 1.3 局部感受野（local receptive fields）在之前的全连接神经网络中，一个样例的输入被转换为一个一维向量。但在一个卷积网络中，把输入看作是一个按照28×28排列的正方形，或者当有颜色通道的时候，比如28x28x3，就是宽高都是28，且有3个颜色通道。比如下图就代表了一个输入 然后，我们通常把输入像素连接到一个隐藏层的神经元，但和全连接神经网络那样每个输入都连接一个隐藏层神经元不同的是，这里我们只是把输入图像进行局部的连接。如此不断地重复，构建起第一个隐藏层。注意如果我们有一个28×28的输入图像，5×5的局部感受野，那么隐藏层中就会有24×24个神经元。这是因为在抵达抵达最右边或最底部的输入图像之前，我们只能把局部感受野向右或向下移动23个神经元。 如上图所示，把图中间的那个看作是可以“滑动的窗口”，他的作用是和输入相应的“感受域”下的像素做运算得到新的值。这个运算就是“卷积”运算了。图上面有详细的运算过程。实际上就是每个相应元素的值相乘，然后把得到的都加起来。这个窗口的本质是其中的数字和一个偏置构成的，通常就把这个窗口叫做滤波器或者卷积核。上图是对于一个颜色通道的输入做卷积操作，但通常是三个颜色通道。中间那个“窗口”是可以滑动的，每次的滑动步长可以人为指定。 1.4 共享权值与偏置（Shared weights and biases）权值共享是指在一个模型的多个函数中使用相同的参数。 在传统的神经网络中，当计算一层的输出时，权值矩阵的每一个元素只使用一次， 当它乘以输入的一个元素后就再也不会用到了。而在卷积神经网络中，我们对24×24的隐藏层神经元的每一个使用相同的权重和偏置，这样可以很好地使用图像的平移不变性（例如稍稍移动一副猫的图像，它仍然是一副猫的图像）。因为这个原因，我们有时候把输入层到隐藏层的映射称为一个特征映射。把定义特征映射的权重称为共享权重，把以这种方式定义特征映射的偏置称为共享偏置。共享权值和偏置通常被称为一个卷积核或者滤波器。共享权值和偏置有一个很大的优点就是，它大大较少了参与卷积网络的参数，它的平移不变性将会使训练更快，有助于我们使用卷积层建立深度网络。 1.5 池化（pooling）在连续的卷积层之间会周期性地插入一个池化层。经过池化层前后，发生的变化如下图所示：它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。上图是一个MAX Pooling的过程，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。 二、卷积神经网络的层首先，让我们对卷积神经网络有一个感性的认识，下图就是一个卷积神经网络的示意图：如上图所示，一个神经网络由若干卷积层（CONV）、Pooling层（POOL）、全连接层（FC）组成。你可以构建各种不同的卷积神经网络，它的常用架构模式为： INPUT\rightarrow\left[\left[CONV\right]\times N\rightarrow POOL\right]\times M\rightarrow\left[FC\right]\times K也就是N个卷积层叠加，然后叠加一个Pooling层（可选），重复这个结构M次，最后叠加K个全连接层。 对于上图来说，该卷积神经网络的架构为： INPUT\rightarrow\left[\left[CONV\right]\times 1\rightarrow POOL\right]\times 2\rightarrow\left[FC\right]\times 2也就是$N=1,M=2,K=2$ 从中我们可以发现卷积神经网络和全连接神经网络的层结构有很大不同。全连接网络每层的神经元是按照一维排列的，也就是排成一条线的样子；而卷积神经网络每层的神经元是按照三维排列的，也就是排成一个长方体的样子，有宽度、高度和深度。 我们看到输入层的宽度和高度对应于输入图像的宽度和高度，而他的深度为1。接着第一个卷积层对这幅图像进行了卷积操作，得到了三个Feature Map。实际上这个卷积层包含三个Filter，也就是三套参数，每个Filter都可以把原始输入图像卷积得到一个Feature Map，三个Filter就可以得到三个Feature Map。至于一个卷积层可以有多少个Filter，那是可以自由设定的。也就是说，卷积层的Filter个数也是一个超参数。我们可以把Feature Map可以看做是通过卷积变换提取到的图像特征，三个Filter就对原始图像提取出三组不同的特征，也就是得到了三个Feature Map，也称做三个通道(channel)。 在第一个卷积层之后，Pooling层对三个Feature Map做了下采样，得到了三个更小的Feature Map。接着，是第二个卷积层，它有5个Filter。每个Fitler都把前面下采样之后的3个Feature Map卷积在一起，得到一个新的Feature Map。这样，5个Filter就得到了5个Feature Map。接着，是第二个Pooling，继续对5个Feature Map进行下采样，得到了5个更小的Feature Map。 最后两层是全连接层。第一个全连接层的每个神经元，和上一层5个Feature Map中的每个神经元相连，第二个全连接层(也就是输出层)的每个神经元，则和第一个全连接层的每个神经元相连，这样得到了整个网络的输出。 至此，我们对卷积神经网络有了最基本的感性认识。接下来，我们将介绍卷积神经网络中各种层的计算和训练。 2.1 卷积层卷积层的参数是一些可学习的滤波器集合（卷积核）构成，滤波器的宽度和高度一般不大，深度与其输入数据保持一致。见下图：在上面的过程中，原图像是32×32×3的图像，我们有一个滤波器（卷积核）为5×5×3，5×5的宽高相比起32×32来说，不怎么大，深度3和输入数据保持一致。一个卷积核在原图像上滑动，可以生成一个activation map，这里有6个不同的卷积核，得到的6个不同的activation map分别表示诸如边缘特征、形状特征等特征图，将这些activation map映射在深度方向上层叠起来就生成了输出数据。所以在用了6个过滤器（卷积层）之后，我们可以得到28×28×6的激活图。对各个activation map的直观感受可以看下图，其中每一个activation map代表着不同层次的特征。 2.1.1 卷积层输出值的计算我们使用一个简单的例子来讲述如何计算卷积，然后，抽象出卷积层的一些重要概念和计算方法。 假设有一个5×5的图像，使用一个3×3的滤波器进行卷积，想得到3×3的Feature Map，如下所示：为了清楚地描述卷积的计算过程，我们首先对图像的每个像素进行编号，用$x_{i,j}$表示图像的第$i$行第$j$列元素；对filter的每个权重进行编号，用$w_{m,n}$表示第$m$行第$n$列权重，用$w_b$表示filter的偏置项；对Feature Map的每个元素进行编号，用$a_{i,j}$表示Feature Map的第$i$行第$j$列元素；用$f$表示激活函数（此处使用Relu函数作为激活函数）。然后使用下列公式计算卷积： a_{i,j}=f\left(\sum_{m=0}^2{\sum_{n=0}^2{w_{m,n}x_{m+i,n+j}}}+w_b\right)例如，对于Feature Map的左上角元素$a_{0,0}$来说，其卷积计算方法为： a_{0,0}=f\left(\sum_{m=0}^2{\sum_{n=0}^2{w_{m,n}x_{m+0,n+0}}}+w_b\right)=Relu\left(4\right)=4按照这个公式可以依次计算出Feature Map中所有的值，下面的动画显示了整个Feature Map的计算过程：上面的计算过程中，步幅（stride）为1。当然步幅可以设为大于1的数。例如，当步幅为2时，Feature Map计算如下：这里我们可以看到，当把步幅设置为2时，Feature Map就变成了2×2了。这说明图像大小、步幅和卷积后的Feature Map大小是有关系的。我们设卷积前的图像宽度为$N$，步幅为$S$，filter的边长为$F$，卷积后Feature Map宽度为$N_f$，如图所示则它们之间的关系为： N_f=\frac{N-F}{S}+1但是这样持续卷积运算下去会出现一些问题，比如下图： 每经过一个filter，得到的激活图就会小一些，要是经过好几个，会导致最后消失殆尽。所以我们通过零填充（zero padding）的方法在原始图像周围补上几圈0，将补上的圈数设为$P$，则改写我们之前的关系式为： N_f=\frac{N+2P-F}{S}+1这里$P$乘以2是加了一圈之后两侧都加了1。例如上方那幅图中，$N=5$，$F=3$，$S=1$，我们想保持卷积前后的尺寸保持不变，即$N_f=N=5$，则零填充的$P$为： P=\frac{(N_f-1)S+F-N}{2}=\frac{(5-1)×1+3-5}{2}=1到此我们讲了深度为1的卷积层的计算方法，如果深度大于1怎么计算呢？其实也是类似的。如果卷积前的图像深度为$D$，那么相应的filter的深度也必须为$D$。我们扩展一些之前的式子，得到深度为$D$的卷积计算公式： a_{i,j}=f\left(\sum_{d=0}^{D-1}{\sum_{m=0}^{M-1}{\sum_{n=0}^{N-1}{w_{d,m,n}x_{d,m+i,n+j}}}}+w_b\right)该式中，$D$为深度；$w_{d,m,n }$表示filter的第d层第m行第n列的权重；$a_{d,i,j }$表示图像的第d层第i行第j列像素。我们前面还曾提到，每个卷积层可以有多个filterr。每个filter和原始图像进行卷积之后，都可以得到一个Feature Map。因此，卷积后Feature Map的深度和卷积层的filter个数是相同的。 下面的动画显示了包含两个filter的卷积层的计算。我们可以看到7×7×3的输入，经过两个3×3×3filter的卷积，其步幅为2，得到了3×3×2的输出，另外我们也会看到下图的Zero Padding是1，也就是在输入元素的周围补了一圈0。Zero Padding对图像边缘部分的特征提取是很有帮助的。 以上就是卷积层的计算方法。这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连（卷积计算规则），且filter的权值对于上一层所有神经元都是一样的。对于包含两个3×3×3的filter的卷积层来说，其参数数量仅有$(3×3×3+1)×2=56$个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。 2.1.2 用卷积公式来表达卷积层计算之前计算卷积层输出的式子很繁冗，最好可以简化一下 2.2 池化层（Pooling layer）Pooling层的主要作用就是通过下采样，去掉Feature Map中不重要的样本，进一步减少参数数量，降低了计算成本，而且可以控制过拟合（overfitting）。池化层并不会对Feature map的深度有影响，即还是会保持原来的深度。 Pooling的方法很多，最常用的是Max Pooling，它实际上就是在$n×n$的样本中取最大值，作为采样后的样本值。下图是2×2 Max Pooling： 此外，还有平均池化（average pooling）和L2-norm池化。平均汇聚历史上比较常用，但是现在已经很少使用了。因为实践证明，最大汇聚的效果比平均汇聚要好。池化层背后的直观推理是：一旦我们知道了原始输入中一个特定的特征，它与其他特征的相对位置就比它的绝对位置更重要。 很多人不喜欢汇聚操作，认为可以不使用它。比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃池化层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用池化层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，无池化层的结构不太可能扮演重要的角色。 2.3 归一化层在卷积神经网络的结构中，提出了很多不同类型的归一化层，有时候是为了实现在生物大脑中观测到的抑制机制。但是这些层渐渐都不再流行，因为实践证明它们的效果即使存在，也是极其有限的。 2.4 全连接层全连接层输出值的计算经网络和全连接神经网络是一样的，这里就不再赘述了。 三、卷积神经网络的训练和全连接神经网络相比，卷积神经网络的训练要复杂一些。但训练的原理是一样的：利用链式求导计算损失函数对每个权重的偏导数（梯度），然后根据梯度下降公式更新权值。训练算法依然是反向传播算法。 我们知道神经网络和反向传播那一节中介绍的反向传播算法，它的整个算法分为三个基本步骤： 1）前向计算每个神经元的输出值$a_j$（$j$表示网络的第$j$个神经元，以下同）； 2）反向计算每个神经元的误差项$\delta_j$，$\delta_j$在有的文献中也叫作敏感度（sensitivity）。它实际上是网络的损失函数$E_d$对神经元加权输出$net_j$的偏导数，即$\delta_j=\frac{\partial E_d}{\partial net_j}$； 3）计算每个神经元连接权重$w_{ij}$的梯度（$w_{ij}$表示从神经元$i$连接到神经元$j$的权重，公式为$\frac{\partial E_d}{\partial w_{ji}}=a_i\delta_j$）,其中，$a_i$表示神经元$i$的输出。 4）根据梯度下降法更新每个权重即可 对于卷积神经网络，由于涉及到局部连接、下采样等操作，影响到了第二部误差项$\delta$的具体计算方法，而权值共享影响了第三步权重$w$的梯度的计算方法。接下来，我们分别介绍卷积层和池化层的训练算法。 3.1 卷积层的训练3.2 Pooling层的训练]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（18）：方差偏差权衡（Bias-Variance Tradeoff）]]></title>
    <url>%2F2017%2F04%2F19%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8818%EF%BC%89%EF%BC%9A%E6%96%B9%E5%B7%AE%E5%81%8F%E5%B7%AE%E6%9D%83%E8%A1%A1%EF%BC%88Bias-Variance%20Tradeoff%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、定义1.1 感性解释Bias和Variance是针对Generalization（泛化、一般化）来说的。在机器学习中，我们用训练数据集学习一个模型，我们通常会定义一个损失函数（Loss Function），然后将这个Loss（或者叫error）的最小化过程，来提高模型的性能（performance）。然而我们学习一个模型的目的是为了解决实际的问题（即将训练出来的模型运用于预测集），单纯地将训练数据集的Loss最小化，并不能保证解决更一般的问题时模型仍然是最优的，甚至不能保证模型是可用的。这个训练数据集的Loss与一般化的数据集（预测数据集）的Loss之间的差异就叫做Generalization error。 而Generalization error又可以细分为Random Error、Bias和Variance三个部分。 首先需要说的是随机误差。它是数据本身的噪声带来的，这种误差是不可避免的。其次如果我们能够获得所有可能的数据集合，并在这个数据集合上将Loss最小化，这样学习到的模型就可以称之为“真实模型”，当然，我们是无论如何都不能获得并训练所有可能的数据的，所以真实模型一定存在，但无法获得，我们的最终目标就是去学习一个模型使其更加接近这个真实模型。 Bias和Variance分别从两个方面来描述了我们学习到的模型与真实模型之间的差距（除去随机误差）。 Bias描述的是对于测试数据集，“用所有可能的训练数据集训练出的所有模型的输出预测结果的期望”与“真实模型”的输出值（样本真实结果）之间的差异。简单讲，就是在样本上拟合的好不好。要想在bias上表现好，low bias，就是复杂化模型，增加模型的参数，但这样容易过拟合 (overfitting)。 Variance则是“不同的训练数据集训练出的模型”的输出值之间的差异。 在一个实际系统中，Bias与Variance往往是不能兼得的。如果要降低模型的Bias，就一定程度上会提高模型的Variance，反之亦然。造成这种现象的根本原因是，我们总是希望试图用有限训练样本去估计无限的真实数据。当我们更加相信这些数据的真实性，而忽视对模型的先验知识，就会尽量保证模型在训练样本上的准确度，这样可以减少模型的Bias。但是，这样学习到的模型，很可能会失去一定的泛化能力，从而造成过拟合，降低模型在真实数据上的表现，增加模型的不确定性。相反，如果更加相信我们对于模型的先验知识，在学习模型的过程中对模型增加更多的限制，就可以降低模型的variance，提高模型的稳定性，但也会使模型的Bias增大。Bias与Variance两者之间的trade-off是机器学习的基本主题之一，机会可以在各种机器模型中发现它的影子。 1.2 图示解释下图将机器学习任务描述为一个打靶的活动：根据相同算法、不同训练数据集训练出的模型，对同一个样本进行预测；每个模型作出的预测相当于是一次打靶。 左上角的示例是理想状况：偏差和方差都非常小。如果有无穷的训练数据，以及完美的模型算法，我们是有办法达成这样的情况的。然而，现实中的工程问题，通常数据量是有限的，而模型也是不完美的。因此，这只是一个理想状况。 右上角的示例表示偏差小而方差大。靶纸上的落点都集中分布在红心周围，它们的期望落在红心之内，因此偏差较小。另一方面，落点虽然集中在红心周围，但是比较分散，这是方差大的表现。 左下角的示例表示偏差大而方差小。显而易见，靶纸上的落点非常集中，说明方差小。但是落点集中的位置距离红心很远，这是偏差大的表现。 右下角的示例则是最糟糕的情况，偏差和方差都非常大。这是我们最不希望看到的结果。 再看一个来自PRML的例子：这是一个曲线拟合的问题，对同分布的不同数据集进行了多次的曲线拟合，左边表示方差（variance），右边表示偏差（bias），绿色是真实值函数。$In \lambda$表示的是模型的复杂度，这个值越小，表示模型的复杂程度越高，在第一行，大家的复杂度都很低的时候，方差是很小的，但是偏差很大；但是到了最后一幅图，我们可以得到，每个人的复杂程度都很高的情况下，不同的函数就有着天壤之别了，所以方差就很大，但此时偏差就很小了。 1.3 数学解释排除人为的失误，人们一般会遇到三种误差来源：随机误差、偏差和方差。 首先需要说明的是随机误差。随机误差是数据本身的噪声带来的，这种误差是不可避免的。一般认为随机误差服从高斯分布，记作$\varepsilon ~N\left(0,\sigma_{\varepsilon}\right)$。因此，若有变量$y$作为预测值，以及$X$作为自变量（协变量），那么我们将数据背后的真实规律$f$记作 y=f(X)+\epsilon偏差和方差则需要在统计上做对应的定义。 偏差（Bias）描述的是通过学习拟合出来的结果的期望，与真实结果之间的差距，记作Bias\left(X\right)=E\left[\hat{f}\left(X\right)\right]-f\left(X\right) 方差（Variance）即为统计学中的定义，描述的是通过学习拟合出来的结果自身的不稳定性，记作E\left[\left(\hat{f}\left(X\right)-E\left[\hat{f}\left(X\right)\right]\right)\right]^2 以均方误差为例，有如下推论： Err\left(X\right)=E\left[\left(y-\hat{f}\left(X\right)\right)^2\right]\\ =E\left[\left(f\left(X\right)+\varepsilon -\hat{f}\left(X\right)\right)^2\right] =\left(E\left[\hat{f}\left(X\right)\right]-f\left(X\right)\right)^2+E\left[\left(\hat{f}\left(X\right)-E\left[\hat{f}\left(X\right)\right]\right)\right]^2+\sigma_{\varepsilon}^{2} =Bias^2+Variance+Random\ Error二、如何Tradeoff2.1 最佳平衡点假设我们现在有一组训练数据，需要训练一个模型（基于梯度的学习）。在训练的起始，Bias很大，因为我们的模型还没有来得及开始学习，也就是与“真实模型”差距很大。然而此时variance却很小，因为训练数据集（training data）还没有来得及对模型产生影响，所以此时将模型应用于“不同的”训练数据集也不会有太大的差异。 而随着训练过程的进行，Bias变小了，因为我们的模型变得“聪明”了，懂得了更多关于“真实模型”的信息，输出值与真实值之间更加接近了。但是如果我们训练得太久了，variance就会变得很大，因为我们除了学习到关于真实模型的信息，还学到了许多具体的，只针对我们使用的训练集（真实数据的子集）的信息。而不同的可能的训练数据集（真实数据的子集）之间的某些特征和噪声是不一致的，这就导致了了我们在很多其他的数据集上就无法获得很好地效果，也就是所谓的Overfitting（过拟合）。 考虑到模型误差是偏差与方差的加和，因此我们可以绘制出这样的图像。 图中的最优位置，实际上是Total Error曲线的拐点。我们知道，连续函数的拐点意味着此处一阶导数的值为0。即 \frac{d\left(Total\ Error\right)}{d\left(Complexity\right)}=\frac{d\left(Bias+Variance\right)}{d\left(Complexity\right)}=\frac{d\left(Bias\right)}{d\left(Complexity\right)}+\frac{d\left(Variance\right)}{d\left(Complexity\right)}=0这个公式给出了寻找最优平衡点的数学描述。若模型复杂度小于平衡点，则模型的偏差会偏高，模型倾向于欠拟合；若模型复杂度大于平衡点，则模型的方差会偏高，模型倾向于过拟合。 3.2 过拟合与欠拟合的外在表现尽管有了上述的数学表述，但是在现实环境中，有时候我们很难计算模型的偏差与方差。因此，我们需要通过外在表现，判断模型的拟合状态：是欠拟合还是过拟合。 同样地，在有限的训练数据集中，不断增加模型的复杂度，意味着模型会尽可能多地降低在训练集上的误差。因此在训练集上，不断地增加模型的复杂度，训练集上的误差会一直下降。 我们把数据分为三个部分：训练数据集、验证数据集、测试数据集。 因此，我们可以绘制出这样的图像。在上图左边区域，训练集与验证集的误差都很高，这块区域的偏差比较高。在右边区域，在验证集上误差很高，但是在训练集上偏差很低，这块区域的方差比较高。我们希望在中间的区域得到一个最优平衡点。 所以，偏差较高（欠拟合）有以下两个特征： 1）训练集误差很高 2）验证集误差和训练集误差差不多大 方差较高（过拟合） 1）训练集误差较低 2）非常高的验证集误差 3.3 如何处理欠拟合与过拟合有了以上的分析，我们就能比较容易地判断模型所处的拟合状态。接下来，我们可以参考Ng提供的处理模型欠拟合与过拟合的一般方法了。 当模型处于欠拟合状态时，根本的办法是增加模型的复杂度。我们一般有以下一些办法： 1）增加模型迭代次数； 2）训练一个复杂度更高的模型：比如在神经网络中增加神经网络层数、在SVM中用非线性SVM（核技术）代替线性SVM 3）获取更多的特征以供训练使用：特征少，对模型信息的刻画就不足够了 4）降低正则化权重：正则化正是为了限制模型的灵活度（复杂度）而设定的，降低其权值可以在模型训练中增加模型复杂度。 当模型处于过拟合状态时，根本的办法是降低模型的复杂度。我们一般有以下一些办法： 1）获取更多的数据：训练数据集和验证数据集是随机选取的，它们有不同的特征，以致在验证数据集上误差很高。更多的数据可以减小这种随机性的影响。 2）减少特征数量 3）增加正则化权重：方差很高时，模型对训练集的拟合很好。实际上，模型很有可能拟合了训练数据集的噪声，拿到验证集上拟合效果就不好了。我们可以增加正则化权重，减小模型的复杂度。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Tradeoff</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（17）：非平衡数据处理]]></title>
    <url>%2F2017%2F04%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8817%EF%BC%89%EF%BC%9A%E9%9D%9E%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[一、Introduction常用的分类算法一般假设不同类的比例是均衡的，现实生活中经常遇到不平衡的数据集，比如广告点击预测（点击转化率一般都很小）、商品推荐（推荐的商品被购买的比例很低）、信用卡欺诈检测等等。 对于不平衡数据集，一般的分类算法都倾向于将样本划分到多数类，体现在模型整体的准确率很高。 但对于极不均衡的分类问题，比如仅有1%的人是坏人，99%的人是好人，最简单的分类模型就是将所有人都划分为好人，模型都能得到99%的准确率，显然这样的模型并没有提供任何的信息。 在类别不平衡的情况下，对模型使用F值或者AUC值是更好的选择。 处理不平衡数据，可以从两方面考虑：一是改变数据分布，从数据层面使得类别更为平衡； 二是改变分类算法，在传统分类算法的基础上对不同类别采取不同的加权方式，使得模型更看重少数类。 本部分对数据层面的一些方法做一个介绍，改变数据分布的方法主要是重采样： 1）过采样：增加少数类样本的数量 2）欠采样：减少多数类样本的数量 3）综合采样：将过采样和欠采样结合 二、过采样2.1 随机过采样采样算法通过某一种策略改变样本的类别分布，以达到将不平衡分布的样本转化为相对平衡分布的样本的目的，而随机采样是采样算法中最简单也最直观易懂的一种方法。 随机过抽样是增加少数类样本数量，可以事先设置多数类与少数类最终的数量比例，在保留多数类样本不变的情况下，根据比例随机复制少数类样本，在使用的过程中为了保证所有的少数类样本信息都会被包含，可以先完全复制一份全量的少数类样本，再随机复制少数样本使得满足数量比例，具体步骤如下： 1.首先在少数类$S_{min}$集合中随机选中一些少数类样本 2.然后通过复制所选样本生成样本集合$E$ 3.将它们添加到$S_{min}$中来扩大原始数据集从而得到新的少数类集合$S_{min-new}$ $S_{min}$中的总样本数增加了 $|E| $个新样本，且$S_{min-new}$ 的类分布均衡度进行了相应的调整，如此操作可以改变类分布平衡度从而达到所需水平。 重复样本过多，容易造成分类器的过拟合 2.2 SMOTE算法(Synthetic Minority Oversampling Technique)在合成抽样技术方面，Chawla NY等人提出的SMOTE过抽样技术是基于随机过采样算法的一种改进方案，由于随机过采样简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题，即使模型学习到的信息过于特别（Specific）而不够泛化(General)。 SMOTE的主要思想是利用特征空间中现存少数类样本之间的相似性来建立人工数据，特别是，对于子集$S_{min}$ $\subset$ $S$，对于每一个样本$x_i\subset S_{min}$使用K-近邻法，其中K-近邻被定义为考虑$S_{min}$中的K个元素本身与$x_i$的欧氏距离在n维特征空间X中表现为最小幅度值的样本。由于不是简单地复制少数类样本，因此可以在一定程度上避免分类器的过度拟合，实践证明此方法可以提高分类器的性能。但是由于对每个少数类样本都生成新样本，因此容易发生生成样本重叠（overlapping）的问题。算法流程如下： 1）对于少数类中的每一个样本$(x_i)$，以欧氏距离为标准计算它到少数类样本集$S_{min}$中所有样本的距离，得到K近邻； 2）根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本$x_i$，从其K近邻中随机选择若干个样本，假设选择的近邻为$\tilde{x}$； 3）对于每一个随机选出的近邻$\tilde{x}$，分别与原样本按照如下的公式构建新的样本:$x_{new}=x+rand\left(0,1\right)\times\left(\tilde{x}-x\right)$ 2.3 Borderline-SMOTE算法原始的SMOTE算法对所有的少数类样本都是一视同仁的，但实际建模过程中发现那些处于边界位置的样本更容易被错分，因此利用边界位置的样本信息产生新样本可以给模型带来更大的提升。Borderline-SMOTE便是将原始SMOTE算法和边界信息算法结合的算法。算法流程如下： 1.首先，对于每个$x_{i}\subset S_{min}$确定一系列K-近邻样本集，称该数据集为$S_{i-kNN}$，且$S_{i-kNN}\subset S$； 2.然后，对每个样本$x_{i}$，判断出最近邻样本集中属于多数类样本的个数，即：|$S_{i-kNN}\cap S_{maj}$|； 3.最后，选择满足下面不等式的$x_{i}$:$\frac{k}{2}$&lt;|$S_{i-kNN} \cap S_{maj}$|&lt;$k$,将其加入危险集$DANGER$， 对危险集中的每一个样本点（最容易被错分的样本），采用普通的$SMOTE$算法生成新的少数类样本。 三、欠采样3.1 随机欠采样减少多数类样本数量最简单的方法便是随机剔除多数类样本，可以事先设置多数类与少数类最终的数量比例，在保留少数类样本不变的情况下，根据比例随机选择多数类样本。 1）首先我们从$S_{maj}$中随机选取一些多数类样本$E$ 2）将这些样本从$S_{maj}$中移除，就有|$S_{maj-new}|=|S_{maj}-|E$| 优点在于操作简单，只依赖于样本分布，不依赖任何距离信息，属于非启发式方法；缺点在于会丢失一部分多数类样本的信息，无法充分利用已有信息。 3.2 Tomek Links方法定义：Tomek links被定义为相反类最近邻样本之间的一对连接。 符号约定：给定一个样本对$\left(x_i,x_j\right)$，其中$x_{i}$ $\in$ $S_{maj}$，$x_{j}$ $\in$ $S_{min}$，记$d\left(x_i,x_j\right)$是样本$x_i$和$x_j$之间的距离 公式表示：如果不存在任何样本$x_k$，使得$d\left( x_i,x_k \right)$ &lt;$d\left( x_i,x_j \right)$ ，那么样本对$\left(x_i,x_j\right)$被称为Tomek Links 使用这种方法，如果两个样本来自Tomek Links，那么他们中的一个样本要么是噪声要么它们都在两类的边界上。所以Tomek Links一般有两种用途：在欠采样中：将Tomek Links中属于是多数类的样本剔除；在数据清洗中，将Tomek Links中的两个样本都剔除。 3.3 NearMiss方法NearMiss方法是利用距离远近剔除多数类样本的一类方法，实际操作中也是借助KNN，总结起来有以下几类： 1）NearMiss-1：在多数类样本中选择与最近的三个少数类样本的平均距离最小的样本 2）NearMiss-2：在多数类样本中选择与最远的3个少数类样本的平均距离最小的样本 3）NearMiss-3：对于每个少数类样本，选择离它最近的给定数量的多数类样本 NearMiss-1和NearMiss-2方法的描述仅有一字之差，但其含义是完全不同的：NearMiss-1考虑的是与最近的3个少数类样本的平均距离，是局部的；NearMiss-2考虑的是与最远的3个少数类样本的平均距离，是全局的。 NearMiss-1方法得到的多数类样本分布也是”不均衡“的，它倾向于在比较集中的少数类附近找到更多的多数类样本，而在孤立的（或者说是离群的）少数类附近找到更少的多数类样本，原因是NearMiss-1方法考虑的局部性质和平均距离。 NearMiss-3方法则会使得每一个少数类样本附近都有足够多的多数类样本，显然这会使得模型的精确度高、召回率低。 实验结果表明得到NearMiss-2的不均衡分类性能最优。 四、Informed UnderstandingInformed欠抽样算法可以解决传统随机欠采样造成的数据信息丢失问题，且表现出较好的不均衡数据分类性能。其中有一些集成（ensemble）的想法，主要有两种方法，分别是EasyEnsemble算法和BalanceCascade算法。 4.1 EasyEnsemble算法它把数据划分为两部分，分别是多数类样本和少数类样本，对于多数类样本$S_{maj}$，通过$n$次有放回抽样生成$n$份子集，少数类样本$S_{min}$分别和这$n$份样本合并训练AdaBoost分类器，这样可以得到$n$个模型，最终的模型采用加权多数表决的方法，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率小的弱分类器的权值，使其在表决中起较小的作用。这里假设多数类样本为N，少数类样本为P，算法流程如下： EasyEnsemble的想法是多次随机欠抽样，尽可能全面地涵盖所有信息，算法特点是利用boosting减小偏差（Adaboost）、bagging减小方差（集成分类器）。实际应用的时候也可以尝试选用不同的分类器来提高分类的效果。 4.2 BalanceCascade算法EasyEnsemble算法训练的子过程是独立的，BalanceCascade则是一种级联算法，这种级联的思想在图像识别中用途非常广泛。算法流程如下： BalanceCascade算法得到的是一个级联分类器，将若干个强分类器由简单到复杂排列，只有和少数类样本特征比较接近的才有可能输入到后面的分类器，比如边界点，因此能更充分地利用多数类样本的信息，一定程度上解决随机欠采样的信息丢失问题。 五、综合采样目前为止我们使用的重采样方法几乎都是只针对某一类样本：对多数类样本欠采样，对少数类样本过采样。也有人提出将欠采样和过采样综合的方法，解决样本类别分布不平衡和过拟合问题，本部分介绍其中的SMOTE+Tomek Links和SMOTE+ENN。 5.1 SMOTE+Tomek LinksSMOTE+Tomek Links方法的算法流程非常简单： 1.利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T 2.剔除T中的Tomek Links对 普通的SMOTE方法生成的少数类样本是通过线性插值得到的，在平衡类别分布的同时也扩张了少数类的样本空间，产生的问题是可能原本属于多数类样本的空间被少数类“入侵”，容易造成模型的过拟合。 Tomek Links对寻找的是那种噪声点或者边界点，可以很好地解决“入侵”的问题，下图红色加号为SMOTE产生的少数类样本，可以看到，红色样本“入侵”到原本属于多数类样本的空间，这种噪声数据问题可以通过Tomek Links很好地解决。 由于第一步SMOTE方法已经很好地平衡了类别分布，因此在使用Tomek Links对的时候考虑剔除所有的Tomek Links对。 5.2 SMOTE+ENNSMOTE+ENN方法和SMOTE+Tomek Links方法的想法和过程都是很类似的： 1）利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T 2）对T中的每一个样本使用KNN（一般K取3）方法预测，若预测结果与实际类别标签不符，则剔除该样本。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>非平衡数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（2）：神经网络MNIST实战]]></title>
    <url>%2F2017%2F04%2F14%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CMNIST%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[近似非常不错 一、数据集与任务介绍MNIST数据集是一个基本的手写字体识别数据集，该数据原本是包含60000个训练图像和10000个测试图像，但这里我们事先对数据进行了划分，从训练样本中抽取10000个数据作为验证集，所以处理后的数据集包含50000个训练样本（training data）、10000个验证样本（validation data）10000个测试样本（test data），都是28乘以28的分辨率。 我们可以先将数据集从GitHub上Clone下来： 1➜ fig git:(master) ✗ git clone https://github.com/lisa-lab/DeepLearningTutorials 可以从这个链接了解对该数据集的加载和处理。 这里任务就是构建神经网络来实现对于MNIST数据集的手写字体识别分类。从任务和输入就能够得到大概的网络结构： 损失函数为平方误差损失函数，激活函数为sigmoid函数。 二、读取数据读取数据由mnist_loader.py这个文件实现。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# -*- coding: utf-8 -*-"""mnist_loader~~~~~~~~~~~~A library to load the MNIST image data. For details of the datastructures that are returned, see the doc strings for ``load_data``and ``load_data_wrapper``. In practice, ``load_data_wrapper`` is thefunction usually called by our neural network code."""#### Libraries# Standard libraryimport cPickleimport gzip# Third-party librariesimport numpy as np#从数据集中载入数据def load_data(): f = gzip.open('../data/mnist.pkl.gz', 'rb') training_data, validation_data, test_data = cPickle.load(f) f.close() return (training_data, validation_data, test_data)#改变数据集的格式def load_data_wrapper(): tr_d, va_d, te_d = load_data() #训练集 training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] training_results = [vectorized_result(y) for y in tr_d[1]] training_data = zip(training_inputs, training_results) #验证集 validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] validation_data = zip(validation_inputs, va_d[1]) #测试集~~~~ test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] test_data = zip(test_inputs, te_d[1]) return (training_data, validation_data, test_data)def vectorized_result(j): """Return a 10-dimensional unit vector with a 1.0 in the jth position and zeroes elsewhere. This is used to convert a digit (0...9) into a corresponding desired output from the neural network.""" e = np.zeros((10, 1)) e[j] = 1.0 return e 2.1 load_data函数12345def load_data(): f = gzip.open('../data/mnist.pkl.gz', 'rb') training_data, validation_data, test_data = cPickle.load(f) f.close() return (training_data, validation_data, test_data) load_data()函数的主要作用就是解压数据集，然后从数据集中把数据取出来。取出来之后的几个变量代表的数据的格式分别如下： training_data：是一个由两个元素构成的元组。其中一个元素是测试图片集合，是一个50000✖️784的numpy ndarray（其中50000行就是样本个数，784列就是一个维度，即一个像素）；第二个元素就是一个测试图片的标签集，是一个50000✖️1的Numpy ndarray，其中指明了每一个样本是什么数字，通俗来说就是这个样子：validation_data 和 test_data 的结构和上面的training_data是一样的，只是数量不一样，这两个是10000行。 2.2 load_data_wrapper()函数1234567891011121314def load_data_wrapper(): tr_d, va_d, te_d = load_data() #训练集 training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] training_results = [vectorized_result(y) for y in tr_d[1]] training_data = zip(training_inputs, training_results) #验证集 validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] validation_data = zip(validation_inputs, va_d[1]) #测试集~~~~ test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] test_data = zip(test_inputs, te_d[1]) return (training_data, validation_data, test_data) 之前的load_data返回的格式虽然很漂亮，但是并不是非常适合我们这里计划的神经网络的结构，因此我们在load_data的基础上使用load_data_wrapper（）函数来进行一点点适当的数据集变换，使得数据集更加适合我们的神经网络训练。 以训练集的变换为例。对于training_inputs来说，就是把之前的返回的training_data[0]，即第一个元素的所有样例都放到一个列表中，简单的来说如下所示： 同样可以知道training_labels的样子为： 然后training_data为zip函数组合，那么training_data为一个列表，其中每个元素是一个元组，二元组又有一个training_inputs和一个training_labels的元素组合而成，如下图： 同理可以推出其他数据的形状。 三、神经网络代码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143# -*- coding: utf-8 -*-import randomimport numpy as npclass Network(object): #初始化神经网络 def __init__(self, sizes): """The list ``sizes`` contains the number of neurons in the respective layers of the network. For example, if the list was [2, 3, 1] then it would be a three-layer network, with the first layer containing 2 neurons, the second layer 3 neurons, and the third layer 1 neuron. The biases and weights for the network are initialized randomly, using a Gaussian distribution with mean 0, and variance 1. Note that the first layer is assumed to be an input layer, and by convention we won't set any biases for those neurons, since biases are only ever used in computing the outputs from later layers.""" self.num_layers = len(sizes)#神经网络层数 self.sizes = sizes#储存各层神经元个数的列表 self.biases = [np.random.randn(y, 1) for y in sizes[1:]]#随机初始化偏置 self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]#随机初始化权重 #前向传播算法 def feedforward(self, a): """Return the output of the network if ``a`` is input.""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a #随机梯度下降（训练数据，迭代次数，小样本数量，学习率，是否有测试集（默认为无）） def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None): """Train the neural network using mini-batch stochastic gradient descent. The ``training_data`` is a list of tuples ``(x, y)`` representing the training inputs and the desired outputs. The other non-optional parameters are self-explanatory. If ``test_data`` is provided then the network will be evaluated against the test data after each epoch, and partial progress printed out. This is useful for tracking progress, but slows things down substantially.""" if test_data: n_test = len(test_data)#若有测试集，则计算其大小 n = len(training_data)#训练集大小 #迭代过程 for j in xrange(epochs): # shuffle() 方法对训练集随机排序 random.shuffle(training_data) # mini_batch是列表中切割之后的列表 mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, eta) if test_data: print "Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;".format(j, self.evaluate(test_data), n_test) else: print "Epoch &#123;0&#125; complete".format(j) def update_mini_batch(self, mini_batch, eta): """Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta`` is the learning rate.""" #存储C对于各个参数的偏导，格式和self.biases和self.weights是一模一样的 nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] #mini_batch中的一个实例调用梯度下降得到各个参数的偏导 for x, y in mini_batch: 从一个实例得到的梯度 delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] #每一个mini_batch更新一下参数 self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] #反向传播（对于每一个实例） def backprop(self, x, y): """Return a tuple ``(nabla_b, nabla_w)`` representing the gradient for the cost function C_x. ``nabla_b`` and ``nabla_w`` are layer-by-layer lists of numpy arrays, similar to ``self.biases`` and ``self.weights``.""" # 存储C对于各个参数的偏导，格式和self.biases和self.weights是一模一样的 nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] # 前向过程 activation = x #存储所有的激活值，一层一层的形式 activations = [x] # list to store all the activations, layer by layer #存储所有的中间值（weighted sum） zs = [] # list to store all the z vectors, layer by layer for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # 反向过程 # 输出层error delta = self.cost_derivative(activations[-1], y) * \ sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Note that the variable l in the loop below is used a little # differently to the notation in Chapter 2 of the book. Here, # l = 1 means the last layer of neurons, l = 2 is the # second-last layer, and so on. It's a renumbering of the # scheme in the book, used here to take advantage of the fact # that Python can use negative indices in lists. # 非输出层 for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def evaluate(self, test_data): """Return the number of test inputs for which the neural network outputs the correct result. Note that the neural network's output is assumed to be the index of whichever neuron in the final layer has the highest activation.""" test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data] return sum(int(x == y) for (x, y) in test_results) # 输出层cost函数对于a的导数 def cost_derivative(self, output_activations, y): """Return the vector of partial derivatives \partial C_x / \partial a for the output activations.""" return (output_activations-y)#### Miscellaneous functions#sigmoid函数def sigmoid(z): """The sigmoid function.""" return 1.0/(1.0+np.exp(-z))#sigmoid函数的导数def sigmoid_prime(z): """Derivative of the sigmoid function.""" return sigmoid(z)*(1-sigmoid(z)) 四、结果比较将隐藏层设为30层，随机梯度下降的迭代次数为30次，小批量数量大小为10，学习速率为3.0 123456789101112131415In [77]: import mnist_loaderIn [78]: import networkIn [80]: training_data,validation_data,test_data = mnist_loader.load_data_wrapper()In [81]: net = network.Network([784,30,10])In [82]: net.SGD(training_data,30,10,3.0,test_data = test_data)Epoch 0: 8185 / 10000Epoch 1: 8363 / 10000Epoch 2: 8404 / 10000Epoch 3: 8447 / 10000······Epoch 25: 9492 / 10000Epoch 26: 9494 / 10000Epoch 27: 9468 / 10000Epoch 28: 9504 / 10000Epoch 29: 9507 / 10000 经过30次迭代之后，神经网络的识别率为95%左右。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>MNIST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习系列（1）：神经网络与反向传播算法]]></title>
    <url>%2F2017%2F04%2F10%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、神经元首先我们从最简单的神经网络——神经元讲起，以下即为一个神经元（Neuron）的图示： 这个神经元是一个以$x_1,x_2,···,x_K$以及截距$b$为输入值的运算单元，其输出为 \alpha =\sigma\left(w^Ta+b\right)=\sigma\left(w_1a_1+w_2a_2+···+w_Ka_K+b\right)其中$w$为权值项，$b$为偏置项，函数$\sigma$被称为“激活函数”。之前在学习感知机的时候，我们知道感知机的激活函数是阶跃函数；而当我们说神经元的时，激活函数往往选择sigmoid函数或tanh函数。激活函数的作用就是将之前加法器输出的函数值$z$进行空间映射，如下图所示： 可以看出，这个单一神经元的输入输出的映射关系其实就是一个逻辑回归（logistic regression）。 关于sigmoid阶跃函数的性质，在逻辑回归中已经了解过了，有一个等式我们会用到：$f^,(z)=f(z)(1-f(z))$。现在我们简要看一下双曲正切函数（tanh）。它的表达式为： f\left(z\right)=\tan\textrm{h}\left(z\right)=\frac{e^z-e^{-z}}{e^z+e^{-z}}它们图像为tanh（z）函数是sigmoid函数的一种变体，它的取值范围为[-1,1]，而不是sigmoid函数的[0,1]，它的导数为$f^，(z)=1-(f(z))^2$ 二、神经网络模型2.1 神经网络模型所谓神经网络就是将许多神经元联结在一起，这样，一个神经元的输出就可以是另一神经元的输入。例如，下图就是一个简单的神经网络： 我们使用圆圈来表示神经网络的输入，标上”+1”的圆圈被称为偏置节点，也就是截距项。神经网络最左边的一层叫做输入层，最右边的一层叫做输出层（本例中，输出层只有一个节点）。中间所有节点组成的一层叫做隐藏层，如此命名是因为我们不能在训练样本中观测到它们的值。同时可以看到，以上神经网络的例子中有3个输入单元（偏置单元不算在内），三个隐藏单元及一个输出单元。 我们用$n_l$来表示神经网络的层数，本例中$n_l=3$，我们将第$l$层记为$L_l$，于是$L_1$是输入层，输出层是$L_{nl}$。本例神经网络有参数$(W,b)=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})$，其中$W_{ij}^{(l)}$是第$l$层第$j$个单元与第$l+1$层第$i$单元之间的联接参数（其实就是连接线上的权重，注意标号前后顺序），$b_i^{(l)}$是第$l+1$层第$i$个单元的偏置项。偏置单元没有输入，即没有单元连向偏置单元，它们总是输出$+1$。同时，我们用$s_l$表示第$l$层的节点数（偏置单元不计在内）。 我们用$a_i^{(l)}$表示第$l$层第$i$个单元的激活值（输出值）。当$l=1$时，$a_i^{(1)}=x_i$，也就是第$i$个输入值（输入值的第$i$个特征）。对于给定参数集合$W,b$，我们的神经网络就可以按照函数$h_{W,b}{(x)}$来计算结果。本例中神经网络的计算步骤如下： a_{1}^{\left(2\right)}=f\left(W_{11}^{\left(1\right)}x_1+W_{12}^{\left(1\right)}x_2+W_{13}^{\left(1\right)}x_3+b_{1}^{\left(1\right)}\right) a_{2}^{\left(2\right)}=f\left(W_{21}^{\left(1\right)}x_1+W_{22}^{\left(1\right)}x_2+W_{23}^{\left(1\right)}x_3+b_{2}^{\left(1\right)}\right) a_{3}^{\left(2\right)}=f\left(W_{31}^{\left(1\right)}x_1+W_{32}^{\left(1\right)}x_2+W_{33}^{\left(1\right)}x_3+b_{3}^{\left(1\right)}\right) h_{w,b}\left(x\right)=a_{1}^{\left(3\right)}=f\left(W_{11}^{\left(1\right)}x_1+W_{12}^{\left(1\right)}x_2+W_{13}^{\left(1\right)}x_3+b_{1}^{\left(2\right)}\right)2.2 具体举例接下来举一个具体的例子来说明这个过程，我们先给神经网络的每个单元写上编号。 图中，输入层有三个节点，我们将其依次编号为1，2，3；隐藏层的4个节点，编号依次为4，5，6，7；最后输出层的两个节点编号为8，9。因为我们这个神经网络是全连接网络，所以可以看到每个节点都和上一层的所有节点有链接。比如我们可以看到隐藏层的节点4，它和输入层的三个节点1，2，3之间都有连接，其连接上的权重分别为$w_{41},w_{42},w_{43}$。那么，我们怎样计算节点4的输出值$a_4$呢？ 为了计算节点4的输出值，我们必须先得到其所有上游节点（也就是节点1，2，3）的输出值。节点1、2、3是输入层的节点，所以，他们的输出值就是向量$\vec{x}$。按照上图画出的对应关系，可以看到节点1、2、3的输出值分别是$x_1,x_2,x_3$。 一旦我们有了节点1、2、3的输出值，我们就可以计算节点4的输出值$a_4$： a_4=f(\vec{w}·\vec{x})=f(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})其中$w_{4b}$是节点4的偏置项，图中没有画出来。而$w_{41},w_{42},w_{43} $分别为节点1、2、3到节点4连接的权重，在给权值$w_{ij}$编号时，我们把目标节点的编号$i$放在前面，把源节点的编号$i$放在后面。 同样，我们可以继续计算出节点5、6、7的输出值$a_5,a_6,a_7$。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值$y_1$: y_1=f(\vec{w}·\vec{x})=f(w_{84}a_4+w_{85}a_5+w_{86}a_6+w_{87}a_7+w_{8b})同理，我们还可以计算出$y_2$的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量$\vec{x}=\left[\begin{array}{c} x_1\\ x_2\\ x_3\\\end{array}\right]$时，神经网络的输出向量$\vec{y}=\left[\begin{array}{c} y_1\\ y_2\\\end{array}\right]$。这里我们也看到，输出向量的维度和输出层神经元个数相同。 2.3 神经网络的矩阵表示神经网络的计算如果用矩阵来表示会很方便，我们先来看看隐藏层的矩阵表示。首先我们把隐藏层4个节点的计算依次排列出来： a_4=f(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})a_5=f(w_{51}x_1+w_{52}x_2+w_{53}x_3+w_{5b})a_6=f(w_{61}x_1+w_{62}x_2+w_{63}x_3+w_{6b})a_7=f(w_{71}x_1+w_{72}x_2+w_{73}x_3+w_{7b})接着，定义神经网络的输入向量$\vec{x}$和隐藏层每个节点的权重向量$\vec{w_j}$。令 \vec{x}=\left[\begin{array}{c} x_1\\ x_2\\ x_3\\ 1\\ \end{array}\right]\vec{w_4}=[w_{41},w_{42},w_{43},w_{4b}]\vec{w_5}=[w_{51},w_{52},w_{53},w_{5b}]\vec{w_6}=[w_{61},w_{62},w_{63},w_{6b}]\vec{w_7}=[w_{71},w_{72},w_{73},w_{7b}]代入之前的一组式子，得到 a_4=f(\vec{w_4}·\vec{x})a_5=f(\vec{w_5}·\vec{x})a_6=f(\vec{w_6}·\vec{x})a_7=f(\vec{w_7}·\vec{x})现在，我们把上述计算$a_4,a_5,a_6,a_7$的四个式子写到一个矩阵里面，每个式子作为矩阵的一行，就可以利用矩阵来表示他们的计算了。令 \vec{a}=\left[\begin{array}{c} a_4\\ a_5\\ a_6\\ a_7\\ \end{array}\right]\vec{W}=\left[\begin{array}{c} \vec{w_4}\\ \vec{w_5}\\ \vec{w_6}\\ \vec{w_7}\\ \end{array}\right]=\left[\begin{matrix} w_{41}& w_{42}& w_{43}& w_{4b}\\ w_{51}& w_{52}& w_{53}& w_{5b}\\ w_{61}& w_{62}& w_{63}& w_{6b}\\ w_{71}& w_{72}& w_{73}& w_{7b}\\ \end{matrix}\right] f\left(\left[\begin{array}{c} x_1\\ x_2\\ ··\\ ··\\ \end{array}\right]\right)=\left[\begin{array}{c} f\left(x_1\right)\\ f\left(x_2\right)\\ f\left(x_3\right)\\ ···\\ \end{array}\right]代入前面的一组式子，得到\vec{a}=f(W·\vec{x})在上式中，$f$是激活函数，在本例中为sigmoid函数；$W$是某一层的权重矩阵；$\vec{x}$是某层的输入向量；$\vec{a}$是某层的输出向量。它说明了神经网络的每一层的作用实际上就是先将输入向量左乘一个数组进行线性变换，得到一个新的向量，然后再对这个向量逐元素应用一个激活函数。 每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重举证分别为$W_1,W_2,W_3,W_4$,每个隐藏层的输出分别是$\vec{a_1},\vec{a_2} ,\vec{a_3} $，神经网络的输入为$\vec{x}$，神经网络的输入为$\vec{y}$，如下图所示： 则每一层的输出向量的计算可以表示为： \vec{a_1}=f(W_1·\vec{x})\vec{a_2}=f(W_2·\vec{a_1})\vec{a_3}=f(W_3·\vec{a_2})\vec{y}=f(W_4·\vec{a_3})这就是神经网络输出值的计算方法。 三、反向传导算法3.1 损失函数与正则化项假设我们有一个固定样本集$\{(x^{(1)},y^{(1)}),···,(x^{(m)},y^{(m)})\}$,它包含$m$个样本。我们可以用批量梯度下降法来求解神经网络。具体来讲，对于单个样例$(x,y)$，其代价函数为： J(W,b;x,y)=\frac{1}{2}||h_{W,b}{(x)}-y||^2这是一个平方误差损失函数。对于包含$m$个样本的数据集，我们可以定义整体的损失函数为： J\left(W,b\right)=\left[\frac{1}{m}\sum_{i=1}^m{J\left(W,b;x^{\left(i\right)},y^{\left(j\right)}\right)}\right]+\frac{\lambda}{2}\sum_{l=1}^{n_l-1}{\sum_{i=1}^{s_l}{\sum_{j=1}^{s_{l+1}}{\left(W_{ji}^{\left(l\right)}\right)^2}}}=\left[\frac{1}{m}\sum_{i=1}^m{\frac{1}{2}}\parallel h_{W,b}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\parallel^2\right]+\frac{\lambda}{2}\sum_{l=1}^{n_l-1}{\sum_{i=1}^{s_l}{\sum_{j=1}^{s_{l+1}}{\left(W_{ij}^{\left(l\right)}\right)^2}}}以上关于$J(W,b)$定义中的第一项是均方误差项，第二项是一个正则化项，也叫权重衰减项，其目的就是减小权重的幅度，防止过度拟合。权重衰减参数$\lambda$用于控制公式中两项的相对重要性。需要注意的是，$J(W,b;x,y)$是针对单个样本计算得到的方差代价函数；$J(W,b)$是整体样本代价函数，它包含权重衰减项。 3.2 反向传播算法反向传播算法其实就是链式求导法则的应用。然而，这个如此简单且显而易见的方法，却是在Roseblatt剔除感知机算法将近30年之后才被发明和普及的。接下来，我们用链式求导法则来推导反向传播算法。 按照机器学习的通用套路，我们先确定神经网络的目标函数，然后用随机梯度下降优化算法去求目标函数最小值时的参数值。 假设我们的参数集合为$\theta =\{w_1,w_2,···,b_1,b_2···\}$，设初始参数为$\theta^0$，将损失函数$L(\theta)$分别对参数求导： \nabla L\left(\theta\right) =\left[\begin{array}{c} \partial L\left(\theta\right)/\partial w_1\\ \partial L\left(\theta\right)/\partial w_2\\ ···\\ \partial L\left(\theta\right)/\partial b_1\\ \partial L\left(\theta\right)/\partial b_2\\ ···\\ \end{array}\right]计算$\nabla L(\theta^0)$，参数更新 \theta ^1=\theta ^0-\eta \nabla L(\theta^0)计算$\nabla L(\theta ^1)$，参数更新 \theta ^2=\theta ^1-\eta \nabla L(\theta^1)因为推导过程需要用到链式法则，具体如下图所示：我们定义整体损失函数为： L\left(\theta\right)=\sum_{n=1}^N{C^n\left(\theta\right)}对参数$w$求偏导： \frac{\partial L\left(\theta\right)}{\partial w}=\sum_{n=1}^N{\frac{\partial C^n\left(\theta\right)}{\partial w}}因此我们只需要求出单个样例的偏导数，就可以推导出整体损失函数的偏导数。根据链式法则，对于某一个节点，如下所示： \frac{\partial C}{\partial w}=\frac{\partial z}{\partial w}\frac{\partial C}{\partial z}容易得到 \partial z/\partial w_1=x_1 \partial z/\partial w_2=x_2我们可以利用前向传导的方法计算出所有层的$\frac{\partial z} {\partial w}$ 我们已经求出了整个偏导数的左半部分，接下来看右半部分，即$\frac{\partial C}{\partial z}$。 根据链式法则得到： \frac{\partial C}{\partial z}=\frac{\partial a}{\partial z}\frac{\partial C}{\partial a}对于$\frac{\partial a}{\partial z}$，我们知道就是激活函数对加法器的偏导，知道了激活函数便知道了$\frac{\partial a}{\partial z}$，我们设其求导结果为$\partial ‘ (z)$，因为$z$在前向传播中已经确定，所以$\partial ‘ (z)$其实是一个常数。接下来看$\frac{\partial C}{\partial a}$ 根据链式求导法则 \frac{\partial C}{\partial a}=\frac{\partial z'}{\partial a}\frac{\partial C}{\partial z'}+\frac{\partial z''}{\partial a}\frac{\partial C}{\partial z''}易知$\frac{\partial z’}{\partial a}$即为权值，而$\frac{\partial C}{\partial z’}$假设其已知，则我们可以得到 \frac{\partial C}{\partial z}=\sigma '\left(z\right)\left[w_3\frac{\partial C}{\partial z'}+w_4\frac{\partial C}{\partial z''}\right]而对于$\frac{\partial C}{\partial z}$的求导，我们需要区分输出层和隐藏层两种情况： 第一种情况。如果已经是输出层了，如下图所示 我们可以直接求得。 第二种情况。如果还处于隐藏层，我们可以根据上述算法不断递归的计算$\partial C /\partial z$，直到抵达输出层。 最后总结一下，我们根据前向传播算法求得所有的$\frac{\partial z}{\partial w}$，根据反向传播算法求得所有的$\frac{\partial C}{\partial z}$（需要用到前向传播算法求得的$\frac{\partial a}{\partial z}$，即$\sigma ‘\left(z\right)$）。这样就可以用更新公式对参数进行迭代更新了。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>反向传播算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日知录（3）：2016人大应统学长倾心经验贴]]></title>
    <url>%2F2017%2F04%2F04%2F%E4%BA%BA%E5%A4%A7%E5%BA%94%E7%BB%9F%E5%AD%A6%E9%95%BF%E5%80%BE%E5%BF%83%E7%BB%8F%E9%AA%8C%E8%B4%B4%2F</url>
    <content type="text"><![CDATA[2016人大考研总算是落下了帷幕，大半年来的努力终于落定。在北京的春天里，窗外飘着杨絮，在图书馆的沙发上码下这难忘时光的的锤炼与凝结。 初试成绩：总分392（政治68、英语73、数学三121、统计学130） 复试成绩：总分292（英语笔试40、英语面试40、专业课笔试77、专业课面试135） 复习起始：初试： 8月10日-12月26日 | 复试： 2月20日-3月11日 日常安排 时刻 事项 6:30-7:30 起床洗漱早餐、七点半准时到海洋楼坐定，考研四个小伙伴轮流占座。 7:30-8:30 扇贝单词打卡一小时（这个习惯坚持到了考研最后一天还是蛮给力的） 8:30-11:00 数学（雷打不动的看书做题） 11:00-11:30 午餐（总会看到一个法兰西女孩，像西西里的美丽传说里的莫妮卡一样风姿卓越，食欲大增） 11:30-13:00 数学（消化午餐的同时还能做做数学） 13:00-14:00 午睡（这个是每日必备良药，不午睡下午的效率等于0） 14:00-17:00 英语（有几天没午睡，这段时间就会是睡过去的） 17:00-17:30 晚餐（晚餐的时候见不到那个法兰西女孩，食欲大减） 17:30-20:00 政治（每天两个半小时足够了） 20:00-23:00 专业课（因为跨专业、这个是真的有很努力在学） 当然每天都不会完完全全的严丝合缝的来，到后期就是固定这个模式了，考研的四个小伙伴也是在不断地磨合中才基本保持了一致的计划，抱团的作用就在于此，当你投入不进去的时候，自然会有外力来催促你，除非你的个人毅力可以抵制该死的懒癌，你求知的欲望超过你对舒适的渴求，否则，建议抱团。 一、考研专业选择本科学的工科，学的不精，但对计算机有接触，算是有点编程的底子，慢慢的了解到大数据专业，自己也摸索着学习了一些有关数据挖掘的模型算法，蛮感兴趣的。最近在拜读吴军博士的《数学之美》，科普下这个领域，更深入的也正处于摸索阶段，之前在人大大数据老胡学长那里要到了几本大块头的书，英文版，头疼中，坚持啃完。当然敲代码的日子我想应该就会换来蓬头垢面的自己吧。 个人对这种跨领域的东西特别着迷，就像最近博弈论的发展方向，复旦韦森教授总结道：“沿着道德哲学、政治哲学把休谟、卢梭、康德到罗尔斯的思想和理论程式化，致力于回复经济学的亚当·斯密的古典传统，即从审慎推理（prudential reasoning——即目前经济学家所言的理性最大化推理）和道德推理（moral reasoning）两个维度研究人们的经济与社会行为，并从中折射出制度的伦理维度和道德基础来。在这个研究向量上的经济学家主要有已过世的诺奖得主哈森伊、宾默尔以及萨金。笔者也发现，由一些谙熟现代博弈论分析工具的一些当代思想家——如美国加州大学洛杉矶分校的人类学家Rob Boyd，美国麻省大学的经济学家Herbert Gintis，慕尼黑大学的经济学家Ernst Fehr，以及麻省理工学院的著名博弈论大师弗登博格（DrewFudenberg）等，最近从文化、互惠合作（reciprocity）、利己和利他行为的产生及其在社会选择中作用等相关领域的探索非常值得我们注意。笔者这里贸然推断，这些文化人类学、经济学和博弈论的跨学科的合作研究，也许在不久的将来会汇合哈森伊、宾默尔以及萨金在伦理与社会选择探索向量上的已有理论探索。这一研究向量的理论从任何当今一门社会科学的学科视角来看均无疑代表了人类认识社会和自身的目前最前沿思考，且与经济学的制度分析的最深层基础密切相关联（这是我把他们的工作也视作为经济学的制度分析的主要理由）。笔者目前甚至乐观地估计，如果在这一研究向量中以哈森伊、宾默尔所代表的“经济学—伦理学—政治哲学—博弈论的交叉分析”与另一方面的“文化人类学—经济学—博弈论”的跨学科研究汇融起来的话，这又将会在二十一世纪谱写并演奏出一首宏大与辉煌的“理论交响曲”，从而极大地推进人类对自身所处的社会和人本身的认识和理解。” 之所以摘录那么长一段，是因为个人对于像韦森教授的文本话语中所展现出来的厚重的学术穿透力，像何怀宏教授这样的对正义美好的生活崇尚、像香港中文大学周保松教授对自由民主的理性而温柔的认可与支持、像钱理群教授对生命的关切和对知识的渴求所呈现的高尚的个人品格尤为佩服，他们的精神与智慧必将永远的接续下去，徜徉于他们的文字中，我最为真切感受到的莫过于他们所严肃实践的一种生活理念，这是一种热切的关怀、是一种直抵生命深处的质问、亦是一种现实世界中的身体力行的实践思想，它们似乎与主流思维相悖，这种影响逐渐在我们这一辈的学子中发生，生根发芽，我们开始从无数年被灌输出来的满脑子“绝对真理”中走出来，开始接受世界的丰富性，这即是自由的开端。感谢他们这样的勇士。 二、考研院校选择本科在北京，这个学校唯一让我眷恋的就是，他可能是全中国最幸福的院校，从学校出发去北大人大，半小时车程，清华徒步即可，那时候挚爱哲学，跑去北大人大哲学院蹭课，每个老师的课都会零零碎碎的听一点，北大戴锦华老师、人大周濂老师的课尤为喜爱。北大国家发展研究院（ccer）的课也去蹭了一学期，那时候汪丁丁的新政治经济学和行为经济学，第一次去上课的时候，斗胆坐在第一排，捧着那本融合了脑神经学、认知心理学、经济学、哲学、政治学、社会学、伦理学、宗教和神秘主义的《新政治经济学》，扑面而来的知识模块，令人头晕目眩，结果便是全程睡过去的，万圣书园刘苏里称道他或许是我们这个时代少有的文艺复兴式的知识人，考研完了再次拜读老师的书，书中呈现出来的渊博之魅让人瞠目结舌。北京最让人眷恋的便是这种藏匿在深处的知识图景，学识对撞的读书会、严肃学术的研讨会、陶冶人心的音乐会、感同身受的小剧场话剧，纵然周围是雾霾缭绕，但当你挖掘到内核，或许就再也不想离开。对北京的这种念想让我毅然决然的选择了北京的研究生院，考研之前，对高校的应用统计专业略微地进行了考查。 人大：经济统计、精算、医学统计、国民经济这几块人大是老字号了，在全国都名列前茅。但最让我热血沸腾的是近两年刚开办的五校协同交叉培养大数据方向专业硕士，对于这个平台，袁卫老师有很赞的叙述：“5所学校参与培养，就是出于学科交叉的考虑。中国人民大学统计学院的学科、专业设置是综合的、应用的，理论和应用兼而有之，应用领域涉及卫生、健康、经济、社会、管理等，总体实力较强。而北京大学和中国科学院大学，大家都知道，他们在计算机、数学和统计理论研究方面相当强，掌握大数据分析技术的前沿。中央财经大学和首都经贸大学是财经类为主的院校，这两所学校侧重于应用人才的培养，特别是面向经济、管理、社会这样的领域。他们和很多行业企业、金融机构有着密切联系。这5所高校分别属于教育部直属高校、中国科学院的高校和地方高校3种类型，各有特色，优势互补，能够建成一个很好的、学科交叉的人才培养协同体。”所以，心里其实早就锁定这个了。 清华：招生人数太少，信息严重不对称，也清楚认识到自己没那么强悍的能力，就没怎么考虑。 北大：之前在网上看到北大444哥考应用统计的雄文，看得我是荷尔蒙暴增，一冲动就买来了所有的参考书，结果发现，专业课的难度自己很难把握。而且北大分数线蛮高的，几乎每年都在390左右。学长应该是毕业了，感谢他，因为我的很多经验都会参考他，当时是打印出来一个字一个字拜读。给出他的经验贴链接吧：我的444分北大考研成功经验谈 其他：考研帮有个帖子，比较全面的分析了全国各考研院校的应用统计专业，大家可以参考下：应用统计全国高校分析 三、各科复习方法3.1 英语复习用书： 扇贝APP 张剑考研真题黄皮书 张剑阅读理解150篇 王江涛高分写作 复习方案： 单词：没有用很厚的单词书，感觉会压垮自己，所以选择了app来背，一开始使用的是新东方的乐词，但是亲测效果不佳，后来经研友推荐上了扇贝的船，从此每天早上就开始打卡背诵，保证一个小时的量。特别要注意的是背单词必须要保持连续性，不能中间隔开半个月或者一个月搁置在那里而不背单词了，每天背，这是一个积少成多的过程，我就是一个反面例子，中间有段时间特别抵触背单词，荒了快一个月，结果就是做阅读的时候单词在耳边就是想不起来意思。到考研前要达到的效果就是看到一个单词就立马反应过来，背上三四遍之后就比较省力了，有的时候一小时可以背诵七百个，当然这是建立在前期不断反复不断反复的基础上，基本每个单词app都是依照记忆曲线帮你安排任务，所以也不用担心会漏背或者背不熟。 完形填空、新题型、翻译：这三部分分值占的比较小，基本上每个人都可以得到个基本分数，所以就没太花时间在这上面，只做了历年真题里面的这些部分，然后看黄皮书的分析，掌握一些技巧即可。 阅读： 张剑黄皮书系列的真题基本上是人手一册，真题的研究对于阅读来说是至关重要的，至少要保证两遍以上的联系和琢磨，网上都说英语考80分以上的都是真题研究了四遍五遍的，这是有一定的道理的，阅读的正确与你对命题老师的出题思路的熟悉程度正相关，基本上每一类题都会有特定的规律性，只有当你顺应了老师们的思维模式，在琢磨选项的正误时你都可以类似于套圈子一样套进去，这正是真题的重要性所在，我们可以接受这种思维模式训练的第一手资料就是真题，任何模拟题都无法与之比拟。当然不是说模拟题不需要，在这里推荐张剑的150篇，还是比较贴近真题的，但他的功效仅仅在于提高对新篇章文本的适应性，在考场上难免会遇到和历年真题风格不一致的文章，这就是模拟题的优势所在，这本书基本涵盖了考研英语阅读出现率比较高的话题，你可以拿它当做拓宽英语话语体系的佐料。我当时是花了大约一个月的时间来做这本书，每天下午四十分钟左右做两篇阅读，其他的时间研究前一天做的那两篇，一直循环下去。这里还要说下怎么研究模拟题和真题。拿到一篇文章，按照你的方法做完，然后就是挨个查单词，分析长难句，挨个解读选项和分析，自己从文中找出依据来，最好自己搭建一个文章的框架。 作文：今年的作文幸亏了王江涛，一开始心存侥幸想背个模板就完事了，后来越来越不安，觉得模板作文我难以驾驭的好，如果成篇的万能句型上去，老师必然不买账，估计就是个低分的下场。当然如果真的可以把一个模板变通到炉火纯青，那也一样OK。但对于我这样四级飘过的，六级未过的naïve青年来说怎么会有那么好的英语修养呢，所以最最后一个月算是逼着自己按照王道长的指示安安分分的背诵了8篇范文，滚瓜烂熟，倒背如流，只能这样。道长说了，英语作文看的是你的语言功底，只要和主题搭上边了，没啥语法错误，词汇量足够，还能写出漂亮的句型来，那就不会差到哪里去。所以，背诵历年真题就是最好的办法，踏踏实实的背诵、默写、仿写，王江涛在书里说的都很清楚。 友情提醒：再一次华丽丽的证明了自己的心理素质是多么不堪一击，哈哈，我边上的童鞋老抖腿，以至于我总是有意无意的要观察他的抖腿频率，这让我几近奔溃，没举报老师怪我太善良，结果就是做阅读完全不在状态，基本上就靠语感在做了，我谢谢他全家，我不造其他人被影响到没，怪只怪自己心理素质就像一块薄冰，一碰就会碎。故，学弟学妹们在考场上一定要杜绝这种危害社会健康的事情发生，一经发现，向朝阳群众学习，立马举报上级。 3.2 政治考了68，不敢拿出来献丑，对于这种被人戏谑的学科，其实是拒绝介绍经验的，先说几个好玩的政治段子：“靠别人，你永远是右倾投降主义，靠自己你才是工农武装割据”、“如果全世界都对你恶语相加，我愿对你说上一世纪社会主义核心价值观”、“你我之间本无缘分，全靠党的章程死撑”、“别低头！GDP会掉！别落泪！资本主义会笑”、“想和你谈一场弘扬社会主义正能量的恋爱，你却要我好好做自己的中国梦”。茶余饭后看点段子还是蛮有意思的。 复习用书：（按出场顺序排列） 《思想政治理论考研大纲解析》 肖秀荣《命题人1000题》 风中劲草《冲刺背诵核心考点》 肖秀荣《命题人冲刺八套卷》 启航《20天20题》 肖秀荣《命题人形势与政策》 肖秀荣《终极预测四套卷》、任汝芬《最后四套卷》、任燕翔《考前预测4套卷》 复习方案： 9月5日-10月10日：九月份考研大纲解析发布，开始快马加鞭的看，一块让人厌恶的砖头，不过你还是得静下心来去啃，尝试着一字一句的过，网上流传着一张图，内容是这样的：恩格斯问大胡子马克思先生：“你在干嘛呢”。马克思心平气和的回答道：“管他呢，反正又不是我背”。中国学生对其抵触的心理可见一斑。但其实真正的马克思先生的思想很可贵，以至于20世纪法国解构主义哲学大师德里达在他的著作《马克思的幽灵》中写道：“不能没有马克思，没有马克思，没有对马克思的记忆，没有马克思的遗产，也就没有将来；无论如何得有某个马克思，得有他的才华，至少得有他的某种精神。现在该维护马克思的幽灵们了。”只是在这个极权盛行的国度，马克思变成了鬼魂，笼罩这苍茫的大地，最后沦落为官方口腔，这才是知识分子真正的悲哀，依附于权力而放弃说理。言归正传，大纲解析的脉络其实是很清晰的，看不懂也没有大碍，拿出肖秀荣先生的《1000题》，看完大纲一个章节的内容你就要把《1000题》上对应的章节的选择题给做了，不要看书，把答案写在一张A4上，注意1000题是要做三遍的，第一遍做都会错的惨不忍睹，错了没事，切忌欺骗自己看看答案把错的题给改对了，因为学长就是这样喜欢自我欺骗的前车之鉴哈哈。马克思主义和毛中特部分或许会让你略微头疼，这种理论性质的东西充斥着新闻联播的气质，但也务必沉住气，到后面解决史纲和思修就是分分钟的事，在高中阶段谁都学过维新变法、辛亥革命之类的，学起来还可能会让你增生一些兴趣。在这里安利一部良心巨制《走向共和》，看完这部电视剧会让你对中国近代史的基本脉络有一个清晰的呈现，记得本科的时候看徐中约先生的《中国近代史》，看得我是心力交瘁，后经学长推荐才去看的这部戏，看完后再回头看那本书，不适感就下降很多。下一步要做的事很重要，就是把你做错的题目，从大纲解析里面找答案，用晨光彩色标记笔标注出来。好了这一遍下来，务必请你自己做一个粗略的知识回顾和框架的搭建，每个章节在讲什么内容，拿毛中特部分举个例子，第一章提纲挈领先介绍了马克思主义中国化的两大理论：毛爷爷思想和中特理论体系，也是同上一部分的马克思主义的衔接，然后从第二章开始，分别是第二章新民主主义革命（1919-1949），第三章社会主义改造（1949-1956），第四章社会主义建设（1956-1978），第五章和第六章插播了总依据和总任务，因为接下来的是第七章改革开放了（1978-不知道啥时候结束），再然后是考研政治的重点，第八章总布局，这章内容及其丰富，包括中特经济、政治、文化、社会、生态文明。紧接着就是祖国统一、外交国际战略，最后两章是建设中特的相关问题，总结陈词就是党好党棒棒哒领导好领导棒棒哒。你可以自己建立一个思维导图，带着这个脉络你可以顺利的进入到下一关，这些基础工作是为后期服务的。 10月10日-11月5日：第二遍重复上一步的内容，看大纲，做1000题，纠错回大纲标注。注意第二遍的时候你可以把答案写在1000题上了，然后看1000题后面的答案，把错的以及你觉得好的题的解析在题目边上标注下，要知道为什么错，举一反三。 11月6日-考研结束：刷完了两编大纲和1000题，这时候会出现一本震撼人心的资料出现—风中劲草，这本书的编排和印刷是下了一番功夫的，他的细节之处可以让你真的佩服这本书的作者，跟进大纲，条理清晰，标注分明，重点突出，考研资料中的扛鼎之作。所以，你一定要把这本书当做是你考研政治的制胜法宝，你该怎么做呢？看，一个字一个字的看，我当时就有种心态，自己可以假装看懂普鲁斯特的《追忆似水年华》，我就必须要看透杨杰先生的《风中劲草》，两个时代的回响多璀璨。直到考研之前，你也不要放下这本书，看的遍数越积越多，你就会达到你自己都意想不到的层次，就是合上书，你大概可以知道哪个知识点在那一页的哪块位置。我保持的速度大概是1天15页左右，15天一本书，到考研结束加起来看了三遍。马原毛中特部分你可以多看几遍，四遍五遍无上限。对了，这时候我还同时做了一件事，就是第一遍的时候只是把1000题上的每一道题都在风中劲草上标注，若是单选题第一题，就标注“单1”，多选类推，同时用彩色笔标注。此外，肖秀荣的《形势与政策》也出来了，买来利用空余时间看两遍。 12月15日-考研结束：各种模拟预测题纷纷登场，这里首推肖秀荣的《8套卷》和《4套卷》，可以去学校打印店购买，便宜实惠，八套卷你都要当做考试一样对待，基本上三十分钟就可以完成一套，完成一套之后看下答案分析，纠错，在风中劲草上标注，如肖秀荣第三套第1题，就标注“肖8三1”。因为之前咱们已经完成了以下任务：大纲解析两遍，1000题两遍，风中劲草两遍左右，再加上不断的标注，8套卷就是检验你复习成果的最佳试题，肖秀荣老师编的资料都棒呆，个人灰常喜欢他的讲课风格，一口流利的方言普通话，考研期间要随时关注肖老师的微博微信，都是同步的，关注一个就可以，把它发布的一些重要文件下载下来，打印研究。八套卷你可以做两遍三遍，注意要举一反三，尤其是错题。然后过几天4套卷就会隆重登场，基本是人手一套，不要迟疑，买来赶紧把客观题做了，按照之前的流程对待客观题。接下来就是万众瞩目的主观题，大家从开始到现在还没有接触过主观题，4套卷就是专门为了主观题准备的，如果你不想留太多的时间和精力在政治上（毕竟政治只有100分，数学和专业课才是重中之重），那就建议你只背诵4套卷的主观题，之前提到的启航20天20题可以翻翻，虽然好，但知识点太多，没时间应付，咱们把赌注押在肖秀荣的4套卷上，背诵的时候主要要挑关键词背诵，自己想法子变通式的背下来，切忌原封不动的机械背诵，虽然这几年老爷子押原题的能力衰弱了，但每年的知识点还是压得相当准的，当然像今年很多人说蒋中挺几乎全压中原题了，没怎么看过他的资料，不予置评。 考场上：考政治的时候觉得选择题so easy啊，做到主观题，有点懵逼了，肖大大押中的题的答案全变成了考研的题干，只能硬着头皮上了，相关的知识点全答上去了，生死未卜的赶脚，有一道家庭美德的就全靠扯了。所以，我的复习方案可以给大家敲响警钟，要是想要考高分的，主观题也是要早点准备的，尽量多参考几个考研机构的预测卷，稍微整理下答题的思路，预防真题出现一些偏题。 说在后面：考研政治的时效性特别明显，16年开了十八届五中全会，那么有关它的内容一定会是考研的重头戏，今年客观题和主观题都有一定的体现，而且比重还不低。所以，形势与政治也要多加注意，有时间就多看几遍，尤其是考前那几天，加深印象，有的关键词列点背诵。肖大大解答过考研命题人如何出题：先确定要考的知识点，然后去报纸和杂志上找相应的材料。所以，我们必须要对知识点分外敏感，在不同的模拟题中间总结出知识点来。 另外推荐几个不错的复习资料： 肖秀荣的【马克思主义基本原理概论逻辑图】，哲学的主观题就全靠这个资料来沥青脉络了。 肖秀荣的【近代史时间轴】：把近代发生的事件按照时间顺序排列，一些重要的知识点也有叙述。 肖秀荣的《知识点提要》八个附录：网上也有，可以看看背背啥的。 这样下来政治需要看的东西也蛮多的，时间要自己控制好，到了后期，专业课要背，英语作文要背，政治也要背，别被他们压垮，挺过去！ 3.3 数学三 复习资料 张宇数学三的视频课 《李永乐复习全书》大红色 《李永乐660题》 《李永乐历年真题》 40套模拟题（《张宇8套卷》、《4套卷》、《永乐6套卷》、《历年合工大最后五套卷》、《400题》） 复习方案： 8月10日-9月1日：花了将近一个月的时间来看张宇的视频，他的整个讲解的框架体系蛮成熟的，按照他的指示把笔记全部抄下来，然后自己尝试着背诵，这样下来就可以搭建起数学三的整体的脉络，知道要考的知识点和题型。我个人认为这个框架的搭建对于学习数学来说太重要了，他可以帮助你以一种高屋建瓴的视角来面对你所遇见的各种题型，而不至于迷失在茫茫的题海中无法自拔，它就像在你的脑海中植入了一份详尽的探险地图，遇见一个题，你可以将其归入体系中的某个知识点，这样的训练增加之后，对你的做题速度和准确率也会有很大的提升。一定要有这样的意识将知识归整而不是碎片化存在于你的大脑中，一个成熟的知识体系都会是如此，麻省理工大学的数学大咖林达华在讲解自己的数学体系时，必然脑海中有这样的一个完备的详尽的清晰的图景。 9月1日-10月15日：进入考研攻坚期，复习全书是必备的，因为数学是上午考，我也象征性的把复习时间安排在了上午，每天看10页左右，消化不了太多，一些原则：1、必须自己拿笔写，切忌眼高手低以为看看就会了/2、切忌还没怎么思考就看答案解析，不会做没事，自己思考的过程尤为关键，在每道题的边上写下自己的思考推算过程以及这道题的关键之处、3、琢磨好久都搞不明白的，可以询问大神研友，或者自己做个标记，以后来解决（我后来忘记我曾经有不会的题了，就是这么大马哈）。数学其实有点像练书法，一开始可能你的水准只能够临摹大师们的作品，还只能学个皮毛，但重复训练达到一定的层次之后，你会形成自己的笔法（数学思维方式），在之后对于从未涉猎的新帖（新题型）也可以驾轻熟重。 10月16日-11月10日：复习全书完了之后，一本虐人无数的660题登场，别以为他全是选择题和填空题，但他的每一道题都是精心锤炼过得，所体现的数学思想方法绝对会让你获益匪浅，他的题目的设置真的恰到好处，细细的琢磨每一道题的精髓，虽然真题是绝对达不到这样的难度的，关键的是思想方法。我大概刷了20多天，但有些题后来又忘记回过头去考虑了，这就落下不少病根， 所以建议复习的早一点，可以有更多的时间来调整自己的复习计划。 11月11日-11月20日：从光棍节那天清晨开始，我拿起了真题，花了十天时间每天完成一套卷子，因为很多题其实在你做全书的时候已经遇到过了，所以其实真题对于真实水平的评估还是有很大偏差的，对完答案，订正，错的题的解题思路思考一遍就完事，当时的分数基本保持在120-140之间，也没有太大的失常，第一，历年真题相对于之前的训练还是简单一些的，第二，平时的训练不紧张，三个小时基本上都是轻松愉快的度过的，那时候每天早上起床就盼着可以做数学真题了，就像恋爱一样。 11月20日-12月23日：接下来来到了我个人最为推崇的一种方法，就是数学套卷模拟，在这个阶段中，每做一套模拟题，就可以把所有的章节的重要内容复习一遍，尽管无法覆盖每一个知识点，但模拟题的编写还是有一定的规律的，可以让你随机的复习到一些重要的知识点。要遵守几个原则：1.三小时一套，时间到了就停止答题，然后根据答案自己批分数。2.必须严格遵守考研数学设定的考场规则，不能看书，不能交头接耳，不能询问学神研友，不能嚼口香糖。3.交叉训练法，每个老师出题的风格可能不一样，你可以先做两套张宇的，再做两套李永乐的，这样循环着做，可以增强你的适应能力，亲测有效。至于该做哪些模拟题，我推荐的都在上边写着，这个方法也是借鉴北大444哥的。为什么要建议这个办法呢，因为考研数学今年风格大变，像线性代数和概率论与数理统计的大题都是很难遇到的，那怎么办呢，就是不断地训练模拟题，不断地遇到新题，不断地提高自己的解题能力，而且这样的实战模拟也会让你开始意识到考场上的时间分配是何其重要，3个小时，挑大肉吃，有的是在太难的，抛开也无妨，考场的战略是需要在平时的训练中积累的。此外，你的书写也需要在这段时间里训练，张宇的8套卷和4套卷都提供了和考研一模一样的答题纸，不要大手大脚的乱答题，解题的条理性要注意。合工大的那几套题真的很不错，今年在考场上做高数的时候相当顺利的原因就是这些题型基本都在那15套卷子里遇到过了，所以，直到高数大题做完，我只花了1个小时20分钟，最后剩下一个半小时左右的时间来解决线代和概率大题，但是，我真是个天生的考场悲剧制造者，详情见下。 考场上： 因为考研期间基本上很少运动，打个球跑个步都是奢侈的不行，散步也成了浪费时间的活儿，后来自己也尝到恶果了，我的小心脏承受不住了，有时候会突然之间心跳加速到200多次每分钟，去校医院检查的时候医生说说是有阵发性室上性心动过速，吓得我够呛，问医生为啥，他说是我的心脏短路了，很多时候是因为焦虑不安或者过度兴奋造成的，好吧，那时候都到了考研的冲刺期，我也只能硬挺着，反正医生说没什么生命危险，然后就听到了考研的那天。在考研数学的考场上，我因为一个小时十分钟就完成了高数部分，high的不行了，一兴奋，犯病了，心跳开始砰砰砰的上去，以至于我无法正常答题，短时间治愈这个病的办法就是蹲下去深呼吸，于是我就申请去走廊自己做深蹲，然后深呼吸，深蹲深呼吸，当时真的快要奔溃了，以为这场试就这么完蛋了。深蹲深呼吸了好久，大约过了十五分钟，还是没有恢复过来，感觉真的没救了，老师也一直在边上看着我，该咋办，我的天哪，情急之下我开始捶自己的胸部，一锤倒是好了，但是背部还是有明显的不适感，总觉得有东西在怼我，就在这样的状态下，勉强答完了题，再加上线代和概率题和之前的训练风格太不一样了，我就有点崩溃了，连时间都看错了，原本是11点半结束，我却以为11点就结束了，情急之下把线代大题答得满卷子都是，感觉都看不清楚了，菩萨保佑吧，希望老师可以手下留情。所以，从我身上可以吸取的教训就是平时要注意身体，有时间就去跑跑步打打球，千万不能输在身体上。最后数学考了121分，也算是谢天谢地了。 3.4 432统计学复习用书： 贾俊平《统计学》第四版（经管类） 贾俊平《统计学》第六版（21世纪统计学系列教材） 何晓群《多元统计分析》 王燕 《时间序列分析》 何晓群《应用回归分析》 复习方案： 贾俊平《统计学》：一开始看的是第六版，作为门外汉的我觉得这本书还是蛮简单的，因为之前学过数理统计的一些课程，所以理解起来也不难，而且框架体系也比较清晰，基本上一个章节一天就OK，看了两遍，然后整理了自己的笔记。后来了解到原来第四版的内容更饱满一些，就把第六版没有的内容补看了下，做了笔记。而且今年出事的时候出了一道实验设计的题，最后阶段预测的时候是万万没有想到会出这个题，所以，建议看第四版，内容全。但我又比较喜欢第六版的表述，两本结合着看吧，但是第六版上没有的内容一定要补全，像实验设计、哑变量、指数平滑、主成分和因子分析、聚类分析（这俩个属于多元统计分析）都要添加上去。非参数统计我看了一遍，整理了下笔记，稍微背诵了下，不过复试的时候有人被问到了，所以也要好好看。复试的时候老师问了我关于哑变量的，幸好看了第四版。 何晓群《多元统计分析》：说实在的这本书写得像哲学，感觉是直接从英文版翻译过来的，很多表述没有那么通俗易懂。我只看到了第八章典型相关分析，真的很难说会不会考之后那几章，就目前来看是小概率事件。这本书最关键的是统计分析方法的基本理论原理、分析步骤和以及去对应可以解决的问题，不需要去死抠推导过程，这不是432需要重视的，但是对于理解还是有帮助的，有兴趣的可以推推看。16年没考这部分内容，但不能预计17年会不会考，要复习的全面一些。一些问答题都要结合历年真题自己根据课本进行总结。 王燕《时间序列分析》：这本书的编写就相对好得多，条理很清晰，思路引导很顺畅，一些例题也比较易懂，重点是各种预测描述模型，今年考了一道08年学硕考过的题：有趋势有季节变动可建立的模型，写出模型形式并简要说明。可见学硕的历年真题也是很有借鉴意义的。之前也考过差分运算的，复习的时候也要注意这种细节，但是这本书里面的例子特别好，几道题对应相应的知识点，只要你一点点看下来理解了，然后把笔记整理好，后期再背诵下，应该没啥问题。 何晓群《应用回归分析》:一直以为何晓群老师是个女老师，后来复试的时候才了解到并非如此。这本书写的也很有条理，多重共线性的后果诊断处理已经多次考到，自相关性和异方差还没出过，今年考了一个判定系数的解释，当时预测了几道觉得会考的题，里面就有判定系数和回归模型的综合评价，初试的时候就考到了，这个虽然比较简单，但可以尝试的方法就是在考试之前，自己预测一些题，自己给自己出题做，涵盖面广一些，会有意想不到的结果的。 关于真题：真题强调上百遍都不夸张，他对于你复习的方向有很大的启示作用。我当时的做法就是把真题整理成八个专题，分别是：《专题一：图表展示与概括性度量》、《专题二：统计量与抽样分布》、《专题三：参数估计与假设检验》、《专题四：分类数据分析》、《专题五：方差分析与实验设计》、《专题六：回归分析》、《专题七：时间序列分析》、《专题八：多元统计分析》，学硕和专硕的历年真题都要整理分类，基本上人大每年考的都包含在八个专题之间，你需要做的就是自己认认真真的从课本上找出答案来，然后总结一遍，一些学硕要求的比较偏数理的可以忽略，需要明确的是，重点一定会反反复复的考，而且乐此不疲，像今年时序和回归的题都是曾经考过的，几乎一模一样。 关于笔记：自己整理的笔记的字迹一定要清晰，条理要很清楚，但这是建立在你把书看了几遍理解透了之后才可以做到的事，当然一开始不理解，到后面反复的背诵就会逐渐清晰起来了。当时我是和真题一样分了八个专题，参照人大大数据陈思聪学长的笔记整理了手写的笔记，学长的笔记结构完整，内容完善，当时是如获至宝，每天看着它整理自己的笔记的心情相当愉悦，对我的专业课起到了至关重要的作用，在这里谢谢学长。在复习过程中，我发现自己常常会对知识的首次记忆有所偏颇，只知其一不知其二，以为已经完全理解了其确切的意思，但其实当我在复试复习的时候再回过头来看往往会有更多新奇的发现，此时的知识域相对来说也会完善一些。 关于背诵：心理学中有一个广为认可的记忆机制，即：我们在记忆的时候将许多线索（诸如对一个原理的发散性理解、当时联想的事物的多样性）一并编码进入记忆中，能否长时间的保持知识的新鲜感或者说在大脑中的活跃度，取决于这些线索是否足够丰富，这就为理解记忆提供了有力的证词。贯彻于专业课的背诵上，其实各个统计方法知识中包含了精确的概念、严谨的逻辑、一般的原则、生动的背景等无数的记忆线索，而并非是孤立的、任意的文本序列，各个点之间具有并列、递进、相互排斥的种种关系，推导和演绎出这种联系，从而由点到面，搭建成一个大的框架体系，就是我个人比较推崇的思维导图，如此进行下去到考研前几天可以看着那张大的框架图自己逐条背诵，口头表达可以和原文范本有出入，但是关键词必须要锁定，大致意思要接近。 其他：大家如果有专业课的问题，可以向我询问，我尽力解答。最近也在恶补专业知识，毕竟是跨专业，害怕一进人大就被各路大神碾压，大家互相学习吧，或许我的专业素养还比不上学弟学妹呢。和考研小伙伴一起建立了一个微信群，大家伙可以加进来在群里分享应用统计的资料、讨论复习过程中遇到的难题、分享考研路上的酸甜苦辣，啥啥啥都可以。因为微信群已满100人，可以加我的个人微信：zhanghua63170140，拉学弟学妹进群。 四、QA师妹皱着眉头问： 师哥好，我也想考人大统计，本科统计，但只是普通一本。旁边人都说人大太难考，因为我是师范学校，而且我们数科院好几年没有人考到人大，感觉挺迷茫的，也不知道该不该换个学校，但是总觉得不甘心，为什么别人能考上我不能啊？ 师兄皱着眉头答： 在现实世界中，我们的决策往往会倚赖过往的历史经验，就像你所述的，你所在的师范学校的数科院没有人考入过人大统计学院，看到这么惨淡的景象，畏惧心理在所难免，既然他们已经为你趟过这条深水，且已证明这不是一条容易的路，那为何我还要继续当做下一个被湍急的河流卷走的“微弱的个体”呢？且不说投入进去的时间成本以及其他一些不可控因素给自身带来难以计数的艰难险阻，万一这一年的所有努力在成绩出来的那一刹那都付之一炬，名校梦从而化为泡影，岂不是做了一次失败的买卖吗？ 可我想告诫你的却是，人大必须要去考，而且要义无反顾的前往，不要有所畏惧，从你的描述中得知并没有太多的现实因素的阻挠，你仅仅在惧怕强大而无耻的经验施加在你身上的不能承受的阻抗。先说说你会在这条幽深曲折的路上看到哪些曼妙的风景，你或许可以涉猎到从未踏入的知识盲区，当你被无数的知识模块所充盈，你会感受到这样的缓慢累积会给你带来前所未有的愉悦与渴求欲的满足，我们时代的知识分隔已经异常凸显出来了，每一个领域会将拥有一套成熟的体系，而当你掌控着庞大细密的知识网时，你便拥有了铠甲，他将带你在众人面前展示话语的力量，那种力量就是需要这些知识来支撑的。其次，你可能可以收获几个志同道合的朋友，网络的延伸将你的诉求与宣告呈现在他们面前，就像现在我正在尝试着与你促膝长谈一样，也必定会有无数的这样的人尝试着与你建立精神上的关联与挂钩，你们摸索着同一片黑夜，也凝望着同一片蓝天，为这笃定的信念挥汗。你要坚信，总有一天，你会与你精神气质相合的那些可爱的人相聚，就像家人一样聚在一起，所以，不要抗拒孤独，那仅仅是你还没有那样强烈的遇见。最后，也是最重要的，你将迎来新的人生，你说你想考人大统计，我相信你的内心必然会有一股洪荒之力在不断地催促着你，会有一种声音在耳边呼唤你，那便是我们最大的动力，这样的声音会在你颓然之时支起你的躯体，无论什么样的生活的贫乏无趣都驱散不了这种称之为信仰的东西，只要你持续地温存这样的声音，去战胜所有的困惑与不安，正是这种不甘让我们变成一个撑起自己所有维度的勇士。是的，你会变成一个勇士，即使被现实迫害的遍体鳞伤，你也依然可以坚毅地挺立原地，然后，舔舐自己的伤口，继续热烈地往前走。那种热烈，只能自己亲手栽培，别人无法给予，你也不能凭空取得，那是一个个白天黑夜的伏案所换来的最盛大的生命花园，你要在漫长的年华里种上玫瑰、植入梧桐、嵌进宝石，让它灿烂的更彻底点吧，即使荒败了，也要在极度的繁盛中逝去。 酷酷的师弟问： 师兄，我是跨专业，感觉对专业课比较迷惘，不知如何下手，可不可以建议一个比较摸得着套路的专业课复习方案呀？ 严肃的师兄答： 432统计学复习流程建议 贾俊平《统计学》第六版+圣才《贾俊平统计学 笔记与课后习题详解》：花费20天左右的时间进行全篇阅读，不遗漏任何一个点，包括概念、公式、解释、注释、表格、图片、例题，每一个字都要盯上至少一秒钟。每看完一章节的所有内容之后，在A4纸上写下课后思考题与练习题的答案，可以翻阅课本，但必须要自己动手整理归纳或者解答一遍，切记眼高手低，能写入教科书的例题就必然会有其存在的必要性，它可以帮助你梳理课本知识，也可以帮你抓住章节重点，整理这些问答题和计算题的过程也是再一次深入理解知识点的过程，绝不可废弃之。课后的思考题和练习题的答案可以在圣才出版的《贾俊平统计学 笔记与课后习题详解》找到，我只找到了第五版对应的（一共331页），已经上传到网盘里。大家也可以参考人大配套的学习指导书，网上可以买到。这样一遍下来对这本书的框架有一定的了解，强烈建议看完每一章节之后画一张框架结构图，理清知识脉络。重点章节是”第3章：数据的图表展示“、”第4章：数据的概括性度量“（这两个章节联合起来会在真题中考察一道大题）；第6章，统计量概念和中心极限定理是重点；第7-13章，所有的都是重点。第1、2、5、14章可以粗略看一下，非重点。 贾俊平《统计学》第二遍：第二遍依旧要有如第一遍的细致程度，且在第一遍的基础上加深理解，争取可以简单的使用自己的话简述一遍，同时要开始做笔记，按照书本的结构组织笔记，且必须要把这本笔记当做是一样艺术品，用心编排、用心写字、用心画图，重点分明、内容完整、结构清晰，切不可潦草糊弄过关，这本笔记是你在日后的复习中常常会碰面的，翻阅起来可以大大地提高效率，而且看起来愉悦舒心一目了然，何乐而不为？切勿盲目求快，做笔记是一个梳理知识点、更深入理解知识点的过程，欲速则不达，抄一遍了事对理解没有丝毫的助益，下笔之前想明白这句话所指涉的是什么、是否还存留我尚未领会的含义、我能否清晰的在脑海里梳理分析流程等等等，这些都可以增益你对细节的深入探索，慢工出细活，相信我，循序渐进的来，一定会有很大的成效。同时这一遍笔记要把管理学第四版中出现的新知识补充到笔记中，其中的新知识点包括：正态性的评估、实验设计、哑变量回归、非参数统计（注：时间序列分析和多元统计分析的部分内容会在其他两本书中会详细展开，不添加进去也无妨）。 《应用回归分析》：这本书囿于时间只看了前八章，也就是到主成分回归与偏最小二乘估计这一章为止，因为在《统计学》书中已经对这部分的内容有了基本的了解，加上《应用回归分析》书中的推导也不是特别艰深，基本上每一步思路都很清晰，大家可以尝试着推导一遍，加深理解，但这并不是重点，重点在于诸如违背回归方程基本假设条件的三种情况（异方差、自相关、多重共线性）的原因、影响、诊断、处理这类偏向论述、步骤与原理的知识点，所以，如果推导有困难，也不必强求。但其实后两章的非线性回归与定性变量回归模型也比较容易理解，虽然初试考察的概率不大，但为兼顾知识结构的完整性以及复试的时候有可能被老师问及，看一下肯定是有好处的。这本《应用回归分析》也是要看一遍，再做一遍笔记，做笔记的方法与上述一样，不再赘述。 《应用时间序列分析》：这本书的条理狠清晰，大致就是平稳时间序列分析、非平稳时间序列的确定性分析和非平稳时间序列的随机性分析三块内容，考试重点在于若干个时间序列分析模型的结构与性质，譬如AR模型、MA模型、ARMA模型、ARIMA模型、指数平滑法、分解模型、含哑变量的多元回归预测模型等等，要搞清楚每一种模型所适用的时间序列类型、模型中每一个参数代表的含义以及分析的思路与步骤，其他的重点包括差分运算、一些基础的概念等。最后一章考的概率不高，但时间序列最后一章内容在时序分析所占地位是很高的（虚假回归、单位根检验、单整与协整）要是想扩充知识点，，也建议看一遍。同样，整个过程也是看一遍书，整理一遍笔记。 《多元统计分析》：聚类分析、判别分析、主成分分析、因子分析、对应分析、典型相关分析必看，后几章个人认为考的几率不大，看个人时间分配。重点考点是这些多元统计分析方法的基本思想、过程细节、重点性质之类的，历年考过因子分析、判别分析、典型相关分析的相关内容，考的概率蛮大的，绝不能弃看。过程还是一样，第一遍，看书、理解、适当推导，第二遍，做笔记，再次理解，加深印象。 八个专题整理：这是学长根据历年真理的考题分布情况总结出来的八个专题，已经在上述的复习中有所呈现，依次是：“专题一：数据的图表展示与概括性度量”；“专题二：统计量与抽样分布”；“专题三：参数估计与假设检验”；“专题四：分类数据分析”；“专题五：方差分析与实验设计”；“专题六：相关分析与回归分析”；“专题七：时间序列分析”；“专题八：多元统计分析”。几乎每一年都是在这八个专题中抽取七个专题的知识点，例如2015年432真题的排布分别为：第一题属于专题一（数据的图表展示与概括性度量）、第二题属于专题四（分类数据分析）、第三题属于专题五（方差分析与实验设计）、第四题属于专题八（多元统计分析）、第五题属于专题三（参数估计与假设检验）、第六题属于专题六（相关分析与回归分析）、第七题属于专题七（时间序列分析）。还是比较有代表性的，其他年份的分布学弟学妹们也都可以总结一下规律，有助于自主预测考题。大家可以以这个思路去归纳整理自己最终的一份笔记，八个专题，每一个专题都要有结构框架，可以借助思维导图这个工具，每一个专题包括四个部分（第一部分：这一专题的课本内容有序的整理；第二部分：重要问答题整理；第三部分：属于这一专题的真题整理（每一道题的答案一定要完整有序地整理）；第四部分：这一专题的思维导图）。整理完这份专题笔记之后时间也就剩下一个月左右，接下来的时间就是背背背，当然要理解地去背，把笔记与思维导图结合起来，背到滚瓜烂熟，背到天昏地暗，到最后阶段会特别难熬，英语作文要背，政治大题要背，专业课要背，抗住压力就是了。 可爱的小师妹问：你的专业课思维导图咋搞咯？还有专业课的复习时间怎么分配？难点不懂怎么办？师兄你有笔记吗？： 依旧严肃的师兄答： 这八个专题的思维导图我已经整理完并放置在百度云群里了，这里给出百度云链接，人大432专业课思维导图 密码: pwu9。当然思维导图要随时自己更新，如果自己觉得需要补充的，可以在思维导图上添加。要想使这个思维导图的效用最大化，就得把框架熟记在心中，在答题的时候一定会有帮助的，会让你的答案有结构有条理，列点回答更加轻松自如，不知道怎么把题目答得全面闪亮？只需把思维导图的一个个点用书本的内容或者你整理的笔记来填充就好啦。不过，话说在前头，理解才是关键。它只是一个框架工具，核心在于知识点。 时间怎么分配呢？：基本上每天我都会花三个小时复习专业课，有的时候白天数学的任务没完成，也会适当压缩专业课的时间，大致的时间安排是《统计学》阅读及笔记25天、《应用回归分析》阅读及笔记20天、《应用时间序列分析》阅读及笔记20天、《多元统计分析》阅读及笔记20天、《八个专题整理》25天、背诵30天。 难点不懂咋办办？：在多元统计分析或者时序分析中会出现一些自己无法理解的地方，诸如推导过程和计算证明，这些确实不是432统计学的考察重点，但是如果不搞清楚这些，对知识点就会感觉隔着一层迷雾，无法透彻地解析整个过程总会叫人不爽快，不求甚解是深层次理解知识点的大敌。但是，时间所迫，实在搞不懂这些玩意儿咋办呢，那就只能退而求其次，可以大概的知道这个推导是在干什么以及它在整个过程中的作用。也足够应对432统计学了。 学长你有笔记吗？：哈哈哈，到最后了学长要黄婆卖瓜自卖自夸了，简要说下学长的专业课笔记，笔记分为七个部分其中前六份都是亲手整理的，就是依据上述的复习过程一步步整理下来，并且经过了精心的排版，保证大家的用户体验一级棒。如果想深入了解资料的细节，可以私聊学长，随时等候你的到来。欢迎添加个人微信：zhanghua63170140 五、写在最后这一路下来，感谢的人很多，人大陈思聪学长（见过学长本人，沉稳贴心哈哈，考研的时候不会的题去问他都会耐心解答），小杰克学长（这个群真的建设的太赞了，复试的资料是向学长要的，帮助很大），老胡学长（最近给我发了好几本砖头书，全英文版，据说是装逼神器），以及在中海洋读研的王淼淼童鞋（跨专业考统计真心不容易，磕磕碰碰了很多次，王淼淼鼎力相助；我后期整理了一个专业课思维导图，也是王淼淼给我的灵感），在幼儿园种花种草的冯涵小朋友，三个考研小伙伴大象、赫姐还有小姨妈（每次拿小姨妈开玩笑真的屡试不爽），还有强悍的北大444哥（他的经验贴真的是棒呆）。最后想感谢的是我的女朋友，她脸上的笑容总能够化解我的忧愁与不安，每次复习到夜深，想起在这座城市的另一边有那么一个人与我一同牵手向前，就会充满力量。 在本科期间，我们所接受的更多的是老师所教授的知识，而到了研究生期间，我们要准备开始制造新的知识，更高层次的目标便是对人类普遍的知识有所贡献，而达到如此境界的来源正是你对知识的渴求与不断地追索，你不再满足于单一的知识面，而渴望搭建更坚固的知识架构。我们不能功利的对待这场磨练你意志的战役，不能仅仅把它看成是获取更多外在利益的工具，虽然确实可以达到这样的效果，但这样的动力绝对不会持久的催促你往更高的知识领域探索。 很多人在中途放弃或是马马虎虎的应付，就是自己内心缺乏行动的信念支撑，倘若只是觉得考上研就一劳永逸了，就没有必要花费那么厚重的时间成本了，因为，考上研只是一个起点，更艰难的路，在前方。 祝愿学弟学妹可以在考研路上发现更多的风景，顺利考上人大。]]></content>
      <categories>
        <category>日知录</category>
      </categories>
      <tags>
        <tag>人大应统</tag>
        <tag>考研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（16）：统计学习概论]]></title>
    <url>%2F2017%2F03%2F20%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8816%EF%BC%89%EF%BC%9A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[一、统计学习1.1 特点统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。赫尔伯特·西蒙曾对学习定义为：“如果一个系统能够执行某个过程改进它的性能，这就是学习。”按照这一观点，统计学习就是计算机系统通过数据及统计方法提高系统性能的机器学习。 1.2 对象统计学习的对象是数据，从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到对数据的分析与预测中去。数据包括存在于计算机及网络上的各种数字、文字、图像、视频、音频及它们的组合。统计学习对于数据的基本假设是同类数据具有一定###的统计规律性。 1.3 目的考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测与分析，同时也要尽可能地提高学习效率。 1.4 方法统计学习由监督学习（supervised learning）、非监督学习（unsupervised learning）、半监督学习（semi-supervised learning）、强化学习（reinforcement learning）等组成 监督学习：从给定的、有限的、用于学习的训练数据（training data）集合出发，假设数据独立同分布，并且假设要学习的模型属于某个函数的集合，称为假设空间（hypothesis space），应用某个评价准则（evaluation criterion），从假设空间中选取一个最优的模型，使它对已知训练数据及未知测试数据在给定的评价准则下有最优的预测；最优模型的选取由算法实现。 三要素：模型的假设空间（模型）、模型选择的准则（策略）、模型学习的算法（算法） 步骤： 1）得到一个有限的训练数据集合 2）确定包含所有可能的模型的假设空间，即学习模型的集合 3）确定模型选择的准则，即学习的策略 4）实现求解最优模型的算法，即学习的算法 5）通过学习方法选择最优模型 6）利用学习的最优模型对新数据进行预测或分析 二、监督学习2.1 定义监督学习的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做一个好的预测。它从训练数据（training data）集合中学习模型，对测试数据（test data）进行预测。 2.2 基本概念2.2.1 输入空间、特征空间与输出空间 输入空间与输出空间：输出与输出所有可能值的集合。通常输出空间远远小于输入空间 特征空间：所有特征向量存在的空间。特征空间的每一维对应于一个特征。模型都定义在特征空间上。 输入实例$x$的特征向量： x=\left(x^{\left(1\right)},x^{\left(2\right)},···,x^{\left(i\right)},···,x^{\left(n\right)}\right)^T其中$x^{(i)}$表示$x$的第$i$个特征，$x_i$表示多个输入向量的第$i$个，即 x_i=\left(x_i^{\left(1\right)},x_i^{\left(2\right)},···,x_i^{\left(n\right)}\right)^T 训练数据和测试数据由输入输出对（即样本）组成，通常表示为： T=\left\{\left(x_1,y_1\right),\left(x_2,y_2\right),···,\left(x_N,y_N\right)\right\} 回归问题：输入变量与输出变量均为连续变量的预测问题。 分类问题：输出变量为有限个离散变量的预测问题。 标注问题：输入变量与输出变量均为变量序列的预测问题。 2.2.2 联合概率分布统计学习假设数据存在一定的统计规律，监督学习的基本假设为$X$和$Y$具有联合概率分布的假设，我们把训练数据与测试数据看作是依联合概率分布$P(X,Y)$独立同分布产生的。 2.2.3 假设空间监督学习的目的在于找到由输入到输出的映射模型集合中最好的一个。这个集合即假设空间。模型可以是概率模型或非概率模型，由条件概率分布$P(Y|X)$或决策函数$Y=f(X)$表示。 三、统计学习三要素3.1 模型模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的条件概率分布（概率模型）或决策函数（非概率模型）。假设空间用$\mathscr{F}$表示，它是由一个参数向量决定的决策函数族： \mathscr{F}=\left\{f\ |\ Y=f_{\theta}\left(X\right),\theta\in R^n\right\}参数向量$\theta$取值于$n$维欧式空间$R^n$，称为参数空间。也可以是一个参数向量决定的条件概率分布族： \mathscr{F}=\left\{P\ |\ P_{\theta}\left(Y|X\right),\theta\in R^n\right\}3.2 策略有了模型的假设空间，接下来需要考虑按照什么样的准则学习或选择最优的模型。 3.2.1 损失函数和风险函数损失函数（loss function）度量一次预测的好坏。损失函数越小，模型就越好常用的损失函数有： 0-1损失函数（0-1 loss function）： L=\left\{\begin{matrix}{} 1& Y\ne f\left(X\right)\\ 0& Y=f\left(X\right)\\ \end{matrix}\right. 平方损失函数（quadratic loss function）: L=\left(Y-f\left(X\right)\right)^2 绝对损失函数（absolute loss function）： L=|Y-f\left(X\right)| 对数损失函数（logarithmic loss function）： L=-\log P\left(Y|X\right) 风险函数（risk function）或期望损失（expected loss）是理论上模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失。 R_{\exp}\left(f\right)=E_p\left[L\left(Y,f\left(X\right)\right)\right]=\int_{}{L\left(y,f\left(x\right)\right)P\left(x,y\right)dxdy}我们学习的目标就是选择期望风险最小的模型。由于联合分布$P(X,Y)$未知，风险函数不能直接计算。这样，一方面根据期望风险最小学习模型要用到联合分布，另一方面联合分布又是未知的，所以监督学习就沦为病态问题。但我们可以计算训练数据集的平均损失，即经验风险（empirical risk）或经验损失（empirical loss）： \textrm{R}_{emp}\left(f\right)=\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}根据大数定理，当样本容量$N$趋于无穷时，经验风险趋于期望风险。自然而然想到可以使用经验风险来估计期望风险。但现实中训练样本数目很小，这种估计往往不理想，需要矫正，以此引出经验风险最小化和结构风险最小化。 3.2.2 经验风险最小化和结构风险最小化经验风险最小化（empirical risk minimization）认为经验风险最小的模型是最优的模型，即求解最优化问题： \underset{f\in\mathscr{F}}{\min}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}当样本容量足够大的时候，经验风险最小化学习效果良好。比如极大似然估计，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。 但是当样本容量很小时，经验风险最小化学习会产生过拟合（over-fitting）的现象。这就引出了结构风险最小化，它等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term），它的定义为： R_{srm}\left(f\right)=\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)其中$J(f)$为模型的复杂度，模型$f$越复杂，复杂度$J(f)$就越大；反之，模型越简单，复杂度$J(f)$就越小，即复杂度表示了对复杂模型的惩罚。$\lambda≥0$是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险和模型复杂度同时小。结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。比如贝叶斯估计中的最大后验概率估计就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。结构风险最小化的策略认为结构风险最小的模型是最优的模型，求解最优模型即求解最优化问题： \min_{f\in\mathscr{F}}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)这样，监督学习问题变成了经验风险或结构风险函数的最优化问题。 3.3 算法学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优化。如何用数值计算求解，如何保证找到全局最优化，并使求解过程高效，是一个重要的问题。 四、模型评估与模型选择4.1 训练误差与测试误差训练误差（training error）是模型关于训练数据集的平均损失： R_{emp}\left(\hat{f}\right)=\frac{1}{N_1}\sum_{i=1}^{N_1}{L\left(y_i,\hat{f}\left(x_i\right)\right)}测试误差(test error)是模型关于测试数据集的平均损失： R_{emp}\left(\hat{f}\right)=\frac{1}{N_2}\sum_{i=1}^{N_2}{L\left(y_i,\hat{f}\left(x_i\right)\right)}测试误差反映了学习方法对未知的测试数据集的预测能力，即泛化能力。 4.2 过拟合与模型选择我们希望选择或学习一个合适的模型。若在空间中存在“真模型”，那我们所选择的模型要与真模型的参数个数相同，所选择的模型的参数向量与真模型的参数向量相近。 过拟合指的是我们以为追求提高模型对训练数据的预测能力，所选模型的复杂度往往会比真模型更高。即学习时选择的模型所包含的参数过多，以致于出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。 模型选择旨在避免过拟合并提高模型的预测能力，模型选择时，不仅要考虑对已知数据的预测能力，而且还要考虑对未知数据的预测能力。下图描述了训练误差和测试误差与模型的复杂度之间的关系： 当模型复杂度增大时，训练误差会逐渐减小并趋于0；而测试误差会先减小，达到最小值后又增大。当选择的模型复杂度过大时，过拟合现象就会发生。所以要选择复杂度适当的模型，已达到测试误差最小的目的。以此引出正则化与交叉验证。 五、正则化与交叉验证5.1 正则化5.1.1 定义模型选择的典型方法是正则化（regularzation）。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如，正则化项可以是模型参数向量的范数。它的一般形式如下： \min_{f\in\mathscr{F}}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)第一项是经验风险，第二项是正则化项，$\lambda≥0$为调整两者之间关系的系数。 5.1.2 不同形式正则化项可以取不同的形式。例如，回归问题中，损失函数是平方误差，正则化项可以是参数向量的$L_2$范数: L\left(w\right)=\frac{1}{N}\sum_{i=1}^N{\left(f\left(x_i;w\right)-y_i\right)^2}+\frac{\lambda}{2}||w||^2也可以是参数向量的$L_1$范数： L\left(w\right)=\frac{1}{N}\sum_{i=1}^N{\left(f\left(x_i;w\right)-y_i\right)^2}+\lambda ||w||_1第一项的经验风险较小的模型可能较复杂（有多个非零参数），这时第二项的模型复杂度会较大。正则化的作用是选择经验风险与模型复杂度同时较小的模型。 5.1.3 奥卡姆剃刀正则化符合奥卡姆剃刀原理，应用于模型选择时变为：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型。从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。可以假设复杂的模型有很小的先验概率，简单的模型有较大的先验概率。 5.2 交叉验证5.2.1 定义如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别是训练集（training set）用来训练模型、验证集（validation set）用于模型的选择、测试集（test set）用于最终对学习方法的评估，最终选择对验证集有最小预测误差的模型。 但是实际应用中数据不充足，所以我们采用交叉验证，它的基本思想是重复的使用数据，把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试和模型选择。 5.2.2 方法 简单交叉验证：首先随机地将已给数据分为两个部分，一部分作为训练集（70%），另一部分作为测试集（30%）；然后用训练集在各种条件下（如不同的参数个数）训练模型，从而得到不同的模型；在测试机上评价各个模型的测试误差，选出测试误差最小的模型。 S折交叉验证（S-fold cross validation）：应用最广泛。首先随即将已给数据切分为S个互不相交的大小相同的子集；然后利用S-1个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的S种选择重复进行；最后选出S次评测中平均测试误差最小的模型。 留一交叉验证leave-one-out cross validation）：S折交叉验证的特殊情形是S=N（N为给定数据集的容量），往往在数据缺乏的情况下使用 六、泛化能力6.1 泛化误差泛化能力是指由该方法学习到的模型对未知数据的预测能力。现实中常常通过测试误差来评价学习方法的泛化能力，但因为测试数据及有限，评价结果不一定可靠。 理论上，通过泛化误差来反映学习方法的泛化能力，泛化误差即用学习到的模型对未知数据预测的误差： R_{\exp}\left(f\right)=E_p\left[L\left(Y,f\left(X\right)\right)\right]=\int_{}{L\left(y,f\left(x\right)\right)P\left(x,y\right)dxdy}泛化误差越小，模型效果就好。泛化误差就是所学习到的模型的期望风险。 七、生成模型与判别模型7.1 判别模型判别模型由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型。它关心的是对给定的输入$X$，应该预测什么样的输出$Y$。典型的判别模型包括：K近邻法、感知机、决策树、逻辑斯谛回归、最大熵模型、支持向量机、提升方法、条件随机场。 判别方法的特点： 直接学习的是条件概率$P(Y|X)$或决策函数$f(X)$，直接面对预测，往往学习的准确率很高； 由于直接学习$P(Y|X)$或$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。 7.2 生成模型生成模型由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型： P\left(Y|X\right)=\frac{P\left(X,Y\right)}{P\left(X\right)}因为模型表示了给定输入$X$产生输出$Y$的生成关系，所以被称为生成模型。典型的生成模型有：朴素贝叶斯、隐马尔科夫模型 生成方法的特点： 生成方法可以还原出联合概率分布$P(X,Y)$，而判别方法不能； 生成方法的学习收敛速度快，即当样本容量增加时，学到的模型可以很快收敛于真实模型； 当存在隐变量时，仍可以用生成方法学习，此时判别方法不能用。 八、分类问题8.1 定义在监督学习中，当输出变量$Y$取有限个离散值时，预测问题便成为分类问题。它从数据中学习一个分类模型或分类决策函数，即学习一个分类器，然后对新的输入进行输出的预测，即进行分类。分为多类分类和二类分类问题。 8.2 学习过程如图所示，分类问题包括学习和分类两个过程。在学习过程中，根据已知的训练数据集利用有效的学习方法学习一个分类器；在分类过程中，利用学习的分类器对新的输入实例进行分类。 8.3 分类准确率8.3.1 定义评价分类器性能的指标一般是分类准确率（accuracy）,即对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。也即损失函数是0-1损失时测试数据集上的准确率。 8.3.2 常用指标通常将关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作：TP（正类预测为正类数）、FN（正类预测为负类书）、FP（负类预测为正类数）、TN（负类预测为负类数） 精确率： P=\frac{TP}{TP+FP} 召回率： R=\frac{TP}{TP+FN} $F_1$值，是精确率和召回率的调和均值，即 \frac{2}{F_1}=\frac{1}{P}+\frac{1}{R} F_1=\frac{2TP}{2TP+FP+FN}精确率与召回率都很高时，$F_1$值也会很高。 8.3.3 分类方法与应用 常见分类统计方法：K近邻、感知机、朴素贝叶斯、决策树、决策列表、逻辑斯谛回归、支持向量机、提升、贝叶斯网络、神经网络Winnow 应用：银行业务构建客户分类模型，对客户按照贷款风险大小分类；网络非法入侵检测；人脸是否出现的检测；网页分类；文本分类等。 九、标注问题9.1 定义标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。它的目的在于学习一个模型，使它能够对观测序列给出标记序列作为预测。标注问题分为学习和标注两个过程： 9.2 应用标注常用的统计学习方法有：隐马尔科夫模型、条件随机场 它在信息抽取、自然语言处理领域被广泛应用。 自然语言处理的词性标注：给定一个由单词组成的句子，对这个句子中的每一个单词进行词性标注，即对一个单词序列预测其对应的词性标记序列。 十、回归问题10.1 定义回归模型表示输入变量和输出变量之间映射的函数，等价于函数拟合：选择一条函数曲线使其很好地拟合已知数据且很好地预测未知数据。可分为一元回归和多元回归，线性回归和非线性回归。它最常用的损失函数为平方损失函数，可以用最小二乘法求解。回归问题分为学习和标注两个过程： 10.2 应用股价预测：将影响股价的信息视作自变量，将股价视为因变量，将过去的数据作为训练数据，学习一个回归模型，并对未来的股价进行预测。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学习概论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（15）：EM算法]]></title>
    <url>%2F2017%2F03%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8815%EF%BC%89%EF%BC%9AEM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[期望最大值（Expectation Maximization，简称EM算法）是在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐藏变量。其主要思想就是通过迭代来建立完整数据的对数似然函数的期望界限，然后最大化不完整数据的对数似然函数。本文将尽可能详尽地描述EM算法的原理。并结合高斯混合模型介绍EM算法是如何求解的。 一、定义EM算法是一种迭代算法，用于含有隐变量（hidden variable）的改了吧模型参数的极大似然估计或极大后验概率估计。EM算法的每次迭代由两步组成：E步-求期望（expectation）；M步-求极大（maximization）。故称为期望极大算法（expectation maximization），简称EM算法。 二、Jensen不等式设$f$是定义域为实数的函数，如果对于所有的实数$x$，$f^{‘’}(x)≥0$，那么$f$是凸函数。当$x$是向量时，如果其$hessian$矩阵$H$是半正定的即$H≥0$，那么$f$是凸函数。如果$f^{‘’}(x)&gt;0$或$H&gt;0$，那么称$f$是严格凸函数。 $Jensen$不等式表述如下： 如果$f$是凸函数，$x$是随机变量，那么：$E[f(x)]≥f(E[x])$。特别地，如果$f$是严格凸函数，$E[f(x)]≥f(E[x])$，那么当且仅当$P(x=E[x])=1$(也就是说$x$是常量)，$E[f(x)]=f(E[x])$; 如果$f$是凹函数，$x$是随机变量，则$E[f(x)]≥f(E[x])$。当$f$是（严格）凹函数当且仅当$-f$是（严格）凸函数。 通过下面这张图，我们可以加深理解： 上图中，函数$f$是凸函数，$X$是随机变量，有0.5的概率为$a$，有0.5的概率是b（就像抛硬币一样）。$X$的期望值就是a和b的中值了，图中可以看到$E[f(x)]≥f(E[x])$成立。 三、EM思想3.1 极大似然估计EM算法推导过程中，会使用到极大似然估计参数。 极大似然估计是一种概率论在统计学的应用。已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察结果，利用结果推出参数的大概值。极大似然估计建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。 这里再给出求极大似然估计值的一般步骤： 1）写出似然函数； 2）对似然函数取对数，并整理； 3）求导数，令导数为0，得到似然方程； 4）解似然方程，得到的参数即为所求； 关于极大似然估计的实例，可以参考wikipedia最大似然估计条目 3.2 EM算法思想下面介绍EM算法的思想： 给定的训练样本是$x^{(1)}，x^{(2)}，···，x^{(m)}$，样例间相互独立，但每个样本对应的类别$z^{(i)}$是未知的，也即隐含变量。我们想找到每个样例隐含的类别$z$，能使得$P(x,z)$最大。$P(x,z)$的最大似然估计如下： l\left(\theta\right)=\sum_{i=1}^m{\log p\left(x;\theta\right)}=\sum_{i=1}^m{\log\sum_z{}p\left(x,z;\theta\right)}第一步是对极大似然函数取对数，第二步是对每个样本实例的每个可能的类别$z$求联合分布概率之和。但是直接求$\theta$一般比较困难，因为有隐藏变量$z$存在，如果$z$是一个已知的数，那么使用极大似然估计来估算会很容易。在这种$z$不确定的情形下，EM算法就派上用场了。 EM算法是一种解决存在隐变量优化问题的有效方法。对于上述情况，由于存在隐变量，不能直接最大化$l(\theta)$，我们可以不断地建立$l$的下界（E步），然后优化下界（M步），依次迭代，直至算法收敛到局部最优。这就是EM算法的核心思想，简单的归纳一下： EM算法通过引入隐变量，使用MLE进行迭代求解参数。通常引入隐含变量后会有两个参数，EM算法首先会固定其中的第一个参数，然后使用MLE计算第二个变量值；接着通过固定第二个变量，再使用MLE估计第一个变量值，依次迭代，直至收敛到局部最优解。 四、EM推导下面来推导EM算法： 对于每一个样例$i$，让$Q_i$表示该样例隐含变量$z$的某种分布，$Q_i$满足的条件是 \sum_z{Q_i\left(z\right)=1\ \\ Q_i\left(z\right)\geqslant 0}（如果$z$是连续的，那么$Q_i$是概率密度函数，需要将求和符号换做积分符号）。比如要将班上学生聚类，假设隐藏变量$z$是身高，那么就是连续的高斯分布。如果是按照隐藏变量是男女，那么就是伯努利分布。 可以由前面阐述的内容得到下面的公式： \sum_i{\log p\left(x^{\left(i\right)};\theta\right)=\sum_i{\log\sum_{z^{\left(i\right)}}{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}}} ········（1） =\sum_i{\log\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}} ········（2） \geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\left(\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}\right)}}······（3）上面三个式子中，式（1）是根据联合概率密度下某个变量的边缘密度求解的（这里把$z$当做是随机变量）。对每一个样本$i$的所有可能类别求等式右边的联合概率密度函数和，也就是得到等式左边为随机变量$x$的边缘概率密度。由于对式（1）直接求导非常困难，我们可以做一个简单的变化，将其分子分母都乘以一个相等的函数$Q_i(Z^{(i)})$，得到式（2）。那么如何从式（2）推导出式（3）呢，这就需要用到之前提到的Jensen不等式。 以下为具体的分析过程： 首先，把（1）式中的$log$函数看成是一个整体，即令$f(x)=log(x)$，因为$(log(x))^”=-1/x^{2}&lt;0$，根据定理可知其为凹函数。 再根据凹函数的Jensen不等式：$f(E[X])&gt;=E[f(x)]$。 到这里，我们可以观察到，在式（2）中，当把$log(x)$看成$f(x)$时，后边的 \sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}其实就是可以类比为离散型随机变量的期望公式。具体的求解可以参照下图中离散型随机变量的期望公式。 我们可以把$Q_i^{(z^{(i)})}$看成是相应的概率$p_i$，把 \frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}看作是$z^{(i)}$的函数$g(z)$，根据期望公式$E\left[g\left(x\right)\right]=\sum_{i=1}^{\infty}{g\left(x_i\right)·p_i}$可以得到： E\left(\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z\right)}\right)=\sum_{z^{\left(i\right)}}{Q_i\left(z\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z\right)}}把上述根据Jensen不等式整合到一起得到： f\left[E\left(g\left(X\right)\right)\right]=\log\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}} \geqslant E\left[f\left(g\left(X\right)\right)\right]=\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}这样我们就得到了式（3）。 现在我们把式（2）和式（3）的不等式次而成：似然函数$L(\theta)≥J(z,Q)$的形式，其中$z$为隐变量，那么我们可以通过不断地最大化$J$的下界，来使得$L(\theta)$不断提高，最终达到它的最大值。借助下图来解释下这个过程： 首先我们固定$\theta$，调整$Q(z)$使下界（绿色曲线）$J(z,Q)$沿着绿色虚线上升至与$L(\theta)$在此点$\theta$处相等（绿色曲线至蓝色曲线），然后固定$Q(z)$，调整$\theta$使下界$J(z,Q)$达到最大值($\theta {_t}$至$\theta _{t+1}$)，然后再固定$\theta$，调整$Q(z)$…….直到收敛到似然函数$L(\theta)$的最大值处的$\theta^*$ 这里有两个问题： 什么时候下界$J(z,Q)$与$L(\theta)$在此点$\theta$处相等？ 为什么一定会收敛？ 首先来解释下第一个问题。在Jensen不等式中说到，当自变量$X=E(X)$时，即为常数的时候，等式成立。而在这里，为： \frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}=c对该式做个变换，将分母移到等号右边，并对所有的$z$求和，得到第一个等号；又因为前面提到的$\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)=1}$得到第二个等号。 \sum_{z^{\left(i\right)}}{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}=\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)c}=c根据上面两个式子可以得到 Q_i\left(z^{\left(i\right)}\right)=\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{\sum_z{p\left(x^{\left(i\right)},z;\theta\right)}} \\\\\ =\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{p\left(x^{\left(i\right)};\theta\right)} \\\\ =p\left(z^{\left(i\right)}|x^{\left(i\right)};\theta\right)到这里，我们推出了在固定参数$\theta$后，使下界拉升的$Q(z)$的计算公式就是后验概率（条件概率），解决了$Q(z)$如何选择的问题。此步就是EM算法的E步，目的是建立$L(\theta)$的下界。接下来的M步，目的是在给定$Q(z)$后，调整$\theta$，从而极大化$L(\theta)$的下界$J$（在固定$Q(z)$后，下界还可以调整的更大）。那么一般的EM算法的步骤如下： 第一步：初始化分布参数$\theta$； 第二步：重复E步和M步直到收敛： E步：根据参数的初始值或上一次迭代的模型参数来计算出的因变量的后验概率（条件概率），其实就是隐变量的期望值，来作为隐变量的当前估计值： \\\\ Q_i\left(z^{\left(i\right)}\right)=p\left(z^{\left(i\right)}|x^{\left(i\right)};\theta\right) M步：最大化似然函数从而获得新的参数值： \theta :=arg\underset{\theta}{\max}\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}} 通过不断地迭代，然后就可以得到使似然函数$L(\theta)$最大化的参数$\theta$了。 接下来我们看第二个问题。上面多次说到直到收敛，那为什么一定会收敛呢？证明如下： 假定$\theta^{(t)}$和$\theta^{(t+1)}$是EM第t次和t+1次迭代后的结果。如果我们证明了$l(\theta^{(t)})≤l(\theta^{(t+1)})$，也就是说极大似然估计单调增加，那么最终我们就会得到极大似然估计的最大值。 下面来证明，选定$\theta^{(t)}$后，我们得到E步： \\\\ Q_i^{(t)}\left(z^{\left(i\right)}\right)=p\left(z^{\left(i\right)}|x^{\left(i\right)};\theta\right)这一步保证了在给定$\theta^(t)$时，Jensen不等式中的等式成立，也就是 l\left(\theta^{\left(t\right)}\right)=\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}然后进行M步，固定$Q_i^{(t)}(z^{(i)})$，并将$\theta^{(t)}$视作变量，对上面的$l(\theta^{(t)})$求导后，得到$\theta^{(t+1)}$,这样经过一些推导会有以下式子成立： l\left(\theta^{\left(t+1\right)}\right)\geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t+1\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}······\textrm{（4）} \geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}······\textrm{（5）} =l\left(\theta^{\left(t\right)}\right)······\textrm{（6）}解释第（4）步，得到$\theta^{(t+1)}$时，只是最大化$l(\theta^{(t)})$，也就是$l(\theta^{(t+1)})$的下界，而没有使等式成立，要想使等式成立只有在固定$\theta$，并按E步得到$Q_i$时才能成立。况且根据我们前面得到的下式，对于所有的$Q_i$和$\theta$都成立 l\left(\theta^{\left(t\right)}\right)\geqslant\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta^{\left(t\right)}\right)}{Q_i\left(z^{\left(i\right)}\right)}}}第（5）式利用的M步的定义，M步就是将$\theta^{(t)}$调整到$\theta^{(t+1)}$，使得下界最大化。这样（5）、（6）就都证明成立了。 再结合之前那个图解释一下这几步推导： 首先（4）对所有的参数都满足，而其等式成立条件只是在固定$\theta$，并调整好$Q$时成立，而第（4）步只是固定$Q$，调整$\theta$，不能保证等式一定成立。对应到图上就是蓝色曲线的峰值与$l(\theta^{(t+1)})$的关系，要使它们相等还必须要固定$\theta$，调整好$Q$；（4）到（5）就是M步的定义，也就是固定$Q$，调整$\theta^{(t)}$至$\theta^{(t+1)}$，对应到图上即为蓝色曲线与红色曲线交点处至蓝色曲线峰值。（5）到（6）是前面E步所保证等式成立条件。也就是说E步会将下界拉到与$l(\theta)$一个特定值（这里为$\theta^{(t)}$）一样的高度，而此时发现下界仍然可以上升，因此经过$M$步后，下界又被拉升，但达不到与$l(\theta)$另外一个特定值（$\theta^{(t+1)}$）一样的高度，之后E步又将下界拉到了与这个特定值一样的高度，循环往复，直到达到最大值。 这样就证明了$l(\theta)$会单调增加。如果要判断收敛情况，可以这样来做：一种收敛方法是$l(\theta)$不再变化，还有一种就是变化幅度很小，即根据$l(\theta^{(t+1)})=l(\theta^{(t)})$的值来决定。 从前面的推导中我们知道$l(\theta)≥J(Q,\theta)$，EM也可以看做是$J$的坐标上升法，如下图所示： 图中的直线式迭代优化的路径，可以看到每一步都会向最优值前进一步，而且前进路线是平行于坐标轴的，因为每一步只优化一个变量。这犹如在x-y坐标系中找一个曲线的极值，然而曲线函数不能直接求导，因此什么梯度下降方法就不适用了。但固定一个变量后，另外一个可以通过求导得到，因此可以使用坐标上升法，一次固定一个变量，对另外的求极值，最后逐步逼近极值。对应到EM上，E步：固定θ，优化Q；M步：固定Q，优化θ；交替将极值推向最大。 五、EM的应用：混合高斯模型待补充 六、EM的应用：EM聚类以下的聚类图来自维基百科，可以生动的看出 待补充 七、参考资料The EM Algorithm混合高斯模型和EM算法cs229-notes8]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>EM算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（14）：关联分析]]></title>
    <url>%2F2017%2F03%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8814%EF%BC%89%EF%BC%9A%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[一、关联分析1.1 引言在数据挖掘与机器学习中，关联规则（Association Rules）是一种较为常用的无监督学习算法，与分类、聚类等算法不同的是，这一类算法的主要目的在于发掘数据内在结构特征之间的关联性。 简单一点来说，就是在大规模的数据集中寻找一些有意义有价值的关系。有了这些关系，一方面，可以帮助我们拓宽对数据及其特征的理解；另一方面，则可以实现推荐系统的构建与应用（例如购物篮分析等）。 在对关联规则有了基本的认识后，我们对其进行进一步的细分，以日常生活中的关联性举例，在逛超市的顾客中，购买面包的人很大程度上会购买牛奶，这一类的关联性被称为简单关联规则；再例如，购买汽车遮阳板的很多顾客会在近期内购买零度玻璃水，这样的事例不仅反映了事物间的关联关系，而且还具有时间上的先后顺序，因此这一类的关联性被称为序列关联规则。 广义上的关联规则包含了简单关联和序列关联，接下来我们分别对这两块知识进行深入学习。 1.2 简单关联规则初探首先我们需要明确关联分析中的一些基本概念： 事务：指关联分析中的分析对象，我们可以把它理解成为一种宽泛行为（例如顾客的一次超市购买行为，电脑的使用者的一次网页浏览行为等都可以称之为事务），由事务标识（TID）与项目集合组成。 项集：即事务中的一组项目的集合，单个的项目可以是一种商品、一个网页链接等。假设$X$为项集，$I$为项目全体且$I=\{i_1,i_2,···,i_n\}$，那么项集$X\subseteq I$。进一步的，如果$X$中包含$p$个项目，则称该项集为$p-$项集。以上图为例，这里包含了4个事务，$I$包含了5个项目。对于第一个事务而言，由于$X$包含了三个项目，所以该$X$是一个$3-$项集。明确了基本概念后，接下来学习关联规则的一般表现形式 X\rightarrow Y\left(S=s\%,C=c\%\right)其中： $X$和$Y$分别为规则的前项和后项，前项为项目或项集，后项表示某种结论或事实。 $S=s\%$表示规则支持度为$s\%$，$C=c\%$表示规则置信度为$c\%$ 到这里大家可能会疑惑，直接得到关联规则不就可以了吗？为什么要在结论中加入支持度和置信度呢？这就涉及到关联分析中非常重要的一块内容——有效性的判别 1.3 简单关联规则的有效性实际上，在数据中使用关联分析进行探索时，我们可以找出很多关联规则，但并非所有的关联规则都是有效的，有的可能令人信服的程度并不高，也有的可能适用范围很有限，带有这些特征的所谓“关联规则”，我们则称之为不具有“有效性”。判断一条关联规则是否有效，需要用到以下两大测度指标，即规则置信度与规则支持度。 1.规则置信度（Confidence）置信度是对简单关联规则准确度的测量，定义为包含项目$A$的事务中同时也包含项目$B$的概率，数学表述为： Confidence\left(A\rightarrow B\right)=P\left(B|A\right)=\frac{P\left(AB\right)}{P\left(A\right)}置信度的本质就是我们所学过的条件概率，置信度越高，则说明$A$出现则$B$出现的可能性也就越高。假设在电脑$\rightarrow$杀毒软件的关联规则中，置信度$C=60\%$，表示购买电脑的顾客中有$60\%$的顾客也购买了杀毒软件。 2.规则支持度（Support） 支持度测量了简单关联规则应用的普适性，定义为项目$A$与项目$B$同时出现的概率，数学表述为： Support\left(A\rightarrow B\right)=P\left(B\cap A\right)=P\left(AB\right)假设某天共有100个顾客到商场购买物品，其中有10个顾客同时购买了电脑和杀毒软件，那么上述关联规则的支持度就为10%，同样，支持度越高，表明某一关联规则的适用性就越大。 一个有效的简单关联规则，势必同时具有较高的置信度与支持度。因为，如果支持度较高而置信度较低，则证明规则的可信度差；而相反，如果支持度较低而置信度较高，则说明规则的应用范围较小。 举例来说，假设在1000个顾客购买行为的事务中，只有一个顾客购买了烧烤炉，同时也只有他购买了碳，虽然规则“烧烤炉$\rightarrow$碳”的置信度很高，为100%，但支持度仅有0.1%，说明这条规则缺乏普遍性，应用价值不高。 所以一个有效的关联规则，必须具有较高的置信度与支持度，那么在实际应用中，我们就需要给定最小的置信度$C_{min}$与支持度$S_{min}$，只要同时大于$C_{min}$和$S_{min}$的规则，我们才可以将其定义为是“有效”的。 1.4 简单关联规则的实用性在对关联规则的有效性有一个基本的掌握后，我们在此基础上进行进一步的探讨——关联规则的实用性。 关联规则的实用性主要体现在以下两个方面： 1）是否具有实际意义。例如“怀孕$\rightarrow $女性”的关联规则就没有实用价值。 2）是否具有指导意义，即帮助我们在现有的基础上做出有价值的优化。 对第二点进一步展开说明，假设“牛奶$\rightarrow $男性顾客（$S=40\%，C=40\%$）”在$C_{min}$和$S_{min}$均为20%时是一条有效规则时，如果进一步计算发现顾客中男性的比例也为40%，也就是说购买牛奶的男性顾客等于所有顾客中的男性比例，那么这条规则就是一条前后项无关的随机性关联，因此它就没有有意义的指导信息，不具有实用性。 如何衡量关联规则具有实用性呢？这里我们就需要借助规则的提升度了。 规则提升度（Lift）：置信度与后项支持度之比，数学表述为： Lift\left(A\rightarrow B\right)=\frac{Confidence\left(A\rightarrow B\right)}{P\left(B\right)}=\frac{P\left(AB\right)}{P\left(A\right)P\left(B\right)}提升度反映了项目$A$的出现对项目$B$出现的影响程度。从统计角度来看，如果$A$的出现对项$B$的出现没有影响，即$A$与$B$相互独立的化，$P(AB)=P(A)P(B)$，此时规则提升度为1。所以，具有实用性的关联规则应该是提升度大于1的规则，即$A$的出现对$B$的出现有促进作用。同样，提升度越大，证明规则实用性越强。 这样我们就阐述清楚了关联规则的一些基本假定与判别标准，当数据集较小时，关联规则的使用较为简单，但是如果数据集很大的话，如何在这海量的数据中快速找出关联规则呢？这就引出了进一步要叙述的内容——简单关联规则下的$Apriori$算法。 二、Apriori算法2.1 简介在数据量庞大的前提下，由于简单搜索可能产生大量无效的关联规则，并导致计算效率底下。出于克服这些弊端的目的，Apriori算法应运而生，该算法自1996年提出后，经过不断地完善和发展，已成为简单关联分析中的核心算法。 2.2 频繁项集的相关定义频繁项集很好理解，他是指大于等于最小支持度$S_{min}$的项集。其中，若频繁项集中包含一个项目，则成为频繁$1-$项集，记为$L_1$；若包含$k$个项目，则成为频繁$k-$项集，记为$L_k$。频繁项集具有以下两个性质，这俩条性质将应用于我们后面频繁项集及其关联规则的寻找中： 1）频繁项集的子集必为频繁项集（假设项集$\{A,C\}$是频繁项集，那么$\{A\}$和$\{C\}$也为频繁项集） 2）非频繁集的超集一定也是非频繁的（假设项集$\{D\}$不是频繁项集，那么$\{A,D\}$和$\{C,D\}$也不是频繁项集） 进一步，当某一个$L_k$的所有超集都是频繁项集时，我们就可以称此$L_k$为最大频繁$k-$项集，确定它的目的就在于使之后的到的关联规则具有较高的普适性。 2.3 寻找频繁项集对频繁项集的寻找，是Apriori算法提高寻找规则效率的关键。它采用迭代的方式逐层寻找下层的超集，并在超集中发现频繁项集。经过层层迭代，直到最顶层得到最大频繁项集为止。在每一轮的迭代中都包含以下两个步骤： 1）产生候选集$C_k$，它是有可能成为频繁项集的项目集合； 2）修剪候选集$C_k$，即基于$C_k$计算相应的支持度，并依据最小支持度$S_{min}$对候选集$C_k$进行删减，得到新的候选集$C_{k+1}$，如此循环迭代，直到无法产生候选项集为止，这样最后一轮所得到的频繁项集就是Apriori所要求的最大频繁项集。 接下来我们以一个下例子帮助理解：假设我们指定的最小支持阀度为0.5（计数≥2） 在第一轮迭代过程中，由于$D$的支持度小于0.5（只有0.25），所以没有进入频繁项集，其余均进入频繁项集，定义为$L_1$。 在第二轮迭代中，候选集$C_2$是$L_1$中所有项目的组合，计算各项目支持度，淘汰$\{A,B\}$和$\{A,E\}$，其余进入频繁项集，定义为$L_2$。 在第三轮迭代中，只有$\{B,C,E\}$进入候选集$C_3$，而其余都没有进入，之所以会这样，是因为这里使用到了前面所提到的频繁项集的第二个性质：非频繁项集的超集一定也是非频繁的。所以，包含$\{A,B\}$与$\{A,E\}$的超集是不可能成为频繁项集的。 由于$L_3$不能继续构成候选集$C_4$，所以迭代结束，得到的最大频繁项集为$L_3\{B,C,E\}$。 2.4 在最大频繁项集的基础上产生简单关联规则得到最大频繁项集并不是最终的目的。之前在判断关联规则的有效性时，我们学习了置信度与支持度两个指标。其中，支持度已经在寻找最大频繁项集的过程中发挥了作用，那么，在接下来关联规则的产生上，就轮到置信度大显身手了。 首先，每个频繁项集都需要计算所有非空子集$L^*$的置信度，公式为 C_{L'\rightarrow\left\{L-L'\right\}}=\frac{P\left(L\right)}{P\left(L'\right)}如果所求得的$C_{L’\rightarrow\left\{L-L’\right\}}$大于我们自行指定的$C_{min}$，则生成相应的关联规则${L’\rightarrow\left\{L-L’\right\}}$ 在上面的例子中，$L_3{\{B,C,E\}}$的非空子集就包括$\{B\}$，$\{C\}$，$\{E\}$，$\{B,C\}$，$\{B,E\}$，$\{C,E\}$，举例来说，根据公式可计算得到 C_{C\rightarrow\left\{B,E\right\}}=\frac{P\left(B,C,E\right)}{P\left(C\right)}=\frac{2}{3}=66.7\%其余置信度依次为：$C_{B\rightarrow\left\{C,E\right\}}=66.7\%$，$C_{E\rightarrow\left\{B,C\right\}}=66.7\%$，$C_{\left\{B,C\right\}\rightarrow E}=100\%$，$C_{\left\{B,E\right\}\rightarrow C}=66.7\%$，$C_{\left\{C,E\right\}\rightarrow B}=100\%$ 如果我么设定$C_{min}=80\%$的话，只有$C_{\left\{C,E\right\}\rightarrow B}$和$C_{\left\{B,C\right\}\rightarrow E}$可以入围，如果设定为$50\%$，那么六条规则就都是有效规则了。置信度的选取和支持度一样，只有结合具体应用情况，算法才能给到我们切合实际的结论。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>关联分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（13）：推荐系统（3）—矩阵分解技术]]></title>
    <url>%2F2017%2F03%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8813%EF%BC%89%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%883%EF%BC%89%E2%80%94%E2%80%94%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[随着Netflix Prize推荐比赛的成功举办，近年来隐语义模型（Latent Factor MOdel，LFM）受到越来越多的关注。隐语义模型最早在文本挖掘领域被提出，用于寻找文本的隐含语义，相关的模型常见的有潜在语义分析（Latent Semantic Analysis,LSA）、LDA（Latent Dirichlet Allocation）的主题模型（Topic Model）、矩阵分解（Matrix Factorization）等等。 其中矩阵分解技术是实现隐语义模型使用最广泛的一种方法，其思想也正来源于此，注明的推荐领域大神Yehuda Koren更是凭借矩阵分解模型勇夺Netflix Prize推荐比赛冠军，以矩阵分解为基础，Yehuda Koren在数据挖掘和机器学习相关的国际顶级会议（SIGIR,SIGKDD,RecSys等）发表了很多文章，将矩阵分解模型的优势发挥得淋漓尽致。实验结果表明，在个性化推荐中使用矩阵分解模型要明显优于传统的基于邻域的协同过滤(又称基于领域的协同过滤)方法，如UserCF、ItemCF等，这也使得矩阵分解成为了目前个性化推荐研究领域中的主流模型。 需要说明的是，协同过滤方法分为两大类，一类为上述基于领域的方法，第二类为基于模型的方法，即隐语义模型，矩阵分解模型是隐语义模型最为成功的一种实现，不作特别说明的情况下，本文将隐语义模型和矩阵分解看做同一概念，User-Item矩阵和User-Item评分矩阵为同一概念。 一、传统的SVD算法说到矩阵分解，我们首先想到的就是奇异值分解SVD。我们可以把User-Item评分矩阵M进行SVD分解，并通过选择部分较大的一些奇异值来同时进行降维，也就是说矩阵M此时分解为： M_{m\times n}=U_{m\times k}\Sigma_{k\times k}V^T_{k\times n}其中k是矩阵MM中较大的部分奇异值的个数，一般会远远的小于用户数和物品数。 如果我们要预测第i个用户对第j个物品的评分$m_{ij}$,则只需要计算$u^T_iΣv_j$即可。通过这种方法，我们可以将评分表里面所有没有评分的位置得到一个预测评分，通过找到最高的若干个评分对应的物品推荐给用户。 可以看出这种方法简单直接，似乎很有吸引力。但是有一个很大的问题我们忽略了，就是SVD分解要求矩阵是稠密的，也就是说矩阵的所有位置不能有空白。有空白时我们的MM是没法直接去SVD分解的。大家会说，如果这个矩阵是稠密的，那不就是说我们都已经找到所有用户物品的评分了嘛，那还要SVD干嘛! 的确，这是一个问题，传统SVD采用的方法是对评分矩阵中的缺失值进行简单的补全，比如用全局平均值或者用用户物品平均值补全，得到补全后的矩阵。接着可以用SVD分解并降维。但填充本身会造成很多问题，其一，填充大大增加了数据量，增加了算法复杂度。其二，简单粗暴的数据填充很容易造成数据失真。 虽然有了上面的补全策略，我们的传统SVD在推荐算法上还是较难使用。因为我们的用户数和物品一般都是超级大，随便就成千上万了。这么大一个矩阵做SVD分解是非常耗时的。那么有没有简化版的矩阵分解可以用呢？我们下面来看看实际可以用于推荐系统的矩阵分解。 二、Funk-SVD算法2.1 基本思想Funk-SVD的核心思想认为用户的兴趣只受少数几个因素的影响，因此将稀疏且高维的User-Item评分矩阵分解为两个低维矩阵，即通过User、Item评分信息来学习到的用户特征矩阵P和物品特征矩阵Q，通过重构的低维矩阵预测用户对产品的评分。由于用户和物品的特征向量维度比较低，因而可以通过梯度下降(Gradient Descend)的方法高效地求解，分解示意图如下所示。 2.2 Funk-SVDSimon Funk在博客上公开发表了一个只考虑已有评分记录的矩阵分解方法，称为Funk-SCD，也就是被Yehuda Koren称为隐语义模型的矩阵分解方法。 它的出发点为，既然将一个矩阵做SVD分解成3个矩阵很耗时，同时还面临稀疏的问题，那么我们能不能避开稀疏问题，同时只分解成两个矩阵呢？也就是说，现在期望我们的矩阵M这样进行分解： M_{m\times n}=P^T_{m\times k}Q_{k\times n}我们知道SVD分解已经很成熟了，但是Funk-SVD如何将矩阵M分解成为P和Q呢？这里采用了线性回归的思想。我们的目标是让用户的评分和用矩阵乘积得到的评分残差尽可能的小，也就是说，可以用均方差作为损失函数，来寻找最终的P和Q。 对于某一个用户评分$m_{ij}$如果用Funk-SVD进行矩阵分解，则对应的表示为$q_j^Tp_i$，采用均方差作为损失函数，则我们期望$(m_{ij}-q_j^Tp_i)^2$尽可能的小，如果考虑所有的物品和样本的组合，则我们期望最小化下式： \sum_{i，j}(m_{ij}-q^T_jp_i)^2只要我们能够最小化上面的式子，并求出极值所对应的$p_i,q_j$，则我们最终可以得到矩阵P和Q，那么对于任意矩阵M任意一个空白评分的位置，我们可以通过$q_j^Tp_i$计算预测评分，很漂亮的方法！ 当然，在实际应用中，为了防止过拟合，会加入一个$L_2$的正则化项，因此正是的Funk-SVD的优化目标函数$J(p,q)$是这样的： arg \underset {p_jq_j}{min}\sum_{(i,j)\in K}(m_{ij}-q_j^Tp_i)^2+\lambda (||p_i||^2_2+||q_j||^2_2)其中$K$为已有评分记录的$(i,j)$对集合，$m_{ij}$为用户$i$对物品$$j$的真实评分，$\lambda$是正则化系数，需要调参。假设输入评分矩阵为M，它是$M \times N$维矩阵，通过直接优化以上损失函数得到用户特征矩阵$P(M\times N)$和物品特征矩阵$Q(K\times N)$,其中$K&lt;&lt;M，N$。对于这个优化问题，一般通过梯度下降法来进行优化得到结果。 将上式分别对$p_i,q_j$求导我们得到： \frac{∂J}{∂p_i}=-2(m_{ij}-q_j^Tp_i)q_j+2\lambda p_i$$$$\frac{∂J}{∂q_j}=-2(m_{ij}-q_j^Tp_i)p_i+2\lambda q_j在梯度下降法迭代时，$p_i,q_j$的迭代公式为： p_i=p_i=\alpha [(m_{ij}-q_j^Tp_i)q_j-\lambda p_i]q_j=q_j+\alpha [(m_{ij}-q_j^Tp_i)p_i-\lambda q_j ]通过迭代我们最终可以得到P和Q，进而用于推荐。Funk-SVD算法虽然思想很简单，但在实际应用中效果非常好，这真是验证了大道至简。 三、Bias-SVD在Funk-SVD算法火爆之后，出现了很多Funk-SVD的改进版算法。其中Bias算是改进的比较成功的一种算法。 Funk-SVD方法通过学习用户和物品的特征向量进行预测，即用户和物品的交互信息。用户的特征向量代表了用户的兴趣，物品的特征向量代表了物品的特点，且每一个维度相互对应，两个向量的内积表示用户对该物品的喜好程度。但是我们观测到的评分数据大部分都是都是和用户或物品无关的因素产生的效果，即有很大一部分因素是和用户对物品的喜好无关而只取决于用户或物品本身特性的。例如，对于乐观的用户来说，它的评分行为普遍偏高，而对批判性用户来说，他的评分记录普遍偏低，即使他们对同一物品的评分相同，但是他们对该物品的喜好程度却并不一样。同理，对物品来说，以电影为例，受大众欢迎的电影得到的评分普遍偏高，而一些烂片的评分普遍偏低，这些因素都是独立于用户或产品的因素，而和用户对产品的的喜好无关。 我们把这些独立于用户或独立于物品的因素称为偏置(Bias)部分，将用户和物品的交互即用户对物品的喜好部分称为个性化部分。事实上，在矩阵分解模型中偏好部分对提高评分预测准确率起的作用要大大高于个性化部分所起的作用，以Netflix Prize推荐比赛数据集为例为例，Yehuda Koren仅使用偏置部分可以将评分误差降低32%，而加入个性化部分能降低42%，也就是说只有10%是个性化部分起到的作用，这也充分说明了偏置部分所起的重要性，剩下的58%的误差Yehuda Koren将称之为模型不可解释部分，包括数据噪音等因素。 偏置部分主要由三个子部分组成，分别是 训练集中所有评分记录的全局平均数$\mu$，表示了训练数据的总体评分情况，对于固定的数据集，它是一个常数。 用户偏置$b_i$，独立于物品特征的因素，表示某一特定用户的打分习惯。例如，对于批判性用户对于自己的评分比较苛刻，倾向于打低分；而乐观型用户则打分比较保守，总体打分要偏高。 物品偏置$b_j$，特立于用户兴趣的因素，表示某一特定物品得到的打分情况。以电影为例，好片获得的总体评分偏高，而烂片获得的评分普遍偏低，物品偏置捕获的就是这样的特征。 则偏置部分表示为 b_{ij}=\mu+b_i+b_j则加入了偏置项以后的优化目标函数$J(p,q)$是这样 arg \underset {p_jq_j}{min}\sum_{(i,j)\in K}(m_{ij}-\mu-b_i-b_j-q_j^Tp_i)^2+\lambda (||p_i||^2_2+||q_j||^2_2+||b_i||_2^2+||b_j||_2^2)这个优化目标也可以采用梯度下降法求解。和Funk-SVD不同的是，此时我们多了两个偏置项$b_i,b_j$,$p_i,p_j$的迭代公式和$Funk-SVD$类似，只是每一步的梯度导数稍有不同而已。而$b_i,b_j$一般可以初始设置为0向量，然后参与迭代。 b_i=b_i+α(m_{ij}−μ−b_i−b_j−q^T_jp_i−λb_i)b_j=b_j+α(m_{ij}−μ−b_i−b_j−q^T_jp_i−λb_j)通过迭代我们最终可以得到$P$和$Q$，进而用于推荐。Bias-SVD增加了一些额外因素的考虑，因此在某些场景会比FunkSVD表现好。 四、SVD++SVD++算法在Bias-SVD算法上进一步做了增强，这里它增加考虑用户的隐式反馈。 对于某一个用户$i$，它提供了隐式反馈的物品集合定义为$N(i)$，这个用户对某个物品$j$对应的隐式反馈修正的评分值为$c_{ij}$，那么该用户所有的评分修正值为$\sum_{s\in N(i)} c_{sj}$，一般我们将它们表示为用$q_j^Ty_s$形式，则加入了隐式反馈以后的优化目标函数$J(p,q)$是这样的： arg \underset {p_jq_j}{min}\sum_{(i,j)\in K}(m_{ij}-\mu-b_i-b_j-q_j^Tp_i-q_j^T|N(i)|^{-1/2}\Sigma_{s\in N(i)} y_s)^2+\lambda (||p_i||^2_2+||q_j||^2_2+||b_i||_2^2+||b_j||_2^2+\Sigma_{s\in N(i)}||y_s||^2)其中，引入$|N(i)|^{-1/2}$是为了消除不同$|N(i)|$个数引起的差异。如果需要考虑用户的隐式反馈时，使用SVD++是个不错的选择。 五、矩阵分解的优缺点矩阵分解方法将高维User-Item评分矩阵映射为两个低维用户和物品矩阵，解决了数据稀疏性问题。 使用矩阵分解具有以下优点： 比较容易编程实现，随机梯度下降方法依次迭代即可训练出模型。比较低的时间和空间复杂度，高维矩阵映射为两个低维矩阵节省了存储空间，训练过程比较费时，但是可以离线完成；评分预测一般在线计算，直接使用离线训练得到的参数，可以实时推荐。 预测的精度比较高，预测准确率要高于基于领域的协同过滤以及内容过滤等方法。 非常好的扩展性，很方便在用户特征向量和物品特征向量中添加其它因素，例如添加隐性反馈因素的SVD++，此方法的详细实现参见文献《Koren Y. Factorization meets the neighborhood: a multifaceted collaborative filtering model》；添加时间动态time SVD++，此方法将偏置部分和用户兴趣都表示成一个关于时间的函数，可以很好的捕捉到用户的兴趣漂移，欲知详细实现请阅读文献《Koren Y. Collaborative filtering with temporal dynamics》。 矩阵分解的不足主要有： 模型训练比较费时。 推荐结果不具有很好的可解释性，分解出来的用户和物品矩阵的每个维度* 无法和现实生活中的概念来解释，无法用现实概念给每个维度命名，只能理解为潜在语义空间。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（13）：推荐系统（2）—基于领域的协同过滤]]></title>
    <url>%2F2017%2F03%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8813%EF%BC%89%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E9%A2%86%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%2F</url>
    <content type="text"><![CDATA[基于邻域的算法是推荐系统中最基本的算法，在学术界和业界都有广泛研究与应用。它分为两大类，一类是基于用户的协同过滤算法，另一类是基于物品的协同过滤算法。 一、基于用户的协同过滤算法（user-based collaborative filtering）UserCF是推荐系统的元老级算法，也是最为著名的算法，它标志了推荐系统的诞生。这里首先介绍最基础的算法，然后在此基础上提出不同的改进方法。 1.1 基础算法在一个在线个性推荐系统中，当一个用户A需要个性化推荐时，可以先找到和他有相似兴趣的其他用户，然后将那些用户喜欢的、而用户A没有听说过的物品推荐给A。这种方法就是基于用户的协同过滤算法 如上图，我们收集到用户-电影评价矩阵，假设用户A对于物品D的评价为NULL，这时我们对比用户A、用户B、用户C的特征向量（以物品评价为特征），可以发现用户A和用户C的相似度较大，这时我们可以认为，对于用户C喜欢的物品D，用户A也应该喜欢它，这时就把物品D推荐给用户A。 UserCF主要包括两个步骤： 1）寻找和目标用户兴趣相似的用户集合这里的关键就是计算两个用户的兴趣相似度，这里，协同过滤算法主要利用行为的相似度计算兴趣的相似度。给定用户u和用户v，令$N(u)$表示用户$u$曾经有过正反馈（用户的行为倾向于指用户喜欢该物品，反之，负反馈指用户的行为倾向于指用户不喜欢该物品）的物品集合，令$N(v)$为用户$v$曾经有过正反馈的物品集合。我们可以使用Jaccard或者余弦相似度来计算它们之间的兴趣相似度。 Jaccard公式： w_{uv}=\frac{|N\left(u\right)\cap N\left(v\right)|}{|N\left(u\right)\cup N\left(v\right)|} 余弦相似度： w_{uv}=\frac{|N\left(u\right)\cap N\left(v\right)|}{\sqrt{|N\left(u\right)||N\left(v\right)|}} 2）找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给目标用户。 1.2 离线算法评测书中通过MovieLens数据集上的离线试验来测评算法的性能。UserCF只有一个重要参数K，即为每个用户选出K个和他兴趣最相似的用户，然后推荐那K个用户感兴趣的物品。下图为选择不同的K值时算法的性能。逐一分析各个指标： 准确率和召回率：准确率、召回率与K不成线性关系，在此数据集中，K=80左右会获得比较高的准确率和召回率。选择合适的K对获得高的推荐系统精要比较重要，但对K值不是很敏感，保持在一定区域内即可。 流行度：K越大则UserCF推荐结果就越热门，流行度就越高。因为K决定了UserCFA给你做推荐时参考多少和你兴趣相似的其他用户的兴趣，如果K越大，参考的人越多，结果就越来越趋近于全局热门的物品。 覆盖率：覆盖率随着K的增大而减小。因为随着K的增大，UserCF越来越倾向于推荐热门的物品，从而对长尾物品的推荐越来越少，覆盖率就越来越小了 此外对于Random算法（每次随机挑选10个用户没有产生过行为的物品推荐给当前用户）和MostPopular算法（按照物品的流行度给用户推荐他没有产生过行为的物品中最热门的10个物品）这两种基础算法（选取K=80），MostPopular算法准确率和召回率很高，但覆盖率非常低。与这两个极端相比，UserCF的准确率和召回率高得多，覆盖率也很高。 1.3 用户相似度计算的改进用余弦相似度来度量用户之间兴趣相似度过于粗糙，因为两个用户对热门物品采取过相同的行为并不能说明他们兴趣相似，但对于冷门物品才去过相同的行为更能说明他们兴趣的相似度。John S.Breese提出了以下公式： w_{uv}=\frac{\sum_{i\in N\left(u\right)\cap N\left(v\right)}{\frac{1}{\log\left(1+|N\left(i\right)|\right)}}}{\sqrt{|N\left(u\right)||N\left(v\right)|}}该公式通过$1/\log\left(1+|N\left(i\right)|\right)$惩罚了用户$u$和用户$v$共同兴趣列表中热门物品对他们相似度的影响。将基于上述用户相似度公式的UserCF算法记为User-IIF算法。 1.4 实际应用相比基于物品的协同过滤算法ItemCF，UserCFA在目前的实际应用中使用并不多。其中最著名的使用者是$Digg$，它的推荐思路为：用户在$Digg$中主要通过”顶”和”踩”两种行为表达自己对文章的看法。当用户顶了一篇文章，$Digg$就认为该用户对这篇文章有兴趣，而且愿意把这篇文章推荐给其他用户。然后$Digg$找到所有在该用户顶文章之前也顶了这一篇文章的其他用户，然后给他推荐那些人最近顶的其他文章。 二、基于物品的协同过滤算法（item-based collaborative filtering）基于物品的协同过滤算法是目前业界应用最多的算法。亚马逊、Netflix、Hulu、Youtube的推荐算法的基础都是ItemCF。 2.1 基础算法UserCF存在一些缺点： 1）随着网站的用户数目越来越大，计算用户兴趣相似度矩阵将越来越困难，其运算时间复杂度和空间复杂度的增长和用户数的增长近似于平方关系 2）很难对推荐结果作出解释 所以，亚马逊提出了基于物品的协同过滤算法。它给用户推荐那些和他们之前喜欢的物品相似的物品，主要通过分析用户的行为记录计算物品之间的相似度，它认为物品A和物品B具有很大的相似度是因为喜欢物品A的用户大都也喜欢物品B，它也可以利用用户的历史行为给推荐结果提供解释。 同样，我们对比物品A、物品B、物品C的特征向量（以用户对该物品的喜好程度为特征），发现物品A和物品C很像，就把用品C推荐给喜欢物品A的用户C。 ItemCF主要包括两个步骤： 1）计算物品之间的相似度 可以使用下面的公式定义物品的相似度： w_{ij}=\frac{|N\left(i\right)\cap N\left(j\right)|}{|N\left(i\right)|}分母$|N(i)|$是喜欢物品$i$的用户数，而分子$|N\left(i\right)\cap N\left(j\right)|$是同时喜欢物品$i$和物品$j$的用户数。可以理解为喜欢物品$i$的用户中有多少比例的用户也喜欢物品$j$。但是如果$j$很热门，很多人喜欢，那么$w_{ij}$就会很大，接近1，也就是会造成任何物品都会和热门的物品有很大的相似度。为了避免推荐出热门的物品，可以使用下面的公式： w_{ij}=\frac{|N\left(i\right)\cap N\left(j\right)|}{\sqrt{|N\left(i\right)||N\left(j\right)|}}它惩罚了物品$j$的权重，因此减轻了热门物品会和很多物品相似的可能性。从上面的定义可以看到，在协同过滤中两个物品产生相似度是因为它们共同被很多用户喜欢，也就是每个用户都可以通过它们的历史兴趣列表给物品“贡献”相似度。 2）根据物品的相似度和用户的历史行为给用户生成推荐列表]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（13）：推荐系统（1）—简介]]></title>
    <url>%2F2017%2F03%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8813%EF%BC%89%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[一、定义1.1 从搜索引擎说起人们在寻找信息时，常常需要借助搜索引擎，主动地提供准确的关键词来进行相应的搜索，但是当用户无法找到准确描述自己需求的关键字时，搜索引擎就无能为力了。 和搜索引擎一样，推荐系统也是一种帮助用户快速发现有用信息的工具。但是和搜索引擎不同的是，推荐系统不需要用户提供明确的需求，而是通过分析用户的历史行为给用户的兴趣建模，从而主动给用户推荐能够满足它们兴趣与需求的信息。 1.2 推荐系统的具体应用近十年来，推荐引擎对互联网用户来说随处可见。以亚马逊为例，简单说说实际的应用。Amazon有个性化商品推荐列表和相关商品的推荐列表，以下为一种基于物品的的推荐算法（item-based method），它会给我推荐那些和之前我喜欢的物品相似的物品，因为我之前查找过关于算法数据结构相关的书籍，所以就给我推荐了以下这些计算机领域的经典图书除了个性化推荐列表，亚马逊另一个重要的推荐应用就是相关推荐列表。一种是包含购买此商品的顾客也同时购买，另一种是包含浏览过这个商品的顾客购买的其他商品.此外，还有一个应用就是打包销售，同时购买这些商品往往会给你一定的折扣。除了亚马逊等电子商务领域的推荐系统应用，此外还有像Netflix会向用户推荐电影，豆瓣电台给用户推荐喜欢的歌曲，Facebook给用户推荐个性化物品和好友，Flipboard给特定的用户推荐特定领域的阅读资讯，雅虎的个性化广告投放等等。 二、推荐系统评测一个好的推荐系统不仅仅能够准确预测用户的行为，而且能够扩展用户的视野，帮助用户发现那些他们可能会感兴趣，但却不那么容易发现的东西。同时还要能帮助商家将那些被埋没在长尾的好商品介绍给可能对他们感兴趣的用户。 2.1 推荐系统实验方法首先，人们需要通过实验的办法来计算和获取一些指标，从而全面地测评推荐系统的好坏。主要的实验方法有离线实验、用户调查、在线AB测试。 2.1.1 离线实验一般离线试验的步骤如下： 1）通过日志系统获取用户行为数据，并按照一定格式生成一个标准的数据集； 2）将数据集按照一定的规则分成训练集和测试集； 3）在训练集上训练用户兴趣模型，在测试集上进行预测； 4）通过事先定义的离线指标评测算法在测试集上的预测结果。离线试验不需要有对实际系统的控制权、不需要用户参与实验、速度快、可以测试大量算法。但是它无法计算商业上关心的指标（点击率、转化率）、离线试验的指标和商业指标存在差距。 2.1.2 用户调查因为离线实验的指标与实际的商业指标存在差距，所以需要将算法直接上线测试，但在这之前必须进行用户调查，否则直接进行在线实验会有较高的风险，因为对算法会不会降低用户满意度谁都没有把握。用户调查可以获得很多体现用户主观感受的指标，相对在线实验风险很低，出现错误后很容易弥补。但是招募测试用户代价较大，很难组织大规模的测试一款能过户，因此会使得测试结果的统计意义不足。 2.1.3 在线实验AB测试时最常用的在线测评算法的实验方法。它通过一定的规则将用户随机分成机组，并对不同组的用户采用不同的算法，然后通过统计不同组用户的各种不同的评测指标比较不同算法，比如可以统计不同组用户的点击率，通过点击率比较不同算法的性能。AB测试可以公平获得不同算法实际在线时的性能指标，包括商业上关注的指标。但是周期较长，必须进行长期的实验才能得到可靠的结果，因此常常只会用它测试那些在离线实验和用户调查中表现很好的算法。以上就是一个新的推荐算法最终上线所需要做的事。总结一下： 1）首先，需要通过离线试验证明它在很多离线指标上优于现有算法； 2）然后，需要通过用户调查确定它的用户满意度不低于现有算法。 3）最后，通过在线的AB测试确定它在我们关心的指标上优于现有的算法。 2.2 离线评测指标令$R(u)$是根据用户在训练集上的行为给用户作出的推荐列表，而$T(u)$是用户在测试集上的行为列表。 准确率（Precison）: Precision=\frac{\sum_{u\in U}{|R\left(u\right)\cap T\left(u\right)|}}{\sum_{u\in U}{|R\left(u\right)|}} 召回率（Recall）： \textrm{Re}call=\frac{\sum_{u\in U}{|R\left(u\right)\cap T\left(u\right)|}}{\sum_{u\in U}{|T\left(u\right)|}} 覆盖率(Coverage)，描述了一个推荐系统对物品长尾的发掘能力， 最简单的定义为推荐系统能够推荐出来的物品占总物品集合的比例 信息熵 : H=-\sum_{i=1}^n{p\left(i\right)\log p\left(i\right)} 基尼系数： Gini=\sum_{i=1}^n{p_i\left(1-p_i\right)} 马太效应：即强者更强，弱者更弱。若一个系统会增大热门物品和非热门物品的流行度差异，让热门的物品更加热门，不认的物品更加不热门，那么这个系统就有马太效应。推荐系统的初衷是希望消除马太效应，使得各种物品都能被展示给它们感兴趣的某一类人群。但是现在主流的推荐算法都具有马太效应。可以使用基尼系数来评测推荐系统是否具有马太效应。如果G1是从初始用户行为中计算出的物品流行度的基尼系数，G2是从推荐列表中计算出的物品流行度的基尼系数，那么如果G2&gt;G1，就说明推荐系统具有马太效应。 多样性：如果推荐列表比较多样，覆盖了用户绝大多数的兴趣点，那么就会增加用户找到感兴趣物品的概率。因此给用户的推荐列表也需要满足用户广泛的兴趣，即多样性。 多样性描述了推荐列表物品两两之间的不相似性。假设$s(i,j)\in[0,1]$定义了物品$i$和$j$之间的相似度，那么用户$u$的推荐列表$R(u)$的多样性定义如下： Diversity\left(R\left(u\right)\right)=1-\frac{\sum_{i,j\in R\left(u\right),i\ne j}{s\left(i,j\right)}}{\frac{1}{2}|R\left(u\right)|\left(|R\left(u\right)-1|\right)} 而推荐系统整体的多样性可以定义为所有用户推荐列表多样性的平均值： Diversity=\frac{1}{|U|}\sum_{u\in U}{Diversity\left(R\left(u\right)\right)} 新颖性：给用户推荐那些它们以前没有听说过的物品。最简单方法是利用推荐结果的平均流行度，因为越不热门的物品越可能让用户觉得新颖。但是这个指标比较粗略，因为不同用户不知道的东西是不同的。需要用户调查来准确统计新颖性。 惊喜度：惊喜度和新颖性两者之间是有区别的，如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（12）：SVM（6）—SVM与LR的异同]]></title>
    <url>%2F2017%2F03%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%886%EF%BC%89%E2%80%94%E2%80%94SVM%E4%B8%8ELR%E7%9A%84%E5%BC%82%E5%90%8C%2F</url>
    <content type="text"><![CDATA[本文讲述LR与SVM的异同点 一、LR与SVM的相同点 LR和SVM都是分类算法，都是监督学习算法。 如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。LR也是可以用核函数的，至于为什么通常在SVM中运用核函数而不在LR中运用，后面讲到他们之间区别的时候会重点分析。总之，原始的LR和SVM都是线性分类器，这也是为什么通常没人问你决策树和LR什么区别，决策树和SVM什么区别，你说一个非线性分类器和一个线性分类器有什么区别？ LR和SVM都是判别模型。判别模型会生成一个表示P(Y|X)的判别函数（或预测模型），而生成模型先计算联合概率p(Y,X)然后通过贝叶斯公式转化为条件概率。简单来说，在计算判别模型时，不会计算联合概率，而在计算生成模型时，必须先计算联合概率。或者这样理解：生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。当然，这也是为什么很少有人问你朴素贝叶斯和LR以及朴素贝叶斯和SVM有什么区别。 LR和SVM在学术界和工业界都广为人知并且应用广泛。 二、LR与SVM的不同点1.损失函数 我们先来看一下带松弛变量的 SVM 和正则化的逻辑回归它们的损失函数：其中 $g(z)=(1+exp(−z))^{−1}$可以将两者统一起来:这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重,两者的根本目的都是一样的。即支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。 影响SVM决策面的样本点只有少数的支持向量，当在支持向量外添加或减少任何样本点对分类决策面没有任何影响；而在LR中，每个样本点都会影响决策面的结果。用下图进行说明： 支持向量机改变非支持向量样本并不会引起决策面的变化逻辑回归中改变任何样本都会引起决策面的变化 因此线性SVM不直接依赖于数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要先对数据做平衡处理。​ 2.核技巧 在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。 ​这个问题理解起来非常简单。分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。通过上面的第二点不同点可以了解，在计算决策面时，SVM转化为对偶问题后，只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的），这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。​ 3.正则项​​根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。但是逻辑回归相对来说模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些。但是SVM的理论基础更加牢固，有一套结构化风险最小化的理论基础，虽然一般使用的人不太会去关注。 4.异常值 两者对异常的敏感度也不一样。同样的线性分类情况下，如果异常点较多的话，无法剔除，首先LR，LR中每个样本都是有贡献的，最大似然后会自动压制异常的贡献，SVM+软间隔对异常还是比较敏感，因为其训练只需要支持向量，有效样本本来就不高，一旦被干扰，预测结果难以预料。 5.normalization 两个模型对数据和参数的敏感程度不同，Linear SVM比较依赖penalty的系数和数据表达空间的测度，而（带正则项的）LR比较依赖对参数做L1 regularization的系数。但是由于他们或多或少都是线性分类器，所以实际上对低维度数据overfitting的能力都比较有限，相比之下对高维度数据，LR的表现会更加稳定，为什么呢？ 因为Linear SVM在计算margin有多“宽”的时候是依赖数据表达上的距离测度的，换句话说如果这个测度不好（badly scaled，这种情况在高维数据尤为显著），所求得的所谓Large margin就没有意义了，这个问题即使换用kernel trick（比如用Gaussian kernel）也无法完全避免。所以使用Linear SVM之前一般都需要先对数据做normalization，而求解LR（without regularization）时则不需要或者结果不敏感。 Linear SVM 和 LR 有什么异同？]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
        <tag>LR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（12）：SVM（5）—对偶]]></title>
    <url>%2F2017%2F03%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%885%EF%BC%89%E2%80%94%E2%80%94%E5%AF%B9%E5%81%B6%2F</url>
    <content type="text"><![CDATA[原文地址：支持向量机：Duality在之前关于 support vector 的推导中，我们提到了 dual ，这里再来补充一点相关的知识。这套理论不仅适用于 SVM 的优化问题，而是对于所有带约束的优化问题都适用的，是优化理论中的一个重要部分。简单来说，对于任意一个带约束的优化都可以写成这样的形式： 形式统一能够简化推导过程中不必要的复杂性。其他的形式都可以归约到这样的标准形式，例如一个 $maxf(x)$ 可以转化为 $min−f(x)$等。假如 $f_0,f_1,…,f_m $全都是凸函数，并且 $h_1,…,h_p$ 全都是仿射函数（就是形如 $Ax+b$ 的形式），那么这个问题就叫做凸优化（Convex Optimization）问题。凸优化问题有许多优良的性质，例如它的极值是唯一的。不过，这里我们并没有假定需要处理的优化问题是一个凸优化问题。 虽然约束条件能够帮助我们减小搜索空间，但是如果约束条件本身就是比较复杂的形式的话，其实是一件很让人头痛的问题，为此我们希望把带约束的优化问题转化为无约束的优化问题。为此，我们定义 Lagrangian 如下：它通过一些系数把约束条件和目标函数结合在了一起。当然 Lagrangian 本身并不好玩，现在让我们来让他针对 $λ$ 和 $ν$ 最大化，令：这里 $λ⪰0$ 理解为向量 $λ$ 的每一个元素都非负即可。这个函数 $z(x)$ 对于满足原始问题约束条件的那些 $x$ 来说，其值等于 $f_0(x)$ ，这很容易验证，因为满足约束条件的$x$ 会使得 $h_i(x)=0$ ，因此最后一项消掉了，而 $f_i(x)≤0$ ，并且我们要求了 $λ⪰0$ ，因此 $λ_if_i(x)≤0$ ，所以最大值只能在它们都取零的时候得到，这个时候就只剩下 $f_0(x)$ 了。因此，对于满足约束条件的那些 $x$ 来说，$f_0(x)=z(x)$ 。这样一来，原始的带约束的优化问题其实等价于如下的无约束优化问题：因为如果原始问题有最优值，那么肯定是在满足约束条件的某个 $x^∗$ 取得，而对于所有满足约束条件的 $x ，z(x) 和 f_0(x)$ 都是相等的。至于那些不满足约束条件的 $x$ ，原始问题是无法取到的，否则极值问题无解。很容易验证对于这些不满足约束条件的$x$ 有 $z(x)=∞$，这也和原始问题是一致的，因为求最小值得到无穷大可以和“无解”看作是相容的。 到这里，我们成功把带约束问题转化为了无约束问题，不过这其实只是一个形式上的重写，并没有什么本质上的改变。我们只是把原来的问题通过 Lagrangian 写作了如下形式：这个问题（或者说原始的带约束的形式）称作 primal problem 。如果你看过之前关于 SVM 的推导，那么肯定就知道了，相对应的还有一个 dual problem ，其形式非常类似，只是把 min 和 max 交换了一下：交换之后的 dual problem 和原来的 primal problem 并不相等，直观地，我们可以这样来理解：胖子中最瘦的那个都比瘦骨精中最胖的那个要胖。当然这是很不严格的说法，而且扣字眼的话可以纠缠不休，所以我们还是来看严格数学描述。和刚才的 $z(x)$ 类似，我们也用一个记号来表示内层的这个函数，记：并称 $g(λ,ν)$ 为 Lagrange dual function （不要和 L 的 Lagrangian 混淆了）。g 有一个很好的性质就是它是 primal problem 的一个下界。换句话说，如果 primal problem 的最小值记为 $p^∗$ ，那么对于所有的 $λ⪰0$ 和 $ν$ ，我们有：因为对于极值点（实际上包括所有满足约束条件的点）$x^∗$，注意到 $λ⪰0$ ，我们总是有因此于是这样一来就确定了 g 的下界性质，于是 实际上就是最大的下界。这是很自然的，因为得到下界之后，我们自然地就希望得到最好的下界，也就是最大的那一个——因为它离我们要逼近的值最近呀。记 dual problem 的最优值为 $d^∗$ 的话，根据上面的推导，我们就得到了如下性质：这个性质叫做 weak duality ，对于所有的优化问题都成立。其中 $p^∗−d^∗$ 被称作 duality gap 。需要注意的是，无论 primal problem 是什么形式，dual problem 总是一个 convex optimization 的问题——它的极值是唯一的（如果存在的话），并且有现成的软件包可以对凸优化问题进行求解（虽然求解 general 的 convex optimization 实际上是很慢并且只能求解规模较小的问题的）。 这样一来，对于那些难以求解的 primal problem （比如，甚至可以是 NP 问题），我们可以通过找出它的 dual problem ，通过优化这个 dual problem 来得到原始问题的一个下界估计。或者说我们甚至都不用去优化这个 dual problem ，而是（通过某些方法，例如随机）选取一些 $λ⪰0$ 和 $ν$ ，带到 $g(λ,ν)$ 中，这样也会得到一些下界（只不过不一定是最大的那个下界而已）。当然要选 λ 和 ν 也并不是总是“随机选”那么容易，根据具体问题，有时候选出来的 $λ$ 和 $ν$ 带入 $g$ 会得到 $−∞ $，这虽然是一个完全合法的下界，然而却并没有给我们带来任何有用的信息。 故事到这里还没有结束，既然有 weak duality ，显然就会有 strong duality 。所谓 strong duality ，就是 这是一个很好的性质，strong duality 成立的情况下，我们可以通过求解 dual problem 来优化 primal problem ，在 SVM 中我们就是这样做的。当然并不是所有的问题都能满足 strong duality ，在讲 SVM 的时候我们直接假定了 strong duality 的成立，这里我们就来提一下 strong duality 成立的条件。 不过，这个问题如果要讲清楚，估计写一本书都不够，应该也有不少专门做优化方面的人在研究这相关的问题吧，我没有兴趣（当然也没有精力和能力）来做一个完整的介绍，相信大家也没有兴趣来看这样的东西——否则你肯定是专门研究优化方面的问题的了，此时你肯定比我懂得更多，也就不用看我写的介绍啦。 所以，这里我们就简要地介绍一下 Slater 条件和 KKT 条件。Slater 条件是指存在严格满足约束条件的点 $x$ ，这里的“严格”是指 $f_i(x)≤0$ 中的“小于或等于号”要严格取到“小于号”，亦即，存在 x 满足 我们有：如果原始问题是 Convex 的并且满足 Slater 条件的话，那么 strong duality 成立。需要注意的是，这里只是指出了 strong duality 成立的一种情况，而并不是唯一情况。例如，对于某些非 convex optimization 的问题，strong duality 也成立。这里我们不妨回顾一下 SVM 的 primal problem ，那是一个 convex optimization 问题（QP 是凸优化问题的一种特殊情况），而 Slater 条件实际上在这里就等价于是存在这样的一个超平面将数据分隔开来，亦即是“数据是可分的”。当数据不可分是，strong duality 不能成立，不过，这个时候我们寻找分隔平面这个问题本身也就是没有意义的了，至于我们如何通过把数据映射到特征空间中来解决不可分的问题，这个当时已经介绍过了，这里就不多说了。 让我们回到 duality 的话题。来看看 strong duality 成立的时候的一些性质。 假设 $x^∗$ 和 $(λ^∗,ν^∗)$ 分别是 primal problem 和 dual problem 的极值点，相应的极值为 $p^∗$ 和 $d^∗$ ，首先 $p^∗=d^∗$ ，此时我们可以得到由于两头是相等的，所以这一系列的式子里的不等号全部都可以换成等号。根据第一个不等号我们可以得到 $x^∗$ 是 $L(x,λ^∗,ν^∗)$ 的一个极值点，由此可以知道 $L(x,λ^∗,ν^∗)$ 在 $x^∗$ 处的梯度应该等于 0 ，亦即：此外，由第二个不等式，又显然$ λ^∗_if_i(x^∗)$ 都是非正的，因此我们可以得到这个条件叫做 complementary slackness 。显然，如果 $λ^∗_i&gt;0$，那么必定有 $f_i(x^∗)=0$ ；反过来，如果 $f_i(x^∗)&lt;0$ 那么可以得到 $λ^∗_i=0$ 。这个条件正是我们在介绍支持向量的文章末尾时用来证明那些非支持向量（对应于 $f_i(x^∗)&lt;0$）所对应的系数 $α_i$ （在本文里对应 $λ_i$ ）是为零的.)再将其他一些显而易见的条件写到一起，就是传说中的 KKT (Karush-Kuhn-Tucker) 条件：任何满足 strong duality （不一定要求是通过 Slater 条件得到，也不一定要求是凸优化问题）的问题都满足 KKT 条件，换句话说，这是 strong duality 的一个必要条件。 不过，当原始问题是凸优化问题的时候（当然还要求原函数是可微的，否则 KKT 条件的最后一个式子就没有意义了），KKT 就可以升级为充要条件。换句话说，如果 primal problem 是一个凸优化问题，且存在 $x^˜$ 和 $(λ^˜,ν^˜)$ 满足 KKT 条件，那么它们分别是 primal problem 和 dual problem 的极值点并且 strong duality 成立。其证明也比较简单，首先 primal problem 是凸优化问题的话，$g(λ,ν)=min_xL(x,λ,ν)$ 的求解对每一组固定的 $(λ,ν)$ 来说也是一个凸优化问题，由 KKT 条件的最后一个式子，知道 $x^˜$ 是 $min_xL(x,λ^˜,ν^˜)$ 的极值点（如果不是凸优化问题，则不一定能推出来），亦即： 最后一个式子是根据 KKT 条件的第二和第四个条件得到。由于 g 是 f0 的下界，这样一来，就证明了 duality gap 为零，也就是说，strong duality 成立。 到此为止，做一下总结。我们简要地介绍了 duality 的概念，基本上没有给什么具体的例子。不过由于内容比较多，为了避免文章超长，就挑了一些重点讲了一下。总的来说，一个优化问题，通过求出它的 dual problem ，在只有 weak duality 成立的情况下，我们至少可以得到原始问题的一个下界。而如果 strong duality 成立，则可以直接求解 dual problem 来解决原始问题，就如同经典的 SVM 的求解过程一样。有可能 dual problem 比 primal problem 更容易求解，或者 dual problem 有一些优良的结构（例如 SVM 中通过 dual problem 我们可以将问题表示成数据的内积形式从而使得 kernel trick 的应用成为可能）。此外，还有一些情况会同时求解 dual 和 primal problem ，比如在迭代求解的过程中，通过判断 duality gap 的大小，可以得出一个有效的迭代停止条件。]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
        <tag>对偶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（12）：SVM（4）—SMO]]></title>
    <url>%2F2017%2F03%2F02%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%884%EF%BC%89%E2%80%94%E2%80%94SMO%2F</url>
    <content type="text"><![CDATA[SMO算法是一种启发式算法，其基本思想是：如果所有变量的解都满足最优化问题的KKT条件，那么这个优化问题的解就得到了，因为KKT条件是该优化问题的充分必要条件。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题。这个二次规划问题关于这两个变量的解应该是更接近原始二次规划问题的解，因为这会使得原始二次规划问题的目标函数值变得更小。重要的是，这时子问题可以通过解析方法求解，这样就可以大大提升整个算法的计算速度。子问题有两个变量，一个是违反KKT条件最严重的那个，另一个由约束条件自动确定。如果SMO算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。 四、序列最小最优化算法（SMO）通常对于优化问题，我们没有办法的时候就会想到最笨的办法，也就是梯度下降。注意我们这里的问题是要求最大值，只要在前面加上一个负号就可以转化为求最小值，所以$Gradient Descent$和$Gradient Ascend$并没有什么本质的区别，其基本思想直观上来说就是：梯度是函数值增幅最大的方向，因此只要沿着梯度的反方向走，就能使得函数值减小得越大，从而期望迅速达到最小值。当然普通的$Gradient Descent$并不能保证达到最小值，因为很有可能陷入一个局部极小值。不过对于二次规划问题，极值只有一个，所以是没有局部极值的问题。 另外还有一种叫做$Coordinate Descend$的变种，它每次只选择一个维度，例如$a=(a_1,···,a_n)$，它每次选取$a_i$为变量，而将其他都看成是常数，从而原始的问题在这一步编程一个一元函数，然后针对这个一元函数求最小值，如此反复轮换不同的维度进行迭代。$Coordinate Descend$的主要用处在于那些原本很复杂，但是如果只限制在一维的情况下则变得很简单甚至可以直接求极值的情况，例如我们这里的问题，暂且不管约束条件，如果只看目标函数的话，当$a$只有一个分量是变量的时候，这就是一个普通的一元二次函数的极值问题，初中生也会做，带入公式即可。 然后这里还有一个问题就是约束条件的存在，其实如果没有约束条件的话，本身就是一个多元的二次规划问题，也是很好求解的。但是有了约束条件，结果让$Coordinate Descend$变得很尴尬了，直接根据第二个约束条件$\sum_{i=1}^N{a_iy_i=0}$,$a_1$的值立即就可以定下来，事实上，迭代每个坐标维度，最后发现优化根本进行不下去，因为迭代了一轮之后会发现根本没有任何进展，一切停留在初始值。 所以SMO一次选取了两个坐标来进行优化。例如，我们假设现在选取$a_1$和$a_2$为变量，其余为常量，则根据约束条件我们有： \sum_{i=1}^N{a_iy_i=0}\Longrightarrow a_2=\frac{1}{y_2}\left( -\sum_{i=3}^N{a_iy_i-a_1y_1} \right) \Longleftrightarrow y_2\left( K-a_1y_1 \right)其中那个从3到n的作和都是常量，我们统一记作K。将这个式子代入原来的目标函数中，可以消去$a_2$，从而变成一个一元二次函数。总之现在变成了一个带区间约束的一元二次函数极值问题。唯一要注意的就是这里的约束条件，一个就是$a_1$本身需要满足$0≤a_i≤C$,然后由于$a_2$也要满足同样的约束，即：$0≤y_2(K-a_1y_1)≤C$，可以得带$a_1$的一个可行区间，同$[0,C]$交集即可得到最终的可行区间。投影到$a_1$轴上所对应的区间即是$a_1$的取值范围，在这个区间内求二次函数的最大值即可完成SMO的一步迭代。 同$Coordinate Descent$一样，SMO也会选取不同的两个$coordinate$维度进行优化，可以看出由于每一个迭代步骤实际上是一个可以直接求解的一元二次函数极值问题，所以求解非常高效。此外，SMO也并不是一次或随机地选取两个坐标函数极值问题，而是有一些启发式的策略来选取最优的两个坐标维度。]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
        <tag>SMO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（12）：SVM（3）—非线性支持向量机]]></title>
    <url>%2F2017%2F03%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%883%EF%BC%89%E2%80%94%E2%80%94%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。此为核方法，是比支持向量机更为一般的机器学习方法。 三、非线性支持向量机与核函数3.1 核技巧前面我们介绍了线性情况下的支持向量机，他通过寻找一个现行的超平面来达到对数据线性分类的目的。不过，由于是线性方法，所以对非线性的数据就没有办法处理了。 例如图中的两类数据，分别分布为两个圆圈的形状，不论是任何高级的分类器，只要他是线性的，就没有办法处理，SVM也不行。因为这样的数据本身就是线性不可分的。 此数据集为两个半径不同的圆圈加上了少量的噪音得到，所以一个理想的分界应该是一个圆圈而不是一条直线。如果用$X_1和X_2$来表示这个二维平面的两个坐标的话，则此方程可以写作 a_1X_1+a_2X_{1}^{2}+a_3X_2+a_4X_{2}^{2}+a_5X_1X_2+a_6=0注意上面的形式，如果我们构造另外一个无谓的空间，其中五个坐标的值分别为$Z_1=X_1,Z_2=X_1^2,Z_3=X_2,Z_4=X_2^2,Z_5=X_1·X_2$ 那么显然，上面的方程在新的坐标系下可以写作： \sum_{i=1}^5{a_iZ_i+}a_6=0如果我们做一个映射$\phi :R^2\rightarrow R^5$，将$X$按照上面的规则映射为$Z$那么在新的空间中原来的数据将变成线性可分的，从而使用之前我们推倒的线性分类算法就可以进行处理了。这正是核方法处理非线性问题的基本思想。 总结一下，用线性分类方法求解非线性分类问题分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。核技巧就属于这样的方法。 现在回到SVM的情形，假设原始的数据是非线性的，我们通过一个映射$\phi \left( · \right) $将其映射到一个高维空间中，数据变得线性可分了，这个时候，我们就可以使用原来的推导来进行计算，只是所有的推导现在是在新的空间，而不是原始空间中进行。当然，推导过程也并不是可以简单地直接类比的，例如，原本我们要求超平面的法向量$w$，但是如果映射之后得到的新空间的维度是无穷维的（确实会出现这样的情况，比如后面会提到的高斯核函数），要表示一个无穷维的向量描述起来就比较麻烦。 我们似乎可以这样做，拿到非线性数据，就找一个映射$\phi（·） $，然后一股脑把原来的数据映射到新空间，再做线性SVM即可。但是在之前对一个二维空间做映射，选择的新空间是原始空间的所有一阶和二阶的组合，得到了五个维度；但如果原始空间是三维，我们就会得到19维的新空间，这个数目是呈爆炸性增长的，这给映射的计算带来了很大困难，而且如果遇到无穷维的情况，就根本无从计算了，所以就需要核函数出马了。 核技巧的想法是，再学习与预测中只定义核函数$K\left( x,z \right) $，而不显式地定义映射函数$\phi（·） $。不像之前是映射到高维空间中，然后再根据内积公式进行计算，现在我们直接在原来的低维空间中进行计算，而不需要显式的写出映射后的结果。通常，直接计算$K(x,z)$比较容易，而通过$\phi \left( x \right) \mathrm{和}\phi \left( z \right) $计算$K(x,z)$并不容易。 最理想的情况下，我们希望知道数据的具体形状和分布，从而得到一个刚好可以将数据映射成线性可分的$\phi（·） $，然后通过这个$\phi（·） $得到对应的$K(·，·)$进行内积计算。然而，第二步通常是非常困难甚至完全没法做的。不过，由于第一步也是几乎无法做到的，因为对于任意的数据分析其形状找到合适的映射本身就不是什么容易的事情，所以，人们通常是“胡乱”选择一个核函数即可——我们直到她对应了某个映射，虽然我们不知道这个映射具体是什么，由于我们的计算只需要核函数即可，所以我们也并不关心也没有必要求出所对应的映射的具体形式。 我们注意到在线性支持向量机的对偶问题中，无论是目标函数还是决策函数（分离超平面）都只涉及输入实例与实例之间的内积。在对偶问题的目标函数中的内积$x_i·x_j$可以用核函数$K(x_i·x_j)=\phi \left( x_i \right) ·\phi \left( x_j \right) $来代替，此时对偶问题的目标函数成为： \underset{a}{\max}\ \ \ \ \sum_{i=1}^N{a_i-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_jK⟨ x_i,x_j ⟩}}}同样，分类决策函数中的内积也可以用核函数代替，而分类决策函数式成为 sign\left( \sum_{i=1}^N{a_{i}^{*}y_iK⟨ x_i,x⟩}+b \right)在核函数$K(x,z)$给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐性地在特征空间进行的，不需要显式地定义特征空间和映射函数。这样的技巧称为核技巧，它是巧妙地利用线性分类学习方法与核函数解决非线性问题的技术。在实际应用中，往往依赖领域知识直接选择核函数，核函数选择的有效性需要通过实验验证。 3.2 常用核函数通常人们会从一些常用的核函数中选择，根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数。 多项式核函数polynomial kernel function K\left( x,z \right) =\left( x·z+1 \right) ^p对应的支持向量机是一个p次多项式分类器。在此情形下，分类决策函数成为f\left( x \right) =sign\left( \sum_{i=1}^N{a_iy_i\left( x_i·x+1 \right) ^p+b} \right) 高斯核函数gaussian kernel function K\left( x,z \right) =\exp \left( -\frac{||x-z||^2}{2\sigma ^2} \right)这个核就是会将原始空间映射为无穷维空间的那个家伙。不过如果$\sigma$选得很大的话，高次特征上的权重实际上衰减的非常快，所以实际上相当于一个低维的子空间；反过来，如果$\sigma$选得很小，则可以将任意的数据映射为线性可分，当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数$\sigma$，高斯核实际上具有相当高的灵活性，也是使用最为广泛的核函数之一。它对应的支持向量机是高斯径向基函数分类器，在此情形下，分类决策函数称为f\left( x \right) =sign\left( \sum_{i=1}^N{a_iy_i\exp \left( -\frac{||x-z||^2}{2\sigma ^2} \right) +b} \right) 字符串核函数：核函数不仅可以定义在欧式空间上，还可以定义在离散数据的集合上，比如，字符串核实定义在字符串集合上的核函数，字符串核函数在文本分类、信息检索、生物信息学等方面都有应用。 线性核 $κ(x_1,x_2)=⟨x_1,x_2⟩$ ，这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了。 最后，总结一下：对于非线性的情况，SVM 的处理方法是选择一个核函数 κ(⋅,⋅) ，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。由于核函数的优良品质，这样的非线性扩展在计算量上并没有比原来复杂多少，这一点是非常难得的。当然，这要归功于核方法——除了 SVM 之外，任何将计算表示为数据点的内积的方法，都可以使用核方法进行非线性扩展。 非线性支持向量机学习算法步骤如下： 选取适当的核函数$K(x,z)$和适当的参数$C$，构造并求解最优化问题 \underset{a}{\max}\,\,\,\,\,\,\,\,\sum_{i=1}^N{a_i-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_jK\left( x_i,x_j \right)}}} s.t.\ \ \ \ \sum_{i=1}^N{a_iy_i=0} 0\le a_i\le C\ \ ,\ \ i=1,2,···,N求得最优解 a^*=\left( a_{1}^{*},a_{2}^{*},···,a_{N}^{*} \right) ^T 选择$a^$的一个正分量$a_i^$适合约束条件$0&lt;a_i&lt;C$,计算 b^*=y_j-\sum_{i=1}^N{y_ia_{i}^{*}K\left( x_i·x_j \right)} 构造决策函数： f(X)=sign\left( \sum_{i=1}^N{a_{i}^{*}y_iK\left( x_i,x \right)}+b \right)当$K(x,z)$是正定核函数时，该问题为凸二次规划问题，解是存在的。 3.3 感性认识比如我们有一个一维的数据分布是如下图的样子，你想把它用一个直线来分开，你发现是不可能的，因为他们是间隔的。所以不论你画在哪，比如绿色竖线，都不可能把两个类分开。 但是我们使用一个简单的升维的方法，把原来一维的空间投射到二维中，$x-&gt;(x, x^2)$。比如: 0-&gt;(0,0)1-&gt;(1,1)2-&gt;(2,4) 这时候就线性可分了 再举个例子，在一个二维平面里面，这样的情况是不可能只用一个平面来分类的，但是只要把它投射到三维的球体上，就可能很轻易地分类。 理论上，由于train set是有限的，当你把data投射到无限维度的空间上是一定可以在train set上完美分类的，至于在test set上就不一定了。 在实用中，很多使用者都是盲目地试验各种核函数，并扫描其中的参数，选择效果最好的，来“避免过拟合”。至于什么样的核函数适用于什么样的问题，大多数人都不懂。核函数要满足的条件称为Mercer’s condition。使用SVM的很多人甚至都不知道这个条件，也不关心它；有些不满足该条件的函数也被拿来当核函数用。 Mercer’s Condition机器学习有很多关于核函数的说法，核函数的定义和作用是什么？]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
        <tag>非线性支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（12）：SVM（2）—线性支持向量机]]></title>
    <url>%2F2017%2F02%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[当训练数据近似线性可分时，通过软间隔最大化学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机。 二、线性支持向量机与软间隔最大化2.1 线性支持向量机通常情况是，训练数据中有一些特异点outlier，将这些特异点除去后，剩下大部分的样本点组成的集合是线性可分的。 线性不可分意味着某些样本点不能满足函数间隔大于等于1的约束条件。为了解决这个问题，可以对每个样本点引进一个松弛变量$\xi \geqslant 0$，使函数间隔加上松弛变量大于等于1.这样，约束条件变成 y_i\left( w·x_i+b \right) \geqslant 1-\xi _i同时，对每个松弛变量$\xi \geqslant 0$，支付一个代价$\xi \geqslant 0$。当然，如果我们允许$\xi \geqslant 0$任意大的话，那任意的超平面都是符合条件的了。所以，我们在原来的目标函数后面加上一项，使得这些 $\xi \geqslant 0$的总和也要最小：目标函数由原来的$\frac{1}{2}||w||^2$变成 \frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i}这里，$C&gt;0$称为惩罚参数，一般事先由应用问题决定，控制目标函数中两项（“寻找 $margin$ 最大的超平面”和“保证数据点偏差量最小”）之间的权重，$C$越大时对误分类的惩罚增大，$C$值小时对误分类的惩罚减小。最小化目标函数包含两层含义：使$\frac{1}{2}||w||^2$尽量小即间隔尽量大，同时使误分类点的个数尽量小，C是调和二者的系数。则有以下优化问题： \underset{w,b,\xi}{\min}\ \frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i} s.t.\ \ y_i\left( w·x_i+b \right) \geqslant 1-\xi _i\ ,\ i=1,2,···,N \xi _i\geqslant 0,\ i=1,2,···\mathrm{，}N可证明$w$的解是唯一的，但$b$的解不唯一，$b$的解存在于一个区间。 用之前的方法将限制加入到目标函数中，得到如下原始最优化问题的拉格朗日函数： L\left( w,b,\xi ,a,u \right) =\frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i-\sum_{i=1}^N{a_i\left( y_i\left( w·x_i+b \right) -1+\xi _i \right) -\sum_{i=1}^N{u_i\xi _i}}}首先求拉格朗日函数针对$w,b,\xi $的极小。 \frac{\partial L}{\partial w}=0\Rightarrow w=\sum_{i=1}^N{a_iy_ix_i} \frac{\partial L}{\partial b}=0\Rightarrow \sum_{i=1}^N{a_iy_i=0} \frac{\partial L}{\partial \xi _i}=0\Rightarrow C-a_i-u_i=0，i=1,2,3···,N将它们代入拉格朗日函数，得到和原来一样的目标函数。 \underset{a}{\max}\ \ -\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) +\sum_{i=1}^N{a_i}}} s.t.\ \sum_{i=1}^N{a_iy_i=0} C-a_i-u_i=0 a_i\geqslant 0 u_i\geqslant 0不过，由于我们得到$C-a_i-u_i=0$，而又有$u_i&gt;0$（作为拉格朗日乘子的条件）,因此有$a_i≤C$,所以整个dual问题现在写作： \underset{a}{\max}\ \ -\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j⟨x_i·x_j ⟩+\sum_{i=1}^N{a_i}}} s.t.\ \sum_{i=1}^N{a_iy_i=0} 0\le a_i\le C\ ,\ \ i=1,2,···,N和之前的结果对比一下，可以看到唯一的区别就是现在拉格朗日乘子$a$多了一个上限$C$。而 Kernel 化的非线性形式也是一样的，只要把$⟨x_i,x_j⟩$ 换成 $κ(x_i,x_j)$ 即可。 构造并求解上述二次规划问题后求得最优解 a^*=\left( a_{1}^{*},a_{2}^{*},···,a_{N}^{*} \right) ^T然后计算 w^*=\sum_{i=1}^N{a_{i}^{*}y_ix_i}选择$a^$的一个分量$a_i^$适合约束条件$0&lt;a_i&lt;C$,计算 b^*=y_j-\sum_{i=1}^N{a_{i}^{*}y_i⟨x_i·x_j ⟩}对任一适合条件都可求得一个$b^*$，但是由于原始问题对$b$的求解并不唯一，所以实际计算时可以取在所有符合条件的样本点上的平均值。 2.2 支持向量再现性不可分的情况下，将对偶问题的解中对应于$a_i^*&gt;0$的样本点$(x_i,y_i)$的实例$x_i$称为支持向量（软间隔的支持向量）。如图所示，这时的支持向量要比线性可分时的情况复杂一些。 图中，分离超平面由实线表示，间隔边界由虚线表示。正例点由$。$表示，负例点由$×$表示。图中还标出了实例$x_i$到间隔边界的距离$\frac{\xi _i}{||w||}$。 软间隔的支持向量$x_i$要么在间隔边界上，要么在间隔边界与分离超平面之间，要么在分离超平面误分类一侧。 若$a_i^*&lt;C$，则$\xi _i=0$，支持向量恰好落在间隔边界上； 若$a_i^*=C,0&lt;\xi _i&lt;1$，则分类正确，$x_i$在间隔边界与分离超平面之间； 若$a_i^*=C，\xi _i=1$则$x_i$在分隔超平面上； 若$a_i^*=C,\xi _i&gt;1$，则$x_i$位于分离超平面误分一侧。 2.3 Hinge损失函数线性支持向量机学习除了原始最优化问题，还有另外一种解释，就是最优化以下目标函数： \sum_i^{N}[1-y_i(w·x_i+b)]_++\lambda||w||^2目标函数的第一项是经验损失或经验风险，函数 L(y·(w·x+b))=[1-y(w·x+b)]_+称为合页损失函数（hinge loss function）。下标”+”表示以下取正值的函数： \left[z\right]_+=\left\{\begin{array}{l} z\ ,\ z>0\\ 0\ ,\ z\le 0\\ \end{array}\right.这就是说，当样本点$(x_i,y_i)$被正确分类且函数间隔（确信度）$y_i(w·x_i+b)$大于1时，损失是0，否则损失是$1-y_i(w·x_i+b)$。目标函数的第二项是系数为$\lambda$的$w$的$L_2$范数，是正则化项。 接下来证明线性支持向量机原始最优化问题： \underset{w,b,\xi}{\min}\ \frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i} s.t.\ \ y_i\left( w·x_i+b \right) \geqslant 1-\xi _i\ ,\ i=1,2,···,N \xi _i\geqslant 0,\ i=1,2,···\mathrm{，}N等价于最优化问题 \underset{w,b}{min }\sum_i^{N}[1-y_i(w·x_i+b)]_++\lambda||w||^2先令$[1-y_i(w·x_i+b)]_+=\xi_i$，则$\xi_i≥0$，第二个约束条件成立；由$[1-y_i(w·x_i+b)]_+=\xi_i$，当$1-y_i(w·x_i+b)&gt;0$时，有$y_i(w·x_i+b)=1-\xi_i$;当$1-y_i(w·x_i+b)≤0$时，$\xi_i=0$，有$y_i(w·x_i+b)≥1-\xi_i$，所以第一个约束条件成立。所以两个约束条件都满足，最优化问题可以写作 \underset{w,b}{min}\sum_{i=1}^N\xi_i+\lambda||w||^2若取$\lambda =\frac{1}{2C}$则\underset{w,b}{min} \frac{1}{C}(\frac{1}{2} ||w||^2+C\sum_{i=1}^N \xi_i)与原始最优化问题等价。 合页损失函数图像如图所示，横轴是函数间隔$y(w·x+b)$，纵轴是损失。由于函数形状像一个合页，故名合页损失函数。 图中还画出了0-1损失函数，可以认为它是一个二类分类问题的真正的损失函数，而合页损失函数是0-1损失函数的上界。由于0-1损失函数不是连续可导的，直接优化其构成的目标函数比较困难，可以认为线性支持向量机是优化由0-1损失函数的上界（合页损失函数）构成的目标函数。这时的上界损失函数又称为代理损失函数（surrogate function）。图中虚线显示的是感知机的损失函数$[-y_i(w·x_i+b)]_+$。这时当样本点$(x_i,y_i)$被正确分类时，损失是0，否则损失是$-y_i(w·x_i+b)$，相比之下，合页损失函数不仅要分类正确，而且确信度足够高时损失才是0，也就是说，合页损失函数对学习有更高的要求]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
        <tag>线性支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（12）：SVM（1）—线性可分支持向量机]]></title>
    <url>%2F2017%2F02%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8812%EF%BC%89%EF%BC%9ASVM%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[当训练数据线性可分时，通过硬间隔最大化，学习一个线性可分支持向量机。 一般地，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求分离超平面，解是唯一的。也就是它不仅将正负实例点分开，而且对最难分的实例点（离分离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。 一、线性可分支持向量机与硬间隔最大化1.1 线性可分支持向量机假设给定一个特征空间上的训练数据集 T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\}其中$x_i\in R^n,y_i\in \left\{ +1,-1 \right\} ,i=1,2,···,N$，$x_i$为第$i$个特征向量，也称为实例，$y_i$为$x_i$的类标记，当$y_i=+1$时，称$x_i$为正例；当$y_i=-1$时，称$x_i$为负例，$(x_i,y_i)$称为样本点。再假设训练数据集是线性可分的。 给定线性可分训练数据集，通过间隔最大化得到的分离超平面为 w^T·x+b=0以及相应的分类决策函数 f\left( x \right) =sign\left( w^T·x+b \right)该决策函数称为线性可分支持向量机 1.2 函数间隔与几何间隔一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面确定的情况下$|w^T·x+b|$能够相对地表示点$x$距离超平面的远近。而$w^T·x+b$的符号与类标记的符号是否一致能够表示分类是否正确，所以可用$y(w^T·x+b)$来表示分类的正确性与确信度，这就是函数间隔functional margin的概念 但是，函数间隔有一个不足之处，就是在选择分离超平面时，只要成比例地改变$w$和$b$，超平面并没有变化，而函数间隔却以同样比例变化了。因此，我们可以对分离超平面的法向量$w$加上某些约束，使得间隔确定，此时函数间隔成为几何间隔geometric margin。 对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为 \gamma _i=y_i\left( \frac{w}{||w||}·x_i+\frac{b}{||w||} \right)定义超平面$(w,b)$关于训练数据集$T$的几何间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的几何间隔之最小值，即 \gamma =\underset{i=1,···,N}{\min}\gamma _i超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的带符号的距离，当样本点被超平面正确分类时就是实例点到超平面的距离。 函数间隔与几何间隔的关系为 \gamma =\frac{\hat{\gamma}}{||w||}若$||w||=1$，那么函数间隔和几何间隔相等。如果超平面参数$w$和$b$成比例地改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变。 1.3 间隔最大化支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。几何间隔最大的分离超平面是唯一的。 间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据记性分类。即，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。 最大间隔分离超平面 接下来求一个几何间隔最大的分离超平面，即最大间隔分离超平面。具体地，可以表示为下面的约束最优化问题： \underset{w,b}{\max}\,\,\gamma s.t\,\,\,\,y_i\left( \frac{w}{||w||}·x_i+\frac{b}{||w||} \right) \geqslant \gamma \,\,,\,\,i=1,2,···,N即最大化超平面$(w,b)$关于训练数据集的几何间隔$\gamma $，约束条件表示的是超平面$(w,b)$关于每个训练样本点的几何间隔至少是$\gamma $ 根据几何间隔和函数间隔的关系，可以将此问题改写为 \underset{w,b}{\max}\ \frac{\hat{\gamma}}{||w||} s.t\ \ y_i\left( w·x_i+b \right) \geqslant \hat{\gamma}\ ,\ i=1,2,···,N函数间隔$\hat{\gamma}$的取值不影响最优化的解。函数间隔因为$w$，$b$按比例改变为$\lambda w\mathrm{，}\lambda b$而成为$\lambda \hat{\gamma}$，但是对最优化问题中的不等式约束没有影响，对目标函数的优化也没有影响，即两者等价。这样，我们可以取$ \hat{\gamma}=1$，代入后注意到最大化$\frac{1}{||w||}$和最小化$\frac{1}{2}||w||^2$是等价的，因为我们关心的并不是最优情况下目标函数的具体数值。于是就得到下面的线性可分支持向量机学习的最优化问题。 构造并求解约束最优化问题： \underset{w,b}{\min}\frac{1}{2}||w||^2 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s.t\ \ y_i\left( w·x_i+b \right) -1\geqslant 0,\ i=1,2,···,N求得最优解$w^$,$b^$ 由此得到分割超平面： w^*·x+b^*=0分类决策函数 f\left( x \right) =sign\left( w^*·x+b \right) 这其实是一个凸二次规划convex quadratic programming 问题，凸优化问题是指约束最优化问题 \underset{w}{\min}\ \ \ f\left( w \right) \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s.t\ \ \ g_i\left( w \right) \le 0\ ,\ i=1,2,···,k \ \ \ \ \ \ \ \ \ \ \ \ \ h_i\left( w \right) =0\ ,\ i=1,2,···,l\其中，目标函数$f(w)$和约束函数$g_i(w)$都是$R^n$上的连续可微的凸函数，约束函数$h_i(w)$是$R^n$上的仿射函数。当目标函数$f(w)$是二次函数且约束函数$g_i(w)$是仿射函数时，上述凸优化问题成为凸二次规划问题。 支持向量和间隔边界 在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例成为支持向量support vector。支持向量是使约束条件式等号成立的点，即 y_i\left( w·x_i+b \right) -1=0对$y_i=+1$的实例点，支持向量在超平面 H_1\mathrm{：}w·x_i+b=1对$y_i=-1$的负例点，支持向量在超平面 H_2\mathrm{：}w·x_i+b=-1 可以看到两个支撑着中间的长带的超平面，它们到中间的分离超平面的距离相等，为什么一定是相等的呢？，即我们所能得到的最大的几何间隔 $\tilde{\gamma}$ 。而“支撑”这两个超平面的必定会有一些点，试想，如果某超平面没有碰到任意一个点的话，那么我就可以进一步地扩充中间的 gap ，于是这个就不是最大的 margin 了。由于在 $n$ 维向量空间里一个点实际上是和以原点为起点，该点为终点的一个向量是等价的，所以这些“支撑”的点便叫做支持向量。 注意到$H_1$和$H_2$平行，并且没有实例点落在他们中间。在$H_1$和$H_2$之间形成一条长带，分离超平面与他们平行且位于他们中间。长带的宽度，即$H_1$与$H_2$之间的距离成为间隔margin，间隔依赖于分离超平面的法向量$w$，等于$\frac{2}{||w||}$。$H_1$和$H_2$称为间隔边界。 在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。如果移动支持向量将改变所求的解；但是如果在将俄边界以外移动其他实例点，甚至去掉这些点，则解是不会改变的。由于支持向量在确定分离超平面中起着决定性作用，所以将这种分类模型称为支持向量机。支持向量机的个数一般都很少，所以支持向量机由很少的“重要的”训练样本确定。 很显然，由于这些 supporting vector 刚好在边界上，所以它们是满足 $y(w^Tx+b)=1$ ，而对于所有不是支持向量的点，也就是在“阵地后方”的点，则显然有$y(w^Tx+b)&gt;1$ 。事实上，当最优的超平面确定下来之后，这些后方的点就完全成了路人甲了，它们可以在自己的边界后方随便飘来飘去都不会对超平面产生任何影响。这样的特性在实际中有一个最直接的好处就在于存储和计算上的优越性，例如，如果使用 100 万个点求出一个最优的超平面，其中是支持向量的有 100 个，那么我只需要记住这 100 个点的信息即可，对于后续分类也只需要利用这 100 个点而不是全部 100 万个点来做计算。 1.4 学习的对偶算法为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分支持向量机的对偶算法dual algorithm。 这样做的优点是，一是对偶问题往往更容易求解；二是引入核函数，进而推广到非线性分类问题。 首先构建拉格朗日函数，为此，对每一个不等式约束引进拉格朗日乘子$a_i≥0，i=1,2,···,N$定义拉格朗日函数： L\left( w,b,a \right) =\frac{1}{2}||w||^2-\sum_{i=1}^N{a_i(y_i\left( w·x_i+b )-1\right)}其中，$a=\left( a_1,a_2,···,a_N \right) ^T$然后我们令 θ(w)=\underset{a_i≥0}{max}L(w,b,α)容易验证，当某个约束条件不满足时，例如 $y_i(w^Tx_i+b)&lt;1$，那么我们显然有 $θ(w)=∞$ （只要令$α_i=∞$即可）。而当所有约束条件都满足时，则有 $θ(w)=\frac{1}{2}||w||_2$ ，亦即我们最初要最小化的量。因此，在要求约束条件得到满足的情况下最小化 $\frac{1}{2}||w||_2$ 实际上等价于直接最小化 $θ(w)$（当然，这里也有约束条件，就是 $α_i≥0,i=1,…,n$），因为如果约束条件没有得到满足，$θ(w)$ 会等于无穷大，自然不会是我们所要求的最小值。 具体写出来，我们现在的目标函数变成了： \underset{w,b}{min}\theta(w)=\underset{w,b}{\min}\underset{a_i≥0}{\max}L\left( w,b,a \right) =p^*这里用 $p^∗$ 表示这个问题的最优值，这个问题和我们最初的问题是等价的。不过，现在我们来把最小和最大的位置交换一下： \underset{a_i≥0}{\max}\underset{w,b}{\min}L\left( w,b,a \right) =d^∗当然，交换以后的问题不再等价于原问题，这个新问题的最优值用 $d^∗$ 来表示。并，我们有 $d^∗≤p^∗$，这在直观上也不难理解，最大值中最小的一个总也比最小值中最大的一个要大吧！总之，第二个问题的最优值$d^∗$ 在这里提供了一个第一个问题的最优值$p^∗$ 的一个下界，在满足某些条件的情况下，这两者相等，这个时候我们就可以通过求解第二个问题来间接地求解第一个问题。具体来说，就是要满足 KKT 条件，这里暂且先略过不说，直接给结论：我们这里的问题是满足 KKT 条件的，因此现在我们便转化为求解第二个问题。 为了得到对偶问题的解，需要先求$L(w,b,a)$对$w,b$的极小，再求对$a$的极大。 求$\underset{w,b}{\min}L\left( w,b,a \right)$将拉格朗日函数$L(w,b,a)$分别对$w,b$求偏导数并令其为$0$。 \nabla _wL\left( w,b,a \right) =w-\sum_{i=1}^N{a_iy_ix_i}=0 \nabla _bL\left( w,b,a \right) =\sum_{i=1}^N{a_iy_i}=0得到 w=\sum_{i=1}^N{a_iy_ix_i} \sum_{i=1}^N{a_iy_i=0}将其代入拉格朗日函数，得到 L\left( w,b,a \right) =\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) -\sum_{i=1}^N{a_iy_i\left( \left( \sum_{j=1}^N{a_jy_jx_j} \right) ·x_i+b \right) +\sum_{i=1}^N{a_i}}}} =-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) +\sum_{i=1}^N{a_i}}} 求$\underset{w,b}{\min}L\left( w,b,a \right)$对$a$的极大，即是对偶问题 \underset{a}{\max} \sum_{i=1}^N{a_i}-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right)}} s.t.\ \sum_{i=1}^N{a_iy_i=0} a_i\geqslant 0,\ \ i=1,2,···,N 将目标函数由求极大转换为极小，就得到下面与之等价的对偶最优化问题。 \underset{a}{\min}\ \ \ \frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right)}}-\sum_{i=1}^N{a_i} s.t.\ \sum_{i=1}^N{a_iy_i=0} a_i\geqslant 0,\ \ i=1,2,···,N让我们先来看看推导过程中得到的一些有趣的形式。首先就是关于我们的 hyper plane ，对于一个数据点 x 进行分类，实际上是通过把 x 带入到 $f(x)=w^Tx+b$ 算出结果然后根据其正负号来进行类别划分的。而前面的推导中我们得到$f(w)=∑^n_{i=1}α_iy_ix_i$ ，因此 f(x)=(∑^n_{i=1}α_iy_ix_i)^Tx+b=∑^n_{i=1}α_iy_i ⟨x_i,x⟩+b这里的形式的有趣之处在于，对于新点 x 的预测，只需要计算它与训练数据点的内积即可（这里 ⟨⋅,⋅⟩ 表示向量内积），这一点至关重要，是之后使用 Kernel 进行非线性推广的基本前提。此外，所谓 Supporting Vector 也在这里显示出来——事实上，所有非 Supporting Vector 所对应的系数 α 都是等于零的，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据即可。 为什么非支持向量对应的 α 等于零呢？直观上来理解的话，就是这些“后方”的点——正如我们之前分析过的一样，对超平面是没有影响的，由于分类完全有超平面决定，所以这些无关的点并不会参与分类问题的计算，因而也就不会产生任何影响了。 这个结论也可由刚才的推导中得出，回忆一下我们刚才通过 Lagrange multiplier 得到的目标函数： \underset{a_i≥0}{max}L(w,b,a)=\underset{a_i≥0}{max}\frac{1}{2}||w||^2-\sum_{i=1}^na_i(y_i(w^Tx_i+b)-1)注意到如果 $x_i$ 是支持向量的话，上式中$(y_i(w^Tx_i+b)-1)$部分是等于 0 的（因为支持向量的 functional margin 等于 1 ），而对于非支持向量来说，函数间隔会大于 1 ，因此这个部分是大于零的，而 $a_i$ 又是非负的，为了满足最大化，$α_i$ 必须等于 0 。 线性可分支持向量机学习算法 输入：线性可分训练数据集$T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\} $,其中,$x_i\in R^n,y_i\in \left\{ +1,-1 \right\} ,i=1,2,···,N$ 输出：最大间隔分离超平面和分类决策函数 步骤如下 构造并求解约束最优化问题 \underset{a}{\min}\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{a_ia_jy_iy_j\left( x_i·x_j \right) -\sum_{i=1}^N{a_i}}} s.t.\ \ \ \ \sum_{i=1}^N{a_iy_j=0} a_i\geqslant 0,\ i=1,2,···,N求得最优解$a^=(a_1^,a_2^,···,a_N^)$ 计算 w^*=\sum_i^{}{a_{i}^{*}y_ix_i}并选择$a^$的一个正分量$a_j^&gt;0$，计算 b^*=y_j-\sum_{i=1}^N{a_{i}^{*}y_i\left( x_i·x_j \right)} 求得分离超平面 w^*·x+b^*=0分类决策函数： f(x)=sign(w^*·x+b^*) 在线性可分支持向量机中，$w^和b^$只依赖于训练数据中对应于$a_i^&gt;0$的样本点$x_i,y_i$,而其他样本点对$w^和b^$没有影响。我们将训练数据中对应于$a_i^&gt;0$的实例点$x_i\in R^n$称为支持向量。 对于线性可分问题，上述线性可分支持向量机的学习（硬间隔最大化）算法是完美的。但是，训练数据集线性可分是理想的情形。在现实问题中，训练数据集往往是线性不可分的，即在样本中出现噪声或特异点。此时，有更一般的学习算法。 1.5 KKT条件对于包含等式和不等式约束的一般优化问题KKT条件（$x^*$是最优解的必要条件）为 上式便称为不等式约束优化问题的KKT（Karush-Kuhn-Tucker）条件.$\mu _{j}$称为KKT乘子，当约束起作用时$\mu_1&gt;0,g_1(x)=0$，当约束不起作用时$\mu_1=0,g_1(x)&lt;0$ 更细致的推导可以看这篇文章：浅谈最优化问题的KKT条件 证明可以将线性可分支持向量机的原始问题和对偶问题等同起来的充分必要条件KKT条件，即得 \nabla _wL\left( w^*,b^*,a^* \right) =w^*-\sum_{i=1}^N{a_iy_ix_i=0} \nabla _bL\left( w^*,b^*,a^* \right) =-\sum_{i=1}^N{a_{i}^{*}y_i=0} a_{i}^{*}\left( y_i\left( w^*·x_i+b^* \right) -1 \right) =0\ ,\ i=1,2,···,N y_i\left( w^*·x_i+b^* \right) -1\geqslant 0\ ,\ 1,2,···,N a_{i}^{*}\geqslant 0\ ,\ i=1,2,···,N由此得 w^*=\sum_i^{}{a_{i}^{*}y_ix_i}其中至少有一个$a_j^&gt;0$(反证法，假设$a^=0$，由上可知$w^=0$，而$w^=0$不是原始最优化问题的解，产生矛盾)，对此$j$有 y_j\left( w^*·x_j+b^* \right) -1=0 a_{j}^{*}y_jx_j·x_i+b^*=1/y_j=y_j b^*=y_j-\sum_{i=1}^N{a_{i}^{*}y_i\left( x_i·x_j \right)}综上所述，对于给定的线性可分训练数据集，可以首先求对偶问题的解$a^$;再利用求得原始问题的解$w^,b^*$,从而得到分离超平面及分类决策函数。这种算法称为线性可分支持向量机的对偶学习算法，是线性可分支持向量机学习的基本算法。]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（11）：聚类（4）—密度最大值聚类]]></title>
    <url>%2F2017%2F02%2F24%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%884%EF%BC%89%2F</url>
    <content type="text"><![CDATA[五、密度最大值聚类5.1 引言2014年6月，Alex Rodriguez和Alessandro Laio在$Science$上发表了一篇名为《Clustering by fast search and find of density peaks》的文章，提供了一种简洁而优美的聚类算法，是一种基于密度的聚类方法，可以识别各种形状的类簇，并且参数很容易确定。它克服了DBSCAN中不同类的密度差别大、邻域范围难以设定的问题，鲁棒性强。在文章中提出的聚类方法DPCA算法（Desity Peaks Clustering Algorithm）基于这样一种假设：对于一个数据集，聚类中心被一些低局部密度的数据点包围，而且这些低局部密度点距离其他有高局部密度的点的距离都比较大。 5.2 若干概念 局部密度$\rho_i$的定义为：\rho_i=\sum_j{\chi\left(d_{ij}-d_c\right)}其中， \chi\left(x\right)=\left\{\begin{array}{l} 1\ if\ x\rho_i}{\min}\left(d_{ij}\right)即在局部密度高于对象$i$的所有对象中，到对象$i$最近的距离。而极端地，对于密度最大的那个对象，我们设置$\delta=max(d_{ij})$；只有那些密度是局部或者全局最大的点才会有远大于正常值的高局部密度点距离。 5.3 聚类过程这个聚类实例摘自作者的PPT讲演，在一个二维空间中对数据进行聚类，具体步骤如下： 1、首先计算每一个点的局部密度$\rho_i$，如图中，$\rho_1=7,\rho_8=5,\rho_{10}=4$ 2、然后对于每一个点$i$计算在局部密度高于对象$i$的所有对象中，到对象$i$最近的距离$\delta$ 3、对每一个点，绘制出局部密度与高局部密度点距离的关系散点图 4、图上的异常点即为簇中心。如图所示，1和10两点的局部密度和高局部密度距离都很大，将其作为簇中心。 5、将其他的点分配给距离其最近的有着更高的局部密度的簇。（Assign each point to the same cluster of its nearest neighbor of higher density）左图是所有点在二维空间的分布，右图是以$\rho$为横坐标，以$\delta$为纵坐标绘制的决策图。容易发现，1和10两个点的$\rho_i$和$\delta_i$都比较大，作为簇的中心点。26、27、28三个点的$\delta$也比较大，但是$\rho比较小$，所以是异常点。 5.4 一些关键点 簇中心的识别 那些有着比较大的局部密度$\rho_i$和很大的高局部密度$\delta_i$的点被认为是簇的中心；而高局部密度距离$\delta_i$较大但局部密度$\rho_i$较小的点是异常点；确定簇中心之后，其他点按照距离已知簇的中心最近进行分类，也可以按照密度可达的方法进行分类。但是，这里我们在确定聚类中心时，没有定量地分析，而是通过肉眼观察，包含很多的主观因素。在上图中可以分明地用肉眼判断聚类中心，但是有些情况下无法用肉眼来判断。不过，对于那些在决策图中无法用肉眼判断出聚类中心的情形，作者在文中给出了一种确定聚类中心个数的提醒：计算一个将$\rho$值和$\delta$值综合考虑的量 \gamma_i=\rho_i\delta_i显然$\gamma$值越大，越有可能是聚类中心。因此，只需对其降序排列，然后从前往后截取若干个数据点作为聚类中心就可以了。我们把排序后的$\gamma$在坐标平面（下标为横轴，$\gamma$值为纵轴）画出来，由图可见，非聚类中心的$gamma$值比较平滑，而从非聚类中心过渡到聚类中心时$\gamma$有一个明显的跳跃，这个跳跃用肉眼或数值检测应该可以判断出来。作者在文末还提到，对于人工随机生成的数据集，$\gamma$的分布还满足幂次定律，即$log\gamma$，且斜率依赖于数据维度。 截断距离$d_c$的选择 一种推荐做法是选择$d_c$，使得平均每个点的邻居数为所有点的1%~2%。参数$d_c$的选取，从某种意义上决定这聚类算法的成败，取得太大或者太小都不行：如果取得太大，将使得每个数据点的$\rho$值都很大以致区分度不高，极端情况是取$d_c&gt;d_{max}$，则所有的数据点都归属于一个Cluster了；如果$d_c$取得太小，同一个Cluster中就可能被拆分成多个，极端情况是$d_c&lt;d_{min}$，则每个数据点都单独称为一个Cluster。作者将比例锁定在数据量的1%~2%，也是基于肉感数据集的经验值。 选定簇中心之后 在聚类分析中, 通常需要确定每个点划分给某个类簇的可靠性. 在该算法中, 可以首先为每个类簇定义一个边界区域(border region), 亦即划分给该类簇但是距离其他类簇的点的距离小于$d_c$的点(这个区域由这样的数据点构成：它们本身属于该Cluster，但在与其距离不超过$d_c$的范围内，存在属于其他Cluster的数据点). 然后为每个类簇找到其边界区域的局部密度最大的点, 令其局部密度为$\rho_h$. 该类簇中所有局部密度大于$\rho_h$的点被认为是类簇核心的一部分(亦即将该点划分给该类簇的可靠性很大), 其余的点被认为是该类簇的光晕(halo), 亦即可以认为是噪音. 图例如下A图为生成数据的概率分布，B、C二图为分别从该分布中生成了4000，1000个点。D,E分别是B,C两组数据的决策图（decision tree），可以看到两组数据都只有五个点有比较大的$\rho_i$和很大的$\delta_i$，这些点作为类簇的中心，在确定了类簇的中心之后，每个点被划分到各个类簇（彩色点），或者划分到类簇光晕（黑色点），F图展示的是随着抽样点数量的增多，聚类的错误率在逐渐下降，说明该算法是鲁棒的。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（11）：聚类（3）—DBSCAN]]></title>
    <url>%2F2017%2F02%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[四、DBSCAN算法4.1 密度聚类方法密度聚类方法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。这类算法能克服基于距离的算法只能发现“类圆”（凸）的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。其代表算法为DBSCAN算法和密度最大值算法。 4.2 DBSCAN算法原理DBCSAN（Density-Based Spatial Clustering of Applications with Noise）是一个比较有代表性的基于密度的聚类算法。与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在有“噪声”的数据中发现任意形状的聚类。 4.3 若干概念 对象的$\varepsilon -$领域：给定对象在半径$\varepsilon$内的区域 核心对象：对于给定的数目$m$，如果一个对象的$\varepsilon -$领域至少包含$m$个对象，则称该对象为核心对象。 直接密度可达：给定一个对象集合$D$，如果p是在q的$\varepsilon -$领域内，而q是一个核心对象，我们说对象p从对象q出发时直接密度可达的。如图$\varepsilon =1,m=5$，q是一个核心对象，从对象q出发到对象p是直接密度可达的。 密度可达：如果存在一个对象链$p_1p_2···p_n$，$p_1=q,p_n=p$，对$p_i\in D,(1≤i≤n)$,$p_{i+1}$是从$p_i$关于$\varepsilon$和$m$直接密度可达的，则对象$p$是从对象$q$和$m$密度可达的。 密度相连：如果对象集合$D$中存在一个对象$O$，使得对$p$和$q$是从$O$关于$\varepsilon $和$m$密度可达的，那么对象$p$和$q$是关于$\varepsilon $和$m$密度相连的。 簇：一个基于密度的簇是最大的密度相连对象的集合。 噪声：不包含在任何簇中的对象称为噪声。 4.4 算法步骤下面这张图来自WIKI，图上有若干个点，其中标出了A、B、C、N这四个点，据此来说明这个算法的步骤： 1、首先随机选择A点为算法实施的切入点，我们将$\varepsilon $设置为图中圆的半径，对象个数$m（minPts）$设定为4。这里我们看到，A点的$\varepsilon - $领域包含4个对象（自己也包含在内），大于等于$m(minPts)$，则创建A作为核心对象的新簇，簇内其他点都（暂时）标记为边缘点。 2、然后在标记的边缘点中选取一个重复上一步，寻找并合并核心对象直接密度可达的对象。对暂时标记为边缘点反复递归上述算法，直至没有新的点可以更新簇时，算法结束。这样就形成了一个以A为起始的一个聚类，为图中红色的中心点和黄色的边缘点 3、如果还有Points未处理，再次新产生一个类别来重新启动这个算法过程。遍历所有数据，如果有点既不是边缘点也不是中心点，将其标记为噪音。 从上述算法可知： 每个簇至少包含一个核心对象； 非核心对象可以是簇的一部分，构成了簇的边缘（edge）； 包含过少对象的簇被认为是噪声； 4.5 总结 优点 无需确定聚类个数：DBSCAN does not require one to specify the number of clusters in the data a priori, as opposed to k-means. 可以发现任意形状的聚类：DBSCAN can find arbitrarily shaped clusters. It can even find a cluster completely surrounded by (but not connected to) a different cluster. Due to the MinPts parameter, the so-called single-link effect (different clusters being connected by a thin line of points) is reduced. 对噪声具有鲁棒性，可有效处理噪声：DBSCAN has a notion of noise, and is robust to outliers. 只需两个参数，对数据输入顺序不敏感：DBSCAN requires just two parameters and is mostly insensitive to the ordering of the points in the database. (However, points sitting on the edge of two different clusters might swap cluster membership if the ordering of the points is changed, and the cluster assignment is unique only up to isomorphism.) 加快区查询：DBSCAN is designed for use with databases that can accelerate region queries, e.g. using an R* tree. 参数可由领域专家设置：The parameters minPts and ε can be set by a domain expert, if the data is well understood. 缺点 边界点不完全确定性：DBSCAN is not entirely deterministic: border points that are reachable from more than one cluster can be part of either cluster, depending on the order the data is processed. Fortunately, this situation does not arise often, and has little impact on the clustering result[citation needed]: both on core points and noise points, DBSCAN is deterministic. DBSCAN*[4] is a variation that treats border points as noise, and this way achieves a fully deterministic result as well as a more consistent statistical interpretation of density-connected components. 维数灾导致欧几里得距离度量失效：The quality of DBSCAN depends on the distance measure used in the function regionQuery(P,ε). The most common distance metric used is Euclidean distance. Especially for high-dimensional data, this metric can be rendered almost useless due to the so-called “Curse of dimensionality”, making it difficult to find an appropriate value for ε. This effect, however, is also present in any other algorithm based on Euclidean distance. 不能处理密度差异过大（密度不均匀）的聚类（会导致参数无法适用于所有聚类）：DBSCAN cannot cluster data sets well with large differences in densities, since the minPts-ε combination cannot then be chosen appropriately for all clusters. 参数选择在数据与规模不能很好理解的情况下，很难选择，若选取不当，聚类质量下降： If the data and scale are not well understood, choosing a meaningful distance threshold ε can be difficult.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（11）：聚类（2）—Kmeans]]></title>
    <url>%2F2017%2F02%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[三、K-Means算法3.1 原理K-Means算法属于基于划分的聚类算法，对N 维欧氏空间中的点进行聚类，是一种最简单的无监督学习方法。它通过迭代来实现，其基本思想是：每次确定K个类别中心，然后将各个结点归属到与之距离最近的中心点所在的Cluster，然后将类别中心更新为属于各Cluster的所有样本的均值，反复迭代，直至类别中心不再发生变化或变化小于某阈值。 3.2 基本假设K-Means聚类需要对数据进行一个基本假设：对于每一个 cluster ，我们可以选出一个中心点 (center) ，使得该 cluster 中的所有的点到该中心点的距离小于到其他 cluster 的中心的距离。虽然实际情况中得到的数据并不能保证总是满足这样的约束，但这通常已经是我们所能达到的最好的结果，而那些误差通常是固有存在的或者问题本身的不可分性造成的。例如下图所示的两个高斯分布，从两个分布中随机地抽取一些数据点出来，混杂到一起，现在要让你将这些混杂在一起的数据点按照它们被生成的那个分布分开来：由于这两个分布本身有很大一部分重叠在一起了，例如，对于数据点 2.5 来说，它由两个分布产生的概率都是相等的，你所做的只能是一个猜测；稍微好一点的情况是 2 ，通常我们会将它归类为左边的那个分布，因为概率大一些，然而此时它由右边的分布生成的概率仍然是比较大的，我们仍然有不小的几率会猜错。而整个阴影部分是我们所能达到的最小的猜错的概率，这来自于问题本身的不可分性，无法避免。因此，我们将 k-means 所依赖的这个假设看作是合理的。 3.3 算法步骤假定输入样本为$S=x_1,x_2,···,x_n$，则算法步骤为： 1、选择初始的K个类别中心$\mu_1,\mu_2,···,\mu_k$。这个过程通常是针对具体地问题有一些启发式的选取方法，或者大多数情况下采用随机选取的办法。因为K-Means并不能保证全局最优，而是否能收敛到全局最优解其实和初值的选取有很大的关系，所以有时候我们会多次选取初值跑一个K-Means，并取其中最好的一次结果。 2、对于每个样本$x_i$，将其标记为距离类别中心最近的类别，即：label_i=arg\underset{1\le j\le k}{\min}||x_i-\mu_j|| 3、将每个类别中心更新为隶属于该类别的所有样本的均值 \mu_j=\frac{1}{|c_j|}\sum_{i\in c_j}{x_i} 4、重复前两步，直到类别中心的变化小于某阈值或者达到最大迭代次数 3.4 理论分析基于上述的假设，我们导出K-Means所要优化的目标函数：设我们一共有N个数据点需要分为K个Cluster，K-Means需要最小化的损失函数为： J=\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^K{r_{ij}||x_i-\mu_j||^2}}这个函数，其中$r_{ij}$在数据点$n$被归类到$Cluster(j) $的时候为1，否则为0.直接寻找$r_{ij}$和$\mu_j$来最小化$J$并不容易，不过我们可以通过反复迭代以下两步的方法来进行： 1、先固定$\mu_j$，选择最优的$r_{ij}$，很容易看出，只要将数据点归类到离它最近的那个中心就能保证$J$最小，通俗来讲，因为每个样本点都有一个$r_{ij}$，不是0就是1，那么我们要想让$J$最小，就要保证当一个样本的$r_{ij=1}$时，与类别中心距离的平方和达到最小。这一步即 2、然后固定$r_{ij}$，再求最优的$\mu_j$。将$J$对$\mu_k$求导并令导数等于零，即令 \frac{\partial J}{\partial\mu_j}=\sum_{i=1}^{N_j}{r_{ij}\left(x_i-\mu_j\right)}=0很容易得到$J$最小的时候$\mu_j$应该满足 \mu_j=\frac{\sum_i{r_{ij}x_i}}{\sum_i{r_{ij}}} $\mu_j$的值是所有$Cluster(j)$中的数据点的平均值。由于每一次迭代都是取到$J$的最小值，因此$J$智慧不断地减小或者保持不变，而不会增加，这保证了K-Means最终或到达一个极小值。虽然K-Means并不能保证总是得到全局最优解，但是对于这样的问题，像K-Means这样复杂度的算法，这样的结果已经是很不错了。 3.5 算法演练下面看一个来自WIKI的实例 1、随机生成三个初始的中心点（这个中心点不一定是样本点），即图中红、绿、蓝三个小圈； 2、计算每个样本点与这三个中心店的距离，并将它们归属到离得最近的中心点对应的Cluster。此时图中分成了三个簇，分别是红色、绿色、蓝色部分； 3、重新分别计算三个簇中所有样本点的类别中心，指定为新的类别中心。此时红色、绿色、蓝色类的中点都发生了迁移。 4、反复迭代第2步和第3步，直至收敛。 3.6 总结 优点： 是解决聚类问题的一种经典算法，简单、快速 对处理大数据集，该算法保持可伸缩性和高效率 当簇近似为高斯分布时，它的效果较好 缺点 在簇的平均值可被定义的情况下才能使用，可能不适用于某些应用 必须事先给出K，而且对初值敏感，对于不同的初始值，结果可能不同 只能发现球状Cluster，不适合于发现非凸形状的簇或者大小差别很大的簇 对噪声和孤立点数据敏感，如簇中含有异常点，将导致均值偏离严重。因为均值体现的是数据集的整体特征，容易掩盖数据本身的特性。比如数组1，2，3，4，100的均值为22，显然距离“大多数”数据1、2、3、4比较远，如果改成数组的中位数3，在该实例中更为稳妥，这种聚类也叫作K-mediods聚类]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（11）：聚类（1）—简介]]></title>
    <url>%2F2017%2F02%2F20%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8811%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、引言聚类（Clustering）算法就是对大量未知标注的数据集，按照数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。聚类是一种无监督算法。给定一个有$N$个对象的数据集，构造数据的$K$个簇，$k≤n$，同时满足，每个簇至少包含一个对象，每一个对象属于且仅属于一个簇，将满足上述条件的$K$个簇称作一个合理划分。它的主要思想是对于给定的类别数目$K$，首先给出初始划分，通过迭代改变样本和簇的隶属关系，使得每一次改进之后的划分方案都较前一次好。 聚类算法主要包括以下五类： 基于分层的聚类（hierarchical methods） 这种方法对给定的数据集进行逐层，直到某种条件满足为止。具体可分为合并型的“自下而上”和分裂型的“自下而上”两种方案。如在“自下而上”方案中，初始时每一个数据记录都组成一个单独的组，在接下来的迭代中，它把那些相互邻近的组合并成一个组，直到所有的记录组成一个分组或者某个条件满足为止。代表算法有：BIRCH算法（1996）、CURE算法、CHAMELEON算法等。 基于划分的聚类（partitioning methods） 给定一个有N个记录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K&lt;N,而且这K个分组满足下列条件：（1）每一个分组至少包含一个数据记录；（2）每一个数据记录属于且仅属于一个分组（咋某些模糊聚类算法中可以放宽条件）。对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准是：同一分组中的记录越近越好，而不同分组中的记录越远越好。使用这个基本思想的算法有：K-means算法、K-medoids算法、CLARANS算法 基于密度的聚类（density-based methods） 基于密度的方法和其他方法的一个根本区别是：它不是基于各种各样的距离的，而是基于魔都的，这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。这个方法的指导思想为：只要一个区域的点的密度大过某个阈值，就把它加到与之相近的聚类中去，代表算法有：DBSCAN（Density-Based Spatial Clustering of Applic with Noise）算法（1996）、OPTICS（Ordering Points to Identify Clustering Structure）算法（1999）、DENCLUE算法（1998）、WaveCluster算法（1998，具有O（N）时间复杂性，但只适用于低维数据） 基于网格的聚类（grid-based methods） 这种方法首先将数据空间划分成为有限个单元（cell）的网络结构，所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关，它只与把数据空间分成多少个单元有关。代表算法有：STING（Statistical Information Grid）、CLIQUE（Clustering In Quest）算法（1998）、WaveCluster算法。其中STRING算法把数据空间层次地划分为单元格，依赖于存储在网格单元中的统计信息进行聚类；CLIQUE算法结合了密度和网格的方法。 基于模型的聚类（model-based methods） 基于模型的方法给每一个聚类假定一个模型，然后去寻找能够很好地满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。通常有两种尝试方向：统计的方案和神经网络的方案。 二、相似度、距离计算方法 给定$n$维空间$R^n$中的两个向量$X=(x_1,x_2,···,x_n)^T$和$y=(y_1,y_2,···,y_n)^T$，$x,y$之间的距离可以反映两者的相似程度，一般采用$L_p$距离 dist\left( X,Y \right) =\left( \sum_{i=1}^n{|x_i-y_i|^p} \right) ^{\frac{1}{p}}其中$p≥1$，也称为闵可夫斯基距离（Minkowski）距离。常用的$p$为$1,2,+\infty$，此时相应的距离公式分别为 1.当$p=1$时，称为曼哈顿距离（Manhattan distance），改名字的由来起源于在纽约市去测量街道之间的距离就是由人不行的步数来确定的。 d\left(x,y\right)=\sum_{i=1}^n{|x_i-y_i|} 当$p=2$时，称为欧几里得距离（Euclidean distance） d\left(x,y\right)=\left(\sum_{i=1}^n{\left(x_i-y_i\right)^2}\right)^{\frac{1}{2}} 当$p=+\infty$时，称为最大值距离（Maximum distance） d\left(x,y\right)=\underset{1\le i\le n}{\max}|x_i-y_i| 杰卡德相似系数（Jaccard） J\left( A,B \right) =\frac{|A\cap B|}{|A\cup B|} 余弦相似度（Cosine Similarity） \cos\left(\theta\right)=\frac{x^Ty}{|x|·|y|} pearson相似系数 \rho _{XY}=\frac{cov\left( X,Y \right)}{\sigma _x\sigma _y}=\frac{E\left[ \left( x-u_x \right) \left( y-u_y \right) \right]}{\sigma _x\sigma _y} 相对熵（K-L）距离 D\left(p||q\right)=\sum_x{p\left(x\right)\log\frac{p\left(x\right)}{q\left(x\right)}}=E_{p\left(x\right)}\log\frac{p\left(x\right)}{q\left(x\right)} Hellinger距离 D_a\left(p||q\right)=\frac{2}{1-a^2}\left(1-\int{p\left(x\right)^{\frac{1+a}{2}}q\left(x\right)^{\frac{1-a}{2}}}dx\right) 余弦相似度与pearson相似系数的比较 $n$维向量$x$和$y$的夹角记作$\theta$，根据余弦定理，其余弦值为： \cos\left(\theta\right)=\frac{x^Ty}{|x|·|y|}=\frac{\sum_{i=1}^n{x_iy_i}}{\sqrt{\sum_{i=1}^n{x_{i}^{2}}}·\sqrt{\sum_{i=1}^n{y_{i}^{2}}}}这两个向量的相关系数是： \rho_{XY}=\frac{cov\left(X,Y\right)}{\sigma_x\sigma_y}=\frac{E\left[\left(x-u_x\right)\left(y-u_y\right)\right]}{\sigma_x\sigma_y} =\frac{\sum_{i=1}^n{\left(x_i-\mu_x\right)\left(y_i-\mu_y\right)}}{\sqrt{\sum_{i=1}^n{\left(x_i-\mu_x\right)^2}}\sqrt{\sum_{i=1}^n{\left(y_i-\mu_y\right)^2}}}相关系数即将$x,y$坐标向量各自平移到原点后的夹角余弦。这即揭示了为何文档间求距离使用夹角余弦，因为这个物理量表征了文档去均值化后的随机向量间的相关系数。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（10）：朴素贝叶斯]]></title>
    <url>%2F2017%2F02%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8810%EF%BC%89%EF%BC%9A%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯（Naive Bayes）是基于贝叶斯定理与特征条件假设的分类方法。 对于给定的训练数据集，首先基于特征条件独立假设学习输入、输出的联合分布；然后基于此模型，对给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$。 朴素贝叶斯实现简单，学习与预测的效率都很高，是一种常用的方法。 一、朴素贝叶斯的学习与分类1.1贝叶斯定理先看什么是条件概率 $P(A|B$表示事件$B已经发生的前提下，事件$A发生的概率，叫做事件$B$发生下事件$A$的条件概率。其基本求解公式为 P\left(A|B\right)=\frac{P\left(AB\right)}{P\left(B\right)}贝叶斯定理便是基于条件概率，通过$P(A|B)$来求$P(B|A)$： P\left(B|A\right)=\frac{P\left(A|B\right)·P\left(B\right)}{P\left(A\right)}顺便提一下，上式中的分母，可以根据全概率公式分解为： P\left(A\right)=\sum_{i=1}^n{P\left(B_i\right)P\left(A|B_i\right)}1.2 特征条件独立假设这一部分开始朴素贝叶斯的理论推导，从中你会深刻地理解什么是特征条件独立假设。 给定训练数据集$(X,Y)$，其中每个样本$X$都包括$n$维特征，即$x=(x_1,x_2,···,x_n)$，类标记集合含有$K$种类别，即$y=(y_1,y_2,···,y_k)$ 如果现在来了一个新样本$x$我们要怎么判断它的类别?从概率的角度来看，这个问题就是给定$x$，它属于哪个类别的概率更大。那么问题就转化为求解$P(y_1|x),P(y_2|x),P(y_k|x)$中最大的那个，即求后验概率最大的输出：$arg\underset{y_k}{\max}P\left(y_k|x\right)$ 那$P(y_k|x)$怎么求解？答案就是贝叶斯定理： P\left(y_k|x\right)=\frac{P\left(x|y_k\right)·P\left(y_k\right)}{P\left(x\right)}根据全概率公式，可以进一步分解上式中的分母： P\left(y_k|x\right)=\frac{P\left(x|y_k\right)·P\left(y_k\right)}{\sum_{i=1}^n{P\left(x|y_k\right)P\left(y_k\right)}} （公式1）先不管分母，分子中的$P(y_k)$是先验概率，根据训练集就可以简单地计算出来，而条件概率$P(x|y_k)=P(x_1,x_2,···,x_n|y_k)$，它的参数规模是指数数量级别的，假设第$i$维特征$x_i$可取值的个数有$S_i$个，类别取值个数为$k$个，那么参数个数为$k\prod_{j=1}^n{S_j}$ 这显然是不可行的。针对这个问题，朴素贝叶斯算法对条件概率分布做了独立性的假设，通俗地讲就是说假设各个维度的特征$x_1,x_2,···,x_n$互相独立，由于这是一个较强的假设，朴素贝叶斯算法也因此得名。在这个假设的前提上，条件概率可以转化为： P\left(x|y_i\right)=P\left(x_1,x_2,···,x_n|y_i\right)=\prod_{i=1}^n{P\left(x_i|y_i\right)} （公式2）这样参数规模就降到了$\sum_{i=1}^n{S_ik}$ 以上就是针对条件概率所作出的特征条件独立性假设，至此，先验概率$P(y_k)$和条件概率$P(x|y_k)$的求解问题就都解决了，那么我们是不是可以求解我们所需要的后验概率$P(y_k|x)$了 答案是肯定的。我们继续上面关于$P(y_k|x)$的推导，将公式2代入公式1中得到： P\left(y_k|x\right)=\frac{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}{\sum_k{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}}于是朴素贝叶斯分类器可表示为： f\left(x\right)=arg\underset{y_k}{\max}P\left(y_k|x\right)=arg\underset{y_k}{\max}\frac{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}{\sum_k{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}}因为对于所有的$y_k$，上式中的分母的值都是一样的（为什么？注意到全加符号就容易理解了），所以可以忽略分母部分，朴素贝叶斯分裂期最终表示为： f\left(x\right)=arg\underset{y_k}{\max}P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}二、朴素贝叶斯法的参数估计2.1 极大似然估计根据上述，可知朴素贝叶斯要学习的东西就是$P(Y=c_k)$和$P(X^{j}=a_{jl}|Y=c_k)$，可以应用极大似然估计法估计相应的概率（简单讲，就是用样本来推断模型的参数，或者说是使得似然函数最大的参数）。 先验概率$P(Y=c_k)$的极大似然估计是 P\left(Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(y_i=c_k\right)}}{N},\,\,k=1,2,···,K也就是用样本中$c_k$的出现次数除以样本容量。 推导如下： 设第$j$个特征$x^{(j)}$可能取值的集合为${a_{j1},a_{j2},···,a_{jl}}$，条件概率$P(X^{j}=a_{jl}|Y=c_k)$的极大似然估计是： P\left(X^{\left(j\right)}=a_{jl}|Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(x_{i}^{\left(j\right)}=a_{jl},y_{i=}c_k\right)}}{\sum_{i=1}^N{I\left(y_i=c_k\right)}}式中，$x_i^{j}$是第$i$个样本的第$j$个特征。 例题如下： 2.2 贝叶斯估计极大似然估计有一个隐患，假设训练数据中没有出现某种参数与类别的组合怎么办？比如上例中当$Y=1$对应的$X^{(1)}$的取值只有$1$和$2$。这样可能会出现所要估计的概率值为0的情况，但是这不代表真实数据中就没有这样的组合。这时会影响到后验概率的计算结果，使分类产生偏差。解决办法是贝叶斯估计。 条件概率的贝叶斯估计： P_{\lambda}\left(X^{\left(j\right)}=a_{jl}\parallel Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(x_{i}^{\left(j\right)}=a_{jl},y_{i=}c_k\right)}+\lambda}{\sum_{i=1}^N{I\left(y_i=c_k\right)}+S_j\lambda}其中$\lambda≥0$，$S_j$表示$x_j$可能取值的中数。分子和分母分别比极大似然估计多了一点东西，其意义为在随机变量各个取值的频数上赋予一个正数$\lambda≥0$。当$\lambda=0$时就是极大似然估计。常取$\lambda=1$，这时称为拉普拉斯平滑。 先验概率的贝叶斯估计： P_{\lambda}\left(Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(y_i=c_k\right)}+\lambda}{N+K\lambda}例题如下： 三、python代码实现3.1 朴素贝叶斯文档分类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129# -*- coding: utf-8 -*-"""Created on 下午5:28 22 03 2017bayes algorithm: classify a words as good or bad [text classify]@author: plushunter"""from numpy import *class Naive_Bayes: def __init__(self): self._creteria = "NB" #创建不重复词集 def _creatVocabList(self,dataSet): vocabSet = set([]) # 创建一个空的SET for document in dataSet: vocabSet = vocabSet | set(document) # 并集 return list(vocabSet) # 返回不重复词表（SET的特性） #文档词集向量模型 def _setOfWordToVec(self,vocabList, inputSet): """ 功能:给定一行词向量inputSet，将其映射至词库向量vocabList，出现则标记为1，否则标记为0. """ returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 return returnVec #文档词袋模型 def _bagOfsetOfWordToVec(self,vocabList, inputSet): """ 功能：对每行词使用第二种统计策略，统计单个词的个数，然后映射到此库中 输出：一个n维向量，n为词库的长度，每个取值为单词出现的次数 """ returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] += 1 #更新此处代码 return returnVec def _trainNB0(self,trainMatrix, trainCategory): """ 输入：训练词矩阵trainMatrix与类别标签trainCategory,格式为Numpy矩阵格式 功能：计算条件概率p0Vect、p1Vect和类标签概率pAbusive """ numTrainDocs = len(trainMatrix)#样本个数 numWords = len(trainMatrix[0])#特征个数，此处为词库长度 pAbusive = sum(trainCategory) / float(numTrainDocs)#计算负样本出现概率（先验概率） p0Num = ones(numWords)#初始词的出现次数为1，以防条件概率为0，影响结果 p1Num = ones(numWords)#同上 p0Denom = 2.0#类标记为2，使用拉普拉斯平滑法, p1Denom = 2.0 #按类标记进行聚合各个词向量 for i in range(numTrainDocs): if trainCategory[i] == 0: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) else: p1Num += trainMatrix[i] p1Denom += sum(trainMatrix[i]) p1Vect = log(p1Num / p1Denom)#计算给定类标记下，词库中出现某个单词的概率 p0Vect = log(p0Num / p0Denom)#取log对数，防止条件概率乘积过小而发生下溢 return p0Vect, p1Vect, pAbusive def _classifyNB(self,vec2Classify, p0Vec, p1Vec, pClass1): """ 该算法包含四个输入: vec2Classify表示待分类的样本在词库中的映射集合， p0Vec表示条件概率P(wi|c=0)P(wi|c=0)， p1Vec表示条件概率P(wi|c=1)P(wi|c=1)， pClass1表示类标签为1时的概率P(c=1)P(c=1)。 p1=ln[p(w1|c=1)p(w2|c=1)…p(wn|c=1)p(c=1)] p0=ln[p(w1|c=0)p(w2|c=0)…p(wn|c=0)p(c=0)] log取对数为防止向下溢出 功能:使用朴素贝叶斯进行分类,返回结果为0/1 """ p1 = sum(vec2Classify * p1Vec) + log(pClass1) p0 = sum(vec2Classify * p0Vec) + log(1 - pClass1) if p1 &gt; p0: return 1 else: return 0 #test def testingNB(self,testSample): "step1：加载数据集与类标号" listOPosts, listClasses = loadDataSet() "step2：创建词库" vocabList = self._creatVocabList(listOPosts) "step3：计算每个样本在词库中出现的情况" trainMat = [] for postinDoc in listOPosts: trainMat.append(self._bagOfsetOfWordToVec(vocabList, postinDoc)) p0V, p1V, pAb = self._trainNB0(trainMat, listClasses) "step4：测试" thisDoc = array(self._bagOfsetOfWordToVec(vocabList, testSample)) result=self._classifyNB(thisDoc, p0V, p1V, pAb) print testSample, 'classified as:', result # return result#### 加载数据集def loadDataSet(): postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] classVec = [0, 1, 0, 1, 0, 1] # 1 is abusive, 0 not return postingList, classVec#测试if __name__=="__main__": clf = Naive_Bayes() testEntry = [['love', 'my', 'girl', 'friend'], ['stupid', 'garbage'], ['Haha', 'I', 'really', "Love", "You"], ['This', 'is', "my", "dog"], ['maybe','stupid','worthless']] for item in testEntry: clf.testingNB(item) 3.2 使用朴素贝叶斯过滤垃圾邮件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# -*- coding: utf-8 -*-"""Created on 下午8:47 22 03 2017Email_Classify @author: plushunter """import reimport Bayesfrom numpy import *# mysent='This book is the best book on Python or M.L I have ever laid eyes upon.'# regEx = re.compile('\\W*')# listOfTokens=regEx.split(mysent)# tok=[tok.upper() for tok in listOfTokens if len(tok)&gt;0]# print tok## emailText=open('email/ham/6.txt').read()# listOfTokens=regEx.split(emailText)# print listOfTokensdef textParse(bigString): import re listOfTokens=re.split(r'\w*',bigString) return [tok.lower() for tok in listOfTokens if len(tok)&gt;2]def spamTest(): clf = Bayes.Naive_Bayes() docList=[] classList=[] fullText=[] for i in range(1,26): wordList=textParse(open('email/spam/%d.txt'%i).read()) docList.append(wordList) fullText.extend(wordList) classList.append(1) wordList=textParse(open('email/ham/%i.txt'%i).read()) docList.append(wordList) fullText.extend(wordList) classList.append(0) vocabList=clf._creatVocabList(docList) trainingSet=range(50);testSet=[] for i in range(10): randIndex=int(random.uniform(0,len(trainingSet))) testSet.append(trainingSet[randIndex]) del(trainingSet[randIndex]) trainMatix=[];trainClasses=[] for docIndex in trainingSet: trainMatix.append(clf._bagOfsetOfWordToVec(vocabList,docList[docIndex])) trainClasses.append(classList[docIndex]) p0V,p1V,pSpam=clf._trainNB0(array(trainMatix),array(trainClasses)) errorCount = 0 for docIndex in testSet: wordVector = clf._bagOfsetOfWordToVec(vocabList,docList[docIndex]) if clf._classifyNB(array(wordVector), p0V, p1V, pSpam)!=classList[docIndex]: errorCount+=1 print 'the error rate is :',float(errorCount)/len(testSet) 四、参考资料判别模型·生成模型·朴素贝叶斯方法维基百科：Naive Bayes classifier数学之美番外篇：平凡而又神奇的贝叶斯方法朴素贝叶斯理论推导与三种常见模型机器学习实战]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（9）：感知机]]></title>
    <url>%2F2017%2F02%2F10%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%889%EF%BC%89%EF%BC%9A%E6%84%9F%E7%9F%A5%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[Introduction感知机（perceptron）是二类分类的线性分类模型，输入为实例的特征向量，输出为实例的类别，取+1和-1二值。 感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，导入基于误分类的损失函数，利用梯度下降对损失函数进行极小化，求得感知机模型，属于判别模型 感知机学习算法简单易于实现，分为原始形式和对偶形式。1957年由Rosenblatt提出，是神经网络和支持向量机的基础 本章框架如下： 感知机模型 感知机的学习策略（损失函数） 感知机学习算法（原始形式与对偶形式），并证明算法的收敛性 一、 感知机模型1.1 感知机模型感知机是一种线性分类器，属于判别模型。 假设我们的输入空间（特征空间）是$\chi \subseteq R^{\boldsymbol{n}}$，输出空间是$\boldsymbol{y}=\left\{ +1,-1 \right\}$。输入$\boldsymbol{x}\in \boldsymbol{\chi }$表示实例的特征向量，对应于输入空间（特征空间）的点；输出$y\in \boldsymbol{y}$表示实例的类别。由输入空间到输出空间的函数 f\left( x \right) =\mathrm{sign}\left( \boldsymbol{w}·x+\boldsymbol{b}\right)其中，$\boldsymbol{w}\in \boldsymbol{R}^{\boldsymbol{n}}$为权值或权值向量，$\boldsymbol{b}\in \boldsymbol{R}^{\boldsymbol{n}}$叫做偏置，$\mathrm{sign}$是符号函数，即 \mathrm{sign}\left( \mathrm{x} \right) =\left\{ \begin{array}{l} +1\mathrm{，\ x}\geqslant 0\\ -1\mathrm{，\ x}0误分类点到超平面的距离： -\frac{1}{||w||}y_i\left( w·x+b \right)则误分类点到超平面的总距离： -\frac{1}{||w||}\sum_{x_i\in M}{y_i\left( w·x_i+b \right)}据上述我们定义损失函数为: L\left( w,b \right) =-\sum_{x_i\in M}{y_i\left( w·x_i+b \right)}其中$M$为误分类点的集合，此即为感知机学习的经验风险函数。一个特定样本点的损失函数，在误分类时是参数$w,b$的线性函数，在正确分类时是0.因此，给定训练数据集$T$，损失函数$L(w,b)$是$w,b$的连续可导函数。感知机学习的策略就是在假设空间中选取使损失函数最小的模型参数，即感知机模型。 三、感知机学习算法这样我们就把感知机的学习问题转化为求解损失函数的最优化问题，最优化的方法是随机梯度下降法。 3.1 感知机学习算法首先我们确定要求解的最优化问题是： \min_{w,b}L\left( w,b \right) =-\sum_{x_i\in M}{y_i\left( w·x_i+b \right)}通过随机梯度下降法来求解最优化问题。首先，任意选择一个超平面$w_0,b_0$，然后用梯度下降法不断地极小化目标函数，一次随机选取一个误分类点使其梯度下降，而不是一次使$M$中所有误分类点的梯度下降。 计算得到梯度为： \nabla _wL\left( w,b \right) =-\sum_{x_i\in M}{y_ix_i} \nabla _bL\left( w,b \right) =-\sum_{x_i\in M}{y_i}对权值进行更新： w\gets w+\eta y_ix_i b\gets b+\eta y_i其中$\eta$称为学习率，通过迭代可以期待损失函数不断减小，直到为0. 对于上述算法过程，我们可以有一个直观的解释：当一个实例点被误分类，则调整$w,b$的值，使分离超平面向该误分类点的一侧移动，以较少该误分类点与超平面的距离，直至超平面越过该误分类点使其被正确分类。当然感知机学习算法由于采用不同的初值或选取不同的误分类点，解可以不同。 3.2 对偶形式对偶形式的基本想法是，将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$w$和$b$，我们假设初始值$w_0$和$b_0$均为0。对误分类点($x_i$,$y_i$)通过 w\gets w+\eta y_ix_i b\gets b+\eta y_i逐步修改$w,b$,设修改$n$次，则最后学习到的$w,b$可以分别表示为 w=\sum_{i=1}^N{n_i\eta y_ix_i}=\sum_{i=1}^N{a_iy_ix_i} b=\sum_{i=1}^N{a_iy_i}当$\eta =1$时，表示第$i$个实例点由于误分而进行更新的次数。实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分类、换句话说，这样的实例对学习结果影响最大。 因为对偶形式的训练实例仅以内积的形式出现。为了方便，可预先将训练实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵。 G=\left[ x_i·x_j \right] _{N\times N}与原始形式一样，感知机学习算法的对偶形式迭代是收敛的，存在多个解。 总结感知机学习算法的对偶形式如下： 输入：线性可分的数据集训练数据集$T=\left\{ \left( x_1,y_1 \right) ,\left( x_2,y_2 \right) ,···,\left( x_N,y_N \right) \right\} $，其中$x_i\in \chi =\boldsymbol{R}^{\boldsymbol{n}}$$y_i\in \boldsymbol{y}=\left\{ -1,+1 \right\} ,i=1,2,···,N $，学习率$\eta \left( 0&lt;\eta \le 1 \right) $； 输出：$a,b$；感知机模型 f\left( x \right) =sign\left( \sum_{j=1}^N{a_jy_jx_j}·x+b \right)其中$a=\left( a_1,a_2,···,a_N \right) ^T$ 1）$a\gets 0,b\gets 0$ 2）在训练集中选取数据$\left( x_i,y_i \right)$ 3）如果$y_i\left( \sum_{j=1}^N{a_jy_jx_j·x_i+b} \right) \le 0$ a_i\gets a_i+\eta b\gets b+\eta y_i 4）转至(2)直到没有误分类数据 四、参考资料李航《统计学习方法》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>感知机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（8）：XgBoost]]></title>
    <url>%2F2017%2F01%2F26%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%888%EF%BC%89%EF%BC%9AXgBoost%2F</url>
    <content type="text"><![CDATA[一、XGBoost简介在数据建模中，经常采用Boosting方法通过将成百上千个分类准确率较低的树模型组合起来，成为一个准确率很高的预测模型。这个模型会不断地迭代，每次迭代就生成一颗新的树。但在数据集较复杂的时候，可能需要几千次迭代运算，这将造成巨大的计算瓶颈。 针对这个问题。华盛顿大学的陈天奇博士开发的XGBoost（eXtreme Gradient Boosting）基于C++通过多线程实现了回归树的并行构建，并在原有Gradient Boosting算法基础上加以改进，从而极大地提升了模型训练速度和预测精度。 在Kaggle的希格斯子信号识别竞赛，XGBoost因为出众的效率与较高的预测准确度在比赛论坛中引起了参赛选手的广泛关注，在1700多支队伍的激烈竞争中占有一席之地。随着它在Kaggle社区知名度的提高，最近也有队伍借助XGBoost在比赛中夺得第一。其次，因为它的效果好，计算复杂度不高，也在工业界中有大量的应用。 二、监督学习的三要素因为Boosting Tree本身是一种有监督学习算法，要讲Boosting Tree，先从监督学习讲起。在监督学习中有几个逻辑上的重要组成部件，粗略地可以分为：模型、参数、目标函数和优化算法。 2.1 模型模型指的是给定输入$x_i$如何去预测输出$y_i$。我们比较常见的模型如线性模型（包括线性回归和Logistic Regression）采用线性加和的方式进行预测 \hat{y}_i=\sum_j{w_jx_{ij}}这里的预测值$y$可以由不同的解释，比如我们可以把它作为回归目标的输出，或者进行$sigmoid$变换得到概率（即用$\frac{1}{1+e^{-\hat{y}_i}}$来预测正例的概率），或者作为排序的指标等。而一个线性模型根据$y$的解释不通（以及设计对应的目标函数）用到回归、分类或者排序等场景。 2.2 参数参数就是我们根据模型要从数据里头学习的东西，比如线性模型中的线性系数： \varTheta =\left\{w_j|j=1,2,···,d\right\}2.3 目标函数：误差函数+正则化项模型和参数本身指定了给定输入我们如何预测，但是没有告诉我们如何去寻找一个比较好的参数，这个时候就需要目标函数函数登场了。一般地目标函数包含两项：一项是损失函数，它说明了我们的模型有多拟合数据；另一项是正则化项，它惩罚了复杂模型。 1）$L(\varTheta)$：损失函数$L=\sum_{i=1}^n{l\left(y_i,\hat{y}_i\right)}$，常见的损失函数有： 平方损失：$l\left(y_i,\hat{y}_i\right)=\left(y_i-\hat{y}_i\right)^2$ Logistic损失：$l\left(y_i,\hat{y}_i\right)=y_i\ln\left(1+e^{-y_i}\right)+\left(1-y_i\right)\ln\left(1+e^{y_i}\right)$ 2）$\varOmega\left(\varTheta\right)$：正则化项，之所以要引入它是因为我们的目标是希望生成的模型能准确地预测新的样本（即应用于测试数据集），而不是简单地拟合训练集的结果（这样会导致过拟合）。所以需要在保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能。而正则化项就是用于惩罚复杂模型，避免模型过分拟合训练数据。常用的正则有$L1$正则与$L2$正则 $L1$正则（lasso）：$\varOmega\left(w\right)=\lambda ||w||_1$ $L2$正则：$\varOmega\left(w\right)=\lambda ||w||^2$ 这样目标函数的设计来自于统计学习里面的一个重要概念叫做Bias-variance tradeoff（偏差-方差权衡），比较感性的理解，$Bias$可以理解为假设我们有无限多数据的时候，可以训练出最好的模型所拿到的误差。而$Variance$是因为我们只有有限数据，其中随机性带来的误差。目标中误差函数鼓励我们的模型尽量去拟合训练数据，这样相对来说最后的模型会有比较少的$Bias$。而正则化项则鼓励更加简单的模型。因为当模型简单之后，有限数据拟合出来结果的随机性比较小，不容易过拟合，使得最后模型的预测更加稳定。 2.4 优化算法上面三个部分包含了机器学习的主要成分，也是机器学习工具划分模型比较有效的办法。其实这几部分之外，还有一个优化算法，就是给定目标函数之后怎么学的问题。有时候我们往往只知道“优化算法”，而没有仔细考虑目标函数的设计问题，比如常见的例子如决策树的学习算法的每一步去优化基尼系数，然后剪枝，但是没有考虑到后面的目标是什么。而这些启发式优化方法背后往往隐含了一个目标函数，理解了目标函数本身也有利于我们设计相应的学习算法。 三、回归树与树集成3.1 回归树在介绍$XGBoost$之前，首先得了解一下回归树和树集成的概念，其实在$AdaBoost$算法中已经详细讲述过这一部分了。Boosting Tree最基本的组成部分叫做回归树（regression tree），下面就是一个回归树的例子。它把输入根据输入的属性分配到各个叶子节点，而每个叶子节点上面都会有一个实数分数。具体地，下图给出了一个判断用户是否会喜欢电脑游戏的回归树模型，每个树叶的得分对应了该用户有多可能喜欢电脑游戏（分值越大可能性越大）。 3.2 树集成上图中的回归树只用到了用户年龄和性别两个信息，过于简单，预测的准确性自然有限。一个回归树往往过于简单无法有效地预测，因此一个更加强有力的模型叫做tree ensemble。在上图中使用两个回归树对用户是否喜欢电脑游戏进行了预测，并将两个回归树的预测结果加和得到单个用户的预测结果。在实际的预测模型建立过程中，我们通过不断地增加新的回归树，并给每个回归树赋予合适的权重，在此基础上综合不同的回归树得分获得更为准确的预测结果，这也就是树集成的基本思路。在预测算法中，随机森林和提升树都采用了树集成的方法，但是在具体地模型构造和参数调整的方法有所差别。 在这个树集成模型中，我们可以认为参数对应了树的结构，以及每个叶子节点上面的预测分数。 那么我们如何来学习这些参数。在这一部分，答案可能千奇百怪，但是最标准的答案始终是一个：定义合理的目标函数，然后去尝试优化这个目标函数。决策树学习往往充满了启发式算法，如先优化基尼系数，然后再剪枝，限制最大深度等等。其实这些启发式算法背后往往隐含了一个目标函数，而理解目标函数本身也有利于我们设计学习算法。 四、XGBoost的推导过程4.1 XGBoost的目标函数与泰勒展开对于tree ensemble，我们可以把某一个迭代后集成的模型写成为：\hat{y}_i=\sum_{k=1}^K{f_k\left(x_i\right)},\ f_k\in\mathscr{F}其中每个$f$是一个在函数空间($\mathscr{F}$)里面的函数，而$\mathscr{F}$对应了所有regression tree的集合。我们设计的目标函数也需要遵循前面的主要原则，包含两部分 Obj\left(\varTheta\right)=\sum_{i=1}^n{l\left(y_i,\hat{y}_i\right)}+\sum_{k=1}^K{\varOmega\left(f_k\right)}其中第一部分是训练损失，如上面所述的平方损失或者Logistic Loss等，第二部分是每棵树的复杂度的和。因为现在我们的参数可以认为是在一个函数空间里面，我们不能采用传统的如SGD之类的算法来学习我们的模型，因此我们会采用一种叫做additive training的方式。即每次迭代生成一棵新的回归树，从而使预测值不断逼近真实值（即进一步最小化目标函数）。每一次保留原来的模型不变，加入一个新的函数$f$到模型里面：其中$\hat{y}_i\left(t-1\right)$就是前$t-1$轮的模型预测，$f_t{(x_i)}$为新$t$轮加入的预测函数。这里自然就涉及一个问题：如何选择在每一轮中加入的$f(x_i)$呢？答案很直接，选取的$f(x_i)$必须使得我们的目标函数尽量最大地降低（这里应用到了Boosting的基本思想，即当前的基学习器重点关注以前所有学习器犯错误的那些数据样本，以此来达到提升的效果）。先对目标函数进行改写，表示如下：如果我们考虑平方误差作为损失函数，公式可改写为：更加一般的，对于不是平方误差的情况，我们可以采用如下的泰勒展开近似来定义一个近似的目标函数，方便我们进行下一步的计算。 泰勒展开一般表达式为：用泰勒展开来近似我们原来的目标：首先定义得到如果移除掉常数项，我们会发现这个目标函数有一个非常明显的特点，它只依赖于每个数据点的在误差函数上的一阶导数和二阶导数。可能有人会问，这个方式似乎比我们之前学过的决策树学习难懂。为什么要花这么多力气来做推导呢？ 这是因为，这样做首先有理论上的好处，它会使我们可以很清楚地理解整个目标是什么，并且一步一步推导出如何进行树的学习。然后这一个抽象的形式对于工程商实现机器学习工具也是非常有帮助的。因为它包含所有可以求到的目标函数，也就是说有了这个形式，我们写出来的代码可以用来求解包括回归、分类和排序的各种问题，正式的推导可以使得机器学习的工具更加一般化。 4.2 决策树的复杂度到目前为止我们讨论了目标函数中训练误差的部分。接下来我们讨论如何定义树的复杂度。我们先对于$f$的定义做一下细化，把树拆分成结构部分$q$和叶子权重部分$w$。其中结构部分$q$把输入映射到叶子的索引号上面去，而$w$给定了每个索引号对应的叶子分数是什么。当我们给定了如上定义之后，我们可以定义一棵树的复杂度如下。这个复杂度包含了一棵树里面节点的个数，以及每个树叶子节点上面输出分数的$L2$范数平方。当然这不是唯一的一种定义方式，不过这一定义方式学习出的树效果一般都比较不错。下图给出了复杂度计算的一个例子。 4.3 目标函数的最小化接下来是最关键的一步，在这种新的顶一下，我们可以把目标函数进行如下改写，其中$I$被定义为每个叶子上面样本集合$I_j=\{i| q(x_i)=j\}$这一目标包含了$T$个互相独立的单变量二次函数我们可以定义那么这个目标函数可以进一步改写成如下的形式，假设我们已经知道树的结构$q$，我们可以通过这个目标函数来求解出最好的$w$，以及最好的$w$对应的目标函数最大的增益可以观察到上式是由$T$个相互独立的单变量二次函数再加上$L1$范数构成。这样的特性意味着单个树叶的权重计算与其他树叶的权重无关，所以我们可以非常方便计算第$j$个树叶的权重，以及目标函数。由此，我们将目标函数转换为一个一元二次方程求最小值的问题（在此式中，变量为$w_j$，函数本质上是关于$w_j$的二次函数），略去求解步骤，最终结果如下所示：乍一看目标函数的计算与回归树的结构$q$函数没有什么关系，但是如果我们仔细回看目标函数的构成，就会发现其中$G_j$和$H_j$的取值是由第$j$个树叶上数据样本所决定的。而第$j$个树上所具有的数据样本则是由树结构$q$函数决定的。也就是说，一旦回归树的结构$q$确定，那么相应的目标函数就能够根据上式计算出来。那么回归树的生成问题也就转换为找到一个最优的树结构$q$，使得它具有最小的目标函数。 计算求得的$Obj$代表了当指定一个树的结构的时候，目标函数上面最多减少多少。我们可以把它叫做结构分数（structure score）。可以把它认为是类似于基尼系数一样更加一般的对于树结构进行打分的函数。下面是一个具体的打分函数计算的例子，它根据决策树的预测结果得到各样本的梯度数据，然后计算出实际的结构分数。这个分数越小，代表这个树的结构越好： 4.4 枚举树的结果——贪心法在前面分析的基础上，当寻找到最优的树结构时，我们可以不断地枚举不同树的结构，利用这个打分函数来寻找一个最优结构的树，加入到我们的模型中，然后再重复这样的操作。不过枚举所有树结构这个操作不太可行，在这里XGBoost采用了常用的贪心法，即每一次尝试区队已有的叶子加入一个分割。对于一个剧透的分割方案，我们可以获得的增益可以由如下公式计算得到： 这个公式形式上跟ID3算法（采用信息熵计算增益）或者CART算法（采用基尼指数计算增益） 是一致的，都是用分裂后的某种值减去分裂前的某种值，从而得到增益。为了限制树的生长，我们可以加入阈值，当增益大于阈值时才让节点分裂，上式中的$\gamma$即阈值，它是正则项里叶子节点数T的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。另外，上式中还有一个系数$\lambda$，是正则项里leaf score的$L2$模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性。 对于每次扩展，我们还是要枚举所有可能的分割方案，那么如何高效地枚举所有的分割呢？假设需要枚举所有$x&lt;a$这样的条件，那么对于某个特定的分割$a$我们要计算$a$左边和右边的导数和，在实际应用中如下图所示： 我们可以发现对于所有的$a$，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度与$G_L$和$G_R$。然后用上面的公式计算每个分割方案的分数就可以了。 但需要注意是：引入的分割不一定会使得情况变好，因为在引入分割的同时也引入新叶子的惩罚项。所以通常需要设定一个阈值，如果引入的分割带来的增益小于一个阀值的时候，我们可以剪掉这个分割。此外在XGBoost的具体实践中，通常会设置树的深度来控制树的复杂度，避免单个树过于复杂带来的过拟合问题。 以上介绍了如何通过目标函数优化的方法比较严格地推导出boosted tree的学习的整个过程。因为有这样一般的推导，得到的算法可以直接应用到回归，分类排序等各个应用场景中去。 五、QA5.1 机器学习算法中GBDT和XGBOOST的区别有哪些？ 基分类器的选择：传统GBDT以CART作为基分类器，XGBoost还支持线性分类器，这个时候XGBoost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 二阶泰勒展开：传统GBDT在优化时只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，XGBoost工具支持自定义损失函数，只要函数可一阶和二阶求导。 方差-方差权衡：XGBoost在目标函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数$T$、每个叶子节点上输出分数的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性。 Shrinkage（缩减）：相当于学习速率（xgboost中的$\epsilon$）。XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率） 列抽样（column subsampling）：XGBoost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是XGBoost异于传统GBDT的一个特性。 缺失值处理：XGBoost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。即对于特征的值有缺失的样本，XGBoost可以自动学习出它的分裂方向。 XGBoost工具支持并行：Boosting不是一种串行的结构吗?怎么并行的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第$t$次迭代的损失函数里包含了前面$t-1$次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block(块)结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 线程缓冲区存储：按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer（缓冲区），主要是结合多线程、数据压缩、分片的方法，然后再计算，提高算法的效率。 可并行的近似直方图算法：树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。 5.2 为什么在实际的 kaggle 比赛中 gbdt 和 random forest 效果非常好？转载自知乎这是一个非常好，也非常值得思考的问题。换一个方式来问这个问题：为什么基于 tree-ensemble 的机器学习方法，在实际的 kaggle 比赛中效果非常好？ 通常，解释一个机器学习模型的表现是一件很复杂事情，而这篇文章尽可能用最直观的方式来解释这一问题。 我主要从三个方面来回答楼主这个问题。 理论模型 （站在 vc-dimension 的角度） 实际数据 系统的实现 （主要基于 xgboost） 通常决定一个机器学习模型能不能取得好的效果，以上三个方面的因素缺一不可。 5.2.1 站在理论模型的角度统计机器学习里经典的 vc-dimension 理论告诉我们：一个机器学习模型想要取得好的效果，这个模型需要满足以下两个条件： 模型在我们的训练数据上的表现要不错，也就是trainning error 要足够小。 模型的vc-dimension要低。换句话说，就是模型的自由度不能太大，以防overfit. 当然，这是我用大白话描述出来的，真正的 vc-dimension 理论需要经过复杂的数学推导，推出 vc-bound. vc-dimension 理论其实是从另一个角度刻画了一个我们所熟知的概念，那就是 bias variance trade-off. 好，现在开始让我们想象一个机器学习任务。对于这个任务，一定会有一个 “上帝函数” 可以完美的拟合所有数据（包括训练数据，以及未知的测试数据）。很可惜，这个函数我们肯定是不知道的 （不然就不需要机器学习了）。我们只可能选择一个 “假想函数” 来 逼近 这个 “上帝函数”，我们通常把这个 “假想函数” 叫做 hypothesis. 在这些 hypothesis 里，我们可以选择 svm, 也可以选择 logistic regression. 可以选择单棵决策树，也可以选择 tree-ensemble (gbdt, random forest). 现在的问题就是，为什么 tree-ensemble 在实际中的效果很好呢？ 区别就在于 “模型的可控性”。先说结论，tree-ensemble 这样的模型的可控性是好的，而像 LR 这样的模型的可控性是不够好的（或者说，可控性是没有 tree-ensemble 好的）。为什么会这样？别急，听我慢慢道来。 我们之前说，当我们选择一个 hypothsis 后，就需要在训练数据上进行训练，从而逼近我们的 “上帝函数”。我们都知道，对于 LR 这样的模型。如果 underfit，我们可以通过加 feature，或者通过高次的特征转换来使得我们的模型在训练数据上取得足够高的正确率。而对于 tree-enseble 来说，我们解决这一问题的方法是通过训练更多的 “弱弱” 的 tree. 所以，这两类模型都可以把 training error 做的足够低，也就是说模型的表达能力都是足够的。但是这样就完事了吗？没有，我们还需要让我们的模型的 vc-dimension 低一些。而这里，重点来了。在 tree-ensemble 模型中，通过加 tree 的方式，对于模型的 vc-dimension 的改变是比较小的。而在 LR 中，初始的维数设定，或者说特征的高次转换对于 vc-dimension 的影响都是更大的。换句话说，tree-ensemble 总是用一些 “弱弱” 的树联合起来去逼近 “上帝函数”，一次一小步，总能拟合的比较好。而对于 LR 这样的模型，我们很难去猜到这个“上帝函数”到底长什么样子（到底是2次函数还是3次函数？上帝函数如果是介于2次和3次之间怎么办呢？）。所以，一不小心我们设定的多项式维数高了，模型就 “刹不住车了”。俗话说的好，步子大了，总会扯着蛋。这也就是我们之前说的，tree-ensemble 模型的可控性更好，也即更不容易 overfitting. 5.2.2 站在数据的角度除了理论模型之外, 实际的数据也对我们的算法最终能取得好的效果息息相关。kaggle 比赛选择的都是真实世界中的问题。所以数据多多少少都是有噪音的。而基于树的算法通常抗噪能力更强。比如在树模型中，我们很容易对缺失值进行处理。除此之外，基于树的模型对于 categorical feature 也更加友好。 除了数据噪音之外，feature 的多样性也是 tree-ensemble 模型能够取得更好效果的原因之一。通常在一个kaggle任务中，我们可能有年龄特征，收入特征，性别特征等等从不同 channel 获得的特征。而特征的多样性也正是为什么工业界很少去使用 svm 的一个重要原因之一，因为 svm 本质上是属于一个几何模型，这个模型需要去定义 instance 之间的 kernel 或者 similarity （对于linear svm 来说，这个similarity 就是内积）。这其实和我们在之前说过的问题是相似的，我们无法预先设定一个很好的similarity。这样的数学模型使得 svm 更适合去处理 “同性质”的特征，例如图像特征提取中的 lbp 。而从不同 channel 中来的 feature 则更适合 tree-based model, 这些模型对数据的 distributation 通常并不敏感。 5.2.3 站在系统实现的角度除了有合适的模型和数据，一个良好的机器学习系统实现往往也是算法最终能否取得好的效果的关键。一个好的机器学习系统实现应该具备以下特征： 正确高效的实现某种模型。我真的见过有些机器学习的库实现某种算法是错误的。而高效的实现意味着可以快速验证不同的模型和参数。 系统具有灵活、深度的定制功能。 系统简单易用。 系统具有可扩展性, 可以从容处理更大的数据。 到目前为止，xgboost 是我发现的唯一一个能够很好的满足上述所有要求的 machine learning package. 在此感谢青年才俊 陈天奇。在效率方面，xgboost 高效的 c++ 实现能够通常能够比其它机器学习库更快的完成训练任务。在灵活性方面，xgboost 可以深度定制每一个子分类器，并且可以灵活的选择 loss function（logistic，linear，softmax 等等）。除此之外，xgboost还提供了一系列在机器学习比赛中十分有用的功能，例如 early-stop， cv 等等 在易用性方面，xgboost 提供了各种语言的封装，使得不同语言的用户都可以使用这个优秀的系统。 最后，在可扩展性方面，xgboost 提供了分布式训练（底层采用 rabit 接口），并且其分布式版本可以跑在各种平台之上，例如 mpi, yarn, spark 等等。 有了这么多优秀的特性，自然这个系统会吸引更多的人去使用它来参加 kaggle 比赛。 综上所述，理论模型，实际的数据，良好的系统实现，都是使得 tree-ensemble 在实际的 kaggle 比赛中“屡战屡胜”的原因]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>XgBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笑忘录（7）：以赛亚·伯林 谈话录 摘]]></title>
    <url>%2F2017%2F01%2F23%2F%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%887%EF%BC%89%EF%BC%9A%E4%BB%A5%E8%B5%9B%E4%BA%9A%C2%B7%E4%BC%AF%E6%9E%97%20%E8%B0%88%E8%AF%9D%E5%BD%95%20%E6%91%98%2F</url>
    <content type="text"><![CDATA[其一他反对那种以为可以依据科学的、政治的、甚至美学的价值在人世间创造一个乌托邦的主张。鉴于人类历史实际上是各种经常相互碰撞的价值和思想的产生地及其变化发展的实验场所这一事实，伯林追溯多元论在伦理学、政治学和美学等领域的出现。 但是，如果我说的不错，不光最终解决这个概念本身是不切实际的，而且，各种价值之间也不可避免地是相互碰撞的、不可协调的。最终解决的可能性（即使我们忘记了这个词组带有希特勒时期的恐怖感）会制造出一种幻觉，一种非常危险的幻觉。因为，如果人们真的相信这种解决是可能的，那么，为了达到这个目标付出多少都绝不为过：为了使人类永远公正、幸福、富于创造性以及和谐协调，有什么不可以为此付出的呢？为了做成这样的蛋卷，我们可以打破无限数量的鸡蛋，这就是列宁、托洛茨基以及我们所了解的波尔布特的信念。既然我知道通往社会问题最终解决的唯一正确道路，我也就知道人类车队必须沿着什么路线走；因为你没有我这种知识，你就不能有选择自由，哪怕是最低限度的选择自由，否则你就达不到目的地。你声明采取某种方式将使你更幸福、更自由，或将使你有自由呼吸的空间，而我知道你这样想是错误的。我知道你需要什么，人民大众需要什么。如果出现由于无知或恶意而酿成的反抗，那就必须振亚下去，为了大多数人永远幸福，消灭成千上万人也许是必要的。除了心甘情愿地将他们全都牺牲掉，我们，明白此中道理的我们，又有什么选择？“（扭曲的人性之材） 《往事与随想》赫尔岑 选择概念（关键地位） 在思想史的工作中，伯林研究了那些勇敢地、公开地跟占统治地位的理性体系作斗争的思想，赞赏他们的观点和立场。他特别重视这些思想家的自由思想。正式对这种普遍存在的自由思想的肯定和褒赏，显示了他的思想史研究具有重要意义。通过他的研究工作，伯林向我们宣示，在人类历史上没有绝对的价值，而且，人类历史与众多悲剧性后果相伴，充满着那些企图通过坚信最终绝对真理而避免做出悲剧性选择的人们的困苦。 普列汉诺夫 伯林：”普列汉诺夫的确是一位富有才华的马克思主义著作家。我完全被他的书迷住了，因为他学识渊博，说理精辟，行文机智，情趣横生，极富吸引力。他是真正的马克思主义之父“ 赫尔岑 伯林：”赫尔岑成了我的人生楷模。他是一个非常杰出的作家，一个敏锐的真正的政治思想家，非常有独创性。他的自传大概是我一生中度过的最精彩的自传，比卢梭的自传还要好。正是赫尔岑使我爱上了社会思想史和政治思想史，这就是我研究思想史的真正的开端。“ 《日瓦戈医生》 帕斯捷尔纳克 伯林：”他当然是一位伟大的诗人。这么说吧，诗人可以有两种类型。第一种诗人，写诗时是诗人，而写散文诗是作家，像普希金。第二种诗人，写诗时是诗人，而写散文时也是诗人。帕斯捷尔纳克就属于第二种类型的诗人。他的散文总是诗化的散文，我看他本质上不是一个散文作家。他是晚近俄罗斯伟大的诗人之一，他的小说是伟大的诗化小说。尽管他置身于那虚浮造作的时代中心，却能诚实地描写爱——男主人公对女主人公的爱，极少有作家能够做到这点。正是他的诗使他赢得俄国人和阅读俄文作品的外国人的广泛钦佩，实际上，只有约瑟夫·布罗茨基的成就可与他相媲美。阿赫玛托娃和曼德尔施塔姆都差得远。依我看，帕斯捷尔纳克在各方面都堪称是活着的最优秀的俄国诗人。但是并非所有天才都表里如一，帕斯捷尔纳克也同样如此。他谈起话来稀奇古怪，经常让听者捉摸不透，但总让你感到才气逼人。再也没有比听他谈话更迷人的事了。据我的体会，只有弗吉尼亚·伍尔夫谈到某些东西时像他那样迷人。当然弗吉尼亚·伍尔夫有点狂妄。“ 马雅可夫斯基 伯林：”他是一个大胆的革新者，一个惊人的雄辩家，一个真正的革命者，但是，他的诗作我看比不上帕斯捷尔纳克、曼德尔施塔姆和阿赫玛托娃。“ 如果你对某些思想有兴趣，并引起你思考一些问题，那么你就不能不考虑这些思想的历史。因为思想不是单子，他们不是在真空中产生的，而跟人们的信念、生活方式、人生观和世界观紧密相连。思想之间相互碰撞和影响，并不断呈现，成为所谓”智性气候“的组成部分，他和物质因素一样，形成人们的行为和感情，并且历史地变迁着。 哲学不是一种积累性的学科。古代那些基本的哲学思想、观点、理论和见解现在仍然是哲学的中心内容。他们有其特定的横贯历史的生命。 哲学，如果教得好的话，其用处之一就是让人透过政治上冠冕堂皇的辞藻，识别各种谬论、欺骗、恶作剧、赘疣、感情上的讹诈，以及各种各样的诡辩和伪装，他能大大增强人们的批判能力。 哲学不外是要在看不到办法的地方力图去寻找问题的答案。自我理解是哲学的主要目的之一，哲学的目标就是要理解人、事物、词语三者之间的相互关系。 在叔本华那里我没有发现包罗万象的形而上学大厦那样的东西。人们可能对叔本华的体系一无所知，但照样能从他许多尖锐的有时是深刻的见解中获得教益。黑格尔的体系在我看来，似乎是希腊神话中的独眼巨人波吕斐摩斯的阴森森的黑洞，一进去就出不来，每一个脚印都指示一条道路，正如拉丁诗人所说的那样。 伯林：”他（指施特劳斯）和他的门生都相信，善于恶、对与错都直接得自某种先天的启示，某种”形而上学之眼“，也就是靠使用柏拉图式的那种无缘分享的理性官能。柏拉图、亚里士多德、《圣经》、犹太教法典、迈蒙尼德，也许还有阿奎那和中世纪的其他经院哲学家，都通晓什么样的人生才是最美好的，他们的门生现在也执着于此，而我却没有这种荣幸。“ 其二世界上存在的一切不外乎就是人、物和人脑中的观念————目标、情感、希望、畏惧、选择、想象的情景和所有其他形式的人类经验。这就是我所认识的全部东西。我无法做到无所不知。也许有一个永恒真理和永恒价值的世界，有一种只有真正的思想家才能具有的魔眼，而这只属于恐怕我永远无法进入的极少数精英的领地。 在关键时期，在历史转折关头，当各种因素大体上平衡地出现的时候，个人以及他们的抉择的行动，本身不一定可被预见（确实很少被预见），但却能决定历史的进程。我不相信历史是一部戏剧（这是赫尔岑使用的概念，他认为历史不是一部多幕剧，一部由上帝或大自然赋予主题的演出，不是有图像可辨的地毯）。而马克思和黑格尔都认为历史是一部有结局的多幕剧，它在达到高潮之后（在马克思看来，要经过可怕的冲突、苦难、灾祸）天堂之门将会启开，那就是戏剧的收场，历史（马克思称为史前时期）便会从此结束，一切事物都永恒协调，人们将合乎理性地合作共事。 维科和赫尔德相信历史进程有一定的形式，但不相信历史是一部有结局的戏剧。 我感兴趣的是维科和赫尔德的文化多元性的信念。实际上，每种文化都有自己的重心，各种文化有着各不相同的、新颖的、不可预见的思想及其互相冲突的倾向。维科最先理解到，文化就是世界相对于社会的意义，就是男男女女对于他们自身与别人和环境发生关联的集体意识；文化影响思想、情感、行为、举动的特定形式；文化是多种多样的。维科划分不同时期的文化，赫尔德则对不同时代的不同民族的文明做了区分。 我认为历史不是一个呆板的单线条的进步过程。伏尔泰说，历史是理性、知识和艺术品创造不断进步的过程，有时被可怕的干扰所打破，突然陷入野蛮状态。 预言是一种普遍却难以信赖的活动。我在维科和赫尔德的著作中经所读到的，是人类历史固有的文化多样性的观点。历史并不直线行走，不同文化之间相互作用，有时就是因果性质的作用。通向未来或过去的道路不存在唯一的钥匙，他不好跟自然科学作类比。后者的定律对重复出现的因果链是开放的，这样的因果链能总结为一般的规律。 我们能够理解不同民族和地区人们的生活方式（即使他们的生活方式跟我们的差异很大，即使他们憎恨我们或有时候被我们所谴责），这样的事实表明，我们大家能够穿越时空进行沟通。当我们认了理解了那些与我们在文化上有很大差别的群体的时候，即意味着某种强大的富于同情心的理解、洞察和共感的存在。即便其他文化排斥我们，依靠移情的想象力，我们也可以设想，为什么他们会产生这样的思想和感情，并采取相应的行动达到预定的目标。 一般性的价值观是有的，这是关于人类的经验事实，莱布尼茨称为事实的真理而不是原理性的真理。不同时空的芸芸众生，绝大多数人都共同拥有某些价值观，不论这些价值观是否自觉明晰，也不论他们在态度、举止和行动上的表现如何。另一方面，人与人之间、社会与社会之间，又确实存在着很大的差异。如果你确实了解了个人之间、团体之间、民族之间、各个完整的文明之间所存在的差异，运用想象进入他们的思想、情感世界、设想你自己置身于他们的生存环境中会怎样认知世界并审视自己与他们的关系，那么，即使你对所观察到的东西很反感（全部了解当然并不等于全部谅解），也肯定会减少盲目的偏执和狂热。想象会产生狂热，但通过想象洞察了不同于自己的境况，结果必定能减少狂热。 我认为，人们在把一个有思考里的人称为疯子或神经错乱者时务必小心谨慎。迫害不是来自神经错乱，而产生于把骇人听闻的谬误深信为真理，进而导致罄竹难书的恶果。 了解自己及他人，懂得理性的方法，掌握作为知识和全部科学基础的证据，以及力图验证直观确定性，这些对我们来说都有着根本的重要性。人权这个观念建立在一个正确的信念之上，那就是普遍存在着某种特定的品性。自由、正义、对幸福的追求、真诚、爱。这符合整个人类的利益，而不只是符合作为这个或那个民族、宗教、职业、身份的成员的利益。满足这些要求，保护人们这些要求不被忽视或否认，都是正当的。 你必须懂得什么是正义，什么是自由，什么是社会契约，并对不同类别的自由、权威、义务作出区分等等。政治理论的分野往往围绕“为什么有些人要服从理你些人”这个中心问题来展开。多数政治理论都是对这个问题的回答。实质上不是为什么服从，而是为什么应该服从和服从到什么程度。关于消极自由的问题是：拦在我面前有什么障碍要排除？其他人怎样妨碍着我？其他人这样做是有意的还是无意的？是间接的还是有制度依据的？ 关于积极自由的问题是：谁管我？别人管还是自己管？如果是别人，他凭借什么权利？什么权威？如果我有权自主，自己管自己，那么，我会不会失去这个权利？能不能丢掉这个权利？放弃这个权利再恢复这个权利？具体怎么做？还有，谁制定法律？或谁执行法律？征求过我的意见吗？是多数人在统治吗？为什么？是因为上帝、牧师、还是党？是出于公共舆论的压力？传统的压力？还是摄于什么权威？ 积极自由在正常生活中虽然更重要，但与消极自由相比更频繁地被歪曲和滥用。历史上虚伪的积极自由所造成的危害比现代虚伪的消极自由所造成的危害更大。 真诚地相信错误的东西是很危险的，是没有道德价值或精神价值的，至少是令人遗憾的。 自由社会的好处在于容许各种各样相互冲突的意见存在而不被压制。 多元论确认：既然对于道德和政治问题以至任何价值问题不可能有一个最终的解答，并且，人们给出的或有权给出的某些解答是相互矛盾的，那么，在实际生活的某些领域，有些价值便可能变得互不相容，这样，如果要避免破坏性的冲突的话，就应该妥协，而最低限度的宽容，不管你情不情愿，都是必不可少的。 人们可以选择一种生活或另一张生活，而不能同时过两种生活；没有更为根本的标准用来决定正确的选择；既然选择这种也行，选择那种也行，在客观上就不能说一种生活优于另一种生活。它是人们想做什么和成为什么的问题。 浪漫主义认为，价值不是发现而是创造出来的，生活的目的就是生活本身。生活就是生活，没有目的。 政治哲学的任务是审视生活。要做的事就是审查为实现各种社会目标而提出的种种主张的合理性，检查为确定和实现这些目标而采取的种种方法的正当性。政治哲学要力图澄清构成有关观点的词和概念，使人们能理解自己相信的是什么，自己的行动表示什么。政治哲学还对那些维护或者反对人们所追求的各种目标的辩论作出评价，并防止麦克米兰所引述的胡说八道。 对人类的问题，追求一种唯一的、最后的、普遍的解决，无异于是追求海市蜃楼。对于人类生活破坏严重的，莫过于那种迷信了：凡美好生活都是跟政治或者军事力量相联结的。 多数英国哲学家似乎都太单薄、太技术化；跟英国哲学家相比，多数法国哲学家似乎都太含糊、太夸饰。 我认为马基雅维利是指出现实的各种价值是相互冲突的第一人。依照马基雅维利的观点，你可以选择做一个罗马人或一个基督教徒和殉道者，或者起码可以做一个当权者统治下的受害者。 任何真正的问题在某种意义上都是当代的问题。 我认为维科是理解了并告诉我们什么是人类文化的第一人。他不自觉地确立了文化的观念。就我所知，在他之前没有谁有过这样的想法，要努力去重构人们是如何看待生活在周围环境中的自己，如何看待（或感受）与自己发生关联的自然界和其他人————作为在时间中持续存在下来的一类生灵。他反思思想、情感、世界观等各类行为以及肉体的、情绪的、理智的、精神的等多种反应的本质，而正是这些行为和反应构成了文化。如果你想了解人们怎样生活，你必须了解他们的崇拜仪式，文字的内涵，他们通常运用什么类型的想象、明喻、隐喻，他们如何吃、喝、抚养小孩，如何看待自己，如何过私人的、社会的、经济的和政治的生活等等。作为一种模式的文化不是一个孤立的有机体，而是一种存在方式，树立这种理念正是维科对思想史的主要贡献。维科的值得重视的观点是：各不相同的人类思想、行为、感情和行动是互相联结和互相启发的。 米什莱按照维科的思路，认为历史就是社会跟自然力量作斗争并力图运用自然力量去创造让人们能生存和发展的生活方式的历史。人的历史是跟自然界，跟各种力量，跟一切人为的和非人为的障碍进行斗争的历史，这就是米什莱关于人类从各种羁绊中朝向自我解放不断进步的观点。 我们谈论自然界，但我们所知道的自然界仅是我们在外部世界所发现的东西。我们也看见和感触我们的身体，但我们还能说出他作为人之具体化有什么样的感受，这是一种“内在的审视”；人既是观察者，又是行动者。这就是“新科学”的大概意思。理解对于意图、感情、希望、恐惧、努力、意识和无意识的认识，而科学是对处在空间中的物体的认识。换句话说，我们可以看见桌子是什么样的，但我们看不见桌子为什么是这样的。理解过去的文化就是去理解前任所追求的东西；他们怎样看待与他人发生关联的自己，怎样看待世界以及生活在这个世界中的自己。 其三在赫尔德看来，“归属于”的意思是，你说什么，不必多做解释，大家就能了解；你的姿态、语言、所有参与交流的因素，不需敬你熟悉的人作介绍，大家都能把握。是语言、习惯、姿态或本能的反应创造了联合和团结，即创造了具有自己特色的观点、文化和社会共同体。 我一开始读维科的著作简直就被他迷住了。我总是从接受邀请做讲演或写文章开始研究的。 唯一真实的东西是精神，是人与上帝的关系，人与人的关系，别无其他了。内在的精神，个人的灵魂的底蕴，内心世界，这是唯一实在的东西，至于礼节、学问和教阶制度，统统不在话下。 赫尔德乐观地相信，人类大花园中的所有花卉都能和谐地生长，各种文化都能相互激励，为创造这种和谐的境界作出自己的贡献。绝不主张政治上的民族主义，政治上的民族主义必然大致侵略和培植民族自豪感。一个民族不是一个国家，而是一个文华实体，同一民族的人说共同的语言，生活在共同的地域，有着共同的习惯、共同的历史额共同的传统。 依我的看法，强烈的民族主义不过是耻辱心理的表现。高度发达的民族不会产生民族主义。民族主义是对伤害的反应。民资注意对一切事物均构成威胁。民族主义就等于我们对自己说，因为我们是德国人或法国人，所以我们是最优秀的人，我们完全有权做我们要做的事，一旦你把一切行为的根据放在民族这个超越个人的权威上，那就会扩展到政党，到阶级，到教会，通往压迫的道路便从此打开了。 你不能阻止科学的进步，造成灾难的不是武器，而在于使用武器的人。智能的进步是不能阻止的，人们所能做的是防止科学的滥用。廓清腐败的社会，荡涤一切污泥浊水，然后再向前进。 熊彼特正确的说过，那些相信观念必定绝对不变的人是偶像崇拜者。文明意味着必须允许变化的可能，意味着永不停息地去追求自己信奉的理想，为之献身也在所不惜。 不同的个人、集团。文化之间可以沟通，因为人的价值并非无限地多；他们共属于一条水平线，即客观的常常又相互矛盾的人类价值，在他们之间必须进行（常常是痛苦的）选择。 我自己感觉不到有这种既在现实生活之内又超越现实生活的实在。我不是宗教徒。但我对信教者的宗教体验评价颇高。我深深地被犹太教堂也包括基督教堂和伊斯兰教寺院中的宗教仪式所打动。我想，不理解信教是怎么一回事的人恐怕也不理解人为什么而活着。因此干巴巴的无神论者都是瞎子聋子，不了解人生的深刻体验，或者说不了解人生的内在底蕴，就像瞎子不能欣赏美景一样。光有感觉能力的人不能充分理解他人，包括信教者、不信教者、神秘主义者、儿童、诗人、艺术家等等。 我有一种深信不疑的看法，有些道德的、社会的和政治的价值是相互抵触的，任何一个社会总有一些价值是不能彼此调和的。换句话说，人们爱以生存的某种最终的价值，不光在实践上而且在原则上、在概念上都是不可兼得的，或者说不可彼此结合的。没有哪一个精于心机的人，同时又是无所计较，一切都听其自然的人。你不能把充分的自由跟充分的平等结合起来。给狼充分自由就不能同时也给羊有充分自由。正义和慈悲，知识和幸福，如此等等，都可能相互冲突，不可兼得。既然是这样，人类的问题（归根到底是如何生活的问题）就不可能全都求得完满的解决。这不是因为实际上有困难，找不到妥善的解决方法，而是因为这些价值本身在概念性质上都是有缺陷的。乌托邦式的解决在原则上没有缺陷，可以成立，但这样的解决是企图把不可结合的东西结合起来。某些人类的价值之所以不能相互结合，就因为他们本身是不能并存的。因此只能在彼此之间进行选择。选择可能很痛苦，如果你选择A，你就得忍痛失去B。在最终的各种人类价值之间不可避免要作出这样的选择。在任何可以想象的社会，选择都可能是痛苦而且是不可避免的。互不相容的价值本身始终是不能相容的。我们所能做的是防止选择太痛苦，这就意味着，我们需要有一种机制，使得人们对各种价值的追求尽可能不违背自己深刻的道德信念。在多元化的自由社会里，不可避免要作出各种妥协和折中，经过权衡利弊而避免最坏的情况。再三斟酌，取其一方。平等多重要？自有多重要？正义多重要？慈悲多重要？善良对重要？真理多重要？掂量掂量就知道了。知识和幸福也不总是牢牢结合的。一个知道自己患了癌症的人不会因为有了这种知识而感到幸福；无知会使他少一些自由，但同时却使他觉得多了一些幸福。这就是说，人生问题的某种最终解决，没有普遍适用的始终不变的可行保准。那些相信可能有完美无缺的社会的人必定以为，为了实现这种美好的社会，作出多大牺牲都是必要的，为了达到这种理想的目标，付出多大的代价都是值得的。他们想，如果必须要流血才能创造这种美好的社会，那么就流血吧，不管流谁的血，也不管流多少血。不打破鸡蛋怎么能做出上等的蛋卷，课时人们一旦养成打破鸡蛋的习惯，他们久不久罢手，鸡蛋打破了，蛋卷却没有做成。凡是以为对人生问题可以求得最终解决的这种狂热的信念，不能把导致灾难、痛苦、流血和可怕的压迫。]]></content>
      <categories>
        <category>笑忘录</category>
      </categories>
      <tags>
        <tag>以赛亚·伯林</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（7）：GBDT]]></title>
    <url>%2F2017%2F01%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%887%EF%BC%89%EF%BC%9AGBDT%2F</url>
    <content type="text"><![CDATA[引言GBDT（Gradient Boosting Decision Tree）是一种迭代的决策树算法，又叫 MART（Multiple Additive Regression Tree)，它通过构造一组弱的学习器（树），并把多颗决策树的结果累加起来作为最终的预测输出。该算法将决策树与集成思想进行了有效的结合。 GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合。自算法的诞生之初，它就和SVM一起被认为是泛化能力（generalization）较强的算法。近些年来更因为被用于构建搜索排序的机器学习模型而引起广泛的关注。它最早见于yahoo，后被广泛应用在搜索排序、点击率预估上。业界中，Facebook使用其来自动发现有效的特征、特征组合，来作为LR模型中的特征，以提高 CTR预估（Click-Through Rate Prediction）的准确性；GBDT在淘宝的搜索及预测业务上也发挥了重要作用。 除此之外，GBDT还是目前竞赛中最为常用的一种机器学习算法，因为它不仅可以适用于多种场景，而且相比较于其他算法还有着出众的准确率，如此优异的性能也让GBDT收获了机器学习领域的“屠龙刀”这一赞誉。 本文首先介绍GBDT中的DT，即回归树，这是它的基础算法；然后叙述提升树，它是以决策树为基函数的提升方法；接着介绍GBDT中的GB，即梯度提升；最后导出GBDT算法的整个流程。 二、Regression Desicion Tree：回归树2.1 回归树简介树模型也分为决策树和回归树，决策树常用来分类问题，回归树常用来预测问题。决策树常用于分类标签值，比如用户性别、网页是否是垃圾页面、用户是不是作弊；而回归树常用于预测真实数值，比如用户的年龄、用户点击的概率、网页相关程度等等。 回归树总体流程类似于分类树，区别在于，回归树的每一个节点都会得到一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值寻找最优切分变量和最优切分点，但衡量的准则不再是分类树中的基尼系数，而是平方误差最小化。也就是被预测错误的人数越多，平方误差就越大，通过最小化平方误差找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 由于GBDT的核心在与累加所有树的结果作为最终结果，而分类树得到的离散分类结果对于预测分类并不是这么的容易叠加（稍等后面会看到，其实并不是简单的叠加，而是每一步每一棵树拟合的残差和选择分裂点评价方式都是经过公式推导得到的），而对基于回归树所得到的数值进行加减是有意义的（例如10岁+5岁-3岁=12岁），这是区别于分类树的一个显著特征（毕竟男+女=是男是女?，这样的运算是毫无道理的），GBDT在运行时就使用到了回归树的这个性质，它将累加所有树的结果作为最终结果。所以GBDT中的树都是回归树，而不是分类树，它用来做回归预测，当然回归树经过调整之后也能用来做分类。 2.2 回归树的生成首先看一个简单的回归树生成实例： 接下来具体说说回归树是如何进行特征选择生成二叉回归树的。假设$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集 D=\{(x_1,y_1),(x_2,y_2),···,(x_N,y_N)\}我们利用最小二乘回归树生成算法来生成回归树$f(x)$，即在训练数据集所在的输入空间中，递归地将每个区域分为两个子区域并决定每个子区域上的输出值，构建二叉决策树，步骤如下： 1）选择最优切分变量$j$与切分点$s$，求解 \min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1\left(j,s\right)}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2\left(j,s\right)}{\left(y_i-c_2\right)^2}\right]遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值得对$j,s$ 2）用选定的对$(j,s)$划分区域并决定相应的输出值： R_1\left(j,s\right)=\left\{x|x^{\left(j\right)}\le s\right\}\ ,\ R_2\left(j,s\right)=\left\{x|x^{\left(j\right)}>s\right\} \hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_2\left(j,s\right)}{y_i}\ ,\ x\in R_m\ ,\ m=1,2 3）继续对两个子区域调用步骤（1），（2），直至满足停止条件。 4）将输入空间划分为$M$个区域$R_1,R_2,···,R_M$，在每个单元$R_m$上有一个固定的输出值$c_m$，生成决策树： f\left(x\right)=\sum_{m=1}^M{\hat{c}_m\textrm{I}\left(\textrm{x}\in\textrm{R}_{\textrm{m}}\right)} 三、Boosting Decision Tree：提升树3.1 提升树模型提升方法采用加法模型（即基函数的线性组合）与前向分布算法。以决策树为基函数的提升方法称为提升树（Boosting tree）。对分类问题构建的决策树是二叉分类树，对回归问题构建决策树是二叉回归树。提升树是迭代多棵回归树来共同决策。当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差 = 真实值 - 预测值 。提升树即是整个迭代过程生成的回归树的累加。提升树模型可以表示为决策树的加法模型： f_M\left(x\right)=\sum_{m=1}^M{T\left(x;\varTheta_m\right)}其中$T\left(x;\varTheta_m\right)$表示决策树；$\varTheta_m$为决策树的参数；$M$为树的个数。 3.2 提升树算法对回归问题的提升树算法来说，给定当前模型 $f_{m-1}{(x)}$只需要简单地拟合当前模型的残差。现将回归问题的提升树算法叙述如下： 1）初始化$f_0{(x)}=0$ 2）对$m=1,2,···,M$ a）计算残差r_{mi}=y_i-f_{m-1}\left(x_i\right)\ ,\ i=1,2,···,N b）拟合残差$r_{mi}$学习一个回归树，得到 $T\left(x;\varTheta_m\right)$ c）更新$f_m{(x)}=f_{m-1}{(x)}+T(x;\varTheta_m )$ 3）得到回归问题提升树 f_M\left(x\right)=\sum_{m=1}^M{T\left(x;\varTheta_m\right)} 接下来通过训练一个用于预测年龄的模型来展现算法的运行流程 1）首先，训练集有4个人$A,B,C,D$，它们的年龄分别是$14,16,24,26$，其中$A,B$分别是高一和高三学生；$C,D$分别是应届毕业生和工作两年的员工，可用于分枝的特征包括上网时长、购物金额、上网时段和对百度知道的使用方式。如果是一棵传统的回归决策树来训练，会得到下图所示结果： 2）但是如果用GBDT来做这件事，由于数据太少，我们限定叶子节点最多有两个，即每棵树都只有一个分枝，并且限定只限定两棵树。我们会得到如下所示结果：第一棵树的分枝与之前一样，也是使用购物金额进行区分，两拨人各自用年龄均值作为预测值，得到残差值-1、1、-1、1，然后拿这些残差值替换初始值去训练生成第二棵回归树，如果新的预测值和残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。第一棵树的分枝与之前一样，也是使用购物金额进行区分，两拨人各自用年龄均值作为预测值，得到残差值-1、1、-1、1，然后拿这些残差值替换初始值去训练生成第二棵回归树，如果新的预测值和残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。第二棵树只有两个值1和-1，直接可分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。 3）将两棵回归树预测结果进行汇总，解释如下： A：14岁高一学生；购物较少；经常问学长问题；预测年龄A = 15 – 1 = 14 B：16岁高三学生；购物较少；经常被学弟问问题；预测年龄B = 15 + 1 = 16 C：24岁应届毕业生；购物较多，经常问师兄问题；预测年龄C = 25 – 1 = 24 D：26岁工作两年员工；购物较多，经常被师弟问问题；预测年龄D = 25 + 1 = 26 对比初始的回归树与GBDT所生成的回归树，可以发现，最终的结果是相同的，那我们为什么还要使用GBDT呢？ 答案就是对模型过拟合的考虑。过拟合是指为了让训练集精度更高，学到了很多“仅在训练集上成立的规律”，导致换一个数据集后，当前规律的预测精度就不足以使人满意了。毕竟，在训练精度和实际精度（或测试精度）之间，后者才是我们想要真正得到的。 在上面这个例子中，初始的回归树为达到100%精度使用了3个特征（上网时长、时段、网购金额），但观察发现，分枝“上网时长&gt;1.1h”很显然过拟合了，不排除恰好A上网1.5h, B上网1小时，所以用上网时间是不是&gt;1.1小时来判断所有人的年龄很显然是有悖常识的。 而在GBDT中，两棵回归树仅使用了两个特征（购物金额与对百度知道的使用方式）就实现了100%的预测精度，其分枝依据更合乎逻辑（当然这里是相比较于上网时长特征而言），算法在运行中也体现了“如无必要，勿增实体”的奥卡姆剃刀原理。 3.3 提升树实例下表为训练数据，$x$的取值范围为区间$[0.5,10.5]$，$y$的取值范围为区间$[5.0,10.0]$，学习这个回归问题的提升树模型，考虑只用二叉树作为基函数：（1）步骤一：求$f_1(x)$即回归树$T_1(x)$ 1）首先通过以下优化问题： \min_s\left[\min_{c_1}\sum_{x_i\in R_1}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2}{\left(y_i-c_2\right)^2}\right]求解训练数据的切分点$s$： R_1=\left\{x|x\le s\right\},R_2=\left\{x|x>s\right\}容易求得在$R_1$，$R_2$内部使平方误差达到最小值的$c_1,c_2$为 c_1=\frac{1}{N_1}\sum_{x_i\in R_1}{y_i}\\ ,\\ c_2=\frac{1}{N_2}\sum_{x_i\in R_2}{y_i}这里$N_1,N_2$是$R_1,R_2$的样本点数。 2）具体地，求解训练数据的切分点。根据所给数据，考虑如下切分点： 1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5对各切分点，不难求出相应的$R_1,R_2,c_1,c_2$及 m\left(s\right)=\min_{c_1}\sum_{x_i\in R_1}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2}{\left(y_i-c_2\right)^2}例如，当$s=2.5$时， R_1=\{1,2\}，R_2=\{3,4,···,9,10\}，c_1=5.63,c_2=7.73 m\left(s\right)=\min_{c_1}\sum_{x_i\in R_1}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2}{\left(y_i-c_2\right)^2}=12.07遍历所有的$s$，计算$m(s)$，结果列表如下：可知当$s=6.5$时$m(s)$达到最小值，此时 R_1=\{1,2,···,6\},R_2=\{7,8,9,10\},c_1=6.24,c_2=8.91所以回归树$T_1(x)$为 T_2\left(x\right)=\left\{\begin{matrix} 6.24& x]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（6）：AdaBoost]]></title>
    <url>%2F2017%2F01%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%886%EF%BC%89%EF%BC%9AAdaBoost%2F</url>
    <content type="text"><![CDATA[一、集成学习1.1 定义所谓集成学习（ensemble learning），是指通过构建多个弱学习器，然后结合为一个强学习器来完成分类任务。并相较于弱分类器而言，进一步提升结果的准确率。严格来说，集成学习并不算是一种分类器，而是一种学习器结合的方法。 下图显示了集成学习的整个流程：首次按产生一组“个体学习器”，这些个体学习器可以是同质的（homogeneous）（例如全部是决策树），这一类学习器被称为基学习器（base learner），相应的学习算法称为“基学习算法”；集成也可包含不同类型的个体学习器（例如同时包含决策树和神经网络），这一类学习器被称为“组件学习器”（component learner）。 集成学习通过将多个学习器进行结合，可获得比单一学习器显著优越的泛化性能，它基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好，直观一点理解，就是我们平时所说的“三个臭皮匠，顶个诸葛亮”，通过使用多个决策者共同决策一个实例的分类从而提高分类器的泛化能力。 1.2 集成学习的条件当然，这种通过集成学习来提高学习器（这里特指分类器）的整体泛化能力也是有条件的： 首先，分类器之间应该具有差异性，即要有“多样性”。很容易理解，如果使用的是同一个分类器，那么集成起来的分类结果是不会有变化的。‘ 其次，每个个体分类器的分类精度必须大于0.5，如果$p&lt;0.5$那么随着集成规模的增加，分类精度会下降；但如果是大于0.5的话，那么最后最终分类精度是可以趋于1的。 因此，要获得好的集成，个体学习器应该“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”，即学习器间具有差异。 1.3 集成学习的分类当前，我们可以立足于通过处理数据集生成差异性分类器，即在原有数据集上采用抽样技术获得多个训练数据集来生成多个差异性分类器。根据个体学习器的生成方式，目前集成学习方法大致可分为两大类：第一类是个体学习器之间存在强依赖关系、必须串行生成的序列化方法，这种方法的代表是“Boosting”；第二类是个体学习器间不存在强依赖关系、可同时生成的并行化方法，它的代表是“Bagging”和“Random Forest” Bagging：通过对原数据进行有放回的抽取，构建出多个样本数据集，然后用这些新的数据集训练多个分类器。因为是有放回的采用，所以一些样本可能会出现多次，而其他样本会被忽略。该方法是通过降低基分类器方法来改善泛化能力，因此Bagging的性能依赖于基分类器的稳定性，如果基分类器是不稳定的，Bagging有助于减低训练数据的随机扰动导致的误差，但是如果基分类器是稳定的，即对数据变化不敏感，那么Bagging方法就得不到性能的提升，甚至会降低。 Boosting：提升方法是一个迭代的过程，通过改变样本分布，使得分类器聚集在那些很难分的样本上，对那些容易错分的数据加强学习，增加错分数据的权重，这样错分的数据再下一轮的迭代就有更大的作用（对错分数据进行惩罚）。 Bagging与Boosting的区别： 二者的主要区别是取样方式不同。Bagging采用均匀取样，而Boosting根据错误率来取样，因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各轮训练集之间相互独立，而Boostlng的各轮训练集的选择与前面各轮的学习结果有关；Bagging的各个预测函数没有权重，而Boosting是有权重的；Bagging的各个预测函数可以并行生成，而Boosting的各个预测函数只能顺序生成。对于象神经网络这样极为耗时的学习方法。Bagging可通过并行训练节省大量时间开销。 bagging是减少variance，而boosting是减少bias。Bagging 是 Bootstrap Aggregating 的简称，意思就是再取样 (Bootstrap) 然后在每个样本上训练出来的模型取平均，所以是降低模型的 variance. Bagging 比如 Random Forest 这种先天并行的算法都有这个效果。Boosting 则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行加权，所以随着迭代不断进行，误差会越来越小，所以模型的 bias 会不断降低。这种算法无法并行。 二、AdaBoost算法2.1 AdaBoost算法思想对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确地分类规则（强分类器）容易得多。提升算法就是从弱学习算法出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。 这样，对提升方法来说，有两个问题需要回答：一是在每一轮如果改变训练数据的权值或概率分布；二是如何将弱分类器组合成一个强分类器。对于第一个问题，AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注，于是，分类问题就被一系列的弱分类器“分而治之”。至于第二个问题，即弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率较大的弱分类器的权值，使其在表决中起较小的作用。 AdaboostBoost的算法的框架如下图所示具体来说，整个AdaBoost算法包括以下三个步骤： 1）初始化训练样本的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：$1/N$。 2）训练弱分类器。具体训练过程中，如果某个样本已经被准确地分类，那么在构造下一个训练集中，它的权值就会被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本被用于训练下一个分类器，整个训练过程如果迭代地进行下去，使得分类器在迭代过程中逐步改进。 3）将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中权重较大，否则较小。得到最终分类器。 2.2 AdaBoost算法流程现在叙述AdaBoost算法。假定给定一个二类分类的训练数据集 T=\{(x_1,y_1),(x_2,y_2),···,(x_n,y_n)\}其中$y_i$属于二分类的标记组合，即$y_i\in\{+1,-1\}$，AdaBoost算法利用一下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合成一个强分类器。 步骤一：首先，初始化训练数据的权值分布。假设每一个训练样本最开始时都被赋予相同的权值：$1/N$，即每个训练样本在基本分类器的学习中作用相同，这一假设保证步骤一能够在原始数据上学习基本分类器$G_1{(x)}$，数学化的语言表示为： D_1=\left(w_{11},w_{12},···,w_{1i},···,w_{1N}\right)\ ,\ w_{1i}=\frac{1}{N}\ ,i=1,2,···,N步骤二：AdaBoost反复学习基本分类器，在每一轮$m=1,2,···,M$顺次执行下列操作： 1）使用当前权值分布为$D_m$的训练数据集，学习得到基分类 G_m\left(x\right):\chi\rightarrow\left\{-1,+1\right\} 2）计算上一步得到的基分类器$G_m{(x)}$在训练数据集上的分类误差率$e_m$为 e_m=P\left(G_m\left(x\right)\ne y_i\right)=\frac{\sum_{i=1}^N{w_{mi}I\left(G_m\left(x_i\right)\ne y_i\right)}}{\sum_{i=1}^N{w_{mi}}}=\sum_{i=1}^N{w_{mi}I\left(G_m\left(x_i\right)\ne y_i\right)}这里$w_{mi}$表示第$m$轮中第$i$个实例的权值，$\sum_{i=1}^N{w_{mi}=1}$。这表明，$G_m{(x)}$在加权的训练数据集上的分类误差率是被$G_m{(x)}$误分类样本的权值之和，由此可以看出数据权值分布$D_m$与基本分类器$G_m{(x)}$的分类误差率的关系。 3）计算$G_m$前面的权重系数$a_m$，该系数表示$G_m$在最终分类器中的重要程度，目的在于使我们得到基分类器在最终分类器中所占的权值，系数计算公式如下： a_m=\frac{1}{2}\log\frac{1-e_m}{e_m}这里的对数是自然对数，由表达式可知，当$e_m≤\frac{1}{2}$时，$a_m≥0$，并且$a_m$随着$e_m$的减小而增大，意味着分类误差越小的基本分类器在最终分类器的作用越大，而$e_m≥\frac{1}{2}$则刚好相反，这正好验证了集成学习中每个个体分类器的分类精度必须大于0.5的前提条件。 4）更新训练数据集的权值分布为下一轮作准备 D_{m+1}=\left(w_{m+1,1},w_{m+1,2},···,w_{m+1,i},···,w_{m+1,N}\right)\,\,其中 w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right),i=1,2,···,N我们也可以写成： w_{m+1,i}=\left\{\begin{matrix} \frac{w_{mi}}{Z_m}e^{-a_m}\ ,& G_m\left(x_i\right)=y_i\\ \frac{w_{mi}}{Z_m}e^{a_m}\ ,& G_m\left(xi\right)\ne y_i\\ \end{matrix}\right.由此可知，被基本分类器$G_m{(x)}$误分类样本的权值得以扩大，而被正确分类样本的权值得以缩小。两两比较，误分类样本的权值$e^{2a_m}=\frac{e_m}{1-e_m}$倍。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作业，这是AdaBoost的一个特点。这里我们还引入了一个规范化因子，它的作用在于使$D_{m+1}$成为一个概率分布。具体公式为Z_m=\sum_{i=1}^N{w_{mi}}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right),i=1,2,···,N重复步骤二中的1至4步骤，得到一系列的权重参数$a_m$和基分类器$G_m$。 步骤三：将上一步得到的基分类器根据权重参数线性组合 f\left(x\right)=\sum_{m=1}^M{a_mG_m\left(x\right)}得到最终分类器$G_{(x)}$ G\left(x\right)=sign\left(f\left(x\right)\right)=sign\left(\sum_{m=1}^M{a_mG_m\left(x\right)}\right)线性组合$f(x)$实现了$M$个基本分类器的加权表决。系数$a_m$表示了基本分类器$G_m{(x)}$的重要性，这里，所有的$a_m$之和并不为1。$f(x)$的符号决定实例$x$的类，$f(x)$的绝对值表示分类的确信度，利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点。 2.3 AdaBoost算法的一个实例下图为给定的训练样本，假设$Y\in \{+1,-1\}$，且弱分类器由$xv$产生（$v$为阈值，目的在于使分类器在训练样本上的分类误差率最低），接下来我们就要使用AdaBoost算法得到一个强分类器。 首先，初始化训练数据的权值分布，得到： D_1=(w_{11},w_{12},w_{1,10}) , w_{1i}=\frac{1}{10},i=1,2,····,10 在此基础上，开始M轮迭代。 根据X和Y的对应关系，要把这10个数据分为两类，一类是1，一类是-1，根据数据的特点发现：$（0，1，2，6，7，8）$对应的类是1，$(3,4,5,9)$对于的类是-1，抛开孤独的9不说，$(0,1,2),(3,4,5),(6,7,8)$这是3类不同的数据，分别对应的类是$(1,-1,1)$,直观上推测可知，可以找到对应的数据分界点，比如$2.5、5.5、8.5$，将这几类数据分成两类。 1.第一次迭代（m=1）: 1）在第一次迭代时，已知$w_{1i}=\frac{1}{10}$，经过计算可得：在权值分布为$D_1$的训练数据上，阈值$v$取2.5或8.5时分类误差率为0.3，取5.5时分类误差率为0.4，遵循分类误差率最低原则，从2.5或8.5 中任意选取一个阈值，这里选取2.5，故基本分类器为G_1\left(x\right)=\left\{\begin{matrix}1&x2.5\\\end{matrix}\right. 2）$G_1{(x)}$在训练集上的误差率：e_1=P(G_1{(x_i)≠y_i})=0.3 3) 根据$e_1$计算得到$G_1{(x)}$的系数：a_1=\frac{1}{2}\log\frac{1-e_1}{e_1}=0.4236这个系数就代表$G_1{(x)}$在最终的分类函数中所占的权值。 4）更新训练数据的权值分布，用于下一轮迭代 D_{m+1}=\left(w_{m+1,1},w_{m+1,2},···,w_{m+1,i},···,w_{m+1,N}\right)\,\,其中 w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp\left(-\alpha_my_iG_m\left(x_i\right)\right),i=1,2,···,N由此得到$D_2=(0.0715,0.0715,0.715,0.0715,0.0715,0.715,0.166,0.166,0.166,0.0715)$根据$D_2$可知，分对的样本权重由0.1下降到了0.0715，分错的样本$(6,7,8)$的权值由0.1上升至0.166。此时分类函数为$f_1{(x)}=0.4236G_1{(x)}$，第一个分类器$sign[f_1{(x)}]$在训练样本上有三个误分类点（第一轮的误分类点即第一个基分类器的误分类点）。 2.第二次迭代（m=2）: 1）在上一轮迭代中，我们获知了新一轮的权重分布$D_2$，在此基础上，经过计算可得，阈值$v$是8.5时分类误差率最低，因此第二个基本分类器为G_2\left(x\right)=\left\{\begin{matrix}1&x8.5\\\end{matrix}\right. 2）误差率：e_2=P(G_2{(x_i)≠y_i})=0.0715×3=0.2143 3）$G_2{(x)}$的系数为:a_2=\frac{1}{2}\log\frac{1-e_2}{e_2}=0.6496 4）更新训练样本的权值分布，得到$D_3=(0.0455,0.0455,0.0455,0.1667，0.1667,0.1667,0.1060,0.1060,0.1060,0.0455)$，相较于$D_2$，被分错的样本3，4，5的权值变大，其他被分对的样本的权值变小。经过第二轮迭代后，分类函数为$f_2{(x)}=0.4236G_1{(x)}+0.6496G_2{(x)}$，第二个分类器为$sign[f_2{(x)}]=sign[0.4236G_1{(x)}+0.6496G_2{(x)}]$。将10个样本点依次带入到第二个分类器中，可得到此时依然有着3个误分类点$(3,4,5)$，为此需要进行下一轮迭代。 3.第三次迭代（m=3）: 1）在上一轮迭代中，我们获知了新一轮的权重分布$D_3$，在此基础上，经过计算可得，阈值$v$是5.5时分类误差率最低，因此第三个基本分类器为 G_3\left(x\right)=\left\{\begin{matrix}{} 1& x>5.5\\ -1& x]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（5）：随机森林]]></title>
    <url>%2F2017%2F01%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%885%EF%BC%89%EF%BC%9A%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[一、基本原理顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。 我们可以这样比喻随机森林算法：每一棵决策树就是一个精通于某一个窄领域的专家（因为我们从M个特征中选择m个让每一棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。 下图为随机森林算法的示意图： 随机森林算法有很多优点： 在数据集上表现良好，两个随机性的引入，使得随机森林不容易陷入过拟合 在当前的很多数据集上，相对其他算法有着很大的优势，两个随机性的引入，使得随机森林具有很好的抗噪声能力 它能够处理很高维度（feature很多）的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化 可生成一个$Proximities=（p_{ij}）$矩阵，用于度量样本之间的相似性： $p_{ij}=a_{ij}/N$,$ a_{ij}$表示样本i和j出现在随机森林中同一个叶子结点的次数，N随机森林中树的颗数 在创建随机森林的时候，对generlization error使用的是无偏估计 训练速度快，可以得到变量重要性排序（两种：基于OOB误分率的增加量和基于分裂时的GINI下降量 在训练过程中，能够检测到feature间的互相影响 容易做成并行化方法 实现比较简单 二、随机森林的生成2.1 生成步骤步骤如下： 1）如果训练集大小为$N$，对于每棵树而言，随机且有放回地从训练集中抽取$N$个训练样本（bootstrap抽样方法），作为该树的训练集；每棵树的训练集都是不同的，但里面包含重复的训练样本 2）如果每个样本的特征维度为$M$，指定一个常数$m$，且$m$&lt;$M$，随机地从$M$个特征中选取$m$个特征子集，每次树进行分裂时，从这$m$个特征中选择最优的； 3）每棵树都尽可能最大程度地生长，并且没有剪枝过程。 2.2 影响分类效果的参数随机森林的分类效果（即错误率）与以下两个因素有关： 1）森林中任意两棵树的相关性：相关性越大，错误率越大 2）森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低 减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。 2.3 袋外误差率如何选择最优的特征个数m，要解决这个问题，我们主要依据计算得到的袋外错误率oob error（out-of-bag error）。 随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。 我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。所以对于每棵树而言，部分训练实例没有参与这棵树的生成，它们称为第k棵树的oob样本。 袋外错误率（oob error）计算方式如下： 1）对每个样本计算它作为oob样本的树对它的分类情况 2）以简单多数投票作为该样本的分类结果 3）最后用误分个数占样本总数的比率作为随机森林的oob误分率 三、随机采样与完全分裂 在建立每一棵决策树的过程中，有两点需要注意，分别是采样与完全分裂。 3.1 随机采样首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M个feature中，选择m个(m &lt;&lt; M)。 3.1.1 有放回抽样的解释如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是”有偏的”，都是绝对”片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是”求同”，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是”盲人摸象”。 3.1.2 对Bagging的改进随机森林对Bagging的改进就在于随机采用的不同，即以下两点： 1）Random forest是选与输入样本的数目相同多的次数（可能一个样本会被选取多次，同时也会造成一些样本不会被选取到），而bagging一般选取比输入样本的数目少的样本； 2）bagging是用全部特征来得到分类器，而Random forest是需要从全部特征中选取其中的一部分来训练得到分类器； 一般Random forest效果比bagging效果好！ 3.2 完全分裂之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。一般很多的决策树算法都一个重要的步骤 - 剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。 按这种算法得到的随机森林中的每一棵都是很弱的，但是大家组合起来就很厉害了。 四、随机森林的变体也可以使用SVM、Logistic回归等其他分 类器，习惯上，这些分类器组成的“总分类器”，仍然叫做随机森林。 比如回归问题，图中离散点为臭氧(横轴)和温度(纵轴)的关系，试拟合变化曲线，记原始数据为D，长度为N(即图中有N个离散点) 算法过程为： 1）做100次bootstrap，每次得到的数据Di，Di的长度为N 2）对于每一个Di，使用局部回归(LOESS)拟合一条曲线(图 中灰色线是其中的10条曲线) 3）将这些曲线取平均，即得到红色的最终拟合曲线 4）显然，红色的曲线更加稳定，并且没有过拟合明显减弱]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>随机森林</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（4）：决策树]]></title>
    <url>%2F2017%2F01%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[本文结合李航博士的《统计学习方法》与周志华老师的《机器学习》决策树部分，《统计学习方法》重理论的证明推导，《机器学习》注重讲解算法的特点与扩展。 INTRODUCTION决策树（Decision Tree）是数据挖掘中一种基本的分类和回归方法，它呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，可以认为是$if-then$规则的集合，也可认为是定义在特征空间与类空间上的条件概率分布。下图是一个简单的决策树示例： 决策树模型的主要优点是模型具有可读性，分类速度快。在学习时，利用训练数据，根据损失函数最小化原则建立决策树模型；而在预测时，对新的数据，利用决策树模型进行分类。主要的决策树算法有ID3算法、C4.5算法和CART算法。 一个性能良好的决策树，是一个与训练数据矛盾较小的决策树，同时又具有很好地泛化能力。言外之意就是说，好的决策树不仅对训练样本有很好的分类效果，对于测试集也有较低的误差率。一个决策树的学习过程包括三个步骤：特征选择、决策树的生成以及决策树的修剪。 一、决策树模型的两种解释1.1 决策树模型分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶节点。内部结点表示一个特征或属性，叶节点表示一个类。 1.1.1 决策树与if-then规则可以将决策树看成一个if-then规则的集合。即由决策树的根结点到叶节点的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。 决策树的路径或其对应的if-then规则集合的重要性质：互斥且完备（每一个实例都被一条路径或一条规则所覆盖，且只被一条路径或一条规则所覆盖，这里的覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件） 1.1.1 决策树与条件概率分布决策树还表示给定特征条件下类的条件概率分布，它定义在特征空间的一个划分。将特征空间划分为互不相交的单元，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的每一条路径对应于划分中的一个单元。 假设$X$为表示特征的随机变量，$Y$为表示类的随机变量，那么这个条件概率分布可以表示为$P(X|Y)$,各叶结点上的条件概率往往偏向于某一个类，即属于某一类的概率越大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。 二、特征选择2.1 特征选择问题若利用一个特征进行分类的结果与随机分类的结果没有很大差异，则称这个特征是没有分类能力的。特征选择的准则是信息增益或信息增益比。直观上，若一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割为子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益可以表示这一直观的准则。 2.2 信息增益2.2.1 熵在信息论与概率统计中，熵表示随机变量不确定性的度量。设$X$是一个取有限个值得离散随机变量，其概率分布为 P\left( X=x_i \right) =p_i,i=1,2,···,n则随机变量$X$的熵定义为 H\left( X \right) =-\sum_{i=1}^n{p_i\log p_i}若$p_i$等于0，定义$0log0=0$，熵的单位为比特或者纳特。 2.2.2 条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望 H\left( Y|X \right) =\sum_{i=1}^n{p_iH\left( Y|X=x_i \right)}经验熵和经验条件熵：当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵和条件经验熵。 2.2.3 信息增益信息增益表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度。特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即 g\left( D,A \right) =H\left( D \right) -H\left( D|A \right)一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。 于是我们可以应用信息增益准则来选择特征，信息增益表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。对数据集$D$而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。 2.2.4 信息增益算法根据信息增益准则的特征选择方法为对训练数据集（或子集）$D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。 在描述算法前，先对符号进行说明：设训练数据集为$D$，$|D|$表示其样本容量，即样本个数。设有$K$个类$C_k$,$k=1,2,···,K$,$|C_k|$为属于类$C_k$的样本个数，$\sum_{k=1}^K{|C_k|=|D|}$。设特征$A$有$n$个不同的取值${a_1,a_2,···,a_n}$,根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,···,D_n$,$|D_i|$为$D_i$的样本个数，$\sum_{i=1}^n{|D_i|=|D|}$。记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$,即$D_{ik}=D_i\cap{C_k}$,$D_{ik}$为$D_{ik}$的样本个数。 具体算法步骤如下： 1）计算数据集$D$的经验熵$H(D)$H\left( D \right) =-\sum_{k=1}^K{\frac{|C_k|}{|D|}\log _2\frac{|C_k|}{|D|}} 2）计算特征$A$对数据集$D$的经验条件熵$H(D|A)$H\left( D|A \right) =\sum_{i=1}^n{\frac{|D_i|}{|D|}H\left( D_i \right)}=-\sum_{i=1}^n{\frac{|D_i|}{|D|}\sum_{k=1}^K{\frac{|D_{ik}|}{|D_i|}\log _2\frac{|D_{ik}|}{|D_i|}}} 3）计算信息增益g\left( D,A \right) =H\left( D \right) -H\left( D|A \right) 2.3 信息增益比以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。 信息增益比表示特征$A$对训练数据集$D$的信息增益比。$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比，即 g_R\left( D,A \right) =\frac{g\left( D,A \right)}{H_A\left( D \right)}2.4 基尼系数分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼系数定义为 Gini\left( p \right) =\sum_{k=1}^K{p_k\left( 1-p_k \right) =1-\sum_{k=1}^K{p_k^2}}若样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分，即 D_1=\left\{ \left( x,y \right) \in D|A\left( x \right) =0 \right\} \mathrm{，}D_2=D-D_1则在特征A的条件下，集合$D$的基尼指数定义为 Gini\left( D,A \right) =\frac{|D_1|}{|D|}Gini\left( D_1 \right) +\frac{|D_2|}{|D|}Gini\left( D_2 \right)基尼系数Gini(D)表示集合$D$的不确定性，表示经A=a分割后集合D的不确定性。基尼系数越大，样本集合的不确定性越大，与熵类似。 从下图可以看出基尼指数和熵之半的曲线很接近，都可以近似地代表分类误差率。 三、决策树的生成3.1 ID3算法ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地建构决策树。 其具体方法为：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。ID3相当于用极大似然法进行概率模型的选择。但是ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。 其算法步骤如下： 1） 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将类$C_k$作为该结点的类标记，返回$T$; 2）若$A=\varnothing $,则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$; 3） 否则，按算法5.1计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$; 4） 如果$A_g$的信息增益小于阈值$\varepsilon $，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$ 5） 否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$,将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子节点构成树$T$,返回$T$ 6） 对第i个子结点，以$D_i$为训练集,以$A-{A_g}$为特征集，递归地调用（1）~（5），得到子树$T_i$，返回$T$。 3.2 C4.5与ID3算法相似，C4.5算法对ID3算法进行了改进，C4.5在生成的过程中，用信息增益比来选择特征 3.3 CART分类树与回归树（classification and regression tree，CART）模型（Breiman）由特征选择、树生成及剪枝组成，既可用于分类也可用于回归。CART是在给定输入随机变量X条件下输出变量Y的条件概率分布的学习方法。它假定决策树是二叉树，内部取值为“是”（左分支）和“否”（右分支）。它的基本步骤为 1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大。 2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这是用损失函数最小作为剪枝的标准。 3.3.1 分类树对分类树用基尼系数（Gini index）最小化准则，进行特征选择，生成二叉树。 具体算法步骤如下： 1）设结点的训练数据集为D，计算现有特征对该数据集的基尼指数。此时，对每一个特征A，对其可能取的每个值$a$，根据样本点对$A=a$的测试为”是”或者“否”将D分割为$D_1$和$D_2$两部分，计算其基尼系数。 2）在所有可能的特征A以及他们所有可能的切分点$a$中，选择基尼系数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。 3）对两个子结点递归地调用上述两个步骤，直至满足停止条件。 4）生成CART决策树 3.3.2 回归树首先看一个简单的回归树生成实例： 接下来具体说说回归树是如何进行特征选择生成二叉回归树的。假设$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集 D=\{(x_1,y_1),(x_2,y_2),···,(x_N,y_N)\}我们利用最小二乘回归树生成算法来生成回归树$f(x)$，即在训练数据集所在的输入空间中，递归地将每个区域分为两个子区域并决定每个子区域上的输出值，构建二叉决策树，步骤如下： 1）选择最优切分变量$j$与切分点$s$，求解 \min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1\left(j,s\right)}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2\left(j,s\right)}{\left(y_i-c_2\right)^2}\right]遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值得对$j,s$ 2）用选定的对$(j,s)$划分区域并决定相应的输出值： R_1\left(j,s\right)=\left\{x|x^{\left(j\right)}\le s\right\}\ ,\ R_2\left(j,s\right)=\left\{x|x^{\left(j\right)}>s\right\} \hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_2\left(j,s\right)}{y_i}\ ,\ x\in R_m\ ,\ m=1,2 3）继续对两个子区域调用步骤（1），（2），直至满足停止条件。 4）将输入空间划分为$M$个区域$R_1,R_2,···,R_M$，在每个单元$R_m$上有一个固定的输出值$c_m$，生成决策树： f\left(x\right)=\sum_{m=1}^M{\hat{c}_m\textrm{I}\left(\textrm{x}\in\textrm{R}_{\textrm{m}}\right)} 四、决策树的剪枝4.1 剪枝决策树的过拟合指的是学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决过拟合的办法是考虑决策树的复杂度，对已生成的决策树进行简化，即剪枝（从已生成的树上裁剪调一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型）。 设树$T$的叶结点个数为$|T|$,$t$是树$T$的叶结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,···,K$，$H_t(T)$为叶结点$t$上的经验熵，$a≥0$为参数，则决策树学习的损失函数可以定义为 { C }_{ a }(T)=\sum _{ t=1 }^{ |T| }{ { N }_{ t }{ H }_{ t }(T)+a } |T|其中经验熵为 { H }_{ t }(T)=-\sum _{ k }{ \frac { { N }_{ tk } }{ N_{ t } } log\frac { { N }_{ tk } }{ { N }_{ t } } }在损失函数中，将右端第一项记作 C\left( T \right) =\sum_{t=1}^{|T|}{N_tH_t\left( T \right) =-\sum_{t=1}^{|T|}{\sum_{k=1}^K{N_{tk}\log \frac{N_{tk}}{N_t}}}}这时有 C_a\left( T \right) =C\left( T \right) +a|T|其中，$C(T)$表示模型对训练数据的预测误差,即模型与训练数据的拟合程度，$|T|$表示模型复杂度，参数$a≥0$控制两者之间的影响。较大的$a$促使选择较简单的模型，较小的$a$促使选择较复杂的模型。$a=0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。 决策树生成只考虑了通过信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。此损失函数的极小化等价于正则化的极大似然估计，即利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。 4.2 CART剪枝CART剪枝算法从“完全生长”的决策树的底端减去一些子树，使决策树变小（模型变简单），从而能够对未知数据有更准确的预测其具体步骤如下： 1）首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个字数序列$\left\{ T_0,T_1,T_{2,….,}T_n \right\}$; 在剪枝过程中，计算子树的损失函数： C_a\left( T \right) =C\left( T \right) +a|T|其中，$T$为任意子树，$C(T)$为对训练数据的预测误差（如基尼系数），$|T|$为子树的叶结点个数，$a≥0$为参数，$C_a(T)$为参数是$a$时的子树$T$的整体损失。参数$a$权衡训练数据的拟合程度与模型的复杂度。 对固定的$a$，一定存在使损失函数$C_a(T)$最小的子树，将其表示为$T_a$。$T_a$在损失函数$C_a(T)$最小的意义下是最优的，且是唯一的。$a$大的时候，最优子树$T_a$偏小；当$a$小的时候，最优子树$T_a$偏大。极端情况，$a=0$时，整体树是最优的。当$a\rightarrow \infty$，根结点组成的单结点树是最优的。 Breiman等人证明：可以用递归地方法对树进行剪枝。将$a$从小增大，$0=a_0&lt;a_1&lt;…..a_n&lt;+\infty$产生一系列的区间$[a_i,a_{i+1})$,$i=0,1,…,n$;剪枝得到的子树序列对应着区间$a\in \left[ a_i,a_{i+1} \right)$，$i=0,1,2,…,n$的最优子树序列为$\left\{ T_0,T_1,T_2,…,T_n \right\}$，序列的子树是嵌套的。 具体地，从整体树$T_0$开始剪枝，对$T_0$的人以内部结点$t$，以$t$为单结点树的损失函数是 C_a\left( t \right) =C\left( t \right) +\alpha以$t$为根结点的子树$T_t$的损失函数是 C_a(T_t)=C(T_t)+a|T_t|当$a=0$及$a$充分小时，有不等式 C_a\left( T_t \right)小于C_a\left( T \right)当$a$增大时，在某一$a$有 C_a(T_t)等于C_a(t)当$a$再增大时，有不等式 C_a(T_t)大于C_a(T)只要$\alpha =\frac{C\left( t \right) -C\left( T_t \right)}{|T_t|-1}$ ，$T_t$与$t$有相同的损失函数值，而$t$的结点少，因此$t$比$T_t$更可取，对$T_t$进行剪枝。 为此，对$T_0$中的每一个内部结点$t$，计算 g\left( t \right) =\frac{C\left( t \right) -C\left( T_t \right)}{|T_t|-1}它表示剪枝后整体损失函数减少的程度。在$T_0$中剪去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$a_1$，$T_1$为区间$[a_1,a_2)$的最优子树。 如此剪枝下去，直至得到根结点。在这一过程中，不断得增加$a$的值，产生新的区间。 2）在剪枝得到的子树序列$T_0,T_1,…,T_n$中通过交叉验证选取最优子树$T_a$ 具体地，利用独立的验证数据集，测试子树序列$T_0,T_1,…,T_n$中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中，每棵子树$T_0,T_1,…,T_n$都对应一个参数$a_1,a_2,…,a_n$。所以当最优子树$T_k$确定时，对应的$a_k$也就确定了，即得到最由决策树$T_a$. 五、参考资料李航《统计学习方法》周志华《机器学习》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笑忘录（6）：胡乱记]]></title>
    <url>%2F2017%2F01%2F15%2F%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%886%EF%BC%89%EF%BC%9A%E8%83%A1%E4%B9%B1%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[其一：人女人的风度在于表达她对自己的看法，以及界定别人对待她的分寸。她的风度从姿态、声音、见解、表情、服饰、品味和选定的场合上体现出来——实际上，她所作的一切，无一不为她的风度增色。……男性观察女性；女性注意自己被别人观察。这不仅决定了大多数的男女关系，还决定了女性自己的内在关系，女性自身的观察者是男性，而被观察者为女性。这样被动主动的交互构成了男人们最基本的窥淫欲和恋物癖的现实表现以及女人在这个环境下被预设为被男人品赏的艺术品的尴尬局面。 知识分子对人民的引导应该是一个缓慢感化逐渐深入的过程，一开始就让老百姓穿高端的品牌，吃喝上等的烟酒是不切实际的，他们穿着不合适，吃进去还反胃，最后还弄个“文化强行入侵”的罪名，如此悲剧来源于知识分子自以为是的旨趣，该杜绝。阳春白雪对下里巴人所持有的态度不应是嫌弃，该是怜悯。 我们都是时代的弄潮儿，这不错，但是必须要清楚地认识到，我们弄得潮有大也有小，也有压根儿就不知道怎么弄潮，你不该为之抱怨，这是历史的必然，总会有潜藏的智慧不被挖掘，总会有好马遇不到伯乐。但是，于自身来说，你不该愤世骇俗，你得用尽全力去弄点潮出来，加把劲，再加把劲，说不定就弄出大潮来了嘞。 其实我们真的太年轻，年轻到不懂得原来爱情到了考虑现实的时候，到了和金钱权利挂钩之后，就再也不是原来的摸样的了。所以，趁自己还没有沾染那些东西的时候，好好地珍惜这份来之不易的爱情。如果你要考虑现实，那就去考虑吧，跟着自己的心走。每个人确实需要更长远的眼光，要看到远方的事情，但是青春真的就那么就一眨眼就不在了，有多少人做着自己喜欢的事情，爱着真正爱的人，过着内心喜欢和向往的生活？有多少人在不停忙碌着，却不知是为了什么？有多少人在混混沌沌着，也不知是为了什么？有太多的牵绊，有太多的不舍，有太多的不甘心。待到我们放下这些所谓的不舍、牵绊与不甘心，也正是我们真正活着的时候。一个人永远守护另一个人是不可能的。希望你记住我，记住我曾经这样存在过。记忆这东西总有些不可思议。身临其境的时候，几乎未曾意识到那片风景，未曾觉得它有什么撩人情怀之处，但到了十几年之后我们可能还无法忘怀。我老是介于“不充分”和“完全不够”之间，我总是感到饥渴，真想拼着劲儿得到一次爱。容许我百分之百的任性。村上春树讲：“ 把人生当做饼干罐就可以了。饼干罐不是装了各种各样的饼干，喜欢的和不喜欢的都在里面吗？如果先一个劲儿的挑你喜欢吃的，那么剩下的就全是不大喜欢的。每次遇到麻烦我就这样想，先把这个应付过去，往下就好办了。 计划实在是一样满足虚荣心的好玩意儿，你欢腾于欣赏顺利达成的自己并且毫不自禁的开始畅想那种虚无的满足感，这样的力量实在是软弱的，毫无生机的，既然没实质的胜利，那你就是个十足的阿q，精神胜利的恶魔在肆虐你，让你分不清方向，你依旧在狂笑，不带一点点对自己的歉意。 所谓的有力不取决于肉体所能承载的负重和劳累，每天听到上千种声音，每天走6000步，每天看到1000种颜色，每天接触不一样滋味的空气，我的器官接受了周围的一切顺其自然的，你想看到的或者想避开的，都会一溜烟的在你神经的深处划过轨迹，记住的便是你的选择和潜意识里面想要的得到的，所以珍惜你记住的同时缅怀你没记住的。 其二：物电影的发展不是绝对的，而是在一盘散沙里面掏出金来，时不时又混点泥沙进去，往复循环，进步的地方我们看的很明显，但是退步的趋势也是在很大一部分的元素里面可以显而易见的。看电影是一个艺术门道，主观的感觉是用来解决温饱的，那么深刻地剖析就是让你开始挑剔一部好电影的“色香味”，才可以真的把整个电影的精髓挖掘出来。 谁都有你想不到的难题，困扰于心的在别人看来都是些鸡毛蒜皮的小事。在这里可以学到一个受用的经验：当缠绕着你的那些忧思无法被自己打败以及翻越的时候，把视线放远些，不要局限于一个角度，你看到的就不会是墙角的草，而是大草原的葱茏，那是何其美妙的世界。当然，这需要有一定的阅历（阅读，经历很重要），去扩展你的经历簿。 如果把马蜂窝移到自己的居室，帮里头的蜜蜂精心准备好繁茂生长的花朵，恰到好处的阳光，各种生命的迹象，以至于让他们察觉不到自己身处异境，即使有人类在他们周围逡巡，也不会折回或做出抵抗。让这样的环境持续尽量久一些，蜜蜂也会在自己毫无察觉的情况下发生细微的变化，那是依赖于环境的惯性适应。强行改变人类的行为亦是如此，扭曲变形，失去重心没有防线，离开安全区域的不安与彷徨，偏离轨道与甚者再也无可复原。 对于哲学问题除非你长期为之苦思冥想，否则你根本说不清到底是些什么问题。要对哲学史有很好的说明，你必须竭尽所能从其”内部“看清各个哲学问题，设身处地地进入你所讨论的哲学家们的内心世界。你必须弄清那些问题对为之乐此不疲的哲学家意味着什么，弄清哲学家们始终关注的焦点。不如此”艰辛“是不可能写出真正的思想史的。另外，除非你自己专注于相关领域并进行深入研究，否则你无法写出他人在该领域艰难跋涉的历史。意识形态的历史，严格来说，只有那些热衷于意识形态问题并懂得如何思考意识形态问题的人才能写。 哲学解决的是观念之间、词语之间或表述方法之间的冲突产生出来的各种疑难。不同于经验问题。关于生活的目的、善和恶、自由和必然、客观性和相对性等的一系列问题，既不能考查阅最高级的辞典来解决，也不能用经验方法或数学推理方法来解决。设身处地地进入思想家们的内心和世界观是必要的，移情也是不可或缺的，尽管这样做面临证据不足和不确定性，乃至困难重重。诸如在研究马克思时，应该力图使自己像马克思本人在柏林、在巴黎、在布鲁塞尔和在伦敦那样，思考它的各种概念、范畴及其德语词汇。他们的思想是怎样产生的？在什么特定的时间、地点、社会条件下产生的？他们的思想可能很多人都有同感，但毕竟那是属于他们自己的。你必须不断反问自己，是什么东西让他们烦恼？什么东西使他们对这些问题苦苦思索？他们的理论或著作是怎么样在他们头脑中成熟的？人们不能完全抽象地超历史地谈论各种思想；但是，人们也不能孤立地仅在具体的历史环境中来描述各种思想，好像这些思想在他们的框架之外没有任何意义似的。这是一种复杂的、含糊不清的、需要借助心理学视野以及丰富想象力的研究工作，他不可能获得什么必然性的结论、在多数情况下，只能达到高度的持之有故和言之有理，达到理智能力的首尾一贯和清楚明白，还有独创性和有效性。]]></content>
      <categories>
        <category>笑忘录</category>
      </categories>
      <tags>
        <tag>电影</tag>
        <tag>哲学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（3）：逻辑斯谛回归]]></title>
    <url>%2F2017%2F01%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%883%EF%BC%89%EF%BC%9A%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[一、逻辑斯谛分布介绍逻辑斯谛回归模型之前，首先看一个并不常见的概率分布，即逻辑斯谛分布。设$X$是连续随机变量，$X$服从逻辑斯谛分布是指$X$具有下列的分布函数和密度函数： F\left(x\right)=P\left(X\le x\right)=\frac{1}{1+e^{-\left(x-\mu\right)/\gamma}} f\left(x\right)=F^,\left(x\right)=\frac{e^{-\left(x-\mu\right)/\gamma}}{\gamma\left(1+e^{-\left(x-\mu\right)/\gamma}\right)^2}式中，$\mu$为位置参数，$\gamma&gt;0 $为形状参数。逻辑斯谛的分布的密度函数$f(x)$和分布函数$F(x)$的图形如下图所示。其中分布函数属于逻辑斯谛函数，其图形为一条$S$形曲线。该曲线以点$(\mu,\frac{1}{2})$为中心对称，即满足 F\left(-x+\mu\right)-\frac{1}{2}=-F\left(x+\mu\right)+\frac{1}{2}曲线在中心附近增长较快，在两端增长速度较慢。形状参数$\gamma$的值越小，曲线在中心附近增长得越快。 二、逻辑斯谛回归模型线性回归的应用场合大多是回归分析，一般不用在分类问题上。原因可以概括为以下两个： 1）回归模型是连续型模型，即预测出的值都是连续值（实数值），非离散值； 2）预测结果受样本噪声的影响比较大。 2.1 LR模型表达式LR模型表达式为参数化的逻辑斯谛函数（默认参数$\mu=0,\gamma=1$）,即 h_{\theta}\left(x\right)=\frac{1}{1+e^{-\theta^Tx}}其中$h_\theta{(x)}$作为事件结果$y=1$的概率取值。这里,$x\in R^{n+1},y\in \{1,0\},\theta\in R^{n+1}$是权值向量。其中权值向量$w$中包含偏置项，即$w=(w_0,w_1,···,w_n)，x=(1,x_1,x_2,···,x_n)$ 2.2 理解LR模型2.2.1 对数几率一个事件发生的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是$p$，那么该事件的几率为$\frac{p}{1-p}$，该事件的对数几率（log odds）或logit函数是： logit\left(p\right)=\log\frac{p}{1-p}对LR而言，根据模型表达式可以得到： \log\frac{h_{\theta}\left(x\right)}{1-h_{\theta}\left(x\right)}=\theta^Tx即在LR模型中，输出$y=1$的对数几率是输入$x$的线性函数。或者说输出$y=1$的对数几率是由输入$x$的线性函数表示的模型，即LR模型 2.2.2 函数映射除了从对数几率的角度理解LR外，从函数映射也可以理解LR模型。 考虑对输入实例$x$进行分类的线性表达式$\theta^T$，其值域为实数域。通过LR模型表达式可以将线性函数$\theta^Tx$的结果映射到(0,1)区间，取值表示为结果为1的概率（在二分类场景中）。 线性函数的值越接近于正无穷大，概率值就越接近1；反之，其值越接近于负无穷，概率值就越接近0。这样的模型就是LR模型。 LR本质上还是线性回归，知识特征到结果的映射过程中加了一层函数映射（即sigmoid函数），即先把特征线性求和，然后使用sigmoid函数将线性和约束至（0，1）之间，结果值用于二分或回归预测。 2.2.3 概率解释LR模型多用于解决二分类问题，如广告是否被点击（是/否）、商品是否被购买（是/否）等互联网领域中常见的应用场景。但是实际场景中，我们又不把它处理成“绝对的”分类问题，而是用其预测值作为事件发生的概率。 这里从事件、变量以及结果的角度给予解释。 我们所能拿到的训练数据统称为观测样本。问题：样本是如何生成的？ 一个样本可以理解为发生的一次事件，样本生成的过程即事件发生的过程。对于0/1分类问题来讲，产生的结果有两种可能，符合伯努利试验的概率假设。因此，我们可以说样本的生成过程即为伯努利试验过程，产生的结果（0/1）服从伯努利分布。这里我们假设结果为1的概率为$h_\theta{(x)}$，结果为0的概率为$1-h_\theta{(x)}$。 那么对于第$i$个样本，概率公式表示如下： P(y^{(i)}=1|x^{(i)};\theta )=h_\theta{(x^{(i)})}$$$$P(y^{(i)}=0 |x^{(i)};\theta )=1- h_\theta{(x^{(i)})}将上面两个公式合并在一起，可得到第$i$个样本正确预测的概率： P(y^{(i)}|x^{(i)};\theta)=(h_\theta(x^{(i)})^{y(i)})·（1-h_\theta(x^{(i)}))^{1-y(i)}上式是对一个样本进行建模的数据表达。对于所有的样本，假设每条样本生成过程独立，在整个样本空间中（N个样本）的概率分布（即似然函数）为： P\left(Y|X;\theta\right)=\prod_{i=1}^N{\left(h_{\theta}\left(x^{\left(i\right)}\right)^{y^{\left(i\right)}}\left(1-h_{\theta}\left(x^{\left(i\right)}\right)^{1-y^{\left(i\right)}}\right)\right)}通过极大似然估计（Maximum Likelihood Evaluation，简称MLE）方法求概率参数。具体地，第三节给出了通过随机梯度下降法（SGD）求参数。 三、模型参数估计3.1 Sigmoid函数 上图所示即为sigmoid函数，它的输入范围为$-\infty\rightarrow +\infty$，而值域刚好为$(0,1)$，正好满足概率分布为$(0,1)$的要求。用概率去描述分类器，自然要比阈值要来的方便。而且它是一个单调上升的函数，具有良好的连续性，不存在不连续点。 此外非常重要的，sigmoid函数求导后为：以下的推导中会用到，带来了很大的便利。 3.2 参数估计推导上一节的公式不仅可以理解为在已观测的样本空间中的概率分布表达式。如果从统计学的角度可以理解为参数$\theta$似然性的函数表达式（即似然函数表达式）。就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大。参数在整个样本空间的似然函数可表示为： L\left(\theta\right)=P\left(\overrightarrow{Y}|X;\theta\right) =\prod_{i=1}^N{P\left(y^{\left(i\right)}\parallel x^{\left(i\right)};\theta\right)} =\prod_{i=1}^N{\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)^{y\left(i\right)}\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)^{1-y^{\left(i\right)}}}为了方便参数求解，对这个公式取对数，可得对数似然函数： l\left(\theta\right)=\sum_{i=1}^N{\log l\left(\theta\right)} =\sum_{i=1}^N{y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)}最大化对数似然函数其实就是最小化交叉熵误差（Cross Entropy Error）。先不考虑累加和，我们针对每一个参数$w_j$求偏导： \frac{\partial}{\partial\theta_j}l\left(\theta\right)=\left(y\frac{1}{h_{\theta}\left(x\right)}-\left(1-y\right)\frac{1}{1-h_{\theta}\left(x\right)}\right)\frac{\partial}{\partial\theta_j}h_{\theta}\left(x\right) =\left(\frac{y\left(1-h_{\theta}\left(x\right)\right)-\left(1-y\right)h_{\theta}\left(x\right)}{h_{\theta}\left(x\right)\left(1-h_{\theta}\left(x\right)\right)}\right)h_{\theta}\left(x\right)\left(1-h_{\theta}\left(x\right)\right)\frac{\partial}{\partial\theta_j}\theta^Tx =\left(y-h_{\theta}\left(x\right)\right)x_j最后，通过扫描样本，迭代下述公式可求得参数： \theta_j:=\theta_j+a\left(y^{\left(i\right)}-h_{\theta}\left(x^{\left(i\right)}\right)\right)x_{j}^{\left(i\right)}其中$a$表示学习率，又称学习步长。此外还有Batch GD，共轭梯度，拟牛顿法（LBFGS），ADMM分布学习算法等都可以用来求解参数。另作优化算法一章进行补充。 以上的推导是LR模型的核心部分，在机器学习相关面试中，LR模型公式推导可能是考察频次最高的一个点。要将其熟练推导。 3.3 分类边界知道如何求解参数后，我们看一下模型得到的最后结果是什么样的。假设我们的决策函数为： y^∗=1, \ \ if \ \ P(y=1|x)>0.5选择0.5作为阈值是一个一般的做法，实际应用时特定的情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。很容易看出，当$\theta ^Tx&gt;0$时，$y=1$，否则$y=0$。$\theta ^Tx=0$是模型隐含的分类平面（在高维空间中，我们说是超平面）。所以说逻辑回归本质上是一个线性模型，但是这不意味着只有线性可分的数据能通过LR求解，实际上，我们可以通过特征变换的方式把低维空间转换到高维空间（kernel trick），而在低维空间不可分的数据，到高维空间中线性可分的几率会高一些。下面两个图的对比说明了线性分类曲线和非线性曲线（通过特征映射）。左图是一个线性可分的数据集，右图在原始空间中线性不可分，但是在特征转换$[x_1,x_2]=&gt;[x_1,x_2,x_1^2,x_2^2,x_1x_2]$后的空间是线性可分的，对应的原始空间中分类边界为一条椭圆曲线。 不过，通常使用的kernel都是隐式的，也就是找不到显式地把数据从低维映射到高维的函数，而只能计算高维空间中的数据点的内积。在这种情况下，logistic regression模型就不能再表示成$w^Tx+b$的形式（原始形式primal form），而只能表示成$\sum_ia_i+b$的形式（对偶形式dual form）。忽略b，则原始形式的模型蚕食只有$w$，只需要一个数据点那么多的存储量；而对偶形式的模型不仅需要存储各个$a_i$，还要存储训练数据$x_i$本身，这个存储量就大了。 SVM也具有原始形式和对偶形式，相比之下，SVM的对偶形式是稀疏的，即只有支持向量的$a_i$才非零，才需要存储相应的$x_i$，所以，在非线性可分的情况下，SVM用的更多。 四、延伸4. 1 生成模型与判别模型逻辑回归是一种判别模型，表现为直接对条件概率$P(y|x)$建模，而不关心背后的数据分布$P(x,y)$。而高斯贝叶斯（Gaussian Naive Bayes）是一种生成模型，先对数据的联合分布建模，再通过贝叶斯公式来计算属于各个类别的后验概率，即： p(y|x)=\frac{P(x|y)P(y)}{\sum P(x|y)P(y)}通常假设$P(x|y)$是高斯分布，$P(y)$是多项式分布，相应的参数可以通过最大似然估计得到。如果我们考虑二分类问题，通过简单的变化可以得到： log\frac{P(y=1|x)}{P(y=0|x)}=log\frac{P(x|y=1)}{P(x|y=0)}+log\frac{P(y=1)}{P(y=0)}=-\frac{(x-\mu_1)^2}{2\sigma_1^2}+\frac{(x-\mu_0)^2}{2\sigma_0^2}+\theta_0如果$\sigma_1=\sigma_0$，二次项会抵消，我们得到一个简单的线性关系： log\frac{P(y=1|x)}{P(y=0|x)}=\theta^Tx上式进一步可以得到： P(y=1|x)=\frac{e^{\theta^Tx}}{1+e^{e^Tx}}=\frac{1}{1+e^{-\theta^Tx}}可以看到，这个概率和逻辑回归中的形式是一样的，这种情况下高斯贝叶斯和LR会学习到同一个模型。实际上，在更一般的假设（P(x|y)的分布属于指数分布族）下，我们都可以得到类似的结论。 4.2 多分类如果$y$不是在$[0,1]$中取值，而是在$K$个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当$K$个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果$K$个类别是互斥的，即 $y=i$ 的时候意味着 $y$ 不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）。模型通过 softmax 函数来对概率建模，具体形式如下： P(y=i|x,\theta)=\frac{^{e^{\theta^T_ix}}}{\sum_j^K e^{\theta_j^Tx}}$$而决策函数为：$$y^*=argmax_iP(y=i|x,\theta )对于的损失函数为 J(\theta)=-\frac{1}{N}\sum_i^N\sum_j^KP(y_i=j)log\frac{e^{\theta_i^Tx}}{\sum e^{\theta_k^Tx}}类似的，我们也可以通过梯度下降或其他高阶方法来求解该问题，这里不再赘述。 4.3 应用这里以预测用户对品类的购买偏好为例，介绍一下美团是如何用逻辑回归解决工作中问题的。该问题可以转换为预测用户在未来某个时间段是否会购买某个品类，如果把会购买标记为1，不会购买标记为0，就转换为一个二分类问题。我们用到的特征包括用户在美团的浏览，购买等历史信息，见下表：其中提取的特征的时间跨度为30天，标签为2天。生成的训练数据大约在7000万量级（美团一个月有过行为的用户），我们人工把相似的小品类聚合起来，最后有18个较为典型的品类集合。如果用户在给定的时间内购买某一品类集合，就作为正例。有了训练数据后，使用Spark版的LR算法对每个品类训练一个二分类模型，迭代次数设为100次的话模型训练需要40分钟左右，平均每个模型2分钟，测试集上的AUC也大多在0.8以上。训练好的模型会保存下来，用于预测在各个品类上的购买概率。预测的结果则会用于推荐等场景。 由于不同品类之间正负例分布不同，有些品类正负例分布很不均衡，我们还尝试了不同的采样方法，最终目标是提高下单率等线上指标。经过一些参数调优，品类偏好特征为推荐和排序带来了超过1%的下单率提升。 此外，由于LR模型的简单高效，易于实现，可以为后续模型优化提供一个不错的baseline，我们在排序等服务中也使用了LR模型。 逻辑回归的数学模型和求解都相对比较简洁，实现相对简单。通过对特征做离散化和其他映射，逻辑回归也可以处理非线性问题，是一个非常强大的分类器。因此在实际应用中，当我们能够拿到许多低层次的特征时，可以考虑使用逻辑回归来解决我们的问题。 4.4 LR与SVM两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。但是逻辑回归相对来说模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些。但是SVM的理论基础更加牢固，有一套结构化风险最小化的理论基础，虽然一般使用的人不太会去关注。还有很重要的一点，SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。 两者对异常的敏感度也不一样。同样的线性分类情况下，如果异常点较多的话，无法剔除，首先LR，LR中每个样本都是有贡献的，最大似然后会自动压制异常的贡献，SVM+软间隔对异常还是比较敏感，因为其训练只需要支持向量，有效样本本来就不高，一旦被干扰，预测结果难以预料。 参考资料对线性回归，logistic回归和一般回归的认识 Logistic Regression 模型简介]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑斯谛回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笑忘录（5）：离别记]]></title>
    <url>%2F2017%2F01%2F10%2F%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%885%EF%BC%89%EF%BC%9A%E7%A6%BB%E5%88%AB%2F</url>
    <content type="text"><![CDATA[离别的仪式以其该有的苍白的方式展开了，一个拥抱，一声保重，一段你们来时走过的路，它竭力地想被赋予更多的力量与情感，那么看吧，看看在我们心灵藏匿最深的地方，那里骚动起了真诚而又岌岌可危的猛兽，在故意掩藏的角落里缠绵、撕咬、交姌。你可以尽情嗅到生命的律动，几近沸腾的血液在体内加快步伐，血泵察觉到了仪式的情绪，它已经完全失去控制，它发了疯似的让灵动的双眼淌着热泪，身体仿佛接受了呼啸的寒风开始战栗起来，此刻即将分别，每一个鲜活的生命就要前往这个国度的各个角落，那里的星辰大海，下一次见面的时候请一定告诉我。 生活总需要特定的仪式来配合鼓舞，四年前我们仪式着来，四年后仪式着走，这无关乎虚荣，也绝不是空穴矫情，站在这个时间的关口，回望过去的日子，它不温不火，但也一定不平凡。当年踏上这片土地å，驾着云轻吟功与名，如今洒脱地背着旗帜四处宣告这四年的狂热与藏匿，我们汲汲求索，生活让我们的心灵焦灼躁动，却反而褪出一种光泽来，它们每一样都隐秘而伟大，孕育着一股神奇的力量。 我们曾如孩童般痴笑着欢送旧时光，如酒神般渴求下一秒思想的闪耀，过去哪怕是闯入一块新天地的边境，也要花光我们所有的力气，迟来的矍铄如同一杯烈酒，蓄满无畏与重生的快感，愿我们承受得住这些深渊，一直自命不凡地在里面挣扎，生命不止，它最好一刻也不要停止，也愿我们每一次触碰这个星球时都可以感受到有一种声音在呼喊，那声音在遥远的未来回响，也发轫于不朽的丰碑与偶像存留的尘土。 我们即将面临一次非同寻常、却令人困倦的旅程，我们今天就要乘上一列所谓的失控列车。没有人可以诚恳地向我们展示前方有什么，而那些落在后面的人也会因接续我们这一毫无秩序的开端而爱莫能助。但我们必须要承认，这段旅途有去无回，我们可以宽慰自身的也只能是这样的思想：无论遇到什么不开心的事情，无论是哪一个车站都会一闪而过，那只是影片的一个人微妙的段落，列车绝不会在一个车站停留地太久。今天我们一起驻守的这块地方，即将成为过去的一站，我们只能挥手道别，在它还保持着正常的模样之前，在它还没有成为一张照片之前，就让我们怀着我们所有的温情再看它一眼，那也是在打量我们的过去。 先生们，我只是吝啬地留了一个苦涩的脸庞在你们记忆里，而你们在我的视野里留下了一个个鲜活的梦。篮球梦、炸金花梦、LOL梦，亦或是妹子梦、富帅梦、伟光正梦，它们都即将循着自己的路子熠熠生辉，都端正着自己的仪式带着未完成的梦离开吧，在812留下的痕迹就让我们在年迈的时候再来品味，倘若你们还记得床底的乌烟瘴气与泛黄纯白色纸巾，就算是重走了一回青春。]]></content>
      <categories>
        <category>笑忘录</category>
      </categories>
      <tags>
        <tag>笑忘录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（2）：线性回归]]></title>
    <url>%2F2017%2F01%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%882%EF%BC%89%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[一、线性回归模型线性回归假设特征和结果满足线性关系。其实线性关系的表达能力非常强大，每个特征对结果的影响强弱可以由前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后再参与线性计算。这样就可以表达特征与结果之间的非线性关系。 我们可以有这样的模型表达： y=\theta_0+\theta_1x_1+\theta_2x_2+···+\theta_nx_n其中，$x_1,x_2,···,x_n$表示自变量（特征分量），$y$表示因变量，$\theta_i$表示对应自变量（特征）的权重，$\theta_0$是偏倚项（又称为截距）。 对于参数$\theta$，在物理上可以解释为：在自变量（特征）之间相互独立的前提下，$\theta_i$反映自变量$x_i$对因变量$y$的影响程度，$\theta_i$越大，说明$x_i$对结果$y$的影响越大。因此，我们可以通过每个自变量（特征）前面的参数，可以很直观的看出那些特征分量对结果的影响比较大。 如果令$x_0=1,y=h_\theta{(x)}$，可以将上述模型写成向量形式，即： h_\theta\left(x\right)=\sum_{i=0}^n{\theta_ix_i}=\theta^Tx其中$\theta=(\theta_0,\theta_1,···,\theta_n)，x=(1,x_1,x_2,···,x_n)$均为向量，$\theta^T$为$\theta$的转置。 在上述公式中，假设特征空间与输入空间$x$相同。准确地讲，模型表达式要建立的是特征空间与结果之间的关系。在一些应用场合中，需要将输入空间映射到特征空间中，然后建模，定义映射函数为$\varPhi\left(x\right)$，因此我们可以把公式写成更通用的表达公式： h_\theta\left(x\right)=\theta^T\varPhi\left(x\right)特征映射相关技术，包括特征哈希、特征学习、$Kernel$等。 二、目标函数2.1 目标函数上面的公式的参数向量$\theta$是$n+1$维的，每个参数的取值是实数集合，也就是说参数向量$\theta$在$n+1$维实数空间中取值结果有无穷种可能。 那么，如何利用一个规则或机制帮助我们评估求得的参数$\theta$，并且使得线性模型效果最佳呢？直观地认为，如果求得参数$\theta$线性求和后，得到的结果$h_\theta{(x)}$与真实值$y$之差越小越好。 这时我们需要映入一个函数来衡量$h_\theta{(x)}$表示真实值$y$好坏的程度，该函数称为损失函数（loss function，也称为错误函数）。数学表示如下： J\left(\theta\right)=\frac{1}{2}\sum_{i=1}^n{\left(\left(h_\theta\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)\right)^2} \min_\theta J\left(\theta\right)这个损失函数用的是$x^{(i)}$的预测值$h_\theta{(x^{(i)})}$与真实值$y^{(i)}$之差的平方和。如果不考虑诸如过拟合等其他问题，这就是我们需要优化的目标函数。 2.2 目标函数的概率解释一般地，机器学习中不同的模型会有相应的目标函数。而回归模型（尤其是线性回归类）的目标函数通常用平方损失函数来作为优化的目标函数（即真实值与预测值之差的平方和）。为什么要选用误差平方和作为目标函数呢？答案可以从概率论中的中心极限定理、高斯分布等知识中找到。 2.2.1 中心极限定理目标函数的概率解释需要用到中心极限定理。中心极限定理本身就是研究独立随机变量和的极限分布为正态分布的问题。 中心极限定理的公式表示为：设$n$个随机变量$X_1,X_2,···,X_n$相互独立，均具有相同的数学期望与方差，即$E(X_i)=\mu ;D(X_i)=\sigma^2$，令$Y_n$为随机变量之和，有 Y_n=X_1+X_2+···+X_n Z_n=\frac{Y_n-E\left(Y_n\right)}{\sqrt{D\left(Y_n\right)}}=\frac{Y_n-n\mu}{\sqrt{n}\sigma}\rightarrow N\left(0,1\right)称随机变量$Z_n$为$n$个随机变量$X_1,X_2,···,X_n$的规范和。 它的定义为：设从均值为$\mu$、方差为$\sigma^2$（有限）的任意一个总体中抽取样本量为$n$的样本，当$n$充分大时，样本均值的抽样分布$\frac{Y_n}{n}$近似服从于均值为$\mu$、方差为$\sigma^2$的正态分布。 2.2.2 高斯分布假设给定一个输入样例$x^{(i)}$根据公式得到预测值$\theta^Tx^{(i)}$与真实值$y^{(i)}$之间存在误差，即为$\varepsilon^{\left(i\right)}$。那么，它们之间的关系表示如下： y^{\left(i\right)}=\theta^Tx^{\left(i\right)}+\varepsilon^{\left(i\right)}而这里假设误差$\varepsilon^{\left(i\right)}$服从标准高斯分布是合理的。 解释如下： 回归模型的最终目标是通过函数表达式建立自变量$x$与结果$y$之间的关系，希望通过$x$能较为准确地表示结果$y$。而在实际的应用场合中，很难甚至不可能把导致$y$的所有变量（特征）都找出来，并放到回归模型中。那么模型中存在的$x$通常认为是影响结果$y$最主要的变量集合（又称为因子，在ML中称为特征集）。根据中心极限定理，把那些对结果影响比较小的变量（假设独立同分布）之和认为服从正态分布是合理的。 可以用一个示例来说明误差服从高斯分布是合理的： $Andrew Ng$的课程中第一节线性回归的例子中，根据训练数据建立房屋的面积$x$与房屋的售价$y$之间的函数表达。它的数据集把房屋面积作为最为主要的变量。除此之外我们还知道房屋所在的地段（地铁、学区、城区、郊区），周边交通状况，当地房价、楼层、采光、绿化面积等等诸多因素会影响房价。 实际上，因数据收集问题可能拿不到所有影响房屋售价的变量，可以假设多个因素变量相互独立，根据中心极限定理，认为变量之和服从高斯分布。即： \epsilon^{\left(i\right)}=y^{\left(i\right)}-\theta^Tx^{\left(i\right)}那么$x$和$y$的条件概率可表示为： p\left(y^{\left(i\right)}|x^{\left(i\right)};\theta\right)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}{2\sigma^2}\right)2.2.3 极大似然估计与损失函数极小化等价根据上述公式估计得到一条样本的结果概率，模型的最终目标是希望在全部样本上预测最准，也就是概率积最大，这个概率积就是似然函数。优化的目标函数即为似然函数，表示如下： \max_\theta L\left(\theta\right)=\prod_{i=1}^m{\frac{1}{\sqrt{2\pi}\sigma}}\exp\left(-\frac{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}{2\sigma^2}\right)对$L(x)$取对数，可得对数似然函数： \max_\theta l\left(\theta\right)=-m\log\sqrt{2\pi}\sigma -\frac{1}{2\sigma^2}\sum_{i=1}^m{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}由于$n,\sigma$都为常数，因此上式等价于 \min_\theta\frac{1}{2}\sum_{i=1}^m{\left(y^{\left(i\right)}-\theta^Tx^{\left(i\right)}\right)^2}我们可以发现，经过最大似然估计推导出来的待优化的目标函数与平方损失函数是等价的。因此可以得出结论： 线性回归误差平方损失极小化与极大似然估计等价。其实在概率模型中，目标函数的原函数（或对偶函数）极小化（或极大化）与极大似然估计等价，这是一个带有普遍性的结论。比如在最大熵模型中，有对偶函数极大化与极大似然估计等价的结论。 那上面为什么是条件概率$p(y|x;\theta)$呢？因为我们希望预测值与真实值更接近，这就意味着希望求出来的参数$\theta$，在给定输入$x$的情况下，得到的预测值等于真实值得可能性越大越好。而$\theta$，$x$均为前提条件，因此用条件概率$p(y|x;\theta)$表示。即$p(y|x;\theta)$越大，越能说明估计的越准确。当然也不能一味地只有该条件函数，还要考虑拟合过度以及模型的泛化能力问题。 三、参数估计如何调整参数$\theta$使得$J(\theta)$取得最小值？方法有很多，这里介绍几种比较经典的方法，即最小二乘法、梯度下降法以及牛顿法。 3.1 最小二乘法3.1.1 目标函数的矩阵形式将$m$个$n$维样本组成矩阵$X$： \left(\begin{matrix} 1& x_{1}^{\left(1\right)}& x_{1}^{\left(2\right)}& ···& x_{1}^{\left(n\right)}\\ 1& x_{2}^{\left(1\right)}& x_{2}^{\left(2\right)}& ···& x_{2}^{\left(n\right)}\\ ···& ···& ···& & \\ 1& x_{m}^{\left(1\right)}& x_{m}^{\left(2\right)}& ···& x_{m}^{\left(n\right)}\\ \end{matrix}\right)则目标函数的矩阵形式为 J\left(\theta\right)=\frac{1}{2}\sum_{i=1}^m{\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^2}=\frac{1}{2}\left(X\theta -y\right)^T\left(X\theta -y\right)3.1.2 最小二乘法求解对$\theta$求导，梯度（矩阵求导）： \nabla_\theta J\left(\theta\right)=\nabla_\theta\left(\frac{1}{2}\left(X\theta-y\right)^T\left(X\theta-y\right)\right) =\nabla_\theta\left(\frac{1}{2}\left(\theta^TX^TX\theta-\theta^TX^Ty-y^Ty\right)\right) =\frac{1}{2}\left(2X^TX\theta-X^Ty-\left(y^TX\right)^T\right) =X^TX\theta-X^Ty令其为零，求得驻点： \theta=\left(X^TX\right)^{-1}X^Ty3.2 梯度下降法梯度下降法是按下面的流程进行的： 1）首先对$\theta$赋值，这个值可以是随机的，也可是让$\theta$是一个全零的向量； 2）改变$\theta$的值，使得$J(\theta)$按梯度下降的方向进行减少。 为了更清楚，给出下面的图： 这是一个表示参数$\theta$与目标函数$J(\theta)$的关系图，红色的部分是表示$J(\theta)$有比较高的取值，我们需要的是，能够让$J(\theta)$的值尽量的低。也就是深蓝色的部分。$\theta_0$和$\theta_1$表示$\theta$向量的两个维度。 在上面提到梯度下降法的第一步是给$\theta$一个初值，假设随机给的初值是在图上的十字点。然后我们将$\theta$按照梯度下降的方向进行调整，就会使得$J(\theta)$往更低的方向进行变化，如图所示，算法的结束将是在$\theta$下降到无法继续下降为止。 当然，可能梯度下降的最终点并非是全局最小点，可能是一个局部最小点，比如下面这张图中描述的就是一个局部最小点，这是我们重新选择了一个初始点得到的，看来我们这个算法会在很大程度上被初始点的选择影响而陷入局部最小点。 ![屏幕快照 2017-04-04 下午6.03.23](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-04%20%E4%B8%8B%E5%8D%886.03.23.png) 下面对于目标函数$J(\theta)$求偏导数： \frac{\partial}{\partial\theta_j}J\left(\theta\right)=\frac{\partial}{\partial\theta_j}\frac{1}{2}\left(h_{\theta}\left(x\right)-y\right)^2 =2·\frac{1}{2}\left(h_{\theta}\left(x\right)-y\right)\frac{\partial}{\partial\theta_j}\left(h_{\theta}\left(x\right)-y\right) =\left(h_{\theta}\left(x\right)-y\right)x_j下面是更新的过程，也就是$\theta_i$会向着梯度最小的方向进行减少。$\theta$表示更新之前的值，$a$表示步长，也就是每次按照梯度减少的方向变化多少，由于求得是极小值，因此梯度方向是偏导数的反方向，结果为 \theta :=\theta_j+a\left(h_{\theta}\left(x\right)-y\right)x_j一个很重要的地方值得注意的是，梯度是有方向的，对于一个向量$\theta$，每一维分量$\theta_i$都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管他是全局的还是局部的。 在对目标函数$J(\theta)$求偏导时，可以用更简单的数学语言（倒三角表示梯度）进行描述： \nabla_{\theta}J=\left[\begin{array}{c} \frac{\partial}{\partial\theta_0}J\\ ···\\ ···\\ \frac{\partial}{\partial\theta_n}J\\ \end{array}\right] \theta :=\theta +a\nabla_{\theta}J将梯度下降法应用到线性回归有三种方式：批处理梯度下降法、随机梯度下降法。 3.2.1 批量梯度下降法（BGD）![屏幕快照 2017-04-04 下午6.36.09](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-04%20%E4%B8%8B%E5%8D%886.36.09.png)可以看出，参数$\theta$的值每更新一次都要遍历样本集中的所有的样本，得到新的$\theta_j$，看是否满足阈值要求，若满足，则迭代结束，根据此值就可以得到；否则继续迭代。注意到，虽然梯度下降法易受到极小值的影响，但是一般的线性规划问题只有一个极小值，所以梯度下降法一般可以收敛到全局的最小值。例如，$J$是二次凸函数，则梯度下降法的示意图为：![屏幕快照 2017-04-04 下午6.40.55](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-04%20%E4%B8%8B%E5%8D%886.40.55.png)图中，一圈上表示目标函数的函数值类似于地理上的等高线，从外圈开始逐渐迭代，最终收敛全局最小值。 3.2.2 随机梯度下降算法（SGD）在这个算法中，我们每次更新只用到一个训练样本，若根据当前严格不能进行迭代得到一个，此时会得到一个，有新样本进来之后，在此基础上继续迭代，又得到一组新的和，以此类推。 批量梯度下降法，每更新一次，需要用到样本集中的所有样本；随机梯度下降法，每更新一次，只用到训练集中的一个训练样本，所以一般来说，随机梯度下降法能更快地使目标函数达到最小值（新样本的加入，随机梯度下降法有可能会使目标函数突然变大，迭代过程中在变小。所以是在全局最小值附近徘徊，但对于实际应用俩说，误差完全能满足要求）。另外，对于批量梯度下降法，如果样本集增加了一些训练样本，就要重新开始迭代。由于以上原因，当训练样本集较大时，一般使用随机梯度下降法。 四、参考资料对线性回归，logistic回归和一般回归的认识]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（1）：K近邻]]></title>
    <url>%2F2017%2F01%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%881%EF%BC%89%EF%BC%9AK%E8%BF%91%E9%82%BB%2F</url>
    <content type="text"><![CDATA[一、K近邻算法K近邻算法简单、直观。首先给出一张图，根据这张图来理解最近邻分类器。 根据上图所示，有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形或者红色小三角形），下面，我们就要解决这个问题：给这个绿色的圆分类。 我们常说，物以类聚，人以群分，判别一个人是一个什么样的人，常常可以从他身边的朋友入手，所谓观其友，而识其人。我们不是要判别上图中那个绿色的圆是属于那一类数据么，好说，从他的另据下手。但一次性看多少个邻居呢？从上图中，你还可以看到： 如果K=3,绿色圆点最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。 如果K=5,绿色圆点的最近5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色这个待分类点属于蓝色的正方形一类。 于此，我们看到，KNN算法为给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，这K个实例的多数属于某个类，就把该输入实例分为这个类。 K近邻法算法步骤如下： 输入：训练数据集$T={(x_1,y_1),(x_2,y_2),···，（x_N,y_N）}$，其中，$x_i$是实例的特征向量，$y_i$是实例的类别；新实例的特征向量$x$ 输出：新实例$x$所属的类别$y$ 1)根据给定的距离度量，在训练集$T$中找出与$x$最邻近的$k$个点，涵盖这k个点的$x$领域记作$N_k(x)$; 2)在$N_k(x)$中根据分类决策规则（如多数表决）决定$x$的类别$y$: y=arg\underset{c_j}{\max}\sum_{x_i\in N_k\left( x \right)}^{}{I\left( y_i=c_i \right) \ \ ,\ \ i=1,2,···,N;j=1,2,···,K}其中$I$为指示函数，即当$y_i=c_i$时为1，否则为0. 二、K近邻模型2.1 模型k近邻法中，当训练集、距离度量、K值以及分类决策规则确定后，对于任何一个新的输入实例，它所属的类唯一地确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。 2.2 距离度量特征空间中两个实例点的距离可以反映出两个实力点之间的相似性程度。K近邻模型的特征空间一般是N维实数向量空间，使用的距离可以是欧式距离，也可以是其他距离。 欧氏距离：最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中 d\left( x,y \right) =\sqrt{\sum_{i=1}^n{\left( x_i-y_i \right) ^2}} 曼哈顿距离：我们可以定义曼哈顿距离的正式意义为$L1$距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投射的距离总和。 通俗来讲，想想你在曼哈顿要从一个十字路口开车到另一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”,此即曼哈顿距离名称的来源，同时，曼哈顿距离也称为城市街区距离。 d\left( x,y \right) =\sum_{i=1}^n{|x_i-y_i|} 切比雪夫距离： d\left( x,y \right) =\underset{k\rightarrow \infty}{\lim}\left( \sum_{i=1}^n{|x_i-y_i|^k} \right) ^{\frac{1}{k}} 闵可夫斯基距离：它不是一种距离，而是一组距离的定义。 d\left( x,y \right) =\left( \sum_{i=1}^n{|x_i-y_i|^k} \right) ^{\frac{1}{k}} 当p=1时，即曼哈顿距离 当p=2时，即欧式距离 当$p\rightarrow \infty$，即切比雪夫距离 标准化欧氏距离：对样本集先进行标准化$\hat{x}_i=\frac{x_i-\bar{x}}{s}$经过简单的推导就可以得到来标准化欧氏距离。 d\left( x,y \right) =\sqrt{\sum_{i=1}^n{\left( \frac{x_i-y_i}{s} \right) ^2}} 夹角余弦：几何中夹角余弦可用来衡量两个向量方向的相似度，机器学习中借用这一概念来衡量向量之间的相似度。 \cos \left( \theta \right) =\frac{a·b}{|a|·|b|}2.3 K值的选择K值得选择会对K近邻法的结果产生重大影响。 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值得减小就意味着整体模型变得复杂，容易发生过拟合（容易受到训练数据的噪声而产生的过拟合的影响）。 如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减小学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远的训练实例也会对预测器作用，使预测发生错误，且K值得增大就意味着整体的模型变得简单。 如果K=N。那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的（最近邻列表中可能包含远离其近邻的数据点），如下图所示。在实际应用中，K值一般取一个比较小的数值。通常采用交叉验证法来选取最优的K值（经验规则：K一般低于训练样本数的平方根）。 2.4 分类决策规则K近邻法中的分类决策规则往往是多数表决，即由输入实例的K个邻近的训练实例中的多数类决定输入实例的类。 三、K近邻的优缺点3.1 优点 简单、易于理解、易于实现、无需估计参数、无需训练。 适合对稀有事件进行分类（如大概流式率很低时，比如0.5%，构造流失预测模型）； 特别适合多酚类问题，如根据基因特征来判断其功能分类，KNN比SVM的表现要好。 3.2 缺点 懒惰算法，对测试样本分类时的计算量大，内存开销大，评分慢。 可解释性较差，无法给出决策树那样的规则。 当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 KNN是一种懒惰算法，平时不好好学习，考试（对测试样本分类）时才临阵磨枪（临时去找K个近邻），懒惰的后果，构造模型很简单，但在测试样本分类地系统开销大，因为要扫描全部训练样本并计算距离。已经有一些方法提高计算的效率，例如压缩训练样本量。 决策树和基于规则的分类器都是积极学习eager learner的例子，因为一旦训练数据可用，它们就开始学习从输入属性到类标号的映射模型。一个相反的策略是推迟对训练数据的建模，直到需要分类测试样例时再进行。采用这种策略的技术被称为消极学习法lazy learner。最近邻分类器就是这样的一种方法。 四、python代码实现4.1 K-近邻算法简单示例KNN算法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#!/usr/bin/python# coding=utf-8"""Created on Feb 22 2017KNN@author: plushunter"""# coding=utf-8#!/usr/bin/pythonfrom numpy import *import operatorclass KNNClassifier(): def __init__(self,k=3): self._k=k def _calDistance(self,inputX,trainX): dataSetSize=trainX.shape[0] # tile for array and repeat for matrix in Python diffMat=tile(inputX,(dataSetSize,1))-trainX sqDiffMat=diffMat**2 # take the sum of difference from all dimensions,axis=0是按列求和,axis=1 是按行求和 sqDistances=sqDiffMat.sum(axis=1) distances=sqDistances**0.5 # argsort returns the indices that would sort an array.argsort函数返回的是数组值从小到大的索引值 # http://www.cnblogs.com/100thMountain/p/4719503.html # find the k nearest neighbours sortedDistIndicies = distances.argsort() return sortedDistIndicies def _classify(self,sample,trainX,trainY): if isinstance(sample,ndarray) and isinstance(trainX,ndarray) and isinstance(trainY,ndarray): pass else: try: sample=array(sample) trainX=array(trainX) trainY=array(trainY) except: raise TypeError("numpy.ndarray required for trainX and ..") sortedDistIndicies=self._calDistance(sample,trainX) classCount=&#123;&#125;#create the dictionary for i in range(self._k): label=trainY[sortedDistIndicies[i]] classCount[label]=classCount.get(label,0)+1 #get(label,0) : if dictionary 'classCount' exist key 'label', return classCount[label]; else return 0 sorteditem=sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True) #operator.itemgetter(1) can be substituted by 'key = lambda x: x[1]' return sorteditem[0][0] def classify(self,inputX,trainX,trainY): if isinstance(inputX,ndarray) and isinstance(trainX,ndarray) \ and isinstance(trainY,ndarray): pass else: try: inputX = array(inputX) trainX = array(trainX) trainY = array(trainY) except: raise TypeError("numpy.ndarray required for trainX and ..") d = len(shape(inputX)) results=[] if d == 1: result = self._classify(inputX,trainX,trainY) results.append(result) else: for i in range(len(inputX)): result = self._classify(inputX[i],trainX,trainY) results.append(result) return resultsif __name__=="__main__": trainX = [[1,1.1], [1,1], [0,0], [0,0.1]] trainY = ['A','A','B','B'] clf=KNNClassifier(k=3) inputX = [[0,0.1],[0,0]] result = clf.classify(inputX,trainX,trainY) print result#output which type these belongs to/Users/HuaZhang/anaconda2/bin/python /Users/HuaZhang/Desktop/GitHub/machine-lerning/KNN/KNN.py['B', 'B']Process finished with exit code 0 4.2 KNN实现手写识别系统1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/usr/bin/python# coding=utf-8"""Created on Mar 22 2017KNN: Hand Writing@author: plushunter"""from numpy import *import operatorfrom os import listdirimport KNNdef img2vector(filename): returnVect = zeros((1, 1024)) fr = open(filename) for i in range(32): lineStr = fr.readline() # n = len(lineStr) # if not n: # continue for j in range(32): returnVect[0, 32 * i + j] = int(lineStr[j]) return returnVectdef loadDataSet(filedir): FileList = listdir(filedir) m = len(FileList) X = zeros((m,1024)) Y = [] for i in range(m): fileNameStr = FileList[i] classNumStr = int(fileNameStr.split('_')[0]) Y.append(classNumStr) X[i, :] = img2vector(filedir + "/" + fileNameStr) return X,Ydef handWritingClassTest(inputX,inputY,trainX,trainY): cls=KNN.KNNClassifier(k=3) error=0.0 result = cls.classify(inputX,trainX,trainY) for i in range(len(result)): if result[i] != inputY[i]: error+=1 precision_rate =1- error /len(inputY) print precision_rate # return errorRatedef main(): trainDir = "digits/trainingDigits" testDir = "digits/testDigits" trainX,trainY = loadDataSet(trainDir) inputX,inputY = loadDataSet(testDir) handWritingClassTest(inputX,inputY,trainX,trainY)if __name__=="__main__": main()#output precision_rate/Users/HuaZhang/anaconda2/bin/python /Users/HuaZhang/Desktop/GitHub/machine-lerning/KNN/HandWriting.py0.988372093023Process finished with exit code 0]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>K近邻</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笑忘录（4）：二零一七第一天]]></title>
    <url>%2F2017%2F01%2F01%2F%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%884%EF%BC%89%EF%BC%9A%E4%BA%8C%E9%9B%B6%E4%B8%80%E4%B8%83%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[其一二零一六就像是一出异乎寻常的戏码，在戏里，我扮演了一个再寻常不过的角色，狼狈得被汹涌的剧情推搡着向前，迷失在混乱的舞台，舞台中央的鲜花与热泪，在光环底下肆意飞扬，那个失落的孩子站在舞台边呆呆凝视，视力模糊，它们在晃动，狂妄的审视，或者只是在给生活与艺术授予赞词。 如何才可以抓住风？如何才可以让仅存的模样愈加沉醉？如何让这伟大的世界变得安静啊？ 我想要回感受力，我察觉到它们正离我而去，嗅觉不再敏锐，耳旁嘈杂不堪，视觉里容不下美，身体的每一个部位都在宣告，你不要再挣扎，你已结束兴盛的时代，那个王朝早已落幕，你只能徘徊在硝烟四起的边疆了罢。 我不甘啊，我还那么年轻。 其二二零一七呐，有好多期待，我要带着不甘重新上路了。 第一件，物。神奇的物，帮助我们和这个曼妙的世界沟通。它们总能贴合我们的感官，让那些险些失去的感觉聚焦，消沉从而变得细腻可触，欢喜也可以沉浸许久，它们与我们的身体交融。私人物品就像一种符号，为其在和自己关系密切的现实世界与社交网络中的身份提供具象化的证据。它是生活故事的标签，这些故事依附于私人物品铺展开来，串联起一个人的过去、现在与未来。 携带着与气质相符的物，让物呈现你的身体与灵魂吧，又何尝不可呢，不要说话，闭上眼睛，不要透露心声，关上襟怀。物不是其它，它就是你。 第二件，人。怎么会有那么多可爱的人呢？我历数不过来了，我默默看着他们走过、停留、消逝，然后有一天，又重新出现，谁知道是哪一天，谁又知道会出现在哪个场景里，我唯一可以做的，便是等待着随便哪一种未来。 有的人，因为音乐而靠近有的人，因为日常运作而汇合有的人，因为城市的穿梭而擦肩而过有的人，你永远遇见不到，但它们都有繁盛的心灵花园，你要学会自己去探望。 第三件，事。 我做不好很多事情，它们有的躺在我的计划簿子里，有的早已糜烂，有的做了一半就没有兴致了，有的也因为心有余而力不足，变得苍白烂尾。能做的就那么多，唯一希望的便是可以做的更好一些。 忽然觉得计划这玩意儿真神奇，它跨越时间区间去规定通往未来之路，让未来似乎变得清晰可触，在计划里，每个人都是自恋的，他们看到了更加完好的自己，从这种虚假的完备换取了瞬时的慰藉，每次向计划簿里添加精致的清单之时，便会看到镜像中的自己，绚丽地燃烧。而慢慢地，热情燃烧殆尽，才发觉，自己偏移的太远。 其三二零一七要做更多有趣的事啊，生活雅致还得有。 计算机真是一个让人着迷的东西，还有很多的未知领域需要探索，让好奇心与求知欲都尽情地来驱动自己吧。 机器学习同样让人痴迷，16年下半年的时间支离破碎的，都没有系统的梳理下，作为今年的主线吧。 你好久不读书了，清单正在消沉，逐渐变得索然无味，当你再次拾起的时候，我相信它们会踊跃呼唤你的名。 美食、音乐、旅行，这些美妙生活的配方，没有它们，可能就没有那些人了，你也失去了让感受力生根发芽的机会。 我还是很瘦，每次在健身房的镜子前看着自己的样子，一脸嫌弃。 分享真是是一件让人振奋的事，一个人的日志，可以直接通往他的心灵花园，所以很多人选择把它藏起来，那是属于自己的东西。很多人只与自己说真话，虽然看起来他和很多人对话；也有很多人不与别人说话，它们还没有准备好，它们害怕话一说出口就会变得浅薄无趣，就索性让自己保有最温热的部分了。]]></content>
      <categories>
        <category>笑忘录</category>
      </categories>
      <tags>
        <tag>笑忘录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笑忘录（3）：享书记]]></title>
    <url>%2F2016%2F12%2F24%2F%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%883%EF%BC%89%EF%BC%9A%E4%BA%AB%E4%B9%A6%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[小序现在书就在我的手上，约摸有个十来本，我想说说我接下来和他们即将擦出的火花。 对，书还没放上书架，买来只是随意堆在床边，还没有带来和井然有序俱来的那种轻微无聊的单调滋味，我还没有准备好把淘来的书展示给朋友们看，就像是没有精心剪辑的影片，那种随意拼凑出来的画面固然有吸引力，但是我会赋予更多关于藏书的意义（每本书的每一册都有其命运）。 其一从我接手一本书那一时刻起，我便承担起了这本书的美妙的命运，或者说这本书将在我的手上重生，这会是一次有价值的邂逅，和自己产生共鸣的那一刻我便认定了这孩子，他让我同样获取了孩童心态，我就像是个孩子一样抚摸每一页，感受那种苍老或幽静的感觉，我为他取一个专属于我与他之间的名字，遇到一个好的主人成为你存在在这个世界上的荣耀，你可以安详的躺在我的书架上，享受散发你香味的漫长的一生，哪一天或许我心血来潮了，我会来看看你，或者和你深入的交往沟通，你需要理解我占有你时陶醉的心情，我也会因为复活你的生命沾沾自喜，于你而言，就像是复活了一个时代，那是我最深层次的动机。 我享受这样的快感——我将你们锁入我的圈子，永久成为我无法拒绝的暧昧对象。每一个关于你的回忆和念头，便是我的所有财富的基座，支架和锁钥，我打开每一个味觉嗅觉视觉触觉神经细胞，探索你的每一个细节，你的精髓我会暗自藏在心里，由此引发的灵感仿佛可以透过你们看到遥远的过去，出版那一天的情形，那位伟岸的作者在字里行间留下的最捉摸不透的暗号，装帧师傅设计封面时所被浸透的头脑风暴，你以前的那个主人如何把你捧在手心私密的对话。 其二该是遇到怎样的多舛而繁华的命运才可以到我手上，我感谢每一位呵护你照顾你帮助你与你彻夜畅谈的每一位先人们，现在我是“老者”，你会陪伴我走向我最寂寥最繁荣的日子。 我或许不会再回头去看你，就如曾经有个庸人赞美一番阿那托尔·法朗士的书斋，最后问了一个常见的 问题：“法朗士先生，这些书您都读过了吗？”回答是足以说明问题的： “还不到十分之一。不过我想您并不是每天都用您的塞弗尔瓷器吧？” 我想你安安静静的躺在我帮你精心设计的书架上，和周围的那些你的兄弟姐妹们和谐的交谈，就像是智者之间毫无遮拦的对话，那是你们伟大的灵魂有缘的碰撞和融合，又偶尔，我会来和你们打个招呼，或和你们中的某一位共处出在小屋子中，一盏台灯， 一杯清茶，度过一个思想火花四溅的午后。 我突然敞开怀笑了，哦， 藏书者的一大乐事，散逸人的一大福祉。我愿无声无息，无誉无毁躲在施比兹韦格的“书虫”面具后。没有人比这样的人更有富足感 了，因为附体的神灵或是精怪，会让爱着你们的人——与你们保持着最为亲密的拥有关系。就像是你们活在我的精神世界里面一样，我不会忍心舍弃你们而去。]]></content>
      <categories>
        <category>笑忘录</category>
      </categories>
      <tags>
        <tag>阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笑忘录（2）：疯子]]></title>
    <url>%2F2016%2F12%2F20%2F%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%882%EF%BC%89%EF%BC%9A%E7%96%AF%E5%AD%90%2F</url>
    <content type="text"><![CDATA[其一二十一世纪的第十三个年头，顺势来到的一天，圣索菲亚教堂前，游客不多，一些人交流，一些人快步，一些人，像我一样，对着教堂说话，像是对着自己。 丹尼斯大街与路易斯大街交汇的十字路口中央，高空中悬着一个雕塑，底下刷成灰色的塑像，凝视状。街边的的花满楼里，歌妓一刻不停，欢歌轻舞，北上的旅人在肉欲慰藉里且度今宵。 一个神经病突然发出声，左边，右边，上头，后面，四周的人头朝向了一个声音，他开始搜索眼神，轻率，讶异；唇角，僵硬，微微收拢；线条轮廓，百无聊赖的平淡，妇女的乳房变得苍白，持续的缓慢的最终鲜明确凿的凸现：抑郁寡欢。 格格不入，对峙，退却。他捂住耳朵，周围的表情幻化成压抑的声源，他没有停止搜寻的迹象，那是一件危险的事。 那是两天前发生的事，他来到了夜幕下的哈尔滨。 其二为了躲开原本那座城市刺眼的眼神，他才开始北上。几天前，他读毕一本书，八百余页，一整天，于是就再也看不清路人了，他没有变瞎，慢跑到紫禁城红墙下，已是深夜，红墙看不明颜色，但他知道颜色恰巧到中古悠然，也知道这是最契合这座城市意蕴的地方，他不准备停留，沿着府右街过景山前街，在南北池子大街的交汇处的东华门处停下，路上一对夜归的老夫妻互相搀扶前行，凝视十分钟，他没有意识到自己正对着他们傻笑，老夫妻被吓得赶忙加快脚步，一瘸一拐，他对着空白的街景继续傻笑。 十分钟时间不长，对他的外在来讲也不长，只是意识流开始缠上身，前两分钟，唐宋时代的风俗人情，三四分钟；来自柏拉图的洞穴的幻影浮现；五六分钟，霸王别姬，哪吒传奇；六七分钟，竖排繁体的《脂砚斋重评石头记》，优雅笃定的当下感，博尔赫斯在图书馆，沈从文的边城；最后三分钟，《洛丽塔》，《人间失格》，《一个陌生女人的来信》，《2666》，《荒原狼》，《树上的男爵》，《瓦尔登湖》，《伊豆的舞女》，《到灯塔去》，《都柏林人》，若即若离。 他累了，找个旅馆睡下，一头扎进了一个没有亮光的胡同，那是一家老店。沦落为蜗居在老城区角落的廉价旅馆，早已徒有虚名。窄小巷子中的灰白色混凝土小楼，如同所有以临时心态搭建的建筑，苟且度日。接待处服务员，胖而迟钝的中年妇女，磕瓜子看着面前的电视屏幕，麻木的表情。走廊上铺陈一长条红色地毯，清洗。睡觉对他来说还不算是一件煎熬的事，可以在十分钟内很快睡去。夜深了。 其三这是他常会有的那几天，业已习惯了这样的日子，例如还有几次，他一个人乘着火车，火车往南开，去的是一个中部城市，城市很大。 火车上，乘客不多。一些时间说话，一些时间睡觉，一些时间喝水与看望黑夜，一些时间思考不着边际的问题。10个小时后，火车抵达值夏的城市。下车，出地道。出站口两扇敞开木门，一角灰蓝色天空。微风缭绕。广场上出租汽车和三轮车颇显冷落，生意寥寥。在清晨的光景里，这个城略显闹腾，低矮旧楼被雨水洗刷成暗色，路边耸立广告牌上，词汇带有时光倒退30年的落伍气息。他的精神一振，知道来到正确的地方，稍许加快了脚步，往前。]]></content>
      <categories>
        <category>笑忘录</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[笑忘录（1）：第二十二封情书]]></title>
    <url>%2F2016%2F12%2F05%2F%E7%AC%91%E5%BF%98%E5%BD%95%EF%BC%881%EF%BC%89%EF%BC%9A%E7%AC%AC%E4%BA%8C%E5%8D%81%E4%BA%8C%E5%B0%81%E6%83%85%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[还记得上一次给你写信是你从南方回到北方的那个夜晚，我激动地关掉播放到一半的电影，颇似少女祈祷般，浸润在回忆过往的文字中，静静地等待着那个北方姑娘，回到属于她的地方。正是那天，我感觉到我在这里的生活渐渐有了生机。现在，距离你来的那一天已经过去快三年了，我已记不清我们有过多少次争吵，但回想起来那些模糊的场景，却总觉还是温暖的，那正是我们共同度过寻常生活的印记。昨天我们又争吵了。 回想这些天发生在我们之间的争吵、不合拍、嫌弃、甚至厌恶，我不知道有一些情感是否来的真实，但它确实从我们内心生发出来了，回头看那些生涩的言语，都让我倍感无助，感觉到心情的起伏已经无法被理智控制，就像河水猛兽般汹涌的侵蚀自己敏感却又极力克制的心，无法逆转，无法平复，根本无法像平常那样和周围的人说话。我总会在想：爱情是不痛苦的，它是纯快乐，不该掺进别的，尤其不该掺进痛苦，记得一首外国诗这样说：“啊！“爱情”！他们大大的误解了你！他们说你的甜蜜是痛苦，当你丰富的果实比任何果实都甜蜜。” 然而，我们真切的在爱情里尝到过痛苦，撕心裂肺的感觉。面对离别，我们却只能远望对方的背影渐渐离去；面对不理解，脑海里只会倍增对对方的不满；如若行为与本人价值观不符，便会用自己的意志控制对方，使对方的自由关进牢笼；如若争辩，我们也异常坚定的秉持自己的立场，尝试使用任何办法说服，如果对方的乖张让你感到舒适，战斗才停歇下来，否则就是一场无穷无尽的无聊的口舌之争，对，我们也常常使用这个套数让对方处于劣势（我不想在这里和你发生这些无聊的口舌之争）；服软的那一方看似更爱对方一些，不服输的总是多爱自己一些。 我们都在做一些我们自己也无法理解的事，我们沉浸在不理性，肆意泼洒的情绪里，我们在戏剧舞台上疯狂地表演，我们虚伪地接受爱情荒诞可笑的模样。有了爱，我们便有了最长的触角，伸向我们不曾触及的外部世界，伸向早已麻木倦怠的感官，但同时也把我们最柔软的部分显露在对方面前，大部分时间它得到了铠甲的庇护，而总有一天会有数不尽的痛触，让你质问爱情的真面目。 “什么是爱，爱是恒久忍耐，爱是彼此包容，爱是相对付出”出自圣经，高中的时候，和班上的同学你问我答，说得好轻巧，但当注入真实体验的时候，我们已经开始怀疑，我们所经历的爱情，到底是不是真正的爱情了。在现实的情境与企盼的盛景有落差时，我常有幻灭的感觉，爱情难道不是那样吗？爱情不该是这样的，它不会辜负良苦用心的我们的吧？ 罗兰巴特对爱情的不同阶段的场景有这么一段描述： 尽管恋人的表述仅仅是纷纭的情境，他们骚动起来全无秩序可言，不比在屋子里胡飞乱舞的苍蝇的轨迹更有规律，我还是能——至少是在回忆或者想像中——给爱情的发展找出一定的规律来。爱情的旅程似乎分为三个阶段（或者三幕戏）：首先是一见钟情，是闪电般的“迷上”“被俘虏”（我被一个形象迷住了）；然后便是一连串的相逢（约会、电话、情书、短途旅行），在此期间，我如痴如醉地发掘着情偶的完美，也就是说，对象与我的欲望之间那种完全出乎我意料的契合：这是初时的柔情，田园诗一般的光阴。在这幸福时光之后便是“一连串”恋爱的麻烦——持续不断的痛苦、创伤、焦虑、忧愁、怨恨、失望、窘迫还有陷阱——我们都成了里面的困兽，老是提心吊胆，生怕爱情衰退，怕这衰退不仅会毁了对方和我，还会毁了当初的缘分，那种神奇的情投意合。从这漫长的隧道中走出来，我又能重见天日了：这也许是因为我成功的找到了解决了不幸爱情的辩证出路——维持爱情、但脱离梦幻，冷静现实的面对它。） 我想我们正在经历如他所述的爱情低谷，我们眼前一片黑暗，对方的形象也因此而模糊、歪曲。爱情在这个年纪注定是不能冷静的，在荷尔蒙的鼓动下，它露出了狂傲不羁的模样，我们的一半是孩子，一半又竭力挣脱这样的形象。但总有一天会磨合出一个更为安静的环境供爱情生长，愿爱情如我们所期盼的那样，我们会脱离梦境、从那漫长的黑暗隧道中走出来，如你所说：内心笃定，爱则长久。我们为了彼此，做最好的自己，然后留给岁月。在长久的陪伴中，我们越来越相似于彼此，直到连微笑的弧度都一样。直到看着越来越像彼此的自己埋怨不起来。我想，这就是我们的爱情，它来源的没有任何预兆，也没有任何可以消失的理由。它并不逊色生离死别的爱情，相比之下更多了青春奋斗的最美好的回忆。 晚安。]]></content>
      <categories>
        <category>笑忘录</category>
      </categories>
      <tags>
        <tag>笑忘录</tag>
      </tags>
  </entry>
</search>
