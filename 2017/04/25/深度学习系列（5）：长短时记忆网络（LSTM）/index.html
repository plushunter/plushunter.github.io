<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="深度学习,长短时记忆网络," />





  <link rel="alternate" href="/atom.xml" title="Free Will" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/img/favicon.ico?v=5.1.0" />






<meta name="description" content="一、长期依赖问题（Long-Term Dependencies）循环神经网络（RNN）在实际应用中很难处理长距离依赖的问题。 有的时候，我们仅仅需要知道先前的信息来完成预测任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词，比如我们预测“the clouds are in the sky”最后的词的时候，我们不需要任何其他的上下文，很显然下一个词就是sky。在这种情况下，相关的信息与需要">
<meta name="keywords" content="深度学习,长短时记忆网络">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习系列（5）：长短时记忆网络（LSTM）">
<meta property="og:url" content="http://yoursite.com/2017/04/25/深度学习系列（5）：长短时记忆网络（LSTM）/index.html">
<meta property="og:site_name" content="Free Will">
<meta property="og:description" content="一、长期依赖问题（Long-Term Dependencies）循环神经网络（RNN）在实际应用中很难处理长距离依赖的问题。 有的时候，我们仅仅需要知道先前的信息来完成预测任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词，比如我们预测“the clouds are in the sky”最后的词的时候，我们不需要任何其他的上下文，很显然下一个词就是sky。在这种情况下，相关的信息与需要">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-24%20%E4%B8%8B%E5%8D%889.46.59.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-24%20%E4%B8%8B%E5%8D%889.47.31.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-24%20%E4%B8%8B%E5%8D%889.54.13.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-24%20%E4%B8%8B%E5%8D%889.55.44.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-24%20%E4%B8%8B%E5%8D%8810.04.47.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-24%20%E4%B8%8B%E5%8D%8810.05.53.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-25%20%E4%B8%8B%E5%8D%881.37.22.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-25%20%E4%B8%8B%E5%8D%881.48.40.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-25%20%E4%B8%8B%E5%8D%882.01.50.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-25%20%E4%B8%8B%E5%8D%882.06.23.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-25%20%E4%B8%8B%E5%8D%882.33.59.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-25%20%E4%B8%8B%E5%8D%882.41.32.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-25%20%E4%B8%8B%E5%8D%883.08.02.png">
<meta property="og:updated_time" content="2017-04-26T07:14:49.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习系列（5）：长短时记忆网络（LSTM）">
<meta name="twitter:description" content="一、长期依赖问题（Long-Term Dependencies）循环神经网络（RNN）在实际应用中很难处理长距离依赖的问题。 有的时候，我们仅仅需要知道先前的信息来完成预测任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词，比如我们预测“the clouds are in the sky”最后的词的时候，我们不需要任何其他的上下文，很显然下一个词就是sky。在这种情况下，相关的信息与需要">
<meta name="twitter:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-24%20%E4%B8%8B%E5%8D%889.46.59.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"hide"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>







  <title> 深度学习系列（5）：长短时记忆网络（LSTM） | Free Will </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Free Will</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-yiyu">
          <a href="/yiyu" rel="section">
            
            呓语
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/25/深度学习系列（5）：长短时记忆网络（LSTM）/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="狗皮膏药">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/img/v.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Free Will">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Free Will" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                深度学习系列（5）：长短时记忆网络（LSTM）
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-25T23:14:45+08:00">
                2017-04-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="一、长期依赖问题（Long-Term-Dependencies）"><a href="#一、长期依赖问题（Long-Term-Dependencies）" class="headerlink" title="一、长期依赖问题（Long-Term Dependencies）"></a>一、长期依赖问题（Long-Term Dependencies）</h2><p>循环神经网络（RNN）在实际应用中很难处理长距离依赖的问题。</p>
<p>有的时候，我们仅仅需要知道先前的信息来完成预测任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词，比如我们预测“the clouds are in the sky”最后的词的时候，我们不需要任何其他的上下文，很显然下一个词就是sky。在这种情况下，相关的信息与需要预测的词位置之间的间隔很小，而RNN可以学会使用较近距离的信息。</p>
<a id="more"></a>
<p><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-24%20%E4%B8%8B%E5%8D%889.46.59.png" alt="屏幕快照 2017-04-24 下午9.46.59"></p>
<p>但是到了一个更加复杂的场景，假设我们试着预测“I grew up in France……I speak fluent French”中最后的词，从这句话的信息来看，下一个词很有可能是一种语言的名字，但具体到是哪种语言，我们就需要在与之距离较远的“I grew up in France”中得到。这说明相关信息与当前预测位置之间的间隔就肯定变得相当的大。</p>
<p>不幸的是，在这个间隔不断增大时，RNN会丧失学习到连接如此远的信息的能力。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-24%20%E4%B8%8B%E5%8D%889.47.31.png" alt="屏幕快照 2017-04-24 下午9.47.31"></p>
<p>当然，在理论上，RNN绝对可以处理这样的长期依赖问题。人们可以通过调参来解决，但是在实践中，RNN肯定不能够成功学习到这些知识。<a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="external">Bengio, et al. (1994)</a>等人对该问题进行了深入的研究，它们发现一些使训练RNN变得非常困难的相当根本的原因。</p>
<p>既然找到了问题的原因，那我们就能解决它。从问题的定位到解决，科学家们大概花了7、8年的时间。终于有一天，Hochreiter和Schmidhuber两位科学家发明出长短时记忆网络，一举解决了这个问题。</p>
<h2 id="二、LSTM的核心思想"><a href="#二、LSTM的核心思想" class="headerlink" title="二、LSTM的核心思想"></a>二、LSTM的核心思想</h2><p>Long Short Term网络，一般就叫做LSTM，是一种特殊的RNN变体，它可以学习长期依赖信息。LSTM由Hochreiter和Schmidhuber在1997年提出，并在近期被Alex Graves进行了改良和推广。在很多问题上，LSTM都取得了相当巨大的成功，并得到了广泛的使用。<br>LSTM通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是LSTM的默认属性，而非需要付出很大的代价才能获得的能力！<br>所有的RNN都具有一种重复神经网络模块的链式的形式。在标准的RNN中，这个重复的模块只有一个非常简单的结构，例如一个tanh层。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-24%20%E4%B8%8B%E5%8D%889.54.13.png" alt="屏幕快照 2017-04-24 下午9.54.13"><br>LSTM同样是这样的结构，但是其中重复的模块拥有一个不同的结构。不同于单一神经网络层，这里有四个以非常特殊的方式进行交互的小器件。<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-24%20%E4%B8%8B%E5%8D%889.55.44.png" alt="屏幕快照 2017-04-24 下午9.55.44"><br>图中每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表pointwise的操作，比如向量的和，而黄色的矩阵就是学习到的神经网络层。</p>
<p>LSTM的关键在于细胞（Cell），水平线在细胞内贯穿运行。细胞类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在水平线上很容易保持不变。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-24%20%E4%B8%8B%E5%8D%8810.04.47.png" alt="屏幕快照 2017-04-24 下午10.04.47"></p>
<p>LSTM通过精心设计“门”结构来去除或者增加信息到Cell上。门是一种让信息选择式通过的方法（过滤器）。它们包含一个sigmoid神经网络层和一个pointwise乘法操作。<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-24%20%E4%B8%8B%E5%8D%8810.05.53.png" alt="屏幕快照 2017-04-24 下午10.05.53"></p>
<p>Sigmoid层输出0到1之间的数值，描述每个部分有多少量可以通过。0代表“不许任何量通过”，1就指“允许任意量通过”</p>
<h2 id="三、LSTM的前向计算"><a href="#三、LSTM的前向计算" class="headerlink" title="三、LSTM的前向计算"></a>三、LSTM的前向计算</h2><p>LSTM用两个门来控制单元状态Cell的内容，一个是遗忘门（forget gate），它决定了上一时刻的单元状态$c_t-1$有多少保留到当前时刻$c_t$；另一个是输入门（input gate），他决定了当前时刻网络的输入$x_t$有多少保存到单元状态$c_t$。LSTM用输出门（output gate）来控制单元状态$c_t$有多少输出到LSTM的当前输出值$h_t$。</p>
<h3 id="3-1-遗忘门"><a href="#3-1-遗忘门" class="headerlink" title="3.1 遗忘门"></a>3.1 遗忘门</h3><p>我们先看一下遗忘门：$$f_t=\sigma(W_f·[h_{t-1,x_t}]+b_f)$$上式中，$W_f$是遗忘门的权重矩阵，$[h_{t-1},x_t]$表示把两个向量连接成一个更长的向量，$b_f$是遗忘门的偏置项，$\sigma$是sigmoid函数。若输入的维度是$d_x$，隐藏层的维度是$d_h$，单元状态的维度是$d_c$（通常$d_c=d_h$），则遗忘门的权重矩阵$W_f$维度是$d_c×(d_h+d_x)$。事实上，权重矩阵$W_f$都是两个矩阵拼接而成的：一个是$W_{fh}$，它对应着输入项$h_{t-1}$，其维度为$d_c×d_h$；一个是$W_{fx}$，它对应着输入项$x_t$，其维度为$d_c×d_x$。$W_f$可以写为：$$<br>\left[W_f\right]\left[\begin{array}{c}<br>    h_{t-1}\\<br>    x_t\\<br>\end{array}\right]=\left[\begin{matrix}<br>    W_{fh}&amp;        W_{fx}\\<br>\end{matrix}\right]\left[\begin{array}{c}<br>    h_{t-1}\\<br>    x_t\\<br>\end{array}\right]=W_{fh}·h_{t-1}+W_{fx}x_t<br>$$</p>
<p>所以总结一下，遗忘门的作用为控制有多少上一时刻的memory cell中的信息可以累积到当前时刻的memory cell中。其数学公式可以写作：$$f_t = sigmoid(W_{fx}·x_t+W_{fh}·h_{t-1}+b_i)$$其计算图示如下：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-25%20%E4%B8%8B%E5%8D%881.37.22.png" alt="屏幕快照 2017-04-25 下午1.37.22"></p>
<h3 id="3-2-输入门"><a href="#3-2-输入门" class="headerlink" title="3.2 输入门"></a>3.2 输入门</h3><p>接下来看输入门：$$i_t=\sigma(W_i·[h_{t-1},x_t]+b_i)$$上式中，$W_i$是输入们的权重矩阵，$b_i$是输入门的偏置项。下图表示了输入门的计算：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-25%20%E4%B8%8B%E5%8D%881.48.40.png" alt="屏幕快照 2017-04-25 下午1.48.40"></p>
<p>接下来，我们计算用于描述当前输入的单元状态$\tilde{c}_t$，它是根据上一次的输出和本次输入来计算的：$$\tilde{c}_t=\tan\textrm{h}\left(W_c·\left[h_{t-1},x_t\right]+b_c\right)$$下图是$\tilde{c}_t$的计算：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-25%20%E4%B8%8B%E5%8D%882.01.50.png" alt="屏幕快照 2017-04-25 下午2.01.50"><br>现在，我们计算当前时刻的单元状态$c_t$。它是由上一次的单元状态$c_{t-1}$按元素乘以遗忘门$f_t$，再用当前输入的单元状态$\tilde{c}_t$按元素乘以输入门$i_t$，再将两个积加和产生的：<br>$$<br>c_t=f_t°c_{t-1}+i_t°\tilde{c}_t<br>$$下图是$c_t$的计算图示：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-25%20%E4%B8%8B%E5%8D%882.06.23.png" alt="屏幕快照 2017-04-25 下午2.06.23"><br>这样，我们就把LSTM关于当前的记忆$\tilde{c}_t$和长期的记忆$c_{t-1}$组合在一起，形成了新的单元状态$c_t$。由于遗忘门的控制，它可以保存很久很久之前的信息，由于输入门的控制，它又可以避免当前无关紧要的内容进入记忆。</p>
<h3 id="3-3-输出门"><a href="#3-3-输出门" class="headerlink" title="3.3 输出门"></a>3.3 输出门</h3><p>下面，我们要看看输入们，它控制了长期记忆对当前输出的影响：$$o_t=\sigma(W_o·[h_{t-1},x_t]+b_o)$$下图表示输出门的计算：<br><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-25%20%E4%B8%8B%E5%8D%882.33.59.png" alt="屏幕快照 2017-04-25 下午2.33.59"><br>LSTM最终的输出，是由输出门和单元状态共同确定的：$$<br>h_t=o_t°\tan\textrm{h}\left(c_t\right)<br>$$下图表示LSTM最终输出的计算：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-25%20%E4%B8%8B%E5%8D%882.41.32.png" alt="屏幕快照 2017-04-25 下午2.41.32"></p>
<h2 id="四、LSTM的训练"><a href="#四、LSTM的训练" class="headerlink" title="四、LSTM的训练"></a>四、LSTM的训练</h2><p>LSTM的训练算法仍然是反向传播算法，它主要有下面三个步骤：</p>
<ul>
<li>1）前向计算每个神经元的输出值，对于LSTM来说，即$f_t、i_t、c_t、o_t、h_t$五个向量的值。</li>
<li>2）反向计算每个神经元的误差项$\delta$值。与循环神经网络一样，LSTM误差项的反向传播也是包括两个方向：一个是沿着时间的反向传播，即从当前t时刻开始，计算每个时刻的误差项；一个是将误差项向上一层传播。</li>
<li>3）根据相应的误差项，计算每个权重的梯度。</li>
</ul>
<p>首先，我们队推导中用到的一些公式、符号做一下必要的说明。</p>
<p>接下来的推导中，我们设定gate的激活函数为sigmoid函数，输出的激活函数为tanh函数。它们的导数分别为：$$<br>\sigma\left(z\right)=y=\frac{1}{1+e^{-z}}<br>$$$$<br>\sigma ‘\left(z\right)=y\left(1-y\right)<br>$$$$<br>\tan\textrm{h}\left(z\right)=y=\frac{e^z-e^{-z}}{e^z+e^{-z}}<br>$$$$<br>\tan\textrm{h’}\left(z\right)=1-y^2<br>$$<br>从上面可以看出，sigmoid和tanh函数的导数都是原函数的函数。这样，我们一旦计算原函数的值，就可以用它来计算出导数的值。</p>
<p>LSTM需要学习的参数共有8组，分别是：遗忘门的权重矩阵$W_f$和偏置项$b_f$、输入门的权重矩阵$W_i$和偏置项$b_i$、输出门的权重矩阵$W_o$和偏置项$b_o$，以及计算单元状态的权重矩阵$W_c$和偏置项$b_c$，因为权重矩阵的两部分在反向传播中使用不同的公式，因此在后续的推导中，权重矩阵$W_f、W_i、W_c、W_o$都会被写成分开的两个矩阵：$W_{fh}、W_{fx}、W_{ih}、W_{ix}、W_{oh}、W_{ox}、W_{ch}、W_{cx}$。</p>
<p>我们解释一下按元素乘$o$符号。当$o$作用于两个向量时，运算如下：$$<br>a°b=\left[\begin{array}{c}<br>    a_1\\<br>    a_2\\<br>    ···\\<br>    a_n\\<br>\end{array}\right]°\left[\begin{array}{c}<br>    b_1\\<br>    b_2\\<br>    ···\\<br>    b_n\\<br>\end{array}\right]=\left[\begin{array}{c}<br>    a_1b_1\\<br>    a_2b_2\\<br>    ···\\<br>    a_nb_n\\<br>\end{array}\right]<br>$$当$o$作用于一个向量和一个矩阵时，运算如下：$$<br>a°X=\left[\begin{array}{c}<br>    a_1\\<br>    a_2\\<br>    ···\\<br>    a_n\\<br>\end{array}\right]°\left[\begin{matrix}<br>    x_{11}&amp;        x_{12}&amp;        ···&amp;        x_{1n}\\<br>    x_{21}&amp;        x_{22}&amp;        ···&amp;        x_{2n}\\<br>    ···&amp;        ···&amp;        ···&amp;        ···\\<br>    x_{n1}&amp;        x_{n2}&amp;        ···&amp;        x_{nn}\\<br>\end{matrix}\right]=\left[\begin{matrix}<br>    a_1x_{11}&amp;        a_1x_{12}&amp;        ···&amp;        a_{1n}x_{1n}\\<br>    a_2x_{21}&amp;        a_2x_{22}&amp;        ···&amp;        a_2x_{2n}\\<br>    ···&amp;        ···&amp;        ···&amp;        ···\\<br>    a_nx_{n1}&amp;        a_nx_{n2}&amp;        ···&amp;        a_nx_{nn}\\<br>\end{matrix}\right]<br>$$当$o$作用于两个矩阵时，两个矩阵对应位置的元素相乘。按元素乘可以再某些情况下简化矩阵和向量的运算。例如，当一个对角矩阵右乘一个矩阵时，相当于用对角矩阵的对角线组成的向量按元素乘那个矩阵：$diag[a]·X=a °X$当一个行向量右乘一个对角矩阵时，相当于这个行向量按元素乘那个矩阵对角线组成的向量：$$a^T·diag[b]=a°b$$上面这俩点，在后续推导中会多次用到。</p>
<p>在t时刻，LSTM的输出值为$h_t$。我们定义t时刻的误差项$\delta_t$为：<br>$$\delta_t=\frac{\partial E}{\partial h_t}$$注意，这里假设误差项是损失函数对输出值的导数，而不是对加权输入$net^l$的导数。因为LSTM有四个加权输入，分别对应$f_t、i_t、c_t、o_t$，我们希望往上一层传递一个误差项而不是四个。但我们仍然要定义出这四个加权输入，以及他们对应的误差项。</p>
<p>$$$net_{f,t}=W_f[h_{t-1},x_t]+b_f=W_{fh}h_{t-1}+W_{fx}x_t+b_f$$$$net_{i,t}=W_i[h_{t-1},x_t]+b_i=W_{ih}h_{t-1}+W_{ix}x_t+b_i$$$$net_{\tilde{c},t}=W_c\left[h_{t-1},x_t\right]+b_c=W_{ch}h_{t-1}+W_{cx}x_t+b_c$$$$net_{o,t}=W_o\left[h_{t-1},x_t\right]+b_o=W_{oh}h_{t-1}+W_{ox}x_t+b_o$$$$<br>\delta_{f,t}=\frac{\partial E}{\partial net_{f,t}}<br>$$$$<br>\delta_{i,t}=\frac{\partial E}{\partial net_{i,t}}<br>$$$$<br>\delta_{\tilde{c},t}=\frac{\partial E}{\partial net_{c,t}}<br>$$$$<br>\delta_{o,t}=\frac{\partial E}{\partial net_{o,t}}<br>$$</p>
<h3 id="4-1-误差项沿时间的反向传播"><a href="#4-1-误差项沿时间的反向传播" class="headerlink" title="4.1 误差项沿时间的反向传播"></a>4.1 误差项沿时间的反向传播</h3><p>沿时间反向传导误差项，就是要计算出$t-1$时刻的误差项$\delta_{t-1}$。$$<br>\delta_{t-1}^{T}=\frac{\partial E}{\partial h_{t-1}}<br>$$$$<br>=\frac{\partial E}{\partial h_t}\frac{\partial h_t}{\partial h_{t-1}}<br>$$$$<br>=\delta_{t}^{T}\frac{\partial h_t}{\partial h_{t-1}}<br>$$我们知道，$\frac{\partial h_t}{\partial h_{t-1}}$是一个jacobian矩阵。如果隐藏层$h$的维度是N的话，那么它就是一个$N×N$矩阵。为了求出它，我们列出$h_t$的计算公式：$$c_t=f_t°c_{t-1}+i_t°\tilde{c}_t$$$$h_t=o_t°\tan\textrm{h}\left(c_t\right)$$显然，$o_t、f_t、i_t、\tilde{c}_t$都是$h_{t-1}$的函数，那么，利用全导数公式可得：</p>
<h3 id="4-2-将误差项传递到上一层"><a href="#4-2-将误差项传递到上一层" class="headerlink" title="4.2 将误差项传递到上一层"></a>4.2 将误差项传递到上一层</h3><h3 id="4-3-权重梯度的计算"><a href="#4-3-权重梯度的计算" class="headerlink" title="4.3 权重梯度的计算"></a>4.3 权重梯度的计算</h3><h2 id="五、LSTM的变体—GRU（Gated-Recurrent-Unit）"><a href="#五、LSTM的变体—GRU（Gated-Recurrent-Unit）" class="headerlink" title="五、LSTM的变体—GRU（Gated Recurrent Unit）"></a>五、LSTM的变体—GRU（Gated Recurrent Unit）</h2><p>前面我们讲了一种最为普通的LSTM，事实上LSTM存在很多变体，许多论文中的LSTM都或多或少的不太一样。只要遵守几个关键点，就可以根据需求设计需要的Gated RNNS。在众多的LSTM变体中，GRU也许是最成功的一种。它对LSTM做了很多简化，同时却保持着和LSTM相同的效果。因此，GRU最近变得越来越流行。</p>
<p>GRU对LSTM做了两个大改动：</p>
<ul>
<li>1）将输入门、遗忘门、输出门变为两个门：更新门（Update Gate）$z_t$和重置门（Reset Gate）$r_t$。</li>
<li>2）将单元状态与输出合并为一个状态：$h$</li>
</ul>
<p>GRU的前向计算公式为：<br>$$z_t=\sigma(W_z·[h_{t-1},x_t])$$$$r_t=\sigma(W_r·[h_{t-1},x_t])$$$$<br>\tilde{h}_t=\tan\textrm{h}\left(W·\left[r_t°h_{t-1},x_t\right]\right)<br>$$$$<br>h=\left(1-z_t\right)°h_{t-1}+z_t°\tilde{h}_t<br>$$下图是GRU的示意图：<img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-25%20%E4%B8%8B%E5%8D%883.08.02.png" alt="屏幕快照 2017-04-25 下午3.08.02"><br>GRU的训练算法比LSTM相对也要简单一些</p>
<p>当然还有很多其他的变体，如<a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="external"> Gers &amp; Schmidhuber (2000) </a>提出的LSTM变体增加了“peephole connection”；另一种变体使用coupled 遗忘和输入门对遗忘和需要的信息一同做出决定。<a href="https://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="external">Yao, et al. (2015)</a> 提出的Depth Gated RNN。还有用一些完全不同的观点来解决长期依赖的问题，如<a href="https://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="external">Koutnik, et al. (2014) </a>提出的Clockwork RNN。</p>
<p>但<a href="https://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="external">Greff, et al. (2015)</a>给出了流行变体的比较，结论是它们基本上是一样的。<a href="http://proceedings.mlr.press/v37/jozefowicz15.pdf" target="_blank" rel="external">Jozefowicz, et al. (2015) </a>则在超过一万种RNN架构上进行了测试，发现一些架构在某些任务上也取得了比LSTM更好的结果。</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/长短时记忆网络/" rel="tag"># 长短时记忆网络</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/04/26/深度学习系列（6）：递归神经网络/" rel="next" title="深度学习系列（6）：递归神经网络">
                <i class="fa fa-chevron-left"></i> 深度学习系列（6）：递归神经网络
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/04/23/深度学习系列（4）：循环神经网络（RNN）/" rel="prev" title="深度学习系列（4）：循环神经网络（RNN）">
                深度学习系列（4）：循环神经网络（RNN） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/img/v.png"
               alt="狗皮膏药" />
          <p class="site-author-name" itemprop="name">狗皮膏药</p>
          <p class="site-description motion-element" itemprop="description">在隆冬，我终于知道，我身上有一个不可战胜的夏天</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">40</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">43</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、长期依赖问题（Long-Term-Dependencies）"><span class="nav-text">一、长期依赖问题（Long-Term Dependencies）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、LSTM的核心思想"><span class="nav-text">二、LSTM的核心思想</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三、LSTM的前向计算"><span class="nav-text">三、LSTM的前向计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-遗忘门"><span class="nav-text">3.1 遗忘门</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-输入门"><span class="nav-text">3.2 输入门</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-输出门"><span class="nav-text">3.3 输出门</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四、LSTM的训练"><span class="nav-text">四、LSTM的训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-误差项沿时间的反向传播"><span class="nav-text">4.1 误差项沿时间的反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-将误差项传递到上一层"><span class="nav-text">4.2 将误差项传递到上一层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-权重梯度的计算"><span class="nav-text">4.3 权重梯度的计算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#五、LSTM的变体—GRU（Gated-Recurrent-Unit）"><span class="nav-text">五、LSTM的变体—GRU（Gated Recurrent Unit）</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">狗皮膏药</span>
</div>



        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  



  




	




  
  

  
  


  

  

  


</body>
</html>
