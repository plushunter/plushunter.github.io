<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="机器学习,逻辑斯谛回归," />





  <link rel="alternate" href="/atom.xml" title="Free Will" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/img/favicon.ico?v=5.1.0" />






<meta name="description" content="一、逻辑斯谛分布介绍逻辑斯谛回归模型之前，首先看一个并不常见的概率分布，即逻辑斯谛分布。设$X$是连续随机变量，$X$服从逻辑斯谛分布是指$X$具有下列的分布函数和密度函数：">
<meta name="keywords" content="机器学习,逻辑斯谛回归">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习算法系列（3）：逻辑斯谛回归">
<meta property="og:url" content="http://yoursite.com/2017/01/12/机器学习算法系列（3）：逻辑斯谛回归/index.html">
<meta property="og:site_name" content="Free Will">
<meta property="og:description" content="一、逻辑斯谛分布介绍逻辑斯谛回归模型之前，首先看一个并不常见的概率分布，即逻辑斯谛分布。设$X$是连续随机变量，$X$服从逻辑斯谛分布是指$X$具有下列的分布函数和密度函数：">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-04%20%E4%B8%8B%E5%8D%888.55.00.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-06%20%E4%B8%8B%E5%8D%8811.55.11.png">
<meta property="og:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-07%20%E4%B8%8A%E5%8D%8812.00.18.png">
<meta property="og:updated_time" content="2017-04-11T15:46:18.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习算法系列（3）：逻辑斯谛回归">
<meta name="twitter:description" content="一、逻辑斯谛分布介绍逻辑斯谛回归模型之前，首先看一个并不常见的概率分布，即逻辑斯谛分布。设$X$是连续随机变量，$X$服从逻辑斯谛分布是指$X$具有下列的分布函数和密度函数：">
<meta name="twitter:image" content="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-04%20%E4%B8%8B%E5%8D%888.55.00.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"hide"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>







  <title> 机器学习算法系列（3）：逻辑斯谛回归 | Free Will </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  








 <!-- hexo-inject:begin --><!-- hexo-inject:end --><script src="https://s22.cnzz.com/z_stat.php?id=1262584047&web_id=1262584047" language="JavaScript"></script>





  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Free Will</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-yiyu">
          <a href="/yiyu" rel="section">
            
            呓语
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/01/12/机器学习算法系列（3）：逻辑斯谛回归/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="狗皮膏药">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/img/v.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Free Will">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Free Will" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                机器学习算法系列（3）：逻辑斯谛回归
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-01-12T23:14:45+08:00">
                2017-01-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="一、逻辑斯谛分布"><a href="#一、逻辑斯谛分布" class="headerlink" title="一、逻辑斯谛分布"></a>一、逻辑斯谛分布</h2><p>介绍逻辑斯谛回归模型之前，首先看一个并不常见的概率分布，即逻辑斯谛分布。设$X$是连续随机变量，$X$服从逻辑斯谛分布是指$X$具有下列的分布函数和密度函数：</p>
<a id="more"></a>
<p>$$<br>F\left(x\right)=P\left(X\le x\right)=\frac{1}{1+e^{-\left(x-\mu\right)/\gamma}}<br>$$$$<br>f\left(x\right)=F^,\left(x\right)=\frac{e^{-\left(x-\mu\right)/\gamma}}{\gamma\left(1+e^{-\left(x-\mu\right)/\gamma}\right)^2}<br>$$<br>式中，$\mu$为位置参数，$\gamma&gt;0 $为形状参数。<br>逻辑斯谛的分布的密度函数$f(x)$和分布函数$F(x)$的图形如下图所示。其中分布函数属于逻辑斯谛函数，其图形为一条$S$形曲线。该曲线以点$(\mu,\frac{1}{2})$为中心对称，即满足$$<br>F\left(-x+\mu\right)-\frac{1}{2}=-F\left(x+\mu\right)+\frac{1}{2}<br>$$曲线在中心附近增长较快，在两端增长速度较慢。形状参数$\gamma$的值越小，曲线在中心附近增长得越快。</p>
<center><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-04%20%E4%B8%8B%E5%8D%888.55.00.png" alt="屏幕快照 2017-04-04 下午8.55.00"></center>

<h2 id="二、逻辑斯谛回归模型"><a href="#二、逻辑斯谛回归模型" class="headerlink" title="二、逻辑斯谛回归模型"></a>二、逻辑斯谛回归模型</h2><p>线性回归的应用场合大多是回归分析，一般不用在分类问题上。原因可以概括为以下两个：</p>
<ul>
<li>1）回归模型是连续型模型，即预测出的值都是连续值（实数值），非离散值；</li>
<li>2）预测结果受样本噪声的影响比较大。</li>
</ul>
<h3 id="2-1-LR模型表达式"><a href="#2-1-LR模型表达式" class="headerlink" title="2.1 LR模型表达式"></a>2.1 LR模型表达式</h3><p>LR模型表达式为参数化的逻辑斯谛函数（默认参数$\mu=0,\gamma=1$）,即$$<br>h_{\theta}\left(x\right)=\frac{1}{1+e^{-\theta^Tx}}$$其中$h_\theta{(x)}$作为事件结果$y=1$的概率取值。这里,$x\in R^{n+1},y\in \{1,0\},\theta\in R^{n+1}$是权值向量。其中权值向量$w$中包含偏置项，即$w=(w_0,w_1,···,w_n)，x=(1,x_1,x_2,···,x_n)$</p>
<h3 id="2-2-理解LR模型"><a href="#2-2-理解LR模型" class="headerlink" title="2.2 理解LR模型"></a>2.2 理解LR模型</h3><h4 id="2-2-1-对数几率"><a href="#2-2-1-对数几率" class="headerlink" title="2.2.1 对数几率"></a>2.2.1 对数几率</h4><p>一个事件发生的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是$p$，那么该事件的几率为$\frac{p}{1-p}$，该事件的对数几率（log odds）或logit函数是：$$<br>logit\left(p\right)=\log\frac{p}{1-p}<br>$$对LR而言，根据模型表达式可以得到：$$<br>\log\frac{h_{\theta}\left(x\right)}{1-h_{\theta}\left(x\right)}=\theta^Tx<br>$$即在LR模型中，输出$y=1$的对数几率是输入$x$的线性函数。或者说输出$y=1$的对数几率是由输入$x$的线性函数表示的模型，即LR模型</p>
<h4 id="2-2-2-函数映射"><a href="#2-2-2-函数映射" class="headerlink" title="2.2.2 函数映射"></a>2.2.2 函数映射</h4><p>除了从对数几率的角度理解LR外，从函数映射也可以理解LR模型。<br>考虑对输入实例$x$进行分类的线性表达式$\theta^T$，其值域为实数域。通过LR模型表达式可以将线性函数$\theta^Tx$的结果映射到(0,1)区间，取值表示为结果为1的概率（在二分类场景中）。<br>线性函数的值越接近于正无穷大，概率值就越接近1；反之，其值越接近于负无穷，概率值就越接近0。这样的模型就是LR模型。<br>LR本质上还是线性回归，知识特征到结果的映射过程中加了一层函数映射（即sigmoid函数），即先把特征线性求和，然后使用sigmoid函数将线性和约束至（0，1）之间，结果值用于二分或回归预测。</p>
<h4 id="2-2-3-概率解释"><a href="#2-2-3-概率解释" class="headerlink" title="2.2.3 概率解释"></a>2.2.3 概率解释</h4><p>LR模型多用于解决二分类问题，如广告是否被点击（是/否）、商品是否被购买（是/否）等互联网领域中常见的应用场景。但是实际场景中，我们又不把它处理成“绝对的”分类问题，而是用其预测值作为事件发生的概率。</p>
<p>这里从事件、变量以及结果的角度给予解释。</p>
<p>我们所能拿到的训练数据统称为观测样本。问题：样本是如何生成的？</p>
<p>一个样本可以理解为发生的一次事件，样本生成的过程即事件发生的过程。对于0/1分类问题来讲，产生的结果有两种可能，符合伯努利试验的概率假设。因此，我们可以说样本的生成过程即为伯努利试验过程，产生的结果（0/1）服从伯努利分布。这里我们假设结果为1的概率为$h_\theta{(x)}$，结果为0的概率为$1-h_\theta{(x)}$。</p>
<p>那么对于第$i$个样本，概率公式表示如下：<br>$$P(y^{(i)}=1|x^{(i)};\theta )=h_\theta{(x^{(i)})}$$$$P(y^{(i)}=0  |x^{(i)};\theta )=1- h_\theta{(x^{(i)})}$$<br>将上面两个公式合并在一起，可得到第$i$个样本正确预测的概率：<br>$$P(y^{(i)}|x^{(i)};\theta)=(h_\theta(x^{(i)})^{y(i)})·（1-h_\theta(x^{(i)})^{y(i)}）$$<br>上式是对一个样本进行建模的数据表达。对于所有的样本，假设每条样本生成过程独立，在整个样本空间中（N个样本）的概率分布（即似然函数）为：$$<br>P\left(Y|X;\theta\right)=\prod_{i=1}^N{\left(h_{\theta}\left(x^{\left(i\right)}\right)^{y^{\left(i\right)}}\left(1-h_{\theta}\left(x^{\left(i\right)}\right)^{1-y^{\left(i\right)}}\right)\right)}<br>$$通过极大似然估计（Maximum Likelihood Evaluation，简称MLE）方法求概率参数。具体地，第三节给出了通过随机梯度下降法（SGD）求参数。</p>
<h2 id="三、模型参数估计"><a href="#三、模型参数估计" class="headerlink" title="三、模型参数估计"></a>三、模型参数估计</h2><h3 id="3-1-Sigmoid函数"><a href="#3-1-Sigmoid函数" class="headerlink" title="3.1 Sigmoid函数"></a>3.1 Sigmoid函数</h3><center><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-06%20%E4%B8%8B%E5%8D%8811.55.11.png" alt="屏幕快照 2017-04-06 下午11.55.11"></center>

<p>上图所示即为sigmoid函数，它的输入范围为$-\infty\rightarrow +\infty$，而值域刚好为$(0,1)$，正好满足概率分布为$(0,1)$的要求。用概率去描述分类器，自然要比阈值要来的方便。而且它是一个单调上升的函数，具有良好的连续性，不存在不连续点。</p>
<p>此外非常重要的，sigmoid函数求导后为：</p>
<p><center><img src="http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-07%20%E4%B8%8A%E5%8D%8812.00.18.png" alt="屏幕快照 2017-04-07 上午12.00.18"></center><br>以下的推导中会用到，带来了很大的便利。</p>
<h3 id="3-2-参数估计推导"><a href="#3-2-参数估计推导" class="headerlink" title="3.2 参数估计推导"></a>3.2 参数估计推导</h3><p>上一节的公式不仅可以理解为在已观测的样本空间中的概率分布表达式。如果从统计学的角度可以理解为参数$\theta$似然性的函数表达式（即似然函数表达式）。参数在整个样本空间的似然函数可表示为：$$<br>L\left(\theta\right)=P\left(\overrightarrow{Y}|X;\theta\right)<br>$$$$<br>=\prod_{i=1}^N{P\left(y^{\left(i\right)}\parallel x^{\left(i\right)};\theta\right)}<br>$$$$<br>=\prod_{i=1}^N{\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)^{y\left(i\right)}\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)^{1-y^{\left(i\right)}}}<br>$$为了方便参数求解，对这个公式取对数，可得对数似然函数：$$<br>l\left(\theta\right)=\sum_{i=1}^N{\log l\left(\theta\right)}<br>$$$$<br>=\sum_{i=1}^N{y^{\left(i\right)}\log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)}<br>$$最大化对数似然函数其实就是最小化交叉熵误差（Cross Entropy Error）。<br>先不考虑累加和，我们针对每一个参数$w_j$求偏导：$$<br>\frac{\partial}{\partial\theta_j}l\left(\theta\right)=\left(y\frac{1}{h_{\theta}\left(x\right)}-\left(1-y\right)\frac{1}{1-h_{\theta}\left(x\right)}\right)\frac{\partial}{\partial\theta_j}h_{\theta}\left(x\right)<br>$$$$<br>=\left(\frac{y\left(1-h_{\theta}\left(x\right)\right)-\left(1-y\right)h_{\theta}\left(x\right)}{h_{\theta}\left(x\right)\left(1-h_{\theta}\left(x\right)\right)}\right)h_{\theta}\left(x\right)\left(1-h_{\theta}\left(x\right)\right)\frac{\partial}{\partial\theta_j}\theta^Tx<br>$$$$<br>=\left(y-h_{\theta}\left(x\right)\right)x_j<br>$$最后，通过扫描样本，迭代下述公式可求得参数：$$<br>\theta_j:=\theta_j+a\left(y^{\left(i\right)}-h_{\theta}\left(x^{\left(i\right)}\right)\right)x_{j}^{\left(i\right)}<br>$$其中$a$表示学习率，又称学习步长。此外还有Batch GD，共轭梯度，拟牛顿法（LBFGS），ADMM分布学习算法等都可以用来求解参数。另作优化算法一章进行补充。</p>
<p>以上的推导是LR模型的核心部分，在机器学习相关面试中，LR模型公式推导可能是考察频次最高的一个点。要将其熟练推导。</p>
<h2 id="四、Softmax回归"><a href="#四、Softmax回归" class="headerlink" title="四、Softmax回归"></a>四、Softmax回归</h2><p><code>待补充</code></p>
<h2 id="五、参考资料"><a href="#五、参考资料" class="headerlink" title="五、参考资料"></a>五、参考资料</h2><p><a href="http://www.cnblogs.com/jerrylead/archive/2011/03/05/1971867.html" target="_blank" rel="external"><code>对线性回归，logistic回归和一般回归的认识</code></a></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/逻辑斯谛回归/" rel="tag"># 逻辑斯谛回归</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/01/15/胡乱记/" rel="next" title="胡乱记">
                <i class="fa fa-chevron-left"></i> 胡乱记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/01/10/离别/" rel="prev" title="离别记">
                离别记 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/img/v.png"
               alt="狗皮膏药" />
          <p class="site-author-name" itemprop="name">狗皮膏药</p>
          <p class="site-description motion-element" itemprop="description">在隆冬，我终于知道，我身上有一个不可战胜的夏天</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">47</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">64</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、逻辑斯谛分布"><span class="nav-text">一、逻辑斯谛分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、逻辑斯谛回归模型"><span class="nav-text">二、逻辑斯谛回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-LR模型表达式"><span class="nav-text">2.1 LR模型表达式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-理解LR模型"><span class="nav-text">2.2 理解LR模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-对数几率"><span class="nav-text">2.2.1 对数几率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-函数映射"><span class="nav-text">2.2.2 函数映射</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-3-概率解释"><span class="nav-text">2.2.3 概率解释</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三、模型参数估计"><span class="nav-text">三、模型参数估计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Sigmoid函数"><span class="nav-text">3.1 Sigmoid函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-参数估计推导"><span class="nav-text">3.2 参数估计推导</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四、Softmax回归"><span class="nav-text">四、Softmax回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#五、参考资料"><span class="nav-text">五、参考资料</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">狗皮膏药</span>
</div>



        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  



  




	




  
  

  
  


  

  

  


</body>
</html>
